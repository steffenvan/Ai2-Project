<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004785">
<note confidence="0.815672">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 145-147, Lisbon, Portugal, 2000.
</note>
<title confidence="0.962915">
Shallow Parsing as Part-of-Speech Tagging*
</title>
<author confidence="0.995964">
Miles Osborne
</author>
<affiliation confidence="0.998755">
University of Edinburgh
Division of Informatics
</affiliation>
<address confidence="0.7207165">
2 Buccleuch Place
Edinburgh EH8 9LW , Scotland
</address>
<email confidence="0.951461">
osborne@cogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.978123" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885714285714">
Treating shallow parsing as part-of-speech tag-
ging yields results comparable with other, more
elaborate approaches. Using the CoNLL 2000
training and testing material, our best model
had an accuracy of 94.88%, with an overall FB1
score of 91.94%. The individual FB1 scores for
NPs were 92.19%, VPs 92.70% and PPs 96.69%.
</bodyText>
<sectionHeader confidence="0.995506" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912875">
Shallow parsing has received a reasonable
amount of attention in the last few years (for
example (Ramshaw and Marcus, 1995)). In
this paper, instead of modifying some existing
technique, or else proposing an entirely new ap-
proach, we decided to build a shallow parser us-
ing an off-the-shelf part-of-speech (POS) tagger.
We deliberately did not modify the POS tag-
ger&apos;s internal operation in any way. Our results
suggested that achieving reasonable shallow-
parsing performance does not in general require
anything more elaborate than a simple POS tag-
ger. However, an error analysis suggested the
existence of a small set of constructs that are
not so easily characterised by finite-state ap-
proaches such as ours.
</bodyText>
<sectionHeader confidence="0.941472" genericHeader="method">
2 The Tagger
</sectionHeader>
<bodyText confidence="0.999688285714286">
We used Ratnaparkhi&apos;s maximum entropy-
based POS tagger (Ratnaparkhi, 1996). When
tagging, the model tries to recover the most
likely (unobserved) tag sequence, given a se-
quence of observed words.
For our experiments, we used the binary-only
distribution of the tagger (Ratnaparkhi, 1996).
</bodyText>
<sectionHeader confidence="0.854821" genericHeader="method">
3 Convincing the Tagger to Shallow
</sectionHeader>
<subsectionHeader confidence="0.439977">
Parse _
</subsectionHeader>
<bodyText confidence="0.999973617647059">
The insight here is that one can view (some
of) the differences between tagging and (shal-
low) parsing as one of context: shallow pars-
ing requires access to a greater part of the
surrounding lexical/POS syntactic environment
than does simple POS tagging. This extra in-
formation can be encoded in a state.
However, one must balance this approach
with the fact that as the amount of information
in a state increases, with limited training ma-
terial, the chance of seeing such a state again
in the future diminishes. We therefore would
expect performance to increase as we increased
the amount of information in a state, and then
decrease when overfitting and/or sparse statis-
tics become dominate factors.
We trained the tagger using &apos;words&apos; that were
various &apos;configurations&apos; (concatenations) of ac-
tual words, POS tags, chunk-types, and/or suf-
fixes or prefixes of words and/or chunk-types.
By training upon these concatenations, we help
bridge the gap between simple POS tagging and
shallow parsing.
In the rest of the paper, we refer to what the
tagger considers to be a word as a configura-
tion. A configuration will be a concatenation of
various elements of the training set relevant to
decision making regarding chunk assignment. A
&apos;word&apos; will mean a word as found in the train-
ing set. &apos;Tags&apos; refer to the POS tags found in
the training set. Again, such tags may be part
of a configuration. We refer to what the tagger
considers as a tag as a prediction. Predictions
will be chunk labels.
</bodyText>
<sectionHeader confidence="0.996308" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.991721">
We now give details of the experiments we ran.
</bodyText>
<footnote confidence="0.709799">
* The full version of this paper can be found at
http://www.cogsci.ed.ac.ukrosborne/shallow.ps To make matters clearer, consider the following
</footnote>
<page confidence="0.948771">
145
</page>
<table confidence="0.614015">
fragment of the training set:
Word wl W2 W3
POS Tag ti t2 t3
Chunk Cl C2 C3
</table>
<bodyText confidence="0.997970125">
Words are w1, w2 and w3, tags are tl, t2 and t3
and chunk labels are cl, c2 and c3. Throughout,
we built various configurations when predicting
the chunk label for word w1.1
With respect to the situation just mentioned
(predicting the label for word wi), we gradu-
ally increased the amount of information in each
configuration as follows:
</bodyText>
<listItem confidence="0.997373">
1. A configuration consisting of just words
(word wi). Results:
</listItem>
<table confidence="0.994988923076923">
Chunk type P R FBI.
Overall 88.06 88.71 88.38
ADJP 67.57 51.37 58.37
ADVP 74.34 74.25 74.29
CONJP 54.55 66.67 60.00
INTJ 100.00 50.00 66.67
LST 0.00 0.00 0.00
NP 87.84 89.41 88.62
PP 94.80 95.91 95.35
PRT 71.00 66.98 68.93
SBAR 82.30 72.15 76.89
VP 86.68 88.15 87.41
Overall accuracy: 92.76%
</table>
<listItem confidence="0.541647">
2. A configuration consisting of just tags (tag
ti). Results:
</listItem>
<table confidence="0.985335846153846">
Chunk type P R FB1
Overall 88.15 88.07 88.11
ADJP 67.99 54.79 60.68
ADVP 71.61 70.79 71.20
CONJP 35.71 55.56 43.48
INTJ 0.00 0.00 0.00
LST 0.00 0.00 0.00
NP 89.47 89.57 89.52
PP 87.70 95.28 91.33
PRT 52.27 21.70 30.67
SBAR 83.92 31.21 45.50
VP 90.38 91.18 90.78
Overall accuracy 92.66%.
</table>
<listItem confidence="0.87200875">
3. Both words, tags and the current chunk
label (w1, ti, ci) in a configuration. We
allowed the tagger access to the current
chunk label by training another model with
</listItem>
<bodyText confidence="0.874897555555556">
&apos;For space reasons, we had to remove many of these
experiments. The longer version of the paper gives rele-
vant details.
configurations consisting of tags and words
(wi and t1). The training set was then re-
duced to consist of just tag-word configura-
tions and tagged using this model. After-
wards, we collected the predictions for use
in the second model. Results:
</bodyText>
<table confidence="0.983283307692308">
Chunk type P R FB1
Overall 89.79 90.70 90.24
ADJP 69.61 57.53 63.00
ADVP 74.72 77.14 75.91
CONJP 54.55 66.67 60.00
INTJ 50.00 50.00 50.00
LST 0.00 0.00 0.00
NP 89.80 91.12 90.4
PP 95.15 96.26 95.70
PRT 71.84 69.81 70.81
SBAR 85.63 80.19 82.82
VP 89.54 91.31 90.41
Overall accuracy: 93.79%
</table>
<bodyText confidence="0.991014076923077">
4. The final configuration made an attempt
to take deal with sparse statistics. It con-
sisted of the current tag t1, the next tag t2,
the current chunk label ci, the last two let-
ters of the next chunk label c2, the first two
letters of the current word wi and the last
four letters of the current word wi. This
configuration was the result of numerous
experiments and gave the best overall per-
formance. The results can be found in Ta-
ble 1.
We remark upon our experiments in the com-
ments section.
</bodyText>
<sectionHeader confidence="0.998401" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.999762615384615">
We examined the performance of our final
model with respect to the testing material and
found that errors made by our shallow parser
could be grouped into three categories: diffi-
cult syntactic constructs, mistakes made in the
training or testing material by the annotators,
and errors peculiar to our approach.2
Taking each category of the three in turn,
problematic constructs included: co-ordination,
punctuation, treating ditransitive VPs as being
transitive VPs, confusions regarding adjective
or adverbial phrases, and copulars seen as be-
ing possessives.
</bodyText>
<footnote confidence="0.99940775">
2The raw results can be found at: http://www.cog
scLed.ac.ukrosborne/con1100-results.txt The mis-anal-
ysed sentences can be found at: http://www.cogsci.ed.
ac.ukrosborne/con1100-results.txt.
</footnote>
<page confidence="0.997689">
146
</page>
<bodyText confidence="0.999121928571428">
Mistakes (noise) in the training and testing
material were mainly POS tagging errors. An
additional source of errors were odd annotation
decisions.
The final source of errors were peculiar to our
system. Exponential distributions (as used by
our tagger) assign a non-zero probability to all
possible events. This means that the tagger will
at times assign chunk labels that are illegal, for
example assigning a word the label I-NP when
the word is not in a NP. Although these errors
were infrequent, eliminating them would require
&apos;opening-up&apos; the tagger and rejecting illegal hy-
pothesised chunk labels from consideration.
</bodyText>
<sectionHeader confidence="0.991721" genericHeader="conclusions">
6 Comments
</sectionHeader>
<bodyText confidence="0.998547111111111">
As was argued in the introduction, increasing
the size of the context produces better results,
and such performance is bounded by issues such
as sparse statistics. Our experiments suggest
that this was indeed true.
We make no claims about the generality of
our modelling. Clearly it is specific to the tagger
used.
In more detail, we found that:
</bodyText>
<listItem confidence="0.952039818181818">
• PPs seem easy to identify.
• ADJP and ADVP chunks were hard to
identify correctly. We suspect that im-
provements here require greater syntactic
information than just base-phrases.
• Our performance at NPs should be
improved-upon. In terms of modelling, we
did not treat any chunk differently from
any other chunk. We also did not treat any
words differently from any other words.
• The performance using just words and just
</listItem>
<bodyText confidence="0.974581615384615">
POS tags were roughly equivalent. How-
ever, the performance using both sources
was better than when using either source
of information in isolation. The reason for
this is that words and POS tags have differ-
ent properties, and that together, the speci-
ficity of words can overcome the coarseness
of tags, whilst the abundance of tags can
deal with the sparseness of words.
Our results were not wildly worse than those
reported by Buchholz et al (Sabine Buchholz
and Daelemans, 1999). This comparable level of
performance suggests that shallow parsing (base
</bodyText>
<table confidence="0.999897583333333">
test data precision recall Fp=1
ADJP 72.42% 64.16% 68.04
ADVP 75.94% 79.10% 77.49
CONJP 50.00% 55.56% 52.63
INTJ 100.00% 50.00% 66.67
LST 0.00% 0.00% 0.00
NP 91.92% 92.45% 92.19
PP 95.95% 97.44% 96.69
PRT 73.33% 72.64% 72.99
SBAR 86.40% 80.75% 83.48
VP 92.13% 93.28% 92.70
all 91.65% 92.23% 91.94
</table>
<tableCaption confidence="0.9709355">
Table 1: The results for configuration 4. Overall
accuracy: 94.88%
</tableCaption>
<bodyText confidence="0.999728875">
phrasal recognition) is a fairly easy task. Im-
provements might come from better modelling,
dealing with illegal chunk sequences, allowing
multiple chunks with confidence intervals, sys-
tem combination etc, but we feel that such im-
provements will be small. Given this, we believe
that base-phrasal chunking is close to being a
solved problem.
</bodyText>
<sectionHeader confidence="0.987491" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999982666666667">
We would like to thank Erik Tjong Kim Sang
for supplying the evaluation code, and Donnla
Nic Gearailt for dictating over the telephone,
and from the top-of-her-head, a Perl program
to help extract wrongly labelled sentences from
the results.
</bodyText>
<sectionHeader confidence="0.998303" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844">
Claire Cardie and David Pierce. 1998. Error-Driven
Pruning of Treebank Grammars for Base Noun
Phrase Identification. In Proceedings of the 17th
International Conference on Computational Lin-
guistics, pages 218-224.
Lance A. Ramshaw and Mitchell P. Marcus.
1995. Text Chunking Using Transformation-
Based Learning. In Proceedings of the 3rd ACL
Workshop on Very Large Corpora, pages 82-94,
June.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Part-Of-Speech Tagger. In Proceed-
ings of Empirical Methods in Natural Lan-
guage, University of Pennsylvania, May. Tagger:
ftp://ftp.cis.upenn.edu/pub/adwaitijna.
Jorn Veenstra Sabine Buchholz and Walter Daele-
mans. 1999. Cascaded Grammatical Relation As-
signment. In Proceedings of EMNLP/VLC-99.
Association for Computational Linguistics.
</reference>
<page confidence="0.998102">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.688129">
<note confidence="0.955186">of CoNLL-2000 and LLL-2000, 145-147, Lisbon, Portugal, 2000.</note>
<title confidence="0.949917">Shallow Parsing as Part-of-Speech Tagging*</title>
<author confidence="0.999889">Miles Osborne</author>
<affiliation confidence="0.991297333333333">University of Division of 2 Buccleuch</affiliation>
<address confidence="0.875075">Edinburgh EH8 9LW ,</address>
<email confidence="0.995839">osborne@cogsci.ed.ac.uk</email>
<abstract confidence="0.984212125">Treating shallow parsing as part-of-speech tagging yields results comparable with other, more elaborate approaches. Using the CoNLL 2000 training and testing material, our best model had an accuracy of 94.88%, with an overall FB1 score of 91.94%. The individual FB1 scores for were VPs 92.70% and PPs 96.69%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>David Pierce</author>
</authors>
<title>Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>218--224</pages>
<marker>Cardie, Pierce, 1998</marker>
<rawString>Claire Cardie and David Pierce. 1998. Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification. In Proceedings of the 17th International Conference on Computational Linguistics, pages 218-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text Chunking Using TransformationBased Learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd ACL Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="727" citStr="Ramshaw and Marcus, 1995" startWordPosition="103" endWordPosition="106">Part-of-Speech Tagging* Miles Osborne University of Edinburgh Division of Informatics 2 Buccleuch Place Edinburgh EH8 9LW , Scotland osborne@cogsci.ed.ac.uk Abstract Treating shallow parsing as part-of-speech tagging yields results comparable with other, more elaborate approaches. Using the CoNLL 2000 training and testing material, our best model had an accuracy of 94.88%, with an overall FB1 score of 91.94%. The individual FB1 scores for NPs were 92.19%, VPs 92.70% and PPs 96.69%. 1 Introduction Shallow parsing has received a reasonable amount of attention in the last few years (for example (Ramshaw and Marcus, 1995)). In this paper, instead of modifying some existing technique, or else proposing an entirely new approach, we decided to build a shallow parser using an off-the-shelf part-of-speech (POS) tagger. We deliberately did not modify the POS tagger&apos;s internal operation in any way. Our results suggested that achieving reasonable shallowparsing performance does not in general require anything more elaborate than a simple POS tagger. However, an error analysis suggested the existence of a small set of constructs that are not so easily characterised by finite-state approaches such as ours. 2 The Tagger </context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text Chunking Using TransformationBased Learning. In Proceedings of the 3rd ACL Workshop on Very Large Corpora, pages 82-94, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part-Of-Speech Tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of Empirical Methods in Natural</booktitle>
<tech>Tagger: ftp://ftp.cis.upenn.edu/pub/adwaitijna.</tech>
<institution>Language, University of Pennsylvania,</institution>
<contexts>
<context position="1400" citStr="Ratnaparkhi, 1996" startWordPosition="212" endWordPosition="213">chnique, or else proposing an entirely new approach, we decided to build a shallow parser using an off-the-shelf part-of-speech (POS) tagger. We deliberately did not modify the POS tagger&apos;s internal operation in any way. Our results suggested that achieving reasonable shallowparsing performance does not in general require anything more elaborate than a simple POS tagger. However, an error analysis suggested the existence of a small set of constructs that are not so easily characterised by finite-state approaches such as ours. 2 The Tagger We used Ratnaparkhi&apos;s maximum entropybased POS tagger (Ratnaparkhi, 1996). When tagging, the model tries to recover the most likely (unobserved) tag sequence, given a sequence of observed words. For our experiments, we used the binary-only distribution of the tagger (Ratnaparkhi, 1996). 3 Convincing the Tagger to Shallow Parse _ The insight here is that one can view (some of) the differences between tagging and (shallow) parsing as one of context: shallow parsing requires access to a greater part of the surrounding lexical/POS syntactic environment than does simple POS tagging. This extra information can be encoded in a state. However, one must balance this approac</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-Speech Tagger. In Proceedings of Empirical Methods in Natural Language, University of Pennsylvania, May. Tagger: ftp://ftp.cis.upenn.edu/pub/adwaitijna.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorn Veenstra Sabine Buchholz</author>
<author>Walter Daelemans</author>
</authors>
<title>Cascaded Grammatical Relation Assignment.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC-99. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8443" citStr="Buchholz and Daelemans, 1999" startWordPosition="1391" endWordPosition="1394">hunk differently from any other chunk. We also did not treat any words differently from any other words. • The performance using just words and just POS tags were roughly equivalent. However, the performance using both sources was better than when using either source of information in isolation. The reason for this is that words and POS tags have different properties, and that together, the specificity of words can overcome the coarseness of tags, whilst the abundance of tags can deal with the sparseness of words. Our results were not wildly worse than those reported by Buchholz et al (Sabine Buchholz and Daelemans, 1999). This comparable level of performance suggests that shallow parsing (base test data precision recall Fp=1 ADJP 72.42% 64.16% 68.04 ADVP 75.94% 79.10% 77.49 CONJP 50.00% 55.56% 52.63 INTJ 100.00% 50.00% 66.67 LST 0.00% 0.00% 0.00 NP 91.92% 92.45% 92.19 PP 95.95% 97.44% 96.69 PRT 73.33% 72.64% 72.99 SBAR 86.40% 80.75% 83.48 VP 92.13% 93.28% 92.70 all 91.65% 92.23% 91.94 Table 1: The results for configuration 4. Overall accuracy: 94.88% phrasal recognition) is a fairly easy task. Improvements might come from better modelling, dealing with illegal chunk sequences, allowing multiple chunks with co</context>
</contexts>
<marker>Buchholz, Daelemans, 1999</marker>
<rawString>Jorn Veenstra Sabine Buchholz and Walter Daelemans. 1999. Cascaded Grammatical Relation Assignment. In Proceedings of EMNLP/VLC-99. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>