<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000177">
<title confidence="0.99815">
Joint Satisfaction of Syntactic and Pragmatic Constraints
Improves Incremental Spoken Language Understanding
</title>
<author confidence="0.994311">
Andreas Peldszus
</author>
<affiliation confidence="0.9575">
University of Potsdam
Department for Linguistics
</affiliation>
<email confidence="0.292859">
peldszus@uni-potsdam.de
</email>
<author confidence="0.991763">
Timo Baumann
</author>
<affiliation confidence="0.9644525">
University of Hamburg
Department for Informatics
</affiliation>
<email confidence="0.992805">
baumann@informatik.uni-hamburg.de
</email>
<sectionHeader confidence="0.99554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819809523809">
We present a model of semantic processing
of spoken language that (a) is robust against
ill-formed input, such as can be expected
from automatic speech recognisers, (b) re-
spects both syntactic and pragmatic con-
straints in the computation of most likely
interpretations, (c) uses a principled, ex-
pressive semantic representation formalism
(RMRS) with a well-defined model the-
ory, and (d) works continuously (produc-
ing meaning representations on a word-
by-word basis, rather than only for full
utterances) and incrementally (computing
only the additional contribution by the new
word, rather than re-computing for the
whole utterance-so-far).
We show that the joint satisfaction of syn-
tactic and pragmatic constraints improves
the performance of the NLU component
(around 10 % absolute, over a syntax-only
baseline).
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999813">
Incremental processing for spoken dialogue sys-
tems (i. e., the processing of user input even while
it still may be extended) has received renewed at-
tention recently (Aist et al., 2007; Baumann et
al., 2009; Buß and Schlangen, 2010; Skantze and
Hjalmarsson, 2010; DeVault et al., 2011; Purver
et al., 2011). Most of the practical work, how-
ever, has so far focussed on realising the poten-
tial for generating more responsive system be-
haviour through making available processing re-
sults earlier (e. g. (Skantze and Schlangen, 2009)),
but has otherwise followed a typical pipeline ar-
chitecture where processing results are passed
only in one direction towards the next module.
</bodyText>
<author confidence="0.767693">
Okko Bull
</author>
<affiliation confidence="0.954042">
University of Potsdam
Department for Linguistics
</affiliation>
<email confidence="0.877245">
okko@ling.uni-potsdam.de
</email>
<author confidence="0.9741">
David Schlangen
</author>
<affiliation confidence="0.934605">
University of Bielefeld
Department for Linguistics
</affiliation>
<bodyText confidence="0.982659">
david.schlangen@uni-bielefeld.de
In this paper, we investigate whether the other
potential advantage of incremental processing—
providing “higher-level”-feedback to lower-level
modules, in order to improve subsequent process-
ing of the lower-level module—can be realised as
well. Specifically, we experimented with giving a
syntactic parser feedback about whether semantic
readings of nominal phrases it is in the process of
constructing have a denotation in the given con-
text or not. Based on the assumption that speak-
ers do plan their referring expressions so that they
can successfully refer, we use this information to
re-rank derivations; this in turn has an influence
on how the derivations are expanded, given con-
tinued input. As we show in our experiments, for
a corpus of realistic dialogue utterances collected
in a Wizard-of-Oz setting, this strategy led to an
absolute improvement in computing the intended
denotation of around 10 % over a baseline (even
more using a more permissive metric), both for
manually transcribed test data as well as for the
output of automatic speech recognition.
The remainder of this paper is structured as fol-
lows: We discuss related work in the next section,
and then describe in general terms our model and
its components. In Section 4 we then describe the
data resources we used for the experiments and
the actual implementation of the model, the base-
lines for comparison, and the results of our exper-
iments. We close with a discussion and an outlook
on future work.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999004">
The idea of using real-world reference to inform
syntactic structure building has been previously
explored by a number of authors. Stoness et al.
(2004, 2005) describe a proof-of-concept imple-
</bodyText>
<page confidence="0.96481">
514
</page>
<note confidence="0.976566">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 514–523,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99992803125">
mentation of a “continuous understanding” mod-
ule that uses reference information in guiding a
bottom-up chart-parser, which is evaluated on a
single dialogue transcript. In contrast, our model
uses a probabilistic top-down parser with beam
search (following Roark (2001)) and is evalu-
ated on a large number of real-world utterances
as processed by an automatic speech recogniser.
Similarly, DeVault and Stone (2003) describe a
system that implements interaction between a
parser and higher-level modules (in this case, even
more principled, trying to prove presuppositions),
which however is also only tested on a small, con-
structed data-set.
Schuler (2003) and Schuler et al. (2009) present
a model where information about reference is
used directly within the speech recogniser, and
hence informs not only syntactic processing but
also word recognition. To this end, the processing
is folded into the decoding step of the ASR, and
is realised as a hierarchical HMM. While techni-
cally interesting, this approach is by design non-
modular and restricted in its syntactic expressiv-
ity.
The work presented here also has connections
to work in psycholinguistics. Pad´o et al. (2009)
present a model that combines syntactic and se-
mantic models into one plausibility judgement
that is computed incrementally. However, that
work is evaluated for its ability to predict reading
time data and not for its accuracy in computing
meaning.
</bodyText>
<sectionHeader confidence="0.998829" genericHeader="method">
3 The Model
</sectionHeader>
<subsectionHeader confidence="0.992055">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999906818181818">
Described abstractly, the model computes the
probability of a syntactic derivation (and its ac-
companying logical form) as a combination of a
syntactic probability (as in a typical PCFG) and
a semantic or pragmatic plausibility.1 The prag-
matic plausibility here comes from the presuppo-
sition that the speaker intended her utterance to
successfully refer, i. e. to have a denotation in the
current situation (a unique one, in the case of def-
inite reference). Hence, readings that do have a
denotation are preferred over those that do not.
</bodyText>
<footnote confidence="0.99756025">
1Note that, as described below, in the actual implemen-
tation the weights given to particular derivations are not real
probabilities anymore, as derivations fall out of the beam and
normalisation is not performed after re-weighting.
</footnote>
<bodyText confidence="0.999861444444444">
The components of our model are described in
the following sections: first the parser which com-
putes the syntactic probability in an incremental,
top-down manner; the semantic construction al-
gorithm which associates (underspecified) logi-
cal forms to derivations; the reference resolution
component that computes the pragmatic plausi-
bility; and the combination that incorporates the
feedback from this pragmatic signal.
</bodyText>
<subsectionHeader confidence="0.998179">
3.2 Parser
</subsectionHeader>
<bodyText confidence="0.999993575">
Roark (2001) introduces a strategy for incremen-
tal probabilistic top-down parsing and shows that
it can compete with high-coverage bottom-up
parsers. One of the reasons he gives for choosing
a top-down approach is that it enables fully left-
connected derivations, where at every process-
ing step new increments directly find their place
in the existing structure. This monotonically en-
riched structure can then serve as a context for in-
cremental language understanding, as the author
claims, although this part is not further developed
by Roark (2001). He discusses a battery of dif-
ferent techniques for refining his results, mostly
based on grammar transformations and on con-
ditioning functions that manipulate a derivation
probability on the basis of local linguistic and lex-
ical information.
We implemented a basic version of his parser
without considering additional conditioning or
lexicalizations. However, we applied left-facto-
rization to parts of the grammar to delay cer-
tain structural decisions as long as possible. The
search-space is reduced by using beam search. To
match the next token, the parser tries to expand
the existing derivations. These derivations are
stored in a priorized queue, which means that the
most probable derivation will always be served
first. Derivations resulting from rule expansions
are kept in the current queue, derivations result-
ing from a successful lexical match are pushed in
a new queue. The parser proceeds with the next
most probable derivation until the current queue
is empty or until a threshhold is reached at which
remaining analyses are pruned. This threshhold
is determined dynamically: If the probability of
the current derivation is lower than the product of
the best derivation’s probability on the new queue,
the number of derivations in the new queue, and a
base beam factor (an initial parameter for the size
of the search beam), then all further old deriva-
</bodyText>
<page confidence="0.998659">
515
</page>
<figureCaption confidence="0.997987">
Figure 1: An example network of incremental units, including the levels of words, POS-tags, syntactic derivations
and logical forms. See section 3 for a more detailed description.
</figureCaption>
<figure confidence="0.822481024193548">
TextualWordIU
TagIU
CandidateAnalysisIU
FormulaIU
CandidateAnalysisIU
LD=[]
P=1.00
S=[S*,S!]
TextualWordIU
$TopOfWords
FormulaIU
[ [l0:a1:i2]
{ [l0:a1:i2] } ]
TagIU
$TopOfTags
CandidateAnalysisIU
LD=[s*/s,kon,s*, s/vp, vp/vvimp-v1, m(vvimp)]
P=0.14
S=[V1, kon, S*, S!]
CandidateAnalysisIU
LD=[s*/s, s/vp, vp/vvimp-v1, m(vvimp)]
P=0.49
S=[V1, S!]
FormulaIU
[ [l0:a1:e2]
{ [l0:a1:e2] }
ARG1(a1,x8),
l6:a7:addressee(x8),
l0:a1:_nehmen(e2)]
TextualWordIU
nimm
FormulaIU
TagIU
vvimp
CandidateAnalysisIU
LD=[v1/np-vz, np/det-n1, m(det)]
P=0.2205
S=[N1, VZ, S!]
CandidateAnalysisIU
LD=[v1/np-vz, np/pper, i(det)]
P=0.00441
S=[pper, VZ, S!]
FormulaIU
[ [l0:a1:e2]
{ [l18:a19:x14] [l0:a1:e2] }
ARG1(a1,x8),
l6:a7:addressee(x8),
l0:a1:_nehmen(e2),
ARG2(a1,x14),
BV(a13,x14),
RSTR(a13,h21),
BODY(a13,h22),
l12:a13:_def(),
qeq(h21,l18)]
TextualWordIU
den
FormulaIU
TagIU
det
CandidateAnalysisIU
LD=[n1/adjp-n1, adjp/adja, i(nn)]
P=0.002646
S=[adja, N1, VZ, S!]
CandidateAnalysisIU
LD=[n1/nadj-nz, nadj/adja, i(nn)]
P=0.000441
S=[adja, NZ, VZ, S!]
FormulaIU
[ [l0:a1:e2]
{ [l29:a30:x14] [l0:a1:e2] }
ARG1(a1,x8),
l6:a7:addressee(x8),
l0:a1:_nehmen(e2),
ARG2(a1,x14),
BV(a13,x14),
RSTR(a13,h21),
BODY(a13,h22),
l12:a13:_def(),
l18:a19:_winkel(x14),
qeq(h21,l18)]
CandidateAnalysisIU
LD=[n1/nn-nz, m(nn)]
P=0.06615
S=[NZ, VZ, S!]
TextualWordIU
winkel
FormulaIU
FormulaIU
TagIU
nn
CandidateAnalysisIU
LD=[nz/eps, vz/advp-vz, advp/adv, i(appr)]
P=0.00007938
S=[adv, VZ, S!]
FormulaIU
[ [l0:a1:e2]
{ [l42:a43:x44] [l29:a30:x14] [l0:a1:e2] }
ARG1(a1,x8),
l6:a7:addressee(x8),
l0:a1:_nehmen(e2),
ARG2(a1,x14),
BV(a13,x14),
RSTR(a13,h21),
BODY(a13,h22),
l12:a13:_def(),
l18:a19:_winkel(x14),
ARG1(a40,x14),
ARG2(a40,x44),
l39:a40:_in(e41),
qeq(h21,l18)]
CandidateAnalysisIU
LD=[nz/pp-nz, pp/appr-np, m(appr)]
P=0.0178605
S=[NP, NZ, VZ, S!]
CandidateAnalysisIU
LD=[nz/advp-nz, advp/adv, i(appr)]
P=0.0003969
S=[adv, NZ, VZ, S!]
TextualWordIU
in
FormulaIU
FormulaIU
TagIU
appr
</figure>
<bodyText confidence="0.99810155882353">
tions are pruned. Due to probabilistic weighing
and the left factorization of the rules, left recur-
sion poses no direct threat in such an approach.
Additionally, we implemented three robust lex-
ical operations: insertions consume the current
token without matching it to the top stack item;
deletions can “consume” a requested but actu-
ally non-existent token; repairs adjust unknown
tokens to the requested token. These robust op-
erations have strong penalties on the probability
to make sure they will survive in the derivation
only in critical situations. Additionally, only a
single one of them is allowed to occur between
the recognition of two adjacent input tokens.
Figure 1 illustrates this process for the first few
words of the example sentence “nimm den winkel
in der dritten reihe” (take the bracket in the third
row), using the incremental unit (IU) model to
represent increments and how they are linked; see
(Schlangen and Skantze, 2009).2 Here, syntactic
2Very briefly: rounded boxes in the Figures represent
IUs, and dashed arrows link an IU to its predecessor on the
same level, where the levels correspond to processing stages.
The Figure shows the levels of input words, POS-tags, syn-
tactic derivations and logical forms. Multiple IUs sharing
derivations (“CandidateAnalysisIUs”) are repre-
sented by three features: a list of the last parser ac-
tions of the derivation (LD), with rule expansions
or (robust) lexical matches; the derivation proba-
bility (P); and the remaining stack (S), where S*
is the grammar’s start symbol and S! an explicit
end-of-input marker. (To keep the Figure small,
we artificially reduced the beam size and cut off
alternatives paths, shown in grey.)
</bodyText>
<subsectionHeader confidence="0.998597">
3.3 Semantic Construction Using RMRS
</subsectionHeader>
<bodyText confidence="0.982133">
As a novel feature, we use for the representation
of meaning increments (that is, the contributions
of new words and syntactic constructions) as well
as for the resulting logical forms the formalism
Robust Minimal Recursion Semantics (Copestake,
2006). This is a representation formalism that was
originally constructed for semantic underspecifi-
cation (of scope and other phenomena) and then
adapted to serve the purposes of semantics repre-
the same predecessor can be regarded as alternatives. Solid
arrows indicate which information from a previous level an
IU is grounded in (based on); here, every semantic IU is
grounded in a syntactic IU, every syntactic IU in a POS-tag-
IU, and so on.
</bodyText>
<page confidence="0.99136">
516
</page>
<bodyText confidence="0.999160902173913">
sentations in heterogeneous situations where in-
formation from deep and shallow parsers must be
combined. In RMRS, meaning representations of
a first order logic are underspecified in two ways:
First, the scope relationships can be underspeci-
fied by splitting the formula into a list of elemen-
tary predications (EP) which receive a label E and
are explicitly related by stating scope constraints
to hold between them (e.g. qeq-constraints). This
way, all scope readings can be compactly repre-
sented. Second, RMRS allows underspecification
of the predicate-argument-structure of EPs. Ar-
guments are bound to a predicate by anchor vari-
ables a, expressed in the form of an argument re-
lation ARGREL(a,x). This way, predicates can
be introduced without fixed arity and arguments
can be introduced without knowing which predi-
cates they are arguments of. We will make use of
this second form of underspecification and enrich
lexical predicates with arguments incrementally.
Combining two RMRS structures involves at
least joining their list of EPs and ARGRELs and
of scope constraints. Additionally, equations be-
tween the variables can connect two structures,
which is an essential requirement for semantic
construction. A semantic algebra for the combi-
nation of RMRSs in a non-lexicalist setting is de-
fined in (Copestake, 2007). Unsaturated semantic
increments have open slots that need to be filled
by what is called the hook of another structure.
Hook and slot are triples [E:a:x] consisting of a
label, an anchor and an index variable. Every vari-
able of the hook is equated with the corresponding
one in the slot. This way the semantic representa-
tion can grow monotonically at each combinatory
step by simply adding predicates, constraints and
equations.
Our approach differs from (Copestake, 2007)
only in the organisation of the slots: In an incre-
mental setting, a proper semantic representation
is desired for every single state of growth of the
syntactic tree. Typically, RMRS composition as-
sumes that the order of semantic combination is
parallel to a bottom-up traversal of the syntactic
tree. Yet, this would require for every incremental
step first to calculate an adequate underspecified
semantic representation for the projected nodes
on the lower right border of the tree and then to
proceed with the combination not only of the new
semantic increments but of the complete tree. For
our purposes, it is more elegant to proceed with
semantic combination in synchronisation with the
syntactic expansion of the tree, i.e. in a top-down
left-to-right fashion. This way, no underspecifica-
tion of projected nodes and no re-interpretation of
already existing parts of the tree is required. This,
however, requires adjustments to the slot structure
of RMRS. Left-recursive rules can introduce mul-
tiple slots of the same sort before they are filled,
which is not allowed in the classic (R)MRS se-
mantic algebra, where only one named slot of
each sort can be open at a time. We thus organize
the slots as a stack of unnamed slots, where mul-
tiple slots of the same sort can be stored, but only
the one on top can be accessed. We then define
a basic combination operation equivalent to for-
ward function composition (as in standard lambda
calculus, or in CCG (Steedman, 2000)) and com-
bine substructures in a principled way across mul-
tiple syntactic rules without the need to represent
slot names.
Each lexical items receives a generic represen-
tation derived from its lemma and the basic se-
mantic type (individual, event, or underspecified
denotations), determined by its POS tag. This
makes the grammar independent of knowledge
about what later (semantic) components will ac-
tually be able to process (“understand”).3 Parallel
to the production of syntactic derivations, as the
tree is expanded top-down left-to-right, seman-
tic macros are activated for each syntactic rule,
composing the contribution of the new increment.
This allows for a monotonic semantics construc-
tion process that proceeds in lockstep with the
syntactic analysis.
Figure 1 (in the ”FormulaIU” box) illustrates
the results of this process for our example deriva-
tion. Again, alternatives paths have been cut to
keep the size of the illustration small. Notice that,
apart from the end-of-input marker, the stack of
semantic slots (in curly brackets) is always syn-
chronized with the parser’s stack.
</bodyText>
<subsectionHeader confidence="0.98324">
3.4 Computing Noun Phrase Denotations
</subsectionHeader>
<bodyText confidence="0.995112">
Formally, the task of this module is, given a model
M of the current context, to compute the set of
all variable assignments such that M satisfies 0:
G = {g  |M |=9 0}. If |G |&gt; 1, we say that 0
refers ambiguously; if |G |= 1, it refers uniquely;
</bodyText>
<footnote confidence="0.988594">
3This feature is not used in the work presented here, but
it could be used for enabling the system to learn the meaning
of unknown words.
</footnote>
<page confidence="0.9902">
517
</page>
<bodyText confidence="0.99989675">
and if 1!91 = 0, it fails to refer. This process does
not work directly on RMRS formulae, but on ex-
tracted and unscoped first-order representations of
their nominal content.
</bodyText>
<sectionHeader confidence="0.674891" genericHeader="method">
3.5 Parse Pruning Using Reference
Information
</sectionHeader>
<bodyText confidence="0.999919071428572">
After all possible syntactic hypotheses at an in-
crement have been derived by the parser and
the corresponding semantic representations have
been constructed, reference resolution informa-
tion can be used to re-rank the derivations. If
pragmatic feedback is enabled, the probability of
every reprentation that does not resolve in the cur-
rent context is degraded by a constant factor (we
used 0.001 in our experiments described below,
determined by experimentation). The degradation
thus changes the derivation order in the parsing
queue for the next input item and increases the
chances of degraded derivations to be pruned in
the following parsing step.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.94451">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999984923076923">
We use data from the Pentomino puzzle piece do-
main (which has been used before for example
by (Fern´andez and Schlangen, 2007; Schlangen et
al., 2009)), collected in a Wizard-of-Oz study. In
this specific setting, users gave instructions to the
system (the wizard) in order to manipulate (select,
rotate, mirror, delete) puzzle pieces on an upper
board and to put them onto a lower board, reach-
ing a pre-specified goal state. Figure 2 shows an
example configuration. Each participant took part
in several rounds in which the distinguishing char-
acteristics for puzzle pieces (color, shape, pro-
posed name, position on the board) varied widely.
In total, 20 participants played 284 games.
We extracted the semantics of an utterance
from the wizard’s response action. In some cases,
such a mapping was not possible to do (e. g. be-
cause the wizard did not perform a next action,
mimicking a non-understanding by the system),
or potentially unreliable (if the wizard performed
several actions at or around the end of the utter-
ance). We discarded utterances without a clear se-
mantics alignment, leaving 1687 semantically an-
notated user utterances. The wizard of course was
able to use her model of the previous discourse for
resolving references, including anaphoric ones; as
</bodyText>
<figureCaption confidence="0.993345">
Figure 2: The game board used in the study, as pre-
sented to the player: (a) the current state of the game
on the left, (b) the goal state to be reached on the right.
</figureCaption>
<bodyText confidence="0.999939185185185">
our study does not focus on these, we have dis-
regarded another 661 utterances in which pieces
are referred to by pronouns, leaving us with 1026
utterances for evaluation. These utterances con-
tained on average 5.2 words (median 5 words;
std dev 2 words).
In order to test the robustness of our method,
we generated speech recognition output using an
acoustic model trained for spontaneous (German)
speech. We used leave-one-out language model
training, i. e. we trained a language model for ev-
ery utterance to be recognized which was based
on all the other utterances in the corpus. Unfor-
tunately, the audio recordings of the first record-
ing day were too quiet for successful recognition
(with a deletion rate of 14 %). We thus decided
to limit the analysis for speech recognition out-
put to the remaining 633 utterances from the other
recording days. On this part of the corpus word
error rate (WER) was at 18 %.
The subset of the full corpus that we used for
evaluation, with the utterances selected according
to the criteria described above, nevertheless still
only consists of natural, spontaneous utterances
(with all the syntactic complexity that brings) that
are representative for interactions in this type of
domain.
</bodyText>
<subsectionHeader confidence="0.974477">
4.2 Grammar and Resolution Model
</subsectionHeader>
<bodyText confidence="0.999954333333333">
The grammar used in our experiments was hand-
constructed, inspired by a cursory inspection of
the corpus and aiming to reach good coverage
</bodyText>
<page confidence="0.994097">
518
</page>
<table confidence="0.9195084">
Words Predicates Status
nimm nimm(e) -1
nimm den nimm(e,x) def(x) 0
nimm den Winkel nimm(e,x) def(x) winkel(x) 0
nimm den Winkel in nimm(e,x) def(x) winkel(x) in(x,y) 0
</table>
<tableCaption confidence="0.726841">
nimm den Winkel in der nimm(e,x) def(x) winkel(x) in(x,y) def(y) 0
nimm den Winkel in der dritten nimm(e,x) def(x) winkel(x) in(x,y) def(y) third(y) 1
nimm den Winkel in der dritten Reihe nimm(e,x) def(x) winkel(x) in(x,y) def(y) third(y) row(y) 1
Table 1: Example of logical forms (flattened into first-order base-language formulae) and reference resolution
results for incrementally parsing and resolving ‘nimm den winkel in der dritten reihe’
</tableCaption>
<bodyText confidence="0.999984333333333">
for a core fragment. We created 30 rules, whose
weights were also set by hand (as discussed be-
low, this is an obvious area for future improve-
ment), sparingly and according to standard intu-
itions. When parsing, the first step is the assign-
ment of a POS tag to each word. This is done by
a simple lookup tagger that stores the most fre-
quent tag for each word (as determined on a small
subset of our corpus).4
The situation model used in reference resolu-
tion is automatically derived from the internal
representation of the current game state. (This
was recorded in an XML-format for each utter-
ance in our corpus.) Variable assignments were
then derived from the relevant nominal predicate
structures,5 consisting of extracted simple pred-
ications, e. g. red(x) and cross(x) for the NP in
a phrase such as “take the red cross”. For each
unique predicate argument X in these EP struc-
tures (such as as x above), the set of domain ob-
jects that satisfied all predicates of which X was
an argument were determined. For example for
the phrase above, X mapped to all elements that
were red and crosses.
Finally, the size of these sets was determined:
no elements, one element, or multiple elements,
as described above. Emptiness of at least one set
denoted that no resolution was possible (for in-
stance, if no red crosses were available, x’s set
was empty), uniqueness of all sets denoted that
an exact resolution was possible while multiple
elements in at least some sets denoted ambiguity.
This status was then leveraged for parse pruning,
as per Section 3.5.
A more complex example using the scene de-
picted in Figure 2 and the sentence “nimm den
</bodyText>
<footnote confidence="0.995005">
4A more sophisticated approach has recently been pro-
posed by Beuck et al. (2011); this could be used in our setup.
5The domain model did not allow making a plausibility
judgement based on verbal resolution.
</footnote>
<bodyText confidence="0.999755571428571">
winkel in der dritten reihe” (take the bracket in the
third row) is shown in Table 1. The first column
shows the incremental word hypothesis string, the
second the set of predicates derived from the most
recent RMRS representation and the third the res-
olution status (-1 for no resolution, 0 for some res-
olution and 1 for a unique resolution).
</bodyText>
<subsectionHeader confidence="0.995963">
4.3 Baselines and Evaluation Metric
4.3.1 Variants / Baselines
</subsectionHeader>
<bodyText confidence="0.999969366666666">
To be able to accurately quantify and assess the
effect of our reference-feedback strategy, we im-
plemented different variants / baselines. These all
differ in how, at each step, the reading is deter-
mined that is evaluated against the gold standard,
and are described in the following:
In the Just Syntax (JS) variant, we simply take
single-best derivation, as determined by syntax
alone and evaluate this.
The External Filtering (EF) variant adds in-
formation from reference resolution, but keeps
it separate from the parsing process. Here, we
look at the 5 highest ranking derivations (as de-
termined by syntax alone), and go through them
beginning at the highest ranked, picking the first
derivation where reference resolution can be per-
formed uniquely; this reading is then put up for
evaluation. If there is no such reading, the highest
ranking one will be put forward for evaluation (as
in JS).
Syntax/Pragmatics Interaction (SPI) is the
variant described in the previous section. Here,
all active derivations are sent to the reference res-
olution module, and are re-weighted as described
above; after this has been done, the highest-
ranking reading is evaluated.
Finally, the Combined Interaction and Fil-
tering (CIF) variant combines the previous two
strategies, by using reference-feedback in com-
puting the ranking for the derivations, and then
</bodyText>
<page confidence="0.99589">
519
</page>
<bodyText confidence="0.999771333333333">
again using reference-information to identify the
most promising reading within the set of 5 highest
ranking ones.
</bodyText>
<subsectionHeader confidence="0.932961">
4.3.2 Metric
</subsectionHeader>
<bodyText confidence="0.999996">
When a reading has been identified according
to one of these methods, a score s is computed as
follows: s = 1, if the correct referent (according
to the gold standard) is computed as the denota-
tion for this reading; s = 0 if no unique referent
can be computed, but the correct one is part of the
set of possible referents; s = −1 if no referent
can be computed at all, or the correct one is not
part of the set of those that are computed.
As this is done incrementally for each word
(adding the new word to the parser chart), for an
utterance of length m we get a sequence of m
such numbers. (In our experiments we treat the
“end of utterance” signal as a pseudo-word, since
knowing that an utterance has concluded allows
the parser to close off derivations and remove
those that are still requiring elements. Hence, we
in fact have sequences of m+1 numbers.) A com-
bined score for the whole utterance is computed
according to the following formula:
</bodyText>
<equation confidence="0.996494">
M
su = (sn * n/m)
n=1
</equation>
<bodyText confidence="0.999441578947368">
(where sn is the score at position n). The fac-
tor n/m causes “later” decisions to count more
towards the final score, reflecting the idea that
it is more to be expected (and less harmful) to
be wrong early on in the utterance, whereas the
longer the utterance goes on, the more pressing
it becomes to get a correct result (and the more
damaging if mistakes are made).6
Note that this score is not normalised by utter-
ance length m; the maximally achievable score
being (m + 1)/2. This has the additional ef-
fect of increasing the weight of long utterances
when averaging over the score of all utterances;
we see this as desirable, as the analysis task be-
comes harder the longer the utterance is.
We use success in resolving reference to eval-
uate the performance of our parsing and semantic
construction component, where more tradition-
ally, metrics like parse bracketing accuracy might
</bodyText>
<footnote confidence="0.901268">
6This metric compresses into a single number some of
the concerns of the incremental metrics developed in (Bau-
mann et al., 2011), which can express more fine-grainedly
the temporal development of hypotheses.
</footnote>
<bodyText confidence="0.999340714285714">
be used. But as we are building this module for an
interactive system, ultimately, accuracy in recov-
ering meaning is what we are interested in, and so
we see this not just as a proxy, but actually as a
more valuable metric. Moreover, this metric can
be applied at each incremental step, which is not
clear how to do with more traditional metrics.
</bodyText>
<subsectionHeader confidence="0.98777">
4.4 Experiments
</subsectionHeader>
<bodyText confidence="0.999966913043478">
Our parser, semantic construction and reference
resolution modules are implemented within the
InproTK toolkit for incremental spoken dialogue
systems development (Schlangen et al., 2010). In
this toolkit, incremental hypotheses are modified
as more information becomes available over time.
Our modules support all such modifications (i. e.
also allow to revert their states and output if word
input is revoked).
As explained in Section 4.1, we used offline
recognition results in our evaluation. However,
the results would be identical if we were to use
the incremental speech recognition output of In-
proTK directly.
The system performs several times faster than
real-time on a standard workstation computer. We
thus consider it ready to improve practical end-to-
end incremental systems which perform within-
turn actions such as those outlined in (Buß and
Schlangen, 2010).
The parser was run with a base-beam factor of
0.01; this parameter may need to be adjusted if a
larger grammar was used.
</bodyText>
<sectionHeader confidence="0.780968" genericHeader="evaluation">
4.5 Results
</sectionHeader>
<bodyText confidence="0.999956647058824">
Table 2 shows an overview of the experiment re-
sults. The table lists, separately for the manual
transcriptions and the ASR transcripts, first the
number of times that the final reading did not re-
solve at all, or to a wrong entitiy; did not uniquely
resolve, but included the correct entity in its de-
notiation; or did uniquely resolve to the correct
entity (-1, 0, and 1, respectively). The next lines
show “strict accuracy” (proportion of “1” among
all results) at the end of utterance, and “relaxed
accuracy” (which allows ambiguity, i.e., is the set
{0,1}). incr.scr is the incremental score as de-
scribed above, which includes in the evaluation
the development of references and not just the fi-
nal state. (And in that sense, is the most appro-
priate metric here, as it captures the incremental
behaviour.) This score is shown both as absolute
</bodyText>
<page confidence="0.985327">
520
</page>
<table confidence="0.999879866666667">
JS EF SPI CIF
−1 563 518 364 363
0 197 198 267 268
1 264 308 392 392
str.acc. 25.7% 30.0% 38.2% 38.2%
rel.acc. 44.9% 49.3% 64.2% 64.3%
incr.scr −1568 −1248 −536 −504
avg.incr.scr −1.52 −1.22 −0.52 −0.49
−1 362 348 254 255
0 122 121 173 173
1 143 158 196 195
str.acc. 22.6% 25.0% 31.0% 30.8%
rel.acc. 41.2% 44.1% 58.3% 58.1%
incr.scr −1906 −1730 −1105 −1076
avg.incr.scr −1.86 −1.69 −1.01 −1.05
</table>
<tableCaption confidence="0.99496">
Table 2: Results of the Experiments. See text for explanation of metrics.
</tableCaption>
<table confidence="0.498969">
transcript
recogntion
</table>
<bodyText confidence="0.999810454545455">
number as well as averaged for each utterance.
As these results show, the strategy of provid-
ing the parser with feedback about the real-world
utility of constructed phrases (in the form of refer-
ence decisions) improves the parser, in the sense
that it helps the parser to successfully retrieve the
intended meaning more often compared to an ap-
proach that only uses syntactic information (JS)
or that uses pragmatic information only outside
of the main programme: 38.2 % strict or 64.2 %
relaxed for SPI over 25.7 % / 44.9 % for JS, an
absolute improvement of 12.5 % for strict or even
more, 19.3 %, for the relaxed metric; the incre-
mental metric shows that this advantage holds not
only at the final word, but also consistently within
the utterance, the average incremental score for
an utterance being −0.49 for SPI and −1.52
for JS. The improvement is somewhat smaller
against the variant that uses some reference infor-
mation, but does not integrate this into the parsing
process (EF), but it is still consistently present.
Adding such n-best-list processing to the output
of the parser+reference-combination (as variant
CIF does) finally does not further improve the
performance noticeably. When processing par-
tially defective material (the output of the speech
recogniser), the difference between the variants
is maintained, showing a clear advantage of SPI,
although performance of all variants is degraded
somewhat.
Clearly, accuracy is rather low for the base-
line condition (JS); this is due to the large num-
ber of non-standard constructions in our sponta-
neous material (e.g., utterances like “l¨oschen, un-
ten” (delete, bottom) which we did not try to cover
with syntactic rules, and which may not even con-
tain NPs. The SPI condition can promote deriva-
tions resulting from robust rules (here, deletion)
which then can refer. In general though state-of-
the art grammar engineering may narrow the gap
between JS and SPI – this remains to be tested –
but we see as an advantage of our approach that
it can improve over the (easy-to-engineer) set of
core grammar rules.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999990222222222">
We have described a model of semantic process-
ing of natural, spontaneous speech that strives
to jointly satisfy syntactic and pragmatic con-
straints (the latter being approximated by the as-
sumption that referring expressions are intended
to indeed successfully refer in the given context).
The model is robust, accepting also input of the
kind that can be expected from automatic speech
recognisers, and incremental, that is, can be fed
input on a word-by-word basis, computing at each
increment only exactly the contribution of the new
word. Lastly, as another novel contribution, the
model makes use of a principled formalism for se-
mantic representation, RMRS (Copestake, 2006).
While the results show that our approach of
combining syntactic and pragmatic information
can work in a real-world setting on realistic
data—previous work in this direction has so far
</bodyText>
<page confidence="0.991905">
521
</page>
<bodyText confidence="0.99997795">
only been at the proof-of-concept stage—there is
much room for improvement. First, we are now
exploring ways of bootstrapping a grammar and
derivation weights from hand-corrected parses.
Secondly, we are looking at making the variable
assignment / model checking function probabilis-
tic, assigning probabilities (degree of strength of
belief) to candidate resolutions (as for example
the model of Schlangen et al. (2009) does). An-
other next step—which will be very easy to take,
given the modular nature of the implementation
framework that we have used—will be to integrate
this component into an interactive end-to-end sys-
tem, and testing other domains in the process.
Acknowledgements We thank the anonymous
reviewers for their helpful comments. The work
reported here was supported by a DFG grant in
the Emmy Noether programme to the last author
and a stipend from DFG-CRC (SFB) 632 to the
first author.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999945372093023">
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K. Tanenhaus. 2007. Incremental under-
standing in human-computer dialogue and experi-
mental evidence for advantages over nonincremen-
tal methods. In Proceedings of Decalog 2007, the
11th International Workshop on the Semantics and
Pragmatics of Dialogue, Trento, Italy.
Timo Baumann, Michaela Atterer, and David
Schlangen. 2009. Assessing and improving the per-
formance of speech recognition for incremental sys-
tems. In Proceedings of the North American Chap-
ter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT)
2009 Conference, Boulder, Colorado, USA, May.
Timo Baumann, Okko Bu13, and David Schlangen.
2011. Evaluation and optimization of incremen-
tal processors. Dialogue and Discourse, 2(1):113–
141.
Niels Beuck, Arne K¨ohn, and Wolfgang Menzel.
2011. Decision strategies for incremental pos tag-
ging. In Proceedings of the 18th Nordic Con-
ference of Computational Linguistics, NODALIDA-
2011, Riga, Latvia.
Okko Bu13 and David Schlangen. 2010. Modelling
sub-utterance phenomena in spoken dialogue sys-
tems. In Proceedings of the 14th International
Workshop on the Semantics and Pragmatics of Dia-
logue (Pozdial2010), pages 33–41, Poznan, Poland,
June.
Ann Copestake. 2006. Robust minimal recursion se-
mantics. Technical report, Cambridge Computer
Lab. Unpublished draft.
Ann Copestake. 2007. Semantic composition with
(robust) minimal recursion semantics. In Proceed-
ings of the Workshop on Deep Linguistic Process-
ing, DeepLP ’07, pages 73–80, Stroudsburg, PA,
USA. Association for Computational Linguistics.
David DeVault and Matthew Stone. 2003. Domain
inference in incremental interpretation. In Proceed-
ings of ICOS 4: Workshop on Inference in Compu-
tational Semantics, Nancy, France, September. IN-
RIA Lorraine.
David DeVault, Kenji Sagae, and David Traum. 2011.
Incremental Interpretation and Prediction of Utter-
ance Meaning for Interactive Dialogue. Dialogue
and Discourse, 2(1):143–170.
Raquel Fern´andez and David Schlangen. 2007. Re-
ferring under restricted interactivity conditions. In
Simon Keizer, Harry Bunt, and Tim Paek, editors,
Proceedings of the 8th SIGdial Workshop on Dis-
course and Dialogue, pages 136–139, Antwerp,
Belgium, September.
Ulrike Pad´o, Matthew W Crocker, and Frank Keller.
2009. A probabilistic model of semantic plausi-
bility in sentence processing. Cognitive Science,
33(5):794–838.
Matthew Purver, Arash Eshghi, and Julian Hough.
2011. Incremental semantic construction in a di-
alogue system. In J. Bos and S. Pulman, editors,
Proceedings of the 9th International Conference on
Computational Semantics (IWCS), pages 365–369,
Oxford, UK, January.
Brian Roark. 2001. Robust Probabilistic Predictive
Syntactic Processing: Motivations, Models, and
Applications. Ph.D. thesis, Department of Cogni-
tive and Linguistic Sciences, Brown University.
David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In EACL ’09: Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 710–718.
Association for Computational Linguistics, mar.
David Schlangen, Timo Baumann, and Michaela At-
terer. 2009. Incremental reference resolution: The
task, metrics for evaluation, and a bayesian filtering
model that is sensitive to disfluencies. In Proceed-
ings of SIGdial 2009, the 10th Annual SIGDIAL
Meeting on Discourse and Dialogue, London, UK,
September.
David Schlangen, Timo Baumann, Hendrik
Buschmeier, Okko Bu13, Stefan Kopp, Gabriel
Skantze, and Ramin Yaghoubzadeh. 2010. Middle-
ware for Incremental Processing in Conversational
Agents. In Proceedings of SigDial 2010, Tokyo,
Japan, September.
</reference>
<page confidence="0.971138">
522
</page>
<reference confidence="0.99969540625">
William Schuler, Stephen Wu, and Lane Schwartz.
2009. A framework for fast incremental interpre-
tation during speech decoding. Computational Lin-
guistics, 35(3).
William Schuler. 2003. Using model-theoretic se-
mantic interpretation to guide statistical parsing and
word recognition in a spoken language interface. In
Proceedings of the 41st Meeting of the Association
for Computational Linguistics (ACL 2003), Sap-
poro, Japan. Association for Computational Lin-
guistics.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards incremental speech generation in dialogue
systems. In Proceedings of the SIGdial 2010 Con-
ference, pages 1–8, Tokyo, Japan, September.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2009), pages 745–753, Athens,
Greece, March.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, Massachusetts.
Scott C. Stoness, Joel Tetreault, and James Allen.
2004. Incremental parsing with reference inter-
action. In Proceedings of the Workshop on In-
cremental Parsing at the ACL 2004, pages 18–25,
Barcelona, Spain, July.
Scott C. Stoness, James Allen, Greg Aist, and Mary
Swift. 2005. Using real-world reference to improve
spoken language understanding. In AAAI Workshop
on Spoken Language Understanding, pages 38–45.
</reference>
<page confidence="0.998932">
523
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768414">
<title confidence="0.99918">Joint Satisfaction of Syntactic and Pragmatic Improves Incremental Spoken Language Understanding</title>
<author confidence="0.998997">Andreas Peldszus</author>
<affiliation confidence="0.9826325">University of Potsdam Department for Linguistics</affiliation>
<email confidence="0.940439">peldszus@uni-potsdam.de</email>
<author confidence="0.993433">Timo Baumann</author>
<affiliation confidence="0.994836">University of Hamburg Department for Informatics</affiliation>
<email confidence="0.981997">baumann@informatik.uni-hamburg.de</email>
<abstract confidence="0.993662272727273">We present a model of semantic processing of spoken language that (a) is robust against ill-formed input, such as can be expected from automatic speech recognisers, (b) respects both syntactic and pragmatic constraints in the computation of most likely interpretations, (c) uses a principled, expressive semantic representation formalism (RMRS) with a well-defined model theory, and (d) works continuously (producing meaning representations on a wordby-word basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far). We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gregory Aist</author>
<author>James Allen</author>
<author>Ellen Campana</author>
<author>Carlos Gomez Gallo</author>
<author>Scott Stoness</author>
<author>Mary Swift</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Incremental understanding in human-computer dialogue and experimental evidence for advantages over nonincremental methods.</title>
<date>2007</date>
<booktitle>In Proceedings of Decalog 2007, the 11th International Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="1316" citStr="Aist et al., 2007" startWordPosition="182" endWordPosition="185">ntinuously (producing meaning representations on a wordby-word basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far). We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline). 1 Introduction Incremental processing for spoken dialogue systems (i. e., the processing of user input even while it still may be extended) has received renewed attention recently (Aist et al., 2007; Baumann et al., 2009; Buß and Schlangen, 2010; Skantze and Hjalmarsson, 2010; DeVault et al., 2011; Purver et al., 2011). Most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g. (Skantze and Schlangen, 2009)), but has otherwise followed a typical pipeline architecture where processing results are passed only in one direction towards the next module. Okko Bull University of Potsdam Department for Linguistics okko@ling.uni-potsdam.de David Schlangen University </context>
</contexts>
<marker>Aist, Allen, Campana, Gallo, Stoness, Swift, Tanenhaus, 2007</marker>
<rawString>Gregory Aist, James Allen, Ellen Campana, Carlos Gomez Gallo, Scott Stoness, Mary Swift, and Michael K. Tanenhaus. 2007. Incremental understanding in human-computer dialogue and experimental evidence for advantages over nonincremental methods. In Proceedings of Decalog 2007, the 11th International Workshop on the Semantics and Pragmatics of Dialogue, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>Michaela Atterer</author>
<author>David Schlangen</author>
</authors>
<title>Assessing and improving the performance of speech recognition for incremental systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT) 2009 Conference,</booktitle>
<location>Boulder, Colorado, USA,</location>
<contexts>
<context position="1338" citStr="Baumann et al., 2009" startWordPosition="186" endWordPosition="189">ng meaning representations on a wordby-word basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far). We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline). 1 Introduction Incremental processing for spoken dialogue systems (i. e., the processing of user input even while it still may be extended) has received renewed attention recently (Aist et al., 2007; Baumann et al., 2009; Buß and Schlangen, 2010; Skantze and Hjalmarsson, 2010; DeVault et al., 2011; Purver et al., 2011). Most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g. (Skantze and Schlangen, 2009)), but has otherwise followed a typical pipeline architecture where processing results are passed only in one direction towards the next module. Okko Bull University of Potsdam Department for Linguistics okko@ling.uni-potsdam.de David Schlangen University of Bielefeld Departmen</context>
</contexts>
<marker>Baumann, Atterer, Schlangen, 2009</marker>
<rawString>Timo Baumann, Michaela Atterer, and David Schlangen. 2009. Assessing and improving the performance of speech recognition for incremental systems. In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT) 2009 Conference, Boulder, Colorado, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>Okko Bu13</author>
<author>David Schlangen</author>
</authors>
<title>Evaluation and optimization of incremental processors. Dialogue and Discourse,</title>
<date>2011</date>
<volume>2</volume>
<issue>1</issue>
<pages>141</pages>
<marker>Baumann, Bu13, Schlangen, 2011</marker>
<rawString>Timo Baumann, Okko Bu13, and David Schlangen. 2011. Evaluation and optimization of incremental processors. Dialogue and Discourse, 2(1):113– 141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niels Beuck</author>
<author>Arne K¨ohn</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Decision strategies for incremental pos tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 18th Nordic Conference of Computational Linguistics, NODALIDA2011,</booktitle>
<location>Riga, Latvia.</location>
<marker>Beuck, K¨ohn, Menzel, 2011</marker>
<rawString>Niels Beuck, Arne K¨ohn, and Wolfgang Menzel. 2011. Decision strategies for incremental pos tagging. In Proceedings of the 18th Nordic Conference of Computational Linguistics, NODALIDA2011, Riga, Latvia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okko Bu13</author>
<author>David Schlangen</author>
</authors>
<title>Modelling sub-utterance phenomena in spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th International Workshop on the Semantics and Pragmatics of Dialogue (Pozdial2010),</booktitle>
<pages>33--41</pages>
<location>Poznan, Poland,</location>
<marker>Bu13, Schlangen, 2010</marker>
<rawString>Okko Bu13 and David Schlangen. 2010. Modelling sub-utterance phenomena in spoken dialogue systems. In Proceedings of the 14th International Workshop on the Semantics and Pragmatics of Dialogue (Pozdial2010), pages 33–41, Poznan, Poland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Robust minimal recursion semantics.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>Cambridge Computer Lab.</institution>
<note>Unpublished draft.</note>
<contexts>
<context position="12574" citStr="Copestake, 2006" startWordPosition="1828" endWordPosition="1829"> of the derivation (LD), with rule expansions or (robust) lexical matches; the derivation probability (P); and the remaining stack (S), where S* is the grammar’s start symbol and S! an explicit end-of-input marker. (To keep the Figure small, we artificially reduced the beam size and cut off alternatives paths, shown in grey.) 3.3 Semantic Construction Using RMRS As a novel feature, we use for the representation of meaning increments (that is, the contributions of new words and syntactic constructions) as well as for the resulting logical forms the formalism Robust Minimal Recursion Semantics (Copestake, 2006). This is a representation formalism that was originally constructed for semantic underspecification (of scope and other phenomena) and then adapted to serve the purposes of semantics reprethe same predecessor can be regarded as alternatives. Solid arrows indicate which information from a previous level an IU is grounded in (based on); here, every semantic IU is grounded in a syntactic IU, every syntactic IU in a POS-tagIU, and so on. 516 sentations in heterogeneous situations where information from deep and shallow parsers must be combined. In RMRS, meaning representations of a first order lo</context>
<context position="33423" citStr="Copestake, 2006" startWordPosition="5298" endWordPosition="5299">pontaneous speech that strives to jointly satisfy syntactic and pragmatic constraints (the latter being approximated by the assumption that referring expressions are intended to indeed successfully refer in the given context). The model is robust, accepting also input of the kind that can be expected from automatic speech recognisers, and incremental, that is, can be fed input on a word-by-word basis, computing at each increment only exactly the contribution of the new word. Lastly, as another novel contribution, the model makes use of a principled formalism for semantic representation, RMRS (Copestake, 2006). While the results show that our approach of combining syntactic and pragmatic information can work in a real-world setting on realistic data—previous work in this direction has so far 521 only been at the proof-of-concept stage—there is much room for improvement. First, we are now exploring ways of bootstrapping a grammar and derivation weights from hand-corrected parses. Secondly, we are looking at making the variable assignment / model checking function probabilistic, assigning probabilities (degree of strength of belief) to candidate resolutions (as for example the model of Schlangen et a</context>
</contexts>
<marker>Copestake, 2006</marker>
<rawString>Ann Copestake. 2006. Robust minimal recursion semantics. Technical report, Cambridge Computer Lab. Unpublished draft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Semantic composition with (robust) minimal recursion semantics.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Deep Linguistic Processing, DeepLP ’07,</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14336" citStr="Copestake, 2007" startWordPosition="2105" endWordPosition="2106"> way, predicates can be introduced without fixed arity and arguments can be introduced without knowing which predicates they are arguments of. We will make use of this second form of underspecification and enrich lexical predicates with arguments incrementally. Combining two RMRS structures involves at least joining their list of EPs and ARGRELs and of scope constraints. Additionally, equations between the variables can connect two structures, which is an essential requirement for semantic construction. A semantic algebra for the combination of RMRSs in a non-lexicalist setting is defined in (Copestake, 2007). Unsaturated semantic increments have open slots that need to be filled by what is called the hook of another structure. Hook and slot are triples [E:a:x] consisting of a label, an anchor and an index variable. Every variable of the hook is equated with the corresponding one in the slot. This way the semantic representation can grow monotonically at each combinatory step by simply adding predicates, constraints and equations. Our approach differs from (Copestake, 2007) only in the organisation of the slots: In an incremental setting, a proper semantic representation is desired for every singl</context>
</contexts>
<marker>Copestake, 2007</marker>
<rawString>Ann Copestake. 2007. Semantic composition with (robust) minimal recursion semantics. In Proceedings of the Workshop on Deep Linguistic Processing, DeepLP ’07, pages 73–80, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Matthew Stone</author>
</authors>
<title>Domain inference in incremental interpretation.</title>
<date>2003</date>
<booktitle>In Proceedings of ICOS 4: Workshop on Inference in Computational Semantics,</booktitle>
<publisher>INRIA Lorraine.</publisher>
<location>Nancy, France,</location>
<contexts>
<context position="4304" citStr="DeVault and Stone (2003)" startWordPosition="642" endWordPosition="645">roceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 514–523, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics mentation of a “continuous understanding” module that uses reference information in guiding a bottom-up chart-parser, which is evaluated on a single dialogue transcript. In contrast, our model uses a probabilistic top-down parser with beam search (following Roark (2001)) and is evaluated on a large number of real-world utterances as processed by an automatic speech recogniser. Similarly, DeVault and Stone (2003) describe a system that implements interaction between a parser and higher-level modules (in this case, even more principled, trying to prove presuppositions), which however is also only tested on a small, constructed data-set. Schuler (2003) and Schuler et al. (2009) present a model where information about reference is used directly within the speech recogniser, and hence informs not only syntactic processing but also word recognition. To this end, the processing is folded into the decoding step of the ASR, and is realised as a hierarchical HMM. While technically interesting, this approach is</context>
</contexts>
<marker>DeVault, Stone, 2003</marker>
<rawString>David DeVault and Matthew Stone. 2003. Domain inference in incremental interpretation. In Proceedings of ICOS 4: Workshop on Inference in Computational Semantics, Nancy, France, September. INRIA Lorraine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Kenji Sagae</author>
<author>David Traum</author>
</authors>
<title>Incremental Interpretation and Prediction of Utterance Meaning for Interactive Dialogue. Dialogue and Discourse,</title>
<date>2011</date>
<contexts>
<context position="1416" citStr="DeVault et al., 2011" startWordPosition="198" endWordPosition="201">tterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far). We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline). 1 Introduction Incremental processing for spoken dialogue systems (i. e., the processing of user input even while it still may be extended) has received renewed attention recently (Aist et al., 2007; Baumann et al., 2009; Buß and Schlangen, 2010; Skantze and Hjalmarsson, 2010; DeVault et al., 2011; Purver et al., 2011). Most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g. (Skantze and Schlangen, 2009)), but has otherwise followed a typical pipeline architecture where processing results are passed only in one direction towards the next module. Okko Bull University of Potsdam Department for Linguistics okko@ling.uni-potsdam.de David Schlangen University of Bielefeld Department for Linguistics david.schlangen@uni-bielefeld.de In this paper, we investiga</context>
</contexts>
<marker>DeVault, Sagae, Traum, 2011</marker>
<rawString>David DeVault, Kenji Sagae, and David Traum. 2011. Incremental Interpretation and Prediction of Utterance Meaning for Interactive Dialogue. Dialogue and Discourse, 2(1):143–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
<author>David Schlangen</author>
</authors>
<title>Referring under restricted interactivity conditions.</title>
<date>2007</date>
<booktitle>Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>136--139</pages>
<editor>In Simon Keizer, Harry Bunt, and Tim Paek, editors,</editor>
<location>Antwerp, Belgium,</location>
<marker>Fern´andez, Schlangen, 2007</marker>
<rawString>Raquel Fern´andez and David Schlangen. 2007. Referring under restricted interactivity conditions. In Simon Keizer, Harry Bunt, and Tim Paek, editors, Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 136–139, Antwerp, Belgium, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Pad´o</author>
<author>Matthew W Crocker</author>
<author>Frank Keller</author>
</authors>
<title>A probabilistic model of semantic plausibility in sentence processing.</title>
<date>2009</date>
<journal>Cognitive Science,</journal>
<volume>33</volume>
<issue>5</issue>
<marker>Pad´o, Crocker, Keller, 2009</marker>
<rawString>Ulrike Pad´o, Matthew W Crocker, and Frank Keller. 2009. A probabilistic model of semantic plausibility in sentence processing. Cognitive Science, 33(5):794–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Arash Eshghi</author>
<author>Julian Hough</author>
</authors>
<title>Incremental semantic construction in a dialogue system.</title>
<date>2011</date>
<booktitle>Proceedings of the 9th International Conference on Computational Semantics (IWCS),</booktitle>
<pages>365--369</pages>
<editor>In J. Bos and S. Pulman, editors,</editor>
<location>Oxford, UK,</location>
<contexts>
<context position="1438" citStr="Purver et al., 2011" startWordPosition="202" endWordPosition="205">ntally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far). We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline). 1 Introduction Incremental processing for spoken dialogue systems (i. e., the processing of user input even while it still may be extended) has received renewed attention recently (Aist et al., 2007; Baumann et al., 2009; Buß and Schlangen, 2010; Skantze and Hjalmarsson, 2010; DeVault et al., 2011; Purver et al., 2011). Most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g. (Skantze and Schlangen, 2009)), but has otherwise followed a typical pipeline architecture where processing results are passed only in one direction towards the next module. Okko Bull University of Potsdam Department for Linguistics okko@ling.uni-potsdam.de David Schlangen University of Bielefeld Department for Linguistics david.schlangen@uni-bielefeld.de In this paper, we investigate whether the other p</context>
</contexts>
<marker>Purver, Eshghi, Hough, 2011</marker>
<rawString>Matthew Purver, Arash Eshghi, and Julian Hough. 2011. Incremental semantic construction in a dialogue system. In J. Bos and S. Pulman, editors, Proceedings of the 9th International Conference on Computational Semantics (IWCS), pages 365–369, Oxford, UK, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, and Applications.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Cognitive and Linguistic Sciences, Brown University.</institution>
<contexts>
<context position="4159" citStr="Roark (2001)" startWordPosition="621" endWordPosition="622">ture building has been previously explored by a number of authors. Stoness et al. (2004, 2005) describe a proof-of-concept imple514 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 514–523, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics mentation of a “continuous understanding” module that uses reference information in guiding a bottom-up chart-parser, which is evaluated on a single dialogue transcript. In contrast, our model uses a probabilistic top-down parser with beam search (following Roark (2001)) and is evaluated on a large number of real-world utterances as processed by an automatic speech recogniser. Similarly, DeVault and Stone (2003) describe a system that implements interaction between a parser and higher-level modules (in this case, even more principled, trying to prove presuppositions), which however is also only tested on a small, constructed data-set. Schuler (2003) and Schuler et al. (2009) present a model where information about reference is used directly within the speech recogniser, and hence informs not only syntactic processing but also word recognition. To this end, t</context>
<context position="6551" citStr="Roark (2001)" startWordPosition="994" endWordPosition="995">he weights given to particular derivations are not real probabilities anymore, as derivations fall out of the beam and normalisation is not performed after re-weighting. The components of our model are described in the following sections: first the parser which computes the syntactic probability in an incremental, top-down manner; the semantic construction algorithm which associates (underspecified) logical forms to derivations; the reference resolution component that computes the pragmatic plausibility; and the combination that incorporates the feedback from this pragmatic signal. 3.2 Parser Roark (2001) introduces a strategy for incremental probabilistic top-down parsing and shows that it can compete with high-coverage bottom-up parsers. One of the reasons he gives for choosing a top-down approach is that it enables fully leftconnected derivations, where at every processing step new increments directly find their place in the existing structure. This monotonically enriched structure can then serve as a context for incremental language understanding, as the author claims, although this part is not further developed by Roark (2001). He discusses a battery of different techniques for refining h</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, and Applications. Ph.D. thesis, Department of Cognitive and Linguistic Sciences, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Gabriel Skantze</author>
</authors>
<title>A general, abstract model of incremental dialogue processing.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>710--718</pages>
<contexts>
<context position="11542" citStr="Schlangen and Skantze, 2009" startWordPosition="1665" endWordPosition="1668">ested but actually non-existent token; repairs adjust unknown tokens to the requested token. These robust operations have strong penalties on the probability to make sure they will survive in the derivation only in critical situations. Additionally, only a single one of them is allowed to occur between the recognition of two adjacent input tokens. Figure 1 illustrates this process for the first few words of the example sentence “nimm den winkel in der dritten reihe” (take the bracket in the third row), using the incremental unit (IU) model to represent increments and how they are linked; see (Schlangen and Skantze, 2009).2 Here, syntactic 2Very briefly: rounded boxes in the Figures represent IUs, and dashed arrows link an IU to its predecessor on the same level, where the levels correspond to processing stages. The Figure shows the levels of input words, POS-tags, syntactic derivations and logical forms. Multiple IUs sharing derivations (“CandidateAnalysisIUs”) are represented by three features: a list of the last parser actions of the derivation (LD), with rule expansions or (robust) lexical matches; the derivation probability (P); and the remaining stack (S), where S* is the grammar’s start symbol and S! an</context>
</contexts>
<marker>Schlangen, Skantze, 2009</marker>
<rawString>David Schlangen and Gabriel Skantze. 2009. A general, abstract model of incremental dialogue processing. In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 710–718. Association for Computational Linguistics, mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Timo Baumann</author>
<author>Michaela Atterer</author>
</authors>
<title>Incremental reference resolution: The task, metrics for evaluation, and a bayesian filtering model that is sensitive to disfluencies.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGdial 2009, the 10th Annual SIGDIAL Meeting on Discourse and Dialogue,</booktitle>
<location>London, UK,</location>
<contexts>
<context position="18860" citStr="Schlangen et al., 2009" startWordPosition="2848" endWordPosition="2851">he derivations. If pragmatic feedback is enabled, the probability of every reprentation that does not resolve in the current context is degraded by a constant factor (we used 0.001 in our experiments described below, determined by experimentation). The degradation thus changes the derivation order in the parsing queue for the next input item and increases the chances of degraded derivations to be pruned in the following parsing step. 4 Experiments and Results 4.1 Data We use data from the Pentomino puzzle piece domain (which has been used before for example by (Fern´andez and Schlangen, 2007; Schlangen et al., 2009)), collected in a Wizard-of-Oz study. In this specific setting, users gave instructions to the system (the wizard) in order to manipulate (select, rotate, mirror, delete) puzzle pieces on an upper board and to put them onto a lower board, reaching a pre-specified goal state. Figure 2 shows an example configuration. Each participant took part in several rounds in which the distinguishing characteristics for puzzle pieces (color, shape, proposed name, position on the board) varied widely. In total, 20 participants played 284 games. We extracted the semantics of an utterance from the wizard’s res</context>
</contexts>
<marker>Schlangen, Baumann, Atterer, 2009</marker>
<rawString>David Schlangen, Timo Baumann, and Michaela Atterer. 2009. Incremental reference resolution: The task, metrics for evaluation, and a bayesian filtering model that is sensitive to disfluencies. In Proceedings of SIGdial 2009, the 10th Annual SIGDIAL Meeting on Discourse and Dialogue, London, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Timo Baumann</author>
<author>Hendrik Buschmeier</author>
<author>Okko Bu13</author>
<author>Stefan Kopp</author>
<author>Gabriel Skantze</author>
<author>Ramin Yaghoubzadeh</author>
</authors>
<title>Middleware for Incremental Processing in Conversational Agents.</title>
<date>2010</date>
<booktitle>In Proceedings of SigDial 2010,</booktitle>
<location>Tokyo, Japan,</location>
<marker>Schlangen, Baumann, Buschmeier, Bu13, Kopp, Skantze, Yaghoubzadeh, 2010</marker>
<rawString>David Schlangen, Timo Baumann, Hendrik Buschmeier, Okko Bu13, Stefan Kopp, Gabriel Skantze, and Ramin Yaghoubzadeh. 2010. Middleware for Incremental Processing in Conversational Agents. In Proceedings of SigDial 2010, Tokyo, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Stephen Wu</author>
<author>Lane Schwartz</author>
</authors>
<title>A framework for fast incremental interpretation during speech decoding.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="4572" citStr="Schuler et al. (2009)" startWordPosition="683" endWordPosition="686">erence information in guiding a bottom-up chart-parser, which is evaluated on a single dialogue transcript. In contrast, our model uses a probabilistic top-down parser with beam search (following Roark (2001)) and is evaluated on a large number of real-world utterances as processed by an automatic speech recogniser. Similarly, DeVault and Stone (2003) describe a system that implements interaction between a parser and higher-level modules (in this case, even more principled, trying to prove presuppositions), which however is also only tested on a small, constructed data-set. Schuler (2003) and Schuler et al. (2009) present a model where information about reference is used directly within the speech recogniser, and hence informs not only syntactic processing but also word recognition. To this end, the processing is folded into the decoding step of the ASR, and is realised as a hierarchical HMM. While technically interesting, this approach is by design nonmodular and restricted in its syntactic expressivity. The work presented here also has connections to work in psycholinguistics. Pad´o et al. (2009) present a model that combines syntactic and semantic models into one plausibility judgement that is compu</context>
</contexts>
<marker>Schuler, Wu, Schwartz, 2009</marker>
<rawString>William Schuler, Stephen Wu, and Lane Schwartz. 2009. A framework for fast incremental interpretation during speech decoding. Computational Linguistics, 35(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
</authors>
<title>Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics (ACL</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4546" citStr="Schuler (2003)" startWordPosition="680" endWordPosition="681">odule that uses reference information in guiding a bottom-up chart-parser, which is evaluated on a single dialogue transcript. In contrast, our model uses a probabilistic top-down parser with beam search (following Roark (2001)) and is evaluated on a large number of real-world utterances as processed by an automatic speech recogniser. Similarly, DeVault and Stone (2003) describe a system that implements interaction between a parser and higher-level modules (in this case, even more principled, trying to prove presuppositions), which however is also only tested on a small, constructed data-set. Schuler (2003) and Schuler et al. (2009) present a model where information about reference is used directly within the speech recogniser, and hence informs not only syntactic processing but also word recognition. To this end, the processing is folded into the decoding step of the ASR, and is realised as a hierarchical HMM. While technically interesting, this approach is by design nonmodular and restricted in its syntactic expressivity. The work presented here also has connections to work in psycholinguistics. Pad´o et al. (2009) present a model that combines syntactic and semantic models into one plausibili</context>
</contexts>
<marker>Schuler, 2003</marker>
<rawString>William Schuler. 2003. Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface. In Proceedings of the 41st Meeting of the Association for Computational Linguistics (ACL 2003), Sapporo, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>Anna Hjalmarsson</author>
</authors>
<title>Towards incremental speech generation in dialogue systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGdial 2010 Conference,</booktitle>
<pages>1--8</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="1394" citStr="Skantze and Hjalmarsson, 2010" startWordPosition="194" endWordPosition="197">is, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far). We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline). 1 Introduction Incremental processing for spoken dialogue systems (i. e., the processing of user input even while it still may be extended) has received renewed attention recently (Aist et al., 2007; Baumann et al., 2009; Buß and Schlangen, 2010; Skantze and Hjalmarsson, 2010; DeVault et al., 2011; Purver et al., 2011). Most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g. (Skantze and Schlangen, 2009)), but has otherwise followed a typical pipeline architecture where processing results are passed only in one direction towards the next module. Okko Bull University of Potsdam Department for Linguistics okko@ling.uni-potsdam.de David Schlangen University of Bielefeld Department for Linguistics david.schlangen@uni-bielefeld.de In th</context>
</contexts>
<marker>Skantze, Hjalmarsson, 2010</marker>
<rawString>Gabriel Skantze and Anna Hjalmarsson. 2010. Towards incremental speech generation in dialogue systems. In Proceedings of the SIGdial 2010 Conference, pages 1–8, Tokyo, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>David Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>745--753</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="1660" citStr="Skantze and Schlangen, 2009" startWordPosition="238" endWordPosition="241">performance of the NLU component (around 10 % absolute, over a syntax-only baseline). 1 Introduction Incremental processing for spoken dialogue systems (i. e., the processing of user input even while it still may be extended) has received renewed attention recently (Aist et al., 2007; Baumann et al., 2009; Buß and Schlangen, 2010; Skantze and Hjalmarsson, 2010; DeVault et al., 2011; Purver et al., 2011). Most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g. (Skantze and Schlangen, 2009)), but has otherwise followed a typical pipeline architecture where processing results are passed only in one direction towards the next module. Okko Bull University of Potsdam Department for Linguistics okko@ling.uni-potsdam.de David Schlangen University of Bielefeld Department for Linguistics david.schlangen@uni-bielefeld.de In this paper, we investigate whether the other potential advantage of incremental processing— providing “higher-level”-feedback to lower-level modules, in order to improve subsequent processing of the lower-level module—can be realised as well. Specifically, we experime</context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Gabriel Skantze and David Schlangen. 2009. Incremental dialogue processing in a micro-domain. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2009), pages 745–753, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="16284" citStr="Steedman, 2000" startWordPosition="2429" endWordPosition="2430">sting parts of the tree is required. This, however, requires adjustments to the slot structure of RMRS. Left-recursive rules can introduce multiple slots of the same sort before they are filled, which is not allowed in the classic (R)MRS semantic algebra, where only one named slot of each sort can be open at a time. We thus organize the slots as a stack of unnamed slots, where multiple slots of the same sort can be stored, but only the one on top can be accessed. We then define a basic combination operation equivalent to forward function composition (as in standard lambda calculus, or in CCG (Steedman, 2000)) and combine substructures in a principled way across multiple syntactic rules without the need to represent slot names. Each lexical items receives a generic representation derived from its lemma and the basic semantic type (individual, event, or underspecified denotations), determined by its POS tag. This makes the grammar independent of knowledge about what later (semantic) components will actually be able to process (“understand”).3 Parallel to the production of syntactic derivations, as the tree is expanded top-down left-to-right, semantic macros are activated for each syntactic rule, co</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Stoness</author>
<author>Joel Tetreault</author>
<author>James Allen</author>
</authors>
<title>Incremental parsing with reference interaction.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing at the ACL 2004,</booktitle>
<pages>18--25</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3634" citStr="Stoness et al. (2004" startWordPosition="545" endWordPosition="548"> the output of automatic speech recognition. The remainder of this paper is structured as follows: We discuss related work in the next section, and then describe in general terms our model and its components. In Section 4 we then describe the data resources we used for the experiments and the actual implementation of the model, the baselines for comparison, and the results of our experiments. We close with a discussion and an outlook on future work. 2 Related Work The idea of using real-world reference to inform syntactic structure building has been previously explored by a number of authors. Stoness et al. (2004, 2005) describe a proof-of-concept imple514 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 514–523, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics mentation of a “continuous understanding” module that uses reference information in guiding a bottom-up chart-parser, which is evaluated on a single dialogue transcript. In contrast, our model uses a probabilistic top-down parser with beam search (following Roark (2001)) and is evaluated on a large number of real-world utterances as processed </context>
</contexts>
<marker>Stoness, Tetreault, Allen, 2004</marker>
<rawString>Scott C. Stoness, Joel Tetreault, and James Allen. 2004. Incremental parsing with reference interaction. In Proceedings of the Workshop on Incremental Parsing at the ACL 2004, pages 18–25, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Stoness</author>
<author>James Allen</author>
<author>Greg Aist</author>
<author>Mary Swift</author>
</authors>
<title>Using real-world reference to improve spoken language understanding.</title>
<date>2005</date>
<booktitle>In AAAI Workshop on Spoken Language Understanding,</booktitle>
<pages>38--45</pages>
<marker>Stoness, Allen, Aist, Swift, 2005</marker>
<rawString>Scott C. Stoness, James Allen, Greg Aist, and Mary Swift. 2005. Using real-world reference to improve spoken language understanding. In AAAI Workshop on Spoken Language Understanding, pages 38–45.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>