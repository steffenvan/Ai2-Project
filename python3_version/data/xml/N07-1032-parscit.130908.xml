<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000254">
<title confidence="0.576653">
Cross-Instance Tuning of Unsupervised Document Clustering Algorithms*
</title>
<author confidence="0.352097">
Damianos Karakos, Jason Eisner
and Sanjeev Khudanpur
</author>
<affiliation confidence="0.344082">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.854258">
Baltimore, MD 21218
</address>
<email confidence="0.999312">
{damianos,eisner,khudanpur}@jhu.edu
</email>
<note confidence="0.55686225">
Carey E. Priebe
Dept. of Applied Mathematics
and Statistics
Johns Hopkins University
</note>
<address confidence="0.854064">
Baltimore, MD 21218
</address>
<email confidence="0.996605">
cep@jhu.edu
</email>
<sectionHeader confidence="0.994739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957870967742">
In unsupervised learning, where no train-
ing takes place, one simply hopes that
the unsupervised learner will work well
on any unlabeled test collection. How-
ever, when the variability in the data is
large, such hope may be unrealistic; a
tuning of the unsupervised algorithm may
then be necessary in order to perform well
on new test collections. In this paper,
we show how to perform such a tuning
in the context of unsupervised document
clustering, by (i) introducing a degree of
freedom, α, into two leading information-
theoretic clustering algorithms, through
the use of generalized mutual informa-
tion quantities; and (ii) selecting the value
of α based on clusterings of similar, but
supervised document collections (cross-
instance tuning). One option is to perform
a tuning that directly minimizes the error
on the supervised data sets; another option
is to use “strapping” (Eisner and Karakos,
2005), which builds a classifier that learns
to distinguish good from bad clusterings,
and then selects the α with the best pre-
dicted clustering on the test set. Experi-
ments from the “20 Newsgroups” corpus
show that, although both techniques im-
prove the performance of the baseline al-
gorithms, “strapping” is clearly a better
choice for cross-instance tuning.
</bodyText>
<footnote confidence="0.930908">
*This work was partially supported by the DARPA GALE
program (Contract No¯ HR0011-06-2-0001) and by the JHU
WSE/APL Partnership Fund.
</footnote>
<sectionHeader confidence="0.993336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981264705882">
The problem of combining labeled and unlabeled
examples in a learning task (semi-supervised learn-
ing) has been studied in the literature under various
guises. A variety of algorithms (e.g., bootstrapping
(Yarowsky, 1995), co-training (Blum and Mitchell,
1998), alternating structure optimization (Ando and
Zhang, 2005), etc.) have been developed in order to
improve the performance of supervised algorithms,
by automatically extracting knowledge from lots of
unlabeled examples. Of special interest is the work
of Ando and Zhang (2005), where the goal is to build
many supervised auxiliary tasks from the unsuper-
vised data, by creating artificial labels; this proce-
dure helps learn a transformation of the input space
that captures the relatedness of the auxiliary prob-
lems to the task at hand. In essence, Ando and Zhang
(2005) transform the semi-supervised learning prob-
lem to a multi-task learning problem; in multi-task
learning, a (usually large) set of supervised tasks is
available for training, and the goal is to build mod-
els which can simultaneously do well on all of them
(Caruana, 1997; Ben-David and Schuller, 2003; Ev-
geniou and Pontil, 2004).
Little work, however, has been devoted to study
the situation where lots of labeled examples, of one
kind, are used to build a model which is tested on
unlabeled data of a “different” kind. This problem,
which is the topic of this paper, cannot be cast as a
multi-task learning problem (since there are labeled
examples of only one kind), neither can be cast as a
semi-supervised problem (since there are no training
labels for the test task). Note that we are interested
in the case where the hidden test labels may have
no semantic relationship with the training labels; in
</bodyText>
<page confidence="0.956766">
252
</page>
<note confidence="0.7992615">
Proceedings of NAACL HLT 2007, pages 252–259,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999920690140845">
some cases, there may not even be any informa-
tion about the test labels—what they represent, how
many they are, or at what granularity they describe
the data. This situation can arise in the case of un-
supervised clustering of documents from a large and
diverse corpus: it may not be known in what way the
resulting clusters split the corpus (is it in terms of
topic? genre? style? authorship? a combination of
the above?), unless one inspects each resulting clus-
ter to determine its “meaning.”
At this point, we would like to differentiate be-
tween two concepts: a target task refers to a class
of problems that have a common, high-level de-
scription (e.g., the text document clustering task, the
speech recognition task, etc.). On the other hand,
a task instance refers to a particular example from
the class. For instance, if the task is “document
clustering,” a task instance could be “clustering of
a set of scientific documents into particular fields”;
or, if the task is “parsing,” a task instance could be
“parsing of English sentences from the Wall Street
Journal corpus”. For the purposes of this paper, we
further assume that there are task instances which
are unrelated, in the sense that that there are no
common labels between them. For example, if the
task is “clustering from the 20 Newsgroups corpus,”
then “clustering of the computer-related documents
into PC-related and Mac-related” and “clustering
of the politics-related documents into Middle-East-
related and non-Middle-East-related” are two dis-
tinct, unrelated instances. In more mathematical
terms, if task instances T1, T2 take sets of observa-
tions X1, X2 as input, and try to predict labels from
sets S1, S2, respectively, then they are called unre-
lated if X1 n X2 = o and S1 n S2 = o.
The focus of this paper is to study the problem
of cross-instance tuning of unsupervised algorithms:
how one can tune an algorithm, which is used to
solve a particular task instance, using knowledge
from an unrelated task instance. To the best of our
knowledge, this cross-instance learning problem has
only been tackled in (Eisner and Karakos, 2005),
whose “strapping” procedure learns a meta-classifier
for distinguishing good from bad clusterings.
In this paper, we introduce a scalar parameter α
(a new degree of freedom) into two basic unsuper-
vised clustering algorithms. We can tune α to max-
imize unsupervised clustering performance on dif-
ferent task instances where the correct clustering is
known. The hope is that tuning the parameter learns
something about the task in general, which trans-
fers from the supervised task instances to the un-
supervised one. Alternatively, we can tune a meta-
classifier so as to select good values of α on the su-
pervised task instances, and then use the same meta-
classifier to select a good (possibly different) value
of α in the unsupervised case.
The paper is organized as follows: Section 2 gives
a background on text categorization, and briefly de-
scribes the algorithms that we use in our experi-
ments. Section 3 describes our parameterization of
the clustering algorithms using Jensen-R´enyi diver-
gence and Csisz´ar’s mutual information. Experi-
mental results from the “20 Newsgroups” data set
are shown in Section 4, along with two techniques
for cross-instance learning: (i) “strapping,” which, at
test time, picks a parameter based on various “good-
ness” cues that were learned from the labeled data
set, and (ii) learning the parameter from a supervised
data set which is chosen to statistically match the test
set. Finally, concluding remarks appear in Section 5.
</bodyText>
<sectionHeader confidence="0.975844" genericHeader="method">
2 Document Categorization
</sectionHeader>
<bodyText confidence="0.9948777">
Document categorization is the task of deciding
whether a piece of text belongs to any of a set of
prespecified categories. It is a generic text process-
ing task useful in indexing documents for later re-
trieval, as a stage in natural language processing
systems, for content analysis, and in many other
roles (Lewis and Hayes, 1994). Here, we deal
with the unsupervised version of document cate-
gorization, in which we are interested in cluster-
ing together documents which (hopefully) belong to
the same topic, without having any training exam-
ples.1 Supervised information-theoretic clustering
approaches (Torkkola, 2002; Dhillon et al., 2003)
have been shown to be very effective, even with a
small amount of labeled data, while unsupervised
methods (which are of particular interest to us) have
been shown to be competitive, matching the classifi-
cation accuracy of supervised methods.
Our focus in this paper is on document catego-
rization algorithms which use information-theoretic
</bodyText>
<footnote confidence="0.999234">
1By this, we mean that training examples having the same
category labels as the test examples are not available.
</footnote>
<page confidence="0.998841">
253
</page>
<bodyText confidence="0.999860789473684">
criteria, since there are natural ways of generalizing
these criteria through the introduction of tunable pa-
rameters. We use two such algorithms in our exper-
iments, the sequential Information Bottleneck (sIB)
and Iterative Denoising Trees (IDTs); details about
these algorithms appear below.
A note on mathematical notation: We assume
that we have a collection A = {X(1), ... , X(N)}
of N documents. Each document X(i) is essentially
a “bag of words”, and induces an empirical distri-
bution PX(i) on the vocabulary X. Given a sub-
set (cluster) C of documents, the conditional dis-
tribution on X, given the cluster, is just the cen-
troid: PX|C = |c |EX(i)∈C PX(i). If a subcollec-
tion 5 ⊂ A of documents is partitioned into clusters
C1, ... , Cm, and each document X(i) ∈ 5 is as-
signed to a cluster CZ(i), where Z(i) ∈ {1,... , m}
is the cluster index, then the mutual information be-
tween words and corresponding clusters is given by
</bodyText>
<equation confidence="0.975488333333333">
�
I(X; Z|5) =
z∈{1,...,m}
</equation>
<bodyText confidence="0.999444">
where P(z|5) °_ |Cz|/|5 |is the “prior” distribution
on the clusters and D(·k·) is the Kullback-Leibler
divergence (Cover and Thomas, 1991).
</bodyText>
<subsectionHeader confidence="0.973498">
2.1 The Information Bottleneck Method
</subsectionHeader>
<bodyText confidence="0.989968047619048">
The Information Bottleneck (IB) method (Tishby et
al., 1999; Slonim and Tishby, 2000; Slonim et al.,
2002) is one popular approach to unsupervised cat-
egorization. The goal of the IB (with “hard” clus-
tering) is to find clusters such that the mutual in-
formation I(X; Z) between words and clusters is as
large as possible, under a constraint on the number
of clusters. The procedure for finding the maximiz-
ing clustering in (Slonim and Tishby, 2000) is ag-
glomerative clustering, while in (Slonim et al., 2002)
it is based on many random clusterings, combined
with a sequential update algorithm, similar to K-
means. The update algorithm re-assigns each data
point (document) d to its most “similar” cluster C,
in order to minimize I(X; Z|C ∪ {d}), i.e.,
�PX|{d}k�PX|{d}∪C)+(1−S)D( PX|Ck �PX|{d}∪C),
where S = 1
|C|�1. This latter procedure is called
the sequential Information Bottleneck (sIB) method,
and is considered the state-of-the-art in unsuper-
vised document categorization.
</bodyText>
<subsectionHeader confidence="0.996766">
2.2 Iterative Denoising Trees
</subsectionHeader>
<bodyText confidence="0.999956476190476">
Decision trees are a powerful technique for equiva-
lence classification, accomplished through a recur-
sive successive refinement (Jelinek, 1997). In the
context of unsupervised classification, the goal of
decision trees is to cluster empirical distributions
(bags of words) into a given number of classes, with
each class corresponding to a leaf in the tree. They
are built top-down (as opposed to the bottom-up
construction in IB) using maximization of mutual
information between words and clusters I(X; Z|t)
to drive the splitting of each node t; the hope is that
each leaf will contain data points which belong to
only one latent category.
Iterative Denoising Trees (also called Integrated
Sensing and Processing Decision Trees) were intro-
duced in (Priebe et al., 2004a), as an extension of
regular decision trees. Their main feature is that
they transform the data at each node, before split-
ting, by projecting into a low-dimensional space.
This transformation corresponds to feature extrac-
tion; different features are suppressed (or ampli-
fied) by each transformation, depending on what
data points fall into each node (corpus-dependent-
feature-extraction property (Priebe et al., 2004b)).
Thus, dimensionality reduction and clustering are
chosen so that they jointly optimize the local objec-
tive.
In (Karakos et al., 2005), IDTs were used for an
unsupervised hyperspectral image segmentation ap-
plication. The objective at each node t was to maxi-
mize the mutual information between spectral com-
ponents and clusters given the pixels at node t, com-
puted from the projected empirical distributions. At
each step of the tree-growing procedure, the node
which yielded the highest increase in the average,
per-node mutual information, was selected for split-
ting (until a desired number of leaves was reached).
In (Karakos et al., 2007b), the mutual information
objective was replaced with a parameterized form of
mutual information, namely the Jensen-R´enyi diver-
gence (Hero et al., 2001; Hamza and Krim, 2003), of
which more details are provided in the next section.
</bodyText>
<sectionHeader confidence="0.950786" genericHeader="method">
3 Parameterizing Unsupervised Clustering
</sectionHeader>
<bodyText confidence="0.99973">
As mentioned above, the algorithms considered in
this paper (sIB and IDTs) are unsupervised, in the
</bodyText>
<equation confidence="0.558061">
PX|S),
P(z|5)D(
�PX|Czk
SD(
</equation>
<page confidence="0.98875">
254
</page>
<bodyText confidence="0.999984913043478">
sense that they can be applied to test data with-
out any need for tuning. Our procedure of adapt-
ing them, based on some supervision on a different
task instance, is by introducing a parameter into the
unsupervised algorithm. At least for simple cross-
instance tuning, this parameter represents the infor-
mation which is passed between the supervised and
the unsupervised instances.
The parameterizations that we focused on have
to do with the information-theoretic objectives in
the two unsupervised algorithms. Specifically, fol-
lowing (Karakos et al., 2007b), we replace the mu-
tual information quantities in IDTs as well as sIB
with the parameterized mutual information mea-
sures mentioned above. These two quantities pro-
vide estimates of the dependence between the ran-
dom quantities in their arguments, just as the usual
mutual information does, but also have a scalar pa-
rameter α ∈ (0, 1] that controls the sensitivity of the
computed dependence on the details of the joint dis-
tribution of X and Z. As a result, the effect of data
sparseness on estimation of the joint distribution can
be mitigated when computing these measures.
</bodyText>
<subsectionHeader confidence="0.985281">
3.1 Jensen-Renyi Divergence
</subsectionHeader>
<bodyText confidence="0.9996648">
The Jensen-R´enyi divergence was used in (Hero et
al., 2001; Hamza and Krim, 2003) as a measure of
similarity for image classification and retrieval. For
two discrete random variables X, Z with distribu-
tions PX, PZ and conditional PX|Z, it is defined as
</bodyText>
<equation confidence="0.97520025">
Iα(X; Z) = Hα(PX) − X PZ(z)Hα(PX|Z(·|z)),
Z
(1)
where Hα(·) is the R´enyi entropy, given by
1 X !P(x)α , α =6 1. (2)
Hα(P) = log
1 − α
xEX
</equation>
<bodyText confidence="0.9946005">
If α ∈ (0, 1), Hα is a concave function, and hence
Iα(X; Z) is non-negative (and it is equal to zero if
and only if X and Z are independent). In the limit
as α → 1, Hα(·) approaches the Shannon entropy
(not an obvious fact), so Iα(·) reduces to the regular
mutual information. Similarly, we define
</bodyText>
<equation confidence="0.970033">
Iα(X; Z|W) = X PW(w)Iα(X; Z|W = w),
W
</equation>
<bodyText confidence="0.999867117647059">
where Iα(X; Z|W = w) is computed via (1) using
the conditional distribution of X and Z given W.
Except in trivial cases, Hα(·) is strictly larger
than H(·) when 0 &lt; α &lt; 1; this means that the ef-
fects of extreme sparsity (few words per document,
or too few occurrences of non-frequent words) on
the estimation of entropy and mutual information
can be dampened with an appropriate choice of α.
This happens because extreme sparsity in the data
yields empirical distributions which lie at, or close
to, the boundary of the probability simplex. The
entropy of such distributions is usually underesti-
mated, compared to the smooth distributions which
generate the data. R´enyi’s entropy is larger than
Shannon’s entropy, especially in those regions close
to the boundary, and can thus provide an estimate
which is closer to the true entropy.
</bodyText>
<subsectionHeader confidence="0.999528">
3.2 Csiszar’s Mutual Information
</subsectionHeader>
<bodyText confidence="0.965567882352941">
Csisz´ar defined the mutual information of order α as
PZ(z)Dα(PX|Z(·|z)kQ(·)),
(3)
where Dα(·k·) is the R´enyi divergence (Csisz´ar,
1995). It was shown that Ia (X; Z) retains most
of the properties of I(X; Z)—it is a non-negative,
continuous, and concave function of PX, it is con-
vex in PX|Z for α &lt; 1, and converges to I(X; Z) as
α → 1.
Notably, Ia (X; Z) ≤ I(X; Z) for 0 &lt; α &lt; 1;
this means, as above, that α regulates the overesti-
mation of mutual information that may result from
data sparseness.
There is no analytic form for the minimizer of the
right-hand-side of (3) (Csisz´ar, 1995), but it may be
computed via an alternating minimization algorithm
(Karakos et al., 2007a).
</bodyText>
<sectionHeader confidence="0.999773" genericHeader="method">
4 Experimental Methods and Results
</sectionHeader>
<bodyText confidence="0.99975725">
We demonstrate the feasibility of cross-instance tun-
ing with experiments on unsupervised document cat-
egorization from the 20 Newsgroups corpus (Lang,
1995); this corpus consists of roughly 20,000 news
articles, evenly divided among 20 Usenet groups.
Random samples of 500 articles each were chosen
by (Slonim et al., 2002) to create multiple test col-
lections: 250 each from 2 arbitrarily chosen Usenet
</bodyText>
<equation confidence="0.99654925">
X
Ia (X; Z) = min
Q
Z
</equation>
<page confidence="0.977164">
255
</page>
<bodyText confidence="0.99954886">
groups for the Binary test collection, 100 articles
each from 5 groups for the Multi5 test collection,
and 50 each from 10 groups for the Multi10 test col-
lection. Three independent test collections of each
kind (Binary, Multi5 and Multi10) were created, for
a total of 9 collections. The sIB method was used to
separately cluster each collection, given the correct
number of clusters.
A comparison of sIB and IDTs on the same 9 test
collections was reported in (Karakos et al., 2007b;
Karakos et al., 2007a). Matlab code from (Slonim,
2003) was used for the sIB experiments, while the
parameterized mutual information measures of Sec-
tion 3 were used for the IDTs. A comparison was
also made with the EM-based Gaussian mixtures
clustering tool mclust (Fraley and Raftery, 1999),
and with a simple K-means algorithm. Since the
two latter techniques gave uniformly worse cluster-
ings than those of sIB and IDTs, we omit them from
the following discussion.
To show that our methods work beyond the 9 par-
ticular 500-document collections described above,
in this paper we instead use five different randomly
sampled test collections for each of the Binary,
Multi5 and Multi10 cases, making for a total of 15
new test collections in this paper. For diversity, we
ensure that none of the five test collections (in each
case) contain any documents used in the three col-
lections of (Slonim et al., 2002) (for the same case).
We pre-process the documents of each test col-
lection using the procedure2 mentioned in (Karakos
et al., 2007b). The 15 test collections are then
converted to feature matrices—term-document fre-
quency matrices for sIB, and discounted tf/idf ma-
trices (according to the Okapi formula (Gatford et
al., 1995)) for IDTs—with each row of a matrix rep-
resenting one document in that test collection.
2Excluding the subject line, the header of each abstract is
removed. Stop-words such as a, the, is, etc. are removed, and
stemming is performed (e.g., common suffixes such as -ing, -
er, -ed, etc., are removed). Also, all numbers are collapsed
to one symbol, and non-alphanumeric sequences are converted
to whitespace. Moreover, as suggested in (Yang and Pedersen,
1997) as an effective method for reducing the dimensionality of
the feature space (number of distinct words), all words which
occur fewer than t times in the corpus are removed. For the
sIB experiments, we use t = 2 (as was done in (Slonim et al.,
2002)), while for the IDT experiments we use t = 3; these
choices result in the best performance for each method, respec-
tively, on another dataset.
</bodyText>
<subsectionHeader confidence="0.99504">
4.1 Selecting α with “Strapping”
</subsectionHeader>
<bodyText confidence="0.999982777777778">
In order to pick the value of the parameter α for
each of the sIB and IDT test experiments, we use
“strapping” (Eisner and Karakos, 2005), which, as
we mentioned earlier, is a technique for training a
meta-classifier that chooses among possible cluster-
ings. The training is based on unrelated instances of
the same clustering task. The final choice of cluster-
ing is still unsupervised, since no labels (or ground
truth, in general) for the instance of interest are used.
Here, our collection of possible clusterings for
each test collection is generated by varying the α pa-
rameter. Strapping does not care, however, how the
collection was generated. (In the original strapping
paper, for example, Eisner and Karakos (2005) gen-
erated their collection by bootstrapping word-sense
classifiers from 200 different seeds.)
Here is how we choose a particular unsupervised
α-clustering to output for a given test collection:
</bodyText>
<listItem confidence="0.8778738">
• We cluster the test collection (e.g., the first Multi5
collection) with various values of α, namely α =
0.1,0.2,...,1.0.
• We compute a feature vector from each of the
clusterings. Note that the features are computed
</listItem>
<bodyText confidence="0.833022222222222">
from only the clusterings and the data points,
since no labels are available.
• Based on the feature vectors, we predict the
“goodness” of each clustering, and return the
“best” one.
How do we predict the “goodness” of a cluster-
ing? By first learning to distinguish good cluster-
ings from bad ones, by using unrelated instances of
the task on which we know the true labels:
</bodyText>
<listItem confidence="0.974041875">
• We cluster some unrelated datasets with various
values of α, just as we will do in the test condi-
tion.
• We evaluate each of the resulting clusterings us-
ing the true labels on its dataset.3
• We train a “meta-classifier” that predicts the true
rank (or accuracy) of each clustering based on the
feature vector of the clustering.
</listItem>
<footnote confidence="0.977973333333333">
3To evaluate a clustering, one only really needs the true la-
bels on a sample of the dataset, although in our experiments we
did have true labels on the entire dataset.
</footnote>
<page confidence="0.998122">
256
</page>
<bodyText confidence="0.974084833333333">
Specifically, for each task (Binary, Multi5, and
Multi10) and each clustering method (sIB and IDT),
a meta-classifier is learned thus:
• We obtain 10 clusterings (α = 0.1, 0.2, ... ,1.0)
for each of 5 unrelated task instances (datasets
whose construction is described below).
</bodyText>
<listItem confidence="0.962142">
• For each of these 50 clusterings, we compute the
following 14 features: (i) One minus the aver-
age cosine of the angle (in tf/idf space) between
each example and the centroid of the cluster to
which it belongs. (ii) The average R´enyi diver-
gence, computed for parameters 1.0, 0.5, 0.1, be-
tween the empirical distribution of each example
and the centroid of the cluster to which it belongs.
(iii) We create 10 more features, one per α. For
the α used in this clustering, the feature value is
equal to e−0-1r, where r is the average rank of the
clustering (i.e., the average of the 4 ranks result-
ing from sorting all 10 clusterings (per training
example) according to one of the 4 features in (i)
and (ii)). For all other α’s, the feature is set to
zero. Thus, only α’s which yield relatively good
rankings can have non-zero features in the model.
• We normalize each group of 10 feature vectors,
translating and scaling each of the 14 dimensions
to make it range from 0 to 1. (We will do the same
at test time.)
• We train ranking SVMs (Joachims, 2002), with
</listItem>
<bodyText confidence="0.967278068181818">
a Gaussian kernel, to learn how to rank these 50
clusterings given their respective normalized fea-
ture vectors. The values of c, -y (which control
regularization and the Gaussian kernel) were op-
timized through leave-one-out cross validation in
order to maximize the average accuracy of the
top-ranked clustering, over the 5 training sets.
Once a local maximum of the average accuracy
was obtained, further tuning of c, -y to maximize
the Spearman rank correlation between the pre-
dicted and true ranks was performed.
A model trained in this way knows something
about the task, and may work well for many new,
unseen instances of the task. However, we pre-
sume that it will work best on a given test instance
if trained on similar instances. The ideal would be
to match the test collection in every aspect: (i) the
number of training labels should be equal to the
number of desired clusters of the test collection; (ii)
the training clusters should be topically similar to
the desired test clusters.
In our scenario, we enjoy the luxury of plenty
of labeled data that can be used to create similar
instances. Thus, given a test collection A to be
clustered into L clusters, we create similar train-
ing sets by identifying the L training newsgroups
whose centroids in tf/idf space (using the Okapi for-
mula mentioned earlier) have the smallest angle to
the centroid of A.4 (Of course, we exclude news-
groups that appear in A.) We then form a supervised
500-document training set A&apos; by randomly choosing
500/L documents from each of these L newsgroups;
we do this 5 times to obtain 5 supervised training
sets.
Table 1 shows averaged classification errors re-
sulting from strapping (“str” rows) for the Jensen-
R´enyi divergence and Csisz´ar’s mutual information,
used within IDTs and sIB, respectively. (We also
tried the reverse, using Jensen-R´enyi in sIB and
Csisz´ar’s in IDTs, but the results were uniformly
worse in the former case and no better in the latter
case.) The “MI” rows show the classification errors
of the untuned algorithms (α = 1), which, in almost
all cases, are worse than the tuned ones.
</bodyText>
<subsectionHeader confidence="0.996091">
4.2 Tuning α on Statistically Similar Examples
</subsectionHeader>
<bodyText confidence="0.911402809523809">
We now show that strapping outperforms a simpler
and more obvious method for cross-instance tun-
ing. To cluster a test collection A, we could simply
tune the clustering algorithm by choosing the α that
works best on a related task instance.
We again take care to construct a training instance
A&apos; that is closely related to the test instance A. In
fact, we take even greater care this time. Given A,
4For each of the Binary collections, the closest training
newsgroups in our experiments were talk.politics.guns,
talk.religion.misc; for each of the Multi5 collections
the closest newsgroups were sci.electronics, rec.autos,
sci.med, talk.politics.misc, talk.religion.misc, and for
the Multi10 collections they were talk.politics.misc,
rec.motorcycles, talk.religion.misc, comp.graphics,
comp.sys.ibm.pc.hardware, rec.sport.baseball, comp.os.ms-
windows.misc, comp.windows.x, soc.religion.christian,
talk.politics.mideast. Note that each of the Binary test
collections happens to be closest to the same two training
newsgroups; a similar behavior was observed for the Multi5
and Multi10 newsgroups.
</bodyText>
<page confidence="0.989249">
257
</page>
<table confidence="0.999909888888889">
Set Binary Multi5 Multi10
��������
Method
IDTs MI 11.3% 9.9% 42.2%
Iα (str) 10.4% 9.2% 39.0%
Iα (rls) 10.1% 10.4% 42.7%
sIB MI 12.0% 6.8% 38.5%
ICα (str) 11.2% 6.9% 35.8%
ICα (rls) 11.1% 7.4% 37.4%
</table>
<tableCaption confidence="0.999357">
Table 1: Average classification errors for IDTs and
</tableCaption>
<bodyText confidence="0.9924579375">
sIB, using strapping (“str” rows) and regularized
least squares (“rls” rows) to pick α in Jensen-R´enyi
divergence and Csisz´ar’s mutual information. Rows
“MI” show the errors resulting from the untuned al-
gorithms, which use the regular mutual information
objective (α = 1). Results which are better than the
corresponding “MI” results are shown in bold.
we identify the same set of L closest newsgroups as
described above. This time, however, we carefully
select |A|/L documents from each newsgroup rather
than randomly choosing 500/L of them. Specifi-
cally, for each test example (document) X E A, we
add a similar training example X&apos; into A&apos;, chosen as
follows:
We associate each test example X to the most
similar of the L training newsgroups, under a con-
straint that only |A|/L training examples may be as-
sociated to each newsgroup. To do this, we iterate
through all pairs (X, G) where X is a test example
and G is a training newsgroup, in increasing order
by the angle between X and G. If X is not yet asso-
ciated and G is not yet “full,” then we associate X
with G, and choose X&apos; to be the document in G with
the smallest angle to X.
We cluster A&apos; 10 times, for α = 0.1, ... ,1.0,
and we collect supervised error results £(α), α E
{0.1, ... ,1.0}. Now, instead of using the single best
α* = argminα £(α) to cluster A (which may re-
sult in overfitting) we use regularized least-squares
(RLS) (Hastie et al., 2001), where we try to approx-
imate the probability that an α is the best. The esti-
mated probabilities are given by
</bodyText>
<equation confidence="0.986125">
p� = K(λI + K)−1p,
</equation>
<bodyText confidence="0.985477">
where I is the unit matrix, p is the training prob-
ability of the best α (i.e., it is 1 at the position of
α* and zero elsewhere), and K is the kernel matrix,
where K(i, j) = exp(−(£(αi) − £(αj))2/σ2) is
the value of the kernel which expresses the “sim-
ilarity” between two clusterings of the same train-
ing dataset, in terms of their errors. The parame-
ters σ, γ are set to 0.5, 0.1, respectively, after per-
forming a (local) maximization of the Spearman cor-
relation between training accuracies and predicted
probabilities .6, for all 15 training instances. Af-
ter performing a linear normalization of p� to make
it a probability vector, the average predicted value
of α, i.e., α� = P10
i�1 pi αi, (rounded-off to one of
{0.1, ... ,1.0}) is used to cluster A.
Table 1 shows the average classification error re-
sults using RLS (“rls” rows). We can see that, on
average over the 15 test instances, the error rate of
the tuned IDTs and sIB algorithms is lower than that
of the untuned algorithms, so cross-instance tuning
was effective. On the other hand, the errors are
generally higher than that of the strapping method,
which examines the results of using different α val-
ues on A.
</bodyText>
<sectionHeader confidence="0.981522" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999986">
We have considered the problem of cross-instance
tuning of two unsupervised document clustering al-
gorithms, through the introduction of a degree of
freedom into their mutual information objective.
This degree of freedom is tuned using labeled doc-
ument collections (which are unrelated to the test
collections); we explored two approaches for per-
forming the tuning: (i) through a judicious sampling
of training data, to match the marginal statistics of
the test data, and (ii) via “strapping”, which trains a
meta-classifier to distinguish between good and bad
clusterings. Our unsupervised categorization exper-
iments from the “20 Newsgroups” corpus indicate
that, although both approaches improve the base-
line algorithms, “strapping” is clearly a better choice
for knowledge transfer between unrelated task in-
stances.
</bodyText>
<sectionHeader confidence="0.995949" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.85806225">
R. K. Ando and T. Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6:1817–1853, Nov.
</reference>
<page confidence="0.985252">
258
</page>
<reference confidence="0.999869722222222">
S. Ben-David and R. Schuller. 2003. Exploiting task
relatedness for multiple task learning. In Proc. of
the Sixteenth Annual Conference on Learning Theory
(COLT-03).
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of the Workshop on Computational Learning Theory
(COLT-98), pages 92–100.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41–75.
T. Cover and J. Thomas. 1991. Elements of Information
Theory. John Wiley and Sons.
I. Csisz´ar. 1995. Generalized cutoff rates and R´enyi’s
information measures. IEEE Trans. on Information
Theory, 41(1):26–34, January.
I. Dhillon, S. Mallela, and R. Kumar. 2003. A divisive
information-theoretic feature clustering algorithm
for text classification. Journal of Machine Learning
Research (JMLR), Special Issue on Variable and
Feature Selection, pages 1265–1287, March.
J. Eisner and D. Karakos. 2005. Bootstrapping without
the boot. In Proc. 2005 Conference on Human
Language Technology /Empirical Methods in Natural
Language Processing (HLT/EMNLP 2005), October.
T. Evgeniou and M. Pontil. 2004. Regularized multi-task
learning. In Proc. Knowledge Discovery and Data
Mining.
C. Fraley and A. E. Raftery. 1999. Mclust: Software for
model-based cluster analysis. Journal on Classifica-
tion, 16:297–306.
M. Gatford, M. M. Hancock-Beaulieu, S. Jones,
S. Walker, and S. E. Robertson. 1995. Okapi at
TREC-3. In The Third Text Retrieval Conference
(TREC-3), pages 109–126.
A. Ben Hamza and H. Krim. 2003. Jensen-R´enyi
divergence measure: Theoretical and computational
perspectives. In Proc. IEEE Int. Symp. on Information
Theory, Yokohama, Japan, June.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer-Verlag.
A. O. Hero, B. Ma, O. Michel, and J. Gorman. 2001.
Alpha-divergence for classification, indexing and
retrieval. Technical Report CSPL-328, University of
Michigan Ann Arbor, Communications and Signal
Processing Laboratory, May.
F. Jelinek. 1997. Statistical Methods for Speech Recog-
nition. MIT Press.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM Conf. on Knowledge
Discovery and Data Mining (KDD).
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2005. Unsupervised classification via decision trees:
An information-theoretic perspective. In Proc. 2005
International Conference on Acoustics, Speech and
Signal Processing (ICASSP 2005), March.
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2007a. Information-theoretic aspects of iterative
denoising. Submitted to the Journal of Machine
Learning Research, February.
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2007b. Iterative denoising using Jensen-R´enyi diver-
gences with an application to unsupervised document
categorization. In Proc. 2007 International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP 2007), April.
K. Lang. 1995. Learning to filter netnews. In Proc. 13th
Int. Conf. on Machine Learning, pages 331–339.
David D. Lewis and Philip J. Hayes. 1994. Guest
editorial. ACM Transactions on Information Systems,
12(3):231, July.
C. E. Priebe, D. J. Marchette, and D. M. Healy.
2004a. Integrated sensing and processing decision
trees. IEEE Trans. on Pat. Anal. and Mach. Intel.,
26(6):699–708, June.
C. E. Priebe, D. J. Marchette, Y. Park, E. Wegman,
J. Solka, D. Socolinsky, D. Karakos, K. Church,
R. Guglielmi, R. Coifman, D. Lin, D. Healy, M. Ja-
cobs, and A. Tsao. 2004b. Iterative denoising for
cross-corpus discovery. In Proc. 2004 International
Symposium on Computational Statistics (COMPSTAT
2004), August.
N. Slonim and N. Tishby. 2000. Document clustering
using word clusters via the information bottleneck
method. In Research and Development in Information
Retrieval, pages 208–215.
N. Slonim, N. Friedman, and N. Tishby. 2002. Un-
supervised document classification using sequential
information maximization. In Proc. SIGIR’02, 25th
ACM Int. Conf. on Research and Development of
Inform. Retrieval.
N. Slonim. 2003. IBA 1.0: Matlab code for information
bottleneck clustering algorithms. Available from
http://www.princeton.edu/∼nslonim/IB Release1.0/
IB Release1 0.tar.
N. Tishby, F. Pereira, and W. Bialek. 1999. The informa-
tion bottleneck method. In 37th Allerton Conference
on Communication and Computation.
K. Torkkola. 2002. On feature extraction by mutual in-
formation maximization. In Proc. IEEE Int. Conf. on
Acoustics, Speech and Signal Proc. (ICASSP-2002),
May.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Intl. Conf.
on Machine Learning (ICML-97), pages 412–420.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. 33rd
Annual Meeting of the Association for Computational
Linguistics, pages 189–196, Cambridge, MA.
</reference>
<page confidence="0.998533">
259
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.086345">
<title confidence="0.99973">Tuning of Unsupervised Document Clustering</title>
<author confidence="0.994667">Damianos Karakos</author>
<author confidence="0.994667">Jason</author>
<affiliation confidence="0.805971666666667">and Sanjeev Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.964134">Baltimore, MD</address>
<author confidence="0.976036">E Carey</author>
<affiliation confidence="0.894212333333333">Dept. of Applied and Johns Hopkins</affiliation>
<address confidence="0.89854">Baltimore, MD</address>
<email confidence="0.998603">cep@jhu.edu</email>
<abstract confidence="0.956478657142857">In unsupervised learning, where no training takes place, one simply hopes that the unsupervised learner will work well test collection. However, when the variability in the data is large, such hope may be unrealistic; a the unsupervised algorithm may then be necessary in order to perform well on new test collections. In this paper, we show how to perform such a tuning in the context of unsupervised document clustering, by (i) introducing a degree of into two leading informationtheoretic clustering algorithms, through the use of generalized mutual information quantities; and (ii) selecting the value on clusterings of similar, but collections (crossinstance tuning). One option is to perform a tuning that directly minimizes the error on the supervised data sets; another option is to use “strapping” (Eisner and Karakos, 2005), which builds a classifier that learns to distinguish good from bad clusterings, then selects the the best predicted clustering on the test set. Experiments from the “20 Newsgroups” corpus show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. work was partially supported by the DARPA GALE (Contract HR0011-06-2-0001) and by the JHU WSE/APL Partnership Fund.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R K Ando</author>
<author>T Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="2097" citStr="Ando and Zhang, 2005" startWordPosition="309" endWordPosition="312">show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. *This work was partially supported by the DARPA GALE program (Contract No¯ HR0011-06-2-0001) and by the JHU WSE/APL Partnership Fund. 1 Introduction The problem of combining labeled and unlabeled examples in a learning task (semi-supervised learning) has been studied in the literature under various guises. A variety of algorithms (e.g., bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), alternating structure optimization (Ando and Zhang, 2005), etc.) have been developed in order to improve the performance of supervised algorithms, by automatically extracting knowledge from lots of unlabeled examples. Of special interest is the work of Ando and Zhang (2005), where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand. In essence, Ando and Zhang (2005) transform the semi-supervised learning problem to a multi-task learning problem; in multi</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>R. K. Ando and T. Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ben-David</author>
<author>R Schuller</author>
</authors>
<title>Exploiting task relatedness for multiple task learning.</title>
<date>2003</date>
<booktitle>In Proc. of the Sixteenth Annual Conference on Learning Theory (COLT-03).</booktitle>
<contexts>
<context position="2906" citStr="Ben-David and Schuller, 2003" startWordPosition="440" endWordPosition="443">is the work of Ando and Zhang (2005), where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand. In essence, Ando and Zhang (2005) transform the semi-supervised learning problem to a multi-task learning problem; in multi-task learning, a (usually large) set of supervised tasks is available for training, and the goal is to build models which can simultaneously do well on all of them (Caruana, 1997; Ben-David and Schuller, 2003; Evgeniou and Pontil, 2004). Little work, however, has been devoted to study the situation where lots of labeled examples, of one kind, are used to build a model which is tested on unlabeled data of a “different” kind. This problem, which is the topic of this paper, cannot be cast as a multi-task learning problem (since there are labeled examples of only one kind), neither can be cast as a semi-supervised problem (since there are no training labels for the test task). Note that we are interested in the case where the hidden test labels may have no semantic relationship with the training label</context>
</contexts>
<marker>Ben-David, Schuller, 2003</marker>
<rawString>S. Ben-David and R. Schuller. 2003. Exploiting task relatedness for multiple task learning. In Proc. of the Sixteenth Annual Conference on Learning Theory (COLT-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Computational Learning Theory (COLT-98),</booktitle>
<pages>92--100</pages>
<contexts>
<context position="2038" citStr="Blum and Mitchell, 1998" startWordPosition="302" endWordPosition="305"> on the test set. Experiments from the “20 Newsgroups” corpus show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. *This work was partially supported by the DARPA GALE program (Contract No¯ HR0011-06-2-0001) and by the JHU WSE/APL Partnership Fund. 1 Introduction The problem of combining labeled and unlabeled examples in a learning task (semi-supervised learning) has been studied in the literature under various guises. A variety of algorithms (e.g., bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), alternating structure optimization (Ando and Zhang, 2005), etc.) have been developed in order to improve the performance of supervised algorithms, by automatically extracting knowledge from lots of unlabeled examples. Of special interest is the work of Ando and Zhang (2005), where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand. In essence, Ando and Zhang (2005) transform the semi-supervised </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Workshop on Computational Learning Theory (COLT-98), pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning,</booktitle>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="2876" citStr="Caruana, 1997" startWordPosition="438" endWordPosition="439">ecial interest is the work of Ando and Zhang (2005), where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand. In essence, Ando and Zhang (2005) transform the semi-supervised learning problem to a multi-task learning problem; in multi-task learning, a (usually large) set of supervised tasks is available for training, and the goal is to build models which can simultaneously do well on all of them (Caruana, 1997; Ben-David and Schuller, 2003; Evgeniou and Pontil, 2004). Little work, however, has been devoted to study the situation where lots of labeled examples, of one kind, are used to build a model which is tested on unlabeled data of a “different” kind. This problem, which is the topic of this paper, cannot be cast as a multi-task learning problem (since there are labeled examples of only one kind), neither can be cast as a semi-supervised problem (since there are no training labels for the test task). Note that we are interested in the case where the hidden test labels may have no semantic relati</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>R. Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="9424" citStr="Cover and Thomas, 1991" startWordPosition="1520" endWordPosition="1523">irical distribution PX(i) on the vocabulary X. Given a subset (cluster) C of documents, the conditional distribution on X, given the cluster, is just the centroid: PX|C = |c |EX(i)∈C PX(i). If a subcollection 5 ⊂ A of documents is partitioned into clusters C1, ... , Cm, and each document X(i) ∈ 5 is assigned to a cluster CZ(i), where Z(i) ∈ {1,... , m} is the cluster index, then the mutual information between words and corresponding clusters is given by � I(X; Z|5) = z∈{1,...,m} where P(z|5) °_ |Cz|/|5 |is the “prior” distribution on the clusters and D(·k·) is the Kullback-Leibler divergence (Cover and Thomas, 1991). 2.1 The Information Bottleneck Method The Information Bottleneck (IB) method (Tishby et al., 1999; Slonim and Tishby, 2000; Slonim et al., 2002) is one popular approach to unsupervised categorization. The goal of the IB (with “hard” clustering) is to find clusters such that the mutual information I(X; Z) between words and clusters is as large as possible, under a constraint on the number of clusters. The procedure for finding the maximizing clustering in (Slonim and Tishby, 2000) is agglomerative clustering, while in (Slonim et al., 2002) it is based on many random clusterings, combined with</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T. Cover and J. Thomas. 1991. Elements of Information Theory. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Csisz´ar</author>
</authors>
<title>Generalized cutoff rates and R´enyi’s information measures.</title>
<date>1995</date>
<journal>IEEE Trans. on Information Theory,</journal>
<volume>41</volume>
<issue>1</issue>
<marker>Csisz´ar, 1995</marker>
<rawString>I. Csisz´ar. 1995. Generalized cutoff rates and R´enyi’s information measures. IEEE Trans. on Information Theory, 41(1):26–34, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dhillon</author>
<author>S Mallela</author>
<author>R Kumar</author>
</authors>
<title>A divisive information-theoretic feature clustering algorithm for text classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research (JMLR), Special Issue on Variable and Feature Selection,</journal>
<pages>1265--1287</pages>
<contexts>
<context position="7873" citStr="Dhillon et al., 2003" startWordPosition="1258" endWordPosition="1261">ion is the task of deciding whether a piece of text belongs to any of a set of prespecified categories. It is a generic text processing task useful in indexing documents for later retrieval, as a stage in natural language processing systems, for content analysis, and in many other roles (Lewis and Hayes, 1994). Here, we deal with the unsupervised version of document categorization, in which we are interested in clustering together documents which (hopefully) belong to the same topic, without having any training examples.1 Supervised information-theoretic clustering approaches (Torkkola, 2002; Dhillon et al., 2003) have been shown to be very effective, even with a small amount of labeled data, while unsupervised methods (which are of particular interest to us) have been shown to be competitive, matching the classification accuracy of supervised methods. Our focus in this paper is on document categorization algorithms which use information-theoretic 1By this, we mean that training examples having the same category labels as the test examples are not available. 253 criteria, since there are natural ways of generalizing these criteria through the introduction of tunable parameters. We use two such algorith</context>
</contexts>
<marker>Dhillon, Mallela, Kumar, 2003</marker>
<rawString>I. Dhillon, S. Mallela, and R. Kumar. 2003. A divisive information-theoretic feature clustering algorithm for text classification. Journal of Machine Learning Research (JMLR), Special Issue on Variable and Feature Selection, pages 1265–1287, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>D Karakos</author>
</authors>
<title>Bootstrapping without the boot.</title>
<date>2005</date>
<booktitle>In Proc. 2005 Conference on Human Language Technology /Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<contexts>
<context position="1275" citStr="Eisner and Karakos, 2005" startWordPosition="186" endWordPosition="189">then be necessary in order to perform well on new test collections. In this paper, we show how to perform such a tuning in the context of unsupervised document clustering, by (i) introducing a degree of freedom, α, into two leading informationtheoretic clustering algorithms, through the use of generalized mutual information quantities; and (ii) selecting the value of α based on clusterings of similar, but supervised document collections (crossinstance tuning). One option is to perform a tuning that directly minimizes the error on the supervised data sets; another option is to use “strapping” (Eisner and Karakos, 2005), which builds a classifier that learns to distinguish good from bad clusterings, and then selects the α with the best predicted clustering on the test set. Experiments from the “20 Newsgroups” corpus show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. *This work was partially supported by the DARPA GALE program (Contract No¯ HR0011-06-2-0001) and by the JHU WSE/APL Partnership Fund. 1 Introduction The problem of combining labeled and unlabeled examples in a learning task (semi-supervised lear</context>
<context position="5747" citStr="Eisner and Karakos, 2005" startWordPosition="918" endWordPosition="921">ddle-East-related” are two distinct, unrelated instances. In more mathematical terms, if task instances T1, T2 take sets of observations X1, X2 as input, and try to predict labels from sets S1, S2, respectively, then they are called unrelated if X1 n X2 = o and S1 n S2 = o. The focus of this paper is to study the problem of cross-instance tuning of unsupervised algorithms: how one can tune an algorithm, which is used to solve a particular task instance, using knowledge from an unrelated task instance. To the best of our knowledge, this cross-instance learning problem has only been tackled in (Eisner and Karakos, 2005), whose “strapping” procedure learns a meta-classifier for distinguishing good from bad clusterings. In this paper, we introduce a scalar parameter α (a new degree of freedom) into two basic unsupervised clustering algorithms. We can tune α to maximize unsupervised clustering performance on different task instances where the correct clustering is known. The hope is that tuning the parameter learns something about the task in general, which transfers from the supervised task instances to the unsupervised one. Alternatively, we can tune a metaclassifier so as to select good values of α on the su</context>
<context position="19343" citStr="Eisner and Karakos, 2005" startWordPosition="3166" endWordPosition="3169"> whitespace. Moreover, as suggested in (Yang and Pedersen, 1997) as an effective method for reducing the dimensionality of the feature space (number of distinct words), all words which occur fewer than t times in the corpus are removed. For the sIB experiments, we use t = 2 (as was done in (Slonim et al., 2002)), while for the IDT experiments we use t = 3; these choices result in the best performance for each method, respectively, on another dataset. 4.1 Selecting α with “Strapping” In order to pick the value of the parameter α for each of the sIB and IDT test experiments, we use “strapping” (Eisner and Karakos, 2005), which, as we mentioned earlier, is a technique for training a meta-classifier that chooses among possible clusterings. The training is based on unrelated instances of the same clustering task. The final choice of clustering is still unsupervised, since no labels (or ground truth, in general) for the instance of interest are used. Here, our collection of possible clusterings for each test collection is generated by varying the α parameter. Strapping does not care, however, how the collection was generated. (In the original strapping paper, for example, Eisner and Karakos (2005) generated thei</context>
</contexts>
<marker>Eisner, Karakos, 2005</marker>
<rawString>J. Eisner and D. Karakos. 2005. Bootstrapping without the boot. In Proc. 2005 Conference on Human Language Technology /Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Evgeniou</author>
<author>M Pontil</author>
</authors>
<title>Regularized multi-task learning.</title>
<date>2004</date>
<booktitle>In Proc. Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="2934" citStr="Evgeniou and Pontil, 2004" startWordPosition="444" endWordPosition="448">(2005), where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand. In essence, Ando and Zhang (2005) transform the semi-supervised learning problem to a multi-task learning problem; in multi-task learning, a (usually large) set of supervised tasks is available for training, and the goal is to build models which can simultaneously do well on all of them (Caruana, 1997; Ben-David and Schuller, 2003; Evgeniou and Pontil, 2004). Little work, however, has been devoted to study the situation where lots of labeled examples, of one kind, are used to build a model which is tested on unlabeled data of a “different” kind. This problem, which is the topic of this paper, cannot be cast as a multi-task learning problem (since there are labeled examples of only one kind), neither can be cast as a semi-supervised problem (since there are no training labels for the test task). Note that we are interested in the case where the hidden test labels may have no semantic relationship with the training labels; in 252 Proceedings of NAA</context>
</contexts>
<marker>Evgeniou, Pontil, 2004</marker>
<rawString>T. Evgeniou and M. Pontil. 2004. Regularized multi-task learning. In Proc. Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fraley</author>
<author>A E Raftery</author>
</authors>
<title>Mclust: Software for model-based cluster analysis.</title>
<date>1999</date>
<journal>Journal on Classification,</journal>
<pages>16--297</pages>
<contexts>
<context position="17384" citStr="Fraley and Raftery, 1999" startWordPosition="2834" endWordPosition="2837">collection. Three independent test collections of each kind (Binary, Multi5 and Multi10) were created, for a total of 9 collections. The sIB method was used to separately cluster each collection, given the correct number of clusters. A comparison of sIB and IDTs on the same 9 test collections was reported in (Karakos et al., 2007b; Karakos et al., 2007a). Matlab code from (Slonim, 2003) was used for the sIB experiments, while the parameterized mutual information measures of Section 3 were used for the IDTs. A comparison was also made with the EM-based Gaussian mixtures clustering tool mclust (Fraley and Raftery, 1999), and with a simple K-means algorithm. Since the two latter techniques gave uniformly worse clusterings than those of sIB and IDTs, we omit them from the following discussion. To show that our methods work beyond the 9 particular 500-document collections described above, in this paper we instead use five different randomly sampled test collections for each of the Binary, Multi5 and Multi10 cases, making for a total of 15 new test collections in this paper. For diversity, we ensure that none of the five test collections (in each case) contain any documents used in the three collections of (Slon</context>
</contexts>
<marker>Fraley, Raftery, 1999</marker>
<rawString>C. Fraley and A. E. Raftery. 1999. Mclust: Software for model-based cluster analysis. Journal on Classification, 16:297–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gatford</author>
<author>M M Hancock-Beaulieu</author>
<author>S Jones</author>
<author>S Walker</author>
<author>S E Robertson</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1995</date>
<booktitle>In The Third Text Retrieval Conference (TREC-3),</booktitle>
<pages>109--126</pages>
<contexts>
<context position="18324" citStr="Gatford et al., 1995" startWordPosition="2990" endWordPosition="2993">domly sampled test collections for each of the Binary, Multi5 and Multi10 cases, making for a total of 15 new test collections in this paper. For diversity, we ensure that none of the five test collections (in each case) contain any documents used in the three collections of (Slonim et al., 2002) (for the same case). We pre-process the documents of each test collection using the procedure2 mentioned in (Karakos et al., 2007b). The 15 test collections are then converted to feature matrices—term-document frequency matrices for sIB, and discounted tf/idf matrices (according to the Okapi formula (Gatford et al., 1995)) for IDTs—with each row of a matrix representing one document in that test collection. 2Excluding the subject line, the header of each abstract is removed. Stop-words such as a, the, is, etc. are removed, and stemming is performed (e.g., common suffixes such as -ing, - er, -ed, etc., are removed). Also, all numbers are collapsed to one symbol, and non-alphanumeric sequences are converted to whitespace. Moreover, as suggested in (Yang and Pedersen, 1997) as an effective method for reducing the dimensionality of the feature space (number of distinct words), all words which occur fewer than t ti</context>
</contexts>
<marker>Gatford, Hancock-Beaulieu, Jones, Walker, Robertson, 1995</marker>
<rawString>M. Gatford, M. M. Hancock-Beaulieu, S. Jones, S. Walker, and S. E. Robertson. 1995. Okapi at TREC-3. In The Third Text Retrieval Conference (TREC-3), pages 109–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ben Hamza</author>
<author>H Krim</author>
</authors>
<title>Jensen-R´enyi divergence measure: Theoretical and computational perspectives.</title>
<date>2003</date>
<booktitle>In Proc. IEEE Int. Symp. on Information Theory,</booktitle>
<location>Yokohama, Japan,</location>
<contexts>
<context position="12476" citStr="Hamza and Krim, 2003" startWordPosition="1995" endWordPosition="1998">mentation application. The objective at each node t was to maximize the mutual information between spectral components and clusters given the pixels at node t, computed from the projected empirical distributions. At each step of the tree-growing procedure, the node which yielded the highest increase in the average, per-node mutual information, was selected for splitting (until a desired number of leaves was reached). In (Karakos et al., 2007b), the mutual information objective was replaced with a parameterized form of mutual information, namely the Jensen-R´enyi divergence (Hero et al., 2001; Hamza and Krim, 2003), of which more details are provided in the next section. 3 Parameterizing Unsupervised Clustering As mentioned above, the algorithms considered in this paper (sIB and IDTs) are unsupervised, in the PX|S), P(z|5)D( �PX|Czk SD( 254 sense that they can be applied to test data without any need for tuning. Our procedure of adapting them, based on some supervision on a different task instance, is by introducing a parameter into the unsupervised algorithm. At least for simple crossinstance tuning, this parameter represents the information which is passed between the supervised and the unsupervised i</context>
<context position="13946" citStr="Hamza and Krim, 2003" startWordPosition="2232" endWordPosition="2235"> as sIB with the parameterized mutual information measures mentioned above. These two quantities provide estimates of the dependence between the random quantities in their arguments, just as the usual mutual information does, but also have a scalar parameter α ∈ (0, 1] that controls the sensitivity of the computed dependence on the details of the joint distribution of X and Z. As a result, the effect of data sparseness on estimation of the joint distribution can be mitigated when computing these measures. 3.1 Jensen-Renyi Divergence The Jensen-R´enyi divergence was used in (Hero et al., 2001; Hamza and Krim, 2003) as a measure of similarity for image classification and retrieval. For two discrete random variables X, Z with distributions PX, PZ and conditional PX|Z, it is defined as Iα(X; Z) = Hα(PX) − X PZ(z)Hα(PX|Z(·|z)), Z (1) where Hα(·) is the R´enyi entropy, given by 1 X !P(x)α , α =6 1. (2) Hα(P) = log 1 − α xEX If α ∈ (0, 1), Hα is a concave function, and hence Iα(X; Z) is non-negative (and it is equal to zero if and only if X and Z are independent). In the limit as α → 1, Hα(·) approaches the Shannon entropy (not an obvious fact), so Iα(·) reduces to the regular mutual information. Similarly, w</context>
</contexts>
<marker>Hamza, Krim, 2003</marker>
<rawString>A. Ben Hamza and H. Krim. 2003. Jensen-R´enyi divergence measure: Theoretical and computational perspectives. In Proc. IEEE Int. Symp. on Information Theory, Yokohama, Japan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J Friedman</author>
</authors>
<title>The Elements of Statistical Learning.</title>
<date>2001</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="27466" citStr="Hastie et al., 2001" startWordPosition="4535" endWordPosition="4538">es may be associated to each newsgroup. To do this, we iterate through all pairs (X, G) where X is a test example and G is a training newsgroup, in increasing order by the angle between X and G. If X is not yet associated and G is not yet “full,” then we associate X with G, and choose X&apos; to be the document in G with the smallest angle to X. We cluster A&apos; 10 times, for α = 0.1, ... ,1.0, and we collect supervised error results £(α), α E {0.1, ... ,1.0}. Now, instead of using the single best α* = argminα £(α) to cluster A (which may result in overfitting) we use regularized least-squares (RLS) (Hastie et al., 2001), where we try to approximate the probability that an α is the best. The estimated probabilities are given by p� = K(λI + K)−1p, where I is the unit matrix, p is the training probability of the best α (i.e., it is 1 at the position of α* and zero elsewhere), and K is the kernel matrix, where K(i, j) = exp(−(£(αi) − £(αj))2/σ2) is the value of the kernel which expresses the “similarity” between two clusterings of the same training dataset, in terms of their errors. The parameters σ, γ are set to 0.5, 0.1, respectively, after performing a (local) maximization of the Spearman correlation between </context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>T. Hastie, R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical Learning. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A O Hero</author>
<author>B Ma</author>
<author>O Michel</author>
<author>J Gorman</author>
</authors>
<title>Alpha-divergence for classification, indexing and retrieval.</title>
<date>2001</date>
<tech>Technical Report CSPL-328,</tech>
<institution>University of Michigan Ann Arbor, Communications and Signal Processing Laboratory,</institution>
<contexts>
<context position="12453" citStr="Hero et al., 2001" startWordPosition="1991" endWordPosition="1994">rspectral image segmentation application. The objective at each node t was to maximize the mutual information between spectral components and clusters given the pixels at node t, computed from the projected empirical distributions. At each step of the tree-growing procedure, the node which yielded the highest increase in the average, per-node mutual information, was selected for splitting (until a desired number of leaves was reached). In (Karakos et al., 2007b), the mutual information objective was replaced with a parameterized form of mutual information, namely the Jensen-R´enyi divergence (Hero et al., 2001; Hamza and Krim, 2003), of which more details are provided in the next section. 3 Parameterizing Unsupervised Clustering As mentioned above, the algorithms considered in this paper (sIB and IDTs) are unsupervised, in the PX|S), P(z|5)D( �PX|Czk SD( 254 sense that they can be applied to test data without any need for tuning. Our procedure of adapting them, based on some supervision on a different task instance, is by introducing a parameter into the unsupervised algorithm. At least for simple crossinstance tuning, this parameter represents the information which is passed between the supervised</context>
<context position="13923" citStr="Hero et al., 2001" startWordPosition="2228" endWordPosition="2231">ies in IDTs as well as sIB with the parameterized mutual information measures mentioned above. These two quantities provide estimates of the dependence between the random quantities in their arguments, just as the usual mutual information does, but also have a scalar parameter α ∈ (0, 1] that controls the sensitivity of the computed dependence on the details of the joint distribution of X and Z. As a result, the effect of data sparseness on estimation of the joint distribution can be mitigated when computing these measures. 3.1 Jensen-Renyi Divergence The Jensen-R´enyi divergence was used in (Hero et al., 2001; Hamza and Krim, 2003) as a measure of similarity for image classification and retrieval. For two discrete random variables X, Z with distributions PX, PZ and conditional PX|Z, it is defined as Iα(X; Z) = Hα(PX) − X PZ(z)Hα(PX|Z(·|z)), Z (1) where Hα(·) is the R´enyi entropy, given by 1 X !P(x)α , α =6 1. (2) Hα(P) = log 1 − α xEX If α ∈ (0, 1), Hα is a concave function, and hence Iα(X; Z) is non-negative (and it is equal to zero if and only if X and Z are independent). In the limit as α → 1, Hα(·) approaches the Shannon entropy (not an obvious fact), so Iα(·) reduces to the regular mutual in</context>
</contexts>
<marker>Hero, Ma, Michel, Gorman, 2001</marker>
<rawString>A. O. Hero, B. Ma, O. Michel, and J. Gorman. 2001. Alpha-divergence for classification, indexing and retrieval. Technical Report CSPL-328, University of Michigan Ann Arbor, Communications and Signal Processing Laboratory, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10613" citStr="Jelinek, 1997" startWordPosition="1706" endWordPosition="1707">lusterings, combined with a sequential update algorithm, similar to Kmeans. The update algorithm re-assigns each data point (document) d to its most “similar” cluster C, in order to minimize I(X; Z|C ∪ {d}), i.e., �PX|{d}k�PX|{d}∪C)+(1−S)D( PX|Ck �PX|{d}∪C), where S = 1 |C|�1. This latter procedure is called the sequential Information Bottleneck (sIB) method, and is considered the state-of-the-art in unsupervised document categorization. 2.2 Iterative Denoising Trees Decision trees are a powerful technique for equivalence classification, accomplished through a recursive successive refinement (Jelinek, 1997). In the context of unsupervised classification, the goal of decision trees is to cluster empirical distributions (bags of words) into a given number of classes, with each class corresponding to a leaf in the tree. They are built top-down (as opposed to the bottom-up construction in IB) using maximization of mutual information between words and clusters I(X; Z|t) to drive the splitting of each node t; the hope is that each leaf will contain data points which belong to only one latent category. Iterative Denoising Trees (also called Integrated Sensing and Processing Decision Trees) were introdu</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>F. Jelinek. 1997. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In ACM Conf. on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="22546" citStr="Joachims, 2002" startWordPosition="3722" endWordPosition="3723">or the α used in this clustering, the feature value is equal to e−0-1r, where r is the average rank of the clustering (i.e., the average of the 4 ranks resulting from sorting all 10 clusterings (per training example) according to one of the 4 features in (i) and (ii)). For all other α’s, the feature is set to zero. Thus, only α’s which yield relatively good rankings can have non-zero features in the model. • We normalize each group of 10 feature vectors, translating and scaling each of the 14 dimensions to make it range from 0 to 1. (We will do the same at test time.) • We train ranking SVMs (Joachims, 2002), with a Gaussian kernel, to learn how to rank these 50 clusterings given their respective normalized feature vectors. The values of c, -y (which control regularization and the Gaussian kernel) were optimized through leave-one-out cross validation in order to maximize the average accuracy of the top-ranked clustering, over the 5 training sets. Once a local maximum of the average accuracy was obtained, further tuning of c, -y to maximize the Spearman rank correlation between the predicted and true ranks was performed. A model trained in this way knows something about the task, and may work well</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing search engines using clickthrough data. In ACM Conf. on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Karakos</author>
<author>S Khudanpur</author>
<author>J Eisner</author>
<author>C E Priebe</author>
</authors>
<title>Unsupervised classification via decision trees: An information-theoretic perspective.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<booktitle>In Proc. 2005 International Conference on Acoustics, Speech and Signal Processing (ICASSP</booktitle>
<contexts>
<context position="11795" citStr="Karakos et al., 2005" startWordPosition="1888" endWordPosition="1891">Processing Decision Trees) were introduced in (Priebe et al., 2004a), as an extension of regular decision trees. Their main feature is that they transform the data at each node, before splitting, by projecting into a low-dimensional space. This transformation corresponds to feature extraction; different features are suppressed (or amplified) by each transformation, depending on what data points fall into each node (corpus-dependentfeature-extraction property (Priebe et al., 2004b)). Thus, dimensionality reduction and clustering are chosen so that they jointly optimize the local objective. In (Karakos et al., 2005), IDTs were used for an unsupervised hyperspectral image segmentation application. The objective at each node t was to maximize the mutual information between spectral components and clusters given the pixels at node t, computed from the projected empirical distributions. At each step of the tree-growing procedure, the node which yielded the highest increase in the average, per-node mutual information, was selected for splitting (until a desired number of leaves was reached). In (Karakos et al., 2007b), the mutual information objective was replaced with a parameterized form of mutual informati</context>
</contexts>
<marker>Karakos, Khudanpur, Eisner, Priebe, 2005</marker>
<rawString>D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe. 2005. Unsupervised classification via decision trees: An information-theoretic perspective. In Proc. 2005 International Conference on Acoustics, Speech and Signal Processing (ICASSP 2005), March. D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe. 2007a. Information-theoretic aspects of iterative denoising. Submitted to the Journal of Machine Learning Research, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Karakos</author>
<author>S Khudanpur</author>
<author>J Eisner</author>
<author>C E Priebe</author>
</authors>
<title>Iterative denoising using Jensen-R´enyi divergences with an application to unsupervised document categorization.</title>
<date>2007</date>
<booktitle>In Proc. 2007 International Conference on Acoustics, Speech and Signal Processing (ICASSP</booktitle>
<contexts>
<context position="12300" citStr="Karakos et al., 2007" startWordPosition="1969" endWordPosition="1972">ty reduction and clustering are chosen so that they jointly optimize the local objective. In (Karakos et al., 2005), IDTs were used for an unsupervised hyperspectral image segmentation application. The objective at each node t was to maximize the mutual information between spectral components and clusters given the pixels at node t, computed from the projected empirical distributions. At each step of the tree-growing procedure, the node which yielded the highest increase in the average, per-node mutual information, was selected for splitting (until a desired number of leaves was reached). In (Karakos et al., 2007b), the mutual information objective was replaced with a parameterized form of mutual information, namely the Jensen-R´enyi divergence (Hero et al., 2001; Hamza and Krim, 2003), of which more details are provided in the next section. 3 Parameterizing Unsupervised Clustering As mentioned above, the algorithms considered in this paper (sIB and IDTs) are unsupervised, in the PX|S), P(z|5)D( �PX|Czk SD( 254 sense that they can be applied to test data without any need for tuning. Our procedure of adapting them, based on some supervision on a different task instance, is by introducing a parameter in</context>
<context position="16141" citStr="Karakos et al., 2007" startWordPosition="2628" endWordPosition="2631">Dα(PX|Z(·|z)kQ(·)), (3) where Dα(·k·) is the R´enyi divergence (Csisz´ar, 1995). It was shown that Ia (X; Z) retains most of the properties of I(X; Z)—it is a non-negative, continuous, and concave function of PX, it is convex in PX|Z for α &lt; 1, and converges to I(X; Z) as α → 1. Notably, Ia (X; Z) ≤ I(X; Z) for 0 &lt; α &lt; 1; this means, as above, that α regulates the overestimation of mutual information that may result from data sparseness. There is no analytic form for the minimizer of the right-hand-side of (3) (Csisz´ar, 1995), but it may be computed via an alternating minimization algorithm (Karakos et al., 2007a). 4 Experimental Methods and Results We demonstrate the feasibility of cross-instance tuning with experiments on unsupervised document categorization from the 20 Newsgroups corpus (Lang, 1995); this corpus consists of roughly 20,000 news articles, evenly divided among 20 Usenet groups. Random samples of 500 articles each were chosen by (Slonim et al., 2002) to create multiple test collections: 250 each from 2 arbitrarily chosen Usenet X Ia (X; Z) = min Q Z 255 groups for the Binary test collection, 100 articles each from 5 groups for the Multi5 test collection, and 50 each from 10 groups for</context>
<context position="18130" citStr="Karakos et al., 2007" startWordPosition="2961" endWordPosition="2964">IDTs, we omit them from the following discussion. To show that our methods work beyond the 9 particular 500-document collections described above, in this paper we instead use five different randomly sampled test collections for each of the Binary, Multi5 and Multi10 cases, making for a total of 15 new test collections in this paper. For diversity, we ensure that none of the five test collections (in each case) contain any documents used in the three collections of (Slonim et al., 2002) (for the same case). We pre-process the documents of each test collection using the procedure2 mentioned in (Karakos et al., 2007b). The 15 test collections are then converted to feature matrices—term-document frequency matrices for sIB, and discounted tf/idf matrices (according to the Okapi formula (Gatford et al., 1995)) for IDTs—with each row of a matrix representing one document in that test collection. 2Excluding the subject line, the header of each abstract is removed. Stop-words such as a, the, is, etc. are removed, and stemming is performed (e.g., common suffixes such as -ing, - er, -ed, etc., are removed). Also, all numbers are collapsed to one symbol, and non-alphanumeric sequences are converted to whitespace.</context>
</contexts>
<marker>Karakos, Khudanpur, Eisner, Priebe, 2007</marker>
<rawString>D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe. 2007b. Iterative denoising using Jensen-R´enyi divergences with an application to unsupervised document categorization. In Proc. 2007 International Conference on Acoustics, Speech and Signal Processing (ICASSP 2007), April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lang</author>
</authors>
<title>Learning to filter netnews.</title>
<date>1995</date>
<booktitle>In Proc. 13th Int. Conf. on Machine Learning,</booktitle>
<pages>331--339</pages>
<contexts>
<context position="16335" citStr="Lang, 1995" startWordPosition="2657" endWordPosition="2658">on of PX, it is convex in PX|Z for α &lt; 1, and converges to I(X; Z) as α → 1. Notably, Ia (X; Z) ≤ I(X; Z) for 0 &lt; α &lt; 1; this means, as above, that α regulates the overestimation of mutual information that may result from data sparseness. There is no analytic form for the minimizer of the right-hand-side of (3) (Csisz´ar, 1995), but it may be computed via an alternating minimization algorithm (Karakos et al., 2007a). 4 Experimental Methods and Results We demonstrate the feasibility of cross-instance tuning with experiments on unsupervised document categorization from the 20 Newsgroups corpus (Lang, 1995); this corpus consists of roughly 20,000 news articles, evenly divided among 20 Usenet groups. Random samples of 500 articles each were chosen by (Slonim et al., 2002) to create multiple test collections: 250 each from 2 arbitrarily chosen Usenet X Ia (X; Z) = min Q Z 255 groups for the Binary test collection, 100 articles each from 5 groups for the Multi5 test collection, and 50 each from 10 groups for the Multi10 test collection. Three independent test collections of each kind (Binary, Multi5 and Multi10) were created, for a total of 9 collections. The sIB method was used to separately clust</context>
</contexts>
<marker>Lang, 1995</marker>
<rawString>K. Lang. 1995. Learning to filter netnews. In Proc. 13th Int. Conf. on Machine Learning, pages 331–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Philip J Hayes</author>
</authors>
<date>1994</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>12</volume>
<issue>3</issue>
<editor>Guest editorial.</editor>
<contexts>
<context position="7563" citStr="Lewis and Hayes, 1994" startWordPosition="1214" endWordPosition="1217">t time, picks a parameter based on various “goodness” cues that were learned from the labeled data set, and (ii) learning the parameter from a supervised data set which is chosen to statistically match the test set. Finally, concluding remarks appear in Section 5. 2 Document Categorization Document categorization is the task of deciding whether a piece of text belongs to any of a set of prespecified categories. It is a generic text processing task useful in indexing documents for later retrieval, as a stage in natural language processing systems, for content analysis, and in many other roles (Lewis and Hayes, 1994). Here, we deal with the unsupervised version of document categorization, in which we are interested in clustering together documents which (hopefully) belong to the same topic, without having any training examples.1 Supervised information-theoretic clustering approaches (Torkkola, 2002; Dhillon et al., 2003) have been shown to be very effective, even with a small amount of labeled data, while unsupervised methods (which are of particular interest to us) have been shown to be competitive, matching the classification accuracy of supervised methods. Our focus in this paper is on document categor</context>
</contexts>
<marker>Lewis, Hayes, 1994</marker>
<rawString>David D. Lewis and Philip J. Hayes. 1994. Guest editorial. ACM Transactions on Information Systems, 12(3):231, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Priebe</author>
<author>D J Marchette</author>
<author>D M Healy</author>
</authors>
<title>Integrated sensing and processing decision trees.</title>
<date>2004</date>
<journal>IEEE Trans. on Pat. Anal. and Mach. Intel.,</journal>
<volume>26</volume>
<issue>6</issue>
<contexts>
<context position="11240" citStr="Priebe et al., 2004" startWordPosition="1806" endWordPosition="1809"> context of unsupervised classification, the goal of decision trees is to cluster empirical distributions (bags of words) into a given number of classes, with each class corresponding to a leaf in the tree. They are built top-down (as opposed to the bottom-up construction in IB) using maximization of mutual information between words and clusters I(X; Z|t) to drive the splitting of each node t; the hope is that each leaf will contain data points which belong to only one latent category. Iterative Denoising Trees (also called Integrated Sensing and Processing Decision Trees) were introduced in (Priebe et al., 2004a), as an extension of regular decision trees. Their main feature is that they transform the data at each node, before splitting, by projecting into a low-dimensional space. This transformation corresponds to feature extraction; different features are suppressed (or amplified) by each transformation, depending on what data points fall into each node (corpus-dependentfeature-extraction property (Priebe et al., 2004b)). Thus, dimensionality reduction and clustering are chosen so that they jointly optimize the local objective. In (Karakos et al., 2005), IDTs were used for an unsupervised hyperspe</context>
</contexts>
<marker>Priebe, Marchette, Healy, 2004</marker>
<rawString>C. E. Priebe, D. J. Marchette, and D. M. Healy. 2004a. Integrated sensing and processing decision trees. IEEE Trans. on Pat. Anal. and Mach. Intel., 26(6):699–708, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Priebe</author>
<author>D J Marchette</author>
<author>Y Park</author>
<author>E Wegman</author>
<author>J Solka</author>
<author>D Socolinsky</author>
<author>D Karakos</author>
<author>K Church</author>
<author>R Guglielmi</author>
<author>R Coifman</author>
<author>D Lin</author>
<author>D Healy</author>
<author>M Jacobs</author>
<author>A Tsao</author>
</authors>
<title>Iterative denoising for cross-corpus discovery. In</title>
<date>2004</date>
<booktitle>Proc. 2004 International Symposium on Computational Statistics (COMPSTAT</booktitle>
<contexts>
<context position="11240" citStr="Priebe et al., 2004" startWordPosition="1806" endWordPosition="1809"> context of unsupervised classification, the goal of decision trees is to cluster empirical distributions (bags of words) into a given number of classes, with each class corresponding to a leaf in the tree. They are built top-down (as opposed to the bottom-up construction in IB) using maximization of mutual information between words and clusters I(X; Z|t) to drive the splitting of each node t; the hope is that each leaf will contain data points which belong to only one latent category. Iterative Denoising Trees (also called Integrated Sensing and Processing Decision Trees) were introduced in (Priebe et al., 2004a), as an extension of regular decision trees. Their main feature is that they transform the data at each node, before splitting, by projecting into a low-dimensional space. This transformation corresponds to feature extraction; different features are suppressed (or amplified) by each transformation, depending on what data points fall into each node (corpus-dependentfeature-extraction property (Priebe et al., 2004b)). Thus, dimensionality reduction and clustering are chosen so that they jointly optimize the local objective. In (Karakos et al., 2005), IDTs were used for an unsupervised hyperspe</context>
</contexts>
<marker>Priebe, Marchette, Park, Wegman, Solka, Socolinsky, Karakos, Church, Guglielmi, Coifman, Lin, Healy, Jacobs, Tsao, 2004</marker>
<rawString>C. E. Priebe, D. J. Marchette, Y. Park, E. Wegman, J. Solka, D. Socolinsky, D. Karakos, K. Church, R. Guglielmi, R. Coifman, D. Lin, D. Healy, M. Jacobs, and A. Tsao. 2004b. Iterative denoising for cross-corpus discovery. In Proc. 2004 International Symposium on Computational Statistics (COMPSTAT 2004), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Tishby</author>
</authors>
<title>Document clustering using word clusters via the information bottleneck method.</title>
<date>2000</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>208--215</pages>
<contexts>
<context position="9548" citStr="Slonim and Tishby, 2000" startWordPosition="1538" endWordPosition="1541">iven the cluster, is just the centroid: PX|C = |c |EX(i)∈C PX(i). If a subcollection 5 ⊂ A of documents is partitioned into clusters C1, ... , Cm, and each document X(i) ∈ 5 is assigned to a cluster CZ(i), where Z(i) ∈ {1,... , m} is the cluster index, then the mutual information between words and corresponding clusters is given by � I(X; Z|5) = z∈{1,...,m} where P(z|5) °_ |Cz|/|5 |is the “prior” distribution on the clusters and D(·k·) is the Kullback-Leibler divergence (Cover and Thomas, 1991). 2.1 The Information Bottleneck Method The Information Bottleneck (IB) method (Tishby et al., 1999; Slonim and Tishby, 2000; Slonim et al., 2002) is one popular approach to unsupervised categorization. The goal of the IB (with “hard” clustering) is to find clusters such that the mutual information I(X; Z) between words and clusters is as large as possible, under a constraint on the number of clusters. The procedure for finding the maximizing clustering in (Slonim and Tishby, 2000) is agglomerative clustering, while in (Slonim et al., 2002) it is based on many random clusterings, combined with a sequential update algorithm, similar to Kmeans. The update algorithm re-assigns each data point (document) d to its most </context>
</contexts>
<marker>Slonim, Tishby, 2000</marker>
<rawString>N. Slonim and N. Tishby. 2000. Document clustering using word clusters via the information bottleneck method. In Research and Development in Information Retrieval, pages 208–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Friedman</author>
<author>N Tishby</author>
</authors>
<title>Unsupervised document classification using sequential information maximization.</title>
<date>2002</date>
<booktitle>In Proc. SIGIR’02, 25th ACM Int. Conf. on Research and Development of Inform. Retrieval.</booktitle>
<contexts>
<context position="9570" citStr="Slonim et al., 2002" startWordPosition="1542" endWordPosition="1545"> the centroid: PX|C = |c |EX(i)∈C PX(i). If a subcollection 5 ⊂ A of documents is partitioned into clusters C1, ... , Cm, and each document X(i) ∈ 5 is assigned to a cluster CZ(i), where Z(i) ∈ {1,... , m} is the cluster index, then the mutual information between words and corresponding clusters is given by � I(X; Z|5) = z∈{1,...,m} where P(z|5) °_ |Cz|/|5 |is the “prior” distribution on the clusters and D(·k·) is the Kullback-Leibler divergence (Cover and Thomas, 1991). 2.1 The Information Bottleneck Method The Information Bottleneck (IB) method (Tishby et al., 1999; Slonim and Tishby, 2000; Slonim et al., 2002) is one popular approach to unsupervised categorization. The goal of the IB (with “hard” clustering) is to find clusters such that the mutual information I(X; Z) between words and clusters is as large as possible, under a constraint on the number of clusters. The procedure for finding the maximizing clustering in (Slonim and Tishby, 2000) is agglomerative clustering, while in (Slonim et al., 2002) it is based on many random clusterings, combined with a sequential update algorithm, similar to Kmeans. The update algorithm re-assigns each data point (document) d to its most “similar” cluster C, i</context>
<context position="16502" citStr="Slonim et al., 2002" startWordPosition="2682" endWordPosition="2685">s the overestimation of mutual information that may result from data sparseness. There is no analytic form for the minimizer of the right-hand-side of (3) (Csisz´ar, 1995), but it may be computed via an alternating minimization algorithm (Karakos et al., 2007a). 4 Experimental Methods and Results We demonstrate the feasibility of cross-instance tuning with experiments on unsupervised document categorization from the 20 Newsgroups corpus (Lang, 1995); this corpus consists of roughly 20,000 news articles, evenly divided among 20 Usenet groups. Random samples of 500 articles each were chosen by (Slonim et al., 2002) to create multiple test collections: 250 each from 2 arbitrarily chosen Usenet X Ia (X; Z) = min Q Z 255 groups for the Binary test collection, 100 articles each from 5 groups for the Multi5 test collection, and 50 each from 10 groups for the Multi10 test collection. Three independent test collections of each kind (Binary, Multi5 and Multi10) were created, for a total of 9 collections. The sIB method was used to separately cluster each collection, given the correct number of clusters. A comparison of sIB and IDTs on the same 9 test collections was reported in (Karakos et al., 2007b; Karakos e</context>
<context position="18000" citStr="Slonim et al., 2002" startWordPosition="2939" endWordPosition="2942">999), and with a simple K-means algorithm. Since the two latter techniques gave uniformly worse clusterings than those of sIB and IDTs, we omit them from the following discussion. To show that our methods work beyond the 9 particular 500-document collections described above, in this paper we instead use five different randomly sampled test collections for each of the Binary, Multi5 and Multi10 cases, making for a total of 15 new test collections in this paper. For diversity, we ensure that none of the five test collections (in each case) contain any documents used in the three collections of (Slonim et al., 2002) (for the same case). We pre-process the documents of each test collection using the procedure2 mentioned in (Karakos et al., 2007b). The 15 test collections are then converted to feature matrices—term-document frequency matrices for sIB, and discounted tf/idf matrices (according to the Okapi formula (Gatford et al., 1995)) for IDTs—with each row of a matrix representing one document in that test collection. 2Excluding the subject line, the header of each abstract is removed. Stop-words such as a, the, is, etc. are removed, and stemming is performed (e.g., common suffixes such as -ing, - er, -</context>
</contexts>
<marker>Slonim, Friedman, Tishby, 2002</marker>
<rawString>N. Slonim, N. Friedman, and N. Tishby. 2002. Unsupervised document classification using sequential information maximization. In Proc. SIGIR’02, 25th ACM Int. Conf. on Research and Development of Inform. Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
</authors>
<title>IBA 1.0: Matlab code for information bottleneck clustering algorithms.</title>
<date>2003</date>
<booktitle>Available from http://www.princeton.edu/∼nslonim/IB Release1.0/ IB Release1 0.tar.</booktitle>
<contexts>
<context position="17148" citStr="Slonim, 2003" startWordPosition="2798" endWordPosition="2799">ons: 250 each from 2 arbitrarily chosen Usenet X Ia (X; Z) = min Q Z 255 groups for the Binary test collection, 100 articles each from 5 groups for the Multi5 test collection, and 50 each from 10 groups for the Multi10 test collection. Three independent test collections of each kind (Binary, Multi5 and Multi10) were created, for a total of 9 collections. The sIB method was used to separately cluster each collection, given the correct number of clusters. A comparison of sIB and IDTs on the same 9 test collections was reported in (Karakos et al., 2007b; Karakos et al., 2007a). Matlab code from (Slonim, 2003) was used for the sIB experiments, while the parameterized mutual information measures of Section 3 were used for the IDTs. A comparison was also made with the EM-based Gaussian mixtures clustering tool mclust (Fraley and Raftery, 1999), and with a simple K-means algorithm. Since the two latter techniques gave uniformly worse clusterings than those of sIB and IDTs, we omit them from the following discussion. To show that our methods work beyond the 9 particular 500-document collections described above, in this paper we instead use five different randomly sampled test collections for each of th</context>
</contexts>
<marker>Slonim, 2003</marker>
<rawString>N. Slonim. 2003. IBA 1.0: Matlab code for information bottleneck clustering algorithms. Available from http://www.princeton.edu/∼nslonim/IB Release1.0/ IB Release1 0.tar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tishby</author>
<author>F Pereira</author>
<author>W Bialek</author>
</authors>
<title>The information bottleneck method.</title>
<date>1999</date>
<booktitle>In 37th Allerton Conference on Communication and Computation.</booktitle>
<contexts>
<context position="9523" citStr="Tishby et al., 1999" startWordPosition="1534" endWordPosition="1537"> distribution on X, given the cluster, is just the centroid: PX|C = |c |EX(i)∈C PX(i). If a subcollection 5 ⊂ A of documents is partitioned into clusters C1, ... , Cm, and each document X(i) ∈ 5 is assigned to a cluster CZ(i), where Z(i) ∈ {1,... , m} is the cluster index, then the mutual information between words and corresponding clusters is given by � I(X; Z|5) = z∈{1,...,m} where P(z|5) °_ |Cz|/|5 |is the “prior” distribution on the clusters and D(·k·) is the Kullback-Leibler divergence (Cover and Thomas, 1991). 2.1 The Information Bottleneck Method The Information Bottleneck (IB) method (Tishby et al., 1999; Slonim and Tishby, 2000; Slonim et al., 2002) is one popular approach to unsupervised categorization. The goal of the IB (with “hard” clustering) is to find clusters such that the mutual information I(X; Z) between words and clusters is as large as possible, under a constraint on the number of clusters. The procedure for finding the maximizing clustering in (Slonim and Tishby, 2000) is agglomerative clustering, while in (Slonim et al., 2002) it is based on many random clusterings, combined with a sequential update algorithm, similar to Kmeans. The update algorithm re-assigns each data point </context>
</contexts>
<marker>Tishby, Pereira, Bialek, 1999</marker>
<rawString>N. Tishby, F. Pereira, and W. Bialek. 1999. The information bottleneck method. In 37th Allerton Conference on Communication and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torkkola</author>
</authors>
<title>On feature extraction by mutual information maximization.</title>
<date>2002</date>
<booktitle>In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP-2002),</booktitle>
<contexts>
<context position="7850" citStr="Torkkola, 2002" startWordPosition="1256" endWordPosition="1257">ment categorization is the task of deciding whether a piece of text belongs to any of a set of prespecified categories. It is a generic text processing task useful in indexing documents for later retrieval, as a stage in natural language processing systems, for content analysis, and in many other roles (Lewis and Hayes, 1994). Here, we deal with the unsupervised version of document categorization, in which we are interested in clustering together documents which (hopefully) belong to the same topic, without having any training examples.1 Supervised information-theoretic clustering approaches (Torkkola, 2002; Dhillon et al., 2003) have been shown to be very effective, even with a small amount of labeled data, while unsupervised methods (which are of particular interest to us) have been shown to be competitive, matching the classification accuracy of supervised methods. Our focus in this paper is on document categorization algorithms which use information-theoretic 1By this, we mean that training examples having the same category labels as the test examples are not available. 253 criteria, since there are natural ways of generalizing these criteria through the introduction of tunable parameters. W</context>
</contexts>
<marker>Torkkola, 2002</marker>
<rawString>K. Torkkola. 2002. On feature extraction by mutual information maximization. In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP-2002), May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Intl. Conf. on Machine Learning (ICML-97),</booktitle>
<pages>412--420</pages>
<contexts>
<context position="18782" citStr="Yang and Pedersen, 1997" startWordPosition="3064" endWordPosition="3067">are then converted to feature matrices—term-document frequency matrices for sIB, and discounted tf/idf matrices (according to the Okapi formula (Gatford et al., 1995)) for IDTs—with each row of a matrix representing one document in that test collection. 2Excluding the subject line, the header of each abstract is removed. Stop-words such as a, the, is, etc. are removed, and stemming is performed (e.g., common suffixes such as -ing, - er, -ed, etc., are removed). Also, all numbers are collapsed to one symbol, and non-alphanumeric sequences are converted to whitespace. Moreover, as suggested in (Yang and Pedersen, 1997) as an effective method for reducing the dimensionality of the feature space (number of distinct words), all words which occur fewer than t times in the corpus are removed. For the sIB experiments, we use t = 2 (as was done in (Slonim et al., 2002)), while for the IDT experiments we use t = 3; these choices result in the best performance for each method, respectively, on another dataset. 4.1 Selecting α with “Strapping” In order to pick the value of the parameter α for each of the sIB and IDT test experiments, we use “strapping” (Eisner and Karakos, 2005), which, as we mentioned earlier, is a </context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Y. Yang and J. Pedersen. 1997. A comparative study on feature selection in text categorization. In Intl. Conf. on Machine Learning (ICML-97), pages 412–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="1999" citStr="Yarowsky, 1995" startWordPosition="299" endWordPosition="300"> the best predicted clustering on the test set. Experiments from the “20 Newsgroups” corpus show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. *This work was partially supported by the DARPA GALE program (Contract No¯ HR0011-06-2-0001) and by the JHU WSE/APL Partnership Fund. 1 Introduction The problem of combining labeled and unlabeled examples in a learning task (semi-supervised learning) has been studied in the literature under various guises. A variety of algorithms (e.g., bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), alternating structure optimization (Ando and Zhang, 2005), etc.) have been developed in order to improve the performance of supervised algorithms, by automatically extracting knowledge from lots of unlabeled examples. Of special interest is the work of Ando and Zhang (2005), where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand. In essence, Ando and Zhan</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>