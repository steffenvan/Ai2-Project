<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003003">
<title confidence="0.9796785">
Using Word-Sense Disambiguation Methods to Classify Web Queries by
Intent
</title>
<author confidence="0.985468">
Emily Pitler Ken Church
</author>
<affiliation confidence="0.9662495">
Computer and Information Science Johns Hopkins University
University of Pennsylvania Human Language Technology Center of Excellence
</affiliation>
<address confidence="0.744762">
Philadelphia, PA 19104, USA Baltimore, MD 21211
</address>
<email confidence="0.999199">
epitler@seas.upenn.edu Kenneth.Church@jhu.edu
</email>
<sectionHeader confidence="0.993891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999848">
Three methods are proposed to classify
queries by intent (CQI), e.g., navigational,
informational, commercial, etc. Follow-
ing mixed-initiative dialog systems, search
engines should distinguish navigational
queries where the user is taking the ini-
tiative from other queries where there are
more opportunities for system initiatives
(e.g., suggestions, ads). The query in-
tent problem has a number of useful appli-
cations for search engines, affecting how
many (if any) advertisements to display,
which results to return, and how to ar-
range the results page. Click logs are
used as a substitute for annotation. Clicks
on ads are evidence for commercial in-
tent; other types of clicks are evidence for
other intents. We start with a simple Naive
Bayes baseline that works well when there
is plenty of training data. When train-
ing data is less plentiful, we back off
to nearby URLs in a click graph, using
a method similar to Word-Sense Disam-
biguation. Thus, we can infer that de-
signer trench is commercial because it is
close to www.saksfifthavenue.com, which
is known to be commercial. The baseline
method was designed for precision and
the backoff method was designed for re-
call. Both methods are fast and do not re-
quire crawling webpages. We recommend
a third method, a hybrid of the two, that
does no harm when there is plenty of train-
ing data, and generalizes better when there
isn’t, as a strong baseline for the CQI task.
</bodyText>
<sectionHeader confidence="0.929826" genericHeader="categories and subject descriptors">
1 Classify Queries By Intent (CQI)
</sectionHeader>
<bodyText confidence="0.999870976744186">
Determining query intent is an important prob-
lem for today’s search engines. Queries are short
(consisting of 2.2 terms on average (Beitzel et al.,
2004)) and contain ambiguous terms. Search en-
gines need to derive what users want from this lim-
ited source of information. Users may be search-
ing for a specific page, browsing for information,
or trying to buy something. Guessing the correct
intent is important for returning relevant items.
Someone searching for designer trench is likely
to be interested in results or ads for trench coats,
while someone searching for world war I trench
might be irritated by irrelevant clothing advertise-
ments.
Broder (2002) and Rose and Levinson (2004)
categorized queries into those with navigational,
informational, and transactional or resource-
seeking intent. Navigational queries are queries
for which a user has a particular web page in mind
that they are trying to navigate to, such as grey-
hound bus. Informational queries are those like
San Francisco, in which the user is trying to gather
information about a topic. Transactional queries
are those like digital camera or download adobe
reader, where the user is seeking to make a trans-
action or access an online resource.
Knowing the intent of a query greatly affects the
type of results that are relevant. For many queries,
Wikipedia articles are returned on the first page
of results. For informational queries, this is usu-
ally appropriate, as a Wikipedia article contains
summaries of topics and links to explore further.
However, for navigational or transactional queries,
Wikipedia is not as appropriate. A user looking
for the greyhound bus homepage is probably not
interested in facts about the company. Similarly,
someone looking to download adobe reader will
not be interested in Wikipedia’s description of the
product’s history. Conversely, for informational
queries, Wikipedia articles tend to be appropriate
while advertisements are not. The user searching
for world war I trench might find the Wikipedia
article on trench warfare useful, while he is prob-
</bodyText>
<page confidence="0.945106">
1428
</page>
<note confidence="0.959093">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1428–1436,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.6340375">
(a) The advertisements and related searches are probably more likely to be clicked on than
the top result for designer trench.
(b) The top result will receive more clicks than the spelling suggestion. Wikipedia often
receives lots of clicks, but not for commercial queries like bestbuy.
</figure>
<figureCaption confidence="0.977763">
Figure 1: Results pages from two major search engines. A search results page has limited real estate that
must be divided between search results, spelling suggestions, query suggestions, and ads.
</figureCaption>
<bodyText confidence="0.999437866666667">
ably not interested in purchasing clothing, or even
World War I related products. We noticed empiri-
cally that queries in the logs tend to have a high
proportion of clicks on the Wikipedia article or
the ads, but almost never both. The Wikipedia
page for Best Buy in Figure 1(b) is probably a
waste of space. Knowing whether a particular
query is navigational, informational, or transac-
tional would improve search and advertising rel-
evance.
After a query is issued, search engines return
a list of results, and possibly also advertisements,
suggestions of related searches, and spelling sug-
gestions. For different queries, these alternatives
have varying utilities to the users. Consider the
queries in Figures 1(a) and 1(b). For designer
trench, the advertisements may well be more use-
ful to the user than the standard set of results. The
query suggestions for designer trench all would
help refine the query, whereas the suggestions for
bestbuy are less useful, as they would either re-
turn the same set of results or take the user to Best
Buy’s competitors’ sites. The spelling suggestion
for best buy instead of bestbuy is also unnecessary.
Devoting more page space to the content that is
likely to be clicked on could help improve the user
experience.
In this paper we consider the task of: given a
class of queries, which types of answer (standard
search, ads, query suggestions, or spelling sug-
</bodyText>
<page confidence="0.992899">
1429
</page>
<bodyText confidence="0.99890196969697">
gestions) are likely to be clicked on? Typos will
tend to have more clicks on the spelling sugges-
tions, informational queries will have more clicks
on Wikipedia pages, and commercial queries will
have more clicks on the ads. The observed behav-
ior of where users click tells us something about
the hidden intentions of the users when they issue
that query.
We focus on commercial intent (Dai et al.,
2006), the intent to purchase a product or service,
to illustrate our method of predicting query intent.
The business model of web search today is heav-
ily dependent on advertising. Advertisers bid on
queries, and then the search results page also con-
tains “sponsored” sites by the advertisers who won
the auction for that query. It is thus advantageous
for the advertisers to bid on queries which are most
likely to result in a commercial transaction. If
a query is classified as likely implying commer-
cial intent, but the advertisers have overlooked this
query, then the search engine may want to sug-
gest that advertisers bid on that query. The search
engine may also want to treat queries classified
as having commercial intent differently, by rear-
ranging the appearance of the page, or by showing
more or fewer advertisements.
This paper starts with a simple Naive Bayes
baseline to classify queries by intent (CQI). Super-
vised methods work well, especially when there is
plenty of annotated data for testing and training.
Unfortunately, since we don’t have as much anno-
tated data as we might like, we propose two work-
arounds:
</bodyText>
<listItem confidence="0.999947">
1. Use click logs as a substitute for annotated
data. Clicks on ads are evidence for commer-
cial intent; other types of clicks are evidence
for other intents.
2. We propose a method similar to Yarowsky
(1995) to generalize beyond the training set.
</listItem>
<sectionHeader confidence="0.999265" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999960383333334">
Click logs have been used for a variety of tasks
involved in information retrieval, including pre-
dicting which pages are the best results for queries
(Piwowarski and Zaragoza, 2007; Joachims, 2002;
Xue et al., 2004), choosing relevant advertise-
ments (Chakrabarti et al., 2008), suggesting re-
lated queries (Beeferman and Berger, 2000), and
personalizing results (Tan et al., 2006). Queries
that have a navigational intent tended to have
a highly skewed click distribution, while users
clicked on a wider range of results after issuing
informational queries. Lee et al. (2005) used the
click distributions to classify navigational versus
informational intents.
While navigational, informational, and
resource-seeking are very broad intentions, other
researchers have looked at personalization and
intent on a per user basis. Downey et al. (2008)
use the last URL visited in a session or the last
search engine result visited as a proxy for the
user’s information goal, and then looked at the
correspondence between information needs and
queries (how the goals are expressed).
We are interested in a granularity of intent
in between navigational/informational/resource-
seeking and personalized intents. For these sorts
of intents, the web pages associated with queries
provide useful information. To classify queries
into an ontology of commercial queries, Broder
et al. (2007) found that a classifier that used the
text of the top result pages performed much bet-
ter than a classifier that used only the query string.
While the results are quite good on their hierarchy
of 6000 types of commercial intents, they manu-
ally constructed about 150 hand-picked examples
each for each of the 6000 intents. Beitzel et al.
(2005) do semi-supervised learning over the query
logs to classify queries into topics, but also train
with hundreds of thousands of manually annotated
queries. Thus, while we also use the query logs
and the identities of web pages of associated with
each query, we are interested in finding methods
that can be applied when that much annotation is
prohibitive.
Semi-supervised methods over the click graph
make it possible to train classifiers after starting
from a much smaller set of seed queries. Li et al.
(2008) used the semi-supervised learning method
described in Zhou et al. (2004) to gain a much
larger training set of examples, and then trained
classifiers for product search or job search on the
expanded set. Random walk methods over the
click graph have also been used to propagate re-
lations between URLs, for tasks such as finding
“adult” content (Craswell and Szummer, 2007)
and suggesting related queries (Antonellis et al.,
2008) and content (Baluja et al., 2008). In our
work we also seek to classify query intent us-
ing the click graph, but we demonstrate the ef-
fectiveness of a simple method by building deci-
</bodyText>
<page confidence="0.942238">
1430
</page>
<bodyText confidence="0.999961272727273">
sion lists of URLs. In addition, we evaluate our
method automatically by using user click rates,
rather than assembling hand-labeled examples for
training and testing.
Dai et al. (2006) also classified queries by com-
mercial intent, but their method involved crawling
the top landing pages for each query, which can
be quite time-consuming. In this paper we investi-
gate the commercial intent problem when crawling
pages is not feasible, and use only the identities of
the top URLs.
</bodyText>
<sectionHeader confidence="0.978183" genericHeader="method">
3 Using Click Logs as a Substitute for
Annotation
</sectionHeader>
<bodyText confidence="0.999904666666667">
Prior work has used click logs in lieu of manual
annotations of relevance ratings, either of web-
pages (Joachims, 2002) or of sponsored search ad-
vertisements (Ciaramita et al., 2008). Here we use
the click logs as a large-scale source of intents.
Logs from Microsoft’s Live Search are used for
training and test purposes. Logs from May 2008
were used for training, and logs from June 2008
were used for testing.
The logs distinguish four types of clicks: (a)
search results, (b) ads, (c) spelling suggestions and
(d) query suggestions. Some prototypical queries
of each type are shown in Table 1. As mentioned
above, clicks on ads are evidence for commercial
intent; other types of clicks are evidence for other
intents. The query, ebay official, is assumed to be
commercial intent, because a large fraction of the
clicks are on ads. In contrast, typos tend to have
relatively more clicks on “did-you-mean” spelling
suggestions.
The query logs contain over a terabyte of
data for each day, and our experiments were
done using months of logs at a time. We
used SCOPE (Chaiken et al., 2008), a script-
ing programming language designed for doing
Map-Reduce (Dean and Ghemawat, 2004) style
computations, to distribute the task of aggre-
gating the counts of each query over thousands
of servers. As the same query is often issued
several times by multiple users across an en-
tire month of search logs, we summarize each
query with four ratios–search results clicks:overall
clicks, ad clicks:overall clicks, spelling sugges-
tion clicks:overall clicks, and query suggestion
clicks:overall clicks.
A couple of steps were taken to ensure reliable
ratios. We are classifying types, not tokens, and
so limited ourselves to those queries with 100 or
more clicks. This still leaves us with over half a
million distinct queries for training and for test-
ing, yet allows us to use click ratios as a substitute
for annotating these huge data sets. If a query was
only issued once and the user clicked on an ad,
that may be more a reflection of the user, rather
than reflecting that the query is 100% commer-
cial. In addition, the ratios compare clicks of one
type with clicks of another, rather than compar-
ing clicks with impressions. There is less risk of a
failure to find fallacy if we count events (clicks) in-
stead of non-events (non-clicks). There are many
reasons for non-clicks, only some of which tell us
about the meaning of the query. There are bots that
crawl pages and never click. Many links can’t be
seen (e.g., if they are below the fold).
Queries are labeled as positive examples of
commercial intent if their ratio is in the top half of
the training set, and negative otherwise. A similar
procedure is used to label queries with the three
other intents.
Our task is to predict future click patterns based
on past click patterns. Note that a query may ap-
pear in both the test set and the training set, al-
though not necessarily with the same label. In fact,
because of the robustness requirement of 100+
clicks, many queries appear in both sets; 506,369
out of 591,122 of the test queries were also present
in the training month. The overlap reflects natural
processes on the web, with a long tail (of queries
that will never be seen again) and a big fat head (of
queries that come up again and again). Throwing
away the overlap would both drastically reduce the
size of the data and make the problem less realistic
for a commercial application.
We therefore report results on various training
set sizes so that we can show both: (a) the abil-
ity of the proposed method to generalize to unseen
queries, and (b) the high performance of the base-
lines in a realistic setting. We vary the number of
new queries by training the methods on subsets of
20%, 40%, 60%, 80%, and 100% of the positive
examples (along with all the negative examples)
in the training set. This led to the test set having
17%, 34%, 52%, 67%, and 86% actual overlap of
these queries, respectively, with the training sets.
</bodyText>
<page confidence="0.898339">
1431
</page>
<table confidence="0.999223125">
Click Type Query Type Example
(Area on Results Page) (Intent)
Spelling Suggestion Typo www.lastmintue.com.au
Ad Commercial Intent ebay official
Query Suggestion Suggestible sears employees (where there are some popular query suggestions
Search Result Standard Search indicating how current employees can navigate to the benefits site,
as well as how others can apply for employment)
craigslist, denver, co
</table>
<tableCaption confidence="0.999865">
Table 1: Queries with a high percentage of clicks in each category
</tableCaption>
<sectionHeader confidence="0.995928" genericHeader="method">
4 Three CQI Methods
</sectionHeader>
<subsectionHeader confidence="0.999842">
4.1 Method 1: Look-up Baseline
</subsectionHeader>
<bodyText confidence="0.999101866666667">
The baseline method checks if a query was present
in the training set, and if so, outputs the label from
the training set. If the query was not present, it
backs off to the appropriate default label: “non-
commercial” for the commercial intent task (and
“non-suggestible”, “not a typo”, etc. for the other
CQI tasks). This very simple baseline method
is effective because the ratios tend to be fairly
stable from one month to the next. The query,
ebay official, for example, has relatively high ad
clicks in both the training month as well as the
test month. The next section will propose an al-
ternative method to address the main weakness of
the baseline method, the inability to generalize be-
yond the queries in the training set.
</bodyText>
<figureCaption confidence="0.650157166666667">
Figure 2: saks and bluefly trench coats are known
to be commercial, while world war I trench is
known to be non-commercial. What about de-
signer trench? We can classify it as commercial
because it shares URLs with the known commer-
cial queries.
</figureCaption>
<subsectionHeader confidence="0.992127333333333">
4.2 Method 2: Using Click Graph Context to
Generalize Beyond the Queries in the
Training Set
</subsectionHeader>
<bodyText confidence="0.989348405405405">
To address the generalization concern, we propose
a method inspired by Yarowsky (1994). Word
sense disambiguation is a classic problem in nat-
ural language processing. Some words have mul-
tiple senses; for instance, bank can either mean
a riverbank or a financial institution, and for var-
ious tasks such as information retrieval, parsing,
or information extraction, it is useful to be able to
differentiate between the possible meanings.
When a word is being used in each sense, it
tends to appear in a different context. For exam-
ple, if the word muddy is nearby bank, the author
is probably using the riverbank sense of the term,
while if the word deposit is nearby, the word is
probably being used with the financial sense.
Yarowksy (1995) thus creates a list of each pos-
sible context, sorted by how strong the evidence is
for a particular sense. To classify a new example,
Yarowsky (1994) finds the most informative collo-
cation pattern that applies to the test example.
In this work, rather than using the surrounding
words as context as in text classification, we con-
sider the surrounding URLs in the click graph as
context. A sample portion of the click graph is
shown in figure 2. The figure shows queries on
the left and URLs on the right. The click graph
was computed on a very large sample of logs com-
puted well before the training period. There is an
edge from a query q to a URL u if at least 10 users
issued q and then clicked on u.
For each URL, we look at its neighboring
queries and calculate the log likelihood ratio of
their labels in the training set. We classify a new
query q according to URL*, the neighboring URL
with the strongest opinion (highest absolute value
of the log likelihood ratio). That is, we compute
URL* with:
</bodyText>
<page confidence="0.785533">
1432
</page>
<equation confidence="0.97442425">
��
�Pr(Intent|Ui) �
��log �
Pr(-,Intent|Ui) �
</equation>
<bodyText confidence="0.96204875">
If the neighboring opinion is positive (that is,
Pr(Intent|URL*) &gt; Pr(-,Intent|URL*)), then
the query q is assigned a positive label. Otherwise,
q is assigned a negative label.
In Figure 2, we classify designer trench as a
commercial query based on the neighbor with
the strongest opinion. In this case, there
was a tie between two neighbors with equally
strong opinions: www.saksfifthavenue.com and
www.bluefly.com/Designer-Trench-Coats. Both
neighbors are associated with queries that were
labeled commercial in the training set: saks and
bluefly trench coats, respectively.
This method allows the labels of training set
queries to propagate through the URLs to new test
set queries.
</bodyText>
<subsectionHeader confidence="0.964349">
4.3 Method 3: Hybrid (“Better Together”)
</subsectionHeader>
<bodyText confidence="0.992183">
We recommend a hybrid of the two methods:
</bodyText>
<listItem confidence="0.977868">
• Method 1: the look-up baseline
• Method 2: use click graph context to gener-
alize beyond the queries in the training set
</listItem>
<bodyText confidence="0.93991525">
Method 1 is designed for precision and method 2
is designed for recall. The hybrid uses method
1 when applicable, and otherwise, backs off to
method 2.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.98774">
5.1 Commercial Intent
</subsectionHeader>
<bodyText confidence="0.999956666666667">
Table 2 and Figures 3(a) and 3(b) compare the per-
formance on the proposed hybrid method with the
baseline. When there is plenty of training mate-
rial, both methods perform about equally well (the
look-up baseline has an F-score of 84.1%, com-
pared with the hybrid method’s F-score of 85.3%),
but generalization becomes important when train-
ing data is severely limited. Figure 3(a) shows
that the proposed method does no harm and might
even help a little when there is plenty of training
data. The hybrid’s main benefit is generalization
to queries beyond the training set. If we severely
limit the size of the training set to just 20% of the
month, as in Figure 3(b), then the proposed hybrid
method is substantially better than the baseline. In
this case, the proposed hybrid method’s F-score
is 65.8%, compared with the look-up method’s F-
score of 28.4%.
</bodyText>
<subsectionHeader confidence="0.997339">
5.2 Other types of clicks
</subsectionHeader>
<bodyText confidence="0.999973291666666">
Table 3 and Figures 4(a) and 4(b) show a similar
pattern for the query suggestion task. In fact, the
pattern is perhaps even stronger for the query sug-
gestion task than commercial intent. When the full
training set is used, the hybrid method achieves
an F-score of 91.9% (precision = 91.5%, recall =
92.3%). When only 20% of the training data is
used, the hybrid method has an F-score of 73.9%,
compared with the baseline’s F-score of 29.6%. A
similar pattern was observed for clicks on search
results.
The one exception is the spelling suggestion
task, where the context heuristic proved ineffec-
tive, for reasons that should not be surprising in
retrospect. Click graph distance is an effective
heuristic for many intents, but not for typos. Users
who issue misspelled the query have the same
goals as users who correctly spell the query, so
we shouldn’t expect URLs to be able to differ-
entiate them. For misspelled queries, for exam-
ple, yuotube, there are correctly spelled queries,
like youtube, with the same intent that will tend to
be associated with the same set of URLs (such as
www.youtube.com).
</bodyText>
<sectionHeader confidence="0.981654" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999863291666667">
We would like to be able to distinguish web
queries by intent. Unfortunately, we don’t have
annotated data for query intent, but we do have
access to large quantities of click logs. The logs
distinguish four types of clicks: (a) search results,
(b) ads, (c) spelling suggestions and (d) query sug-
gestions. Clicks on ads are evidence for commer-
cial intent; other types of clicks are evidence for
other intents. Click logs are huge sources of data,
and while there are privacy concerns, anonymized
logs are beginning to be released for research pur-
poses (Craswell et al., 2009).
Besides commercial intent, queries can also be
divided into two broader classes: queries in which
the user is browsing and queries for which the user
is navigating. Clicks on the ads and query sug-
gestions indicate that users are browsing and will-
ing to look at these alternative suggestions, while
clicks on the search results indicate that the users
were navigating to what they were searching for.
Clicks on typos indicate neither, as presumably the
users are not entering typos on purpose.
Just as dialogue management systems learn
policies for when to allow user initiative (the user
</bodyText>
<figure confidence="0.619346">
argmaxUi∈Nbr(q)
1433
(a) (b)
</figure>
<figureCaption confidence="0.9952152">
Figure 3: Better together: proposed hybrid is no worse than baseline (left) and generalizes better to
unseen tail queries (right). The two panels are the same, except that the training set was reduced on the
right to test generalization error.
Figure 4: Similar to Figures 3(a) and 3(b), adding the decision list method generalizes over the look-up
method for the “suggestible” task.
</figureCaption>
<figure confidence="0.920988">
(a) (b)
</figure>
<bodyText confidence="0.9998779375">
can respond in an open way) versus system ini-
tiative (the system asks the user questions with a
restricted set of possible answers) (Rela˜no et al.,
1999; Scheffler and Young, 2002; Singh et al.,
2002), search engines may want to learn policies
for when the user just wants the search results or
when the user is open to suggestions. When users
want help (they want the search engine to suggest
results), more space on the page should be devoted
to the ads and the query suggestions. When the
users know what it is they want, more of the page
should be given to the search results they asked
for.
We started with a simple baseline for predicting
click location that had great precision, but didn’t
generalize well beyond the queries in the train-
ing set. To improve recall, we proposed a con-
text heuristic that backs off in the click graph.
The backoff method is similar to Yarowsky’s Word
Sense Disambiguation method, except that context
is defined in terms of URLs nearby in click graph
distance, as opposed to words nearby in the text.
Our third method, a hybrid of the baseline
method and the backoff method, is the strongest
baseline we have come up with. The evaluation
showed that the hybrid does no harm when there
is plenty of training data, and generalizes better
when there isn’t.
A direction for further research would be to see
if propagating query intent through URLs that are
not direct neighbors but are further away, perhaps
through random walk methods (Baluja et al., 2008;
</bodyText>
<page confidence="0.971344">
1434
</page>
<table confidence="0.999860428571428">
Training Size Baseline F-score Hybrid Precision / Recall
Method 2 Baseline Method 2 Hybrid
100% 84.1 75.6 85.3 88.2 / 80.4 76.6 / 74.6 85.7 / 85.0
80% 74.4 74.8 83.5 88.2 / 64.3 79.3 / 70.7 86.7 / 80.6
60% 62.4 72.9 80.7 88.3 / 48.2 82.5 / 65.3 87.9 / 74.6
40% 47.9 70.1 76.0 77.5 / 34.7 78.5 / 63.3 80.7 / 66.0
20% 28.4 62.5 65.8 77.6 / 17.4 75.9 / 53.1 74.3 / 59.1
</table>
<tableCaption confidence="0.97592425">
Table 2: The baseline and hybrid methods have comparable F-scores when there is plenty of training
data, but generalization becomes important when training data is severely limited. The proposed hybrid
method generalizes better as indicated by the widening gap in F-scores with smaller and smaller training
sets.
</tableCaption>
<table confidence="0.999929">
Training Size Baseline F-score Hybrid Precision / Recall
Method 2 Baseline Method 2 Hybrid
100% 91.0 86.2 91.9 94.9 / 87.4 90.7 / 82.3 91.5 / 92.3
80% 80.5 85.2 90.6 94.9 / 69.9 91.6 / 79.7 91.9 / 89.4
60% 67.6 83.3 88.6 94.9 / 52.4 92.6 / 75.8 92.3 / 85.1
40% 51.0 79.5 84.7 94.9 / 34.9 87.6 / 72.7 93.0 / 77.8
20% 29.6 69.8 73.9 81.5 / 18.1 90.6 / 56.8 94.0 / 60.8
</table>
<tableCaption confidence="0.710447">
Table 3: F-scores on the query suggestion task. As in the commercial intent task, the proposed hybrid
method does no harm when there is plenty of training data, but generalizes better when training data is
severely limited.
Antonellis et al., 2008) improves classification.
</tableCaption>
<bodyText confidence="0.9995191875">
Similar methods could be applied in future work
to many other applications such labeling queries
and URLs by: language, market, location, time,
intended for a search vertical (such as medicine,
recipes), intended for a type of answer (maps, pic-
tures), as well as inappropriate intent (porn, spam).
In addition to click type, there are many other
features in the logs that could prove useful for
classifying queries by intent, e.g., who issued the
query, when and where. Similar methods could
also be used to personalize search (Teevan et al.,
2008); for queries that mean different things to dif-
ferent people, the Yarowsky method could be ap-
plied to variables such as user, time and place, so
the results reflect what a particular user intended
in a particular context.
</bodyText>
<sectionHeader confidence="0.999201" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999915">
We thank Sue Dumais for her helpful comments
on an early draft of this work. We would also like
to thank the members of the Text Mining, Search,
and Navigation (TMSN) group at Microsoft Re-
search for useful discussions and the anonymous
reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.998435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801730769231">
I. Antonellis, H. Garcia-Molina, and C.C. Chang.
2008. Simrank++: query rewriting through link
analysis of the clickgraph (poster). WWW.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008.
Video suggestion and discovery for youtube: taking
random walks through the view graph. WWW.
D. Beeferman and A. Berger. 2000. Agglomerative
clustering of a search engine query log. In SIGKDD,
pages 407–416.
S.M. Beitzel, E.C. Jensen, A. Chowdhury, D. Gross-
man, and O. Frieder. 2004. Hourly analysis of a
very large topically categorized web query log. SI-
GIR, pages 321–328.
S.M. Beitzel, E.C. Jensen, O. Frieder, D.D. Lewis,
A. Chowdhury, and A. Kolcz. 2005. Improving
automatic query classification via semi-supervised
learning. ICDM, pages 42–49.
A.Z. Broder, M. Fontoura, E. Gabrilovich, A. Joshi,
V. Josifovski, and T. Zhang. 2007. Robust classifi-
cation of rare queries using web knowledge. SIGIR,
pages 231–238.
A. Broder. 2002. A taxonomy of web search. SIGIR,
36(2).
R. Chaiken, B. Jenkins, P. ˚A. Larson, B. Ramsey,
D. Shakib, S. Weaver, and J. Zhou. 2008. SCOPE:
</reference>
<page confidence="0.83013">
1435
</page>
<reference confidence="0.999392194805195">
Easy and efficient parallel processing of massive
data sets. Proceedings of the VLDB Endowment
archive, 1(2):1265–1276.
D. Chakrabarti, D. Agarwal, and V. Josifovski. 2008.
Contextual advertising by combining relevance with
click feedback. WWW.
M. Ciaramita, V. Murdock, and V. Plachouras. 2008.
Online learning from click data for sponsored
search.
N. Craswell and M. Szummer. 2007. Random walks
on the click graph. In Proceedings of the 30th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 239–246.
N. Craswell, R. Jones, G. Dupret, and E. Viegas (Con-
ference Chairs). 2009. Wscd ’09: Proceedings of
the 2009 workshop on web search click data.
H.K. Dai, L. Zhao, Z. Nie, J.R. Wen, L. Wang, and
Y. Li. 2006. Detecting online commercial intention
(OCI). WWW, pages 829–837.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified Data Processing on Large Clusters. OSDI,
pages 137–149.
D. Downey, S. Dumais, D. Liebling, and E. Horvitz.
2008. Understanding the relationship between
searchers’ queries and information goals. In CIKM.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD), ACM.
U. Lee, Z. Liu, and J. Cho. 2005. Automatic identifi-
cation of user goals in Web search. In WWW, pages
391–400.
X. Li, Y.Y. Wang, and A. Acero. 2008. Learning
query intent from regularized click graphs. In SI-
GIR, pages 339–346.
B. Piwowarski and H. Zaragoza. 2007. Predictive user
click models based on click-through history. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
pages 175–182.
J. Rela˜no, D. Tapias, M. Rodr´ıguez, M. Charfuel´an,
and L. Hern´andez. 1999. Robust and flexible
mixed-initiative dialogue for telephone services. In
Proceedings of EACL.
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. WWW, pages 13–19.
K. Scheffler and S. Young. 2002. Automatic learn-
ing of dialogue strategy using dialogue simulation
and reinforcement learning. In Proceedings of HLT,
pages 12–19.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with reinforce-
ment learning: Experiments with the NJFun sys-
tem. Journal of Artificial Intelligence Research,
16(1):105–133.
Bin Tan, Xuehua Shen, and ChengXiang Zhai. 2006.
Mining long-term search history to improve search
accuracy. pages 718–723. KDD.
J. Teevan, S.T. Dumais, and D.J. Liebling. 2008. To
personalize or not to personalize: modeling queries
with variation in user intent. SIGIR, pages 163–170.
Gui-Rong Xue, Hua-Jun Zeng, Zheng Chen, Yong
Yu, Wei-Ying Ma, WenSi Xi, and WeiGuo Fan.
2004. Optimizing web search using web click-
through data. In CIKM ’04: Proceedings of the thir-
teenth ACM international conference on Informa-
tion and knowledge management, pages 118–126.
D. Yarowsky. 1994. Decision lists for lexical ambigu-
ity resolution: Application to accent restoration in
Spanish and French. ACL, pages 88–95.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. ACL, pages
189–196.
D. Zhou, O. Bousquet, T.N. Lal, J. Weston, and
B. Scholkopf. 2004. Learning with Local and
Global Consistency. In NIPS.
</reference>
<page confidence="0.992907">
1436
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000610">
<title confidence="0.99389">Using Word-Sense Disambiguation Methods to Classify Web Queries by Intent</title>
<author confidence="0.999698">Emily Pitler Ken Church</author>
<affiliation confidence="0.99978">Computer and Information Science Johns Hopkins University University of Pennsylvania Human Language Technology Center of Excellence</affiliation>
<address confidence="0.992659">Philadelphia, PA 19104, USA Baltimore, MD 21211</address>
<email confidence="0.999347">epitler@seas.upenn.eduKenneth.Church@jhu.edu</email>
<abstract confidence="0.98651710130719">Three methods are proposed to classify queries by intent (CQI), e.g., navigational, informational, commercial, etc. Following mixed-initiative dialog systems, search engines should distinguish navigational queries where the user is taking the initiative from other queries where there are more opportunities for system initiatives (e.g., suggestions, ads). The query intent problem has a number of useful applications for search engines, affecting how many (if any) advertisements to display, which results to return, and how to arrange the results page. Click logs are used as a substitute for annotation. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. We start with a simple Naive Bayes baseline that works well when there is plenty of training data. When training data is less plentiful, we back off to nearby URLs in a click graph, using a method similar to Word-Sense Disam- Thus, we can infer that detrench commercial because it is to which is known to be commercial. The baseline method was designed for precision and the backoff method was designed for recall. Both methods are fast and do not require crawling webpages. We recommend a third method, a hybrid of the two, that does no harm when there is plenty of training data, and generalizes better when there isn’t, as a strong baseline for the CQI task. 1 Classify Queries By Intent (CQI) Determining query intent is an important problem for today’s search engines. Queries are short (consisting of 2.2 terms on average (Beitzel et al., 2004)) and contain ambiguous terms. Search engines need to derive what users want from this limited source of information. Users may be searching for a specific page, browsing for information, or trying to buy something. Guessing the correct intent is important for returning relevant items. searching for trench likely to be interested in results or ads for trench coats, someone searching for war I trench might be irritated by irrelevant clothing advertisements. Broder (2002) and Rose and Levinson (2004) queries into those with and resource- Navigational queries are queries for which a user has a particular web page in mind they are trying to navigate to, such as grey- Informational queries are those like in which the user is trying to gather information about a topic. Transactional queries those like camera adobe where the user is seeking to make a transaction or access an online resource. Knowing the intent of a query greatly affects the type of results that are relevant. For many queries, Wikipedia articles are returned on the first page of results. For informational queries, this is usually appropriate, as a Wikipedia article contains summaries of topics and links to explore further. However, for navigational or transactional queries, Wikipedia is not as appropriate. A user looking the bus is probably not interested in facts about the company. Similarly, looking to adobe reader not be interested in Wikipedia’s description of the product’s history. Conversely, for informational queries, Wikipedia articles tend to be appropriate while advertisements are not. The user searching war I trench find the Wikipedia on trench warfare useful, while he is prob- 1428 of the 2009 Conference on Empirical Methods in Natural Language pages 6-7 August 2009. ACL and AFNLP (a) The advertisements and related searches are probably more likely to be clicked on than top result for (b) The top result will receive more clicks than the spelling suggestion. Wikipedia often lots of clicks, but not for commercial queries like Figure 1: Results pages from two major search engines. A search results page has limited real estate that must be divided between search results, spelling suggestions, query suggestions, and ads. ably not interested in purchasing clothing, or even World War I related products. We noticed empirically that queries in the logs tend to have a high proportion of clicks on the Wikipedia article or the ads, but almost never both. The Wikipedia page for Best Buy in Figure 1(b) is probably a waste of space. Knowing whether a particular query is navigational, informational, or transactional would improve search and advertising relevance. After a query is issued, search engines return a list of results, and possibly also advertisements, suggestions of related searches, and spelling suggestions. For different queries, these alternatives have varying utilities to the users. Consider the in Figures 1(a) and 1(b). For the advertisements may well be more useful to the user than the standard set of results. The suggestions for trench would help refine the query, whereas the suggestions for less useful, as they would either return the same set of results or take the user to Best Buy’s competitors’ sites. The spelling suggestion buy of also unnecessary. Devoting more page space to the content that is likely to be clicked on could help improve the user experience. In this paper we consider the task of: given a class of queries, which types of answer (standard ads, query suggestions, or spelling sug- 1429 gestions) are likely to be clicked on? Typos will tend to have more clicks on the spelling suggestions, informational queries will have more clicks on Wikipedia pages, and commercial queries will have more clicks on the ads. The observed behavior of where users click tells us something about the hidden intentions of the users when they issue that query. focus on intent et al., 2006), the intent to purchase a product or service, to illustrate our method of predicting query intent. The business model of web search today is heavily dependent on advertising. Advertisers bid on queries, and then the search results page also contains “sponsored” sites by the advertisers who won the auction for that query. It is thus advantageous for the advertisers to bid on queries which are most likely to result in a commercial transaction. If a query is classified as likely implying commercial intent, but the advertisers have overlooked this query, then the search engine may want to suggest that advertisers bid on that query. The search engine may also want to treat queries classified as having commercial intent differently, by rearranging the appearance of the page, or by showing more or fewer advertisements. This paper starts with a simple Naive Bayes baseline to classify queries by intent (CQI). Supervised methods work well, especially when there is plenty of annotated data for testing and training. Unfortunately, since we don’t have as much annotated data as we might like, we propose two workarounds: 1. Use click logs as a substitute for annotated data. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. 2. We propose a method similar to Yarowsky (1995) to generalize beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at personalization and intent on a per user basis. Downey et al. (2008) use the last URL visited in a session or the last search engine result visited as a proxy for the user’s information goal, and then looked at the correspondence between information needs and queries (how the goals are expressed). We are interested in a granularity of intent in between navigational/informational/resourceseeking and personalized intents. For these sorts of intents, the web pages associated with queries provide useful information. To classify queries into an ontology of commercial queries, Broder et al. (2007) found that a classifier that used the text of the top result pages performed much better than a classifier that used only the query string. While the results are quite good on their hierarchy of 6000 types of commercial intents, they manually constructed about 150 hand-picked examples each for each of the 6000 intents. Beitzel et al. (2005) do semi-supervised learning over the query logs to classify queries into topics, but also train with hundreds of thousands of manually annotated queries. Thus, while we also use the query logs and the identities of web pages of associated with each query, we are interested in finding methods that can be applied when that much annotation is prohibitive. Semi-supervised methods over the click graph make it possible to train classifiers after starting from a much smaller set of seed queries. Li et al. (2008) used the semi-supervised learning method described in Zhou et al. (2004) to gain a much larger training set of examples, and then trained classifiers for product search or job search on the expanded set. Random walk methods over the click graph have also been used to propagate relations between URLs, for tasks such as finding “adult” content (Craswell and Szummer, 2007) and suggesting related queries (Antonellis et al., 2008) and content (Baluja et al., 2008). In our work we also seek to classify query intent using the click graph, but we demonstrate the efof a simple method by building deci- 1430 sion lists of URLs. In addition, we evaluate our method automatically by using user click rates, rather than assembling hand-labeled examples for training and testing. Dai et al. (2006) also classified queries by commercial intent, but their method involved crawling the top landing pages for each query, which can be quite time-consuming. In this paper we investigate the commercial intent problem when crawling pages is not feasible, and use only the identities of the top URLs. 3 Using Click Logs as a Substitute for Annotation Prior work has used click logs in lieu of manual annotations of relevance ratings, either of webpages (Joachims, 2002) or of sponsored search advertisements (Ciaramita et al., 2008). Here we use the click logs as a large-scale source of intents. Logs from Microsoft’s Live Search are used for training and test purposes. Logs from May 2008 were used for training, and logs from June 2008 were used for testing. The logs distinguish four types of clicks: (a) search results, (b) ads, (c) spelling suggestions and (d) query suggestions. Some prototypical queries of each type are shown in Table 1. As mentioned above, clicks on ads are evidence for commercial intent; other types of clicks are evidence for other The query, is assumed to be commercial intent, because a large fraction of the clicks are on ads. In contrast, typos tend to have relatively more clicks on “did-you-mean” spelling suggestions. The query logs contain over a terabyte of data for each day, and our experiments were done using months of logs at a time. We used SCOPE (Chaiken et al., 2008), a scripting programming language designed for doing Map-Reduce (Dean and Ghemawat, 2004) style computations, to distribute the task of aggregating the counts of each query over thousands of servers. As the same query is often issued several times by multiple users across an entire month of search logs, we summarize each query with four ratios–search results clicks:overall clicks, ad clicks:overall clicks, spelling suggestion clicks:overall clicks, and query suggestion clicks:overall clicks. A couple of steps were taken to ensure reliable ratios. We are classifying types, not tokens, and so limited ourselves to those queries with 100 or more clicks. This still leaves us with over half a million distinct queries for training and for testing, yet allows us to use click ratios as a substitute for annotating these huge data sets. If a query was only issued once and the user clicked on an ad, that may be more a reflection of the user, rather than reflecting that the query is 100% commercial. In addition, the ratios compare clicks of one type with clicks of another, rather than comparing clicks with impressions. There is less risk of a failure to find fallacy if we count events (clicks) instead of non-events (non-clicks). There are many reasons for non-clicks, only some of which tell us about the meaning of the query. There are bots that crawl pages and never click. Many links can’t be seen (e.g., if they are below the fold). Queries are labeled as positive examples of commercial intent if their ratio is in the top half of the training set, and negative otherwise. A similar procedure is used to label queries with the three other intents. Our task is to predict future click patterns based on past click patterns. Note that a query may appear in both the test set and the training set, although not necessarily with the same label. In fact, because of the robustness requirement of 100+ clicks, many queries appear in both sets; 506,369 out of 591,122 of the test queries were also present in the training month. The overlap reflects natural processes on the web, with a long tail (of queries that will never be seen again) and a big fat head (of queries that come up again and again). Throwing away the overlap would both drastically reduce the size of the data and make the problem less realistic for a commercial application. We therefore report results on various training set sizes so that we can show both: (a) the ability of the proposed method to generalize to unseen queries, and (b) the high performance of the baselines in a realistic setting. We vary the number of new queries by training the methods on subsets of 20%, 40%, 60%, 80%, and 100% of the positive examples (along with all the negative examples) in the training set. This led to the test set having 17%, 34%, 52%, 67%, and 86% actual overlap of these queries, respectively, with the training sets. 1431 Click Type Query Type (Intent) Example (Area on Results Page) Spelling Suggestion Ad Typo www.lastmintue.com.au Query Suggestion Commercial Intent Suggestible ebay official Search Result Standard Search sears employees (where there are some popular query suggestions indicating how current employees can navigate to the benefits site, as well as how others can apply for employment) craigslist, denver, co Table 1: Queries with a high percentage of clicks in each category 4 Three CQI Methods 4.1 Method 1: Look-up Baseline The baseline method checks if a query was present in the training set, and if so, outputs the label from the training set. If the query was not present, it backs off to the appropriate default label: “noncommercial” for the commercial intent task (and “non-suggestible”, “not a typo”, etc. for the other CQI tasks). This very simple baseline method is effective because the ratios tend to be fairly stable from one month to the next. The query, for example, has relatively high ad clicks in both the training month as well as the test month. The next section will propose an alternative method to address the main weakness of the baseline method, the inability to generalize beyond the queries in the training set. 2: trench coats known be commercial, while war I trench to be non-commercial. What about de- We can classify it as commercial because it shares URLs with the known commercial queries. 4.2 Method 2: Using Click Graph Context to Generalize Beyond the Queries in the Training Set To address the generalization concern, we propose a method inspired by Yarowsky (1994). Word sense disambiguation is a classic problem in natural language processing. Some words have mulsenses; for instance, either mean a riverbank or a financial institution, and for various tasks such as information retrieval, parsing, or information extraction, it is useful to be able to differentiate between the possible meanings. When a word is being used in each sense, it tends to appear in a different context. For examif the word nearby the author is probably using the riverbank sense of the term, if the word nearby, the word is probably being used with the financial sense. Yarowksy (1995) thus creates a list of each possible context, sorted by how strong the evidence is for a particular sense. To classify a new example, Yarowsky (1994) finds the most informative collocation pattern that applies to the test example. In this work, rather than using the surrounding words as context as in text classification, we consider the surrounding URLs in the click graph as context. A sample portion of the click graph is shown in figure 2. The figure shows queries on the left and URLs on the right. The click graph was computed on a very large sample of logs computed well before the training period. There is an from a query a URL at least 10 users then clicked on For each URL, we look at its neighboring queries and calculate the log likelihood ratio of their labels in the training set. We classify a new to the neighboring URL with the strongest opinion (highest absolute value of the log likelihood ratio). That is, we compute with: 1432 �� If the neighboring opinion is positive (that is, then query assigned a positive label. Otherwise, assigned a negative label. Figure 2, we classify trench a commercial query based on the neighbor with the strongest opinion. In this case, was a tie between two neighbors with equally opinions: Both neighbors are associated with queries that were commercial in the training set: trench respectively. This method allows the labels of training set queries to propagate through the URLs to new test set queries. 4.3 Method 3: Hybrid (“Better Together”) We recommend a hybrid of the two methods: • Method 1: the look-up baseline • Method 2: use click graph context to generalize beyond the queries in the training set Method 1 is designed for precision and method 2 is designed for recall. The hybrid uses method 1 when applicable, and otherwise, backs off to method 2. 5 Results 5.1 Commercial Intent Table 2 and Figures 3(a) and 3(b) compare the performance on the proposed hybrid method with the baseline. When there is plenty of training material, both methods perform about equally well (the look-up baseline has an F-score of 84.1%, compared with the hybrid method’s F-score of 85.3%), but generalization becomes important when training data is severely limited. Figure 3(a) shows that the proposed method does no harm and might even help a little when there is plenty of training data. The hybrid’s main benefit is generalization to queries beyond the training set. If we severely limit the size of the training set to just 20% of the month, as in Figure 3(b), then the proposed hybrid method is substantially better than the baseline. In this case, the proposed hybrid method’s F-score is 65.8%, compared with the look-up method’s Fscore of 28.4%. 5.2 Other types of clicks Table 3 and Figures 4(a) and 4(b) show a similar pattern for the query suggestion task. In fact, the pattern is perhaps even stronger for the query suggestion task than commercial intent. When the full training set is used, the hybrid method achieves an F-score of 91.9% (precision = 91.5%, recall = 92.3%). When only 20% of the training data is used, the hybrid method has an F-score of 73.9%, compared with the baseline’s F-score of 29.6%. A similar pattern was observed for clicks on search results. The one exception is the spelling suggestion task, where the context heuristic proved ineffective, for reasons that should not be surprising in retrospect. Click graph distance is an effective heuristic for many intents, but not for typos. Users who issue misspelled the query have the same goals as users who correctly spell the query, so we shouldn’t expect URLs to be able to differentiate them. For misspelled queries, for examthere are correctly spelled queries, with the same intent that will tend to be associated with the same set of URLs (such as 6 Conclusion and Future Work We would like to be able to distinguish web queries by intent. Unfortunately, we don’t have annotated data for query intent, but we do have access to large quantities of click logs. The logs distinguish four types of clicks: (a) search results, (b) ads, (c) spelling suggestions and (d) query suggestions. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. Click logs are huge sources of data, and while there are privacy concerns, anonymized logs are beginning to be released for research purposes (Craswell et al., 2009). Besides commercial intent, queries can also be divided into two broader classes: queries in which the user is browsing and queries for which the user is navigating. Clicks on the ads and query suggestions indicate that users are browsing and willing to look at these alternative suggestions, while clicks on the search results indicate that the users were navigating to what they were searching for. Clicks on typos indicate neither, as presumably the users are not entering typos on purpose. Just as dialogue management systems learn for when to allow (the user 1433 (a) (b) Figure 3: Better together: proposed hybrid is no worse than baseline (left) and generalizes better to unseen tail queries (right). The two panels are the same, except that the training set was reduced on the right to test generalization error. Figure 4: Similar to Figures 3(a) and 3(b), adding the decision list method generalizes over the look-up method for the “suggestible” task. (a) (b) respond in an open way) versus initiative (the system asks the user questions with a set of possible answers) et al., 1999; Scheffler and Young, 2002; Singh et al., 2002), search engines may want to learn policies for when the user just wants the search results or when the user is open to suggestions. When users want help (they want the search engine to suggest results), more space on the page should be devoted to the ads and the query suggestions. When the users know what it is they want, more of the page should be given to the search results they asked for. We started with a simple baseline for predicting click location that had great precision, but didn’t generalize well beyond the queries in the training set. To improve recall, we proposed a context heuristic that backs off in the click graph. The backoff method is similar to Yarowsky’s Word Sense Disambiguation method, except that context is defined in terms of URLs nearby in click graph distance, as opposed to words nearby in the text. Our third method, a hybrid of the baseline method and the backoff method, is the strongest baseline we have come up with. The evaluation showed that the hybrid does no harm when there is plenty of training data, and generalizes better when there isn’t. A direction for further research would be to see if propagating query intent through URLs that are not direct neighbors but are further away, perhaps through random walk methods (Baluja et al., 2008; 1434 Training Size Baseline F-score Method 2 Hybrid Precision / Recall Baseline Method 2 Hybrid 100% 84.1 75.6 85.3 88.2 / 80.4 76.6 / 74.6 85.7 / 85.0 80% 74.4 74.8 83.5 88.2 / 64.3 79.3 / 70.7 86.7 / 80.6 60% 62.4 72.9 80.7 88.3 / 48.2 82.5 / 65.3 87.9 / 74.6 40% 47.9 70.1 76.0 77.5 / 34.7 78.5 / 63.3 80.7 / 66.0 20% 28.4 62.5 65.8 77.6 / 17.4 75.9 / 53.1 74.3 / 59.1 Table 2: The baseline and hybrid methods have comparable F-scores when there is plenty of training data, but generalization becomes important when training data is severely limited. The proposed hybrid method generalizes better as indicated by the widening gap in F-scores with smaller and smaller training sets. Training Size Baseline F-score Method 2 Hybrid Precision / Recall Baseline Method 2 Hybrid 100% 91.0 86.2 91.9 94.9 / 87.4 90.7 / 82.3 91.5 / 92.3 80% 80.5 85.2 90.6 94.9 / 69.9 91.6 / 79.7 91.9 / 89.4 60% 67.6 83.3 88.6 94.9 / 52.4 92.6 / 75.8 92.3 / 85.1 40% 51.0 79.5 84.7 94.9 / 34.9 87.6 / 72.7 93.0 / 77.8 20% 29.6 69.8 73.9 81.5 / 18.1 90.6 / 56.8 94.0 / 60.8 Table 3: F-scores on the query suggestion task. As in the commercial intent task, the proposed hybrid method does no harm when there is plenty of training data, but generalizes better when training data is severely limited. Antonellis et al., 2008) improves classification. Similar methods could be applied in future work to many other applications such labeling queries and URLs by: language, market, location, time, intended for a search vertical (such as medicine, recipes), intended for a type of answer (maps, pictures), as well as inappropriate intent (porn, spam). In addition to click type, there are many other features in the logs that could prove useful for classifying queries by intent, e.g., who issued the query, when and where. Similar methods could also be used to personalize search (Teevan et al., 2008); for queries that mean different things to different people, the Yarowsky method could be applied to variables such as user, time and place, so the results reflect what a particular user intended in a particular context. 7 Acknowledgments We thank Sue Dumais for her helpful comments on an early draft of this work. We would also like to thank the members of the Text Mining, Search, and Navigation (TMSN) group at Microsoft Research for useful discussions and the anonymous reviewers for their helpful comments. References I. Antonellis, H. Garcia-Molina, and C.C. Chang. 2008. Simrank++: query rewriting through link of the clickgraph (poster). S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. 2008. Video suggestion and discovery for youtube: taking walks through the view graph. D. Beeferman and A. Berger. 2000. Agglomerative of a search engine query log. In pages 407–416. S.M. Beitzel, E.C. Jensen, A. Chowdhury, D. Grossman, and O. Frieder. 2004. Hourly analysis of a large topically categorized web query log. SIpages 321–328. S.M. Beitzel, E.C. Jensen, O. Frieder, D.D. Lewis, A. Chowdhury, and A. Kolcz. 2005. Improving automatic query classification via semi-supervised pages 42–49. A.Z. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josifovski, and T. Zhang. 2007. Robust classifiof rare queries using web knowledge. pages 231–238. Broder. 2002. A taxonomy of web search. 36(2). Chaiken, B. Jenkins, P. Larson, B. Ramsey, D. Shakib, S. Weaver, and J. Zhou. 2008. SCOPE: 1435 Easy and efficient parallel processing of massive sets. of the VLDB Endowment 1(2):1265–1276. D. Chakrabarti, D. Agarwal, and V. Josifovski. 2008. Contextual advertising by combining relevance with feedback. M. Ciaramita, V. Murdock, and V. Plachouras. 2008. Online learning from click data for sponsored search. N. Craswell and M. Szummer. 2007. Random walks the click graph. In of the 30th annual international ACM SIGIR conference on Reand development in information</abstract>
<note confidence="0.6804252">pages 239–246. N. Craswell, R. Jones, G. Dupret, and E. Viegas (Conference Chairs). 2009. Wscd ’09: Proceedings of the 2009 workshop on web search click data. H.K. Dai, L. Zhao, Z. Nie, J.R. Wen, L. Wang, and Y. Li. 2006. Detecting online commercial intention pages 829–837. J. Dean and S. Ghemawat. 2004. MapReduce: Sim- Data Processing on Large Clusters. pages 137–149.</note>
<abstract confidence="0.97104528">D. Downey, S. Dumais, D. Liebling, and E. Horvitz. 2008. Understanding the relationship between queries and information goals. In T. Joachims. 2002. Optimizing search engines using data. In of the ACM Conference on Knowledge Discovery and Data Mining U. Lee, Z. Liu, and J. Cho. 2005. Automatic identifiof user goals in Web search. In pages 391–400. X. Li, Y.Y. Wang, and A. Acero. 2008. Learning intent from regularized click graphs. In SIpages 339–346. B. Piwowarski and H. Zaragoza. 2007. Predictive user models based on click-through history. In Proceedings of the sixteenth ACM conference on Conon information and knowledge pages 175–182. J. D. Tapias, M. Rodr´ıguez, M. Charfuel´an, and L. Hern´andez. 1999. Robust and flexible mixed-initiative dialogue for telephone services. In of D.E. Rose and D. Levinson. 2004. Understanding user in web search. pages 13–19. K. Scheffler and S. Young. 2002. Automatic learning of dialogue strategy using dialogue simulation reinforcement learning. In of pages 12–19. S. Singh, D. Litman, M. Kearns, and M. Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun sysof Artificial Intelligence 16(1):105–133. Bin Tan, Xuehua Shen, and ChengXiang Zhai. 2006. Mining long-term search history to improve search accuracy. pages 718–723. KDD. J. Teevan, S.T. Dumais, and D.J. Liebling. 2008. To personalize or not to personalize: modeling queries variation in user intent. pages 163–170. Gui-Rong Xue, Hua-Jun Zeng, Zheng Chen, Yong Yu, Wei-Ying Ma, WenSi Xi, and WeiGuo Fan. 2004. Optimizing web search using web clickdata. In ’04: Proceedings of the thirteenth ACM international conference on Informaand knowledge pages 118–126. D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in and French. pages 88–95. D. Yarowsky. 1995. Unsupervised word sense disamrivaling supervised methods. pages 189–196.</abstract>
<note confidence="0.66933225">D. Zhou, O. Bousquet, T.N. Lal, J. Weston, and B. Scholkopf. 2004. Learning with Local and Consistency. In 1436</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Antonellis</author>
<author>H Garcia-Molina</author>
<author>C C Chang</author>
</authors>
<title>Simrank++: query rewriting through link analysis of the clickgraph (poster).</title>
<date>2008</date>
<publisher>WWW.</publisher>
<contexts>
<context position="10346" citStr="Antonellis et al., 2008" startWordPosition="1666" endWordPosition="1669">n that much annotation is prohibitive. Semi-supervised methods over the click graph make it possible to train classifiers after starting from a much smaller set of seed queries. Li et al. (2008) used the semi-supervised learning method described in Zhou et al. (2004) to gain a much larger training set of examples, and then trained classifiers for product search or job search on the expanded set. Random walk methods over the click graph have also been used to propagate relations between URLs, for tasks such as finding “adult” content (Craswell and Szummer, 2007) and suggesting related queries (Antonellis et al., 2008) and content (Baluja et al., 2008). In our work we also seek to classify query intent using the click graph, but we demonstrate the effectiveness of a simple method by building deci1430 sion lists of URLs. In addition, we evaluate our method automatically by using user click rates, rather than assembling hand-labeled examples for training and testing. Dai et al. (2006) also classified queries by commercial intent, but their method involved crawling the top landing pages for each query, which can be quite time-consuming. In this paper we investigate the commercial intent problem when crawling p</context>
<context position="25901" citStr="Antonellis et al., 2008" startWordPosition="4350" endWordPosition="4353">ng sets. Training Size Baseline F-score Hybrid Precision / Recall Method 2 Baseline Method 2 Hybrid 100% 91.0 86.2 91.9 94.9 / 87.4 90.7 / 82.3 91.5 / 92.3 80% 80.5 85.2 90.6 94.9 / 69.9 91.6 / 79.7 91.9 / 89.4 60% 67.6 83.3 88.6 94.9 / 52.4 92.6 / 75.8 92.3 / 85.1 40% 51.0 79.5 84.7 94.9 / 34.9 87.6 / 72.7 93.0 / 77.8 20% 29.6 69.8 73.9 81.5 / 18.1 90.6 / 56.8 94.0 / 60.8 Table 3: F-scores on the query suggestion task. As in the commercial intent task, the proposed hybrid method does no harm when there is plenty of training data, but generalizes better when training data is severely limited. Antonellis et al., 2008) improves classification. Similar methods could be applied in future work to many other applications such labeling queries and URLs by: language, market, location, time, intended for a search vertical (such as medicine, recipes), intended for a type of answer (maps, pictures), as well as inappropriate intent (porn, spam). In addition to click type, there are many other features in the logs that could prove useful for classifying queries by intent, e.g., who issued the query, when and where. Similar methods could also be used to personalize search (Teevan et al., 2008); for queries that mean di</context>
</contexts>
<marker>Antonellis, Garcia-Molina, Chang, 2008</marker>
<rawString>I. Antonellis, H. Garcia-Molina, and C.C. Chang. 2008. Simrank++: query rewriting through link analysis of the clickgraph (poster). WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Baluja</author>
<author>R Seth</author>
<author>D Sivakumar</author>
<author>Y Jing</author>
<author>J Yagnik</author>
<author>S Kumar</author>
<author>D Ravichandran</author>
<author>M Aly</author>
</authors>
<title>Video suggestion and discovery for youtube: taking random walks through the view graph.</title>
<date>2008</date>
<publisher>WWW.</publisher>
<contexts>
<context position="10380" citStr="Baluja et al., 2008" startWordPosition="1672" endWordPosition="1675"> Semi-supervised methods over the click graph make it possible to train classifiers after starting from a much smaller set of seed queries. Li et al. (2008) used the semi-supervised learning method described in Zhou et al. (2004) to gain a much larger training set of examples, and then trained classifiers for product search or job search on the expanded set. Random walk methods over the click graph have also been used to propagate relations between URLs, for tasks such as finding “adult” content (Craswell and Szummer, 2007) and suggesting related queries (Antonellis et al., 2008) and content (Baluja et al., 2008). In our work we also seek to classify query intent using the click graph, but we demonstrate the effectiveness of a simple method by building deci1430 sion lists of URLs. In addition, we evaluate our method automatically by using user click rates, rather than assembling hand-labeled examples for training and testing. Dai et al. (2006) also classified queries by commercial intent, but their method involved crawling the top landing pages for each query, which can be quite time-consuming. In this paper we investigate the commercial intent problem when crawling pages is not feasible, and use only</context>
<context position="24599" citStr="Baluja et al., 2008" startWordPosition="4103" endWordPosition="4106">to Yarowsky’s Word Sense Disambiguation method, except that context is defined in terms of URLs nearby in click graph distance, as opposed to words nearby in the text. Our third method, a hybrid of the baseline method and the backoff method, is the strongest baseline we have come up with. The evaluation showed that the hybrid does no harm when there is plenty of training data, and generalizes better when there isn’t. A direction for further research would be to see if propagating query intent through URLs that are not direct neighbors but are further away, perhaps through random walk methods (Baluja et al., 2008; 1434 Training Size Baseline F-score Hybrid Precision / Recall Method 2 Baseline Method 2 Hybrid 100% 84.1 75.6 85.3 88.2 / 80.4 76.6 / 74.6 85.7 / 85.0 80% 74.4 74.8 83.5 88.2 / 64.3 79.3 / 70.7 86.7 / 80.6 60% 62.4 72.9 80.7 88.3 / 48.2 82.5 / 65.3 87.9 / 74.6 40% 47.9 70.1 76.0 77.5 / 34.7 78.5 / 63.3 80.7 / 66.0 20% 28.4 62.5 65.8 77.6 / 17.4 75.9 / 53.1 74.3 / 59.1 Table 2: The baseline and hybrid methods have comparable F-scores when there is plenty of training data, but generalization becomes important when training data is severely limited. The proposed hybrid method generalizes bette</context>
</contexts>
<marker>Baluja, Seth, Sivakumar, Jing, Yagnik, Kumar, Ravichandran, Aly, 2008</marker>
<rawString>S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. 2008. Video suggestion and discovery for youtube: taking random walks through the view graph. WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
</authors>
<title>Agglomerative clustering of a search engine query log.</title>
<date>2000</date>
<booktitle>In SIGKDD,</booktitle>
<pages>407--416</pages>
<contexts>
<context position="8022" citStr="Beeferman and Berger, 2000" startWordPosition="1297" endWordPosition="1300">ke, we propose two workarounds: 1. Use click logs as a substitute for annotated data. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. 2. We propose a method similar to Yarowsky (1995) to generalize beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at personalization and intent on a per user basis. Downey et al. (2008) use the last URL visited in a session or the last search engine result visited as a proxy</context>
</contexts>
<marker>Beeferman, Berger, 2000</marker>
<rawString>D. Beeferman and A. Berger. 2000. Agglomerative clustering of a search engine query log. In SIGKDD, pages 407–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Beitzel</author>
<author>E C Jensen</author>
<author>A Chowdhury</author>
<author>D Grossman</author>
<author>O Frieder</author>
</authors>
<title>Hourly analysis of a very large topically categorized web query log.</title>
<date>2004</date>
<pages>321--328</pages>
<publisher>SIGIR,</publisher>
<contexts>
<context position="1944" citStr="Beitzel et al., 2004" startWordPosition="305" endWordPosition="308">ommercial because it is close to www.saksfifthavenue.com, which is known to be commercial. The baseline method was designed for precision and the backoff method was designed for recall. Both methods are fast and do not require crawling webpages. We recommend a third method, a hybrid of the two, that does no harm when there is plenty of training data, and generalizes better when there isn’t, as a strong baseline for the CQI task. 1 Classify Queries By Intent (CQI) Determining query intent is an important problem for today’s search engines. Queries are short (consisting of 2.2 terms on average (Beitzel et al., 2004)) and contain ambiguous terms. Search engines need to derive what users want from this limited source of information. Users may be searching for a specific page, browsing for information, or trying to buy something. Guessing the correct intent is important for returning relevant items. Someone searching for designer trench is likely to be interested in results or ads for trench coats, while someone searching for world war I trench might be irritated by irrelevant clothing advertisements. Broder (2002) and Rose and Levinson (2004) categorized queries into those with navigational, informational,</context>
</contexts>
<marker>Beitzel, Jensen, Chowdhury, Grossman, Frieder, 2004</marker>
<rawString>S.M. Beitzel, E.C. Jensen, A. Chowdhury, D. Grossman, and O. Frieder. 2004. Hourly analysis of a very large topically categorized web query log. SIGIR, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Beitzel</author>
<author>E C Jensen</author>
<author>O Frieder</author>
<author>D D Lewis</author>
<author>A Chowdhury</author>
<author>A Kolcz</author>
</authors>
<title>Improving automatic query classification via semi-supervised learning.</title>
<date>2005</date>
<journal>ICDM,</journal>
<pages>42--49</pages>
<contexts>
<context position="9405" citStr="Beitzel et al. (2005)" startWordPosition="1512" endWordPosition="1515">a granularity of intent in between navigational/informational/resourceseeking and personalized intents. For these sorts of intents, the web pages associated with queries provide useful information. To classify queries into an ontology of commercial queries, Broder et al. (2007) found that a classifier that used the text of the top result pages performed much better than a classifier that used only the query string. While the results are quite good on their hierarchy of 6000 types of commercial intents, they manually constructed about 150 hand-picked examples each for each of the 6000 intents. Beitzel et al. (2005) do semi-supervised learning over the query logs to classify queries into topics, but also train with hundreds of thousands of manually annotated queries. Thus, while we also use the query logs and the identities of web pages of associated with each query, we are interested in finding methods that can be applied when that much annotation is prohibitive. Semi-supervised methods over the click graph make it possible to train classifiers after starting from a much smaller set of seed queries. Li et al. (2008) used the semi-supervised learning method described in Zhou et al. (2004) to gain a much </context>
</contexts>
<marker>Beitzel, Jensen, Frieder, Lewis, Chowdhury, Kolcz, 2005</marker>
<rawString>S.M. Beitzel, E.C. Jensen, O. Frieder, D.D. Lewis, A. Chowdhury, and A. Kolcz. 2005. Improving automatic query classification via semi-supervised learning. ICDM, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Z Broder</author>
<author>M Fontoura</author>
<author>E Gabrilovich</author>
<author>A Joshi</author>
<author>V Josifovski</author>
<author>T Zhang</author>
</authors>
<title>Robust classification of rare queries using web knowledge.</title>
<date>2007</date>
<pages>231--238</pages>
<publisher>SIGIR,</publisher>
<contexts>
<context position="9062" citStr="Broder et al. (2007)" startWordPosition="1452" endWordPosition="1455">rchers have looked at personalization and intent on a per user basis. Downey et al. (2008) use the last URL visited in a session or the last search engine result visited as a proxy for the user’s information goal, and then looked at the correspondence between information needs and queries (how the goals are expressed). We are interested in a granularity of intent in between navigational/informational/resourceseeking and personalized intents. For these sorts of intents, the web pages associated with queries provide useful information. To classify queries into an ontology of commercial queries, Broder et al. (2007) found that a classifier that used the text of the top result pages performed much better than a classifier that used only the query string. While the results are quite good on their hierarchy of 6000 types of commercial intents, they manually constructed about 150 hand-picked examples each for each of the 6000 intents. Beitzel et al. (2005) do semi-supervised learning over the query logs to classify queries into topics, but also train with hundreds of thousands of manually annotated queries. Thus, while we also use the query logs and the identities of web pages of associated with each query, </context>
</contexts>
<marker>Broder, Fontoura, Gabrilovich, Joshi, Josifovski, Zhang, 2007</marker>
<rawString>A.Z. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josifovski, and T. Zhang. 2007. Robust classification of rare queries using web knowledge. SIGIR, pages 231–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Broder</author>
</authors>
<title>A taxonomy of web search.</title>
<date>2002</date>
<journal>SIGIR,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="2450" citStr="Broder (2002)" startWordPosition="389" endWordPosition="390">em for today’s search engines. Queries are short (consisting of 2.2 terms on average (Beitzel et al., 2004)) and contain ambiguous terms. Search engines need to derive what users want from this limited source of information. Users may be searching for a specific page, browsing for information, or trying to buy something. Guessing the correct intent is important for returning relevant items. Someone searching for designer trench is likely to be interested in results or ads for trench coats, while someone searching for world war I trench might be irritated by irrelevant clothing advertisements. Broder (2002) and Rose and Levinson (2004) categorized queries into those with navigational, informational, and transactional or resourceseeking intent. Navigational queries are queries for which a user has a particular web page in mind that they are trying to navigate to, such as greyhound bus. Informational queries are those like San Francisco, in which the user is trying to gather information about a topic. Transactional queries are those like digital camera or download adobe reader, where the user is seeking to make a transaction or access an online resource. Knowing the intent of a query greatly affec</context>
</contexts>
<marker>Broder, 2002</marker>
<rawString>A. Broder. 2002. A taxonomy of web search. SIGIR, 36(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chaiken</author>
<author>B Jenkins</author>
<author>P ˚A Larson</author>
<author>B Ramsey</author>
<author>D Shakib</author>
<author>S Weaver</author>
<author>J Zhou</author>
</authors>
<title>SCOPE: Easy and efficient parallel processing of massive data sets.</title>
<date>2008</date>
<booktitle>Proceedings of the VLDB Endowment archive,</booktitle>
<pages>1--2</pages>
<contexts>
<context position="12150" citStr="Chaiken et al., 2008" startWordPosition="1973" endWordPosition="1976">esults, (b) ads, (c) spelling suggestions and (d) query suggestions. Some prototypical queries of each type are shown in Table 1. As mentioned above, clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. The query, ebay official, is assumed to be commercial intent, because a large fraction of the clicks are on ads. In contrast, typos tend to have relatively more clicks on “did-you-mean” spelling suggestions. The query logs contain over a terabyte of data for each day, and our experiments were done using months of logs at a time. We used SCOPE (Chaiken et al., 2008), a scripting programming language designed for doing Map-Reduce (Dean and Ghemawat, 2004) style computations, to distribute the task of aggregating the counts of each query over thousands of servers. As the same query is often issued several times by multiple users across an entire month of search logs, we summarize each query with four ratios–search results clicks:overall clicks, ad clicks:overall clicks, spelling suggestion clicks:overall clicks, and query suggestion clicks:overall clicks. A couple of steps were taken to ensure reliable ratios. We are classifying types, not tokens, and so l</context>
</contexts>
<marker>Chaiken, Jenkins, Larson, Ramsey, Shakib, Weaver, Zhou, 2008</marker>
<rawString>R. Chaiken, B. Jenkins, P. ˚A. Larson, B. Ramsey, D. Shakib, S. Weaver, and J. Zhou. 2008. SCOPE: Easy and efficient parallel processing of massive data sets. Proceedings of the VLDB Endowment archive, 1(2):1265–1276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chakrabarti</author>
<author>D Agarwal</author>
<author>V Josifovski</author>
</authors>
<title>Contextual advertising by combining relevance with click feedback.</title>
<date>2008</date>
<publisher>WWW.</publisher>
<contexts>
<context position="7965" citStr="Chakrabarti et al., 2008" startWordPosition="1289" endWordPosition="1292">nce we don’t have as much annotated data as we might like, we propose two workarounds: 1. Use click logs as a substitute for annotated data. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. 2. We propose a method similar to Yarowsky (1995) to generalize beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at personalization and intent on a per user basis. Downey et al. (2008) use the last URL visited in a se</context>
</contexts>
<marker>Chakrabarti, Agarwal, Josifovski, 2008</marker>
<rawString>D. Chakrabarti, D. Agarwal, and V. Josifovski. 2008. Contextual advertising by combining relevance with click feedback. WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>V Murdock</author>
<author>V Plachouras</author>
</authors>
<title>Online learning from click data for sponsored search.</title>
<date>2008</date>
<contexts>
<context position="11244" citStr="Ciaramita et al., 2008" startWordPosition="1818" endWordPosition="1821"> rates, rather than assembling hand-labeled examples for training and testing. Dai et al. (2006) also classified queries by commercial intent, but their method involved crawling the top landing pages for each query, which can be quite time-consuming. In this paper we investigate the commercial intent problem when crawling pages is not feasible, and use only the identities of the top URLs. 3 Using Click Logs as a Substitute for Annotation Prior work has used click logs in lieu of manual annotations of relevance ratings, either of webpages (Joachims, 2002) or of sponsored search advertisements (Ciaramita et al., 2008). Here we use the click logs as a large-scale source of intents. Logs from Microsoft’s Live Search are used for training and test purposes. Logs from May 2008 were used for training, and logs from June 2008 were used for testing. The logs distinguish four types of clicks: (a) search results, (b) ads, (c) spelling suggestions and (d) query suggestions. Some prototypical queries of each type are shown in Table 1. As mentioned above, clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. The query, ebay official, is assumed to be commercial intent,</context>
</contexts>
<marker>Ciaramita, Murdock, Plachouras, 2008</marker>
<rawString>M. Ciaramita, V. Murdock, and V. Plachouras. 2008. Online learning from click data for sponsored search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Craswell</author>
<author>M Szummer</author>
</authors>
<title>Random walks on the click graph.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>239--246</pages>
<contexts>
<context position="10289" citStr="Craswell and Szummer, 2007" startWordPosition="1658" endWordPosition="1661">we are interested in finding methods that can be applied when that much annotation is prohibitive. Semi-supervised methods over the click graph make it possible to train classifiers after starting from a much smaller set of seed queries. Li et al. (2008) used the semi-supervised learning method described in Zhou et al. (2004) to gain a much larger training set of examples, and then trained classifiers for product search or job search on the expanded set. Random walk methods over the click graph have also been used to propagate relations between URLs, for tasks such as finding “adult” content (Craswell and Szummer, 2007) and suggesting related queries (Antonellis et al., 2008) and content (Baluja et al., 2008). In our work we also seek to classify query intent using the click graph, but we demonstrate the effectiveness of a simple method by building deci1430 sion lists of URLs. In addition, we evaluate our method automatically by using user click rates, rather than assembling hand-labeled examples for training and testing. Dai et al. (2006) also classified queries by commercial intent, but their method involved crawling the top landing pages for each query, which can be quite time-consuming. In this paper we </context>
</contexts>
<marker>Craswell, Szummer, 2007</marker>
<rawString>N. Craswell and M. Szummer. 2007. Random walks on the click graph. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 239–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Craswell</author>
<author>R Jones</author>
<author>G Dupret</author>
<author>E Viegas</author>
</authors>
<title>workshop on web search click data.</title>
<date>2009</date>
<booktitle>Wscd ’09: Proceedings of the</booktitle>
<contexts>
<context position="22099" citStr="Craswell et al., 2009" startWordPosition="3677" endWordPosition="3680">www.youtube.com). 6 Conclusion and Future Work We would like to be able to distinguish web queries by intent. Unfortunately, we don’t have annotated data for query intent, but we do have access to large quantities of click logs. The logs distinguish four types of clicks: (a) search results, (b) ads, (c) spelling suggestions and (d) query suggestions. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. Click logs are huge sources of data, and while there are privacy concerns, anonymized logs are beginning to be released for research purposes (Craswell et al., 2009). Besides commercial intent, queries can also be divided into two broader classes: queries in which the user is browsing and queries for which the user is navigating. Clicks on the ads and query suggestions indicate that users are browsing and willing to look at these alternative suggestions, while clicks on the search results indicate that the users were navigating to what they were searching for. Clicks on typos indicate neither, as presumably the users are not entering typos on purpose. Just as dialogue management systems learn policies for when to allow user initiative (the user argmaxUi∈N</context>
</contexts>
<marker>Craswell, Jones, Dupret, Viegas, 2009</marker>
<rawString>N. Craswell, R. Jones, G. Dupret, and E. Viegas (Conference Chairs). 2009. Wscd ’09: Proceedings of the 2009 workshop on web search click data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H K Dai</author>
<author>L Zhao</author>
<author>Z Nie</author>
<author>J R Wen</author>
<author>L Wang</author>
<author>Y Li</author>
</authors>
<title>Detecting online commercial intention (OCI).</title>
<date>2006</date>
<pages>829--837</pages>
<publisher>WWW,</publisher>
<contexts>
<context position="6302" citStr="Dai et al., 2006" startWordPosition="1011" endWordPosition="1014">likely to be clicked on could help improve the user experience. In this paper we consider the task of: given a class of queries, which types of answer (standard search, ads, query suggestions, or spelling sug1429 gestions) are likely to be clicked on? Typos will tend to have more clicks on the spelling suggestions, informational queries will have more clicks on Wikipedia pages, and commercial queries will have more clicks on the ads. The observed behavior of where users click tells us something about the hidden intentions of the users when they issue that query. We focus on commercial intent (Dai et al., 2006), the intent to purchase a product or service, to illustrate our method of predicting query intent. The business model of web search today is heavily dependent on advertising. Advertisers bid on queries, and then the search results page also contains “sponsored” sites by the advertisers who won the auction for that query. It is thus advantageous for the advertisers to bid on queries which are most likely to result in a commercial transaction. If a query is classified as likely implying commercial intent, but the advertisers have overlooked this query, then the search engine may want to suggest</context>
<context position="10717" citStr="Dai et al. (2006)" startWordPosition="1730" endWordPosition="1733">on the expanded set. Random walk methods over the click graph have also been used to propagate relations between URLs, for tasks such as finding “adult” content (Craswell and Szummer, 2007) and suggesting related queries (Antonellis et al., 2008) and content (Baluja et al., 2008). In our work we also seek to classify query intent using the click graph, but we demonstrate the effectiveness of a simple method by building deci1430 sion lists of URLs. In addition, we evaluate our method automatically by using user click rates, rather than assembling hand-labeled examples for training and testing. Dai et al. (2006) also classified queries by commercial intent, but their method involved crawling the top landing pages for each query, which can be quite time-consuming. In this paper we investigate the commercial intent problem when crawling pages is not feasible, and use only the identities of the top URLs. 3 Using Click Logs as a Substitute for Annotation Prior work has used click logs in lieu of manual annotations of relevance ratings, either of webpages (Joachims, 2002) or of sponsored search advertisements (Ciaramita et al., 2008). Here we use the click logs as a large-scale source of intents. Logs fro</context>
</contexts>
<marker>Dai, Zhao, Nie, Wen, Wang, Li, 2006</marker>
<rawString>H.K. Dai, L. Zhao, Z. Nie, J.R. Wen, L. Wang, and Y. Li. 2006. Detecting online commercial intention (OCI). WWW, pages 829–837.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>S Ghemawat</author>
</authors>
<date>2004</date>
<booktitle>MapReduce: Simplified Data Processing on Large Clusters. OSDI,</booktitle>
<pages>137--149</pages>
<contexts>
<context position="12240" citStr="Dean and Ghemawat, 2004" startWordPosition="1986" endWordPosition="1989">queries of each type are shown in Table 1. As mentioned above, clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. The query, ebay official, is assumed to be commercial intent, because a large fraction of the clicks are on ads. In contrast, typos tend to have relatively more clicks on “did-you-mean” spelling suggestions. The query logs contain over a terabyte of data for each day, and our experiments were done using months of logs at a time. We used SCOPE (Chaiken et al., 2008), a scripting programming language designed for doing Map-Reduce (Dean and Ghemawat, 2004) style computations, to distribute the task of aggregating the counts of each query over thousands of servers. As the same query is often issued several times by multiple users across an entire month of search logs, we summarize each query with four ratios–search results clicks:overall clicks, ad clicks:overall clicks, spelling suggestion clicks:overall clicks, and query suggestion clicks:overall clicks. A couple of steps were taken to ensure reliable ratios. We are classifying types, not tokens, and so limited ourselves to those queries with 100 or more clicks. This still leaves us with over </context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>J. Dean and S. Ghemawat. 2004. MapReduce: Simplified Data Processing on Large Clusters. OSDI, pages 137–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>S Dumais</author>
<author>D Liebling</author>
<author>E Horvitz</author>
</authors>
<title>Understanding the relationship between searchers’ queries and information goals.</title>
<date>2008</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="8532" citStr="Downey et al. (2008)" startWordPosition="1371" endWordPosition="1374">ng relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at personalization and intent on a per user basis. Downey et al. (2008) use the last URL visited in a session or the last search engine result visited as a proxy for the user’s information goal, and then looked at the correspondence between information needs and queries (how the goals are expressed). We are interested in a granularity of intent in between navigational/informational/resourceseeking and personalized intents. For these sorts of intents, the web pages associated with queries provide useful information. To classify queries into an ontology of commercial queries, Broder et al. (2007) found that a classifier that used the text of the top result pages pe</context>
</contexts>
<marker>Downey, Dumais, Liebling, Horvitz, 2008</marker>
<rawString>D. Downey, S. Dumais, D. Liebling, and E. Horvitz. 2008. Understanding the relationship between searchers’ queries and information goals. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="7885" citStr="Joachims, 2002" startWordPosition="1279" endWordPosition="1280"> plenty of annotated data for testing and training. Unfortunately, since we don’t have as much annotated data as we might like, we propose two workarounds: 1. Use click logs as a substitute for annotated data. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. 2. We propose a method similar to Yarowsky (1995) to generalize beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at personalization and i</context>
<context position="11181" citStr="Joachims, 2002" startWordPosition="1810" endWordPosition="1811">e evaluate our method automatically by using user click rates, rather than assembling hand-labeled examples for training and testing. Dai et al. (2006) also classified queries by commercial intent, but their method involved crawling the top landing pages for each query, which can be quite time-consuming. In this paper we investigate the commercial intent problem when crawling pages is not feasible, and use only the identities of the top URLs. 3 Using Click Logs as a Substitute for Annotation Prior work has used click logs in lieu of manual annotations of relevance ratings, either of webpages (Joachims, 2002) or of sponsored search advertisements (Ciaramita et al., 2008). Here we use the click logs as a large-scale source of intents. Logs from Microsoft’s Live Search are used for training and test purposes. Logs from May 2008 were used for training, and logs from June 2008 were used for testing. The logs distinguish four types of clicks: (a) search results, (b) ads, (c) spelling suggestions and (d) query suggestions. Some prototypical queries of each type are shown in Table 1. As mentioned above, clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Lee</author>
<author>Z Liu</author>
<author>J Cho</author>
</authors>
<title>Automatic identification of user goals in Web search.</title>
<date>2005</date>
<booktitle>In WWW,</booktitle>
<pages>391--400</pages>
<contexts>
<context position="8263" citStr="Lee et al. (2005)" startWordPosition="1335" endWordPosition="1338">ze beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at personalization and intent on a per user basis. Downey et al. (2008) use the last URL visited in a session or the last search engine result visited as a proxy for the user’s information goal, and then looked at the correspondence between information needs and queries (how the goals are expressed). We are interested in a granularity of intent in between navigational/informational/resourceseeking a</context>
</contexts>
<marker>Lee, Liu, Cho, 2005</marker>
<rawString>U. Lee, Z. Liu, and J. Cho. 2005. Automatic identification of user goals in Web search. In WWW, pages 391–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>Y Y Wang</author>
<author>A Acero</author>
</authors>
<title>Learning query intent from regularized click graphs.</title>
<date>2008</date>
<booktitle>In SIGIR,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="9916" citStr="Li et al. (2008)" startWordPosition="1596" endWordPosition="1599">ually constructed about 150 hand-picked examples each for each of the 6000 intents. Beitzel et al. (2005) do semi-supervised learning over the query logs to classify queries into topics, but also train with hundreds of thousands of manually annotated queries. Thus, while we also use the query logs and the identities of web pages of associated with each query, we are interested in finding methods that can be applied when that much annotation is prohibitive. Semi-supervised methods over the click graph make it possible to train classifiers after starting from a much smaller set of seed queries. Li et al. (2008) used the semi-supervised learning method described in Zhou et al. (2004) to gain a much larger training set of examples, and then trained classifiers for product search or job search on the expanded set. Random walk methods over the click graph have also been used to propagate relations between URLs, for tasks such as finding “adult” content (Craswell and Szummer, 2007) and suggesting related queries (Antonellis et al., 2008) and content (Baluja et al., 2008). In our work we also seek to classify query intent using the click graph, but we demonstrate the effectiveness of a simple method by bu</context>
</contexts>
<marker>Li, Wang, Acero, 2008</marker>
<rawString>X. Li, Y.Y. Wang, and A. Acero. 2008. Learning query intent from regularized click graphs. In SIGIR, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Piwowarski</author>
<author>H Zaragoza</author>
</authors>
<title>Predictive user click models based on click-through history.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="7869" citStr="Piwowarski and Zaragoza, 2007" startWordPosition="1275" endWordPosition="1278"> well, especially when there is plenty of annotated data for testing and training. Unfortunately, since we don’t have as much annotated data as we might like, we propose two workarounds: 1. Use click logs as a substitute for annotated data. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. 2. We propose a method similar to Yarowsky (1995) to generalize beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at perso</context>
</contexts>
<marker>Piwowarski, Zaragoza, 2007</marker>
<rawString>B. Piwowarski and H. Zaragoza. 2007. Predictive user click models based on click-through history. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rela˜no</author>
<author>D Tapias</author>
<author>M Rodr´ıguez</author>
<author>M Charfuel´an</author>
<author>L Hern´andez</author>
</authors>
<title>Robust and flexible mixed-initiative dialogue for telephone services.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL.</booktitle>
<marker>Rela˜no, Tapias, Rodr´ıguez, Charfuel´an, Hern´andez, 1999</marker>
<rawString>J. Rela˜no, D. Tapias, M. Rodr´ıguez, M. Charfuel´an, and L. Hern´andez. 1999. Robust and flexible mixed-initiative dialogue for telephone services. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rose</author>
<author>D Levinson</author>
</authors>
<title>Understanding user goals in web search.</title>
<date>2004</date>
<pages>13--19</pages>
<publisher>WWW,</publisher>
<contexts>
<context position="2479" citStr="Rose and Levinson (2004)" startWordPosition="392" endWordPosition="395">rch engines. Queries are short (consisting of 2.2 terms on average (Beitzel et al., 2004)) and contain ambiguous terms. Search engines need to derive what users want from this limited source of information. Users may be searching for a specific page, browsing for information, or trying to buy something. Guessing the correct intent is important for returning relevant items. Someone searching for designer trench is likely to be interested in results or ads for trench coats, while someone searching for world war I trench might be irritated by irrelevant clothing advertisements. Broder (2002) and Rose and Levinson (2004) categorized queries into those with navigational, informational, and transactional or resourceseeking intent. Navigational queries are queries for which a user has a particular web page in mind that they are trying to navigate to, such as greyhound bus. Informational queries are those like San Francisco, in which the user is trying to gather information about a topic. Transactional queries are those like digital camera or download adobe reader, where the user is seeking to make a transaction or access an online resource. Knowing the intent of a query greatly affects the type of results that a</context>
</contexts>
<marker>Rose, Levinson, 2004</marker>
<rawString>D.E. Rose and D. Levinson. 2004. Understanding user goals in web search. WWW, pages 13–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Scheffler</author>
<author>S Young</author>
</authors>
<title>Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>12--19</pages>
<contexts>
<context position="23290" citStr="Scheffler and Young, 2002" startWordPosition="3873" endWordPosition="3876">er initiative (the user argmaxUi∈Nbr(q) 1433 (a) (b) Figure 3: Better together: proposed hybrid is no worse than baseline (left) and generalizes better to unseen tail queries (right). The two panels are the same, except that the training set was reduced on the right to test generalization error. Figure 4: Similar to Figures 3(a) and 3(b), adding the decision list method generalizes over the look-up method for the “suggestible” task. (a) (b) can respond in an open way) versus system initiative (the system asks the user questions with a restricted set of possible answers) (Rela˜no et al., 1999; Scheffler and Young, 2002; Singh et al., 2002), search engines may want to learn policies for when the user just wants the search results or when the user is open to suggestions. When users want help (they want the search engine to suggest results), more space on the page should be devoted to the ads and the query suggestions. When the users know what it is they want, more of the page should be given to the search results they asked for. We started with a simple baseline for predicting click location that had great precision, but didn’t generalize well beyond the queries in the training set. To improve recall, we prop</context>
</contexts>
<marker>Scheffler, Young, 2002</marker>
<rawString>K. Scheffler and S. Young. 2002. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proceedings of HLT, pages 12–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>D Litman</author>
<author>M Kearns</author>
<author>M Walker</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="23311" citStr="Singh et al., 2002" startWordPosition="3877" endWordPosition="3880">maxUi∈Nbr(q) 1433 (a) (b) Figure 3: Better together: proposed hybrid is no worse than baseline (left) and generalizes better to unseen tail queries (right). The two panels are the same, except that the training set was reduced on the right to test generalization error. Figure 4: Similar to Figures 3(a) and 3(b), adding the decision list method generalizes over the look-up method for the “suggestible” task. (a) (b) can respond in an open way) versus system initiative (the system asks the user questions with a restricted set of possible answers) (Rela˜no et al., 1999; Scheffler and Young, 2002; Singh et al., 2002), search engines may want to learn policies for when the user just wants the search results or when the user is open to suggestions. When users want help (they want the search engine to suggest results), more space on the page should be devoted to the ads and the query suggestions. When the users know what it is they want, more of the page should be given to the search results they asked for. We started with a simple baseline for predicting click location that had great precision, but didn’t generalize well beyond the queries in the training set. To improve recall, we proposed a context heuris</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>S. Singh, D. Litman, M. Kearns, and M. Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. Journal of Artificial Intelligence Research, 16(1):105–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Tan</author>
<author>Xuehua Shen</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Mining long-term search history to improve search accuracy.</title>
<date>2006</date>
<pages>718--723</pages>
<publisher>KDD.</publisher>
<contexts>
<context position="8068" citStr="Tan et al., 2006" startWordPosition="1304" endWordPosition="1307">ubstitute for annotated data. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. 2. We propose a method similar to Yarowsky (1995) to generalize beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at personalization and intent on a per user basis. Downey et al. (2008) use the last URL visited in a session or the last search engine result visited as a proxy for the user’s information goal, and then loo</context>
</contexts>
<marker>Tan, Shen, Zhai, 2006</marker>
<rawString>Bin Tan, Xuehua Shen, and ChengXiang Zhai. 2006. Mining long-term search history to improve search accuracy. pages 718–723. KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Teevan</author>
<author>S T Dumais</author>
<author>D J Liebling</author>
</authors>
<title>To personalize or not to personalize: modeling queries with variation in user intent.</title>
<date>2008</date>
<pages>163--170</pages>
<publisher>SIGIR,</publisher>
<marker>Teevan, Dumais, Liebling, 2008</marker>
<rawString>J. Teevan, S.T. Dumais, and D.J. Liebling. 2008. To personalize or not to personalize: modeling queries with variation in user intent. SIGIR, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gui-Rong Xue</author>
<author>Hua-Jun Zeng</author>
<author>Zheng Chen</author>
<author>Yong Yu</author>
<author>Wei-Ying Ma</author>
<author>WenSi Xi</author>
<author>WeiGuo Fan</author>
</authors>
<title>Optimizing web search using web clickthrough data. In</title>
<date>2004</date>
<booktitle>CIKM ’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="7904" citStr="Xue et al., 2004" startWordPosition="1281" endWordPosition="1284">ated data for testing and training. Unfortunately, since we don’t have as much annotated data as we might like, we propose two workarounds: 1. Use click logs as a substitute for annotated data. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. 2. We propose a method similar to Yarowsky (1995) to generalize beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing informational queries. Lee et al. (2005) used the click distributions to classify navigational versus informational intents. While navigational, informational, and resource-seeking are very broad intentions, other researchers have looked at personalization and intent on a per user</context>
</contexts>
<marker>Xue, Zeng, Chen, Yu, Ma, Xi, Fan, 2004</marker>
<rawString>Gui-Rong Xue, Hua-Jun Zeng, Zheng Chen, Yong Yu, Wei-Ying Ma, WenSi Xi, and WeiGuo Fan. 2004. Optimizing web search using web clickthrough data. In CIKM ’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 118–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<pages>88--95</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="16725" citStr="Yarowsky (1994)" startWordPosition="2760" endWordPosition="2761"> as the test month. The next section will propose an alternative method to address the main weakness of the baseline method, the inability to generalize beyond the queries in the training set. Figure 2: saks and bluefly trench coats are known to be commercial, while world war I trench is known to be non-commercial. What about designer trench? We can classify it as commercial because it shares URLs with the known commercial queries. 4.2 Method 2: Using Click Graph Context to Generalize Beyond the Queries in the Training Set To address the generalization concern, we propose a method inspired by Yarowsky (1994). Word sense disambiguation is a classic problem in natural language processing. Some words have multiple senses; for instance, bank can either mean a riverbank or a financial institution, and for various tasks such as information retrieval, parsing, or information extraction, it is useful to be able to differentiate between the possible meanings. When a word is being used in each sense, it tends to appear in a different context. For example, if the word muddy is nearby bank, the author is probably using the riverbank sense of the term, while if the word deposit is nearby, the word is probably</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. ACL, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<pages>189--196</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="7634" citStr="Yarowsky (1995)" startWordPosition="1239" endWordPosition="1240"> intent differently, by rearranging the appearance of the page, or by showing more or fewer advertisements. This paper starts with a simple Naive Bayes baseline to classify queries by intent (CQI). Supervised methods work well, especially when there is plenty of annotated data for testing and training. Unfortunately, since we don’t have as much annotated data as we might like, we propose two workarounds: 1. Use click logs as a substitute for annotated data. Clicks on ads are evidence for commercial intent; other types of clicks are evidence for other intents. 2. We propose a method similar to Yarowsky (1995) to generalize beyond the training set. 2 Related Work Click logs have been used for a variety of tasks involved in information retrieval, including predicting which pages are the best results for queries (Piwowarski and Zaragoza, 2007; Joachims, 2002; Xue et al., 2004), choosing relevant advertisements (Chakrabarti et al., 2008), suggesting related queries (Beeferman and Berger, 2000), and personalizing results (Tan et al., 2006). Queries that have a navigational intent tended to have a highly skewed click distribution, while users clicked on a wider range of results after issuing information</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. ACL, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhou</author>
<author>O Bousquet</author>
<author>T N Lal</author>
<author>J Weston</author>
<author>B Scholkopf</author>
</authors>
<title>Learning with Local and Global Consistency.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="9989" citStr="Zhou et al. (2004)" startWordPosition="1607" endWordPosition="1610">000 intents. Beitzel et al. (2005) do semi-supervised learning over the query logs to classify queries into topics, but also train with hundreds of thousands of manually annotated queries. Thus, while we also use the query logs and the identities of web pages of associated with each query, we are interested in finding methods that can be applied when that much annotation is prohibitive. Semi-supervised methods over the click graph make it possible to train classifiers after starting from a much smaller set of seed queries. Li et al. (2008) used the semi-supervised learning method described in Zhou et al. (2004) to gain a much larger training set of examples, and then trained classifiers for product search or job search on the expanded set. Random walk methods over the click graph have also been used to propagate relations between URLs, for tasks such as finding “adult” content (Craswell and Szummer, 2007) and suggesting related queries (Antonellis et al., 2008) and content (Baluja et al., 2008). In our work we also seek to classify query intent using the click graph, but we demonstrate the effectiveness of a simple method by building deci1430 sion lists of URLs. In addition, we evaluate our method a</context>
</contexts>
<marker>Zhou, Bousquet, Lal, Weston, Scholkopf, 2004</marker>
<rawString>D. Zhou, O. Bousquet, T.N. Lal, J. Weston, and B. Scholkopf. 2004. Learning with Local and Global Consistency. In NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>