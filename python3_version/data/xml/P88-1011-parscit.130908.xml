<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998406">
A Logic for Semantic Interpretation&apos;
</title>
<author confidence="0.995076">
Eugene Charniak and Robert Goldman
</author>
<affiliation confidence="0.995002">
Department of Computer Science
</affiliation>
<note confidence="0.57194">
Brown University, Box 1910
Providence RI 02912
</note>
<sectionHeader confidence="0.981597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989444444445">
We propose that logic (enhanced to encode probability
information) is a good way of characterizing semantic in-
terpretation. In support of this we give a fragment of
an axiomatization for word-sense disambiguation, noun-
phrase (and verb) reference, and case disambiguation.
We describe an inference engine (Frail3) which actually
takes this axiomatization and uses it to drive the semantic
interpretation process. We claim three benefits from this
scheme. First, the interface between semantic interpreta-
tion and pragmatics has always been problematic, since
all of the above tasks in general require pragmatic infer-
ence. Now the interface is trivial, since both semantic
interpretation and pragmatics use the same vocabulary
and inference engine. The second benefit, related to the
first, is that semantic guidance of syntax is a side effect
of the interpretation. The third benefit is the elegance
of the semantic interpretation theory. A few simple rules
capture a remarkable diversity of semantic phenomena.
</bodyText>
<sectionHeader confidence="0.981498" genericHeader="method">
I. Introduction
</sectionHeader>
<bodyText confidence="0.990679904761905">
The use of logic to codify natural language syntax is well
known, and many current systems can parse directly off
their axiomatizations (e.g.,)[1]. Many of these systems
simultaneously construct an intermediate &amp;quot;logical form&amp;quot;
using the same machinery. At the other end of language
processing, logic is a well-known tool for expressing the
pragmatic information needed for plan recognition and
speech act recognition [2-4]. In between these extremes
logic appears much less. There has been some movement
in the direction of placing semantic interpretation on a
more logical footing [5,6], but it is nothing like what has
happened at the extremes of the language understanding
process.
To some degree this is understandable. These &amp;quot;mid-
dle&amp;quot; parts, such as word-sense disambiguation, noun
phrase reference, case disambiguation, etc. are notori-
ously difficult, and poorly understood, at least compared
to things like syntax, and the construction of interme-
diate logical form. Much of the reason these areas are
This work has been supported in part by the National Science
Foundation under grants 1ST 8416034 and 1ST 8515005 and Office
</bodyText>
<subsectionHeader confidence="0.645541">
of Naval Research under grant N00014-79-C-0529. 87
</subsectionHeader>
<bodyText confidence="0.999911392857143">
so dark is that they are intimately bound up with prag-
matic reasoning. The correct sense of a word depends on
context, as does pronoun resolution, etc.
Here we rectify this situation by presenting an ax-
iomatization of fragment of semantic interpretation, no-
tably including many aspects previously excluded: word-
sense disambiguation, noun-phrase reference determina-
tion, case determination, and syntactic disambiguation.
Furthermore we describe an inference engine, Frail3,
which can use the logical formulation to carry out seman-
tic interpretation. The description of Frail3 is brief, since
the present paper is primarily concerned with semantic
interpretation. For a more detailed description, see [7].
The work closest to what we present is that by Hobbs
[5]; however, he handles only noun-phrase reference from
the above list, and he does not consider intersentential
influences at all.
Our system, Wimp2 (which uses Frail3), is quite
pretty in two respects. First, it integrates semantic and
pragmatic processing into a uniform whole, all done in
the logic. Secondly, it provides an elegant and concise
way to specify exactly what has to be done by a seman-
tic interpreter. As we shall see, a system that is roughly
comparable to other state-of-the-art semantic interpreta-
tion systems [6,8] can be written down in a page or so of
logical rules.
Wimp2 has been implemented and works on all of
the examples in this paper.
</bodyText>
<sectionHeader confidence="0.909544" genericHeader="method">
II. Vocabularies
</sectionHeader>
<bodyText confidence="0.98905735">
Let us start by giving an informal semantics for the spe-
cial predicates and terms used by the system. Since we
are doing semantic interpretation, we are translating be-
tween a syntactic tree on one hand and the logical, or in-
ternal, representation on the other. Thus.we distinguish
three vocabularies: one for trees, one for the internal rep-
resentation, and one to aid in the translation between the
two.
The vocabulary for syntactic trees assumes that each
word in the sentence is represented as a word instance
which is represented as a word with a numerical post-
fix (e.g., boy22). A word instance is associated with the
actual lexical entry by the predicate word-inst:
(word-inst word-Instance part-of-speech lexical-item).
For example, (word-inst case26 noun case). (We use &amp;quot;part
of speech&amp;quot; to denote those syntactic categories that. are
directly above the terminal symbols in the grammars,
that. is, directly above words.)
The relations between word instances are encoded
with two predicates: syn-pos, and syn-pp. Syn-pos
</bodyText>
<subsectionHeader confidence="0.620165">
(syn-pos relation head sub-constituent),
</subsectionHeader>
<bodyText confidence="0.999322181818182">
indicates that the sub-constituent is the relation of the
head. We distinguish between positional relations and
those indicated by prepositional phrases, which use the
predicate syn-pp, but otherwise look the same. The
propositions denoting syntactic relations are generated
during the parse. The parser follows all possible parses
in a breadth-first search and outputs propositions on a
word-by-word basis. If there is more than one parse and
they disagree on the propositional output, a disjunction
of the outputs is asserted into the database. The &apos;corre-
spondence between trees and formulas is as follows:
</bodyText>
<subsectionHeader confidence="0.902527">
Trees Formulas
</subsectionHeader>
<table confidence="0.991994090909091">
s — up (vp ... head-v (syn-pos subject head-v np)
head-v symbol is s symbol
vp — ... head-v np ... (syn-pos object head-v np)
vp — ... head-v npl (syn-pos indirect-object
np2 ... head-v npl)
(syn-pos object head-v np2)
vp —. ... head-v ... (syn-pp head-prep head-v
(pp prep ...) MP)
pp — prep np (.yn-pp prep-np prep np)
np — ... head-n ... head-n symbol is np symbol
up — pronoun pronoun symbol is
np symbol
np — propernoun propernoun symbol is
np symbol
np — ... adj head-n ... (syn-pos adj adj head-n)
np — ... head-n ... (syn-pp head-prep head-n
(pp prep ...) prep)
up — that s s symbol is np symbol
s — np (vp ... copula (syn-pp head-prep np prep)
(PP Prep ...))
s — lip (vp ... copula (syn-pos adj ad) np)
adj)
</table>
<tableCaption confidence="0.5529951">
This is enough to express a wide variety of simple declar-
ative sentences. Furthermore, since our current parser
implements a transformational account of imperatives,
questions (both yes-no and wh), complement construc-
tions, and subordinate clauses, these are automatically
handled by the above as well. For example, given an ac-
count of &amp;quot;Jack wants to borrow the book.&amp;quot; as derived
from &amp;quot;Jack wants (np that (s Jack borrow the book)).&amp;quot;
or something similar, then the above rules would produce
the following for both (we also indicate after what word
</tableCaption>
<table confidence="0.98115275">
the formula is produced): 88
Words Formulas
Jack (word-inst jackl propernoun jack)
wants (word-inst wantl verb want)
(syn-pos subject wantl jackl)
to
borrow (word-inst borrowl verb borrow)
(syn-pos object wantl borrowl)
(syn-pos subject borrow1 jack 1)
the
book (word-inst bookl noun book)
(syn-pos object borrowl book 1)
</table>
<tableCaption confidence="0.541552333333333">
This &apos;s, of course, a fragment, and most things are not
handled by this analysis: negation, noun-noun combina-
tions, particles, auxiliary verbs, etc.
</tableCaption>
<bodyText confidence="0.92265325">
Now let us consider the internal representation used
for inference about the world. Here we use a simple
predicate-calculus version of frames and slots. We as-
sume only two predicates for this: == and inst. Inst.
</bodyText>
<subsectionHeader confidence="0.671186">
(inst instance frame),
</subsectionHeader>
<bodyText confidence="0.9861299375">
is a two-place predicate on an instance of a frame and
the frame itself, where a &amp;quot;frame&amp;quot; is a set of objects, all
of which are of the same natural kind. Thus (Inst boyl
boy-) asserts that boyl is a member of the set of boys, de-
noted by boy-. (Frames are symbols containing hyphens,
e.g., supermarket-shoping. Where a single English word is
sufficiently descriptive, the hyphen is put at the end.)
The other predicate used to describe the world is the
&amp;quot;better name&amp;quot; relation ==:
(== worse-name better-name).
This is a restricted use of equality. The second argument
is a &amp;quot;better name&amp;quot; for the first, and thus may be freely
substituted for it (but not the reverse). Since slots are
represented as functions, == is used to fill slots in frames.
To fill the agent slot of a particular action, say borrowl,
with a particular person, say jackl, we say
(== (agent borrowl) jackl).
At an implementation level, == causes everything known
about its first argument (the worse name) to be asserted
about the second (the better name). This has the effect
of concentrating all knowledge about all of an object&apos;s
names as facts about the best name.
Frail will take as input a simple frame representation
and translate it into predicate-calculus form. Figure 1
shows a frame for shopping along with the predicate-
calculus translation.
Naturally, a realistic world model requires more than
these two predicates plus slot functions, but the relative
success of fairly simple frame models of reasoning indi-
cates that they are a good starting set. The last set of
predicates (word-sense, case, and role-inst) are used in the
translation itself. They will be defined later.
</bodyText>
<figure confidence="0.609665461538462">
(defframe shop-
isa action
;(inst ?s.shop- action)
slots (agent (person-))
.(inst. (agent. ?s.shop-) person-)
(store-of (store-))
;(inst (store-of ?s.shop-) store-)
acts (go-step
(go- (agent (agent ?shop-))
(destination (store-of ?shop-))))
;(== (agent (go-step ?shop-)) (agent ?shop-))
;(== (destination (go-step ?s.shop-))
(store-of ?s.shop-))
</figure>
<figureCaption confidence="0.999617">
Figure 1: A frame for shopping
</figureCaption>
<sectionHeader confidence="0.851526" genericHeader="method">
III. Word-Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999908714285714">
We can now write down some semantic&apos; interpretation
rules. Let us assume that all words in English have one or
more word senses as their meaning, that these word senses
correspond to frames, and that any particular word in-
stance has as its meaning exactly one of these senses. We
can express this fact for the instances of any particular
lexical entry as follows:
</bodyText>
<equation confidence="0.531095">
(word-inst inst part-of-speech word)
(inst znst sensei)V ... V (inst inst sense)
</equation>
<bodyText confidence="0.991897722222222">
where sensei through sense n are senses of word when it
is used as a part-of-speech (i.e., as a noun, verb, etc.)
Not all words in English have meanings in this sense.
&amp;quot;The&amp;quot; is an obvious example. Rather than complicate
the above rules, we assign such words a &amp;quot;null&amp;quot; mean-
ing, which we represent by the term garbage*. Nothing
is known about garbage* so this has no consequences.
A better axiomatization would also include words which
seem to correspond to functions (e.g., age), but we ignore
such complications.
A minor problem with the above rule is that it re-
quires us to be able to say at the outset (i.e., when we
load the program) what all the word senses are, and new
senses cannot be added in a modular fashion. To fix this
we introduce a new predicate, word-sense:
(word-sense let- item part-of-speech frame)
(word-sense straw noun drink-straw)
(word-sense straw noun animal-straw).
This states that ler-item when used as a part-of-speech
can mean frame.
We also introduce a pragmatically different form of
disjunction, —OR:
(-00R formulai formula2).
In terms of implementation, think of this as inferring
f ormulai in all possible ways and then asserting the dis-
junction of the f ormula2s with each set of bindings. So if
there are two sets of bindings, the result will be to assert 89
(OR formula/bindings&apos; formula2 1 binding s2).
Logically, the meaning of —.OR is that if x xn are
unbound variables in formu/a1, then there must exist x1
xn that make formula&apos; and formula2 true.
We can now express our rule of word-sense ambiguity
as:
(word-inst ?instance ?part-of-speech ?lex-item)
(—OR (word-sense ?lex-item ?part-of-speech ?frame)
(inst ?instance ?frame))
</bodyText>
<sectionHeader confidence="0.559205" genericHeader="method">
IV. The Inference Engine
</sectionHeader>
<bodyText confidence="0.997917782608696">
While it seems clear that the above rule expresses a rather
simple-minded idea of how words relate to their mean-
ings, its computational import may not be so clear. Thus
we now discuss Wimp2, our language comprehension pro-
gram, and its inference engine, Frail3.
Like most rule-based systems, Frail distinguishes for-
ward and backward-chaining use of modus-ponens. All
of our semantic interpretation rules are forward-chaining
rules:
(word-inst ?instance ?part-of-speech ?lex-item)
(OR (word-sense ?lex-item ?part-of-speech ?frame)
(inst ?instance ?frame)))
Thus, whenever a new word instance is asserted, we
forward-chain to a statement that the word denotes an
instance of one of a set of frames.
Next, Frail uses an ATMS [9,10] to keep track of
disjunctions. That is, when we assert (OR formulai
...formulas) we create n assumptions (following DeK-
leer, these are simply integers) and assert each formula
into the data-base, each with a label indicating that the
formula is not true but only true given some assumptions.
Here is an example of how some simple disjunctions come
out.
</bodyText>
<figure confidence="0.983353875">
A (—■ A (OR B C))
(-0 B (OR D E))
Formulas Assumptions Labels
A (0)
B 1 ((1))
C 2 ((2))
D 3 ((13))
E 4 ((14))
</figure>
<figureCaption confidence="0.882125">
Figure 2 represents this pictorially. Here D, for example,
</figureCaption>
<bodyText confidence="0.9295837">
has the label ((13)), which means that it is true if we grant
assumptions 1 and 3. If an assumption (or more gener-
ally, a set of assumptions) leads to a contradiction, the
assumption is declared a &amp;quot;nogood&amp;quot; and formulas which
depend on it are no longer believed. Thus if we learn (not
D) then (1 3) is a nogood. This also has the consequence
that E now has the label (1). It is as if different sets
of assumptions correspond to different worlds. Seman-
tic interpretation then is finding the &amp;quot;best&amp;quot; of the worlds
defined by the linguistic possibilities.
</bodyText>
<figureCaption confidence="0.990866">
Figure 2: Pictorial representation of disjunctions
</figureCaption>
<bodyText confidence="0.9997515">
We said &amp;quot;best&amp;quot; in the last sentence deliberately.
When alternatives can be ruled out on logical grounds the
corresponding assumptions become nogoods, and conclu-
sions from them go away. But it is rare that. all of the can-
didate interpretations (of words, of referents, etc.) reduce
to only one that is logically possible. Rather, there are
usually several which are logically consistent, but some
are more &amp;quot;probable&amp;quot; than others. For this reason, Frail
associates probabilities with sets of assumptions (&amp;quot;alter-
native worlds&amp;quot;) and Wimp eventually &amp;quot;garbage collects&amp;quot;
statements which remain low-probability alternatives be-
cause their assumptions are unlikely. Probabilities also
guide which interpretation to explore. Exactly how this
works is described in [7]. Here we will simply note that
the probabilities are designed to capture the following
intuitions:
</bodyText>
<listItem confidence="0.9989424375">
1. Uncommon vs. common word-senses (marked vs.
unmarked) are indicated by probabilities input by
the system designer and stored in the lexicon.
2. Wimp prefers to find referents for entities (rather
than not finding referents).
3. Possible reasons for actions and entities are preferred
the more specific they are to the action or entity.
(E.g., &amp;quot;shopping&amp;quot; is given a higher probability than
&amp;quot;meeting someone&amp;quot; as an explanation for going to
the supermarket.)
4. Formulas derived in two differents ways are more
probable than they would have been if derived in
either way alone.
5. Disjunctions which lead to already considered
-worlds&amp;quot; are preferred over those which do not hook
up in this way. (We will illustrate this later.)
</listItem>
<sectionHeader confidence="0.466911" genericHeader="method">
V. Case Disambiguation
</sectionHeader>
<bodyText confidence="0.896118475">
Cases are indicated by positional relations (e.g., subject)
and prepositional phrases. We make the simplifying as-
sumption that prepositional phrases only indicate case
relations. As we did for word-sense disambiguation, we
introduce a new predicate that allows us to incrementally
specify how a particular head (a noun or verb) relates to
its syntactic roles. The new predicate,
(case head syntactic-relation slot), 90
states that head can have its slot filled by things which
stand in syntactic-relation to it. For example
(inst ?g go-) (case ?g subject agent).
This can also be expressed in Frail using the typed vari-
ables
(case ?g.go- subject agent).
This says that any instance of a go- can use the subject
position to indicate the agent of the go- event. These facts
can be inherited in the typical way via the isa hierarchy,
so this fact would more generally be expressed as
(case ?a.action- subject agent).
Using case and the previously introduced —OR connec-
tive, we can express the rule of case relations. Formally,
it says that for all syntactic positional relations and all
meanings of the head, there must exist a case relation
which is the significance of that syntactic position:
(syn-pos ?rel ?head ?val) A (inst ?head ?frame)
(—OR (case ?head ?rel ?slot)
(== (?slot ?head) ?val))).
So, we might have
(syn-pos subject gol jackl) A (inst gol go-)
A (case gol subject agent)
(== (agent gol) jackl).
A similar rule holds for case relations indicated by
prepositional phrases.
(syn-pp head-prep ?head ?pinst)
A (syn-pp prep-np ?pinst ?np)
A (word-inst ?pinst prep ?prep) A (inst ?head ?frame)
(--*OR (case ?head ?prep ?slot)
(== (?slot ?head) ?np))
For example, &amp;quot;Jack went to the supermarket.&amp;quot; would
give us
(syn-pp head-prep gol tol) A (case gol to destination)
A (syn-pp prep-np tol supermarketl)
A (word-inst tol prep to) A (inst gol go-)
(== (destination gol) supermarketl).
We now have enough machinery to describe two ways
in which word senses and case relations can help disam-
biguate each other. First consider the sentence
Jack went to the supermarket.
Wimp currently knows two meanings of &amp;quot;go,&amp;quot; to travel
and to die. After &amp;quot;Jack went&amp;quot; Wimp prefers travel (based
upon probability rule 1 and the probabilities assigned to
these two readings in the lexicon) but both are possible.
After &amp;quot;Jack went to&amp;quot; the die reading goes away. This is
because the only formulas satisfying
(case gol to ?slot)
all require gol to be a travel rather than a die. Thus
&amp;quot;die&amp;quot; cannot be a reading since it makes
(—OR (case ?head ?prep ?slot)
(== (?slot ?head) ?val))
A &lt;
—
false (a disjunction of zero disjuncts is false).
We also have enough machinery to see how &amp;quot;selec-
tional restrictions&amp;quot; work in Wimp2. Consider the sen-
tence
Jack fell at the store.
and suppose that Wimp knows two case relations for &amp;quot;at,&amp;quot;
loc and time. This will initially lead to the following
disjunction:
(== (loc fe111) storel)
(== (time fe111) storel).
However, Wimp will know that
(inst (time ?a.action) time-).
As we mentioned earlier, == statements cause everything
known about the first argument 1.0 be asserted about the
second. Thus Wimp will try to believe that. storel is a
time, so (2) becomes a nogood and (1) becomes just. true.
It is important to note that both of these disam-
biguation methods fall out from the basics of the system.
Nothing had to be added.
</bodyText>
<sectionHeader confidence="0.529076" genericHeader="method">
VI. Reference and Explanation
</sectionHeader>
<bodyText confidence="0.996590953488372">
Definite noun phrases (np&apos;s) typically refer to something
already mentioned. Occasionally they do not, however,
and some, like proper names may or may not refer to
an already mentioned entity. Let us simplify by saying
that all np&apos;s may or may not refer to something already
mentioned. (We will return to indefinite np&apos;s later.) We
represent np&apos;s by always creating a new instance which
represents the entity denoted by the np. Should there be
a referent we assert equality between the newly minted
object and the previously mentioned one. Thus, in &amp;quot;Jack
went to the supermarket. He found some milk on the
shelf.&amp;quot;, the recognition that &amp;quot;He&amp;quot; refers to Jack would be
indicated by
(== he24 jack3).
(Remember that == is a best name relation, so this says
that jack3 is a better name for the new instance we cre-
ated to represent the &amp;quot;he,&amp;quot; he24.)
As for representing the basic rule of reference, the
idea is to see the call for a referent as a statement that
something exists. Thus we might try to say
(inst ?x ?frame) (Exists (y \ ?frame) (== ?x ?y)).
This is intended to say, if we are told of an object of type
?frame then there must exist an earlier one y of this same
type to which the new one can be set equal.
The trouble with this formula is that it does not say
&amp;quot;earlier one.&amp;quot; Exists simply says that there has to be one,
whether or not it was mentioned. Furthermore, since we
intend to represent an np like &amp;quot;the taxi- by (inst taxi27
taxi-) and then look for an earlier taxi. the Exists would
be trivially satisfied by taxi27 itself.
Our solution is to introduce a new quantifier called
&amp;quot;previously exists&amp;quot; or PExists. (In [5] a similar end is
achieved by putting weights on formula and looking for
a minimum-weight proof.) Using this new quantifier. we
have
(inst ?x ?frame) (PExists (y \ ?frame) (== ?x ?y)).
If there is more than one a disjunction of equality state-
ments is created. For example, consider the story
Jack went to the supermarket. He found the
milk on the shelf. He paid for it.
The &amp;quot;it&amp;quot; in the last sentence could refer to any of the three
inanimate objects mentioned, so initially the following
disjunction is created:
</bodyText>
<equation confidence="0.403944666666667">
(== it8 shelf6)
(inst it8 inanimate-)&lt;(== it8 milk5)
• (== it8 supermarket2).
</equation>
<bodyText confidence="0.999755272727273">
This still does not allow for the case when there is
no referent for the np. To understand our solution to this
problem it is necessary to note that we originally set out
to create a plan-recognition system. That is to say, we
wanted a program which given a sentence like &amp;quot;Jack got
a rope. He wanted to kill himself.&amp;quot; would recognize that
Jack plans to hang himself. We discuss this aspect of
Wimp2 in greater detail in [7]. Here we simply note that
plans in Wimp2 are represented as frames (as shown in
Figure 1.) and that sub tasks of plans are actions which
fill certain slots of the frame. So the shop- plan has a
go-step in Figure 1. and recognizing the import of &amp;quot;Jack
went to the supermarket.&amp;quot; would be to infer that (==
(go-step shop-74) go61) where go61 represented the verb
in &amp;quot;Jack went to the supermarket.&amp;quot; We generalize this
slightly and say that all inputs must be &amp;quot;explained&amp;quot;; by
this we mean that we must find (or postulate) a frame
in which the input fills a slot. Thus the go-step state-
ment explains go61. The presence of a supermarket in the
story would be explained by (== (store-of shop-74) super-
market64). The rule that everything mentioned must be
explained looks like this:
</bodyText>
<equation confidence="0.765995333333333">
(inst?x ?frame)
(role-inst ?x ?slot ?superfrm)
(Exists (y \ ?superfrm) (== (?slot ?y) ?x))).
</equation>
<bodyText confidence="0.998074666666667">
(Some things cannot be explained, so this rule is not
strict.) Here the role-inst predicate says that ?x can
fill the ?slot role of the frame ?superfrm. E.g., (role-inst
?r.store- store-of shop-) says that stores can fill the store-
of slot in the shop- frame. Here we use Exists, not PExists
since, as in the rope example, we explained the existence
of the rope by postulating a new hanging event. The se-
mantics of Exists is therefore quite standard, simply say-
ing that one must exist, and making no commitment to
whether it was mentioned earlier or not. As a matter of
implementation, we note that it works simply by always
(syn-pp head-prep felll at1)&lt;
</bodyText>
<page confidence="0.997259">
91
</page>
<bodyText confidence="0.999751769230769">
creating a new instance. The impact of this will be seen
in a moment.
We said that. all inputs must be explained, and that
we explain by seeing that the entity fills a slot in a pos-
tulated frame. There is one exception to this. If a newly
mentioned entity refers to an already extant one, then
there is no need to explain it, since it was presumably
explained the first. time it was seen. Thus we combine
our rule of reference with our rule of explanation. Or, to
put it. slightly differently, we handle the exceptions to the
rule of reference (some things do not refer to entities al-
ready present) by saying that those which do not so refer
must be explained instead. This gives the following rule:
</bodyText>
<equation confidence="0.845326">
(inst ?x ?frame) A (not (= ?frame garbage*))
(OR (PExists (y \ ?frame) (== ?x ?y)) .9
(—.0R (role-inst ?x ?superfrm ?slot)
(Exists (s \ ?superfrm)
(== (?slot ?s) ?x))) .1)
</equation>
<bodyText confidence="0.9938986">
Here we added the restriction that the frame in question
cannot be the garbage* frame, which has no properties by
definition. We have also added probabilities to the dis-
junctions that are intended to capture the preference for
previously existing objects (probability rule 2). The rule
of reference has several nice properties. First, it might
seem odd that our rule for explaining things is expressed
in terms of the Exists quantifier, which we said always cre-
ates a new instance. What about a case like &amp;quot;Jack went
to the supermarket. He found the milk on the shelf.&amp;quot;
where we want to explain the second line in terms of the
shopping plan created in the first? As we have things set
up, it simply creates a new shopping plan. But note what
then occurs. First the system asserts (inst new-shopping5
shopping-). This activates the above rule, which must ei-
ther find a referent for it, or try to explain it in terms
of a frame for which it fills a role. In this case there is a
referent, namely the shopping created in the course of the
first line. Thus we get (== new-shopping5 shopping4) and
we have the desired outcome. This example also shows
that the reference rule works on event reference, not just
np reference.
This rule handles reference to &amp;quot;related objects&amp;quot;
rather well. Consider &amp;quot;Jack wanted to play the stereo.
He pushed the on-off button.&amp;quot; Here &amp;quot;the on-off button&amp;quot;
is to be understood as the button &amp;quot;related&amp;quot; to the stereo
mentioned in the first line. In Wimp this falls out from
the rules already described. Upon seeing &amp;quot;the on-off but-
ton&amp;quot; Wimp creates a new entity which must then either
have a referent or an explanation. It does not have the
first, but one good explanation for the presence of an on-
off button is that it fills the on-off-switch slot for some
power-machine. Thus Wimp creates a machine and the
machine then has to be explained. In this case a referent
is found, the stereo from the first sentence.
</bodyText>
<sectionHeader confidence="0.844388" genericHeader="method">
VII. Pragmatic Influence
</sectionHeader>
<bodyText confidence="0.996831461538462">
We finish with three examples illustrating how our se-
mantic interpretation process easily integrates pragmatic
influences: one example of pronoun reference, one of
word-sense disambiguation, and one of syntactic ambi-
guity. First pronoun reference:
Jack went. to the supermarket. He found the
milk on the shelf. He paid for it.
In this example the &amp;quot;milk&amp;quot; of sentence two is seen as the
purchased of shop-1 and the &amp;quot;pay&amp;quot; of sentence three is
postulated to be the pay-step of a shopping event, and
then further postulated to be the same shopping event as
that created earlier. (In each case other possibilities will
be considered, but their probabilities will be much lower.)
Thus when &amp;quot;it&amp;quot; is seen Wimp is in the situation shown in
Figure 3. The important thing here is that the statement
(== it7 milk5) can be derived in two different ways, and
thus its probability is muchligher than the other possible
referents for &amp;quot;it&amp;quot; (probability rule 4). (One derivation has
it that since one pays for what one is shopping for, and
Jack is shopping for milk, he mist be paying for the milk.
The other derivation is that &amp;quot;it&amp;quot; must refer to something,
and the milk is one alternative.)
The second example is one of word-sense disam-
biguation:
Jack ordered a soda. He picked up the straw.
Here sentence one is seens as the order-step of a newly
postulated eat-outl. The soda suggests a drinking event,
which in turn can be explained as the eat-step of eat-
outl. The straw in line two can be one of two kinds of
straw, but the drink-straw interpretation suggests (via a
role-inst statement) a straw-drinking event. This is postu-
lated, and Wimp looks for a previous such event (using
the normal reference rule) and finds the one suggested
by the soda. Wimp prefers to assume that the drink-
ing event suggested by &amp;quot;soda&amp;quot; and that from &amp;quot;straw&amp;quot; are
the same event (probability rule 2) and this preference
is passed back to become a preference for the drink-straw
meaning of &amp;quot;straw&amp;quot; (by probability rule 5). The result is
shown in Figure 4.
Our third and last example shows how semantic
guidance of syntax works:
Janet wanted to kill the boy with some poison.
Starting with the &amp;quot;with&amp;quot; there are two parses which dis-
agree on the attachment of the prepositional phrase (pp).
There are also two case relations the &amp;quot;with&amp;quot; can indi-
cate if it modifies &amp;quot;kill,&amp;quot; instrument and accompaniment.
When Wimp sees &amp;quot;poison&amp;quot; it looks for an explanation of
its presence, postulates a poisoning and which is found
to be potentially coreferential with the &amp;quot;kill.&amp;quot; The result
looks like Figure 5. In this interpretation the poison can
be inferred to be the instrument, of the poisoning, so this
option has higher probability (probability rule 4). This
</bodyText>
<page confidence="0.868928">
92
</page>
<figure confidence="0.777389">
Other alternatives
(inst pay7 pay-)
== (pay-step shop-1) pay7) (== it8 milk5)
(inst it8 inanimate-) . it8 shelf6)
. it9 supermarket2)
</figure>
<figureCaption confidence="0.953523">
Figure 3: A pronoun example
</figureCaption>
<figure confidence="0.969184">
Other alternatives
(inst order2 order-)
</figure>
<figureCaption confidence="0.994421">
Figure 4: A word-sense example
</figureCaption>
<figure confidence="0.999594428571429">
-4,(= (order-step eat-out1) order2)
(= (patient drink3) soda4)
(inst soda4 soda-)
(— (eat-step eat-outl) drink3)
Other alternatives
(— (straw-of drink3) straw3)
(word-inst straw3 noun stew)
(inst straw3 animal-straw) I
(syn-pp head-prep I Accompany
boy1 with1)
the boy with
(syn-pp head-prep
kill1 with1)
Instrument Instr ki111) poison4)
</figure>
<figureCaption confidence="0.9092275">
(inst poison4 poison-) Other alternatives
Figure 5: A syntactic disambiguation example
</figureCaption>
<page confidence="0.935098">
93
</page>
<bodyText confidence="0.999980549019608">
higher probability is passed back to the disjuncts repre- 94 (syn-pos metonymy ?intended ?given)
senting a) the choice of instrument over accompanyment, (OR (== ?intended ?given) .9
and b) the choice of attaching to &amp;quot;kill&amp;quot; over &amp;quot;boy&amp;quot; (prob- (== (creator-of ?intended) ?given) .02)
ability rule 5). This last has the effect of telling the parser --)).
where to attach the pp. This rule would prefer assuming that the two individuals
VIII. Future Research are the same, but would allow other possibilities.
This work can be extended in many ways: increased syn- IX. Conclusion
tactic coverage, more realistic semantic rules, improved We have presented logical rules for a fragment of the
search techniques for possible explanations, etc. Here we semantic interpretation (and plan recognition) process.
will simply look at some fairly straightforward extensions The four simple rules we gave already capture a wide
to the model. variety of semantic and pragmatic phenomena. We are
Our rule preferring finding a referent to not finding a currently working on diverse aspects of semantics, such
referent is not reasonable for indefinite np&apos;s. Thus Wimp as definite vs. indefinite np&apos;s, noun-noun combinations,
currently misinterprets adjectives, non-case uses of prepositions, metonymy and
Jack bought a gun. Mary bought a gun. relative clauses.
since it wants to interpret the second gun as coreferen- References
tial with the first. A simple change would be to have [1] F. Pereira 8,-, D. Warren, &amp;quot;Definite clause grammar
two rules of reference/explanation. The rule for indefi- for language analysis - a survey of the formalism and
nite np&apos;s would look like this: a comparison with augmented transition networks,&amp;quot;
(Inst ?x ?frame) A (not (= ?frame garbage*)) Artificial Intelligence 13 (1980), 231-278.
A (syn-pos indef-det ?x ?det) [2] Philip R. Cohen Az C. Raymond Perrault, &amp;quot;Elements
=. (OR (PExists (y \ ?frame) (== ?x ?y)) .1 of a plan-based theory of speech acts,&amp;quot; Cognitive Sci-
(role-inst ?x ?superfrm ?slot) ence 3(1979), 177-212.
(Exists (s \ ?superfrm) [3] Eugene Charniak, &amp;quot;A neat theory of marker passing,&amp;quot;
(== (?slot ?s) 7x))) .9) AAAI-86 (1986).
This looks just like our earlier rule, except a check for [4] Henry Kautz lz James Allen, &amp;quot;Generalized plan recog-
an intlefinite determiner is added, and the probabilities nition,&amp;quot; AAAI-86 (1986).
are reversed so as to prefer a new object over an already [5] Jerry R. Hobbs Sz Paul Martin, &amp;quot;Local pragmatics,&amp;quot;
existing one. The earlier reference rule would then be Zicai-87 (1987).
modified to make sure that the object did not have an [6] Graeme Hirst, Semantic Interpretation and the Res-
indefinite determiner. olution of Ambiguity, Cambridge University Press,
Another aspect of language which fits rather nicely Cambridge, 1987.
into this framework is metonymy. We have already noted [7] Robert Goldman 8z Eugene Charniak, &amp;quot;A probabilis-
that the work closest to ours is [5], and in fact we can tic ATMS for plan recognition,&amp;quot; forthcomming.
adopt the analysis presented there without a wrinkle. [8] Barbara J. Grosz, Douglas E. Appelt, Paul A. Mar-
This analysis assumes that every np corresponds to two tin St Fernando C.N. Pereira, &amp;quot;Team: an experiment
objects in the story, the one mentioned and the one in- in the design of transportable natural-language inter-
tended. For example: faces,&amp;quot; Artificial Intelligence 32 (1987), 173-243.
I read Proust over summer vacation. [9] Drew V. McDermott, &amp;quot;Contexts and data depen-
The two objects are the entity literally described by the dencies: a synthesis,&amp;quot; IEEE Transactions on Pattern
np (here the person &amp;quot;Proust&amp;quot;) and that intended by the Analysis and Machine Intelligence PAMI-5 (1983).
speaker (here a set of books by Proust). The syntactic [10] Johan deKleer, &amp;quot;An assumption-based TMS,&amp;quot; Artifi-
analysis would be modified to produce the two objects, cial Intelligence 28 (1986), 127-162.
here proustl and read-objl respectively.
(syn-pos direct-object readl read-0Ni)
(word-inst proustl propernoun Proust)
(syn-pos metonymy read-objl proustl)
It is then assumed that there are a finite number of
relations that may hold between these two entities, most
notably equality, but others as well. The rule relating the
two entities would look like this:
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839515">
<title confidence="0.99963">A Logic for Semantic Interpretation&apos;</title>
<author confidence="0.999753">Charniak Goldman</author>
<affiliation confidence="0.943304">Department of Computer Science Brown University, Box 1910</affiliation>
<address confidence="0.945688">Providence RI 02912</address>
<abstract confidence="0.999720421052632">We propose that logic (enhanced to encode probability information) is a good way of characterizing semantic interpretation. In support of this we give a fragment of an axiomatization for word-sense disambiguation, nounphrase (and verb) reference, and case disambiguation. We describe an inference engine (Frail3) which actually takes this axiomatization and uses it to drive the semantic interpretation process. We claim three benefits from this scheme. First, the interface between semantic interpretation and pragmatics has always been problematic, since all of the above tasks in general require pragmatic inference. Now the interface is trivial, since both semantic interpretation and pragmatics use the same vocabulary and inference engine. The second benefit, related to the first, is that semantic guidance of syntax is a side effect of the interpretation. The third benefit is the elegance of the semantic interpretation theory. A few simple rules capture a remarkable diversity of semantic phenomena.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>