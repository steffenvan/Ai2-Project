<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000932">
<title confidence="0.9966465">
Filling Knowledge Base Gaps for Distant Supervision
of Relation Extraction
</title>
<author confidence="0.992853">
Wei Xu* Raphael Hoffmannˆ Le Zhao#,* Ralph Grishman*
</author>
<affiliation confidence="0.938703">
+New York University, New York, NY, USA
</affiliation>
<email confidence="0.974726">
{xuwei, grishman}@cs.nyu.edu
</email>
<affiliation confidence="0.862304">
ˆUniversity of Washington, Seattle, WA, USA
</affiliation>
<email confidence="0.975992">
raphaelh@cs.washington.edu
</email>
<author confidence="0.654688">
#Google Inc., Mountain View, CA, USA
</author>
<email confidence="0.98402">
lezhao@google.com
</email>
<sectionHeader confidence="0.993607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996952">
Distant supervision has attracted recent in-
terest for training information extraction
systems because it does not require any
human annotation but rather employs ex-
isting knowledge bases to heuristically la-
bel a training corpus. However, previous
work has failed to address the problem
of false negative training examples misla-
beled due to the incompleteness of knowl-
edge bases. To tackle this problem, we
propose a simple yet novel framework that
combines a passage retrieval model using
coarse features into a state-of-the-art rela-
tion extractor using multi-instance learn-
ing with fine features. We adapt the in-
formation retrieval technique of pseudo-
relevance feedback to expand knowledge
bases, assuming entity pairs in top-ranked
passages are more likely to express a rela-
tion. Our proposed technique significantly
improves the quality of distantly super-
vised relation extraction, boosting recall
from 47.7% to 61.2% with a consistently
high level of precision of around 93% in
the experiments.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981950761904762">
A recent approach for training information ex-
traction systems is distant supervision, which ex-
ploits existing knowledge bases instead of anno-
tated texts as the source of supervision (Craven
and Kumlien, 1999; Mintz et al., 2009; Nguyen
and Moschitti, 2011). To combat the noisy train-
ing data produced by heuristic labeling in distant
supervision, researchers (Bunescu and Mooney,
2007; Riedel et al., 2010; Hoffmann et al., 2011;
Surdeanu et al., 2012) exploited multi-instance
*This work was done while Le Zhao was at Carnegie
Mellon University.
learning models. Only a few studies have directly
examined the influence of the quality of the train-
ing data and attempted to enhance it (Sun et al.,
2011; Wang et al., 2011; Takamatsu et al., 2012).
However, their methods are handicapped by the
built-in assumption that a sentence does not ex-
press a relation unless it mentions two entities
which participate in the relation in the knowledge
base, leading to false negatives.
</bodyText>
<figure confidence="0.860508">
true
mentions
false
positives
</figure>
<figureCaption confidence="0.9137885">
Figure 1: Noisy training data in distant supervi-
sion
</figureCaption>
<bodyText confidence="0.9996015">
In reality, knowledge bases are often incom-
plete, giving rise to numerous false negatives in
the training data. We sampled 1834 sentences that
contain two entities in the New York Times 2006
corpus and manually evaluated whether they ex-
press any of a set of 50 common Freebase1 rela-
tions. As shown in Figure 1, of the 133 (7.3%)
sentences that truly express one of these relations,
only 32 (1.7%) are covered by Freebase, leaving
101 (5.5%) false negatives. Even for one of the
most complete relations in Freebase, Employee-of
(with more than 100,000 entity pairs), 6 out of 27
sentences with the pattern ‘PERSON executive of
ORGANIZATION’ contain a fact that is not in-
cluded in Freebase and are thus mislabeled as neg-
ative. These mislabelings dilute the discriminative
capability of useful features and confuse the mod-
els. In this paper, we will show how reducing this
source of noise can significantly improve the per-
formance of distant supervision. In fact, our sys-
tem corrects the relation labels of the above 6 sen-
tences before training the relation extractor.
</bodyText>
<footnote confidence="0.943605">
1http://www.freebase.com
</footnote>
<figure confidence="0.9802468">
aligned
mentions
5.5%
2.7% 1.7% false
negatives
</figure>
<page confidence="0.945181">
665
</page>
<note confidence="0.577516">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665–670,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.885896">
Figure 2: Overall system architecture: The system
</figureCaption>
<listItem confidence="0.707143428571429">
(1) matches relation instances to sentences and (2)
learns a passage retrieval model to (3) provide rel-
evance feedback on sentences; Relevant sentences
(4) yield new relation instances which are added
to the knowledge base; Finally, instances are again
(5) matched to sentences to (6) create training data
for relation extraction.
</listItem>
<bodyText confidence="0.99937702">
Encouraged by the recent success of simple
methods for coreference resolution (Raghunathan
et al., 2010) and inspired by pseudo-relevance
feedback (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Matveeva et al., 2006; Cao et al.,
2008) in the field of information retrieval, which
expands or reformulates query terms based on
the highest ranked documents of an initial query,
we propose to increase the quality and quantity
of training data generated by distant supervision
for information extraction task using pseudo feed-
back. As shown in Figure 2, we expand an orig-
inal knowledge base with possibly missing rela-
tion instances with information from the highest
ranked sentences returned by a passage retrieval
model (Xu et al., 2011) trained on the same data.
We use coarse features for our passage retrieval
model to aggressively expand the knowledge base
for maximum recall; at the same time, we exploit
a multi-instance learning model with fine features
for relation extraction to handle the newly intro-
duced false positives and maintain high precision.
Similar to iterative bootstrapping tech-
niques (Yangarber, 2001), this mechanism uses
the outputs of the first trained model to expand
training data for the second model, but unlike
bootstrapping it does not require iteration and
avoids the problem of semantic drift. We further
note that iterative bootstrapping over a single
distant supervision system is difficult, because
state-of-the-art systems (Surdeanu et al., 2012;
Hoffmann et al., 2011; Riedel et al., 2010; Mintz
et al., 2009), detect only few false negatives in the
training data due to their high-precision low-recall
features, which were originally proposed by Mintz
et al. (2009). We present a reliable and novel way
to address these issues and achieve significant
improvement over the MULTIR system (Hoff-
mann et al., 2011), increasing recall from 47.7%
to 61.2% at comparable precision. The key to this
success is the combination of two different views
as in co-training (Blum and Mitchell, 1998):
an information extraction technique with fine
features for high precision and an information
retrieval technique with coarse features for high
recall. Our work is developed in parallel with
Min et al. (2013), who take a very different
approach by adding additional latent variables to
a multi-instance multi-label model (Surdeanu et
al., 2012) to solve this same problem.
</bodyText>
<sectionHeader confidence="0.981193" genericHeader="method">
2 System Details
</sectionHeader>
<bodyText confidence="0.999966333333333">
In this section, we first introduce some formal no-
tations then describe in detail each component of
the proposed system in Figure 2.
</bodyText>
<subsectionHeader confidence="0.972992">
2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.999854481481481">
A relation instance is an expression r(e1, e2)
where r is a binary relation, and e1 and e2 are
two entities having such a relation, for example
CEO-of(Tim Cook, Apple). The knowledge-based
distant supervised learning problem takes as input
(1) E, a training corpus, (2) E, a set of entities
mentioned in that corpus, (3) R, a set of relation
names, and (4) A, a set of ground facts of relations
in R. To generate our training data, we further as-
sume (5) T, a set of entity types, as well as type
signature r(E1, E2) for relations.
We define the positive data set POS(r) to be
the set of sentences in which any related pair
of entities of relation r (according to the knowl-
edge base) is mentioned. The negative data set
RAW (r) is the rest of the training data, which
contain two entities of the required types in the
knowledge base, e.g. one person and one or-
ganization for the CEO-of relation in Freebase.
Another negative data set with more conservative
sense NEG(r) is defined as the set of sentences
which contain the primary entity e1 (e.g. person
in any CEO-of relation in the knowledge base) and
any secondary entity e2 of required type (e.g. or-
ganization for the CEO-of relation) but the relation
does not hold for this pair of entities in the knowl-
edge base.
</bodyText>
<figure confidence="0.998567">
Relation
Extractor
�
Documents
Q
Knowledge
Base
0
Passage
Retriever
O
Pseudo-relevant
Relation Instances
�
T
</figure>
<page confidence="0.949245">
666
</page>
<subsectionHeader confidence="0.986692">
2.2 Distantly Supervised Passage Retrieval
</subsectionHeader>
<bodyText confidence="0.988584214285714">
We extend the learning-to-rank techniques (Liu,
2011) to distant supervision setting (Xu et al.,
2011) to create a robust passage retrieval system.
While relation extraction systems exploit rich and
complex features that are necessary to extract the
exact relation (Mintz et al., 2009; Riedel et al.,
2010; Hoffmann et al., 2011), passage retrieval
components use coarse features in order to provide
different and complementary feedback to informa-
tion extraction models.
We exploit two types of lexical features: Bag-
Of-Words and Word-Position. The two types of
simple binary features are shown in the following
example:
</bodyText>
<table confidence="0.801697">
Sentence: Apple founder Steve Jobs died.
Target (Primary) entity: Steve Jobs
Bag-Of-Word features: ‘apple’ ‘founder’ ‘died’ ‘.’
Word-Position features: ‘apple:-2’ ‘founder:-1’
‘died:+1’ ‘.:+2’
</table>
<bodyText confidence="0.997316454545454">
For each relation r, we assume each sentence
has a binary relevance label to form distantly su-
pervised training data: sentences in POS(r) are
relevant and sentences in NEG(r) are irrelevant.
As a pointwise learning-to-rank approach (Nallap-
ati, 2004), the probabilities of relevance estimated
by SVMs (Platt and others, 1999) are used for
ranking all the sentences in the original training
corpus for each relation respectively. We use Lib-
SVM 2 (Chang and Lin, 2011) in our implementa-
tion.
</bodyText>
<subsectionHeader confidence="0.99824">
2.3 Psuedo-relevance Relation Feedback
</subsectionHeader>
<bodyText confidence="0.999760692307692">
In the field of information retrieval, pseudo-
relevance feedback assumes that the top-ranked
documents from an initial retrieval are likely rel-
evant, and extracts relevant terms to expand the
original query (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Cao et al., 2008). Analogously, our
assumption is that entity pairs that appear in more
relevant and more sentences are more likely to
express the relation, and can be used to expand
knowledge base and reduce false negative noise in
the training data for information extraction. We
identify the most likely relevant entity pairs as fol-
lows:
</bodyText>
<footnote confidence="0.8848205">
2http://www.csie.ntu.edu.tw/-cjlin/
libsvm
</footnote>
<bodyText confidence="0.935658">
initialize A&apos; ←− A
for each relation type r E R do
learn a passage (sentence) retrieval model L(r)
using coarse features and POS(r)UNEG(r)
as training data
score the sentences in the RAW (r) by L(r)
score the entity pairs according to the scores
of sentences they are involved in
select the top ranked pairs of entities, then add
the relation r to their label in Al
end for
We select the entity pairs whose average score
of the sentences they are involved in is greater
than p, where p is a parameter tuned on develop-
ment data.3 The relation extraction model is then
trained using (E, E, R, A&apos;) with a more complete
database than the original knowledge base A.
</bodyText>
<subsectionHeader confidence="0.985471">
2.4 Distantly Supervised Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999948142857143">
We use a state-of-the-art open-source system,
MULTIR (Hoffmann et al., 2011), as the rela-
tion extraction component. MULTIR is based
on multi-instance learning, which assumes that
at least one sentence of those matching a given
entity-pair contains the relation of interest (Riedel
et al., 2010) in the given knowledge base to tol-
erate false positive noise in the training data and
superior than previous models (Riedel et al., 2010;
Mintz et al., 2009) by allowing overlapping rela-
tions. MULTIR uses features which are based on
Mintz et al. (2009) and consist of conjunctions of
named entity tags, syntactic dependency paths be-
tween arguments, and lexical information.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999976">
For evaluating extraction accuracy, we follow the
experimental setup of Hoffmann et al. (2011), and
use their implementation of MULTIR4 with 50
training iterations as our baseline. Our complete
system, which we call IRMIE, combines our pas-
sage retrieval component with MULTIR. We use
the same datasets as in Hoffmann et al. (2011) and
Riedel et al. (2010), which include 3-years of New
York Times articles aligned with Freebase. The
sentential extraction evaluation is performed on
a small amount of manually annotated sentences,
sampled from the union of matched sentences and
</bodyText>
<footnote confidence="0.995358666666667">
3We found P = 0.5 to work well in practice.
4http://homes.cs.washington.edu/
-raphaelh/mr/
</footnote>
<page confidence="0.989747">
667
</page>
<table confidence="0.999156666666667">
Test Data Set P˜ Original Test Set Δ˜F P˜ Corrected Test Set Δ˜F
R˜ F˜ R˜ F˜
MULTIR 80.0 44.6 62.3 92.7 47.7 70.2
IRMIE 84.6 56.1 70.3 +8.0 92.6 61.2 76.9 +6.7
MULTIRLEX 91.8 43.0 67.4 79.6 57.0 68.3
IRMIELEX 89.2 52.5 70.9 +3.5 78.0 69.2 73.6 +5.3
</table>
<tableCaption confidence="0.99836">
Table 1: Overall sentential extraction performance evaluated on the original test set of Hoffmann et
</tableCaption>
<bodyText confidence="0.996498052631579">
al. (2011) and our corrected test set: Our proposed relevance feedback technique yields a substantial
increase in recall.
system predictions. We define 5e as the sentences
where some system extracted a relation and 5F
as the sentences that match the arguments of a
fact in A. The sentential precision and recall is
computed on a randomly sampled set of sentences
from 5e ∪ 5F, in which each sentence is manually
labeled whether it expresses any relation in R.
Figure 3 shows the precision/recall curves for
MULTIR with and without pseudo-relevance feed-
back computed on the test dataset of 1000 sen-
tence used by Hoffmann et al. (2011). With the
pseudo-relevance feedback from passage retrieval,
IRMIE achieves significantly higher recall at a
consistently high level of precision. At the highest
recall point, IRMIE reaches 78.5% precision and
59.2% recall, for an F1 score of 68.9%.
Because the two types of lexical features used in
our passage retrieval models are not used in MUL-
TIR, we created another baseline MULTIRLEX
by adding these features into MULTIR in order
to rule out the improvement from additional infor-
mation. Note that the sentences are sampled from
the union of Freebase matches and sentences from
which some systems in Hoffmann et al. (2011) ex-
tracted a relation. It underestimates the improve-
ments of the newly developed systems in this pa-
per. We therefore also created a new test set of
1000 sentences by sampling from the union of
Freebase matches and sentences where MULTIR-
LEX or IRMIELEX extracted a relation. Table 1
shows the overall precision and recall computed
against these two test datasets, with and without
adding lexical features into multi-instance learn-
ing models. The performance improvement by us-
ing pseudo-feedback is significant (p &lt; 0.05) in
McNemar’s test for both datasets.
</bodyText>
<sectionHeader confidence="0.961765" genericHeader="conclusions">
4 Conclusion and Perspectives
</sectionHeader>
<bodyText confidence="0.998408">
This paper proposes a novel approach to address
an overlooked problem in distant supervision: the
knowledge base is often incomplete causing nu-
</bodyText>
<figure confidence="0.995767555555556">
1.0
0.9
0.8
0.7
IRMIE
MULTIR
0.5
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Recall
</figure>
<figureCaption confidence="0.992316">
Figure 3: Sentential extraction: precision/recall
curves using exact same training and test data,
features and system settings as in Hoffmann et
al. (2011).
</figureCaption>
<bodyText confidence="0.997930333333333">
merous false negatives in the training data. It
greatly improves a state-of-the-art multi-instance
learning model by correcting the most likely false
negatives in the training data based on the ranking
of a passage retrieval model.
In the future, we would like to more tightly inte-
grate a coarser featured estimator of sentential rel-
evance and a finer featured relation extractor, such
that a single joint-model can be learned.
</bodyText>
<sectionHeader confidence="0.984408" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998388">
Supported in part by NSF grant IIS-1018317,
the Air Force Research Laboratory (AFRL)
under prime contract number FA8750-09-C-
0181 and the Intelligence Advanced Research
Projects Activity (IARPA) via Department of In-
terior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of AFRL, IARPA,
DoI/NBC, or the U.S. Government.
</bodyText>
<figure confidence="0.7573075">
Precision
0.6
</figure>
<page confidence="0.981103">
668
</page>
<sectionHeader confidence="0.945912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983821518181818">
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92–100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and
Stephen Robertson. 2008. Selecting good expan-
sion terms for pseudo-relevance feedback. In Pro-
ceedigns of the 31st Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 243–250.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77–86.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541–550.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-based language models. In Proceedings
of the 24th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 120–127.
Tie-Yan Liu. 2011. Learning to Rank for Information
Retrieval. Springer-Verlag Berlin Heidelberg.
Irina Matveeva, Chris Burges, Timo Burkard, Andy
Laucius, and Leon Wong. 2006. High accuracy re-
trieval with multiple nested ranker. In Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 437–444.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2013).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003–1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64–71.
Truc Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 277–282.
John Platt et al. 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regular-
ized likelihood methods. Advances in Large Margin
Classifiers, 10(3):61–74.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 492–501.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148–163.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Text Analysis Conference 2011 Workshop.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455–465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721–729.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation
topics. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1426–1436.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Hans-
Peter Frei, Donna Harman, Peter Sch¨auble, and Ross
Wilkinson, editors, Proceedings of the 19th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 4–11. ACM.
</reference>
<page confidence="0.983136">
669
</page>
<reference confidence="0.998227888888889">
Wei Xu, Ralph Grishman, and Le Zhao. 2011. Passage
retrieval for information extraction using distant su-
pervision. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 1046–1054.
Roman Yangarber. 2001. Scenario customization for
information extraction. Ph.D. thesis, Department of
Computer Science, Graduate School of Arts and Sci-
ence, New York University.
</reference>
<page confidence="0.997946">
670
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479420">
<title confidence="0.999847">Filling Knowledge Base Gaps for Distant of Relation Extraction</title>
<author confidence="0.991493">Raphael Le_Ralph</author>
<affiliation confidence="0.72809">York University, New York, NY,</affiliation>
<address confidence="0.7520485">of Washington, Seattle, WA, Inc., Mountain View, CA,</address>
<email confidence="0.999791">lezhao@google.com</email>
<abstract confidence="0.998922076923077">Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus. However, previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases. To tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features. We adapt the information retrieval technique of pseudorelevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom M Mitchell</author>
</authors>
<title>Combining labeled and unlabeled sata with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT),</booktitle>
<pages>92--100</pages>
<contexts>
<context position="6134" citStr="Blum and Mitchell, 1998" startWordPosition="950" endWordPosition="953">rvision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009), detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these issues and achieve significant improvement over the MULTIR system (Hoffmann et al., 2011), increasing recall from 47.7% to 61.2% at comparable precision. The key to this success is the combination of two different views as in co-training (Blum and Mitchell, 1998): an information extraction technique with fine features for high precision and an information retrieval technique with coarse features for high recall. Our work is developed in parallel with Min et al. (2013), who take a very different approach by adding additional latent variables to a multi-instance multi-label model (Surdeanu et al., 2012) to solve this same problem. 2 System Details In this section, we first introduce some formal notations then describe in detail each component of the proposed system in Figure 2. 2.1 Definitions A relation instance is an expression r(e1, e2) where r is a </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom M. Mitchell. 1998. Combining labeled and unlabeled sata with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT), pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1730" citStr="Bunescu and Mooney, 2007" startWordPosition="251" endWordPosition="254">lation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments. 1 Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. true menti</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihong Cao</author>
<author>Jian-Yun Nie</author>
<author>Jianfeng Gao</author>
<author>Stephen Robertson</author>
</authors>
<title>Selecting good expansion terms for pseudo-relevance feedback.</title>
<date>2008</date>
<booktitle>In Proceedigns of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>243--250</pages>
<contexts>
<context position="4348" citStr="Cao et al., 2008" startWordPosition="669" endWordPosition="672">s Figure 2: Overall system architecture: The system (1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model (Xu et al., 2011) trained on the same data. We use coarse features for our passage retrieval model to aggressively ex</context>
<context position="9709" citStr="Cao et al., 2008" startWordPosition="1530" endWordPosition="1533">relevant. As a pointwise learning-to-rank approach (Nallapati, 2004), the probabilities of relevance estimated by SVMs (Platt and others, 1999) are used for ranking all the sentences in the original training corpus for each relation respectively. We use LibSVM 2 (Chang and Lin, 2011) in our implementation. 2.3 Psuedo-relevance Relation Feedback In the field of information retrieval, pseudorelevance feedback assumes that the top-ranked documents from an initial retrieval are likely relevant, and extracts relevant terms to expand the original query (Xu and Croft, 1996; Lavrenko and Croft, 2001; Cao et al., 2008). Analogously, our assumption is that entity pairs that appear in more relevant and more sentences are more likely to express the relation, and can be used to expand knowledge base and reduce false negative noise in the training data for information extraction. We identify the most likely relevant entity pairs as follows: 2http://www.csie.ntu.edu.tw/-cjlin/ libsvm initialize A&apos; ←− A for each relation type r E R do learn a passage (sentence) retrieval model L(r) using coarse features and POS(r)UNEG(r) as training data score the sentences in the RAW (r) by L(r) score the entity pairs according t</context>
</contexts>
<marker>Cao, Nie, Gao, Robertson, 2008</marker>
<rawString>Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. 2008. Selecting good expansion terms for pseudo-relevance feedback. In Proceedigns of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 243–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="9376" citStr="Chang and Lin, 2011" startWordPosition="1479" endWordPosition="1482">d. Target (Primary) entity: Steve Jobs Bag-Of-Word features: ‘apple’ ‘founder’ ‘died’ ‘.’ Word-Position features: ‘apple:-2’ ‘founder:-1’ ‘died:+1’ ‘.:+2’ For each relation r, we assume each sentence has a binary relevance label to form distantly supervised training data: sentences in POS(r) are relevant and sentences in NEG(r) are irrelevant. As a pointwise learning-to-rank approach (Nallapati, 2004), the probabilities of relevance estimated by SVMs (Platt and others, 1999) are used for ranking all the sentences in the original training corpus for each relation respectively. We use LibSVM 2 (Chang and Lin, 2011) in our implementation. 2.3 Psuedo-relevance Relation Feedback In the field of information retrieval, pseudorelevance feedback assumes that the top-ranked documents from an initial retrieval are likely relevant, and extracts relevant terms to expand the original query (Xu and Croft, 1996; Lavrenko and Croft, 2001; Cao et al., 2008). Analogously, our assumption is that entity pairs that appear in more relevant and more sentences are more likely to express the relation, and can be used to expand knowledge base and reduce false negative noise in the training data for information extraction. We id</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology (ISMB),</booktitle>
<pages>77--86</pages>
<contexts>
<context position="1553" citStr="Craven and Kumlien, 1999" startWordPosition="224" endWordPosition="227"> We adapt the information retrieval technique of pseudorelevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments. 1 Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assum</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology (ISMB), pages 77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke S Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>541--550</pages>
<contexts>
<context position="1774" citStr="Hoffmann et al., 2011" startWordPosition="259" endWordPosition="262">proves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments. 1 Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. true mentions false positives Figure 1: Noisy training</context>
<context position="5617" citStr="Hoffmann et al., 2011" startWordPosition="866" endWordPosition="869">he same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision. Similar to iterative bootstrapping techniques (Yangarber, 2001), this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009), detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these issues and achieve significant improvement over the MULTIR system (Hoffmann et al., 2011), increasing recall from 47.7% to 61.2% at comparable precision. The key to this success is the combination of two different views as in co-training (Blum and Mitchell, 1998): an information extraction technique with fine features for high precision and an </context>
<context position="8427" citStr="Hoffmann et al., 2011" startWordPosition="1338" endWordPosition="1341">equired type (e.g. organization for the CEO-of relation) but the relation does not hold for this pair of entities in the knowledge base. Relation Extractor � Documents Q Knowledge Base 0 Passage Retriever O Pseudo-relevant Relation Instances � T 666 2.2 Distantly Supervised Passage Retrieval We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting (Xu et al., 2011) to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position. The two types of simple binary features are shown in the following example: Sentence: Apple founder Steve Jobs died. Target (Primary) entity: Steve Jobs Bag-Of-Word features: ‘apple’ ‘founder’ ‘died’ ‘.’ Word-Position features: ‘apple:-2’ ‘founder:-1’ ‘died:+1’ ‘.:+2’ For each relation r, we assume each sentence has a binary relevance label to form distantly supervised training data</context>
<context position="10858" citStr="Hoffmann et al., 2011" startWordPosition="1722" endWordPosition="1725">e the sentences in the RAW (r) by L(r) score the entity pairs according to the scores of sentences they are involved in select the top ranked pairs of entities, then add the relation r to their label in Al end for We select the entity pairs whose average score of the sentences they are involved in is greater than p, where p is a parameter tuned on development data.3 The relation extraction model is then trained using (E, E, R, A&apos;) with a more complete database than the original knowledge base A. 2.4 Distantly Supervised Relation Extraction We use a state-of-the-art open-source system, MULTIR (Hoffmann et al., 2011), as the relation extraction component. MULTIR is based on multi-instance learning, which assumes that at least one sentence of those matching a given entity-pair contains the relation of interest (Riedel et al., 2010) in the given knowledge base to tolerate false positive noise in the training data and superior than previous models (Riedel et al., 2010; Mintz et al., 2009) by allowing overlapping relations. MULTIR uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. 3 Exper</context>
<context position="12498" citStr="Hoffmann et al. (2011)" startWordPosition="1989" endWordPosition="1992">ed with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and 3We found P = 0.5 to work well in practice. 4http://homes.cs.washington.edu/ -raphaelh/mr/ 667 Test Data Set P˜ Original Test Set Δ˜F P˜ Corrected Test Set Δ˜F R˜ F˜ R˜ F˜ MULTIR 80.0 44.6 62.3 92.7 47.7 70.2 IRMIE 84.6 56.1 70.3 +8.0 92.6 61.2 76.9 +6.7 MULTIRLEX 91.8 43.0 67.4 79.6 57.0 68.3 IRMIELEX 89.2 52.5 70.9 +3.5 78.0 69.2 73.6 +5.3 Table 1: Overall sentential extraction performance evaluated on the original test set of Hoffmann et al. (2011) and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions. We define 5e as the sentences where some system extracted a relation and 5F as the sentences that match the arguments of a fact in A. The sentential precision and recall is computed on a randomly sampled set of sentences from 5e ∪ 5F, in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for MULTIR with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by</context>
<context position="13749" citStr="Hoffmann et al. (2011)" startWordPosition="2195" endWordPosition="2198"> the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision. At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%. Because the two types of lexical features used in our passage retrieval models are not used in MULTIR, we created another baseline MULTIRLEX by adding these features into MULTIR in order to rule out the improvement from additional information. Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in Hoffmann et al. (2011) extracted a relation. It underestimates the improvements of the newly developed systems in this paper. We therefore also created a new test set of 1000 sentences by sampling from the union of Freebase matches and sentences where MULTIRLEX or IRMIELEX extracted a relation. Table 1 shows the overall precision and recall computed against these two test datasets, with and without adding lexical features into multi-instance learning models. The performance improvement by using pseudo-feedback is significant (p &lt; 0.05) in McNemar’s test for both datasets. 4 Conclusion and Perspectives This paper pr</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S. Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 541–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance-based language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>120--127</pages>
<contexts>
<context position="4306" citStr="Lavrenko and Croft, 2001" startWordPosition="661" endWordPosition="664">. c�2013 Association for Computational Linguistics Figure 2: Overall system architecture: The system (1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model (Xu et al., 2011) trained on the same data. We use coarse features for our </context>
<context position="9690" citStr="Lavrenko and Croft, 2001" startWordPosition="1526" endWordPosition="1529">sentences in NEG(r) are irrelevant. As a pointwise learning-to-rank approach (Nallapati, 2004), the probabilities of relevance estimated by SVMs (Platt and others, 1999) are used for ranking all the sentences in the original training corpus for each relation respectively. We use LibSVM 2 (Chang and Lin, 2011) in our implementation. 2.3 Psuedo-relevance Relation Feedback In the field of information retrieval, pseudorelevance feedback assumes that the top-ranked documents from an initial retrieval are likely relevant, and extracts relevant terms to expand the original query (Xu and Croft, 1996; Lavrenko and Croft, 2001; Cao et al., 2008). Analogously, our assumption is that entity pairs that appear in more relevant and more sentences are more likely to express the relation, and can be used to expand knowledge base and reduce false negative noise in the training data for information extraction. We identify the most likely relevant entity pairs as follows: 2http://www.csie.ntu.edu.tw/-cjlin/ libsvm initialize A&apos; ←− A for each relation type r E R do learn a passage (sentence) retrieval model L(r) using coarse features and POS(r)UNEG(r) as training data score the sentences in the RAW (r) by L(r) score the entit</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance-based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
</authors>
<title>Learning to Rank for Information Retrieval.</title>
<date>2011</date>
<publisher>Springer-Verlag</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="8151" citStr="Liu, 2011" startWordPosition="1296" endWordPosition="1297">on for the CEO-of relation in Freebase. Another negative data set with more conservative sense NEG(r) is defined as the set of sentences which contain the primary entity e1 (e.g. person in any CEO-of relation in the knowledge base) and any secondary entity e2 of required type (e.g. organization for the CEO-of relation) but the relation does not hold for this pair of entities in the knowledge base. Relation Extractor � Documents Q Knowledge Base 0 Passage Retriever O Pseudo-relevant Relation Instances � T 666 2.2 Distantly Supervised Passage Retrieval We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting (Xu et al., 2011) to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position. The two types of simple binary features are shown in the following example: Sentence: Apple founder Steve Job</context>
</contexts>
<marker>Liu, 2011</marker>
<rawString>Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval. Springer-Verlag Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Matveeva</author>
<author>Chris Burges</author>
<author>Timo Burkard</author>
<author>Andy Laucius</author>
<author>Leon Wong</author>
</authors>
<title>High accuracy retrieval with multiple nested ranker.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>437--444</pages>
<contexts>
<context position="4329" citStr="Matveeva et al., 2006" startWordPosition="665" endWordPosition="668">omputational Linguistics Figure 2: Overall system architecture: The system (1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model (Xu et al., 2011) trained on the same data. We use coarse features for our passage retrieval model</context>
</contexts>
<marker>Matveeva, Burges, Burkard, Laucius, Wong, 2006</marker>
<rawString>Irina Matveeva, Chris Burges, Timo Burkard, Andy Laucius, and Leon Wong. 2006. High accuracy retrieval with multiple nested ranker. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 437–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant supervision for relation extraction with an incomplete knowledge base.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<contexts>
<context position="6343" citStr="Min et al. (2013)" startWordPosition="982" endWordPosition="985"> high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these issues and achieve significant improvement over the MULTIR system (Hoffmann et al., 2011), increasing recall from 47.7% to 61.2% at comparable precision. The key to this success is the combination of two different views as in co-training (Blum and Mitchell, 1998): an information extraction technique with fine features for high precision and an information retrieval technique with coarse features for high recall. Our work is developed in parallel with Min et al. (2013), who take a very different approach by adding additional latent variables to a multi-instance multi-label model (Surdeanu et al., 2012) to solve this same problem. 2 System Details In this section, we first introduce some formal notations then describe in detail each component of the proposed system in Figure 2. 2.1 Definitions A relation instance is an expression r(e1, e2) where r is a binary relation, and e1 and e2 are two entities having such a relation, for example CEO-of(Tim Cook, Apple). The knowledge-based distant supervised learning problem takes as input (1) E, a training corpus, (2)</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedigns of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACL),</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="1573" citStr="Mintz et al., 2009" startWordPosition="228" endWordPosition="231">retrieval technique of pseudorelevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments. 1 Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentenc</context>
<context position="5659" citStr="Mintz et al., 2009" startWordPosition="874" endWordPosition="877">arning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision. Similar to iterative bootstrapping techniques (Yangarber, 2001), this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009), detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these issues and achieve significant improvement over the MULTIR system (Hoffmann et al., 2011), increasing recall from 47.7% to 61.2% at comparable precision. The key to this success is the combination of two different views as in co-training (Blum and Mitchell, 1998): an information extraction technique with fine features for high precision and an information retrieval technique with coars</context>
<context position="8382" citStr="Mintz et al., 2009" startWordPosition="1330" endWordPosition="1333">ge base) and any secondary entity e2 of required type (e.g. organization for the CEO-of relation) but the relation does not hold for this pair of entities in the knowledge base. Relation Extractor � Documents Q Knowledge Base 0 Passage Retriever O Pseudo-relevant Relation Instances � T 666 2.2 Distantly Supervised Passage Retrieval We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting (Xu et al., 2011) to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position. The two types of simple binary features are shown in the following example: Sentence: Apple founder Steve Jobs died. Target (Primary) entity: Steve Jobs Bag-Of-Word features: ‘apple’ ‘founder’ ‘died’ ‘.’ Word-Position features: ‘apple:-2’ ‘founder:-1’ ‘died:+1’ ‘.:+2’ For each relation r, we assume each sentence has a binary relevance lab</context>
<context position="11234" citStr="Mintz et al., 2009" startWordPosition="1784" endWordPosition="1787">on extraction model is then trained using (E, E, R, A&apos;) with a more complete database than the original knowledge base A. 2.4 Distantly Supervised Relation Extraction We use a state-of-the-art open-source system, MULTIR (Hoffmann et al., 2011), as the relation extraction component. MULTIR is based on multi-instance learning, which assumes that at least one sentence of those matching a given entity-pair contains the relation of interest (Riedel et al., 2010) in the given knowledge base to tolerate false positive noise in the training data and superior than previous models (Riedel et al., 2010; Mintz et al., 2009) by allowing overlapping relations. MULTIR uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. 3 Experiments For evaluating extraction accuracy, we follow the experimental setup of Hoffmann et al. (2011), and use their implementation of MULTIR4 with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with MULTIR. We use the same datasets as in Hoffmann et al. (2011) and Riedel et al. (2010), which includ</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedigns of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACL), pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh Nallapati</author>
</authors>
<title>Discriminative models for information retrieval.</title>
<date>2004</date>
<booktitle>In Proceedigns of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>64--71</pages>
<contexts>
<context position="9160" citStr="Nallapati, 2004" startWordPosition="1444" endWordPosition="1446">ation extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position. The two types of simple binary features are shown in the following example: Sentence: Apple founder Steve Jobs died. Target (Primary) entity: Steve Jobs Bag-Of-Word features: ‘apple’ ‘founder’ ‘died’ ‘.’ Word-Position features: ‘apple:-2’ ‘founder:-1’ ‘died:+1’ ‘.:+2’ For each relation r, we assume each sentence has a binary relevance label to form distantly supervised training data: sentences in POS(r) are relevant and sentences in NEG(r) are irrelevant. As a pointwise learning-to-rank approach (Nallapati, 2004), the probabilities of relevance estimated by SVMs (Platt and others, 1999) are used for ranking all the sentences in the original training corpus for each relation respectively. We use LibSVM 2 (Chang and Lin, 2011) in our implementation. 2.3 Psuedo-relevance Relation Feedback In the field of information retrieval, pseudorelevance feedback assumes that the top-ranked documents from an initial retrieval are likely relevant, and extracts relevant terms to expand the original query (Xu and Croft, 1996; Lavrenko and Croft, 2001; Cao et al., 2008). Analogously, our assumption is that entity pairs </context>
</contexts>
<marker>Nallapati, 2004</marker>
<rawString>Ramesh Nallapati. 2004. Discriminative models for information retrieval. In Proceedigns of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>End-to-end relation extraction using distant supervision from external semantic repositories.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>277--282</pages>
<contexts>
<context position="1602" citStr="Nguyen and Moschitti, 2011" startWordPosition="232" endWordPosition="235">of pseudorelevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments. 1 Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentence does not express a relation</context>
</contexts>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc Vien T Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 277–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<volume>10</volume>
<issue>3</issue>
<marker>Platt, 1999</marker>
<rawString>John Platt et al. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 10(3):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>492--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4218" citStr="Raghunathan et al., 2010" startWordPosition="648" endWordPosition="651">ssociation for Computational Linguistics, pages 665–670, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 2: Overall system architecture: The system (1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage ret</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492–501. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedigns of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD),</booktitle>
<pages>148--163</pages>
<contexts>
<context position="1751" citStr="Riedel et al., 2010" startWordPosition="255" endWordPosition="258">ique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments. 1 Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. true mentions false positives F</context>
<context position="5638" citStr="Riedel et al., 2010" startWordPosition="870" endWordPosition="873">t a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision. Similar to iterative bootstrapping techniques (Yangarber, 2001), this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009), detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these issues and achieve significant improvement over the MULTIR system (Hoffmann et al., 2011), increasing recall from 47.7% to 61.2% at comparable precision. The key to this success is the combination of two different views as in co-training (Blum and Mitchell, 1998): an information extraction technique with fine features for high precision and an information retrieval</context>
<context position="8403" citStr="Riedel et al., 2010" startWordPosition="1334" endWordPosition="1337">ondary entity e2 of required type (e.g. organization for the CEO-of relation) but the relation does not hold for this pair of entities in the knowledge base. Relation Extractor � Documents Q Knowledge Base 0 Passage Retriever O Pseudo-relevant Relation Instances � T 666 2.2 Distantly Supervised Passage Retrieval We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting (Xu et al., 2011) to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position. The two types of simple binary features are shown in the following example: Sentence: Apple founder Steve Jobs died. Target (Primary) entity: Steve Jobs Bag-Of-Word features: ‘apple’ ‘founder’ ‘died’ ‘.’ Word-Position features: ‘apple:-2’ ‘founder:-1’ ‘died:+1’ ‘.:+2’ For each relation r, we assume each sentence has a binary relevance label to form distantly </context>
<context position="11076" citStr="Riedel et al., 2010" startWordPosition="1756" endWordPosition="1759">ect the entity pairs whose average score of the sentences they are involved in is greater than p, where p is a parameter tuned on development data.3 The relation extraction model is then trained using (E, E, R, A&apos;) with a more complete database than the original knowledge base A. 2.4 Distantly Supervised Relation Extraction We use a state-of-the-art open-source system, MULTIR (Hoffmann et al., 2011), as the relation extraction component. MULTIR is based on multi-instance learning, which assumes that at least one sentence of those matching a given entity-pair contains the relation of interest (Riedel et al., 2010) in the given knowledge base to tolerate false positive noise in the training data and superior than previous models (Riedel et al., 2010; Mintz et al., 2009) by allowing overlapping relations. MULTIR uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. 3 Experiments For evaluating extraction accuracy, we follow the experimental setup of Hoffmann et al. (2011), and use their implementation of MULTIR4 with 50 training iterations as our baseline. Our complete system, which we </context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedigns of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD), pages 148–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Wei Xu</author>
<author>Bonan Min</author>
</authors>
<title>system for kbp slot filling.</title>
<date>2011</date>
<booktitle>In Text Analysis Conference 2011 Workshop.</booktitle>
<location>New york university</location>
<contexts>
<context position="2047" citStr="Sun et al., 2011" startWordPosition="305" endWordPosition="308"> which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. true mentions false positives Figure 1: Noisy training data in distant supervision In reality, knowledge bases are often incomplete, giving rise to numerous false negatives in the training data. We sampled 1834 sentences that contain two entities in the New York Times 2006 corpus and manually evaluated whether they express an</context>
</contexts>
<marker>Sun, Grishman, Xu, Min, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min. 2011. New york university 2011 system for kbp slot filling. In Text Analysis Conference 2011 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>455--465</pages>
<contexts>
<context position="1798" citStr="Surdeanu et al., 2012" startWordPosition="263" endWordPosition="266">istantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments. 1 Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. true mentions false positives Figure 1: Noisy training data in distant supervi</context>
<context position="5594" citStr="Surdeanu et al., 2012" startWordPosition="862" endWordPosition="865">or maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision. Similar to iterative bootstrapping techniques (Yangarber, 2001), this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009), detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these issues and achieve significant improvement over the MULTIR system (Hoffmann et al., 2011), increasing recall from 47.7% to 61.2% at comparable precision. The key to this success is the combination of two different views as in co-training (Blum and Mitchell, 1998): an information extraction technique with fine features for</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Reducing wrong labels in distant supervision for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>721--729</pages>
<contexts>
<context position="2091" citStr="Takamatsu et al., 2012" startWordPosition="313" endWordPosition="316">ses instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. true mentions false positives Figure 1: Noisy training data in distant supervision In reality, knowledge bases are often incomplete, giving rise to numerous false negatives in the training data. We sampled 1834 sentences that contain two entities in the New York Times 2006 corpus and manually evaluated whether they express any of a set of 50 common Freebase1 relations.</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2012</marker>
<rawString>Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 721–729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Wang</author>
<author>James Fan</author>
<author>Aditya Kalyanpur</author>
<author>David Gondek</author>
</authors>
<title>Relation extraction with relation topics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1426--1436</pages>
<contexts>
<context position="2066" citStr="Wang et al., 2011" startWordPosition="309" endWordPosition="312">isting knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance *This work was done while Le Zhao was at Carnegie Mellon University. learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012). However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. true mentions false positives Figure 1: Noisy training data in distant supervision In reality, knowledge bases are often incomplete, giving rise to numerous false negatives in the training data. We sampled 1834 sentences that contain two entities in the New York Times 2006 corpus and manually evaluated whether they express any of a set of 50 co</context>
</contexts>
<marker>Wang, Fan, Kalyanpur, Gondek, 2011</marker>
<rawString>Chang Wang, James Fan, Aditya Kalyanpur, and David Gondek. 2011. Relation extraction with relation topics. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1426–1436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>4--11</pages>
<editor>In HansPeter Frei, Donna Harman, Peter Sch¨auble, and Ross Wilkinson, editors,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="4280" citStr="Xu and Croft, 1996" startWordPosition="657" endWordPosition="660">ria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 2: Overall system architecture: The system (1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model (Xu et al., 2011) trained on the same data. We us</context>
<context position="9664" citStr="Xu and Croft, 1996" startWordPosition="1522" endWordPosition="1525">r) are relevant and sentences in NEG(r) are irrelevant. As a pointwise learning-to-rank approach (Nallapati, 2004), the probabilities of relevance estimated by SVMs (Platt and others, 1999) are used for ranking all the sentences in the original training corpus for each relation respectively. We use LibSVM 2 (Chang and Lin, 2011) in our implementation. 2.3 Psuedo-relevance Relation Feedback In the field of information retrieval, pseudorelevance feedback assumes that the top-ranked documents from an initial retrieval are likely relevant, and extracts relevant terms to expand the original query (Xu and Croft, 1996; Lavrenko and Croft, 2001; Cao et al., 2008). Analogously, our assumption is that entity pairs that appear in more relevant and more sentences are more likely to express the relation, and can be used to expand knowledge base and reduce false negative noise in the training data for information extraction. We identify the most likely relevant entity pairs as follows: 2http://www.csie.ntu.edu.tw/-cjlin/ libsvm initialize A&apos; ←− A for each relation type r E R do learn a passage (sentence) retrieval model L(r) using coarse features and POS(r)UNEG(r) as training data score the sentences in the RAW (</context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 1996. Query expansion using local and global document analysis. In HansPeter Frei, Donna Harman, Peter Sch¨auble, and Ross Wilkinson, editors, Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 4–11. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Ralph Grishman</author>
<author>Le Zhao</author>
</authors>
<title>Passage retrieval for information extraction using distant supervision.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>1046--1054</pages>
<marker>Xu, Grishman, Le Zhao, 2011</marker>
<rawString>Wei Xu, Ralph Grishman, and Le Zhao. 2011. Passage retrieval for information extraction using distant supervision. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP), pages 1046–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Scenario customization for information extraction.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Graduate School of Arts and Science, New York University.</institution>
<contexts>
<context position="5235" citStr="Yangarber, 2001" startWordPosition="810" endWordPosition="811">using pseudo feedback. As shown in Figure 2, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model (Xu et al., 2011) trained on the same data. We use coarse features for our passage retrieval model to aggressively expand the knowledge base for maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision. Similar to iterative bootstrapping techniques (Yangarber, 2001), this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009), detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reli</context>
</contexts>
<marker>Yangarber, 2001</marker>
<rawString>Roman Yangarber. 2001. Scenario customization for information extraction. Ph.D. thesis, Department of Computer Science, Graduate School of Arts and Science, New York University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>