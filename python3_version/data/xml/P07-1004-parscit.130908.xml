<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.995772">
Transductive learning for statistical machine translation
</title>
<author confidence="0.983011">
Nicola Ueffing Gholamreza Haffari and Anoop Sarkar
</author>
<affiliation confidence="0.70277">
National Research Council Canada Simon Fraser University
Gatineau, QC, Canada Burnaby, BC, Canada
</affiliation>
<email confidence="0.958555">
nicola.ueffing@nrc.gc.ca {ghaffar1,anoop}@cs.sfu.ca
</email>
<sectionHeader confidence="0.992914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999650466666667">
Statistical machine translation systems are
usually trained on large amounts of bilin-
gual text and monolingual text in the tar-
get language. In this paper we explore the
use of transductive semi-supervised meth-
ods for the effective use of monolingual data
from the source language in order to im-
prove translation quality. We propose sev-
eral algorithms with this aim, and present the
strengths and weaknesses of each one. We
present detailed experimental evaluations on
the French–English EuroParl data set and on
data from the NIST Chinese–English large-
data track. We show a significant improve-
ment in translation quality on both tasks.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.918936">
In statistical machine translation (SMT), translation
is modeled as a decision process. The goal is to find
the translation t of source sentence s which maxi-
mizes the posterior probability:
arg max p(t  |s) = arg max p(s  |t) · p(t) (1)
</bodyText>
<subsectionHeader confidence="0.389724">
t t
</subsectionHeader>
<bodyText confidence="0.994924666666667">
This decomposition of the probability yields two dif-
ferent statistical models which can be trained in-
dependently of each other: the translation model
p(s  |t) and the target language model p(t).
State-of-the-art SMT systems are trained on large
collections of text which consist of bilingual corpora
(to learn the parameters of p(s  |t)), and of monolin-
gual target language corpora (for p(t)). It has been
shown that adding large amounts of target language
text improves translation quality considerably. How-
ever, the availability of monolingual corpora in the
source language does not help improve the system’s
25
performance. We will show how such corpora can
be used to achieve higher translation quality.
Even if large amounts of bilingual text are given,
the training of the statistical models usually suffers
from sparse data. The number of possible events,
i.e. phrase pairs or pairs of subtrees in the two lan-
guages, is too big to reliably estimate a probabil-
ity distribution over such pairs. Another problem is
that for many language pairs the amount of available
bilingual text is very limited. In this work, we will
address this problem and propose a general frame-
work to solve it. Our hypothesis is that adding infor-
mation from source language text can also provide
improvements. Unlike adding target language text,
this hypothesis is a natural semi-supervised learn-
ing problem. To tackle this problem, we propose
algorithms for transductive semi-supervised learn-
ing. By transductive, we mean that we repeatedly
translate sentences from the development set or test
set and use the generated translations to improve the
performance of the SMT system. Note that the eval-
uation step is still done just once at the end of our
learning process. In this paper, we show that such
an approach can lead to better translations despite
the fact that the development and test data are typi-
cally much smaller in size than typical training data
for SMT systems.
Transductive learning can be seen as a means to
adapt the SMT system to a new type of text. Say a
system trained on newswire is used to translate we-
blog texts. The proposed method adapts the trained
models to the style and domain of the new input.
</bodyText>
<sectionHeader confidence="0.989763" genericHeader="method">
2 Baseline MT System
</sectionHeader>
<bodyText confidence="0.993060666666667">
The SMT system we applied in our experiments is
PORTAGE. This is a state-of-the-art phrase-based
translation system which has been made available
</bodyText>
<note confidence="0.9029975">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 25–32,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.987498">
to Canadian universities for research and education iteration and added to the training data. These se-
purposes. We provide a basic description here; for a lected sentence pairs are replaced in each iteration,
detailed description see (Ueffing et al., 2007). and only the original bilingual training data, L, is
The models (or features) which are employed by kept fixed throughout the algorithm. The process
the decoder are: (a) one or several phrase table(s), of generating sentence pairs, selecting a subset of
which model the translation directionp(s  |t), (b) one good sentence pairs, and updating the model is con-
or several n-gram language model(s) trained with tinued until a stopping condition is met. Note that
the SRILM toolkit (Stolcke, 2002); in the experi- we run this algorithm in a transductive setting which
ments reported here, we used 4-gram models on the means that the set of sentences U is drawn either
NIST data, and a trigram model on EuroParl, (c) from a development set or the test set that will be
a distortion model which assigns a penalty based used eventually to evaluate the SMT system or from
on the number of source words which are skipped additional data which is relevant to the development
when generating a new target phrase, and (d) a word or test set. In Algorithm 1, changing the definition
penalty. These different models are combined log- of Estimate, Score and Select will give us the dif-
linearly. Their weights are optimized w.r.t. BLEU ferent semi-supervised learning algorithms we will
score using the algorithm described in (Och, 2003). discuss in this paper.
This is done on a development corpus which we will Given the probability model p(t  |s), consider the
call dev1 in this paper. The search algorithm imple- distribution over all possible valid translations t for
mented in the decoder is a dynamic-programming a particular input sentence s. We can initialize
beam-search algorithm. this probability distribution to the uniform distribu-
After the main decoding step, rescoring with ad- tion for each sentence s in the unlabeled data U.
ditional models is performed. The system generates Thus, this distribution over translations of sentences
a 5,000-best list of alternative translations for each from U will have the maximum entropy. Under
source sentence. These lists are rescored with the certain precise conditions, as described in (Abney,
following models: (a) the different models used in 2004), we can analyze Algorithm 1 as minimizing
the decoder which are described above, (b) two dif- the entropy of the distribution over translations of U.
ferent features based on IBM Model 1 (Brown et al., However, this is true only when the functions Esti-
1993), (c) posterior probabilities for words, phrases, mate, Score and Select have very prescribed defini-
n-grams, and sentence length (Zens and Ney, 2006; tions. In this paper, rather than analyze the conver-
Ueffing and Ney, 2007), all calculated over the N- gence of Algorithm 1 we run it for a fixed number
best list and using the sentence probabilities which of iterations and instead focus on finding useful def-
the baseline system assigns to the translation hy- initions for Estimate, Score and Select that can be
potheses. The weights of these additional models experimentally shown to improve MT performance.
and of the decoder models are again optimized to 3.2 The Estimate Function
maximize BLEU score. This is performed on a sec- We consider the following different definitions for
ond development corpus, dev2. Estimate in Algorithm 1:
3 The Framework Full Re-training (of all translation models): If
3.1 The Algorithm Estimate(L, T) estimates the model parameters
Our transductive learning algorithm, Algorithm 1, based on L U T, then we have a semi-supervised al-
is inspired by the Yarowsky algorithm (Yarowsky, gorithm that re-trains a model on the original train-
1995; Abney, 2004). The algorithm works as fol- ing data L plus the sentences decoded in the last it-
lows: First, the translation model is estimated based eration. The size of L can be controlled by filtering
on the sentence pairs in the bilingual training data L. the training data (see Section 3.5).
Then, a set of source language sentences, U, is trans- Additional Phrase Table: If, on the other hand, a
lated based on the current model. A subset of good new phrase translation table is learned on T only
translations and their sources, Ti, is selected in each and then added as a new component in the log-linear
26 model, we have an alternative to the full re-training
Algorithm 1 Transductive learning algorithm for statistical machine translation
</bodyText>
<listItem confidence="0.99880475">
1: Input: training set L of parallel sentence pairs. // Bilingual training data.
2: Input: unlabeled set U of source text. // Monolingual source language data.
3: Input: number of iterations R, and size of n-best list N.
4: T_1 := 11. // Additional bilingual training data.
5: i := 0. // Iteration counter.
6: repeat
7: Training step: 7r(i) := Estimate(L,Ti_1).
8: Xi := 11. // The set of generated translations for this iteration.
9: for sentence s E U do
10: Labeling stepff: Decode s using 7r(i) to obtain N best sentence pairs with their scores
11: Xi := Xi U I(tn, s, 7r(i)(tn  |s))N n�1�
12: end for
13: Scoring step: Si := Score(Xi) // Assign a score to sentence pairs (t, s) from X.
14: Selection step: Ti := Select(Xi, Si) // Choose a subset of good sentence pairs (t, s) from X.
15: i := i + 1.
16: until i &gt; R
</listItem>
<bodyText confidence="0.9989761">
of the model on labeled and unlabeled data which
can be very expensive if L is very large (as on the
Chinese–English data set). This additional phrase
table is small and specific to the development or
test set it is trained on. It overlaps with the origi-
nal phrase tables, but also contains many new phrase
pairs (Ueffing, 2006).
Mixture Model: Another alternative for Estimate
is to create a mixture model of the phrase table prob-
abilities with new phrase table probabilities
</bodyText>
<equation confidence="0.994277">
p(s|t) = A - Lp(s|t) + (1 − A) - Tp(s|t) (2)
</equation>
<bodyText confidence="0.99997625">
where Lp and Tp are phrase table probabilities esti-
mated on L and T, respectively. In cases where new
phrase pairs are learned from T, they get added into
the merged phrase table.
</bodyText>
<subsectionHeader confidence="0.99965">
3.3 The Scoring Function
</subsectionHeader>
<bodyText confidence="0.998565">
In Algorithm 1, the Score function assigns a score to
each translation hypothesis t. We used the following
scoring functions in our experiments:
</bodyText>
<listItem confidence="0.843168666666667">
Length-normalized Score: Each translated sen-
tence pair (t, s) is scored according to the model
probability p(t  |s) normalized by the length |t |of the
target sentence:
Score(t, s) = p(t  |s)
Confidence Estimation: The confidence estimation
</listItem>
<bodyText confidence="0.999116444444444">
which we implemented follows the approaches sug-
gested in (Blatz et al., 2003; Ueffing and Ney, 2007):
The confidence score of a target sentence t is cal-
culated as a log-linear combination of phrase pos-
terior probabilities, Levenshtein-based word poste-
rior probabilities, and a target language model score.
The weights of the different scores are optimized
w.r.t. classification error rate (CER).
The phrase posterior probabilities are determined
by summing the sentence probabilities of all trans-
lation hypotheses in the N-best list which contain
this phrase pair. The segmentation of the sentence
into phrases is provided by the decoder. This sum
is then normalized by the total probability mass of
the N-best list. To obtain a score for the whole tar-
get sentence, the posterior probabilities of all target
phrases are multiplied. The word posterior proba-
bilities are calculated on basis of the Levenshtein
alignment between the hypothesis under consider-
ation and all other translations contained in the N-
best list. For details, see (Ueffing and Ney, 2007).
Again, the single values are multiplied to obtain a
score for the whole sentence. For NIST, the lan-
guage model score is determined using a 5-gram
model trained on the English Gigaword corpus, and
on French–English, we use the trigram model which
was provided for the NAACL 2006 shared task.
</bodyText>
<subsectionHeader confidence="0.973407">
3.4 The Selection Function
</subsectionHeader>
<bodyText confidence="0.9989595">
The Select function in Algorithm 1 is used to create
the additional training data Ti which will be used in
</bodyText>
<equation confidence="0.521203333333333">
1
|t|
(3)
</equation>
<page confidence="0.979304">
27
</page>
<bodyText confidence="0.992885893617021">
the next iteration i + 1 by Estimate to augment the corpus use sentences
original bilingual training data. We use the follow-
ing selection functions:
Importance Sampling: For each sentence s in the
set of unlabeled sentences U, the Labeling step in
Algorithm 1 generates an N-best list of translations,
and the subsequent Scoring step assigns a score for
each translation t in this list. The set of generated
translations for all sentences in U is the event space
and the scores are used to put a probability distri-
bution over this space, simply by renormalizing the
scores described in Section 3.3. We use importance
sampling to select K translations from this distri-
bution. Sampling is done with replacement which
means that the same translation may be chosen sev-
eral times. These K sampled translations and their
associated source sentences make up the additional
training data Ti.
Selection using a Threshold: This method com-
pares the score of each single-best translation to a
threshold. The translation is considered reliable and
added to the set Ti if its score exceeds the thresh-
old. Else it is discarded and not used in the addi-
tional training data. The threshold is optimized on
the development beforehand. Since the scores of the
translations change in each iteration, the size of Ti
also changes.
Keep All: This method does not perform any fil-
tering at all. It is simply assumed that all transla-
tions in the set Xi are reliable, and none of them are
discarded. Thus, in each iteration, the result of the
selection step will be Ti = Xi. This method was
implemented mainly for comparison with other se-
lection methods.
3.5 Filtering the Training Data
In general, having more training data improves the
quality of the trained models. However, when it
comes to the translation of a particular test set, the
question is whether all of the available training data
are relevant to the translation task or not. Moreover,
working with large amounts of training data requires
more computational power. So if we can identify a
subset of training data which are relevant to the cur-
rent task and use only this to re-train the models, we
can reduce computational complexity significantly.
We propose to Filter the training data, either
bilingual or monolingual text, to identify the parts
</bodyText>
<table confidence="0.944449814814815">
28
EuroParl phrase table+LM 688K
train100k phrase table 100K
train150k phrase table 150K
dev06 dev1 2,000
test06 test 3,064
Table 1: French–English corpora
corpus use sentences
non-UN phrase table+LM 3.2M
UN phrase table+LM 5.0M
English Gigaword LM 11.7M
multi-p3 dev1 935
multi-p4 dev2 919
eval-04 test 1,788
eval-06 test 3,940
Table 2: NIST Chinese–English corpora
which are relevant w.r.t. the test set. This filtering
is based on n-gram coverage. For a source sentence
s in the training data, its n-gram coverage over the
sentences in the test set is computed. The average
over several n-gram lengths is used as a measure
of relevance of this training sentence w.r.t. the test
corpus. Based on this, we select the top K source
sentences or sentence pairs.
4 Experimental Results
4.1 Setting
We ran experiments on two different corpora: one
is the French–English translation task from the Eu-
roParl corpus, and the other one is Chinese–English
translation as performed in the NIST MT evaluation
(www.nist.gov/speech/tests/mt).
For the French–English translation task, we used
the EuroParl corpus as distributed for the shared task
in the NAACL 2006 workshop on statistical ma-
chine translation. The corpus statistics are shown
in Table 1. Furthermore we filtered the EuroParl
corpus, as explained in Section 3.5, to create two
smaller bilingual corpora (train100k and train150k
in Table 1). The development set is used to optimize
the model weights in the decoder, and the evaluation
is done on the test set provided for the NAACL 2006
shared task.
For the Chinese–English translation task, we used
the corpora distributed for the large-data track in the
setting EuroParl NIST
full re-training w/ filtering ∗ ∗∗
full re-training ∗∗ †
mixture model ∗ †
new phrase table ff:
keep all ∗∗ ∗
imp. sampling norm. ∗∗ ∗
conf. ∗∗ ∗
threshold norm. ∗∗ ∗
conf. ∗∗ ∗
</table>
<tableCaption confidence="0.999698">
Table 3: Feasibility of settings for Algorithm 1
</tableCaption>
<bodyText confidence="0.999573172413793">
2006 NIST evaluation (see Table 2). We used the
LDC segmenter for Chinese. The multiple transla-
tion corpora multi-p3 and multi-p4 were used as de-
velopment corpora. Evaluation was performed on
the 2004 and 2006 test sets. Note that the train-
ing data consists mainly of written text, whereas the
test sets comprise three and four different genres:
editorials, newswire and political speeches in the
2004 test set, and broadcast conversations, broad-
cast news, newsgroups and newswire in the 2006
test set. Most of these domains have characteristics
which are different from those of the training data,
e.g., broadcast conversations have characteristics of
spontaneous speech, and the newsgroup data is com-
paratively unstructured.
Given the particular data sets described above, Ta-
ble 3 shows the various options for the Estimate,
Score and Select functions (see Section 3). The ta-
ble provides a quick guide to the experiments we
present in this paper vs. those we did not attempt due
to computational infeasibility. We ran experiments
corresponding to all entries marked with ∗ (see Sec-
tion 4.2). For those marked ∗∗ the experiments pro-
duced only minimal improvement over the baseline
and so we do not discuss them in this paper. The en-
tries marked as † were not attempted because they
are not feasible (e.g. full re-training on the NIST
data). However, these were run on the smaller Eu-
roParl corpus.
</bodyText>
<subsectionHeader confidence="0.626638">
Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999935238095238">
We evaluated the generated translations using
three different evaluation metrics: BLEU score (Pa-
pineni et al., 2002), mWER (multi-reference word
error rate), and mPER (multi-reference position-
independent word error rate) (Nießen et al., 2000).
Note that BLEU score measures quality, whereas
mWER and mPER measure translation errors. We
will present 95%-confidence intervals for the base-
line system which are calculated using bootstrap re-
sampling. The metrics are calculated w.r.t. one and
four English references: the EuroParl data comes
with one reference, the NIST 2004 evaluation set
and the NIST section of the 2006 evaluation set
are provided with four references each, whereas the
GALE section of the 2006 evaluation set comes
with one reference only. This results in much lower
BLEU scores and higher error rates for the transla-
tions of the GALE set (see Section 4.2). Note that
these values do not indicate lower translation qual-
ity, but are simply a result of using only one refer-
ence.
</bodyText>
<sectionHeader confidence="0.962085" genericHeader="method">
4.2 Results
EuroParl
</sectionHeader>
<bodyText confidence="0.999966296296297">
We ran our initial experiments on EuroParl to ex-
plore the behavior of the transductive learning algo-
rithm. In all experiments reported in this subsec-
tion, the test set was used as unlabeled data. The
selection and scoring was carried out using impor-
tance sampling with normalized scores. In one set
of experiments, we used the 100K and 150K train-
ing sentences filtered according to n-gram coverage
over the test set. We fully re-trained the phrase ta-
bles on these data and 8,000 test sentence pairs sam-
pled from 20-best lists in each iteration. The results
on the test set can be seen in Figure 1. The BLEU
score increases, although with slight variation, over
the iterations. In total, it increases from 24.1 to 24.4
for the 100K filtered corpus, and from 24.5 to 24.8
for 150K, respectively. Moreover, we see that the
BLEU score of the system using 100K training sen-
tence pairs and transductive learning is the same as
that of the one trained on 150K sentence pairs. So
the information extracted from untranslated test sen-
tences is equivalent to having an additional 50K sen-
tence pairs.
In a second set of experiments, we used the whole
EuroParl corpus and the sampled sentences for fully
re-training the phrase tables in each iteration. We
ran the algorithm for three iterations and the BLEU
score increased from 25.3 to 25.6. Even though this
</bodyText>
<page confidence="0.993625">
29
</page>
<figure confidence="0.995363045454545">
24.45
24.4
24.35
24.3
24.25
24.2
24.15
24.1
24.05
0 2 4 6 8 10 12 14 16 18
24.85
24.8
24.75
24.7
24.65
24.6
24.55
24.5
24.45
0 2 4 6 8 10 12 14 16
e so
Iteration Iteration
</figure>
<figureCaption confidence="0.997352">
Figure 1: Translation quality for importance sampling with full re-training on train100k (left) and train150k
(right). EuroParl French–English task.
</figureCaption>
<bodyText confidence="0.971289294117647">
is a small increase, it shows that the unlabeled data
contains some information which can be explored in
transductive learning.
In a third experiment, we applied the mixture
model idea as explained in Section 3.2. The initially
learned phrase table was merged with the learned
phrase table in each iteration with a weight of A =
0.1. This value for A was found based on cross val-
idation on a development set. We ran the algorithm
for 20 iterations and BLEU score increased from
25.3 to 25.7. Since this is very similar to the re-
sult obtained with the previous method, but with an
additional parameter A to optimize, we did not use
mixture models on NIST.
Note that the single improvements achieved here
are slightly below the 95%-significance level. How-
ever, we observe them consistently in all settings.
</bodyText>
<sectionHeader confidence="0.603144" genericHeader="method">
NIST
</sectionHeader>
<bodyText confidence="0.999874209302326">
Table 4 presents translation results on NIST with
different versions of the scoring and selection meth-
ods introduced in Section 3. In these experiments,
the unlabeled data U for Algorithm 1 is the develop-
ment or test corpus. For this corpus U, 5,000-best
lists were generated using the baseline SMT system.
Since re-training the full phrase tables is not feasi-
ble here, a (small) additional phrase table, specific to
U, was trained and plugged into the SMT system as
an additional model. The decoder weights thus had
to be optimized again to determine the appropriate
weight for this new phrase table. This was done on
the dev1 corpus, using the phrase table specific to
dev1. Every time a new corpus is to be translated,
an adapted phrase table is created using transductive
learning and used with the weight which has been
learned on dev1. In the first experiment presented
in Table 4, all of the generated 1-best translations
were kept and used for training the adapted phrase
tables. This method yields slightly higher transla-
tion quality than the baseline system. The second
approach we studied is the use of importance sam-
pling (IS) over 20-best lists, based either on length-
normalized sentence scores (norm.) or confidence
scores (conf.). As the results in Table 4 show, both
variants outperform the first method, with a consis-
tent improvement over the baseline across all test
corpora and evaluation metrics. The third method
uses a threshold-based selection method. Combined
with confidence estimation as scoring method, this
yields the best results. All improvements over the
baseline are significant at the 95%-level.
Table 5 shows the translation quality achieved on
the NIST test sets when additional source language
data from the Chinese Gigaword corpus compris-
ing newswire text is used for transductive learning.
These Chinese sentences were sorted according to
their n-gram overlap (see Section 3.5) with the de-
velopment corpus, and the top 5,000 Chinese sen-
tences were used. The selection and scoring in Al-
gorithm 1 were performed using confidence estima-
tion with a threshold. Again, a new phrase table was
trained on these data. As can be seen in Table 5, this
</bodyText>
<page confidence="0.996173">
30
</page>
<table confidence="0.412741">
select score BLEU[%] mWER[%] mPER[%]
</table>
<tableCaption confidence="0.534974">
Table 4: Translation quality using an additional
adapted phrase table trained on the dev/test sets.
Different selection and scoring methods. NIST
Chinese–English, best results printed in boldface.
</tableCaption>
<bodyText confidence="0.999782583333333">
system outperforms the baseline system on all test
corpora. The error rates are significantly reduced in
all three settings, and BLEU score increases in all
cases. A comparison with Table 4 shows that trans-
ductive learning on the development set and test cor-
pora, adapting the system to their domain and style,
is more effective in improving the SMT system than
the use of additional source language data.
In all experiments on NIST, Algorithm 1 was run
for one iteration. We also investigated the use of an
iterative procedure here, but this did not yield any
improvement in translation quality.
</bodyText>
<sectionHeader confidence="0.996674" genericHeader="method">
5 Previous Work
</sectionHeader>
<bodyText confidence="0.971880142857143">
Semi-supervised learning has been previously ap-
plied to improve word alignments. In (Callison-
Burch et al., 2004), a generative model for word
alignment is trained using unsupervised learning on
parallel text. In addition, another model is trained on
a small amount of hand-annotated word alignment
data. A mixture model provides a probability for
</bodyText>
<table confidence="0.9995709">
system BLEU[%] mWER[%] mPER[%]
eval-04 (4 refs.)
baseline 31.8±0.7 66.8±0.7 41.5±0.5
add Chin. data 32.8 65.7 40.9
eval-06 GALE (1 ref.)
baseline 12.7±0.5 75.8±0.6 54.6±0.6
add Chin. data 13.1 73.9 53.5
eval-06 NIST (4 refs.)
baseline 27.9±0.7 67.2±0.6 44.0±0.5
add Chin. data 28.1 65.8 43.2
</table>
<tableCaption confidence="0.993419">
Table 5: Translation quality using an additional
</tableCaption>
<bodyText confidence="0.98452725">
phrase table trained on monolingual Chinese news
data. Selection step using threshold on confidence
scores. NIST Chinese–English.
word alignment. Experiments showed that putting a
large weight on the model trained on labeled data
performs best. Along similar lines, (Fraser and
Marcu, 2006) combine a generative model of word
alignment with a log-linear discriminative model
trained on a small set of hand aligned sentences. The
word alignments are used to train a standard phrase-
based SMT system, resulting in increased translation
quality .
In (Callison-Burch, 2002) co-training is applied
to MT. This approach requires several source lan-
guages which are sentence-aligned with each other
and all translate into the same target language. One
language pair creates data for another language pair
and can be naturally used in a (Blum and Mitchell,
1998)-style co-training algorithm. Experiments on
the EuroParl corpus show a decrease in WER. How-
ever, the selection algorithm applied there is actually
supervised because it takes the reference translation
into account. Moreover, when the algorithm is run
long enough, large amounts of co-trained data in-
jected too much noise and performance degraded.
Self-training for SMT was proposed in (Ueffing,
2006). An existing SMT system is used to translate
the development or test corpus. Among the gener-
ated machine translations, the reliable ones are au-
tomatically identified using thresholding on confi-
dence scores. The work which we presented here
differs from (Ueffing, 2006) as follows:
</bodyText>
<listItem confidence="0.615467666666667">
• We investigated different ways of scoring and
selecting the reliable translations and compared
our method to this work. In addition to the con-
</listItem>
<table confidence="0.897337866666667">
eval-04 (4 refs.)
baseline 31.8±0.7
keep all 33.1
IS norm. 33.5
conf. 33.2
thr norm. 33.5
conf. 33.5
eval-06 GALE (1 ref.)
baseline 12.7 ±0.5
keep all 12.9
IS norm. 13.2
conf. 12.9
thr norm. 12.7
conf. 13.6
eval-06 NIST (4 refs.)
</table>
<figure confidence="0.9187045">
baseline 27.9±0.7
keep all 28.1
IS norm. 28.7
conf. 28.4
thr norm. 28.3
conf. 29.3
66.8±0.7 41.5±0.5
66.0 41.3
65.8 40.9
65.6 40.4
65.9 40.8
65.3 40.8
75.8 ±0.6 54.6 ±0.6
75.7 55.0
74.7 54.1
74.4 53.5
75.2 54.2
73.4 53.2
67.2 ±0.6 44.0 ±0.5
66.5 44.2
66.1 43.6
65.8 43.2
66.1 43.5
65.6 43.2
</figure>
<page confidence="0.999864">
31
</page>
<bodyText confidence="0.978880666666667">
fidence estimation used there, we applied im-
portance sampling and combined it with confi-
dence estimation for transductive learning.
</bodyText>
<listItem confidence="0.901674363636364">
• We studied additional ways of exploring the
newly created bilingual data, namely re-
training the full phrase translation model or cre-
ating a mixture model.
• We proposed an iterative procedure which
translates the monolingual source language
data anew in each iteration and then re-trains
the phrase translation model.
• We showed how additional monolingual
source-language data can be used in transduc-
tive learning to improve the SMT system.
</listItem>
<sectionHeader confidence="0.999707" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999920266666667">
It is not intuitively clear why the SMT system can
learn something from its own output and is improved
through semi-supervised learning. There are two
main reasons for this improvement: Firstly, the se-
lection step provides important feedback for the sys-
tem. The confidence estimation, for example, dis-
cards translations with low language model scores or
posterior probabilities. The selection step discards
bad machine translations and reinforces phrases of
high quality. As a result, the probabilities of low-
quality phrase pairs, such as noise in the table or
overly confident singletons, degrade. Our experi-
ments comparing the various settings for transduc-
tive learning shows that selection clearly outper-
forms the method which keeps all generated transla-
tions as additional training data. The selection meth-
ods investigated here have been shown to be well-
suited to boost the performance of semi-supervised
learning for SMT.
Secondly, our algorithm constitutes a way of
adapting the SMT system to a new domain or style
without requiring bilingual training or development
data. Those phrases in the existing phrase tables
which are relevant for translating the new data are
reinforced. The probability distribution over the
phrase pairs thus gets more focused on the (reliable)
parts which are relevant for the test data. For an anal-
ysis of the self-trained phrase tables, examples of
translated sentences, and the phrases used in trans-
lation, see (Ueffing, 2006).
</bodyText>
<sectionHeader confidence="0.996257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999142847826087">
S. Abney. 2004. Understanding the Yarowsky Algo-
rithm. Comput. Ling., 30(3).
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003. Confidence estimation for machine transla-
tion. Final report, JHU/CLSP Summer Workshop.
www.clsp.jhu.edu/ws2003/groups/estimate/.
A. Blum and T. Mitchell. 1998. Combining Labeled and
Unlabeled Data with Co-Training. In Proc. Computa-
tional Learning Theory.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Compu-
tational Linguistics, 19(2).
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In Proc. ACL.
C. Callison-Burch. 2002. Co-training for statistical ma-
chine translation. Master’s thesis, School of Informat-
ics, University of Edinburgh.
A. Fraser and D. Marcu. 2006. Semi-supervised training
for statistical word alignment. In Proc. ACL.
S. Nießen, F. J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evalua-
tion for MT research. In Proc. LREC.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. ICSLP.
N. Ueffing and H. Ney. 2007. Word-level confidence es-
timation for machine translation. Computational Lin-
guistics, 33(1):9–40.
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.
2007. NRC’s Portage system for WMT 2007. In
Proc. ACL Workshop on SMT.
N. Ueffing. 2006. Using monolingual source-language
data to improve MT performance. In Proc. IWSLT.
D. Yarowsky. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proc. ACL.
R. Zens and H. Ney. 2006. N-gram posterior
probabilities for statistical machine translation. In
Proc. HLT/NAACL Workshop on SMT.
</reference>
<page confidence="0.999281">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945520">
<title confidence="0.99892">Transductive learning for statistical machine translation</title>
<author confidence="0.98349">Ueffing Gholamreza Haffari Sarkar</author>
<affiliation confidence="0.999982">National Research Council Canada Simon Fraser University</affiliation>
<address confidence="0.996588">Gatineau, QC, Canada Burnaby, BC, Canada</address>
<abstract confidence="0.9978253125">Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language. In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality. We propose several algorithms with this aim, and present the strengths and weaknesses of each one. We present detailed experimental evaluations on the French–English EuroParl data set and on data from the NIST Chinese–English largedata track. We show a significant improvement in translation quality on both tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Understanding the Yarowsky Algorithm.</title>
<date>2004</date>
<journal>Comput. Ling.,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="7609" citStr="Abney, 2004" startWordPosition="1224" endWordPosition="1225"> shown to improve MT performance. and of the decoder models are again optimized to 3.2 The Estimate Function maximize BLEU score. This is performed on a sec- We consider the following different definitions for ond development corpus, dev2. Estimate in Algorithm 1: 3 The Framework Full Re-training (of all translation models): If 3.1 The Algorithm Estimate(L, T) estimates the model parameters Our transductive learning algorithm, Algorithm 1, based on L U T, then we have a semi-supervised alis inspired by the Yarowsky algorithm (Yarowsky, gorithm that re-trains a model on the original train1995; Abney, 2004). The algorithm works as fol- ing data L plus the sentences decoded in the last itlows: First, the translation model is estimated based eration. The size of L can be controlled by filtering on the sentence pairs in the bilingual training data L. the training data (see Section 3.5). Then, a set of source language sentences, U, is trans- Additional Phrase Table: If, on the other hand, a lated based on the current model. A subset of good new phrase translation table is learned on T only translations and their sources, Ti, is selected in each and then added as a new component in the log-linear 26 </context>
</contexts>
<marker>Abney, 2004</marker>
<rawString>S. Abney. 2004. Understanding the Yarowsky Algorithm. Comput. Ling., 30(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blatz</author>
<author>E Fitzgerald</author>
<author>G Foster</author>
<author>S Gandrabur</author>
<author>C Goutte</author>
<author>A Kulesza</author>
<author>A Sanchis</author>
<author>N Ueffing</author>
</authors>
<title>Confidence estimation for machine translation. Final report, JHU/CLSP Summer Workshop.</title>
<date>2003</date>
<note>www.clsp.jhu.edu/ws2003/groups/estimate/.</note>
<contexts>
<context position="10350" citStr="Blatz et al., 2003" startWordPosition="1710" endWordPosition="1713">estimated on L and T, respectively. In cases where new phrase pairs are learned from T, they get added into the merged phrase table. 3.3 The Scoring Function In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence pair (t, s) is scored according to the model probability p(t |s) normalized by the length |t |of the target sentence: Score(t, s) = p(t |s) Confidence Estimation: The confidence estimation which we implemented follows the approaches suggested in (Blatz et al., 2003; Ueffing and Ney, 2007): The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities, Levenshtein-based word posterior probabilities, and a target language model score. The weights of the different scores are optimized w.r.t. classification error rate (CER). The phrase posterior probabilities are determined by summing the sentence probabilities of all translation hypotheses in the N-best list which contain this phrase pair. The segmentation of the sentence into phrases is provided by the decoder. This sum is then normalized by the to</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2003. Confidence estimation for machine translation. Final report, JHU/CLSP Summer Workshop. www.clsp.jhu.edu/ws2003/groups/estimate/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proc. Computational Learning Theory.</booktitle>
<contexts>
<context position="25444" citStr="Blum and Mitchell, 1998" startWordPosition="4200" endWordPosition="4203">a performs best. Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. The word alignments are used to train a standard phrasebased SMT system, resulting in increased translation quality . In (Callison-Burch, 2002) co-training is applied to MT. This approach requires several source languages which are sentence-aligned with each other and all translate into the same target language. One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm. Experiments on the EuroParl corpus show a decrease in WER. However, the selection algorithm applied there is actually supervised because it takes the reference translation into account. Moreover, when the algorithm is run long enough, large amounts of co-trained data injected too much noise and performance degraded. Self-training for SMT was proposed in (Ueffing, 2006). An existing SMT system is used to translate the development or test corpus. Among the generated machine translations, the reliable ones are automatically identified using thresholding on confidence</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proc. Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>D Talbot</author>
<author>M Osborne</author>
</authors>
<title>Statistical machine translation with word- and sentence-aligned parallel corpora.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Callison-Burch, Talbot, Osborne, 2004</marker>
<rawString>C. Callison-Burch, D. Talbot, and M. Osborne. 2004. Statistical machine translation with word- and sentence-aligned parallel corpora. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>Co-training for statistical machine translation. Master’s thesis,</title>
<date>2002</date>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="25160" citStr="Callison-Burch, 2002" startWordPosition="4156" endWordPosition="4157">3.2 Table 5: Translation quality using an additional phrase table trained on monolingual Chinese news data. Selection step using threshold on confidence scores. NIST Chinese–English. word alignment. Experiments showed that putting a large weight on the model trained on labeled data performs best. Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. The word alignments are used to train a standard phrasebased SMT system, resulting in increased translation quality . In (Callison-Burch, 2002) co-training is applied to MT. This approach requires several source languages which are sentence-aligned with each other and all translate into the same target language. One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm. Experiments on the EuroParl corpus show a decrease in WER. However, the selection algorithm applied there is actually supervised because it takes the reference translation into account. Moreover, when the algorithm is run long enough, large amounts of co-trained data injected too much </context>
</contexts>
<marker>Callison-Burch, 2002</marker>
<rawString>C. Callison-Burch. 2002. Co-training for statistical machine translation. Master’s thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Semi-supervised training for statistical word alignment.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="24882" citStr="Fraser and Marcu, 2006" startWordPosition="4111" endWordPosition="4114">LEU[%] mWER[%] mPER[%] eval-04 (4 refs.) baseline 31.8±0.7 66.8±0.7 41.5±0.5 add Chin. data 32.8 65.7 40.9 eval-06 GALE (1 ref.) baseline 12.7±0.5 75.8±0.6 54.6±0.6 add Chin. data 13.1 73.9 53.5 eval-06 NIST (4 refs.) baseline 27.9±0.7 67.2±0.6 44.0±0.5 add Chin. data 28.1 65.8 43.2 Table 5: Translation quality using an additional phrase table trained on monolingual Chinese news data. Selection step using threshold on confidence scores. NIST Chinese–English. word alignment. Experiments showed that putting a large weight on the model trained on labeled data performs best. Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. The word alignments are used to train a standard phrasebased SMT system, resulting in increased translation quality . In (Callison-Burch, 2002) co-training is applied to MT. This approach requires several source languages which are sentence-aligned with each other and all translate into the same target language. One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm. Experime</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>A. Fraser and D. Marcu. 2006. Semi-supervised training for statistical word alignment. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nießen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research. In</title>
<date>2000</date>
<booktitle>Proc. LREC.</booktitle>
<contexts>
<context position="17620" citStr="Nießen et al., 2000" startWordPosition="2907" endWordPosition="2910">onding to all entries marked with ∗ (see Section 4.2). For those marked ∗∗ the experiments produced only minimal improvement over the baseline and so we do not discuss them in this paper. The entries marked as † were not attempted because they are not feasible (e.g. full re-training on the NIST data). However, these were run on the smaller EuroParl corpus. Evaluation Metrics We evaluated the generated translations using three different evaluation metrics: BLEU score (Papineni et al., 2002), mWER (multi-reference word error rate), and mPER (multi-reference positionindependent word error rate) (Nießen et al., 2000). Note that BLEU score measures quality, whereas mWER and mPER measure translation errors. We will present 95%-confidence intervals for the baseline system which are calculated using bootstrap resampling. The metrics are calculated w.r.t. one and four English references: the EuroParl data comes with one reference, the NIST 2004 evaluation set and the NIST section of the 2006 evaluation set are provided with four references each, whereas the GALE section of the 2006 evaluation set comes with one reference only. This results in much lower BLEU scores and higher error rates for the translations o</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>S. Nießen, F. J. Och, G. Leusch, and H. Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="5294" citStr="Och, 2003" startWordPosition="854" endWordPosition="855"> from a development set or the test set that will be a distortion model which assigns a penalty based used eventually to evaluate the SMT system or from on the number of source words which are skipped additional data which is relevant to the development when generating a new target phrase, and (d) a word or test set. In Algorithm 1, changing the definition penalty. These different models are combined log- of Estimate, Score and Select will give us the diflinearly. Their weights are optimized w.r.t. BLEU ferent semi-supervised learning algorithms we will score using the algorithm described in (Och, 2003). discuss in this paper. This is done on a development corpus which we will Given the probability model p(t |s), consider the call dev1 in this paper. The search algorithm imple- distribution over all possible valid translations t for mented in the decoder is a dynamic-programming a particular input sentence s. We can initialize beam-search algorithm. this probability distribution to the uniform distribuAfter the main decoding step, rescoring with ad- tion for each sentence s in the unlabeled data U. ditional models is performed. The system generates Thus, this distribution over translations o</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="17494" citStr="Papineni et al., 2002" startWordPosition="2889" endWordPosition="2893">experiments we present in this paper vs. those we did not attempt due to computational infeasibility. We ran experiments corresponding to all entries marked with ∗ (see Section 4.2). For those marked ∗∗ the experiments produced only minimal improvement over the baseline and so we do not discuss them in this paper. The entries marked as † were not attempted because they are not feasible (e.g. full re-training on the NIST data). However, these were run on the smaller EuroParl corpus. Evaluation Metrics We evaluated the generated translations using three different evaluation metrics: BLEU score (Papineni et al., 2002), mWER (multi-reference word error rate), and mPER (multi-reference positionindependent word error rate) (Nießen et al., 2000). Note that BLEU score measures quality, whereas mWER and mPER measure translation errors. We will present 95%-confidence intervals for the baseline system which are calculated using bootstrap resampling. The metrics are calculated w.r.t. one and four English references: the EuroParl data comes with one reference, the NIST 2004 evaluation set and the NIST section of the 2006 evaluation set are provided with four references each, whereas the GALE section of the 2006 eval</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="4466" citStr="Stolcke, 2002" startWordPosition="712" endWordPosition="713">iption here; for a lected sentence pairs are replaced in each iteration, detailed description see (Ueffing et al., 2007). and only the original bilingual training data, L, is The models (or features) which are employed by kept fixed throughout the algorithm. The process the decoder are: (a) one or several phrase table(s), of generating sentence pairs, selecting a subset of which model the translation directionp(s |t), (b) one good sentence pairs, and updating the model is conor several n-gram language model(s) trained with tinued until a stopping condition is met. Note that the SRILM toolkit (Stolcke, 2002); in the experi- we run this algorithm in a transductive setting which ments reported here, we used 4-gram models on the means that the set of sentences U is drawn either NIST data, and a trigram model on EuroParl, (c) from a development set or the test set that will be a distortion model which assigns a penalty based used eventually to evaluate the SMT system or from on the number of source words which are skipped additional data which is relevant to the development when generating a new target phrase, and (d) a word or test set. In Algorithm 1, changing the definition penalty. These differen</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Word-level confidence estimation for machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="6646" citStr="Ueffing and Ney, 2007" startWordPosition="1066" endWordPosition="1070">ese lists are rescored with the certain precise conditions, as described in (Abney, following models: (a) the different models used in 2004), we can analyze Algorithm 1 as minimizing the decoder which are described above, (b) two dif- the entropy of the distribution over translations of U. ferent features based on IBM Model 1 (Brown et al., However, this is true only when the functions Esti1993), (c) posterior probabilities for words, phrases, mate, Score and Select have very prescribed definin-grams, and sentence length (Zens and Ney, 2006; tions. In this paper, rather than analyze the converUeffing and Ney, 2007), all calculated over the N- gence of Algorithm 1 we run it for a fixed number best list and using the sentence probabilities which of iterations and instead focus on finding useful defthe baseline system assigns to the translation hy- initions for Estimate, Score and Select that can be potheses. The weights of these additional models experimentally shown to improve MT performance. and of the decoder models are again optimized to 3.2 The Estimate Function maximize BLEU score. This is performed on a sec- We consider the following different definitions for ond development corpus, dev2. Estimate </context>
<context position="10374" citStr="Ueffing and Ney, 2007" startWordPosition="1714" endWordPosition="1717">, respectively. In cases where new phrase pairs are learned from T, they get added into the merged phrase table. 3.3 The Scoring Function In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence pair (t, s) is scored according to the model probability p(t |s) normalized by the length |t |of the target sentence: Score(t, s) = p(t |s) Confidence Estimation: The confidence estimation which we implemented follows the approaches suggested in (Blatz et al., 2003; Ueffing and Ney, 2007): The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities, Levenshtein-based word posterior probabilities, and a target language model score. The weights of the different scores are optimized w.r.t. classification error rate (CER). The phrase posterior probabilities are determined by summing the sentence probabilities of all translation hypotheses in the N-best list which contain this phrase pair. The segmentation of the sentence into phrases is provided by the decoder. This sum is then normalized by the total probability mass of </context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>N. Ueffing and H. Ney. 2007. Word-level confidence estimation for machine translation. Computational Linguistics, 33(1):9–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>M Simard</author>
<author>S Larkin</author>
<author>J H Johnson</author>
</authors>
<title>NRC’s Portage system for WMT</title>
<date>2007</date>
<booktitle>In Proc. ACL Workshop on SMT.</booktitle>
<contexts>
<context position="3972" citStr="Ueffing et al., 2007" startWordPosition="631" endWordPosition="634">e new input. 2 Baseline MT System The SMT system we applied in our experiments is PORTAGE. This is a state-of-the-art phrase-based translation system which has been made available Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 25–32, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics to Canadian universities for research and education iteration and added to the training data. These sepurposes. We provide a basic description here; for a lected sentence pairs are replaced in each iteration, detailed description see (Ueffing et al., 2007). and only the original bilingual training data, L, is The models (or features) which are employed by kept fixed throughout the algorithm. The process the decoder are: (a) one or several phrase table(s), of generating sentence pairs, selecting a subset of which model the translation directionp(s |t), (b) one good sentence pairs, and updating the model is conor several n-gram language model(s) trained with tinued until a stopping condition is met. Note that the SRILM toolkit (Stolcke, 2002); in the experi- we run this algorithm in a transductive setting which ments reported here, we used 4-gram</context>
</contexts>
<marker>Ueffing, Simard, Larkin, Johnson, 2007</marker>
<rawString>N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson. 2007. NRC’s Portage system for WMT 2007. In Proc. ACL Workshop on SMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
</authors>
<title>Using monolingual source-language data to improve MT performance.</title>
<date>2006</date>
<booktitle>In Proc. IWSLT. D. Yarowsky.</booktitle>
<contexts>
<context position="9491" citStr="Ueffing, 2006" startWordPosition="1567" endWordPosition="1568">h their scores 11: Xi := Xi U I(tn, s, 7r(i)(tn |s))N n�1� 12: end for 13: Scoring step: Si := Score(Xi) // Assign a score to sentence pairs (t, s) from X. 14: Selection step: Ti := Select(Xi, Si) // Choose a subset of good sentence pairs (t, s) from X. 15: i := i + 1. 16: until i &gt; R of the model on labeled and unlabeled data which can be very expensive if L is very large (as on the Chinese–English data set). This additional phrase table is small and specific to the development or test set it is trained on. It overlaps with the original phrase tables, but also contains many new phrase pairs (Ueffing, 2006). Mixture Model: Another alternative for Estimate is to create a mixture model of the phrase table probabilities with new phrase table probabilities p(s|t) = A - Lp(s|t) + (1 − A) - Tp(s|t) (2) where Lp and Tp are phrase table probabilities estimated on L and T, respectively. In cases where new phrase pairs are learned from T, they get added into the merged phrase table. 3.3 The Scoring Function In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence pair (</context>
<context position="25845" citStr="Ueffing, 2006" startWordPosition="4261" endWordPosition="4262">nguages which are sentence-aligned with each other and all translate into the same target language. One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm. Experiments on the EuroParl corpus show a decrease in WER. However, the selection algorithm applied there is actually supervised because it takes the reference translation into account. Moreover, when the algorithm is run long enough, large amounts of co-trained data injected too much noise and performance degraded. Self-training for SMT was proposed in (Ueffing, 2006). An existing SMT system is used to translate the development or test corpus. Among the generated machine translations, the reliable ones are automatically identified using thresholding on confidence scores. The work which we presented here differs from (Ueffing, 2006) as follows: • We investigated different ways of scoring and selecting the reliable translations and compared our method to this work. In addition to the coneval-04 (4 refs.) baseline 31.8±0.7 keep all 33.1 IS norm. 33.5 conf. 33.2 thr norm. 33.5 conf. 33.5 eval-06 GALE (1 ref.) baseline 12.7 ±0.5 keep all 12.9 IS norm. 13.2 conf</context>
</contexts>
<marker>Ueffing, 2006</marker>
<rawString>N. Ueffing. 2006. Using monolingual source-language data to improve MT performance. In Proc. IWSLT. D. Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>N-gram posterior probabilities for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. HLT/NAACL Workshop on SMT.</booktitle>
<contexts>
<context position="6570" citStr="Zens and Ney, 2006" startWordPosition="1054" endWordPosition="1057">for each from U will have the maximum entropy. Under source sentence. These lists are rescored with the certain precise conditions, as described in (Abney, following models: (a) the different models used in 2004), we can analyze Algorithm 1 as minimizing the decoder which are described above, (b) two dif- the entropy of the distribution over translations of U. ferent features based on IBM Model 1 (Brown et al., However, this is true only when the functions Esti1993), (c) posterior probabilities for words, phrases, mate, Score and Select have very prescribed definin-grams, and sentence length (Zens and Ney, 2006; tions. In this paper, rather than analyze the converUeffing and Ney, 2007), all calculated over the N- gence of Algorithm 1 we run it for a fixed number best list and using the sentence probabilities which of iterations and instead focus on finding useful defthe baseline system assigns to the translation hy- initions for Estimate, Score and Select that can be potheses. The weights of these additional models experimentally shown to improve MT performance. and of the decoder models are again optimized to 3.2 The Estimate Function maximize BLEU score. This is performed on a sec- We consider the</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. N-gram posterior probabilities for statistical machine translation. In Proc. HLT/NAACL Workshop on SMT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>