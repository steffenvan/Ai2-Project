<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000587">
<title confidence="0.964292">
Class-Based Probability Estimation using a Semantic Hierarchy
</title>
<author confidence="0.963989">
Stephen Clark
</author>
<affiliation confidence="0.837168">
Division of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh
</affiliation>
<address confidence="0.988445">
EH8 9LW, UK
</address>
<email confidence="0.999218">
stephenc@cogsci.ed.ac.uk
</email>
<author confidence="0.996446">
David Weir
</author>
<affiliation confidence="0.9963625">
School of Cognitive and Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.8253685">
Falmer, Brighton
BN1 9QH, UK
</address>
<email confidence="0.999259">
david.weir@cogs.susx.ac.uk
</email>
<sectionHeader confidence="0.987709" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937272727273">
This paper concerns the acquisition of a par-
ticular kind of lexical knowledge, namely the
knowledge of which noun senses can fill argu-
ment slots of predicates. Probabilities are used
to represent the knowledge, and classes from
a semantic hierarchy are used to estimate the
probabilities. There is a particular focus on the
problem of how to determine a suitable class,
or level of generalisation, in the hierarchy. A
pseudo disambiguation task is used to compare
different class-based estimation methods.
</bodyText>
<sectionHeader confidence="0.993788" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890793103448">
This paper concerns the problem of how to esti-
mate the probability of a noun sense appearing
as a particular argument to a predicate. The
problem with estimating a probability model
over senses is that this involves a huge number of
parameters, which results in a sparse data prob-
lem. The proposal here is to define a probability
model over senses in a semantic hierarchy, and
exploit the fact that senses can be grouped into
classes consisting of semantically similar senses.
Defining probabilities in terms of classes means
that the number of parameters is reduced. The
assumption underlying this approach is that the
probability of a sense can be approximated by
a probability based on a suitably chosen class.
The hierarchy used is the noun hypernym hi-
erarchy of WordNet (Fellbaum, 1998), which
consists of senses, or lexicalised concepts&apos;, re-
lated by the &apos;is-a-kind-of&apos; relation. If c is-a-
kind-of c&apos;, then c&apos; is a hypernym of c. To es-
timate the probability of a concept, c, appear-
ing as an argument of a predicate, a set of con-
cepts dominated by a hypernym of c is chosen
to represent c. We develop a novel solution to
the problem of how to determine a suitable hy-
pernym, or level of generalisation, in the hier-
archy. A pseudo disambiguation task is used
to compare our class-based estimation method
with some alternative proposals.
</bodyText>
<sectionHeader confidence="0.932338" genericHeader="introduction">
2 The Semantic Hierarchy
</sectionHeader>
<bodyText confidence="0.988302152173913">
We use the noun hypernym hierarchy of Word-
Net, version 1.6. A sense, or concept, in
WordNet is represented by a synset&apos;, which
is the set of synonymous words that can be
used to denote that concept.&apos; For exam-
ple, the synset for the concept (cocaine) is
{ cocaine, cocain, coke, snow, C}. Let syn(c) be
the synset for concept c, and let cn(n) =
{c Inc syn(c) } be the set of concepts that can
be denoted by noun n.
The hierarchy has the structure of a DAG,
with what we call the direct-isa&apos; relation con-
necting nodes in the graph. Let isa = direct-isa*
be the transitive reflexive closure of direct-isa,
so that (c, c&apos;) E isa c&apos; is a hypernym of c. We
use c&apos; = { c I (c, c&apos;) E isa } to denote the set con-
sisting of c&apos; and those concepts dominated by
c&apos;. For example, (animal) is the set consisting
of those concepts that denote kinds of animals.
The probability of a concept appearing as
an argument of a predicate is written p(clv, r),
where c is a concept in WordNet, v is a predi-
cate and r is an argument position. The focus in
this paper is on verbs, but the techniques can
be applied to any predicate that takes nomi-
nal arguments. The probability p(clv,r) is to
be interpreted as follows: this is the probabil-
ity that some noun n in syn(c), when denoting
concept c, appears in position r of verb v (given
v and r). The example used throughout the
paper is p((dog) I run, subj), which is the condi-
tional probability that some noun in the synset
1-Note that we are using concept to refer to a lexi-
calised concept or sense, and not a set of senses. Angled
brackets are used to denote concepts in the hierarchy.
of (dog), when denoting (dog), appears in the
subject position of the verb run.
The data used to estimate the probabilities is
assumed to be in the form of (n, v, r) triples: a
noun, verb and argument position. Such data
can be obtained from a treebank or from a ro-
bust parser. All the data used here have been
obtained using the system of Briscoe and Car-
roll (1997). Note that no distinction is made
between the different senses of a verb, and each
noun is assumed to denote exactly one concept.
</bodyText>
<sectionHeader confidence="0.9932645" genericHeader="method">
3 Class-Based Probability
Estimation
</sectionHeader>
<bodyText confidence="0.999971772727273">
Using the method of maximum likelihood to es-
timate p((dog)Irun, subj) would not be appro-
priate. The problem with maximum likelihood
estimation is that it tends to over-fit the data,
giving too much probability mass to cases seen
in the data, and no probability mass to unseen
cases. The solution proposed here is to base the
probability estimate on a suitably chosen class,
such as (animal), so that the probabilities of un-
seen cases can be inferred from the seen cases.
Before explaining how a suitable class is cho-
sen, we first explain how a set of concepts c&apos;
can be used to estimate p(clv, r). An inappro-
priate strategy would be to simply substitute c&apos;
for the individual concept c, since p(cilv, r) is
the conditional probability that some noun de-
noting a concept in c&apos; appears in position r of
verb v. For example, p((animai)I run, subj) is the
probability that some noun denoting a kind of
animal appears in the subject position of run.
Probabilities of sets of concepts are obtained by
summing over the concepts in the set:
</bodyText>
<equation confidence="0.976328">
p(cilv,r) = &gt;2, p(cillv,r) (1)
c&amp;quot;eci
</equation>
<bodyText confidence="0.995507666666667">
This means that p((animai)Irtin,subj) is likely
to be much greater than p((dog)lrun,subj), and
not a good approximation of p( dog) I run, subj).
What can be done, though, is to condition
on sets of concepts. If it can be shown that
P(vIct,r), for some hypernym, c&apos;, of c, is a rea-
sonable approximation of p(v lc, r), then we have
a way of estimating p(clv,r). The probability
P(vle, r) can be obtained from p(clv,r) using
</bodyText>
<equation confidence="0.999452555555556">
P(vIct,pr)(v=irP) (lv,:)P_(evIlr))
p ( Ir
PPIr) (e Actilv&apos;r) ±Activ&apos;r))
P(vIr) x
P(e&apos;lr)
p((eviiiirr) + (vie&apos; ,r)Pp((evi rr
= P(ctilr)+ k P(cilr))
P(Wk 1r) (&gt;7&gt;gir) ±Actir))
k
</equation>
<figureCaption confidence="0.904777">
Figure 1: Demonstration of how probabilities
can remain constant when moving up the hier-
archy.
</figureCaption>
<equation confidence="0.985132">
Bayes&apos; theorem:
p(clv,r) = p(vIc,r)P(clr) (2)
P(v1r)
</equation>
<bodyText confidence="0.961248722222222">
Since p(cir) and p(vIr) are conditioned on the
argument slot only, it is more likely these can
be estimated satisfactorily using maximum like-
lihood estimation. This leaves p(vic,r). Con-
tinuing with the (dog) example, the proposal is
to estimate p(runiclog), subj) using a maximum
likelihood estimate of p(runli,animal),subj), or
something similar. In Figure 1, it is shown
that if p(vIc, r) = k for each child c&amp;quot;, of c&apos;, and
P(vIct,r) = k, then p(vIct,r) will also be equal
to k.2
Figure 1 shows how probabilities conditioned
on sets of concepts can remain constant when
moving up the hierarchy, and this suggests a
way of finding a suitable set, c&apos;, for concept c:
initially set c&apos; equal to c, and move up the hi-
erarchy, changing the value of c&apos;, until there is
a significant change in p(vict,r).3 Estimates of
</bodyText>
<footnote confidence="0.999313">
2The proof only applies to a tree, whereas WordNet
is a DAG. However, since WordNet is a close approxi-
mation to a tree, we assume this will not be a problem
in practice.
3We assume that p(vr,r) is close to p(vic,r); in fact,
</footnote>
<equation confidence="0.974104666666667">
P(Clr)gc&apos;r) E&apos;/Ev f(c&apos;vi&apos;r)
f (r) E.u/Ev E,/EC f(ci,vi,r)
P(Vir)f(v,r) E,/EC ge,v,r)
f (r) Ev EC f (ci ,vi ,r)
.25(VICI r) f(c&apos;,v,r) E,nE7f(c&amp;quot;,v,r)
f (ci ,r) Ev E,„E7 f (c&amp;quot; ,vi ,r)
</equation>
<tableCaption confidence="0.819179">
Table 1: Maximum Likelihood Estimates —
</tableCaption>
<bodyText confidence="0.987061388888889">
f (c, v, r) is the number of (n, v, r) triples in the
data in which n is being used to denote c; V is
the set of verbs in the data, and C is the set of
concepts.
P(vic, r), for each child c&amp;quot;, of c&apos;, can be compared
to see if p(v lc&apos;, r) has significantly changed. (We
ignore the probability p(vict , r), and consider
the probabilities p(v r) only.)
Before giving the details of the generalisation
procedure, we give the maximum likelihood es-
timates of the relevant probabilities, and deal
with the problem of ambiguous data. The esti-
mates are given in Table 1. The problem is that
the estimates are defined in terms of frequencies
of senses, whereas the data consists of nouns. In
response to this, we estimate f (c, v, r) by simply
distributing the count for each noun n in syn(c)
evenly among all senses of the noun:
</bodyText>
<equation confidence="0.992819">
(c, v , r) = (3)
I cn(n) I
nEsyn(c)
</equation>
<bodyText confidence="0.999820333333333">
where I cn(n)I is the cardinality of cn(n). Resnik
(1998) explains how this apparently crude tech-
nique works surprisingly well.
</bodyText>
<sectionHeader confidence="0.801806" genericHeader="method">
4 Finding a suitable Level of
Generalisation
</sectionHeader>
<bodyText confidence="0.999629901639344">
In this section we give the details of how to
find a suitable class to represent a concept. We
first show how to test if p(vIct , r) changes signif-
icantly by moving up a node in the hierarchy.
Consider the problem of deciding if
p(runl (canine), subj) is a good approxima-
tion of p(runl (dog), subj). (canine) is the
parent of (dog) in WordNet.) To do this,
the probabilities p(runic, subj) are compared
p(W,r) is equal to p(vic,r) when c is a leaf node.
using a chi-squared test, where the c&amp;quot;, are the
children of (canine). In this case, the null
hypothesis of the test is that the probabilities
p(runl, subj) are the same for each child a,.
By judging the strength of the evidence against
the null hypothesis, it can be determined how
similar the true probabilities are likely to be.
If the test indicates that the probabilities
are likely to be very different, then the null
hypothesis is rejected, and the conclusion
is that p(runl (canine), subj) is not a good
approximation of p(rttni(dog) , subj).
An example contingency table, based on
counts obtained from a subset of the BNC,
is given in Table 2. One column contains
estimates of counts arising from concepts in
appearing in the subject position of run:
f run, subj). A second column contains esti-
mates of counts arising from concepts in ap-
pearing in the subject position of a verb other
than run. The figures in brackets are the ex-
pected values, given that the null hypothesis is
true. There is a choice of which statistic to use
in conjunction with the test. The usual statistic
encountered in text books is the Pearson chi-
squared statistic, denoted X2. However, Dun-
ning (1993) claims that the log-likelihood chi-
squared statistic (G2) is more appropriate for
corpus-based NLP. In Section 6, we compare the
two statistics in a task-based evaluation.
For Table 2, the value of G2 is 3.8 and the
value of X2 is 2.5. Assuming a level of signif-
icance of a = 0.05, the critical value is 12.6
(for 6 degrees of freedom). Thus, for this a
value, the null hypothesis would not be re-
jected for either statistic, and the conclusion
would be that there is no reason to suppose
p(runl (canine), subj) is not a reasonable approx-
imation of p(rtml (dog), subj).
A key question is how to select the value for
a. We could just select a value, such as 0.05,
but any value determined in this way is to some
extent arbitrary. An alternative solution is to
treat a as a parameter and set it empirically,
by taking a held-out test set and choosing the
value of a that maximises performance on the
relevant task. Note that this approach sets no
constraints on the value of cc the value could be
as high as 0.995, or as low as 0.0005, depending
on the particular application.
The procedure for finding a suitable class,
</bodyText>
<table confidence="0.9987527">
run, subj) ‘, f(c, subj) f(, subj) =
- f (C, run, subj) Evev Ir, v, subj)
(bitch) 0.3 (0.5) 26.7 (26.6) 27.0
(dog) 12.8 (10.5) 620.4 (622.7) 633.2
(wolf) 0.3 (0.6) 38.7 (38.4) 39.0
(jackal) 0.0 (0.3) 20.0 (19.7) 20.0
(wild_dog) 0.0 (0.0) 3.0 (3.0) 3.0
(hyena) 0.0 (0.2) 10.0 (9.8) 10.0
(fox) 0.0 (1.2) 72.3 (71.1) 72.3
13.4 791.1 804.5
</table>
<tableCaption confidence="0.999618">
Table 2: Contingency table for the children of (canine) in the subject position of run
</tableCaption>
<bodyText confidence="0.999961783783784">
c&apos;, to represent concept c in position r of verb
v works as follows. (We refer to c&apos; as the
&apos;similarity-class&apos; of c with respect to v and r,
and the hypernym c&apos; as top(c, v, r).) Initially, a
variable top is assigned to the concept c itself.
Then, by working up the hierarchy, top is re-
assigned to be successive hypernyms of c. This
continues until the probabilities associated with
the sets of concepts dominated by top and the
siblings of top are significantly different. Once
a node is reached that results in a significant
result for the chi-squared test, the procedure
stops, and top is returned as top(c, v, r). In
cases where a concept has more than one parent,
the parent is chosen that results in the lowest
value of the chi-squared statistic, as this indi-
cates the probabilities are more similar. The set
top(c, v, r) is the similarity-class of c for verb v
and position r.
There may be cases where the conditions for
the appropriate application of a chi-squared test
are not met. One condition that is likely to be
violated is the requirement that expected val-
ues in the contingency table should not be too
small. (A rule of thumb often found in text
books is that the expected values should be
greater than 5.) One response to this problem
is to apply some kind of thresholding, and ei-
ther ignore counts below the threshold, or only
apply the test to tables that do not contain
low counts. The problem with this approach
is that any threshold is, to some extent, arbi-
trary, and there is evidence to suggest that, for
some tasks, low counts are important (Collins
and Brooks, 1995). Another approach would be
to use Fisher&apos;s exact test, which can be applied
to tables regardless of the size of the counts.
The main problem with this test is that it is
computationally expensive, especially for large
contingency tables (and it only applies to tables
with whole number counts).
What we have found in practice is that ap-
plying the chi-squared test to tables with low
counts tends to produce an insignificant result,
and the null hypothesis is not rejected. The
consequences of this for the generalisation pro-
cedure are that low count tables tend to result
in the procedure moving up to the next node
in the hierarchy. But given that the purpose
of the generalisation is to overcome the sparse
data problem, this behaviour is desirable.
Table 3 shows some example generalisation
levels for a small number of hand-picked verbs,
over a range of values for a. The G2 statistic
was used in the chi-squared tests, and the data
were extracted from a subset of the BNC using
the system of Briscoe and Carroll. The number
of times that each verb in the table occurred
in the data (with some object) is shown. The
table indicates that the extent of generalisation
increases with a decrease in the value of a. This
is to be expected, since, given a contingency
table chosen at random, a higher value of a is
more likely to lead to a significant result than a
lower value of a.
The point of the table is not to argue that
the example generalisation levels are &apos;correct&apos;,
but simply to show some examples, and give
some indication of how the generalisation level
changes with values in a. We argue that, since
the purpose of this work is probability estima-
tion, the most suitable level is the one that leads
to the best estimate. So if (hotdog), for exam-
ple, generalises to (sandwich)(in the object po-
</bodyText>
<figure confidence="0.998021272727273">
&lt;entity&gt;
&lt;life_form&gt; &lt;object&gt;
&lt;plant&gt; &lt;animal&gt; &lt;substance&gt;
&lt;artifact&gt;
&lt;solid&gt; &lt;fluid&gt;
&lt;food&gt;
&lt;lobster&gt;
&lt;pizza&gt;
&lt;rope&gt;
&lt;mushroom&gt;
&lt;lobster&gt;
</figure>
<bodyText confidence="0.9993248">
of MDL. In order that every noun is represented
at a leaf node, McCarthy creates new leaf nodes
for each synset at an internal node. However,
unlike Li and Abe, McCarthy does not trans-
form WordNet into a tree, which is strictly re-
quired for Li and Abe&apos;s application of MDL.
This did create a problem, in that many of the
cuts returned by MDL were over-generalising
at the (entity) node. The reason is that
person), which is close to (entity), and domi-
nated by (entity), has two parents: (iif e_f orm)
and (causal_agent). This DAG-like property
was responsible for the over-generalisation, and
so we removed the link between /
per son) and
causal_agent). This appeared to solve the
problem, and the results presented later for the
average degree of generalisation do not show an
over-generalisation compared with those given
in Li and Abe (1998).
</bodyText>
<sectionHeader confidence="0.9827775" genericHeader="method">
6 Pseudo Disambiguation
Experiments
</sectionHeader>
<bodyText confidence="0.9988778125">
The task we used to compare different gener-
alisation techniques is similar to that used by
Pereira et al. (1993) and Rooth et al. (1999).
The task is to decide which of two verbs, v and
vi, is more likely to take a given noun, n, as
an object. The test and training data were ob-
tained as follows. A number of verb direct ob-
ject pairs were extracted from a subset of the
BNC, using the system of Briscoe and Carroll.
All those pairs containing a noun not in Word-
Net were removed, and each verb and argument
was lemmatised. This resulted in a data set of
around 1.3 million (v, n) pairs.
To form a test set, 3,000 of these pairs were
randomly selected, such that each selected pair
contained a fairly frequent verb. (Following
Pereira et al., only those verbs that occurred
between 500 and 5, 000 times in the data were
considered.) Each instance of a selected pair
was then deleted from the data. This was to
ensure that the test data were unseen. The
remaining pairs formed the training data. To
complete the test set, a further fairly frequent
verb, vi, was randomly chosen for each (v, n)
pair. The random choice was made according
to the verb&apos;s frequency in the original data set,
subject to the condition that the pair (e, n) did
not occur in the training data. Given the set of
(v, n, vi) triples, the task is to decide whether
(v, n) or (e, n) is the correct pair.
Using our approach, the disambiguation deci-
sion for each (v, n, v9 triple was made as follows:
</bodyText>
<figure confidence="0.827908285714286">
If max P(clv,obj) &gt; max fi(cle, obj)
cecu(n) cecu(n)
then choose (v, n)
else if max fi(cle,obj) &gt; max Aciv,obj)
can(n) cecn(n)
then choose (e, n)
else choose at random
</figure>
<bodyText confidence="0.9981905">
If n has more than one sense, the sense is
chosen that maximises the relevant probability
estimate; this explains the maximisation over
cn(n). The probability estimates were obtained
using our class-based method, and the G2 statis-
tic was used for the chi-squared test.
Using the association score, the decision for
each test triple was made as follows:
</bodyText>
<figure confidence="0.880555111111111">
If max max A(ci, v, obj) &gt;
cecu(n) ciEh(c)
max max A(ci, v&apos;, obj)
cecu(n) ciEh(c)
then choose (v, n)
else if max max A(ci, vi, obj) &gt;
cecu(n) ciEh(c)
max max A(ci, v, obj)
cecu(n) ciEh(c)
</figure>
<figureCaption confidence="0.386902">
then choose (e, n)
else choose at random
</figureCaption>
<bodyText confidence="0.959827666666667">
We use h(c) to denote the set consisting of the
hypernyms of c. The inner maximisation is over
h(c), assuming c is the chosen sense of n, which
corresponds to Resnik&apos;s method of choosing a
set to represent c. The outer maximisation is
over the senses of n, cn(n), which determines
the sense of n by choosing the sense that max-
imises the association score.
Using MDL, the disambiguation decision was
made as follows (3 is used to denote an estimate
using the MDL approach):
If max 25(n&apos;lv, obj) &gt; max An&apos;Ivi, obj)
</bodyText>
<equation confidence="0.76351075">
n&apos;Esep(n) n&apos;Esep(n)
then choose (v, n)
else if max p(n&apos;le, obj) &gt; max 15(n&apos;lv, obj)
n&apos;Esep(n) n&apos;Esep(n)
</equation>
<bodyText confidence="0.940519333333333">
then choose (V&apos;, n)
else choose at random
Since some nouns appear more than once in
WordNet, the instance of n is chosen that max-
imises the relevant probability estimate. We use
sep(n) to denote the separate instances of n.
</bodyText>
<figure confidence="0.872991">
Generalisation % correct av.gen. sd.gen
technique
Similarity-class 73.5 3.3 2.0
a = 0.0005
a = 0.05 73.2 2.8 1.9
a = 0.3 72.5 2.4 1.8
a = 0.75 73.5 1.9 1.6
a = 0.995 72.8 1.2 1.2
</figure>
<table confidence="0.972051">
Low-class 72.1 0.9 1.0
MDL 68.3 4.1 1.9
Assoc 63.9 4.2 2.1
</table>
<tableCaption confidence="0.8832425">
Table 4: Results for the pseudo disambiguation
task
</tableCaption>
<table confidence="0.775161272727273">
Generalisation % correct av.gen. sd.gen
technique
Similarity-class 66.4 4.5 1.9
a = 0.0005
a = 0.05 68.1 4.1 1.9
a = 0.3 69.8 3.7 1.9
72.1 3.0 1.9
a = 0.995 71.8 1.9 1.6
Low-class 71.0 1.1 1.1
MDL 62.9 4.7 1.9
Assoc 62.6 4.1 2.0
</table>
<tableCaption confidence="0.945401">
Table 5: Results for the pseudo disambiguation
task with 1/5th training data
</tableCaption>
<bodyText confidence="0.999751485714286">
The first set of results is given in Table 4.
Our technique is referred to as the &apos;similarity-
class&apos; technique, and the approach using the as-
sociation score is referred to as &apos;Assoc&apos;. The
results are given for a range of a values, and
demonstrate clearly that the performance of
similarity-class varies little with changes in a,
and similarity-class outperforms both MDL and
Assoc.
We also give a score for our approach using a
simple generalisation procedure, which we call
&amp;quot;Low-class&amp;quot;. The procedure is to select the first
class that has a count greater than zero (rela-
tive to the verb and argument position), which
is likely to return a low level of generalisation,
on the whole. The results show that our gener-
alisation technique only narrowly outperforms
the simple generalisation procedure. Note that
&amp;quot;Low-class&amp;quot; is still using our class-based estima-
tion method, by applying Bayes&apos; theorem and
conditioning on a class, as described in Sec-
tion 3; the difference is in how class is chosen.
To investigate the results, we calculated the
average number of generalised levels for each ap-
proach. The number of generalised levels for
a concept c (relative to a verb v and argu-
ment position r) is the difference in depth be-
tween c and top(c, v, r). To give an example of
how the difference in depth was calculated, sup-
pose (dog) generalised to (placental_mammal) via
canine) and (carnivore); in this case the differ-
ence would be 3. For each test case, the number
of generalised levels for both verbs, v and v&apos;, was
calculated, but only for the chosen sense of n.
The results are given in the third column of Ta-
ble 4, and demonstrate clearly that both MDL
and Assoc are generalising to a greater extent
than similarity-class. (The fourth column gives
a standard deviation figure.) These results sug-
gest that MDL and Assoc are over-generalising,
at least for the purposes of this task.
To investigate why the value for a had no im-
pact on the results, we repeated the experiment,
but with 1/5th of the data. A new data set was
created by taking every 5th pair of the original
1.3 million pairs. A test set of 3, 000 triples was
created from this new data set, as before, but
this time only verbs that occurred between 100
and 1, 000 times were considered. The results
using these test and training data are given in
Table 5.
These results show a variation in performance
across values for a, with an optimal perfor-
mance when a is around 0.75. (Of course, in
practice, the value for a would need to be op-
timised on a held-out set.) But even with this
variation, similarity-class is still out-performing
MDL and Assoc across the whole range of a val-
ues. Note that the a values corresponding to the
lowest scores lead to a significant amount of gen-
eralisation, which provides additional evidence
that MDL and Assoc are over-generalising for
this task. The Low-class method scores highly
for this data set also, but given that the task is
one that apparently favours a low level of gen-
eralisation, the high score is not too surprising.
As a final experiment, we compared the task
performance using the X2, rather than G2,
statistic in the chi-squared test. The results are
given in Table 6 for the complete data set. The
</bodyText>
<table confidence="0.986754166666667">
a value % correct - G2 % correct - X2
0.0005 73.5 (3.3) 73.9 (3.0)
0.05 73.2 (2.8) 73.7 (2.5)
0.3 72.5 (2.4) 73.6 (2.2)
0.75 73.5 (1.9) 73.8 (1.8)
0.995 72.8 (1.2) 72.3 (1.2)
</table>
<tableCaption confidence="0.993602">
Table 6: Disambiguation results for G2 and X2
</tableCaption>
<bodyText confidence="0.998203222222222">
figures in brackets give the average number of
generalised levels. The X2 statistic is perform-
ing at least as well as G2, throwing doubt on the
claim by Dunning (1993) that the G2 statistic
is better suited for use in corpus-based NLP.
The results show clearly that the average level
of generalisation is slightly higher for G2 than
X2. This suggests a possible explanation for
the results presented here, and those in Dunning
(1993), which is that the X2 statistic provides
a less conservative test when counts in the con-
tingency table are low. A less conservative test
is better suited to the pseudo disambiguation
task, since this results in a low level of gener-
alisation, on the whole, which is good for this
task. In contrast, the task that Dunning con-
siders, the discovery of bigrams, is better served
by a more conservative test.
</bodyText>
<sectionHeader confidence="0.995896" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999965416666667">
We have presented a class-based estimation
method that incorporates a procedure for find-
ing a suitable level of generalisation in Word-
Net. This method has been shown to provide
superior performance on a pseudo disambigua-
tion task, compared with two alternative ap-
proaches. One of the features of the generali-
sation procedure is the way that a, the level of
significance in the chi-squared test, is treated as
a parameter. This allows some control over the
extent of generalisation, which can be tailored
to particular tasks.
</bodyText>
<sectionHeader confidence="0.997031" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999952">
This work was supported by an EPSRC stu-
dentship to the first author. We would like
to thank Diana McCarthy for suggesting the
pseudo disambiguation task and providing the
MDL software, John Carroll for supplying the
data, and the anonymous reviewers for their
helpful comments.
</bodyText>
<sectionHeader confidence="0.992296" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99926832">
E. Briscoe and J. Carroll. 1997. Automatic ex-
traction of subcategorisation from corpora.
In Proceedings of the Fifth ACL Confer-
ence on Applied Natural Language Process-
ing, pages 356-363, Washington, DC.
M. Collins and J. Brooks. 1995. Preposi-
tional phrase attachment through a backed-
off model. In Proceedings of the Third Work-
shop on Very Large Corpora, pages 27-38,
Cambridge, MA.
T. Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Com-
putational Linguistics, 19(1):61-74.
C. Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
H. Li and N. Abe. 1998. Generalizing case
frames using a thesaurus and the MDL prin-
ciple. Computational Linguistics, 24(2):217-
244.
D. McCarthy. 2000. Using semantic preferences
to identify verbal participation in role switch-
ing. In Proceedings of the first Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages
256-263, Seattle, WA.
F. Pereira, N. Tishby, and L. Lee. 1993. Dis-
tributional clustering of English words. In
Proceedings of the 31st Annual Meeting of
the Association for Computational Linguis-
tics, pages 183-190, Columbus, OH.
P. Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relation-
ships. Ph.D. thesis, University of Pennsylva-
nia.
P. Resnik. 1998. Wordnet and class-based
probabilities. In Christiane Fellbaum, editor,
WordNet: An Electronic Lexical Database,
chapter 10, pages 239-263. The MIT Press.
F. Ribas. 1995. On learning more appropriate
selectional restrictions. In Proceedings of the
7th Conference of the European Chapter of
the Association for Computational Linguis-
tics, pages 112-118, Dublin, Ireland.
M. Rooth, S. Riezler, D. Prescher, G. Carroll,
and F. Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguis-
tics, pages 104-111, University of Maryland,
MD.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.752975">
<title confidence="0.999097">Class-Based Probability Estimation using a Semantic Hierarchy</title>
<author confidence="0.996024">Stephen</author>
<affiliation confidence="0.994888333333333">Division of University of 2 Buccleuch Place,</affiliation>
<address confidence="0.913462">EH8 9LW,</address>
<email confidence="0.998426">stephenc@cogsci.ed.ac.uk</email>
<affiliation confidence="0.954305">School of Cognitive and Computing University of Falmer,</affiliation>
<address confidence="0.969653">BN1 9QH,</address>
<email confidence="0.999221">david.weir@cogs.susx.ac.uk</email>
<abstract confidence="0.9992955">This paper concerns the acquisition of a particular kind of lexical knowledge, namely the knowledge of which noun senses can fill argument slots of predicates. Probabilities are used to represent the knowledge, and classes from a semantic hierarchy are used to estimate the probabilities. There is a particular focus on the problem of how to determine a suitable class, or level of generalisation, in the hierarchy. A pseudo disambiguation task is used to compare different class-based estimation methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic extraction of subcategorisation from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<location>Washington, DC.</location>
<contexts>
<context position="4192" citStr="Briscoe and Carroll (1997)" startWordPosition="733" endWordPosition="737">aper is p((dog) I run, subj), which is the conditional probability that some noun in the synset 1-Note that we are using concept to refer to a lexicalised concept or sense, and not a set of senses. Angled brackets are used to denote concepts in the hierarchy. of (dog), when denoting (dog), appears in the subject position of the verb run. The data used to estimate the probabilities is assumed to be in the form of (n, v, r) triples: a noun, verb and argument position. Such data can be obtained from a treebank or from a robust parser. All the data used here have been obtained using the system of Briscoe and Carroll (1997). Note that no distinction is made between the different senses of a verb, and each noun is assumed to denote exactly one concept. 3 Class-Based Probability Estimation Using the method of maximum likelihood to estimate p((dog)Irun, subj) would not be appropriate. The problem with maximum likelihood estimation is that it tends to over-fit the data, giving too much probability mass to cases seen in the data, and no probability mass to unseen cases. The solution proposed here is to base the probability estimate on a suitably chosen class, such as (animal), so that the probabilities of unseen case</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>E. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorisation from corpora. In Proceedings of the Fifth ACL Conference on Applied Natural Language Processing, pages 356-363, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backedoff model.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="13232" citStr="Collins and Brooks, 1995" startWordPosition="2348" endWordPosition="2351">uared test are not met. One condition that is likely to be violated is the requirement that expected values in the contingency table should not be too small. (A rule of thumb often found in text books is that the expected values should be greater than 5.) One response to this problem is to apply some kind of thresholding, and either ignore counts below the threshold, or only apply the test to tables that do not contain low counts. The problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important (Collins and Brooks, 1995). Another approach would be to use Fisher&apos;s exact test, which can be applied to tables regardless of the size of the counts. The main problem with this test is that it is computationally expensive, especially for large contingency tables (and it only applies to tables with whole number counts). What we have found in practice is that applying the chi-squared test to tables with low counts tends to produce an insignificant result, and the null hypothesis is not rejected. The consequences of this for the generalisation procedure are that low count tables tend to result in the procedure moving up </context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>M. Collins and J. Brooks. 1995. Prepositional phrase attachment through a backedoff model. In Proceedings of the Third Workshop on Very Large Corpora, pages 27-38, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="10099" citStr="Dunning (1993)" startWordPosition="1779" endWordPosition="1781">cy table, based on counts obtained from a subset of the BNC, is given in Table 2. One column contains estimates of counts arising from concepts in appearing in the subject position of run: f run, subj). A second column contains estimates of counts arising from concepts in appearing in the subject position of a verb other than run. The figures in brackets are the expected values, given that the null hypothesis is true. There is a choice of which statistic to use in conjunction with the test. The usual statistic encountered in text books is the Pearson chisquared statistic, denoted X2. However, Dunning (1993) claims that the log-likelihood chisquared statistic (G2) is more appropriate for corpus-based NLP. In Section 6, we compare the two statistics in a task-based evaluation. For Table 2, the value of G2 is 3.8 and the value of X2 is 2.5. Assuming a level of significance of a = 0.05, the critical value is 12.6 (for 6 degrees of freedom). Thus, for this a value, the null hypothesis would not be rejected for either statistic, and the conclusion would be that there is no reason to suppose p(runl (canine), subj) is not a reasonable approximation of p(rtml (dog), subj). A key question is how to select</context>
<context position="23310" citStr="Dunning (1993)" startWordPosition="4142" endWordPosition="4143">n, the high score is not too surprising. As a final experiment, we compared the task performance using the X2, rather than G2, statistic in the chi-squared test. The results are given in Table 6 for the complete data set. The a value % correct - G2 % correct - X2 0.0005 73.5 (3.3) 73.9 (3.0) 0.05 73.2 (2.8) 73.7 (2.5) 0.3 72.5 (2.4) 73.6 (2.2) 0.75 73.5 (1.9) 73.8 (1.8) 0.995 72.8 (1.2) 72.3 (1.2) Table 6: Disambiguation results for G2 and X2 figures in brackets give the average number of generalised levels. The X2 statistic is performing at least as well as G2, throwing doubt on the claim by Dunning (1993) that the G2 statistic is better suited for use in corpus-based NLP. The results show clearly that the average level of generalisation is slightly higher for G2 than X2. This suggests a possible explanation for the results presented here, and those in Dunning (1993), which is that the X2 statistic provides a less conservative test when counts in the contingency table are low. A less conservative test is better suited to the pseudo disambiguation task, since this results in a low level of generalisation, on the whole, which is good for this task. In contrast, the task that Dunning considers, th</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8345" citStr="(1998)" startWordPosition="1473" endWordPosition="1473">d consider the probabilities p(v r) only.) Before giving the details of the generalisation procedure, we give the maximum likelihood estimates of the relevant probabilities, and deal with the problem of ambiguous data. The estimates are given in Table 1. The problem is that the estimates are defined in terms of frequencies of senses, whereas the data consists of nouns. In response to this, we estimate f (c, v, r) by simply distributing the count for each noun n in syn(c) evenly among all senses of the noun: (c, v , r) = (3) I cn(n) I nEsyn(c) where I cn(n)I is the cardinality of cn(n). Resnik (1998) explains how this apparently crude technique works surprisingly well. 4 Finding a suitable Level of Generalisation In this section we give the details of how to find a suitable class to represent a concept. We first show how to test if p(vIct , r) changes significantly by moving up a node in the hierarchy. Consider the problem of deciding if p(runl (canine), subj) is a good approximation of p(runl (dog), subj). (canine) is the parent of (dog) in WordNet.) To do this, the probabilities p(runic, subj) are compared p(W,r) is equal to p(vic,r) when c is a leaf node. using a chi-squared test, wher</context>
<context position="16020" citStr="(1998)" startWordPosition="2830" endWordPosition="2830">or Li and Abe&apos;s application of MDL. This did create a problem, in that many of the cuts returned by MDL were over-generalising at the (entity) node. The reason is that person), which is close to (entity), and dominated by (entity), has two parents: (iif e_f orm) and (causal_agent). This DAG-like property was responsible for the over-generalisation, and so we removed the link between / per son) and causal_agent). This appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in Li and Abe (1998). 6 Pseudo Disambiguation Experiments The task we used to compare different generalisation techniques is similar to that used by Pereira et al. (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and vi, is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatised. This resulted in a data set of around 1.</context>
</contexts>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--2</pages>
<contexts>
<context position="16020" citStr="Li and Abe (1998)" startWordPosition="2827" endWordPosition="2830"> required for Li and Abe&apos;s application of MDL. This did create a problem, in that many of the cuts returned by MDL were over-generalising at the (entity) node. The reason is that person), which is close to (entity), and dominated by (entity), has two parents: (iif e_f orm) and (causal_agent). This DAG-like property was responsible for the over-generalisation, and so we removed the link between / per son) and causal_agent). This appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in Li and Abe (1998). 6 Pseudo Disambiguation Experiments The task we used to compare different generalisation techniques is similar to that used by Pereira et al. (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and vi, is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatised. This resulted in a data set of around 1.</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>H. Li and N. Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
</authors>
<title>Using semantic preferences to identify verbal participation in role switching.</title>
<date>2000</date>
<booktitle>In Proceedings of the first Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>256--263</pages>
<location>Seattle, WA.</location>
<marker>McCarthy, 2000</marker>
<rawString>D. McCarthy. 2000. Using semantic preferences to identify verbal participation in role switching. In Proceedings of the first Conference of the North American Chapter of the Association for Computational Linguistics, pages 256-263, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="16170" citStr="Pereira et al. (1993)" startWordPosition="2851" endWordPosition="2854">entity) node. The reason is that person), which is close to (entity), and dominated by (entity), has two parents: (iif e_f orm) and (causal_agent). This DAG-like property was responsible for the over-generalisation, and so we removed the link between / per son) and causal_agent). This appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in Li and Abe (1998). 6 Pseudo Disambiguation Experiments The task we used to compare different generalisation techniques is similar to that used by Pereira et al. (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and vi, is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatised. This resulted in a data set of around 1.3 million (v, n) pairs. To form a test set, 3,000 of these pairs were randomly selected, such that each selected pair contained a fairly frequent verb</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183-190, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Resnik, 1993</marker>
<rawString>P. Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Wordnet and class-based probabilities.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database, chapter 10,</booktitle>
<pages>239--263</pages>
<editor>In Christiane Fellbaum, editor,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8345" citStr="Resnik (1998)" startWordPosition="1472" endWordPosition="1473"> r), and consider the probabilities p(v r) only.) Before giving the details of the generalisation procedure, we give the maximum likelihood estimates of the relevant probabilities, and deal with the problem of ambiguous data. The estimates are given in Table 1. The problem is that the estimates are defined in terms of frequencies of senses, whereas the data consists of nouns. In response to this, we estimate f (c, v, r) by simply distributing the count for each noun n in syn(c) evenly among all senses of the noun: (c, v , r) = (3) I cn(n) I nEsyn(c) where I cn(n)I is the cardinality of cn(n). Resnik (1998) explains how this apparently crude technique works surprisingly well. 4 Finding a suitable Level of Generalisation In this section we give the details of how to find a suitable class to represent a concept. We first show how to test if p(vIct , r) changes significantly by moving up a node in the hierarchy. Consider the problem of deciding if p(runl (canine), subj) is a good approximation of p(runl (dog), subj). (canine) is the parent of (dog) in WordNet.) To do this, the probabilities p(runic, subj) are compared p(W,r) is equal to p(vic,r) when c is a leaf node. using a chi-squared test, wher</context>
</contexts>
<marker>Resnik, 1998</marker>
<rawString>P. Resnik. 1998. Wordnet and class-based probabilities. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, chapter 10, pages 239-263. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ribas</author>
</authors>
<title>On learning more appropriate selectional restrictions.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>112--118</pages>
<location>Dublin, Ireland.</location>
<marker>Ribas, 1995</marker>
<rawString>F. Ribas. 1995. On learning more appropriate selectional restrictions. In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics, pages 112-118, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
<author>S Riezler</author>
<author>D Prescher</author>
<author>G Carroll</author>
<author>F Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<institution>University of Maryland, MD.</institution>
<contexts>
<context position="16194" citStr="Rooth et al. (1999)" startWordPosition="2856" endWordPosition="2859">s that person), which is close to (entity), and dominated by (entity), has two parents: (iif e_f orm) and (causal_agent). This DAG-like property was responsible for the over-generalisation, and so we removed the link between / per son) and causal_agent). This appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in Li and Abe (1998). 6 Pseudo Disambiguation Experiments The task we used to compare different generalisation techniques is similar to that used by Pereira et al. (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and vi, is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatised. This resulted in a data set of around 1.3 million (v, n) pairs. To form a test set, 3,000 of these pairs were randomly selected, such that each selected pair contained a fairly frequent verb. (Following Pereira et </context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 104-111, University of Maryland, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>