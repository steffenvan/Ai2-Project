<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014454">
<title confidence="0.996625">
Generic parsing for multi-domain semantic interpretation
</title>
<author confidence="0.990066">
Myroslava Dzikovska*, Mary Swift†, James Allen†, William de Beaumont†
</author>
<affiliation confidence="0.9706815">
* Human Communication Research Centre
University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, United Kingdom
</affiliation>
<email confidence="0.692874">
m.dzikovska@ed.ac.uk
</email>
<affiliation confidence="0.792645">
† Department of Computer Science University of Rochester, Rochester, NY 14627-0226
</affiliation>
<email confidence="0.917266">
{swift, james, wdebeaum}@cs.rochester.edu
</email>
<sectionHeader confidence="0.999636" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894208333333">
Producing detailed syntactic and semantic represen-
tations of natural language is essential for prac-
tical dialog systems such as plan-based assistants
and tutorial systems. Development of such systems
is time-consuming and costly as they are typically
hand-crafted for each application, and dialog corpus
data is more difficult to obtain than text. The TRIPS
parser and grammar addresses these issues by pro-
viding broad coverage of common constructions in
practical dialog and producing semantic representa-
tions suitable for dialog processing across domains.
Our system bootstraps dialog system development
in new domains and helps build parsed corpora.1
Evaluating deep parsers is a challenge (e.g., (Ka-
plan et al., 2004)). Although common bracketing
accuracy metrics may provide a baseline, they are
insufficient for applications such as ours that require
complete and correct semantic representations pro-
duced by the parser. We evaluate our parser on
bracketing accuracy against a statistical parser as a
baseline, then on a word sense disambiguation task,
and finally on full sentence syntactic and semantic
accuracy in multiple domains as a realistic measure
of system performance and portability.
</bodyText>
<sectionHeader confidence="0.906934" genericHeader="method">
2 The TRIPS Parser and Logical Form
</sectionHeader>
<bodyText confidence="0.9937435">
The TRIPS grammar is a linguistically motivated
unification formalism using attribute-value struc-
</bodyText>
<footnote confidence="0.8215196">
&apos;We thank 4 anonymous reviewers for comments.
This material is based on work supported by grants from
ONR #N000149910165, NSF #IIS-0328811, DARPA
#NBCHD030010 via subcontract to SRI #03-000223 and
NSF #E1A-0080124.
</footnote>
<figure confidence="0.491651333333333">
(SPEECHACT sa1 SA REQUEST :content e123)
(F e123 (:* LF::Fill-Container Load)
:Agent pro1 :Theme v1 :Goal v2)
(IMPRO pro1 LF::Person :context-rel *YOU*)
(THE v1 (SET-OF (:* LF::Fruit Orange)))
(THE v2 (:* LF::Vehicle Truck))
</figure>
<figureCaption confidence="0.998393">
Figure 1: LF for Load the oranges into the truck.
</figureCaption>
<bodyText confidence="0.99972452631579">
tures. An unscoped neo-Davidsonian semantic rep-
resentation is built in parallel with the syntactic
representation. A sample logical form (LF) rep-
resentation for Load the oranges into the truck is
shown above. The TRIPS LF provides the neces-
sary information for reference resolution, surface
speech act analysis, and interpretations for a wide
variety of fragmentary utterances and conventional
phrases typical in dialog. The LF content comes
from a domain-independent ontology adapted from
FrameNet (Johnson and Fillmore, 2000; Dzikovska
et al., 2004) and linked to a domain-independent lex-
icon (Dzikovska, 2004).
The parser uses a bottom-up chart algorithm with
beam search. Alternative parses are scored with fac-
tors assigned to grammar rules and lexical entries by
hand, because due to the limited amount of corpus
data we have not yet been able to train a statistical
model that outperforms our hand-tuned factors.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.997220333333333">
As a rough baseline, we compared the bracketing
accuracy of our parser to that of a statistical parser
(Bikel, 2002), Bikel-M, trained on 4294 TRIPS
</bodyText>
<page confidence="0.992258">
196
</page>
<bodyText confidence="0.945382954545455">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 196–197,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
parse trees from the Monroe corpus (Stent, 2001),
task-oriented human dialogs in an emergency res-
cue domain. 100 randomly selected utterances were
held out for testing. The gold standard for evalu-
ation is created with the help of the parser (Swift
et al., 2004). Corpus utterances are parsed, and the
parsed output is checked by trained annotators for
full-sentence syntactic and semantic accuracy, reli-
able with a kappa score 0.79. For test utterances
for which TRIPS failed to produce a correct parse,
gold standard trees were manually constructed inde-
pendently by two linguists and reconciled. Table 1
shows results for the 100 test utterances and for the
subset for which TRIPS finds a spanning parse (74).
Bikel-M performs somewhat better on the bracket-
ing task for the entire test set, which includes utter-
ances for which TRIPS failed to find a parse, but it
is lower on complete matches, which are crucial for
semantic interpretation.
All test utts (100) Spanning parse utts (74)
</bodyText>
<table confidence="0.993382">
R P CM R P CM
BIKEL-M 79 79 42 89 88 54
TRIPS 77 79 65 95 95 86
</table>
<tableCaption confidence="0.993323">
Table 1: Bracketing results for Monroe test sets (R:
recall, P: precision, CM: complete match).
</tableCaption>
<bodyText confidence="0.9978794">
Word senses are an important part of the LF rep-
resentation, so we also evaluated TRIPS on word
sense tagging against a baseline of the most common
word senses in Monroe. There were 546 instances of
ambiguous words in the 100 test utterances. TRIPS
tagged 90.3% (493) of these correctly, compared to
the baseline model of 75.3% (411) correct.
To evaluate portability to new domains, we com-
pared TRIPS full sentence accuracy on a subset
of Monroe that underwent a fair amount of devel-
opment (Tetreault et al., 2004) to corpora of key-
board tutorial session transcripts from new domains
in basic electronics (BEETLE) and differentiation
(LAM) (Table 2). The only development for these
domains was addition of missing lexical items and
two grammar rules. TRIPS full accuracy requires
correct speech act, word sense and thematic role as-
signment as well as complete constituent match.
Error analysis shows that certain senses and sub-
categorization frames for existing words are still
</bodyText>
<table confidence="0.99891625">
Domain Utts Acc. Cov. Prec.
Monroe 1576 70% 1301 84.1%
BEETLE 192 50% 129 75%
LAM 934 42% 579 68%
</table>
<tableCaption confidence="0.775695666666667">
Table 2: TRIPS full sentence syntactic and semantic
accuracy in 3 domains (Acc: full accuracy; Cov.: #
spanning parses; Prec: full acc. on spanning parses).
</tableCaption>
<bodyText confidence="0.997351">
needed in the new domains, which can be rectified
fairly quickly. Finding and addressing such gaps is
part of bootstrapping a system in a new domain.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999932285714286">
Our wide-coverage grammar, together with a
domain-independent ontology and lexicon, produces
semantic representations applicable across domains
that are detailed enough for practical dialog applica-
tions. Our generic components reduce development
effort when porting to new dialog domains where
corpus data is difficult to obtain.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999557083333333">
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In HLT-2002.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004.
Building a computational lexicon and ontology with
framenet. In LREC workshop on Building Lexical Re-
sources from Semantically Annotated Corpora.
M. O. Dzikovska. 2004. A Practical Semantic Represen-
tation For Natural Language Parsing. Ph.D. thesis,
University of Rochester.
C. Johnson and C. J. Fillmore. 2000. The FrameNet
tagset for frame-semantic and syntactic coding of
predicate-argument structure. In ANLP-NAACL 2000.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell III,
A. Vasserman, and R. S. Crouch. 2004. Speed and
accuracy in shallow and deep stochastic parsing. In
HLT-NAACL 2004.
A. J. Stent. 2001. Dialogue Systems as Conversational
Partners. Ph.D. thesis, University of Rochester.
M. D. Swift, M. O. Dzikovska, J. R. Tetreault, and J. F.
Allen. 2004. Semi-automatic syntactic and semantic
corpus annotation with a deep parser. In LREC-2004.
J. Tetreault, M. Swift, P. Prithviraj, M. Dzikovska, and
J. Allen. 2004. Discourse annotation in the Monroe
corpus. In ACL workshop on Discourse Annotation.
</reference>
<page confidence="0.998175">
197
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.210445">
<title confidence="0.999444">Generic parsing for multi-domain semantic interpretation</title>
<author confidence="0.999943">Mary James William de</author>
<affiliation confidence="0.846528">Communication Research University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, United</affiliation>
<address confidence="0.713551">of Computer Science University of Rochester, Rochester, NY</address>
<page confidence="0.352059">james,</page>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<title>Design of a multi-lingual, parallelprocessing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In HLT-2002.</booktitle>
<contexts>
<context position="3277" citStr="Bikel, 2002" startWordPosition="480" endWordPosition="481">comes from a domain-independent ontology adapted from FrameNet (Johnson and Fillmore, 2000; Dzikovska et al., 2004) and linked to a domain-independent lexicon (Dzikovska, 2004). The parser uses a bottom-up chart algorithm with beam search. Alternative parses are scored with factors assigned to grammar rules and lexical entries by hand, because due to the limited amount of corpus data we have not yet been able to train a statistical model that outperforms our hand-tuned factors. 3 Evaluation As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser (Bikel, 2002), Bikel-M, trained on 4294 TRIPS 196 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 196–197, Vancouver, October 2005. c�2005 Association for Computational Linguistics parse trees from the Monroe corpus (Stent, 2001), task-oriented human dialogs in an emergency rescue domain. 100 randomly selected utterances were held out for testing. The gold standard for evaluation is created with the help of the parser (Swift et al., 2004). Corpus utterances are parsed, and the parsed output is checked by trained annotators for full-sentence syntactic and semantic accur</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>D. Bikel. 2002. Design of a multi-lingual, parallelprocessing statistical parsing engine. In HLT-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M O Dzikovska</author>
<author>M D Swift</author>
<author>J F Allen</author>
</authors>
<title>Building a computational lexicon and ontology with framenet.</title>
<date>2004</date>
<booktitle>In LREC workshop on Building Lexical Resources from Semantically Annotated Corpora.</booktitle>
<contexts>
<context position="2780" citStr="Dzikovska et al., 2004" startWordPosition="396" endWordPosition="399">* LF::Vehicle Truck)) Figure 1: LF for Load the oranges into the truck. tures. An unscoped neo-Davidsonian semantic representation is built in parallel with the syntactic representation. A sample logical form (LF) representation for Load the oranges into the truck is shown above. The TRIPS LF provides the necessary information for reference resolution, surface speech act analysis, and interpretations for a wide variety of fragmentary utterances and conventional phrases typical in dialog. The LF content comes from a domain-independent ontology adapted from FrameNet (Johnson and Fillmore, 2000; Dzikovska et al., 2004) and linked to a domain-independent lexicon (Dzikovska, 2004). The parser uses a bottom-up chart algorithm with beam search. Alternative parses are scored with factors assigned to grammar rules and lexical entries by hand, because due to the limited amount of corpus data we have not yet been able to train a statistical model that outperforms our hand-tuned factors. 3 Evaluation As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser (Bikel, 2002), Bikel-M, trained on 4294 TRIPS 196 Proceedings of the Ninth International Workshop on Parsing Technol</context>
</contexts>
<marker>Dzikovska, Swift, Allen, 2004</marker>
<rawString>M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004. Building a computational lexicon and ontology with framenet. In LREC workshop on Building Lexical Resources from Semantically Annotated Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M O Dzikovska</author>
</authors>
<title>A Practical Semantic Representation For Natural Language Parsing.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="2841" citStr="Dzikovska, 2004" startWordPosition="407" endWordPosition="408">ck. tures. An unscoped neo-Davidsonian semantic representation is built in parallel with the syntactic representation. A sample logical form (LF) representation for Load the oranges into the truck is shown above. The TRIPS LF provides the necessary information for reference resolution, surface speech act analysis, and interpretations for a wide variety of fragmentary utterances and conventional phrases typical in dialog. The LF content comes from a domain-independent ontology adapted from FrameNet (Johnson and Fillmore, 2000; Dzikovska et al., 2004) and linked to a domain-independent lexicon (Dzikovska, 2004). The parser uses a bottom-up chart algorithm with beam search. Alternative parses are scored with factors assigned to grammar rules and lexical entries by hand, because due to the limited amount of corpus data we have not yet been able to train a statistical model that outperforms our hand-tuned factors. 3 Evaluation As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser (Bikel, 2002), Bikel-M, trained on 4294 TRIPS 196 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 196–197, Vancouver, October 2005. c�2005 </context>
</contexts>
<marker>Dzikovska, 2004</marker>
<rawString>M. O. Dzikovska. 2004. A Practical Semantic Representation For Natural Language Parsing. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Johnson</author>
<author>C J Fillmore</author>
</authors>
<title>The FrameNet tagset for frame-semantic and syntactic coding of predicate-argument structure.</title>
<date>2000</date>
<booktitle>In ANLP-NAACL</booktitle>
<contexts>
<context position="2755" citStr="Johnson and Fillmore, 2000" startWordPosition="392" endWordPosition="395">::Fruit Orange))) (THE v2 (:* LF::Vehicle Truck)) Figure 1: LF for Load the oranges into the truck. tures. An unscoped neo-Davidsonian semantic representation is built in parallel with the syntactic representation. A sample logical form (LF) representation for Load the oranges into the truck is shown above. The TRIPS LF provides the necessary information for reference resolution, surface speech act analysis, and interpretations for a wide variety of fragmentary utterances and conventional phrases typical in dialog. The LF content comes from a domain-independent ontology adapted from FrameNet (Johnson and Fillmore, 2000; Dzikovska et al., 2004) and linked to a domain-independent lexicon (Dzikovska, 2004). The parser uses a bottom-up chart algorithm with beam search. Alternative parses are scored with factors assigned to grammar rules and lexical entries by hand, because due to the limited amount of corpus data we have not yet been able to train a statistical model that outperforms our hand-tuned factors. 3 Evaluation As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser (Bikel, 2002), Bikel-M, trained on 4294 TRIPS 196 Proceedings of the Ninth International Wo</context>
</contexts>
<marker>Johnson, Fillmore, 2000</marker>
<rawString>C. Johnson and C. J. Fillmore. 2000. The FrameNet tagset for frame-semantic and syntactic coding of predicate-argument structure. In ANLP-NAACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>S Riezler</author>
<author>T H King</author>
<author>J T Maxwell A Vasserman</author>
<author>R S Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In HLT-NAACL</booktitle>
<contexts>
<context position="1125" citStr="Kaplan et al., 2004" startWordPosition="150" endWordPosition="154">cal dialog systems such as plan-based assistants and tutorial systems. Development of such systems is time-consuming and costly as they are typically hand-crafted for each application, and dialog corpus data is more difficult to obtain than text. The TRIPS parser and grammar addresses these issues by providing broad coverage of common constructions in practical dialog and producing semantic representations suitable for dialog processing across domains. Our system bootstraps dialog system development in new domains and helps build parsed corpora.1 Evaluating deep parsers is a challenge (e.g., (Kaplan et al., 2004)). Although common bracketing accuracy metrics may provide a baseline, they are insufficient for applications such as ours that require complete and correct semantic representations produced by the parser. We evaluate our parser on bracketing accuracy against a statistical parser as a baseline, then on a word sense disambiguation task, and finally on full sentence syntactic and semantic accuracy in multiple domains as a realistic measure of system performance and portability. 2 The TRIPS Parser and Logical Form The TRIPS grammar is a linguistically motivated unification formalism using attribu</context>
</contexts>
<marker>Kaplan, Riezler, King, Vasserman, Crouch, 2004</marker>
<rawString>R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell III, A. Vasserman, and R. S. Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Stent</author>
</authors>
<title>Dialogue Systems as Conversational Partners.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="3531" citStr="Stent, 2001" startWordPosition="514" endWordPosition="515">rses are scored with factors assigned to grammar rules and lexical entries by hand, because due to the limited amount of corpus data we have not yet been able to train a statistical model that outperforms our hand-tuned factors. 3 Evaluation As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser (Bikel, 2002), Bikel-M, trained on 4294 TRIPS 196 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 196–197, Vancouver, October 2005. c�2005 Association for Computational Linguistics parse trees from the Monroe corpus (Stent, 2001), task-oriented human dialogs in an emergency rescue domain. 100 randomly selected utterances were held out for testing. The gold standard for evaluation is created with the help of the parser (Swift et al., 2004). Corpus utterances are parsed, and the parsed output is checked by trained annotators for full-sentence syntactic and semantic accuracy, reliable with a kappa score 0.79. For test utterances for which TRIPS failed to produce a correct parse, gold standard trees were manually constructed independently by two linguists and reconciled. Table 1 shows results for the 100 test utterances a</context>
</contexts>
<marker>Stent, 2001</marker>
<rawString>A. J. Stent. 2001. Dialogue Systems as Conversational Partners. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Swift</author>
<author>M O Dzikovska</author>
<author>J R Tetreault</author>
<author>J F Allen</author>
</authors>
<title>Semi-automatic syntactic and semantic corpus annotation with a deep parser.</title>
<date>2004</date>
<booktitle>In LREC-2004.</booktitle>
<contexts>
<context position="3744" citStr="Swift et al., 2004" startWordPosition="548" endWordPosition="551">ur hand-tuned factors. 3 Evaluation As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser (Bikel, 2002), Bikel-M, trained on 4294 TRIPS 196 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 196–197, Vancouver, October 2005. c�2005 Association for Computational Linguistics parse trees from the Monroe corpus (Stent, 2001), task-oriented human dialogs in an emergency rescue domain. 100 randomly selected utterances were held out for testing. The gold standard for evaluation is created with the help of the parser (Swift et al., 2004). Corpus utterances are parsed, and the parsed output is checked by trained annotators for full-sentence syntactic and semantic accuracy, reliable with a kappa score 0.79. For test utterances for which TRIPS failed to produce a correct parse, gold standard trees were manually constructed independently by two linguists and reconciled. Table 1 shows results for the 100 test utterances and for the subset for which TRIPS finds a spanning parse (74). Bikel-M performs somewhat better on the bracketing task for the entire test set, which includes utterances for which TRIPS failed to find a parse, but</context>
</contexts>
<marker>Swift, Dzikovska, Tetreault, Allen, 2004</marker>
<rawString>M. D. Swift, M. O. Dzikovska, J. R. Tetreault, and J. F. Allen. 2004. Semi-automatic syntactic and semantic corpus annotation with a deep parser. In LREC-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Swift</author>
<author>P Prithviraj</author>
<author>M Dzikovska</author>
<author>J Allen</author>
</authors>
<title>Discourse annotation in the Monroe corpus.</title>
<date>2004</date>
<booktitle>In ACL workshop on Discourse Annotation.</booktitle>
<contexts>
<context position="5143" citStr="Tetreault et al., 2004" startWordPosition="791" endWordPosition="794">9 65 95 95 86 Table 1: Bracketing results for Monroe test sets (R: recall, P: precision, CM: complete match). Word senses are an important part of the LF representation, so we also evaluated TRIPS on word sense tagging against a baseline of the most common word senses in Monroe. There were 546 instances of ambiguous words in the 100 test utterances. TRIPS tagged 90.3% (493) of these correctly, compared to the baseline model of 75.3% (411) correct. To evaluate portability to new domains, we compared TRIPS full sentence accuracy on a subset of Monroe that underwent a fair amount of development (Tetreault et al., 2004) to corpora of keyboard tutorial session transcripts from new domains in basic electronics (BEETLE) and differentiation (LAM) (Table 2). The only development for these domains was addition of missing lexical items and two grammar rules. TRIPS full accuracy requires correct speech act, word sense and thematic role assignment as well as complete constituent match. Error analysis shows that certain senses and subcategorization frames for existing words are still Domain Utts Acc. Cov. Prec. Monroe 1576 70% 1301 84.1% BEETLE 192 50% 129 75% LAM 934 42% 579 68% Table 2: TRIPS full sentence syntactic</context>
</contexts>
<marker>Tetreault, Swift, Prithviraj, Dzikovska, Allen, 2004</marker>
<rawString>J. Tetreault, M. Swift, P. Prithviraj, M. Dzikovska, and J. Allen. 2004. Discourse annotation in the Monroe corpus. In ACL workshop on Discourse Annotation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>