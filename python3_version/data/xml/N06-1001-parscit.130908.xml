<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002449">
<title confidence="0.985843">
Capitalizing Machine Translation
</title>
<author confidence="0.975811">
Wei Wang and Kevin Knight and Daniel Marcu
</author>
<affiliation confidence="0.667241">
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA, 90292
</affiliation>
<email confidence="0.984351">
Nwang, kknight, dmarcu}@languageweaver.com
</email>
<sectionHeader confidence="0.995562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999671">
We present a probabilistic bilingual capi-
talization model for capitalizing machine
translation outputs using conditional ran-
dom fields. Experiments carried out on
three language pairs and a variety of ex-
periment conditions show that our model
significantly outperforms a strong mono-
lingual capitalization model baseline, es-
pecially when working with small datasets
and/or European language pairs.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912894736842">
Capitalization is the process of recovering case in-
formation for texts in lowercase. It is also called
truecasing (Lita et al., 2003). Usually, capitalization
itself tries to improve the legibility of texts. It, how-
ever, can affect the word choice or order when inter-
acting with other models. In natural language pro-
cessing, a good capitalization model has been shown
useful for tasks like name entity recognition, auto-
matic content extraction, speech recognition, mod-
ern word processors, and machine translation (MT).
Capitalization can be viewed as a sequence la-
beling process. The input to this process is a sen-
tence in lowercase. For each lowercased word in
the input sentence, we have several available cap-
italization tags: initial capital (IU), all uppercase
(AU), all lowercase (AL), mixed case (MX), and
all having no case (AN). The output of capital-
ization is a capitalization tag sequence. Associ-
ating a tag in the output with the corresponding
</bodyText>
<page confidence="0.844876">
1
</page>
<bodyText confidence="0.958794696969697">
lowercased word in the input results in a surface
form of the word. For example, we can tag the
input sentence “click ok to save your changes to
/home/doc.” into “click IU ok AU to AL save AL
your AL changes AL to AL /home/doc MX . AN”,
getting the surface form “Click OK to save your
changes to /home/DOC .”.
A capitalizer is a tagger that recovers the capi-
talization tag for each input lowercased word, out-
putting a well-capitalized sentence. Since each low-
ercased word can have more than one tag, and as-
sociating a tag with a lowercased word can result
in more than one surface form (e.g., /home/doc MX
can be either /home/DOC or /home/Doc), we need a
capitalization model to solve the capitalization am-
biguities. For example, Lita et al. (2003) use a tri-
gram language model estimated from a corpus with
case information; Chelba and Acero (2004) use a
maximum entropy Markov model (MEMM) com-
bining features involving words and their cases.
Capitalization models presented in most previ-
ous approaches are monolingual because the models
are estimated only from monolingual texts. How-
ever, for capitalizing machine translation outputs,
using only monolingual capitalization models is not
enough. For example, if the sentence “click ok to
save your changes to /home/doc .” in the above
example is the translation of the French sentence
“CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFI-
CATIONS DANS /HOME/DOC .”, the correct capitaliza-
tion result should probably be “CLICK OK TO SAVE
YOUR CHANGES TO /HOME/DOC .”, where all words
are in all upper-case. Without looking into the case
</bodyText>
<note confidence="0.9898215">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 1–8,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999917730769231">
of the MT input, we can hardly get the correct capi-
talization result.
Although monolingual capitalization models in
previous work can apply to MT output, a bilingual
model is more desirable. This is because MT out-
puts usually strongly preserve case from the input,
and because monolingual capitalization models do
not always perform as well on badly translated text
as on well-formed syntactic texts.
In this paper, we present a bilingual capitalization
model for capitalizing machine translation outputs
using conditional random fields (CRFs) (Lafferty et
al., 2001). This model exploits case information
from both the input sentence (source) and the out-
put sentence (target) of the MT system. We define a
series of feature functions to incorporate capitaliza-
tion knowledge into the model.
Experimental results are shown in terms of BLEU
scores of a phrase-based SMT system with the cap-
italization model incorporated, and in terms of cap-
italization precision. Experiments are performed
on both French and English targeted MT systems
with large-scale training data. Our experimental re-
sults show that the CRF-based bilingual capitaliza-
tion model performs better than a strong baseline
capitalizer that uses a trigram language model.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998810736842105">
A simple capitalizer is the 1-gram tagger: the case of
a word is always the most frequent one observed in
training data, with the exception that the sentence-
initial word is always capitalized. A 1-gram capital-
izer is usually used as a baseline for capitalization
experiments (Lita et al., 2003; Kim and Woodland,
2004; Chelba and Acero, 2004).
Lita et al. (2003) view capitalization as a lexi-
cal ambiguity resolution problem, where the lexi-
cal choices for each lowercased word happen to be
its different surface forms. For a lowercased sen-
tence e, a trigram language model is used to find the
best capitalization tag sequence T that maximizes
p(T, e) = p(E), resulting in a case-sensitive sen-
tence E. Besides local trigrams, sentence-level
contexts like sentence-initial position are employed
as well.
Chelba and Acero (2004) frame capitalization as
a sequence labeling problem, where, for each low-
</bodyText>
<figure confidence="0.6502366">
Finput
Lower Case
Train Monolingual
Capitalization Model
Eoutput
</figure>
<figureCaption confidence="0.952297">
Figure 1: The monolingual capitalization scheme employed
by most statistical MT systems.
</figureCaption>
<bodyText confidence="0.998225">
ercased sentence e, they find the label sequence T
that maximizes p(Tle). They use a maximum en-
tropy Markov model (MEMM) to combine features
of words, cases and context (i.e., tag transitions).
Gale et al. (1994) report good results on capital-
izing 100 words. Mikheev (1999) performs capital-
ization using simple positional heuristics.
</bodyText>
<sectionHeader confidence="0.993211" genericHeader="method">
3 Monolingual Capitalization Scheme
</sectionHeader>
<bodyText confidence="0.999750041666667">
Translation and capitalization are usually performed
in two successive steps because removing case infor-
mation from the training of translation models sub-
stantially reduces both the source and target vocabu-
lary sizes. Smaller vocabularies lead to a smaller
translation model with fewer parameters to learn.
For example, if we do not remove the case informa-
tion, we will have to deal with at least nine prob-
abilities for the English-French word pair (click,
cliquez). This is because either “click” or “cliquez”
can have at least three tags (IU, AL, AU), and thus
three surface forms. A smaller translation model re-
quires less training data, and can be estimated more
accurately than otherwise from the same amount
of training data. A smaller translation model also
means less memory usage.
Most statistical MT systems employ the monolin-
gual capitalization scheme as shown in Figure 1. In
this scheme, the translation model and the target lan-
guage model are trained from the lowercased cor-
pora. The capitalization model is trained from the
case-sensitive target corpus. In decoding, we first
turn input into lowercase, then use the decoder to
generate the lowercased translation, and finally ap-
</bodyText>
<figure confidence="0.996231157894737">
Train
Translation Model
Train
Language Model
{r}
{F}
Lower Case
{E}
{e}
Lower Case
Translation
Model
MT Decoder
r
Languagel
Model
e
Capitalization
Monolingual Cap Model
</figure>
<page confidence="0.996258">
2
</page>
<table confidence="0.999570875">
HYDRAULIC HEADER TILT CYLINDER KIT
Kit de v´erin d’inclinaison hydraulique de la plate-forme
haut-parleur avant droit +
HAUT-PARLEUR AVANT DROIT +
Seat Controls, Standard
COMMANDES DU SIGE, STANDARD
loading a saved legend
Chargement d’une l´egende sauvegarde
</table>
<tableCaption confidence="0.834132">
Table 1: Errors made by monolingual capitalization model.
Each row contains a pair of MT input and MT output.
</tableCaption>
<figure confidence="0.9836645">
Pi
Cliquez
Click OK
Ei
</figure>
<figureCaption confidence="0.973911">
Figure 3: Alignment graph. Brackets mean phrase bound-
aries.
</figureCaption>
<figure confidence="0.99119725">
F
E
OK
Word/Phrase Aligner
</figure>
<bodyText confidence="0.9874356">
The bilingual capitalization algorithm recovers
the capitalized sentence E from e, according to the
input sentence F, and the alignment A. Formally,
we look for the best capitalized sentence E* such
that
</bodyText>
<figure confidence="0.999541857142857">
{F}
{E}
Finput
Lower Case
s
MT Decoder
e
Bilingual
Cap Model
Capitalization
alignment
Train Bilingual
Cap Model
Eoutput
</figure>
<figureCaption confidence="0.999219">
Figure 2: A bilingual capitalization scheme.
</figureCaption>
<bodyText confidence="0.9999102">
ply the capitalization model to recover the case of
the decoding output.
The monolingual capitalization scheme makes
many errors as shown in Table 1. Each cell in
the table contains the MT-input and the MT-output.
These errors are due to the capitalizer does not have
access to the source sentence.
Regardless, estimating mixed-cased translation
models, however, is a very interesting topic and
worth future study.
</bodyText>
<sectionHeader confidence="0.996651" genericHeader="method">
4 Bilingual Capitalization Model
</sectionHeader>
<subsectionHeader confidence="0.918594">
4.1 The Model
</subsectionHeader>
<bodyText confidence="0.9999831">
Our probabilistic bilingual capitalization model ex-
ploits case information from both the input sentence
to the MT system and the output sentence from the
system (see Figure 2). An MT system translates a
capitalized sentence F into a lowercased sentence e.
A statistical MT system can also provide the align-
ment A between the input F and the output e; for
example, a statistical phrase-based MT system could
provide the phrase boundaries in F and e, and also
the alignment between the phrases.1
</bodyText>
<footnote confidence="0.8925065">
1We shall explain our capitalization model within the
phrase-based SMT framework, the model, however, could be
</footnote>
<equation confidence="0.989714">
E* = arg maxEEGEN(e)p(E|F,A) (1)
</equation>
<bodyText confidence="0.999585647058823">
where GEN(e) is a function returning the set of
possible capitalized sentences consistent with e. No-
tice that e does not appear in p(EIF, A) because we
can uniquely obtain e from E. p(E F, A) is the cap-
italization model of concern in this paper.2
To further decompose the capitalization model
p(EIF, A), we make some assumptions. As shown
in Figure 3, input sentence F, capitalized output E,
and their alignment can be viewed as a graph. Ver-
tices of the graph correspond to words in F and
E. An edge connecting a word in F and a word
in E corresponds to a word alignment. An edge
between two words in E represents the dependency
between them captured by monolingual n-gram lan-
guage models. We also assume that both E and
F have phrase boundaries available (denoted by the
square brackets), and that A is the phrase alignment.
In Figure 3, Fj is the j-th phrase of F, EZ is the i-th
phrase of E, and they align to each other. We do not
require a word alignment; instead we find it reason-
able to think that a word in EZ can be aligned to any
adapted to syntax-based machine translation, too. To this end,
the translational correspondence is described within a transla-
tion rule, i.e., (Galley et al., 2004) (or a synchronous produc-
tion), rather than a translational phrase pair; and the training
data will be derivation forests, instead of the phrase-aligned
bilingual corpus.
2The capitalization model p(EIF, A) itself does not require
the existence of e. This means that in principle this model can
also be viewed as a capitalized translation model that performs
translation and capitalization in an integrated step. In our paper,
however, we consider the case where the machine translation
output e is given, which is reflected by the the fact that GEN(e)
takes e as input in Formula 1.
</bodyText>
<page confidence="0.993255">
3
</page>
<bodyText confidence="0.99634775">
word in �Fj. A probabilistic model defined on this 4.3 Feature Functions
graph is a Conditional Random Field. Therefore,
it is natural to formulate the bilingual capitalization
model using CRFs:3
</bodyText>
<equation confidence="0.995649666666667">
pa(E |F, A) = 1exp Xz λifi(E, F, A)! (2)
Z(F, A, λ)
i��
</equation>
<bodyText confidence="0.97592425">
where
We define features based on the alignment graph
in Figure 3. Each feature function is defined on a
word.
Monolingual language model feature. The
monolingual LM feature of word Ei is the loga-
rithm of the probability of the n-gram ending at
Ei:
</bodyText>
<equation confidence="0.976028333333333">
X exp Xz !λifi(E, F, A) (3) fLM(Ei, F, A) = log p(Ei|Ei−1, ..., Ei−n+1) (6)
Z(F, A, λ) = i��
EEGEN(e)
</equation>
<bodyText confidence="0.94438575">
fi(E, F, A), i = 1...I are the I features, and
A = (A1,..., AI) is the feature weight vector. Based
on this capitalization model, the decoder in the cap-
italizer looks for the best E∗ such that
</bodyText>
<equation confidence="0.979961">
I
E∗ = arg maxE∈GEN(e,F) Aifi(E, F, A) (4)
i=1
</equation>
<subsectionHeader confidence="0.935845">
4.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.987791684210526">
Following Roark et al. (2004), Lafferty et al. (2001)
and Chen and Rosenfeld (1999), we are looking for
the set of feature weights A maximizing the regu-
larized log-likelihood LLR(A) of the training data
{E(n), F(n), A(n), n = 1, ..., N}.
p should be appropriately smoothed such that it
never returns zero.
Capitalized translation model feature. Sup-
pose E phrase “Click OK” is aligned to F
phrase “Cliquez OK”. The capitalized transla-
tion model feature of “Click” is computed as
log p(Click|Cliquez)+log p(Click|OK). “Click” is
assumed to be aligned to any word in the F phrase.
The larger the probability that “Click” is translated
from an F word, i.e., “Cliquez”, the more chances
that “Click” preserves the case of “Cliquez”. For-
mally, for word Ei, and an aligned phrase pair �El
and �Fm, where Ei E El, the capitalized translation
model feature of Ei is
</bodyText>
<equation confidence="0.997873">
LLR(λ) = N “E(n)|F(n), A(n)” − ||λ||2 fcap·t1(Ei, F, A) = log  |˜Fes,, |p(Ei |Fm,k) (7)
X log p (5) k=1
n�� 2σ2
</equation>
<bodyText confidence="0.999568384615384">
The second term at the right-hand side of For-
mula 5 is a zero-mean Gaussian prior on the pa-
rameters. Q is the variance of the Gaussian prior
dictating the cost of feature weights moving away
from the mean — a smaller value of Q keeps feature
weights closer to the mean. Q can be determined
by linear search on development data.4 The use of
the Gaussian prior term in the objective function has
been found effective in avoiding overfitting, leading
to consistently better results. The choice of LLR as
an objective function can be justified as maximum
a-posteriori (MAP) training within a Bayesian ap-
proach (Roark et al., 2004).
</bodyText>
<footnote confidence="0.9941608">
3We chose CRFs over other sequence labeling models (i.e.
MEMM) because CRFs have no label bias and we do not need
to compute the partition function during decoding.
4In our experiment, we use an empirical value σ = 0.5 as in
(Roark et al., 2004).
</footnote>
<bodyText confidence="0.9988539375">
p(Ei |�Fm,k) is the capitalized translation table. It
needs smoothing to avoid returning zero, and is esti-
mated from a word-aligned bilingual corpus.
Capitalization tag translation feature. The fea-
ture value of E word “Click” aligning to F phrase
“Cliquez OK” is log p(IU|IU)p(click|cliquez) +
log p(IU|AU)p(click|ok). We see that this feature
is less specific than the capitalized translation model
feature. It is computed in terms of the tag transla-
tion probability and the lowercased word translation
probability. The lowercased word translation proba-
bility, i.e., p(click|ok), is used to decide how much
of the tag translation probability, i.e., p(IU|AU),
will contribute to the final decision. The smaller the
word translation probability, i.e., p(click|ok), is, the
smaller the chance that the surface form of “click”
</bodyText>
<page confidence="0.98646">
4
</page>
<bodyText confidence="0.96465">
preserves case from that of “ok”. Formally, this fea-
ture is defined as
</bodyText>
<equation confidence="0.997792">
fcap·tag·t1(Ei, F, A) =
fm,k) × p(T(Ei)|T( Fm,k)) (8)
</equation>
<bodyText confidence="0.992646595744681">
p(ei |�fm,k) is the t-table over lowercased word pairs,
which is the usual “t-table” in a SMT system.
p(T(Ei)|T(�Fm,k)) is the probability of a target cap-
italization tag given a source capitalization tag and
can be easily estimated from a word-aligned bilin-
gual corpus. This feature attempts to help when
fcap−t1 fails (i.e., the capitalized word pair is un-
seen). Smoothing is also applied to both p(ei |fm,k)
and p(T(Ei)|T(�Fm,k)) to handle unseen words (or
word pairs).
Upper-case translation feature. Word Ei is in
all upper case if all words in the corresponding F
phrase Pm are in upper case. Although this fea-
ture can also be captured by the capitalization tag
translation feature in the case where an AU tag in
the input sentence is most probably preserved in the
output sentence, we still define it to emphasize its
effect. This feature aims, for example, to translate
“ABC XYZ” into “UUU VVV” even if all words are
unseen.
Initial capitalization feature. An E word is ini-
tially capitalized if it is the first word that contains
letters in the E sentence. For example, for sentence
“• Please click the button” that starts with a bul-
let, the initial capitalization feature value of word
“please” is 1 because “•” does not contain a letter.
Punctuation feature template. An E word is ini-
tially capitalized if it follows a punctuation mark.
Non-sentence-ending punctuation marks like com-
mas will usually get negative weights.
As one can see, our features are “coarse-grained”
(e.g., the language model feature). In contrast, Kim
and Woodland (2004) and Roark et al. (2004) use
“fine-grained” features. They treat each n-gram as
a feature for, respectively, monolingual capitaliza-
tion and language modeling. Feature weights tuned
at a fine granularity may lead to better accuracy,
but they require much more training data, and re-
sult in much slower training speed, especially for
large-scale learning problems. Coarse-grained fea-
tures enable us to efficiently get the feature values
from a very large training corpus, and quickly tune
the weights on small development sets. For exam-
ple, we can train a bilingual capitalization model on
a 70 million-word corpus in several hours with the
coarse-grained features presented above, but in sev-
eral days with fine-grained n-gram count features.
</bodyText>
<subsectionHeader confidence="0.994439">
4.4 The GEN Function
</subsectionHeader>
<bodyText confidence="0.999985642857143">
Function GEN generates the set of case-sensitive
candidates from a lowercased token. For exam-
ple GEN(mt) = {mt, mT, Mt, MT}. The follow-
ing heuristics can be used to reduce the range of
GEN. The returned set of GEN on a lower-cased to-
ken w is the union of: (i) {w, AU(w), IU(w)}, (ii)
{v|v is seen in training data and AL(v) = w},
and (iii) { �Fm,k|AL( �Fm,k) = AL(w)}. The heuris-
tic (iii) is designed to provide more candidates for
w when it is translated from a very strange input
word �Fm,k in the F phrase �Fm that is aligned to the
phrase that w is in. This heuristic creates good capi-
talization candidates for the translation of URLs, file
names, and file paths.
</bodyText>
<sectionHeader confidence="0.9342" genericHeader="method">
5 Generating Phrase-Aligned Training
Data
</sectionHeader>
<bodyText confidence="0.9999835">
Training the bilingual capitalization model requires
a bilingual corpus with phrase alignments, which are
usually produced from a phrase aligner. In practice,
the task of phrase alignment can be quite computa-
tionally expensive as it requires to translate the en-
tire training corpus; also a phrase aligner is not al-
ways available. We therefore generate the training
data using a naive phrase aligner (NPA) instead of
resorting to a real one.
The input to the NPA is a word-aligned bilingual
corpus. The NPA stochastically chooses for each
sentence pair one segmentation and phrase align-
ment that is consistent with the word alignment. An
aligned phrase pair is consistent with the word align-
ment if neither phrase contains any word aligning
to a word outside the other phrase (Och and Ney,
2004). The NPA chunks the source sentence into
phrases according to a probabilistic distribution over
source phrase lengths. This distribution can be ob-
tained from the trace output of a phrase-based MT
</bodyText>
<equation confidence="0.98054575">
log
p(ei|
� |�f�|
k=1
</equation>
<page confidence="0.98752">
5
</page>
<table confidence="0.9998466">
Languages Entire Corpus (#V) Test-BLEU
Training Dev Test-Prec. (#sents)
EBF (IT) 62M 13K 15K 763
FSE (news) 144M 11K 22K 241
CSE (news) 50M 8K 17K 919
</table>
<tableCaption confidence="0.999773">
Table 2: Corpora used in experiments.
</tableCaption>
<bodyText confidence="0.999894666666667">
decoder on a small development set. The NPA has
to retry if the current source phrase cannot find any
consistent target phrase. Unaligned target words are
attached to the left phrase. Heuristics are employed
to prevent the NPA from not coming to a solution.
Obviously, the NPA is a special case of the phrase
extractor in (Och and Ney, 2004) in that it considers
only one phrase alignment rather than all possible
ones.
Unlike a real phrase aligner, the NPA need not
wait for the training of the translation model to fin-
ish, making it possible for parallelization of transla-
tion model training and capitalization model train-
ing. However, we believe that a real phrase aligner
may make phrase alignment quality higher.
</bodyText>
<sectionHeader confidence="0.998022" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994712">
6.1 Settings
</subsectionHeader>
<bodyText confidence="0.999990777777778">
We conducted capitalization experiments on three
language pairs: English-to-French (E—*F) with a
bilingual corpus from the Information Technology
(IT) domain; French-to-English (F—*E) with a bilin-
gual corpus from the general news domain; and
Chinese-to-English (C—*E) with a bilingual corpus
from the general news domain as well. Each lan-
guage pair comes with a training corpus, a develop-
ment corpus and two test sets (see Table 2). Test-
Precision is used to test the capitalization precision
of the capitalizer on well-formed sentences drawn
from genres similar to those used for training. Test-
BLEU is used to assess the impact of our capitalizer
on end-to-end translation performance; in this case,
the capitalizer may operate on ungrammatical sen-
tences. We chose to work with these three language
pairs because we wanted to test our capitalization
model on both English and French target MT sys-
tems and in cases where the source language has no
case information (such as in Chinese).
We estimated the feature functions, such as the
log probabilities in the language model, from the
training set. Kneser-Ney smoothing (Kneser and
Ney, 1995) was applied to features fLM, fcap·t1,
and fcap·tag·t1. We trained the feature weights of
the CRF-based bilingual capitalization model using
the development set. Since estimation of the feature
weights requires the phrase alignment information,
we efficiently applied the NPA on the development
set.
We employed two LM-based capitalizers as base-
lines for performance comparison: a unigram-based
capitalizer and a strong trigram-based one. The
unigram-based capitalizer is the usual baseline for
capitalization experiments in previous work. The
trigram-based baseline is similar to the one in
(Lita et al., 2003) except that we used Kneser-Ney
smoothing instead of a mixture.
A phrase-based SMT system (Marcu and Wong,
2002) was trained on the bitext. The capitalizer
was incorporated into the MT system as a post-
processing module — it capitalizes the lowercased
MT output. The phrase boundaries and alignments
needed by the capitalizer were automatically in-
ferred as part of the decoding process.
</bodyText>
<subsectionHeader confidence="0.999365">
6.2 BLEU and Precision
</subsectionHeader>
<bodyText confidence="0.999584391304348">
We measured the impact of our capitalization model
in the context of an end-to-end MT system using
BLEU (Papineni et al., 2001). In this context, the
capitalizer operates on potentially ill-formed, MT-
produced outputs.
To this end, we first integrated our bilingual capi-
talizer into the phrase-based SMT system as a post-
processing module. The decoder of the MT sys-
tem was modified to provide the capitalizer with
the case-preserved source sentence, the lowercased
translation, and the phrase boundaries and their
alignments. Based on this information, our bilin-
gual capitalizer recovers the case information of the
lowercased translation, outputting a capitalized tar-
get sentence. The case-restored machine transla-
tions were evaluated against the target test-BLEU
set. For comparison, BLEU scores were also com-
puted for an MT system that used the two LM-based
baselines.
We also assessed the performance of our capital-
izer on the task of recovering case information for
well-formed grammatical texts. To this end, we used
the precision metric that counted the number of cor-
</bodyText>
<page confidence="0.99856">
6
</page>
<bodyText confidence="0.999840125">
rectly capitalized words produced by our capitalizer
on well-formed, lowercased input
To obtain the capitalization precision, we im-
plemented the capitalizer as a standalone program.
The inputs to the capitalizer were triples of a case-
preserved source sentence, a lowercased target sen-
tence, and phrase alignments between them. The
output was the case-restored version of the target
sentence. In this evaluation scenario, the capitalizer
output and the reference differ only in case infor-
mation — word choices and word orders between
them are the same. Testing was conducted on Test-
Precision. We applied the NPA to the Test-Precision
set to obtain the phrases and their alignments be-
cause they were needed to trigger the features in
testing. We used a Test-Precision set that was dif-
ferent from the Test-BLEU set because word align-
ments were by-products only of training of transla-
tion models on the MT training data and we could
not put the Test-BLEU set into the MT training
data. Rather than implementing a standalone word
aligner, we randomly divided the MT training data
into three non-overlapping sets: Test-Precision set,
CRF capitalizer training set and dev set.
</bodyText>
<sectionHeader confidence="0.508413" genericHeader="evaluation">
6.3 Results
</sectionHeader>
<bodyText confidence="0.99993935483871">
The performance comparisons between our CRF-
based capitalizer and the two LM-based baselines
are shown in Table 3 and Table 4. Table 3 shows
the BLEU scores, and Table 4 shows the precision.
The BLEU upper bounds indicate the ceilings that a
perfect capitalizer can reach, and are computed by
ignoring the case information in both the capitalizer
outputs and the reference. Obviously, the precision
upper bounds for all language pairs are 100%.
The precision and end-to-end BLEU based com-
parisons show that, for European language pairs, the
CRF-based bilingual capitalization model outper-
forms significantly the strong LM-based baseline.
We got more than one BLEU point improvement on
the MT translation between English and French, a
34% relative reduction in capitalization error rate for
the French-to-English language pair, and a 42% rel-
ative error rate reduction for the English-to-French
language pair. These results show that source lan-
guage information provides significant help for cap-
italizing machine translation outputs. The results
also show that when the source language does not
have case, as in Chinese, the bilingual model equals
a monolingual one.
The BLEU difference between the CRF-based
capitalizer and the trigram one were larger than
the precision difference. This indicates that the
CRF-based capitalizer performs much better on non-
grammatical texts that are generated from an MT
system due to the bilingual feature of the CRF capi-
talizer.
</bodyText>
<subsectionHeader confidence="0.996275">
6.4 Effect of Training Corpus Size
</subsectionHeader>
<bodyText confidence="0.9999958">
The experiments above were carried out on large
data sets. We also conducted experiments to exam-
ine the effect of the training corpus size on capital-
ization precision. Figure 4 shows the effects. The
experiment was performed on the E—*F corpus. The
bilingual capitalizer performed significantly better
when the training corpus size was small (e.g., un-
der 8 million words). This is common in many do-
mains: when the training corpus size increases, the
difference between the two capitalizers decreases.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99992215">
In this paper, we have studied how to exploit bilin-
gual information to improve capitalization perfor-
mance on machine translation output, and evaluated
the improvement over traditional methods that use
only monolingual language models.
We first presented a probabilistic bilingual cap-
italization model for capitalizing machine transla-
tion outputs using conditional random fields. This
model exploits bilingual capitalization knowledge as
well as monolingual information. We defined a se-
ries of feature functions to incorporate capitalization
knowledge into the model.
We then evaluated our CRF-based bilingual capi-
talization model both on well-formed texts in terms
of capitalization precision, and on possibly ungram-
matical end-to-end machine translation outputs in
terms of BLEU scores. Experiments were per-
formed on both French and English target MT sys-
tems with large-scale training data. Our experimen-
tal results showed that the CRF-based bilingual cap-
</bodyText>
<figure confidence="0.988067">
#correctly capitalized words
precision =
(9)
#total words
</figure>
<page confidence="0.996501">
7
</page>
<table confidence="0.999906833333333">
Translation Unigram BLEU Scores CRF-based Upper
Capitalizer Trigram Capitalizer Bound
Capitalizer
F→E 24.96 26.73 27.92 28.85
E→F 32.63 34.66 36.10 36.17
C→E 23.81 25.92 25.89 -
</table>
<tableCaption confidence="0.993455">
Table 3: Impact of CRF-based capitalizer on end-to-end translation performance compared with two LM-based baselines.
</tableCaption>
<table confidence="0.999897571428571">
Translation Unigram Capitalization (%)
capitalizer Precision CRF-based
Trigram capitalizer
capitalizer
F→E 94.03 98.79 99.20
E→F 91.52 98.47 99.11
C→E 90.77 96.40 96.76
</table>
<tableCaption confidence="0.99953">
Table 4: Impact of CRF-based capitalizer on capitalization precision compared with two LM-based baselines.
</tableCaption>
<figure confidence="0.725928">
Training Corpus Size (MWs)
</figure>
<figureCaption confidence="0.999528">
Figure 4: Capitalization precision with respect to size of train-
ing corpus. LM-based capitalizer refers to the trigram-based
one. Results were on E→F corpus.
</figureCaption>
<bodyText confidence="0.999768818181818">
italization model performs significantly better than a
strong baseline, monolingual capitalizer that uses a
trigram language model.
In all experiments carried out at Language Weaver
with customer (or domain specific) data, MT sys-
tems trained on lowercased data coupled with the
CRF bilingual capitalizer described in this paper
consistently outperformed both MT systems trained
on lowercased data coupled with a strong monolin-
gual capitalizer and MT systems trained on mixed-
cased data.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99969966">
Ciprian Chelba and Alex Acero. 2004. Adaptation of maxi-
mum entroy capitalizer: Little data can help a lot. In Pro-
ceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), Barcelona, Spain.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing Maximum Entropy models. Technical Report
CMUCS-99-108, Carnegie Mellon University.
William A. Gale, Kenneth W. Church, and David Yarowsky.
1994. Discrimination decisions for 100,000-dimensional
spaces. In Current issues in computational linguistics.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What’s in a Translation Rule? In Proceedings of the Human
Language Technology Conference and the North American
Association for Computational Linguistics (HLT-NAACL),
Boston, Massachusetts.
Ji-Hwan Kim and Philip C. Woodland. 2004. Automatic capi-
talization generation for speech input. Computer Speech and
Language, 18(1):67–90, January.
Reinhard Kneser and Hermann Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Signal
Processing (ICASSP) 1995, pages 181–184, Detroit, Michi-
gan. IEEE.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for segmen-
tation and labeling sequence data.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and Nanda
Kambhatla. 2003. tRuEcasIng. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), Sapporo, Japan, July.
Daniel Marcu and William Wong. 2002. A phrase-based, joint
probability model for statistical machine translation. In Pro-
ceedings of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP), Philadelphia, PA.
A. Mikheev. 1999. A knowledge-free method fro capitalized
word disambiguation. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguistics
(ACL), College Park, Maryland, June.
Franz Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. Computational
Linguistics, 30(4).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. BLEU: A method for automatic evaluation
of Machine Translation. Technical Report RC22176, IBM,
September.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004. Discriminative language modeling with condi-
tional random field and the perceptron algorithm. In Pro-
ceedings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL), Barcelona, Spain.
</reference>
<figure confidence="0.990241">
0.1 0.2 0.5 1.0 2.0 4.0 8.0 16.0 32.0 64.0
Precision (x%)
100
99
98
97
96
95
94
93
92
CRF-based capitalizer
LM-based capitalizer
</figure>
<page confidence="0.943232">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828995">
<title confidence="0.99993">Capitalizing Machine Translation</title>
<author confidence="0.999407">Wang Knight</author>
<affiliation confidence="0.934767">Language Weaver,</affiliation>
<address confidence="0.996523">4640 Admiralty Way, Suite</address>
<author confidence="0.924978">Marina del Rey</author>
<author confidence="0.924978">CA</author>
<email confidence="0.982412">kknight,</email>
<abstract confidence="0.997867181818182">We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields. Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entroy capitalizer: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2424" citStr="Chelba and Acero (2004)" startWordPosition="391" endWordPosition="394">c MX . AN”, getting the surface form “Click OK to save your changes to /home/DOC .”. A capitalizer is a tagger that recovers the capitalization tag for each input lowercased word, outputting a well-capitalized sentence. Since each lowercased word can have more than one tag, and associating a tag with a lowercased word can result in more than one surface form (e.g., /home/doc MX can be either /home/DOC or /home/Doc), we need a capitalization model to solve the capitalization ambiguities. For example, Lita et al. (2003) use a trigram language model estimated from a corpus with case information; Chelba and Acero (2004) use a maximum entropy Markov model (MEMM) combining features involving words and their cases. Capitalization models presented in most previous approaches are monolingual because the models are estimated only from monolingual texts. However, for capitalizing machine translation outputs, using only monolingual capitalization models is not enough. For example, if the sentence “click ok to save your changes to /home/doc .” in the above example is the translation of the French sentence “CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFICATIONS DANS /HOME/DOC .”, the correct capitalization result should pr</context>
<context position="4920" citStr="Chelba and Acero, 2004" startWordPosition="782" endWordPosition="785">ments are performed on both French and English targeted MT systems with large-scale training data. Our experimental results show that the CRF-based bilingual capitalization model performs better than a strong baseline capitalizer that uses a trigram language model. 2 Related Work A simple capitalizer is the 1-gram tagger: the case of a word is always the most frequent one observed in training data, with the exception that the sentenceinitial word is always capitalized. A 1-gram capitalizer is usually used as a baseline for capitalization experiments (Lita et al., 2003; Kim and Woodland, 2004; Chelba and Acero, 2004). Lita et al. (2003) view capitalization as a lexical ambiguity resolution problem, where the lexical choices for each lowercased word happen to be its different surface forms. For a lowercased sentence e, a trigram language model is used to find the best capitalization tag sequence T that maximizes p(T, e) = p(E), resulting in a case-sensitive sentence E. Besides local trigrams, sentence-level contexts like sentence-initial position are employed as well. Chelba and Acero (2004) frame capitalization as a sequence labeling problem, where, for each lowFinput Lower Case Train Monolingual Capitali</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entroy capitalizer: Little data can help a lot. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing Maximum Entropy models.</title>
<date>1999</date>
<tech>Technical Report CMUCS-99-108,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="12053" citStr="Chen and Rosenfeld (1999)" startWordPosition="1967" endWordPosition="1970">ature function is defined on a word. Monolingual language model feature. The monolingual LM feature of word Ei is the logarithm of the probability of the n-gram ending at Ei: X exp Xz !λifi(E, F, A) (3) fLM(Ei, F, A) = log p(Ei|Ei−1, ..., Ei−n+1) (6) Z(F, A, λ) = i�� EEGEN(e) fi(E, F, A), i = 1...I are the I features, and A = (A1,..., AI) is the feature weight vector. Based on this capitalization model, the decoder in the capitalizer looks for the best E∗ such that I E∗ = arg maxE∈GEN(e,F) Aifi(E, F, A) (4) i=1 4.2 Parameter Estimation Following Roark et al. (2004), Lafferty et al. (2001) and Chen and Rosenfeld (1999), we are looking for the set of feature weights A maximizing the regularized log-likelihood LLR(A) of the training data {E(n), F(n), A(n), n = 1, ..., N}. p should be appropriately smoothed such that it never returns zero. Capitalized translation model feature. Suppose E phrase “Click OK” is aligned to F phrase “Cliquez OK”. The capitalized translation model feature of “Click” is computed as log p(Click|Cliquez)+log p(Click|OK). “Click” is assumed to be aligned to any word in the F phrase. The larger the probability that “Click” is translated from an F word, i.e., “Cliquez”, the more chances t</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing Maximum Entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>Discrimination decisions for 100,000-dimensional spaces. In Current issues in computational linguistics.</title>
<date>1994</date>
<contexts>
<context position="5842" citStr="Gale et al. (1994)" startWordPosition="927" endWordPosition="930">, e) = p(E), resulting in a case-sensitive sentence E. Besides local trigrams, sentence-level contexts like sentence-initial position are employed as well. Chelba and Acero (2004) frame capitalization as a sequence labeling problem, where, for each lowFinput Lower Case Train Monolingual Capitalization Model Eoutput Figure 1: The monolingual capitalization scheme employed by most statistical MT systems. ercased sentence e, they find the label sequence T that maximizes p(Tle). They use a maximum entropy Markov model (MEMM) to combine features of words, cases and context (i.e., tag transitions). Gale et al. (1994) report good results on capitalizing 100 words. Mikheev (1999) performs capitalization using simple positional heuristics. 3 Monolingual Capitalization Scheme Translation and capitalization are usually performed in two successive steps because removing case information from the training of translation models substantially reduces both the source and target vocabulary sizes. Smaller vocabularies lead to a smaller translation model with fewer parameters to learn. For example, if we do not remove the case information, we will have to deal with at least nine probabilities for the English-French wo</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1994</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1994. Discrimination decisions for 100,000-dimensional spaces. In Current issues in computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a Translation Rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="10516" citStr="Galley et al., 2004" startWordPosition="1696" endWordPosition="1699">ween two words in E represents the dependency between them captured by monolingual n-gram language models. We also assume that both E and F have phrase boundaries available (denoted by the square brackets), and that A is the phrase alignment. In Figure 3, Fj is the j-th phrase of F, EZ is the i-th phrase of E, and they align to each other. We do not require a word alignment; instead we find it reasonable to think that a word in EZ can be aligned to any adapted to syntax-based machine translation, too. To this end, the translational correspondence is described within a translation rule, i.e., (Galley et al., 2004) (or a synchronous production), rather than a translational phrase pair; and the training data will be derivation forests, instead of the phrase-aligned bilingual corpus. 2The capitalization model p(EIF, A) itself does not require the existence of e. This means that in principle this model can also be viewed as a capitalized translation model that performs translation and capitalization in an integrated step. In our paper, however, we consider the case where the machine translation output e is given, which is reflected by the the fact that GEN(e) takes e as input in Formula 1. 3 word in �Fj. A</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004. What’s in a Translation Rule? In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL), Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji-Hwan Kim</author>
<author>Philip C Woodland</author>
</authors>
<title>Automatic capitalization generation for speech input.</title>
<date>2004</date>
<journal>Computer Speech and Language,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="4895" citStr="Kim and Woodland, 2004" startWordPosition="778" endWordPosition="781">zation precision. Experiments are performed on both French and English targeted MT systems with large-scale training data. Our experimental results show that the CRF-based bilingual capitalization model performs better than a strong baseline capitalizer that uses a trigram language model. 2 Related Work A simple capitalizer is the 1-gram tagger: the case of a word is always the most frequent one observed in training data, with the exception that the sentenceinitial word is always capitalized. A 1-gram capitalizer is usually used as a baseline for capitalization experiments (Lita et al., 2003; Kim and Woodland, 2004; Chelba and Acero, 2004). Lita et al. (2003) view capitalization as a lexical ambiguity resolution problem, where the lexical choices for each lowercased word happen to be its different surface forms. For a lowercased sentence e, a trigram language model is used to find the best capitalization tag sequence T that maximizes p(T, e) = p(E), resulting in a case-sensitive sentence E. Besides local trigrams, sentence-level contexts like sentence-initial position are employed as well. Chelba and Acero (2004) frame capitalization as a sequence labeling problem, where, for each lowFinput Lower Case T</context>
<context position="16317" citStr="Kim and Woodland (2004)" startWordPosition="2684" endWordPosition="2687">. Initial capitalization feature. An E word is initially capitalized if it is the first word that contains letters in the E sentence. For example, for sentence “• Please click the button” that starts with a bullet, the initial capitalization feature value of word “please” is 1 because “•” does not contain a letter. Punctuation feature template. An E word is initially capitalized if it follows a punctuation mark. Non-sentence-ending punctuation marks like commas will usually get negative weights. As one can see, our features are “coarse-grained” (e.g., the language model feature). In contrast, Kim and Woodland (2004) and Roark et al. (2004) use “fine-grained” features. They treat each n-gram as a feature for, respectively, monolingual capitalization and language modeling. Feature weights tuned at a fine granularity may lead to better accuracy, but they require much more training data, and result in much slower training speed, especially for large-scale learning problems. Coarse-grained features enable us to efficiently get the feature values from a very large training corpus, and quickly tune the weights on small development sets. For example, we can train a bilingual capitalization model on a 70 million-</context>
</contexts>
<marker>Kim, Woodland, 2004</marker>
<rawString>Ji-Hwan Kim and Philip C. Woodland. 2004. Automatic capitalization generation for speech input. Computer Speech and Language, 18(1):67–90, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backingoff for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</booktitle>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<location>Detroit, Michigan.</location>
<contexts>
<context position="20878" citStr="Kneser and Ney, 1995" startWordPosition="3442" endWordPosition="3445">ormed sentences drawn from genres similar to those used for training. TestBLEU is used to assess the impact of our capitalizer on end-to-end translation performance; in this case, the capitalizer may operate on ungrammatical sentences. We chose to work with these three language pairs because we wanted to test our capitalization model on both English and French target MT systems and in cases where the source language has no case information (such as in Chinese). We estimated the feature functions, such as the log probabilities in the language model, from the training set. Kneser-Ney smoothing (Kneser and Ney, 1995) was applied to features fLM, fcap·t1, and fcap·tag·t1. We trained the feature weights of the CRF-based bilingual capitalization model using the development set. Since estimation of the feature weights requires the phrase alignment information, we efficiently applied the NPA on the development set. We employed two LM-based capitalizers as baselines for performance comparison: a unigram-based capitalizer and a strong trigram-based one. The unigram-based capitalizer is the usual baseline for capitalization experiments in previous work. The trigram-based baseline is similar to the one in (Lita et</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backingoff for m-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 1995, pages 181–184, Detroit, Michigan. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmentation and labeling sequence data.</title>
<date>2001</date>
<contexts>
<context position="3897" citStr="Lafferty et al., 2001" startWordPosition="618" endWordPosition="621">2006. c�2006 Association for Computational Linguistics of the MT input, we can hardly get the correct capitalization result. Although monolingual capitalization models in previous work can apply to MT output, a bilingual model is more desirable. This is because MT outputs usually strongly preserve case from the input, and because monolingual capitalization models do not always perform as well on badly translated text as on well-formed syntactic texts. In this paper, we present a bilingual capitalization model for capitalizing machine translation outputs using conditional random fields (CRFs) (Lafferty et al., 2001). This model exploits case information from both the input sentence (source) and the output sentence (target) of the MT system. We define a series of feature functions to incorporate capitalization knowledge into the model. Experimental results are shown in terms of BLEU scores of a phrase-based SMT system with the capitalization model incorporated, and in terms of capitalization precision. Experiments are performed on both French and English targeted MT systems with large-scale training data. Our experimental results show that the CRF-based bilingual capitalization model performs better than </context>
<context position="12023" citStr="Lafferty et al. (2001)" startWordPosition="1962" endWordPosition="1965"> graph in Figure 3. Each feature function is defined on a word. Monolingual language model feature. The monolingual LM feature of word Ei is the logarithm of the probability of the n-gram ending at Ei: X exp Xz !λifi(E, F, A) (3) fLM(Ei, F, A) = log p(Ei|Ei−1, ..., Ei−n+1) (6) Z(F, A, λ) = i�� EEGEN(e) fi(E, F, A), i = 1...I are the I features, and A = (A1,..., AI) is the feature weight vector. Based on this capitalization model, the decoder in the capitalizer looks for the best E∗ such that I E∗ = arg maxE∈GEN(e,F) Aifi(E, F, A) (4) i=1 4.2 Parameter Estimation Following Roark et al. (2004), Lafferty et al. (2001) and Chen and Rosenfeld (1999), we are looking for the set of feature weights A maximizing the regularized log-likelihood LLR(A) of the training data {E(n), F(n), A(n), n = 1, ..., N}. p should be appropriately smoothed such that it never returns zero. Capitalized translation model feature. Suppose E phrase “Click OK” is aligned to F phrase “Cliquez OK”. The capitalized translation model feature of “Click” is computed as log p(Click|Cliquez)+log p(Click|OK). “Click” is assumed to be aligned to any word in the F phrase. The larger the probability that “Click” is translated from an F word, i.e.,</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmentation and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Vlad Lita</author>
<author>Abe Ittycheriah</author>
<author>Salim Roukos</author>
<author>Nanda Kambhatla</author>
</authors>
<title>tRuEcasIng.</title>
<date>2003</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="752" citStr="Lita et al., 2003" startWordPosition="104" endWordPosition="107">a del Rey, CA, 90292 Nwang, kknight, dmarcu}@languageweaver.com Abstract We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields. Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs. 1 Introduction Capitalization is the process of recovering case information for texts in lowercase. It is also called truecasing (Lita et al., 2003). Usually, capitalization itself tries to improve the legibility of texts. It, however, can affect the word choice or order when interacting with other models. In natural language processing, a good capitalization model has been shown useful for tasks like name entity recognition, automatic content extraction, speech recognition, modern word processors, and machine translation (MT). Capitalization can be viewed as a sequence labeling process. The input to this process is a sentence in lowercase. For each lowercased word in the input sentence, we have several available capitalization tags: init</context>
<context position="2324" citStr="Lita et al. (2003)" startWordPosition="374" endWordPosition="377">our changes to /home/doc.” into “click IU ok AU to AL save AL your AL changes AL to AL /home/doc MX . AN”, getting the surface form “Click OK to save your changes to /home/DOC .”. A capitalizer is a tagger that recovers the capitalization tag for each input lowercased word, outputting a well-capitalized sentence. Since each lowercased word can have more than one tag, and associating a tag with a lowercased word can result in more than one surface form (e.g., /home/doc MX can be either /home/DOC or /home/Doc), we need a capitalization model to solve the capitalization ambiguities. For example, Lita et al. (2003) use a trigram language model estimated from a corpus with case information; Chelba and Acero (2004) use a maximum entropy Markov model (MEMM) combining features involving words and their cases. Capitalization models presented in most previous approaches are monolingual because the models are estimated only from monolingual texts. However, for capitalizing machine translation outputs, using only monolingual capitalization models is not enough. For example, if the sentence “click ok to save your changes to /home/doc .” in the above example is the translation of the French sentence “CLIQUEZ SUR </context>
<context position="4871" citStr="Lita et al., 2003" startWordPosition="774" endWordPosition="777">n terms of capitalization precision. Experiments are performed on both French and English targeted MT systems with large-scale training data. Our experimental results show that the CRF-based bilingual capitalization model performs better than a strong baseline capitalizer that uses a trigram language model. 2 Related Work A simple capitalizer is the 1-gram tagger: the case of a word is always the most frequent one observed in training data, with the exception that the sentenceinitial word is always capitalized. A 1-gram capitalizer is usually used as a baseline for capitalization experiments (Lita et al., 2003; Kim and Woodland, 2004; Chelba and Acero, 2004). Lita et al. (2003) view capitalization as a lexical ambiguity resolution problem, where the lexical choices for each lowercased word happen to be its different surface forms. For a lowercased sentence e, a trigram language model is used to find the best capitalization tag sequence T that maximizes p(T, e) = p(E), resulting in a case-sensitive sentence E. Besides local trigrams, sentence-level contexts like sentence-initial position are employed as well. Chelba and Acero (2004) frame capitalization as a sequence labeling problem, where, for eac</context>
<context position="21489" citStr="Lita et al., 2003" startWordPosition="3530" endWordPosition="3533">, 1995) was applied to features fLM, fcap·t1, and fcap·tag·t1. We trained the feature weights of the CRF-based bilingual capitalization model using the development set. Since estimation of the feature weights requires the phrase alignment information, we efficiently applied the NPA on the development set. We employed two LM-based capitalizers as baselines for performance comparison: a unigram-based capitalizer and a strong trigram-based one. The unigram-based capitalizer is the usual baseline for capitalization experiments in previous work. The trigram-based baseline is similar to the one in (Lita et al., 2003) except that we used Kneser-Ney smoothing instead of a mixture. A phrase-based SMT system (Marcu and Wong, 2002) was trained on the bitext. The capitalizer was incorporated into the MT system as a postprocessing module — it capitalizes the lowercased MT output. The phrase boundaries and alignments needed by the capitalizer were automatically inferred as part of the decoding process. 6.2 BLEU and Precision We measured the impact of our capitalization model in the context of an end-to-end MT system using BLEU (Papineni et al., 2001). In this context, the capitalizer operates on potentially ill-f</context>
</contexts>
<marker>Lita, Ittycheriah, Roukos, Kambhatla, 2003</marker>
<rawString>Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and Nanda Kambhatla. 2003. tRuEcasIng. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="21601" citStr="Marcu and Wong, 2002" startWordPosition="3548" endWordPosition="3551">d bilingual capitalization model using the development set. Since estimation of the feature weights requires the phrase alignment information, we efficiently applied the NPA on the development set. We employed two LM-based capitalizers as baselines for performance comparison: a unigram-based capitalizer and a strong trigram-based one. The unigram-based capitalizer is the usual baseline for capitalization experiments in previous work. The trigram-based baseline is similar to the one in (Lita et al., 2003) except that we used Kneser-Ney smoothing instead of a mixture. A phrase-based SMT system (Marcu and Wong, 2002) was trained on the bitext. The capitalizer was incorporated into the MT system as a postprocessing module — it capitalizes the lowercased MT output. The phrase boundaries and alignments needed by the capitalizer were automatically inferred as part of the decoding process. 6.2 BLEU and Precision We measured the impact of our capitalization model in the context of an end-to-end MT system using BLEU (Papineni et al., 2001). In this context, the capitalizer operates on potentially ill-formed, MTproduced outputs. To this end, we first integrated our bilingual capitalizer into the phrase-based SMT </context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
</authors>
<title>A knowledge-free method fro capitalized word disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>College Park, Maryland,</location>
<contexts>
<context position="5904" citStr="Mikheev (1999)" startWordPosition="939" endWordPosition="940">al trigrams, sentence-level contexts like sentence-initial position are employed as well. Chelba and Acero (2004) frame capitalization as a sequence labeling problem, where, for each lowFinput Lower Case Train Monolingual Capitalization Model Eoutput Figure 1: The monolingual capitalization scheme employed by most statistical MT systems. ercased sentence e, they find the label sequence T that maximizes p(Tle). They use a maximum entropy Markov model (MEMM) to combine features of words, cases and context (i.e., tag transitions). Gale et al. (1994) report good results on capitalizing 100 words. Mikheev (1999) performs capitalization using simple positional heuristics. 3 Monolingual Capitalization Scheme Translation and capitalization are usually performed in two successive steps because removing case information from the training of translation models substantially reduces both the source and target vocabulary sizes. Smaller vocabularies lead to a smaller translation model with fewer parameters to learn. For example, if we do not remove the case information, we will have to deal with at least nine probabilities for the English-French word pair (click, cliquez). This is because either “click” or “c</context>
</contexts>
<marker>Mikheev, 1999</marker>
<rawString>A. Mikheev. 1999. A knowledge-free method fro capitalized word disambiguation. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), College Park, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="18582" citStr="Och and Ney, 2004" startWordPosition="3065" endWordPosition="3068">e alignment can be quite computationally expensive as it requires to translate the entire training corpus; also a phrase aligner is not always available. We therefore generate the training data using a naive phrase aligner (NPA) instead of resorting to a real one. The input to the NPA is a word-aligned bilingual corpus. The NPA stochastically chooses for each sentence pair one segmentation and phrase alignment that is consistent with the word alignment. An aligned phrase pair is consistent with the word alignment if neither phrase contains any word aligning to a word outside the other phrase (Och and Ney, 2004). The NPA chunks the source sentence into phrases according to a probabilistic distribution over source phrase lengths. This distribution can be obtained from the trace output of a phrase-based MT log p(ei| � |�f�| k=1 5 Languages Entire Corpus (#V) Test-BLEU Training Dev Test-Prec. (#sents) EBF (IT) 62M 13K 15K 763 FSE (news) 144M 11K 22K 241 CSE (news) 50M 8K 17K 919 Table 2: Corpora used in experiments. decoder on a small development set. The NPA has to retry if the current source phrase cannot find any consistent target phrase. Unaligned target words are attached to the left phrase. Heuris</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of Machine Translation.</title>
<date>2001</date>
<tech>Technical Report RC22176, IBM,</tech>
<contexts>
<context position="22025" citStr="Papineni et al., 2001" startWordPosition="3618" endWordPosition="3621">previous work. The trigram-based baseline is similar to the one in (Lita et al., 2003) except that we used Kneser-Ney smoothing instead of a mixture. A phrase-based SMT system (Marcu and Wong, 2002) was trained on the bitext. The capitalizer was incorporated into the MT system as a postprocessing module — it capitalizes the lowercased MT output. The phrase boundaries and alignments needed by the capitalizer were automatically inferred as part of the decoding process. 6.2 BLEU and Precision We measured the impact of our capitalization model in the context of an end-to-end MT system using BLEU (Papineni et al., 2001). In this context, the capitalizer operates on potentially ill-formed, MTproduced outputs. To this end, we first integrated our bilingual capitalizer into the phrase-based SMT system as a postprocessing module. The decoder of the MT system was modified to provide the capitalizer with the case-preserved source sentence, the lowercased translation, and the phrase boundaries and their alignments. Based on this information, our bilingual capitalizer recovers the case information of the lowercased translation, outputting a capitalized target sentence. The case-restored machine translations were eva</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. BLEU: A method for automatic evaluation of Machine Translation. Technical Report RC22176, IBM, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random field and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="11999" citStr="Roark et al. (2004)" startWordPosition="1958" endWordPosition="1961">ased on the alignment graph in Figure 3. Each feature function is defined on a word. Monolingual language model feature. The monolingual LM feature of word Ei is the logarithm of the probability of the n-gram ending at Ei: X exp Xz !λifi(E, F, A) (3) fLM(Ei, F, A) = log p(Ei|Ei−1, ..., Ei−n+1) (6) Z(F, A, λ) = i�� EEGEN(e) fi(E, F, A), i = 1...I are the I features, and A = (A1,..., AI) is the feature weight vector. Based on this capitalization model, the decoder in the capitalizer looks for the best E∗ such that I E∗ = arg maxE∈GEN(e,F) Aifi(E, F, A) (4) i=1 4.2 Parameter Estimation Following Roark et al. (2004), Lafferty et al. (2001) and Chen and Rosenfeld (1999), we are looking for the set of feature weights A maximizing the regularized log-likelihood LLR(A) of the training data {E(n), F(n), A(n), n = 1, ..., N}. p should be appropriately smoothed such that it never returns zero. Capitalized translation model feature. Suppose E phrase “Click OK” is aligned to F phrase “Cliquez OK”. The capitalized translation model feature of “Click” is computed as log p(Click|Cliquez)+log p(Click|OK). “Click” is assumed to be aligned to any word in the F phrase. The larger the probability that “Click” is translat</context>
<context position="13564" citStr="Roark et al., 2004" startWordPosition="2232" endWordPosition="2235">ond term at the right-hand side of Formula 5 is a zero-mean Gaussian prior on the parameters. Q is the variance of the Gaussian prior dictating the cost of feature weights moving away from the mean — a smaller value of Q keeps feature weights closer to the mean. Q can be determined by linear search on development data.4 The use of the Gaussian prior term in the objective function has been found effective in avoiding overfitting, leading to consistently better results. The choice of LLR as an objective function can be justified as maximum a-posteriori (MAP) training within a Bayesian approach (Roark et al., 2004). 3We chose CRFs over other sequence labeling models (i.e. MEMM) because CRFs have no label bias and we do not need to compute the partition function during decoding. 4In our experiment, we use an empirical value σ = 0.5 as in (Roark et al., 2004). p(Ei |�Fm,k) is the capitalized translation table. It needs smoothing to avoid returning zero, and is estimated from a word-aligned bilingual corpus. Capitalization tag translation feature. The feature value of E word “Click” aligning to F phrase “Cliquez OK” is log p(IU|IU)p(click|cliquez) + log p(IU|AU)p(click|ok). We see that this feature is less</context>
<context position="16341" citStr="Roark et al. (2004)" startWordPosition="2689" endWordPosition="2692">ture. An E word is initially capitalized if it is the first word that contains letters in the E sentence. For example, for sentence “• Please click the button” that starts with a bullet, the initial capitalization feature value of word “please” is 1 because “•” does not contain a letter. Punctuation feature template. An E word is initially capitalized if it follows a punctuation mark. Non-sentence-ending punctuation marks like commas will usually get negative weights. As one can see, our features are “coarse-grained” (e.g., the language model feature). In contrast, Kim and Woodland (2004) and Roark et al. (2004) use “fine-grained” features. They treat each n-gram as a feature for, respectively, monolingual capitalization and language modeling. Feature weights tuned at a fine granularity may lead to better accuracy, but they require much more training data, and result in much slower training speed, especially for large-scale learning problems. Coarse-grained features enable us to efficiently get the feature values from a very large training corpus, and quickly tune the weights on small development sets. For example, we can train a bilingual capitalization model on a 70 million-word corpus in several h</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random field and the perceptron algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Barcelona, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>