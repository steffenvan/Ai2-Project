<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.985517">
Intentions, Implicatures and Processing of Complex Questions
</title>
<author confidence="0.98814">
Sanda M. Harabagiu and Steven J. Maiorano and Alessandro Moschitti and Cosmin A. Bejan
</author>
<affiliation confidence="0.874245666666667">
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
</affiliation>
<sectionHeader confidence="0.981832" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999305083333333">
In this paper we introduce two methods for
deriving the intentional structure of complex
questions. Techniques that enable the deriva-
tion of implied information are also presented.
We show that both the intentional structure
and the implicatures enabled by it are essen-
tial components of Q/A systems capable of suc-
cessfully processing complex questions. The
results of our evaluation support the claim that
there are multiple interactions between the pro-
cess of answer finding and the coercion of in-
tentions and implicatures.
</bodyText>
<sectionHeader confidence="0.999522" genericHeader="keywords">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.896666">
The Problem of Question Intentions.
</subsectionHeader>
<bodyText confidence="0.9996170625">
When using a Question Answering system to find
information, the user cannot separate the intentions
and beliefs from the formulation of the question. A
direct consequence of this phenomenon is that the user
incorporates his or her intentions and beliefs into the
interrogation. For example, when asking the question:
Q1: What kind of assistance has North Korea received
from the USSR/Russia for its missile program?
the user associate with the question a number of in-
tentions, that maybe expressed a set of intended ques-
tions. Each intended question, in turn generates implied
information, that maybe expresses as implied questions.
For question Q1, a list of intended questions and implied
questions is detailed in Table1.
Most of the intended questions are similar with the
questions evaluated in TREC1. For example questions
</bodyText>
<footnote confidence="0.9926505">
1The Text REtrieval Conferences (TREC) are evaluation
workshops in which Information Retrieval tasks are annually
tested. Since 1999 the performance of question answering sys-
tems are measured in the TREC QA track.
</footnote>
<bodyText confidence="0.999183875">
Qi, Q2 and Q3 are so-called definition questions, since
they ask about defining properties of an object. However
unlike the TREC definition questions, these questions ex-
press unstated intentions of the questioner and need to be
processed in the context of the original complex question
Q1. Questions Q4 and Q5 are factoid questions, request-
ing information about facts or events. Qs asks about the
source of information that enables the answers of ques-
tion Q1.
Questions Qi, Q2, Q3, Q4 and Q5 result from the in-
tentional structure generated when processing question
Q1 or questions similar to it. When intended questions
are generated, their sequential processing (a) represents a
decomposition of the complex question and (b) generates
a scenario for finding information; thus questions like Q1
are also known as scenario questions.
</bodyText>
<sectionHeader confidence="0.846909" genericHeader="introduction">
Intentions and Implicatures.
</sectionHeader>
<bodyText confidence="0.999881333333333">
As Table 1 suggests, the implied information takes the
form of alternatives that guide the answers to intended
questions. For example, question Q�1
</bodyText>
<sectionHeader confidence="0.815872" genericHeader="method">
1 lists alternatives
</sectionHeader>
<bodyText confidence="0.994110736842105">
for the answer to Q1 whereas Q�2
1 lists components of
the answer of Qi. Implicatures may also involve tem-
poral inference, e.g. the implied questions pertaining
to Q3 and Q4. Additionally, the reliability of infor-
mation is commonly an implicature in the case of sce-
nario questions, since the causal and temporal inference
is based on the quality and correctness of the available
data sources. Neither intentions or implicatures are rec-
ognizable at syntactic or semantic level, but they both
play an important role in the question interpretation. In-
terpretations disregard the implied information or the user
intentions determine the extraction of incorrect answers,
thus influence the performance of Q/A systems.
Our solution.
In this paper we present two different mechanisms of
deriving the question implicatures. Both methods start
from the syntactic and semantic content of the interro-
gation. The first method considers only the semantic
</bodyText>
<table confidence="0.99922625">
Intended Questions Implied Questions
Qi1 :What is the USSR/Russia? Qm1
Q1 :Is this the Soviet/Russian government?
m1
2 :Does it include private firms, state-ownedfirms, educational
institutions, and individuals?
Qi2 :What is North Korea? Qm2
Q1 :Is this the North Korean government only?
m2
2 :Does it include private firms, state-ownedfirms, educational
institutions, and individuals?
Qi3 :What is assistance? Qm3
1 :Is it the transfer of complete missile systems, licensing agreements,
components, materials, or plans?
Qm3
2 :Is it the training ofpersonnel?
Qm3
3 :What kind of training?
Qm3
4 :Does transfer include data, and, if so, what kind of data?
Qm3
5 :Does transfer include financial assistance, and, ifso, what kind of
financial assistance?
Qi4 :What are the missiles in the North Qm4
Korean inventory? 1 :Are any based upon Soviet/Russian designs?
Qm4
Q2 :Ifso, which ones?
m4
QQ3 :What was the development timeline of the missiles?
m4
4 :Did any timeline differ significantly from others?
m4
5 :Did North Korea receive assistance from other sources besides
USSR/Russia to develop these missiles?
Qi5 :When did North Korea receive assistance Qm5 1:Was any intended assistance halted, stopped or intercepted?
from the USSR/Russia?
Qi6 :What are the sources of information? Qm6
1 :Are the sources reliable?
Qm6
2 :Is some information contradictory?
</table>
<tableCaption confidence="0.999936">
Table 1: Question decomposition associated with question Q1
</tableCaption>
<bodyText confidence="0.9999617">
meaning of the words used in the question whereas the
second method considers the predicate-argument struc-
ture of the question and candidate answers as a form of
shallow semantics that enables the inference of the inten-
tional structure. Question implicatures are derived from
lexico-semantic paths retrieved from the WordNet lexico-
semantic database. These paths bring forward new con-
cepts, that may be associated with the question implica-
tures when testing the paths against the conversational
maxims introduced by Grice in (Grice, 1975a). For ex-
ample, if the user asks “Will Prime Minister Mori survive
the crisis?”, the first method detects the user’s belief that
the position of the Prime Minister is in jeopardy, since
the concept DANGER is coerced although none of the
question words directly imply it.
The second method generates the intentional structure
of the question, enabling a more structured representa-
tion of the pragmatics of question interpretation. The in-
tentional structure is based on a study that we have con-
ducted for capturing the motivations of a group of users
when asking series of questions in several scenarios. We
show how the intentional structures that we have gathered
guide the coercion of knowledge that helps to support the
acceptance of rejection of computational implicatures.
The derivation of intentional structures is made possi-
ble by predicate-argument structures that are recognized
both at the question level and at the candidate answer
level. In this paper we show how richer semantic ob-
jects can be derived around predicate-argument structures
and how inferential mechanisms can be associated with
such semantic objects for obtaining correct answers. The
rest of the paper is organized as follows. In Section 2
we describe several forms of complex questions that re-
quire the derivation of computational implicatures. Sec-
tion 3 details the models of Question Answering that we
considered and Section 4 shows our methods of deriving
predicate-argument structures and their usage in identi-
fying answers for questions. Section 5 details the inten-
tional structures whereas Section 6 summarizes the con-
clusions.
</bodyText>
<sectionHeader confidence="0.889272" genericHeader="method">
2 Question Complexity
</sectionHeader>
<bodyText confidence="0.9997115625">
Since 1999, the TREC QA evaluations focused on fac-
toid questions, such as “In what year did Joe Di Maggio
compile his 56-game hitting streak?” or “Name a film in
which Jude Law acted.”. The answers to most of these
questions belong to semantic categories associated with
each question class. For example, questions asking about
a date or a year can be answered because Named Entity
Recognizers identify a temporal expression in a candidate
text span. Similarly, names of people or organizations
are provided as answers to questions such as “Who is the
first Russian astronaut?” or “What is the largest software
company in the world?”. Most Named Entity Recogniz-
ers detect names of PEOPLE, ORGANIZATIONS, LOCA-
TIONS, DATES, PRICES and NUMBERS. For factoid Q/A,
the list of name categories needs to be extended, as re-
ported in (Harabagiu et al., 2003) for recognizing many
</bodyText>
<figure confidence="0.785052764705882">
Intended Questions:
Definition Questions:
Complex Question: What kind od assistance has North Korea received from
Question PATTERN: What kind of assistance has X received from Y for Z?
X=North Korea Y=USSR/Russia FOCUS=Z=misile program
Elaboration of FOCUS:
Reliability: &amp;quot;What are the sources of information?&amp;quot;
the USSR/Russia for its missile program?
(1) RESULTATIVE
(2) TEMPORAL
What is X? &amp;quot;What is North Korea?&amp;quot;
What is Y? &amp;quot;What is USSR/Russia?&amp;quot;
What is assistance?
&amp;quot;What are the missiles in the
North Korean inventory?&amp;quot;
&amp;quot;When did North Korea receive
assistance from the USSR/Russia?&amp;quot;
</figure>
<figureCaption confidence="0.999997">
Figure 1: Decomposition of scenario question into intended questions
</figureCaption>
<bodyText confidence="0.999884485714286">
more types of names, e.g. names of movies, names of
diseases, names of battles. Moreover, the semantic cat-
egories of the extended set of names need to be incor-
porated into an answer type taxonomy that enables the
recognition of (a) the expected answer type and (b) the
question class. The taxonomy of expected answer types
is useful because the answer is not always a name; it can
be a lexicalized concept or a concept that is expressed by
a paraphrase.
The TREC evaluations have also considered two more
classes of questions: (1) list questions and (2) definition
questions. The list questions have answers that are typ-
ically assembled from different documents. Such ques-
tions are harder to answer than factoid questions be-
cause the systems must detect duplications. Example of
list questions are “Name singers performing the role of
Donna Elvira in performances of Mozart’s “Don Gio-
vani”.” or “What companies manufacture golf clubs?”.
Definition questions require a different form of process-
ing that factoid questions because no taxonomy of answer
types needs to be used. The expected answer type is a def-
inition, which cannot be represented by a single concept.
Q/A systems assume that definitions are given by follow-
ing a set of linguistic patterns that need to be matched for
extracting the answer. Example of definition questions
are “What is a golden parachute?” or “What is ETA in
Spain?”.
In (Echihabi and Marcu, 2003) a noisy channel model
for Q/A was introduced. This model is based on the
idea that if a given sentence SA contains an answer sub-
string A to a question Q, then SA can be re-written into
Q through a sequence of stochastic operators. Not only a
justification ofthe answer is produced, but the conditional
probability P(Q—SA) re-ranks all candidate answers.
A different viewpoint of Q/A was reported in (Itty-
cheriah et al., 2000). Finding the answers A to a ques-
tion Q was considered a classification problem that maxi-
mizes the conditional probability P(A—Q). This model is
not tractable currently, because (a) the search space is too
large for a text collection like the TREC or the AQUAINT
corpora; and (b) the training data is insufficient. There-
fore, Q/A is modeled by the distribution P(C—A,Q)
where C measures the “correctness” of A to question
Q. By using a hidden variable E that represents the ex-
pected answer type, P(C—A,Q) = ΣE p(C,E—Q,A) =
ΣE p(C—E,Q,A) * p(E—Q,A). Both distributions are
modeled by using the maximum entropy.
All three forms of questions are also useful when pro-
cessing complex questions, determined by a scenario re-
sulting from a problem-solving situation. As illustrated
in Figure 1, a scenario question may be associated with a
pattern. One of the pattern variables represents the focus
of the question. The notion of the question focus was first
introduced by (Lehnert, 1978). The focus represents the
most important concept of the question; a concept deter-
mining the domain of question. In the case of question
Q1, the focus is missile program. The identification of
the focus is based on the predicate-structure of the ques-
tion pattern and on the order of the arguments. Figure 3
shows both the question pattern associated with Q1 and
its predicate-argument structure. The argument with the
role of purpose is ranked highest, and thus it determines
the question focus.
With the exception of the focus, all arguments from
the predicate-argument structure may be used for gener-
ating definition questions. The focus is elaborated upon.
Several forms of elaborations are possible. One is a tem-
poral one, as illustrated in Figure 1. Other are resultative,
causative or manner-based. For example, the knowledge
that assistance in a missile program results in an inven-
</bodyText>
<figure confidence="0.960398">
Step 2(a): Binary Semantic Dependencies
Step 1: Syntactic Parse
Question: When did North Korea receive assistance from USSR/Russia?
Expected Answer Type
when=DATE
receive North Korea USSR/Russia assistance
WHADVP
DATE
When did North Korea receive assistance from USSR/Russia
WRB VBD NNP NNP VB NN IN NNP
SBARQ
BENEFICIARY OBJECT SOURCE
NPB
SQ
Step 2(b): Predicate−Argument Structures
Arguments: assistance=OBJECT
North Korea=BENEFICIARY
USSR/Russia=SOURCE
When=DATE=Expected Answer Type
Predicate: receive
NPB
VP
PP
NPB
</figure>
<figureCaption confidence="0.998604">
Figure 2: Deriving the Expected Answer Type
</figureCaption>
<construct confidence="0.9525155">
Question Pattern: What kind of assistance has X received
from Y for Z?
</construct>
<subsectionHeader confidence="0.547229">
Predicate−argument structure:
</subsectionHeader>
<bodyText confidence="0.99764025">
associated with the focus, i.e. the intended question Q4,
coerce the design and development predicates which are
associated with the missiles as well as the timelines of
possible additional assistance.
</bodyText>
<table confidence="0.732969">
Predicate: receive Object: assistance 3 Models of Question Answering
Arguments: Purpose: Z Source: Y
Beneficiary: X
</table>
<figureCaption confidence="0.997982">
Figure 3: Predicate-argument structure
</figureCaption>
<bodyText confidence="0.964945764705882">
tory of missiles allows for resultative elaboration. Further
knowledge needs to be coerced for generating the implied
questions as possible follow-ups to intended questions.
The relationship between intended questions and im-
plied questions is marked by the presence of multiple
references, e.g. the pronouns it and this or any and
ones. The generation of implied questions is made pos-
sible by knowledge that is coerced from the intended
questions. For example, when asking Qi :“What is
the USSR/Russia?” the coercion process abstracts away
from the concept that needs to be defined, i.e. a coun-
try. The implied question requests confirmation of
the metonymy resolution involving USSR/Russia.This
named entity may represent a country but most likely it
refers to itsgovernment or, as Q�1
2 suggests, organiza-
tions or individuals acting on behalf of the country. Both
</bodyText>
<equation confidence="0.560571">
Q�1
1 and Q�1
</equation>
<bodyText confidence="0.999901172413793">
2 , implied questions derived from the in-
tended question Qi, refer to the metonymy by using the
pronouns this and it respectively. Different forms of coer-
cion are used for Q3 because in this case the knowledge
is associated with the predicate. The implied questions
The processing of questions is typically performed as a
sequence of three processes: (1) Question Processing; (2)
Document Processing and (3) Answer Extraction. In the
case of factoid questions , question processing involves
the classification of questions with the purpose of pre-
dicting what semantic class the answer should belong
to. Thus we may have questions asking about PEOPLE,
ORGANIZATIONS, TIME or LOCATIONS. Since open-
domain Q/A systems process questions regardless of the
domain of interest, question processing must be based on
an extended ontology of answer types. The identification
of the expected answer type is based either on binary se-
mantic dependencies extracted from the syntactic parse of
the question (Harabagiu et al., 2001) or on the predicate-
argument structure of the question. In both cases, the re-
lation to the question stem (i.e. what, who, when) enables
the classification. Figure 2 illustrates a factoid question
generated as an intended question and the derivation of
its expected answer type.
However, many times the expected answer type needs
to be identified from an ontology that has high lexico-
semantic coverage. Many Q/A systems use the WordNet
database for this purpose. In contrast, definition ques-
tions do not require the identification of the expected an-
</bodyText>
<figure confidence="0.975201875">
Definition Question: What is ETA in Spain?
Question Parse:
WHNP
What is ETA in Spain
WP
SBARQ
VBZ
SQ
NP
NPB
NNP
IN
PP
NPB
NNP
Answer Pattern:
Answer:
Question Pattern:
has killed nearly 800 people since
taking up arms in 1968
ETA, a Basque language acronym
for Basque Homeland and Freedom −
Question−Point, a Definition
What is Question−Point in Country?
</figure>
<figureCaption confidence="0.999902">
Figure 4: Patterns for Processing Definition Questions
</figureCaption>
<bodyText confidence="0.999895397435898">
swer type, since they always request a definition. How-
ever, definition questions are matched against a set of pat-
terns, which enables the extraction of the definition from
the candidate answers. Figure 4 illustrates a definition
question, the pattern it matched as well as the extracted
answer.
Both factoid and definition questions can be answered
only if candidate passages are available. The retrieval of
these passages is made possible by keywords that are se-
lected from the question words. The Documents Process-
ing module implements a search engine that returns pas-
sages that are likely to contain the expected answer type
in the case of factoid questions or the definition pattern
in the case of definition questions. The answer extraction
module optimizes the extraction of the correct answer by
unifying the question information with the answer infor-
mation. The unification may be based on pattern match-
ing; on machine learning algorithms based on the ques-
tion and answer features or on abductive reasoning that
justifies the answer correctness.
Current state-of-the-art QA systems search for the can-
didate answer by assuming that the answers are single
concepts, that can be recognized from a hierarchy or by
a Named Entity Recognizer. This is a serious limitation,
but it works well for the factoid, list or definition ques-
tions evaluated in TREC.
The three modules of current Q/A systems reflect the
three functions that need to be considered by any Q/A
model: (1) understanding what the question asks; (2)
identify candidate text passage that might contain the an-
swer; and (3) the extraction of the correct answer. Cur-
rently, the expected answer type represents what ques-
tion asks about: a semantic concept, e.g. the name of a
person, location or organization, kinds of diseases, types
of animals or plants. Generally these semantic concepts
are lexicalized in a single word or in 2-word collocations.
Clearly, this represents a limitation, since often the ques-
tions ask for more than a single concept. As we have
seen in Table1, there is additional intended and implied
information that is requested. Therefore new models of
Question/Answering need to incorporate these additional
forms of knowledge.
When definition questions are processed in current
Q/A systems, they are matched against a pattern, which is
different from the question patterns associated with com-
plex questions similar to those illustrated in Figure 1. In
the case of a definition question like “What is ETA in
Spain?”, the pattern identifies the question-point (QP) as
ETA- the concept that needs to be defined and Spain as
its context. The definition question pattern also contains
several surface-form patterns that are matched in the can-
didate paragraphs. One such pattern is recognized in an
apposition, by [QP, a AP] where AP represents the an-
swer phrase. In the following passage:
“ETA, a Basque language acronym for Basque Homeland
and Freedom - has killed nearly 800 people since taking
up arms in 1968.”
the exact answer representing the definition is identified
in AP: Basque language acronym for Basque Homeland
and Freedom. The fact that Basque country is a region in
Spain allows a justification of the question context.
In this paper, by considering the intentional informa-
tion and the implied information that can be derived when
processing questions, we introduce a novel model of Q/A,
which has access to rich semantic structures and enables
the retrieval of more accurate answers as well as inference
processes that explain the validity and contextual cover-
age of answers.
Figure 5 shows the structure of the novel model of Q/A
we propose. Both Question Processing and Document
Processing have the recognition of predicate-argument
structures as a crux of their models. As reported in (Sur-
deanu et al., 2003), the recognition of predicate-argument
structures depends on features made available by full syn-
tactic parses and by Named Entity Recognizers. As we
shall show in this paper, the predicate-argument struc-
tures enable the recognition of question pattern, the ques-
tion focus and the intentional structure associated with
</bodyText>
<table confidence="0.97132009375">
Question
Intentional Structure
Identification of
Predicate−Argument
Structures
Syntactic Parse
Recognition of Answer Structure
Question Processing Document Processing Answer Processing
Keyword Extraction
Named Entity
Recognition
Recognition of
Question Pattern
Identification of
Question Focus
Syntactic Parse
Identification of
Predicate−Argument Structure
Indexing &amp; Retrieval
based on extended
lexico−semantic knowledge
Recognition of Answer
Structure
Named Entity
Recognition
Validation of Implied Information
Recognition and extention
of intentional structure
Recognition of
Answer Structure
Reference Resolution
Answer
</table>
<figureCaption confidence="0.995028">
Figure 5: Novel Question/Answering Architecture.
</figureCaption>
<bodyText confidence="0.999937384615385">
a question. When the intentions are known, the answer
structure can be identified and the keywords extracted.
For better retrieval of candidate answers, documents are
indexed and retrieved based on the predicate-argument
structures as well as on complex semantic structure asso-
ciated with different question patterns. Similarly, the in-
tentional structures are used for indexing/retrieving can-
didate passages. The Answer Processing function in-
volves the recognition of the answer structure and inten-
tional structure. Often this requires reference resolution.
The implied information coerced from both the question
and the candidate answer is also validated before decid-
ing on the answer correctness.
</bodyText>
<sectionHeader confidence="0.996893" genericHeader="method">
4 Predicate-Argument Structures
</sectionHeader>
<bodyText confidence="0.985102736842105">
To identify predicate-argument structures in questions
and passages, we have: (1) used the Proposition Bank or
PropBank as training data; and (2) a mode for predicting
argument roles similar to the one employed by (Gildea
and Jurafsky, 2002).
PropBank is a one million word corpus annotated with
predicate-argument structures on top of the Penn Tree-
bank 2 Wall Street Journal texts. For any given predicate,
the expected arguments are labeled sequentially from Arg
0 to Arg 4. Generally, Arg 0 stands for agent, Arg 1 for
direct object or theme or patient, Arg 2 for indirect object
or benefactive or instrument or attribute or end state, Arg
3 for start point or benefactive or attribute and Arg4 for
end point. In addition to these core arguments, adjunc-
tative arguments are marked up. They include functional
tags from Treebank, e.g. ArgM-DIR indicates a direc-
tional, ArgM-LOC indicates a locative, and ArgM-TMP
stands for a temporal.
An example of PropBank markup is:
</bodyText>
<figure confidence="0.2807504">
[Arg10 Analysts ] have been [predicate1 expecting ] [Arg1
1
a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 the
U.S. car maker] [Arg2 an eventual 30% state in the British
Company ]. 1
</figure>
<bodyText confidence="0.999757142857143">
The model of identifying the arguments of each pred-
icate consists of two tasks: (1) the recognition of the
boundaries of each argument in the syntactic parse tree;
(2) the identification of the argument role. Each task can
be cast as a separate classifier. Next section describes
our approach based on Support Vector Machines (SVM)
(Vapnik, 1995).
</bodyText>
<subsectionHeader confidence="0.990086">
4.1 Automatic Predicate-Argument extraction
</subsectionHeader>
<bodyText confidence="0.998739222222222">
Given a sentence in natural language, all the predicates
associated with its verbs have to be identified along with
their arguments. This problem can be divided in two sub-
tasks: (a) detection of the target argument boundaries,
i.e. all its compounding words, and (b) classification of
the argument type, e.g. Arg0 or ArgM.
A direct approach to learn both detection and classifi-
cation of predicate arguments is summarized by the fol-
lowing steps:
</bodyText>
<listItem confidence="0.997691636363636">
1. Given a sentence from the training-set, generate a
full syntactic parse-tree;
2. let P and A be the set of predicates and the set of
parse-tree nodes (i.e. the potential arguments), re-
spectively;
3. for each pair &lt;p, a&gt; E P x A:
• extract the feature representation set, Fp,a;
• if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T−
(negative examples).
</listItem>
<figure confidence="0.99948547826087">
0.8
0.77
0.74
F
0.71
0.68
0.65
1 2 3 4 5
Polynomial Degree
(a)
1 2 3 4 5
Polynomial Degree
(b)
i
0.83
0.77
0.74
0.71
0.68
0.8
Arg0
Arg1
ArgM
</figure>
<figureCaption confidence="0.999903">
Figure 6: Single classifiers and Multi-classifier performance for argument extraction.
</figureCaption>
<bodyText confidence="0.999986">
The above T+ and T− sets can be re-organized as pos-
itive T+arg, and negative T−arg, examples for each argu-
ment i. In this way, an individual ONE-vs-ALL SVM
classifier for each argument i can be trained. We adopted
this solution as it is simple and effective (Pradhan et al.,
2003). In the classification phase, given a sentence of the
test-set, all its Fp,a are generated and classified by each
individual SVM classifier. As a final decision, we select
the argument associated with the maximum value among
the scores provided by the SVMs2, i.e. argmaxi∈S Ci,
where S is the target set of arguments.
The discovering of relevant features is a complex task.
Nevertheless there is a common consensus on the basic
features that should be adopted. These standard features,
first proposed in (Gildea and Jurafsky, 2002), are derived
from parse trees as illustrated by Table 2.
</bodyText>
<subsectionHeader confidence="0.994909">
4.2 Parsing Sentence into Predicate Argument
Structures
</subsectionHeader>
<bodyText confidence="0.999575142857143">
For the experiments, we used PropBank
(www.cis.upenn.edu/∼ace) along with Penn-
TreeBank3 2 (www.cis.upenn.edu/∼treebank)
(Echihabi and Marcu, 2003). This corpus contains about
53,700 sentences and a fixed split between training and
testing which has been used in other researches (Gildea
and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et
al., 2003; Chen and Rambow, 2003; Gildea and Hock-
enmaier, 2003; Gildea and Palmer, 2002; Pradhan et al.,
2003). In this split, Sections from 02 to 21 are used for
training, section 23 for testing and sections 1 and 22 as
developing set. We considered all PropBank arguments
from Arg0 to Arg9, ArgA and ArgM even if only Arg0
from Arg4 and ArgM contain enough training/testing
</bodyText>
<footnote confidence="0.954758">
2This is a basic method to pass from binary categorization
into a multi-class categorization problem; several optimization
have been proposed, e.g. (Goh et al., 2001).
3We point out that we removed from the Penn TreeBank the
special tags of noun phrases like Subj and TMP as parsers usu-
ally are not able to provide this information.
</footnote>
<bodyText confidence="0.981667853658537">
data to affect the global performance.
The classifier evaluations were carried out using
the SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/ with the de-
fault polynomial kernel according to a degree d E
{1, 2, 3, 4, 5}. The performances were evaluated using
the F1 measure for both single argument classifiers and
the multi-class classifier.
- PHRASE TYPE (pt): This feature indicates the syntactic
type of the phrase labeled as a predicate argument.
- PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the argu-
ment phrase, expressed as a sequence of nonterminal labels
linked by direction (up or down).
- POSITION (pos) Indicates if the constituent appears be-
fore or after the predicate in the sentence.
- VOICE (voice) This feature distinguishes between active
or passive voice for the predicate phrase.
- HEAD WORD (hw) This feature contains the head word
of the evaluated phrase. Case and morphological informa-
tion are preserved.
- GOVERNING CATEGORY (gov) This feature applies to
noun phrases only, and it indicates if the NP is dominated by
a sentence phrase (typical for subject arguments with active
voice predicates), or by a verb phrase (typical for object
arguments).
- PREDICATE WORD In our implementation this feature
consists of two components: (1) VERB: the word itself
with the case and morphological information preserved; and
(2) LEMMA which represents the verb normalized to lower
case and infinitive form.
Table 2: Standard Features used in Predicate Argument
Extraction.
Figure 6 illustrates the F1 measures for the overall ar-
gument extraction task (i.e. identification and classifica-
tion) according to different polynomial degrees. Figure
6(a) illustrates the F1-performance of single classifiers
for the arguments Arg0, Arg1 and ArgM. Figure 6(b) il-
lustrates the performance for all the arguments (i.e. the
multi-classifier). In general, we were able to recognize
predicate argument structures with an Fl-score of 80%.
</bodyText>
<subsectionHeader confidence="0.9991805">
4.3 Using Predicate-Argument Structures in
Question Answering.
</subsectionHeader>
<bodyText confidence="0.99995071875">
Predicate-argument structures are useful for identifying
candidate answers. Since they recognize long-distance
dependencies between a predicate and one its arguments,
they enable (1) the identification of the exact boundaries
of an answer; and (2) they unify the predicate-argument
relation sought by question with those recognized in can-
didate passages.
Moreover, they are very useful in situations when the
expected answer type of the question could not be recog-
nized. There are two causes when the expected answer
type cannot be identified:
Case1: the answer class is a name that cannot be correctly
classified by an available Named Entity Recognizer, be-
cause its class name is not encoded.
Case2: the answer class cannot be found in the Answer
Type hierarchy. The example from Figure 7 shows an in-
stance of case 1. In this figure, the TREC question Q2054
has a predicate that can be unified with PREDICATES
from the answer passage. The Arg1 of the predicate is
the expected answer, which is identified as ”the Declara-
tion of Independence”. The Arg0 in the question is But-
ton Gwinnett, whereas in the answer, it is underspecified,
and should be resolved to who. This relative pronoun has
Button Gwinnett as one of its antecedents.
In Figure 8 the second case is illustrated. The question
asked about the first argument of the predicate ”measure”,
when its Arg2 = ”a theodolite”. In the answer, Predicate
2, with its infinite form, has as Arg 2 the same ”theodo-
lite”. However, the predicates are lexicalized by different
verbs. In WordNet, the first sense of the verb ”measure”
as the verb ”determine” as a hypernym, therefore Arg1 =
”wind speeds” is the correct answer.
</bodyText>
<sectionHeader confidence="0.992835" genericHeader="method">
5 Intentional Structures
</sectionHeader>
<bodyText confidence="0.9994486875">
The correct interpretation of many questions requires
the inference of implicit information, that is not directly
stated in the question, but merely implied. The mecha-
nisms of recognizing the intentions of the questioner are
helpful means of identifying the implied information. For
example, in the question QI:“Will Prime Minister Mori
survive the crisis?”, the user does not literally mean “Will
Prime Minister Mori be still alive when the political cri-
sis is over ?”, but rather (s)he implies her/his belief that
the current political crisis might cost the Japanese Prime
Minister his job. It is very unlikely that any expert knowl-
edge base covering Japanese politics will encode knowl-
edge covering all situations of political crisis and the pos-
sible outcomes of the prime minister. However, this prag-
matic knowledge is essential for the correct interpretation
of the question.
</bodyText>
<figure confidence="0.93292905">
Q2054: What document did Button Gwinnett sign on the upper left
hand side?
Predicate−argument structure:
ARG1: What document Question Type
PREDICATE: sign
ARG0: Button Gwinnett
ARGM−LOC: on the upper left hand side
Answer: Button Gwinnett, George Walton and Lyman Hall were
Georgians who could have been hanged as traitors for
signing the Declaration of Independence on July 4, 1776.
Predicate−argument structure:
ARG0: Button Gwinnett, George Walton and Lyman Hall
PREDICATE1: were
ARG1(PREDICATE1): Georgians
who
PREDICATE2: could have been hanged
ARG2(PREDICATE2): as traitors
PREDICATE3: signing
ARG1(PREDICATE3): The Declaration of Independence
ARGM−LOC(PREDICATE3): on July 4, 1776
</figure>
<figureCaption confidence="0.972675">
Figure 7: Answer extraction from predicate-argument struc-
tures: Case1
</figureCaption>
<table confidence="0.996908875">
Q2145: What does a theodolite measure?
Predicate−argument structure:
ARG1: What
ARG2: a theodolite
PREDICATE: measure
Answer: The theodolite − a 1940s gadget, no longer in production,
that uses a helium balloon and trigonometry to determine
wind speeds.
Predicate−argument structure:
ARG1(PREDICATE1): The theodolite
a 1940s gadget
that
PREDICATE1: uses
ARG2(PREDICATE1): a helium balloon and trigonometry
PREDICATE2: to determine
ARG1(PREDICATE2): wind speeds
</table>
<figureCaption confidence="0.996328">
Figure 8: Answer extraction from predicate-argument struc-
tures: Case 2
</figureCaption>
<bodyText confidence="0.9998256">
The design of advanced Question&amp;Answering systems
capable of grasping the intention of a professional analyst
when (s)he poses a question depends both on the knowl-
edge of the domain referred by the question as well as on
a variety of rules and conventions that allow the commu-
nication of intentions and beliefs in addition to the literary
meaning of the question. Access to domain knowledge is
granted by a combination of retrieval mechanisms that
bring forward relevant document passages from unstruc-
tured collections of documents, specialized knowledge
</bodyText>
<figure confidence="0.632388">
Question: Will Prime Minister Mori survive the political crisis ?
DANGER ( Prime Minister Mori continues in Position)
</figure>
<figureCaption confidence="0.991526">
Figure 9: Intentional Structure derived from Lexico-Semantic Knowledge.
</figureCaption>
<figure confidence="0.988490894736842">
survive crisis
continue in existence
adversity DANGER
WordNet 1.6
QUERY: Prime &amp; Minister &amp; Mori &amp;DANGER (word)
&amp;quot;vote of non-confidence against
Prime Minister Mori&amp;quot;
Text Information Retrieval
Engine
Text Retrieval
vote of non-confidence =
DANGER(Position)
Japanese Factual Politics
Knowledge Base
resignation
removal
strike
vote
political crisis
</figure>
<bodyText confidence="0.9983978">
bases and/or database access mechanisms. The research
proposed in this project focuses on the derivation and us-
age of pragmatic knowledge that supports the recognition
of question implications, also known as implicatures (cf.
(Grice, 1975b)).
</bodyText>
<subsectionHeader confidence="0.983032">
5.1 Intentional structures Derived from
Lexico-Semantic Knowledge
</subsectionHeader>
<bodyText confidence="0.999990217391304">
The novel idea of this research is to link computa-
tional implicatures, similar to those defined by Grice
(Grice, 1975b), to inferences that can be drawn from
general lexico-semantic knowledge bases such as Word-
Net of FrameNet. Incipient work was described in
(Sanda Harabagiu and Yukawa, 1996), where a method
of using lexico-semantic path for recognizing textual im-
plicatures was presented. To our knowledge, this is the
only computational model of implicatures that was de-
veloped and tested on a large lexico-semantic knowledge
base (e.g. WordNet), enabling successful recognition of
implicatures.
The model proposed in (Sanda Harabagiu and Yukawa,
1996) uncovered a relationship between (a) the coherence
of a text segment; (b) its cohesion expressed by the lexical
paths and (c) the implicatures that can be drawn, mostly
to account for pragmatic knowledge. This relationship
can be extended across documents and across topics, to
learn patterns of textual and Q&amp;A implicatures and the
methods of deriving knowledge that enables their recog-
nition.
The derivation of pragmatic knowledge combines in-
formation from three different sources:
</bodyText>
<listItem confidence="0.990828">
(1) lexical knowledge bases (e.g. WordNet),
(2) expert knowledge bases that can be rapidly formatted
for many domains (e.g. Japanese political knowledge);
and
(3) knowledge supported from the textual information
available from documents. The methodology of combin-
ing these three sources of information is novel.
</listItem>
<bodyText confidence="0.999963243243243">
For question QI, the starting point is the concept iden-
tified as a cue for the expected answer type through meth-
ods described in (Harabagiu et al., 2000). This con-
cept is lexicalized by the verb-object pair survive-crisis.
Verb survive has four distinct senses in the WordNet 1.6
database, whereas noun crisis has two senses. The poly-
semy of the expected answer type increases the difficulty
of the derivation of pragmatic knowledge, but it does not
presupposes the word sense disambiguation of the ex-
pression. The information available in the glosses defin-
ing the WordNet synsets provides helpful information for
expanding the multi-word term defining the expected an-
swer type. By measuring the similarity between the two
senses of the noun crisis and the words encountered as
objects or prepositional attachments in the glosses of the
various senses of the verb survive, we distinguish the
noun adversity and the example cancer as expressing the
closest semantic orientation to the first sense of noun cri-
sis. The similarity is measured by counting the number
of common hypernyms and gloss concepts of hypernyms
of two synsets. Figure 9 illustrates the concepts related
to the question QI, as derived from WordNet lexico-
semantic knowledge base.
The fact that surviving a political crisis has a dangerous
component, indicated by the noun adversity, may also be
supported by inferences drawn from an expert knowledge
base, showing that a political crisis may be dangerous for
political figures in power. However, at this point, the ob-
ject of the dangerous situation is not specified. But sev-
eral concepts indicating dangerous political situations can
be inferred from the expert knowledge base and used in
the query for text evidence. Only when text passages in-
volving Prime Minister Mori are retrieved, clarifications
of the situation are brought to attention: a vote of non-
confidence against the prime minister is considered. This
new information helps inference from the expert knowl-
edge base. The expert knowledge base modeling the
</bodyText>
<figure confidence="0.998437976744186">
Question: Does Iraq have biological weapons ?
Predicate-
Argument
Structure
Intentional Structure of Questions
0* Evidence ( 1-possess ( 2-Iraq, 3-biological weapons ))
4* Means of finding (0)
a. reports
b. inspections
c. assessments ❑ patterns of inspections
5* Source (0)
a. authority
b. reliability ❑ may, would
6* Consequence (0)
a. Enablement
b. Hiding/Presenting finding evidence
a. discover(1,2,3)
b. stockpile(2,3)
c. use(2,3)
d. 1-possess
coercion
x y
: have( Iraq, biological weapons )
Question
pattern
a. develop(2,3)
b. acquire(2,3)
Source/ fact/ reliability
reporter evidence
5.a 0 5.b
Structure
Does x have y ?
possess (x, y)
a. inspections( _,2,3)
b. ban( _,2,3)
biological weapons
a. Types of topic
b. Components
- chemical agents
- mustard gas, VX, sarin
c. Usage
- rockets, artillery shells
Topic (3)
</figure>
<figureCaption confidence="0.999031">
Figure 10: Intentional structure derived from predicate-argument structures.
</figureCaption>
<bodyText confidence="0.999971">
Japanese factional politics confirms that this is a danger-
ous situation for the Prime Minister and that in fact his
position is in jeopardy. Due to this inference from the
expert knowledge base, the concept POSITION replaces
noun existence from the gloss of the second sense of verb
survive, and the pragmatic knowledge required for the in-
terpretation of the implicature is assembled:
The interactions between the three information sources
derives the pragmatic knowledge on which relies the im-
plication of the question. The user had an inherent belief
that Prime Minister Mori might be replaced, and (s)he
queries the Q&amp;A system not only to find information but
also to find support for his/her belief. The intentional
structure is represented as a set of concepts and the re-
lations that span them, as illustrated in Figure 9.
</bodyText>
<subsectionHeader confidence="0.999754">
5.2 Coercion of Intentions
</subsectionHeader>
<bodyText confidence="0.998934016129032">
A second method of deriving the intentional structure of a
question is based on the predicate-argument structure that
is derived from the question and the candidate answers.
Figure 10 illustrates the Intentional Structure of one
such question. The structure of the intentions is deter-
mined by the predicate-argument structure of the ques-
tion and by its pattern. Generally, when asking whether
X posses Y, we want to find (1) evidence of this fact;
(2) we explore different means of finding the informa-
tion; (3) we are interested in the source of information
and (4) the enablers or inhibitors of finding the informa-
tion as well as the consequences of knowing it are of in-
terest. We assign a different index to each object from
the predicate-argument structure, and do the same for
each element of the intentional structure. For instance,
in Figure 2, source(0) is interpreted as source(index=0)
= source(evidence). Another feature of the intentional
structure is determined by the coercions that are associ-
ated with both forms of indexed objects. For example,
the coercion of evidence shows the most typical ways
of finding evidence in the context of the topic of the
question. Figure 2 lists such possibilities as (a) discov-
ering, (b) stockpiling, (c) using and even (d) possess-
ing. These possibilities are inserted in the context of the
topic, since they make use of the indexes for associat-
ing meaning to their representations. In fact, option (a)
discover(1,2,3) reads as discover(index=1, index=2, in-
dex=3) =discover(possesses(Iraq, biological weapons)).
Whereas option (b) stockpile(2,3) can be similarly inter-
preted as stockpile(Iraq, biological weapons). Note that
one of the indexed objects is the topic. The structure of
the topic is define along three semantic dimensions: (1)
hyponyms or examples of other types of the same cate-
gory as the topic; (2) the meronyms or components; and
(3) the functionality or the usage. The derivation of such
a large set of intentional structures helped us learn how
to coerce pragmatic knowledge. We have developed a
probabilistic approach extending the metonymy work of
(Lapata and Lascarides, 2003).
Lapata and Lascarides report a model of interpretation
of verbal metonymy as the point distribution P(e, o, v) of
three variables: the metonymy verb v, its object, and the
sought after interpretation i. For example a verb → ob-
ject relation that needs to be metonymycally interpreted,
is enjoy → movie. In this case v = enjoy, o = movie
and i E {making, watching, directing}. The variables
of the distribution re ordered as &lt;i, v, o&gt; to help factor-
ing P(i, v, o) = P(i) · P(v|i) · P(o|i, v). Each of the
probabilities P(i), P(v|i) and P(o|i, v) can be estimated
using maximum likelihood. As it is illustrated in Fig-
ure 10, we have extended this model to account for: (1)
coercion of topic information; (2) coercion of evidence
of a fact; (3) interpretation of predicate and (4) inter-
pretation of arguments. Since the verb → object rela-
tion translates in one of the predicate-argument relations,
we have coerced the predicate interpretations in the same
way as (Lapata and Lascarides, 2003), but we allowed
for any predicate-argument relation. Argument coercions
were produced by searching the most likely predicates
that used the same arguments. The topic model also in-
corporated topic signatures, similar to these reported in
(E.H. Hovy and Ravichandran, 2002).
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999835">
In this paper we have described the problem of interpret-
ing the question intentions and proposed two methods of
generating the intentional structure of questions. The first
method is based on lexico-semantic chains between con-
cepts that are related to the question. The second method
generates intentional structures by using the predicate-
argument structures of questions and the topic represen-
tation of questions. To derive both forms of intentional
structures, we have relied on information available from
WordNet and on the parsing of questions and answers
in predicate-argument structures. Our experiments show
that the intentional structure may determine a different in-
terpretation of the question, and thus different keywords
can be used to retrieve the answers. Answer extraction
also depends on the semantic relations between the co-
erced interpretations of predicates and arguments. By
selecting a set of 100 questions for test, we have eval-
uated the correctness of the extracted answers when (1)
no intentional knowledge was coerced; (2) implicatures
were derived from lexico-semantic knowledge and (3)
intentional structures were derived based on predicate-
argument structures. An increase of 8structures and one
of 22the impact of each element of the intentional struc-
ture on the Q/A processing.
</bodyText>
<sectionHeader confidence="0.999241" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999505744186046">
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In Pro-
ceedings of the 41st Annual Meeting of the ACL, Sap-
poro, Japan.
Chin-Yew Lin E.H. Hovy, U. Hermjakob and Deepak
Ravichandran. 2002. Using knowledge to facilitate
pinpointing of factoid answers. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING 2002).
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):254–288.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
King-Shy Goh, Edward Chang, and Kwang-Ting Cheng.
2001. SVM binary classifier ensembles for image clas-
sification. In Proceedings of the tenth international
conference on Information and knowledge manage-
ment, pages 395–402.
Paul H. Grice. 1975a. Logic and conversation. In P. Cole
and New York J.L. Morgan (ed.), Academic Press, edi-
tors, Syntax and Sematics Vol.3:Speech Acts, pages 41–
58.
Paul J. Grice. 1975b. Syntax and Semantics Vol.3:Speech
Acts. P. Cole and J. Morgan, editors.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing
using support vector machines. Technical report.
Sanda Harabagiu, Marius Pas¸ca, and Steven Maiorano.
2000. Experiments with open-domain textual question
answering. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING-
2000), pages 292–298, Saarbrucken, Germany,.
Sanda M. Harabagiu, Dan I. Moldovan, Marius Pasca,
Rada Mihalcea, Mihai Surdeanu, Razvan C. Bunescu,
Roxana Girju, Vasile Rus, and Paul Morarescu. 2001.
The role of lexico-semantic feedback in open-domain
textual question-answering. In Meeting of the ACL,
pages 274–281.
S. Harabagiu, D. Moldovan, C. Clark, M. Bodwen,
J. Williams, and J. Bensley. 2003. Answer mining by
combining extraction techniques with abductive rea-
soning. In Notebook of the Twelveth Text REtrieval
Converence (TREC-2003), pages 46–53.
A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi.
2000. IBM’s statistical question answering system.
In Proceedings of the 9th Text REtrieval Conference,
Gaithersburg, MD.
T. Joachims. 1999. T. Joachims, Making large-Scale
SVM Learning Practical. In B. Sch¨olkopf and C.
Burges and A. Smola (ed.), MIT-Press., editor, Ad-
vances in Kernel Methods - Support Vector Learning.
Maria Lapata and Alex Lascarides. 2003. A probabilistic
account of logical metonymy. Computational Linguis-
tics, 29:2:263–317.
Wendy Lehnert. 1978. The process of question answer-
ing. In Lawrence Erlbaum Assoc., Hillsdale.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured
text. In Proceedings of the International Conference
on Data Mining (ICDM-2003).
Dan Moldovan Sanda Harabagiu and Takashi Yukawa.
1996. Testing gricean constraints on a wordnet-based
coherence evaluation system. In Working Notes of the
AAAI-96 Spring Symposium on Computational Impli-
cature, Stanford, CA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of the
41th Annual Conference of the Association for Compu-
tational Linguistics (ACL-03), pages 8–15.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.932922">
<title confidence="0.999223">Intentions, Implicatures and Processing of Complex Questions</title>
<author confidence="0.981261">M Harabagiu J Maiorano Moschitti A</author>
<affiliation confidence="0.999074">University of Texas at Human Language Technology Research</affiliation>
<address confidence="0.996393">Richardson, TX 75083-0688, USA</address>
<abstract confidence="0.996592461538462">In this paper we introduce two methods for deriving the intentional structure of complex questions. Techniques that enable the derivation of implied information are also presented. We show that both the intentional structure and the implicatures enabled by it are essential components of Q/A systems capable of successfully processing complex questions. The results of our evaluation support the claim that there are multiple interactions between the process of answer finding and the coercion of intentions and implicatures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Owen Rambow</author>
</authors>
<title>Use of deep linguistic features for the recognition and labeling of semantic arguments.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="26068" citStr="Chen and Rambow, 2003" startWordPosition="4108" endWordPosition="4111">common consensus on the basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3We point out that we removed from the Penn TreeBank the special tags of noun phrases li</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>John Chen and Owen Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="10401" citStr="Echihabi and Marcu, 2003" startWordPosition="1627" endWordPosition="1630">e singers performing the role of Donna Elvira in performances of Mozart’s “Don Giovani”.” or “What companies manufacture golf clubs?”. Definition questions require a different form of processing that factoid questions because no taxonomy of answer types needs to be used. The expected answer type is a definition, which cannot be represented by a single concept. Q/A systems assume that definitions are given by following a set of linguistic patterns that need to be matched for extracting the answer. Example of definition questions are “What is a golden parachute?” or “What is ETA in Spain?”. In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. This model is based on the idea that if a given sentence SA contains an answer substring A to a question Q, then SA can be re-written into Q through a sequence of stochastic operators. Not only a justification ofthe answer is produced, but the conditional probability P(Q—SA) re-ranks all candidate answers. A different viewpoint of Q/A was reported in (Ittycheriah et al., 2000). Finding the answers A to a question Q was considered a classification problem that maximizes the conditional probability P(A—Q). This model is not tractable currently, beca</context>
<context position="25840" citStr="Echihabi and Marcu, 2003" startWordPosition="4071" endWordPosition="4074">elect the argument associated with the maximum value among the scores provided by the SVMs2, i.e. argmaxi∈S Ci, where S is the target set of arguments. The discovering of relevant features is a complex task. Nevertheless there is a common consensus on the basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2This is a basic method to p</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Abdessamad Echihabi and Daniel Marcu. 2003. A noisy-channel approach to question answering. In Proceedings of the 41st Annual Meeting of the ACL, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin E H Hovy</author>
<author>U Hermjakob</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Using knowledge to facilitate pinpointing of factoid answers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<marker>Hovy, Hermjakob, Ravichandran, 2002</marker>
<rawString>Chin-Yew Lin E.H. Hovy, U. Hermjakob and Deepak Ravichandran. 2002. Using knowledge to facilitate pinpointing of factoid answers. In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Identifying semantic roles using combinatory categorial grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="26098" citStr="Gildea and Hockenmaier, 2003" startWordPosition="4112" endWordPosition="4116"> basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3We point out that we removed from the Penn TreeBank the special tags of noun phrases like Subj and TMP as parsers usu</context>
</contexts>
<marker>Gildea, Hockenmaier, 2003</marker>
<rawString>Daniel Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using combinatory categorial grammar. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistic,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="22344" citStr="Gildea and Jurafsky, 2002" startWordPosition="3486" endWordPosition="3489">tructures are used for indexing/retrieving candidate passages. The Answer Processing function involves the recognition of the answer structure and intentional structure. Often this requires reference resolution. The implied information coerced from both the question and the candidate answer is also validated before deciding on the answer correctness. 4 Predicate-Argument Structures To identify predicate-argument structures in questions and passages, we have: (1) used the Proposition Bank or PropBank as training data; and (2) a mode for predicting argument roles similar to the one employed by (Gildea and Jurafsky, 2002). PropBank is a one million word corpus annotated with predicate-argument structures on top of the Penn Treebank 2 Wall Street Journal texts. For any given predicate, the expected arguments are labeled sequentially from Arg 0 to Arg 4. Generally, Arg 0 stands for agent, Arg 1 for direct object or theme or patient, Arg 2 for indirect object or benefactive or instrument or attribute or end state, Arg 3 for start point or benefactive or attribute and Arg4 for end point. In addition to these core arguments, adjunctative arguments are marked up. They include functional tags from Treebank, e.g. ArgM</context>
<context position="25580" citStr="Gildea and Jurafsky, 2002" startWordPosition="4037" endWordPosition="4040"> can be trained. We adopted this solution as it is simple and effective (Pradhan et al., 2003). In the classification phase, given a sentence of the test-set, all its Fp,a are generated and classified by each individual SVM classifier. As a final decision, we select the argument associated with the maximum value among the scores provided by the SVMs2, i.e. argmaxi∈S Ci, where S is the target set of arguments. The discovering of relevant features is a complex task. Nevertheless there is a common consensus on the basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistic, 28(3):254–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="26123" citStr="Gildea and Palmer, 2002" startWordPosition="4117" endWordPosition="4120"> adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3We point out that we removed from the Penn TreeBank the special tags of noun phrases like Subj and TMP as parsers usually are not able to prov</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>King-Shy Goh</author>
<author>Edward Chang</author>
<author>Kwang-Ting Cheng</author>
</authors>
<title>SVM binary classifier ensembles for image classification.</title>
<date>2001</date>
<booktitle>In Proceedings of the tenth international conference on Information and knowledge management,</booktitle>
<pages>395--402</pages>
<contexts>
<context position="26578" citStr="Goh et al., 2001" startWordPosition="4193" endWordPosition="4196"> researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3We point out that we removed from the Penn TreeBank the special tags of noun phrases like Subj and TMP as parsers usually are not able to provide this information. data to affect the global performance. The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at http://svmlight.joachims.org/ with the default polynomial kernel according to a degree d E {1, 2, 3, 4, 5}. The performances were evaluated using the F1 measure for both single argument classifiers and the multi-class classifier. - PHRASE TYPE (pt): This feature indicates the syntactic type</context>
</contexts>
<marker>Goh, Chang, Cheng, 2001</marker>
<rawString>King-Shy Goh, Edward Chang, and Kwang-Ting Cheng. 2001. SVM binary classifier ensembles for image classification. In Proceedings of the tenth international conference on Information and knowledge management, pages 395–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul H Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Sematics Vol.3:Speech Acts,</booktitle>
<pages>41--58</pages>
<editor>In P. Cole and New York J.L. Morgan (ed.),</editor>
<publisher>Academic Press,</publisher>
<contexts>
<context position="5811" citStr="Grice, 1975" startWordPosition="893" endWordPosition="894">contradictory? Table 1: Question decomposition associated with question Q1 meaning of the words used in the question whereas the second method considers the predicate-argument structure of the question and candidate answers as a form of shallow semantics that enables the inference of the intentional structure. Question implicatures are derived from lexico-semantic paths retrieved from the WordNet lexicosemantic database. These paths bring forward new concepts, that may be associated with the question implicatures when testing the paths against the conversational maxims introduced by Grice in (Grice, 1975a). For example, if the user asks “Will Prime Minister Mori survive the crisis?”, the first method detects the user’s belief that the position of the Prime Minister is in jeopardy, since the concept DANGER is coerced although none of the question words directly imply it. The second method generates the intentional structure of the question, enabling a more structured representation of the pragmatics of question interpretation. The intentional structure is based on a study that we have conducted for capturing the motivations of a group of users when asking series of questions in several scenari</context>
<context position="34034" citStr="Grice, 1975" startWordPosition="5334" endWordPosition="5335">Lexico-Semantic Knowledge. survive crisis continue in existence adversity DANGER WordNet 1.6 QUERY: Prime &amp; Minister &amp; Mori &amp;DANGER (word) &amp;quot;vote of non-confidence against Prime Minister Mori&amp;quot; Text Information Retrieval Engine Text Retrieval vote of non-confidence = DANGER(Position) Japanese Factual Politics Knowledge Base resignation removal strike vote political crisis bases and/or database access mechanisms. The research proposed in this project focuses on the derivation and usage of pragmatic knowledge that supports the recognition of question implications, also known as implicatures (cf. (Grice, 1975b)). 5.1 Intentional structures Derived from Lexico-Semantic Knowledge The novel idea of this research is to link computational implicatures, similar to those defined by Grice (Grice, 1975b), to inferences that can be drawn from general lexico-semantic knowledge bases such as WordNet of FrameNet. Incipient work was described in (Sanda Harabagiu and Yukawa, 1996), where a method of using lexico-semantic path for recognizing textual implicatures was presented. To our knowledge, this is the only computational model of implicatures that was developed and tested on a large lexico-semantic knowledge</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Paul H. Grice. 1975a. Logic and conversation. In P. Cole and New York J.L. Morgan (ed.), Academic Press, editors, Syntax and Sematics Vol.3:Speech Acts, pages 41– 58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul J Grice</author>
</authors>
<title>Syntax and Semantics Vol.3:Speech Acts.</title>
<date>1975</date>
<editor>P. Cole and J. Morgan, editors.</editor>
<contexts>
<context position="5811" citStr="Grice, 1975" startWordPosition="893" endWordPosition="894">contradictory? Table 1: Question decomposition associated with question Q1 meaning of the words used in the question whereas the second method considers the predicate-argument structure of the question and candidate answers as a form of shallow semantics that enables the inference of the intentional structure. Question implicatures are derived from lexico-semantic paths retrieved from the WordNet lexicosemantic database. These paths bring forward new concepts, that may be associated with the question implicatures when testing the paths against the conversational maxims introduced by Grice in (Grice, 1975a). For example, if the user asks “Will Prime Minister Mori survive the crisis?”, the first method detects the user’s belief that the position of the Prime Minister is in jeopardy, since the concept DANGER is coerced although none of the question words directly imply it. The second method generates the intentional structure of the question, enabling a more structured representation of the pragmatics of question interpretation. The intentional structure is based on a study that we have conducted for capturing the motivations of a group of users when asking series of questions in several scenari</context>
<context position="34034" citStr="Grice, 1975" startWordPosition="5334" endWordPosition="5335">Lexico-Semantic Knowledge. survive crisis continue in existence adversity DANGER WordNet 1.6 QUERY: Prime &amp; Minister &amp; Mori &amp;DANGER (word) &amp;quot;vote of non-confidence against Prime Minister Mori&amp;quot; Text Information Retrieval Engine Text Retrieval vote of non-confidence = DANGER(Position) Japanese Factual Politics Knowledge Base resignation removal strike vote political crisis bases and/or database access mechanisms. The research proposed in this project focuses on the derivation and usage of pragmatic knowledge that supports the recognition of question implications, also known as implicatures (cf. (Grice, 1975b)). 5.1 Intentional structures Derived from Lexico-Semantic Knowledge The novel idea of this research is to link computational implicatures, similar to those defined by Grice (Grice, 1975b), to inferences that can be drawn from general lexico-semantic knowledge bases such as WordNet of FrameNet. Incipient work was described in (Sanda Harabagiu and Yukawa, 1996), where a method of using lexico-semantic path for recognizing textual implicatures was presented. To our knowledge, this is the only computational model of implicatures that was developed and tested on a large lexico-semantic knowledge</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Paul J. Grice. 1975b. Syntax and Semantics Vol.3:Speech Acts. P. Cole and J. Morgan, editors.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Jim Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2003</date>
<tech>Technical report.</tech>
<contexts>
<context position="26045" citStr="Hacioglu et al., 2003" startWordPosition="4104" endWordPosition="4107">evertheless there is a common consensus on the basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3We point out that we removed from the Penn TreeBank the special </context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Martin, and Dan Jurafsky. 2003. Shallow semantic parsing using support vector machines. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Marius Pas¸ca</author>
<author>Steven Maiorano</author>
</authors>
<title>Experiments with open-domain textual question answering.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING2000),</booktitle>
<pages>292--298</pages>
<location>Saarbrucken, Germany,.</location>
<marker>Harabagiu, Pas¸ca, Maiorano, 2000</marker>
<rawString>Sanda Harabagiu, Marius Pas¸ca, and Steven Maiorano. 2000. Experiments with open-domain textual question answering. In Proceedings of the 18th International Conference on Computational Linguistics (COLING2000), pages 292–298, Saarbrucken, Germany,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Dan I Moldovan</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>Razvan C Bunescu</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
<author>Paul Morarescu</author>
</authors>
<title>The role of lexico-semantic feedback in open-domain textual question-answering.</title>
<date>2001</date>
<booktitle>In Meeting of the ACL,</booktitle>
<pages>274--281</pages>
<contexts>
<context position="15612" citStr="Harabagiu et al., 2001" startWordPosition="2449" endWordPosition="2452">cessing and (3) Answer Extraction. In the case of factoid questions , question processing involves the classification of questions with the purpose of predicting what semantic class the answer should belong to. Thus we may have questions asking about PEOPLE, ORGANIZATIONS, TIME or LOCATIONS. Since opendomain Q/A systems process questions regardless of the domain of interest, question processing must be based on an extended ontology of answer types. The identification of the expected answer type is based either on binary semantic dependencies extracted from the syntactic parse of the question (Harabagiu et al., 2001) or on the predicateargument structure of the question. In both cases, the relation to the question stem (i.e. what, who, when) enables the classification. Figure 2 illustrates a factoid question generated as an intended question and the derivation of its expected answer type. However, many times the expected answer type needs to be identified from an ontology that has high lexicosemantic coverage. Many Q/A systems use the WordNet database for this purpose. In contrast, definition questions do not require the identification of the expected anDefinition Question: What is ETA in Spain? Question </context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Morarescu, 2001</marker>
<rawString>Sanda M. Harabagiu, Dan I. Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan C. Bunescu, Roxana Girju, Vasile Rus, and Paul Morarescu. 2001. The role of lexico-semantic feedback in open-domain textual question-answering. In Meeting of the ACL, pages 274–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bodwen</author>
<author>J Williams</author>
<author>J Bensley</author>
</authors>
<title>Answer mining by combining extraction techniques with abductive reasoning.</title>
<date>2003</date>
<booktitle>In Notebook of the Twelveth Text REtrieval Converence (TREC-2003),</booktitle>
<pages>46--53</pages>
<contexts>
<context position="8296" citStr="Harabagiu et al., 2003" startWordPosition="1290" endWordPosition="1293">belong to semantic categories associated with each question class. For example, questions asking about a date or a year can be answered because Named Entity Recognizers identify a temporal expression in a candidate text span. Similarly, names of people or organizations are provided as answers to questions such as “Who is the first Russian astronaut?” or “What is the largest software company in the world?”. Most Named Entity Recognizers detect names of PEOPLE, ORGANIZATIONS, LOCATIONS, DATES, PRICES and NUMBERS. For factoid Q/A, the list of name categories needs to be extended, as reported in (Harabagiu et al., 2003) for recognizing many Intended Questions: Definition Questions: Complex Question: What kind od assistance has North Korea received from Question PATTERN: What kind of assistance has X received from Y for Z? X=North Korea Y=USSR/Russia FOCUS=Z=misile program Elaboration of FOCUS: Reliability: &amp;quot;What are the sources of information?&amp;quot; the USSR/Russia for its missile program? (1) RESULTATIVE (2) TEMPORAL What is X? &amp;quot;What is North Korea?&amp;quot; What is Y? &amp;quot;What is USSR/Russia?&amp;quot; What is assistance? &amp;quot;What are the missiles in the North Korean inventory?&amp;quot; &amp;quot;When did North Korea receive assistance from the USSR/</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bodwen, Williams, Bensley, 2003</marker>
<rawString>S. Harabagiu, D. Moldovan, C. Clark, M. Bodwen, J. Williams, and J. Bensley. 2003. Answer mining by combining extraction techniques with abductive reasoning. In Notebook of the Twelveth Text REtrieval Converence (TREC-2003), pages 46–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>W Zhu</author>
<author>A Ratnaparkhi</author>
</authors>
<title>IBM’s statistical question answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th Text REtrieval Conference,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="10827" citStr="Ittycheriah et al., 2000" startWordPosition="1700" endWordPosition="1704">t of linguistic patterns that need to be matched for extracting the answer. Example of definition questions are “What is a golden parachute?” or “What is ETA in Spain?”. In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. This model is based on the idea that if a given sentence SA contains an answer substring A to a question Q, then SA can be re-written into Q through a sequence of stochastic operators. Not only a justification ofthe answer is produced, but the conditional probability P(Q—SA) re-ranks all candidate answers. A different viewpoint of Q/A was reported in (Ittycheriah et al., 2000). Finding the answers A to a question Q was considered a classification problem that maximizes the conditional probability P(A—Q). This model is not tractable currently, because (a) the search space is too large for a text collection like the TREC or the AQUAINT corpora; and (b) the training data is insufficient. Therefore, Q/A is modeled by the distribution P(C—A,Q) where C measures the “correctness” of A to question Q. By using a hidden variable E that represents the expected answer type, P(C—A,Q) = ΣE p(C,E—Q,A) = ΣE p(C—E,Q,A) * p(E—Q,A). Both distributions are modeled by using the maximum</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparkhi, 2000</marker>
<rawString>A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi. 2000. IBM’s statistical question answering system. In Proceedings of the 9th Text REtrieval Conference, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Sch¨olkopf and C. Burges and A. Smola (ed.), MIT-Press., editor,</editor>
<contexts>
<context position="26873" citStr="Joachims, 1999" startWordPosition="4244" endWordPosition="4245">eveloping set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3We point out that we removed from the Penn TreeBank the special tags of noun phrases like Subj and TMP as parsers usually are not able to provide this information. data to affect the global performance. The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at http://svmlight.joachims.org/ with the default polynomial kernel according to a degree d E {1, 2, 3, 4, 5}. The performances were evaluated using the F1 measure for both single argument classifiers and the multi-class classifier. - PHRASE TYPE (pt): This feature indicates the syntactic type of the phrase labeled as a predicate argument. - PARSE TREE PATH (path): This feature contains the path in the parse tree between the predicate phrase and the argument phrase, expressed as a sequence of nonterminal labels linked by direction (up or down). - POSITION (pos) Indicates if the cons</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. T. Joachims, Making large-Scale SVM Learning Practical. In B. Sch¨olkopf and C. Burges and A. Smola (ed.), MIT-Press., editor, Advances in Kernel Methods - Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Alex Lascarides</author>
</authors>
<title>A probabilistic account of logical metonymy.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--2</pages>
<contexts>
<context position="41489" citStr="Lapata and Lascarides, 2003" startWordPosition="6506" endWordPosition="6509">ver(possesses(Iraq, biological weapons)). Whereas option (b) stockpile(2,3) can be similarly interpreted as stockpile(Iraq, biological weapons). Note that one of the indexed objects is the topic. The structure of the topic is define along three semantic dimensions: (1) hyponyms or examples of other types of the same category as the topic; (2) the meronyms or components; and (3) the functionality or the usage. The derivation of such a large set of intentional structures helped us learn how to coerce pragmatic knowledge. We have developed a probabilistic approach extending the metonymy work of (Lapata and Lascarides, 2003). Lapata and Lascarides report a model of interpretation of verbal metonymy as the point distribution P(e, o, v) of three variables: the metonymy verb v, its object, and the sought after interpretation i. For example a verb → object relation that needs to be metonymycally interpreted, is enjoy → movie. In this case v = enjoy, o = movie and i E {making, watching, directing}. The variables of the distribution re ordered as &lt;i, v, o&gt; to help factoring P(i, v, o) = P(i) · P(v|i) · P(o|i, v). Each of the probabilities P(i), P(v|i) and P(o|i, v) can be estimated using maximum likelihood. As it is il</context>
</contexts>
<marker>Lapata, Lascarides, 2003</marker>
<rawString>Maria Lapata and Alex Lascarides. 2003. A probabilistic account of logical metonymy. Computational Linguistics, 29:2:263–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Lehnert</author>
</authors>
<title>The process of question answering.</title>
<date>1978</date>
<booktitle>In Lawrence Erlbaum Assoc.,</booktitle>
<location>Hillsdale.</location>
<contexts>
<context position="11807" citStr="Lehnert, 1978" startWordPosition="1866" endWordPosition="1867">,Q) where C measures the “correctness” of A to question Q. By using a hidden variable E that represents the expected answer type, P(C—A,Q) = ΣE p(C,E—Q,A) = ΣE p(C—E,Q,A) * p(E—Q,A). Both distributions are modeled by using the maximum entropy. All three forms of questions are also useful when processing complex questions, determined by a scenario resulting from a problem-solving situation. As illustrated in Figure 1, a scenario question may be associated with a pattern. One of the pattern variables represents the focus of the question. The notion of the question focus was first introduced by (Lehnert, 1978). The focus represents the most important concept of the question; a concept determining the domain of question. In the case of question Q1, the focus is missile program. The identification of the focus is based on the predicate-structure of the question pattern and on the order of the arguments. Figure 3 shows both the question pattern associated with Q1 and its predicate-argument structure. The argument with the role of purpose is ranked highest, and thus it determines the question focus. With the exception of the focus, all arguments from the predicate-argument structure may be used for gen</context>
</contexts>
<marker>Lehnert, 1978</marker>
<rawString>Wendy Lehnert. 1978. The process of question answering. In Lawrence Erlbaum Assoc., Hillsdale.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role parsing: Adding semantic structure to unstructured text.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Data Mining (ICDM-2003).</booktitle>
<contexts>
<context position="25048" citStr="Pradhan et al., 2003" startWordPosition="3951" endWordPosition="3954"> the words of one argument of p, put Fp,a in T+ (positive examples), otherwise put it in T− (negative examples). 0.8 0.77 0.74 F 0.71 0.68 0.65 1 2 3 4 5 Polynomial Degree (a) 1 2 3 4 5 Polynomial Degree (b) i 0.83 0.77 0.74 0.71 0.68 0.8 Arg0 Arg1 ArgM Figure 6: Single classifiers and Multi-classifier performance for argument extraction. The above T+ and T− sets can be re-organized as positive T+arg, and negative T−arg, examples for each argument i. In this way, an individual ONE-vs-ALL SVM classifier for each argument i can be trained. We adopted this solution as it is simple and effective (Pradhan et al., 2003). In the classification phase, given a sentence of the test-set, all its Fp,a are generated and classified by each individual SVM classifier. As a final decision, we select the argument associated with the maximum value among the scores provided by the SVMs2, i.e. argmaxi∈S Ci, where S is the target set of arguments. The discovering of relevant features is a complex task. Nevertheless there is a common consensus on the basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsin</context>
</contexts>
<marker>Pradhan, Hacioglu, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2003. Semantic role parsing: Adding semantic structure to unstructured text. In Proceedings of the International Conference on Data Mining (ICDM-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan Sanda Harabagiu</author>
<author>Takashi Yukawa</author>
</authors>
<title>Testing gricean constraints on a wordnet-based coherence evaluation system.</title>
<date>1996</date>
<booktitle>In Working Notes of the AAAI-96 Spring Symposium on Computational Implicature,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="34398" citStr="Harabagiu and Yukawa, 1996" startWordPosition="5386" endWordPosition="5389">ke vote political crisis bases and/or database access mechanisms. The research proposed in this project focuses on the derivation and usage of pragmatic knowledge that supports the recognition of question implications, also known as implicatures (cf. (Grice, 1975b)). 5.1 Intentional structures Derived from Lexico-Semantic Knowledge The novel idea of this research is to link computational implicatures, similar to those defined by Grice (Grice, 1975b), to inferences that can be drawn from general lexico-semantic knowledge bases such as WordNet of FrameNet. Incipient work was described in (Sanda Harabagiu and Yukawa, 1996), where a method of using lexico-semantic path for recognizing textual implicatures was presented. To our knowledge, this is the only computational model of implicatures that was developed and tested on a large lexico-semantic knowledge base (e.g. WordNet), enabling successful recognition of implicatures. The model proposed in (Sanda Harabagiu and Yukawa, 1996) uncovered a relationship between (a) the coherence of a text segment; (b) its cohesion expressed by the lexical paths and (c) the implicatures that can be drawn, mostly to account for pragmatic knowledge. This relationship can be extend</context>
</contexts>
<marker>Harabagiu, Yukawa, 1996</marker>
<rawString>Dan Moldovan Sanda Harabagiu and Takashi Yukawa. 1996. Testing gricean constraints on a wordnet-based coherence evaluation system. In Working Notes of the AAAI-96 Spring Symposium on Computational Implicature, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>8--15</pages>
<contexts>
<context position="20380" citStr="Surdeanu et al., 2003" startWordPosition="3221" endWordPosition="3225">tion of the question context. In this paper, by considering the intentional information and the implied information that can be derived when processing questions, we introduce a novel model of Q/A, which has access to rich semantic structures and enables the retrieval of more accurate answers as well as inference processes that explain the validity and contextual coverage of answers. Figure 5 shows the structure of the novel model of Q/A we propose. Both Question Processing and Document Processing have the recognition of predicate-argument structures as a crux of their models. As reported in (Surdeanu et al., 2003), the recognition of predicate-argument structures depends on features made available by full syntactic parses and by Named Entity Recognizers. As we shall show in this paper, the predicate-argument structures enable the recognition of question pattern, the question focus and the intentional structure associated with Question Intentional Structure Identification of Predicate−Argument Structures Syntactic Parse Recognition of Answer Structure Question Processing Document Processing Answer Processing Keyword Extraction Named Entity Recognition Recognition of Question Pattern Identification of Qu</context>
<context position="26022" citStr="Surdeanu et al., 2003" startWordPosition="4100" endWordPosition="4103">es is a complex task. Nevertheless there is a common consensus on the basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3We point out that we removed from the Pen</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03), pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="23613" citStr="Vapnik, 1995" startWordPosition="3702" endWordPosition="3703">ive, and ArgM-TMP stands for a temporal. An example of PropBank markup is: [Arg10 Analysts ] have been [predicate1 expecting ] [Arg1 1 a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 the U.S. car maker] [Arg2 an eventual 30% state in the British Company ]. 1 The model of identifying the arguments of each predicate consists of two tasks: (1) the recognition of the boundaries of each argument in the syntactic parse tree; (2) the identification of the argument role. Each task can be cast as a separate classifier. Next section describes our approach based on Support Vector Machines (SVM) (Vapnik, 1995). 4.1 Automatic Predicate-Argument extraction Given a sentence in natural language, all the predicates associated with its verbs have to be identified along with their arguments. This problem can be divided in two subtasks: (a) detection of the target argument boundaries, i.e. all its compounding words, and (b) classification of the argument type, e.g. Arg0 or ArgM. A direct approach to learn both detection and classification of predicate arguments is summarized by the following steps: 1. Given a sentence from the training-set, generate a full syntactic parse-tree; 2. let P and A be the set of</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>