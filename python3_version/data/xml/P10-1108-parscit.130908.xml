<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989214">
Efficient Inference Through Cascades of Weighted Tree Transducers
</title>
<author confidence="0.997083">
Jonathan May and Kevin Knight
</author>
<affiliation confidence="0.830877333333333">
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
</affiliation>
<email confidence="0.977325">
{jonmay,knight}@isi.edu
</email>
<note confidence="0.9125436">
Heiko Vogler
Technische Universit¨at Dresden
Institut f¨ur Theoretische Informatik
01062 Dresden, Germany
heiko.vogler@tu-dresden.de
</note>
<sectionHeader confidence="0.988548" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837333333333">
Weighted tree transducers have been pro-
posed as useful formal models for rep-
resenting syntactic natural language pro-
cessing applications, but there has been
little description of inference algorithms
for these automata beyond formal founda-
tions. We give a detailed description of
algorithms for application of cascades of
weighted tree transducers to weighted tree
acceptors, connecting formal theory with
actual practice. Additionally, we present
novel on-the-fly variants of these algo-
rithms, and compare their performance
on a syntax machine translation cascade
based on (Yamada and Knight, 2001).
</bodyText>
<sectionHeader confidence="0.98776" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.946330866666667">
Weighted finite-state transducers have found re-
cent favor as models of natural language (Mohri,
1997). In order to make actual use of systems built
with these formalisms we must first calculate the
set of possible weighted outputs allowed by the
transducer given some input, which we call for-
ward application, or the set of possible weighted
inputs given some output, which we call backward
application. After application we can do some in-
ference on this result, such as determining its k
highest weighted elements.
We may also want to divide up our problems
into manageable chunks, each represented by a
transducer. As noted by Woods (1980), it is eas-
ier for designers to write several small transduc-
ers where each performs a simple transformation,
rather than painstakingly construct a single com-
plicated device. We would like to know, then,
the result of transformation of input or output by
a cascade of transducers, one operating after the
other. As we will see, there are various strate-
gies for approaching this problem. We will con-
sider offline composition, bucket brigade applica-
tion, and on-the-fly application.
Application of cascades of weighted string
transducers (WSTs) has been well-studied (Mohri,
1997). Less well-studied but of more recent in-
terest is application of cascades of weighted tree
transducers (WTTs). We tackle application of WTT
cascades in this work, presenting:
</bodyText>
<listItem confidence="0.998637333333333">
• explicit algorithms for application of WTT cas-
cades
• novel algorithms for on-the-fly application of
WTT cascades, and
• experiments comparing the performance of
these algorithms.
</listItem>
<sectionHeader confidence="0.519523" genericHeader="introduction">
2 Strategies for the string case
</sectionHeader>
<bodyText confidence="0.999952407407407">
Before we discuss application of WTTs, it is help-
ful to recall the solution to this problem in the WST
domain. We recall previous formal presentations
of WSTs (Mohri, 1997) and note informally that
they may be represented as directed graphs with
designated start and end states and edges labeled
with input symbols, output symbols, and weights.1
Fortunately, the solution for WSTs is practically
trivial—we achieve application through a series
of embedding, composition, and projection oper-
ations. Embedding is simply the act of represent-
ing a string or regular string language as an iden-
tity WST. Composition of WSTs, that is, generat-
ing a single WST that captures the transformations
of two input WSTs used in sequence, is not at all
trivial, but has been well covered in, e.g., (Mohri,
2009), where directly implementable algorithms
can be found. Finally, projection is another triv-
ial operation—the domain or range language can
be obtained from a WST by ignoring the output or
input symbols, respectively, on its arcs, and sum-
ming weights on otherwise identical arcs. By em-
bedding an input, composing the result with the
given WST, and projecting the result, forward ap-
plication is accomplished.2 We are then left with
a weighted string acceptor (WSA), essentially a
weighted, labeled graph, which can be traversed
</bodyText>
<footnote confidence="0.914099142857143">
1We assume throughout this paper that weights are in
R+ U {+oo1, that the weight of a path is calculated as the
product of the weights of its edges, and that the weight of a
(not necessarily finite) set T of paths is calculated as the sum
of the weights of the paths of T.
2For backward applications, the roles of input and output
are simply exchanged.
</footnote>
<page confidence="0.923209">
1058
</page>
<note confidence="0.971253">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1058–1066,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.9993175">
(a) Input string “a a” embedded in an (b) first WST in cascade (c) second WST in cascade
identity WST
(d) Offline composition approach: (e) Bucket brigade approach: (f) Result of offline or bucket application
Compose the transducers Apply WST (b) to WST (a) after projection
(g) Initial on-the-fly (h) On-the-fly stand-in after exploring (i) On-the-fly stand-in after best path has been found
stand-in for (f) outgoing edges of state ADF
</figure>
<figureCaption confidence="0.999983">
Figure 1: Three different approaches to application through cascades of WSTs.
</figureCaption>
<bodyText confidence="0.999916454545454">
by well-known algorithms to efficiently find the k-
best paths.
Because WSTs can be freely composed, extend-
ing application to operate on a cascade of WSTs
is fairly trivial. The only question is one of com-
position order: whether to initially compose the
cascade into a single transducer (an approach we
call offline composition) or to compose the initial
embedding with the first transducer, trim useless
states, compose the result with the second, and so
on (an approach we call bucket brigade). The ap-
propriate strategy generally depends on the struc-
ture of the individual transducers.
A third approach builds the result incrementally,
as dictated by some algorithm that requests in-
formation about it. Such an approach, which we
call on-the-fly, was described in (Pereira and Ri-
ley, 1997; Mohri, 2009; Mohri et al., 2000). If
we can efficiently calculate the outgoing edges of
a state of the result WSA on demand, without cal-
culating all edges in the entire machine, we can
maintain a stand-in for the result structure, a ma-
chine consisting at first of only the start state of
the true result. As a calling algorithm (e.g., an im-
plementation of Dijkstra’s algorithm) requests in-
formation about the result graph, such as the set of
outgoing edges from a state, we replace the current
stand-in with a richer version by adding the result
of the request. The on-the-fly approach has a dis-
tinct advantage over the other two methods in that
the entire result graph need not be built. A graphi-
cal representation of all three methods is presented
in Figure 1.
</bodyText>
<sectionHeader confidence="0.91355" genericHeader="method">
3 Application of tree transducers
</sectionHeader>
<bodyText confidence="0.999970214285714">
Now let us revisit these strategies in the setting
of trees and tree transducers. Imagine we have a
tree or set of trees as input that can be represented
as a weighted regular tree grammar3 (WRTG) and
a WTT that can transform that input with some
weight. We would like to know the k-best trees the
WTT can produce as output for that input, along
with their weights. We already know of several
methods for acquiring k-best trees from a WRTG
(Huang and Chiang, 2005; Pauls and Klein, 2009),
so we then must ask if, analogously to the string
case, WTTs preserve recognizability4 and we can
form an application WRTG. Before we begin, how-
ever, we must define WTTs and WRTGs.
</bodyText>
<subsectionHeader confidence="0.990107">
3.1 Preliminaries5
</subsectionHeader>
<bodyText confidence="0.999803875">
A ranked alphabet is a finite set E such that ev-
ery member Q E E has a rank rk(Q) E N. We
call E(k) C_ E, k E N the set of those Q E E
such that rk(Q) = k. The set of variables is de-
noted X = {x1, x2,...I and is assumed to be dis-
joint from any ranked alphabet used in this paper.
We use L to denote a symbol of rank 0 that is not
in any ranked alphabet used in this paper. A tree
</bodyText>
<equation confidence="0.4890105">
t E TE is denoted Q(t1, ... , tk) where k &gt; 0,
Q E E(k), and t1, ... , tk E TE. For Q E E(°) we
</equation>
<footnote confidence="0.960876111111111">
3This generates the same class of weighted tree languages
as weighted tree automata, the direct analogue of WSAs, and
is more useful for our purposes.
4A weighted tree language is recognizable iff it can be
represented by a wrtg.
5The following formal definitions and notations are
needed for understanding and reimplementation of the pre-
sented algorithms, but can be safely skipped on first reading
and consulted when encountering an unfamiliar term.
</footnote>
<page confidence="0.995999">
1059
</page>
<bodyText confidence="0.67364175">
write Q ∈ TΣ as shorthand for Q(). For every set
5 disjoint from E, let TΣ(5) = TΣuS, where, for
all s ∈ 5, rk(s) = 0.
We define the positions of a tree
</bodyText>
<equation confidence="0.545059666666667">
t = Q(t1, ... , tk), for k ≥ 0, Q ∈ E(k),
t1, ... , tk ∈ TΣ, as a set pos(t) ⊂ N* such that
pos(t) = {e} ∪ {iv  |1 ≤ i ≤ k, v ∈ pos(ti)}.
</equation>
<bodyText confidence="0.99663675">
The set of leaf positions lv(t) ⊆ pos(t) are those
positions v ∈ pos(t) such that for no i ∈ N,
vi ∈ pos(t). We presume standard lexicographic
orderings &lt; and ≤ on pos.
Let t, s ∈ TΣ and v ∈ pos(t). The label of t
at position v, denoted by t(v), the subtree of t at
v, denoted by t|v, and the replacement at v by s,
denoted by t[s]v, are defined as follows:
</bodyText>
<listItem confidence="0.9968212">
1. For every Q ∈ E(0), Q(e) = Q, Q|ε = Q, and
Q[s]ε = s.
2. For every t = Q(t1, ... , tk) such that
k = rk(Q) and k ≥ 1, t(e) = Q, t|ε = t,
and t[s]ε = s. For every 1 ≤ i ≤ k and
</listItem>
<equation confidence="0.998076">
v ∈ pos(ti), t(iv) = ti(v), t|iv = ti|v, and
t[s]iv = Q(t1, ...,ti−1,ti[s]v, ti+1, ... , tk).
</equation>
<bodyText confidence="0.830369">
The size of a tree t, size(t) is |pos(t)|, the car-
dinality of its position set. The yield set of a tree
is the set of labels of its leaves: for a tree t, yd (t)
= {t(v)  |v ∈ lv(t)}.
Let A and B be sets. Let co : A → TΣ(B)
be a mapping. We extend co to the mapping co :
TΣ(A) → TΣ(B) such that for a ∈ A, co(a) = co(a)
and for k ≥ 0, Q ∈ E(k), and t1, ... , tk ∈ TΣ(A),
co(Q(t1, ... , tk)) = Q(co(t1), ... , co(tk)). We indi-
cate such extensions by describing co as a substi-
tution mapping and then using co without further
comment.
We use R+ to denote the set {w ∈ R  |w ≥ 0}
and R +to denote R+ ∪ {+∞}.
</bodyText>
<listItem confidence="0.924676333333334">
Definition 3.1 (cf. (Alexandrakis and Bozapa-
lidis, 1987)) A weighted regular tree grammar
(WRTG) is a 4-tuple G = (N, E, P, n0) where:
1. N is a finite set of nonterminals, with n0 ∈ N
the start nonterminal.
2. E is a ranked alphabet of input symbols, where
E ∩ N = ∅.
3. P is a tuple (P&apos;, 7r), where P&apos; is a finite set
of productions, each production p of the form
n →− u, n ∈ N, u ∈ TΣ(N), and 7r : P&apos; → R+
is a weight function of the productions. We will
refer to P as a finite set of weighted produc-
</listItem>
<bodyText confidence="0.998695666666667">
tions, each production p of the form nπ(p)→ u.
A production p is a chain production if it is
of the form ni −→ nj, where ni, nj ∈ N.6
</bodyText>
<footnote confidence="0.81535525">
w
6In (Alexandrakis and Bozapalidis, 1987), chain produc-
tions are forbidden in order to avoid infinite summations. We
explicitly allow such summations.
</footnote>
<bodyText confidence="0.993452625">
A WRTG G is in normal form if each produc-
tion is either a chain production or is of the
form n w −→ Q(n1, ... , nk) where Q ∈ E(k) and
n1, ... , nk ∈ N.
For WRTG G = (N, E, P, n0), s, t, u ∈ TΣ(N),
n ∈ N, and p ∈ P of the form n w−→ u, we
obtain a derivation step from s to t by replacing
some leaf nonterminal in s labeled n with u. For-
mally, s ⇒pG t if there exists some v ∈ lv(s)
such that s(v) = n and s[u]v = t. We say this
derivation step is leftmost if, for all v&apos; ∈ lv(s)
where v&apos; &lt; v, s(v&apos;) ∈ E. We henceforth as-
sume all derivation steps are leftmost. If, for
some m ∈ N, pi ∈ P, and ti ∈ TΣ(N) for all
1 ≤ i ≤ m, n0 ⇒p1 t1 ... ⇒p— tm, we say
the sequence d = (p1, ... , pm) is a derivation
of tm in G and that n0 ⇒* tm; the weight of d
is wt(d) = 7r(p1) · ... · 7r(pm). The weighted
tree language recognized by G is the mapping
LG : TΣ → R+ such that for every t ∈ TΣ, LG(t)
is the sum of the weights of all (possibly infinitely
many) derivations of t in G. A weighted tree lan-
guage f : TΣ → R+ is recognizable if there is a
WRTG G such that f = LG.
We define a partial ordering :� on WRTGs
such that for WRTGs G1 = (N1, E, P1, n0) and
G2 = (N2, E, P2, n0), we say G1 -� G2 iff
N1 ⊆ N2 and P1 ⊆ P2, where the weights are
preserved.
Definition 3.2 (cf. Def. 1 of (Maletti, 2008))
A weighted extended top-down tree transducer
(WXTT) is a 5-tuple M = (Q, E, A, R, q0) where:
</bodyText>
<listItem confidence="0.995740923076923">
1. Q is a finite set of states.
2. E and A are the ranked alphabets of in-
put and output symbols, respectively, where
(E ∪ A) ∩ Q = ∅.
3. R is a tuple (R&apos;, 7r), where R&apos; is a finite set
of rules, each rule r of the form q.y →− u for
q ∈ Q, y ∈ TΣ(X), and u ∈ TΔ(Q × X).
We further require that no variable x ∈ X ap-
pears more than once in y, and that each vari-
able appearing in u is also in y. Moreover,
7r : R&apos; → R+ is a weight function of the
rules. As for WRTGs, we refer to R as a finite
set of weighted rules, each rule r of the form
</listItem>
<equation confidence="0.453707">
π(r)
−−→ u.
A WXTT is linear (respectively, nondeleting)
</equation>
<bodyText confidence="0.987866625">
if, for each rule r of the form q.y w−→ u, each
x ∈ yd (y) ∩ X appears at most once (respec-
tively, at least once) in u. We denote the class
of all WXTTs as wxT and add the letters L and N
to signify the subclasses of linear and nondeleting
WTT, respectively. Additionally, if y is of the form
Q(x1, ... , xk), we remove the letter “x” to signify
q.y
</bodyText>
<page confidence="0.910021">
1060
</page>
<bodyText confidence="0.9954">
the transducer is not extended (i.e., it is a “tradi-
tional” WTT (F¨ul¨op and Vogler, 2009)).
For WXTT M = (Q, E, A, R, qo), s, t E TA(Q
x TE), and r E R of the form q.y w�—* u, we obtain
a derivation step from s to t by replacing some
leaf of s labeled with q and a tree matching y by a
transformation of u, where each instance of a vari-
able has been replaced by a corresponding subtree
of the y-matching tree. Formally, s firM t if there
is a position v E pos(s), a substitution mapping
</bodyText>
<equation confidence="0.5954075">
ϕ : X —* TE, and a rule q.y w�—* u E R such that
s(v) = (q, ϕ(y)) and t = s[ϕ0(u)],,, where ϕ0 is
</equation>
<bodyText confidence="0.954810777777778">
a substitution mapping Q x X —* TA(Q x TE)
defined such that ϕ0(q0, x) = (q0, ϕ(x)) for all
q0 E Q and x E X. We say this derivation step
is leftmost if, for all v0 E lv(s) where v0 &lt; v,
s(v0) E A. We henceforth assume all derivation
steps are leftmost. If, for some s E TE, m E N,
ri E R, and ti E TA(Q x TE) for all 1 &lt;_ i &lt;_ m,
(qo, s) fir1 t1 ... fir,, tm, we say the sequence
d = (r1, ... , rm) is a derivation of (s, tm) in M;
the weight of d is wt(d) = π(r1) · ... · π(rm).
The weighted tree transformation recognized by
M is the mapping τM : TE x TA —* R∞� , such
that for every s E TE and t E TA, τM(s, t) is the
sum of the weights of all (possibly infinitely many)
derivations of (s, t) in M. The composition of two
weighted tree transformations τ : TE xTA —* R∞ �
and µ : TA xTr —* R∞ �is the weighted tree trans-
formation (τ; µ) : TE x Tr —* R∞where for every
</bodyText>
<subsectionHeader confidence="0.337639">
s E TE and u E Tr, (τ; µ)(s, u) = Et∈T, τ(s, t)
</subsectionHeader>
<bodyText confidence="0.62556">
· µ(t, u).
</bodyText>
<subsectionHeader confidence="0.99965">
3.2 Applicable classes
</subsectionHeader>
<bodyText confidence="0.992408575">
We now consider transducer classes where recog-
nizability is preserved under application. Table 1
presents known results for the top-down tree trans-
ducer classes described in Section 3.1. Unlike
the string case, preservation of recognizability is
not universal or symmetric. This is important for
us, because we can only construct an application
WRTG, i.e., a WRTG representing the result of ap-
plication, if we can ensure that the language gen-
erated by application is in fact recognizable. Of
the types under consideration, only wxLNT and
wLNT preserve forward recognizability. The two
classes marked as open questions and the other
classes, which are superclasses of wNT, do not or
are presumed not to. All subclasses of wxLT pre-
serve backward recognizability.7 We do not con-
sider cases where recognizability is not preserved
in the remainder of this paper. If a transducer M
of a class that preserves forward recognizability is
applied to a WRTG G, we can call the forward ap-
7Note that the introduction of weights limits recognizabil-
ity preservation considerably. For example, (unweighted) xT
preserves backward recognizability.
plication WRTG M(G)° and if M preserves back-
ward recognizability, we can call the backward ap-
plication WRTG M(G)a.
Now that we have explained the application
problem in the context of weighted tree transduc-
ers and determined the classes for which applica-
tion is possible, let us consider how to build for-
ward and backward application WRTGs. Our ba-
sic approach mimics that taken for WSTs by us-
ing an embed-compose-project strategy. As in
string world, if we can embed the input in a trans-
ducer, compose with the given transducer, and
project the result, we can obtain the application
WRTG. Embedding a WRTG in a wLNT is a triv-
ial operation—if the WRTG is in normal form and
chain production-free,8 for every production of the
form n w�—* σ(n1, ... , nk), create a rule of the form
</bodyText>
<equation confidence="0.985878">
w
n.σ(x1, ... , xk) σ(n1.x1, ... , nk.xk). Range
</equation>
<bodyText confidence="0.99935675">
projection of a wxLNT is also trivial—for every
q E Q and u E TA(Q x X) create a production
of the form q w�—* u0 where u0 is formed from u
by replacing all leaves of the form q.x with the
leaf q, i.e., removing references to variables, and
w is the sum of the weights of all rules of the form
q.y —*� u in R.9 Domain projection for wxLT is
best explained by way of example. The left side of
a rule is preserved, with variables leaves replaced
by their associated states from the right side. So,
the rule q1.σ(γ(x1), x2) w�—* δ(q2.x2, β(α, q3.x1))
would yield the production q1 �—* σ(γ(q3), q2) in
</bodyText>
<equation confidence="0.616391">
w
</equation>
<bodyText confidence="0.999839952380952">
the domain projection. However, a deleting rule
such as q1.σ(x1, x2) w�—* γ(q2.x2) necessitates the
introduction of a new nonterminal L that can gen-
erate all of TE with weight 1.
The only missing piece in our embed-compose-
project strategy is composition. Algorithm 1,
which is based on the declarative construction of
Maletti (2006), generates the syntactic composi-
tion of a wxLT and a wLNT, a generalization
of the basic composition construction of Baker
(1979). It calls Algorithm 2, which determines
the sequences of rules in the second transducer
that match the right side of a single rule in the
first transducer. Since the embedded WRTG is of
type wLNT, it may be either the first or second
argument provided to Algorithm 1, depending on
whether the application is forward or backward.
We can thus use the embed-compose-project strat-
egy for forward application of wLNT and back-
ward application of wxLT and wxLNT. Note that
we cannot use this strategy for forward applica-
</bodyText>
<footnote confidence="0.9990764">
8Without loss of generality we assume this is so, since
standard algorithms exist to remove chain productions
(Kuich, 1998; ´Esik and Kuich, 2003; Mohri, 2009) and con-
vert into normal form (Alexandrakis and Bozapalidis, 1987).
9Finitely many such productions may be formed.
</footnote>
<page confidence="0.983978">
1061
</page>
<bodyText confidence="0.857764">
tion of wxLNT, even though that class preserves
recognizability.
</bodyText>
<figure confidence="0.934320137931034">
Algorithm 1 COMPOSE
1: inputs
2: wxLT M1 = (Q1, Σ, Δ, R1, q10)
3: wLNT M2 = (Q2, Δ, Γ, R2, q20)
4: outputs
5: wxLT M3 = ((Q1 ×Q2), Σ, Γ, R3, (q10, q20)) such
that M3 = (τM1; τM2).
6: complexity
7: O(|R1 |max(|R2|9ize(˜u), |Q2|)), where u˜ is the
largest right side tree in any rule in R1
8: Let R3 be of the form (R&apos;3, π)
9: R3 ← (∅, ∅)
10: Ξ ← {(q10, q20)} {seen states}
11: Ψ ← {(q10, q20)} {pending states}
12: while Ψ =6 ∅ do
13: (q1, q2) ←any element of Ψ
14: Ψ ← Ψ \ {(q1, q2)}
15: for all (q1.y w1
−−→ u) ∈ R1 do
16: for all (z, w2) ∈ COVER(u, M2, q2) do
17: for all (q, x) ∈ yd (z) ∩ ((Q1 × Q2) × X) do
18: if q ∈6 Ξ then
19: Ξ ← Ξ ∪ {q}
20: Ψ ← Ψ ∪ {q}
21: r ← ((q1, q2).y →− z)
22: R&apos;3 ← R&apos;3 ∪ {r}
23: π(r) ← π(r) + (w1 · w2)
24: return M3
4 Application of tree transducer cascades
</figure>
<bodyText confidence="0.975013275862069">
What about the case of an input WRTG and a cas-
cade of tree transducers? We will revisit the three
strategies for accomplishing application discussed
above for the string case.
In order for offline composition to be a viable
strategy, the transducers in the cascade must be
closed under composition. Unfortunately, of the
classes that preserve recognizability, only wLNT
is closed under composition (G´ecseg and Steinby,
1984; Baker, 1979; Maletti et al., 2009; F¨ul¨op and
Vogler, 2009).
However, the general lack of composability of
tree transducers does not preclude us from con-
ducting forward application of a cascade. We re-
visit the bucket brigade approach, which in Sec-
tion 2 appeared to be little more than a choice of
composition order. As discussed previously, ap-
plication of a single transducer involves an embed-
ding, a composition, and a projection. The embed-
ded WRTG is in the class wLNT, and the projection
forms another WRTG. As long as every transducer
in the cascade can be composed with a wLNT
to its left or right, depending on the application
type, application of a cascade is possible. Note
that this embed-compose-project process is some-
what more burdensome than in the string case. For
strings, application is obtained by a single embed-
ding, a series of compositions, and a single projec-
Algorithm 2 COVER
</bodyText>
<listItem confidence="0.864662666666667">
1: inputs
2: u ∈ TΔ(Q1 × X)
3: wT M2 = (Q2, Δ, Γ, R2, q20)
4: state q2 ∈ Q2
5: outputs
6: set of pairs (z, w) with z ∈ TΓ((Q1 × Q2) × X)
formed by one or more successful runs on u by rules
in R2, starting from q2, and w ∈ R +the sum of the
weights of all such runs.
7: complexity
8: O(|R2|5ize(u))
9: if u(ε) is of the form (q1, x) ∈ Q1 × X then
10: zinit ← ((q1, q2), x)
11: else
12: zinit ← ⊥
13: Πlast ← {(zinit, {((ε, ε), q2)}, 1)}
14: for all v ∈ pos(u) such that u(v) ∈ Δ(k) for some
k ≥ 0 in prefix order do
15: Πv ← ∅
16: for all (z, θ, w) ∈ Πlast do
17: for all v&apos; ∈ lv(z) such that z(v&apos;) = ⊥ do
18: for all (θ(v, v&apos;).u(v)(x1, ... , xk) w,
−→ h)∈R2
do
19: θ&apos; ← θ
20: Form substitution mapping ϕ : (Q2 × X)
→ TΓ((Q1 × Q2 × X) ∪ {⊥}).
21: for i = 1 to k do
22: for all v&apos;&apos; ∈ pos(h) such that
h(v&apos;&apos;) = (q&apos;2, xi) for some q&apos;2 ∈ Q2 do
23: θ&apos;(vi, v&apos;v&apos;&apos;) ← q&apos;2
24: if u(vi) is of the form
(q1, x) ∈ Q1 × X then
25: ϕ(q&apos;2, xi) ← ((q1, q&apos;2), x)
26: else
27: ϕ(q&apos;2, xi) ← ⊥
28: Πv ← Πv ∪ {(z[ϕ(h)]v,, θ&apos;, w · w&apos;)}
29: Πlast ← Πv
30: Z ← {z  |(z, θ, w) ∈ Πlast}
</listItem>
<equation confidence="0.44672">
�31: return {(z, w)  |z ∈ Z}
(z,θ,w)EΠlast
</equation>
<bodyText confidence="0.992962">
tion, whereas application for trees is obtained by a
series of (embed, compose, project) operations.
</bodyText>
<subsectionHeader confidence="0.972801">
4.1 On-the-fly algorithms
</subsectionHeader>
<bodyText confidence="0.999812">
We next consider on-the-fly algorithms for ap-
plication. Similar to the string case, an on-the-
fly approach is driven by a calling algorithm that
periodically needs to know the productions in a
WRTG with a common left side nonterminal. The
embed-compose-project approach produces an en-
tire application WRTG before any inference al-
gorithm is run. In order to admit an on-the-fly
approach we describe algorithms that only gen-
erate those productions in a WRTG that have a
given left nonterminal. In this section we ex-
tend Definition 3.1 as follows: a WRTG is a 6-
tuple G = (N, E, P, no, M, G) where N, E, P,
and no are defined as in Definition 3.1, and either
M = G = 0,10 or M is a wxLNT and G is a nor-
mal form, chain production-free WRTG such that
</bodyText>
<footnote confidence="0.963919">
10In which case the definition is functionally unchanged
from before.
</footnote>
<page confidence="0.924983">
1062
</page>
<table confidence="0.994411454545455">
type preserved? source
w[x]T No See w[x]NT
w[x]LT OQ (Maletti, 2009)
w[x]NT No (G´ecseg and Steinby, 1984)
wxLNT Yes (F¨ul¨op et al., 2010)
wLNT Yes (Kuich, 1999)
type preserved? source
w[x]T No See w[x]NT
w[x]LT Yes (F¨ul¨op et al., 2010)
w[x]NT No (Maletti, 2009)
w[x]LNT Yes See w[x]LT
</table>
<tableCaption confidence="0.7393485">
(a) Preservation of forward recognizability (b) Preservation of backward recognizability
Table 1: Preservation of forward and backward recognizability for various classes of top-down tree
</tableCaption>
<bodyText confidence="0.767887">
transducers. Here and elsewhere, the following abbreviations apply: w = weighted, x = extended LHS, L
= linear, N = nondeleting, OQ = open question. Square brackets include a superposition of classes. For
example, w[x]T signifies both wxT and wT.
</bodyText>
<equation confidence="0.366594615384615">
Algorithm 3 PRODUCE
1: inputs
2: WRTG Gin = (Nin, Δ, Pin, n0, M, G) such
that M = (Q, Σ, Δ, R, q0) is a wxLNT and
G = (N, Σ, P, n00, M0, G0) is a WRTG in normal
form with no chain productions
3: nin ∈ Nin
4: outputs
5: WRTG Gout = (Nout, Δ, Pout, n0, M, G), such that
Gin --� Gout and
w
(nin −→ u) ∈ Pout ⇔ (nin −→ u) ∈ M(G).
w
</equation>
<listItem confidence="0.97646144">
6: complexity
7: O(|R||P|size(y)), where y˜ is the largest left side tree
in any rule in R
8: if Pin contains productions of the form nin −→ u then
w
9: return Gin
10: Nout ← Nin
11: Pout ← Pin
12: Let nin be of the form (n, q), where n ∈ N and q ∈ Q.
13: for all (q.y w�
−−→ u) ∈ R do
14: for all (0, w2) ∈ REPLACE(y, G, n) do
15: Form substitution mapping W : Q × X →
To(N × Q) such that, for all v ∈ yd (y) and q0 ∈
Q, if there exist n0 ∈ N and x ∈ X such that 0(v)
= n0 and y(v) = x, then W(q0, x) = (n0, q0).
p0 ← ((n, q) w�·w�
16: −−−−→ W(u))
17: for all p ∈ NORM(p0, Nout) do
18: Let p be of the form n0 −→ 6(n1, ... , nk) for
w
6 ∈ Δ(k).
19: Nout ← Nout ∪ {n0, ... , nk}
20: Pout ← Pout ∪ {p}
21: return CHAIN-REM(Gout)
</listItem>
<bodyText confidence="0.942848571428571">
G --&lt; M(G)°. In the latter case, G is a stand-in for
M(G)°, analogous to the stand-ins for WSAs and
WSTs described in Section 2.
Algorithm 3, PRODUCE, takes as input a
WRTG Gin = (Nin, A, Pin, no, M, G) and a de-
sired nonterminal nin and returns another WRTG,
Gout that is different from Gin in that it has more
productions, specifically those beginning with nin
that are in M(G)°. Algorithms using stand-ins
should call PRODUCE to ensure the stand-in they
are using has the desired productions beginning
with the specific nonterminal. Note, then, that
PRODUCE obtains the effect of forward applica-
Algorithm 4 REPLACE
</bodyText>
<listItem confidence="0.556218808510638">
1: inputs
2: y ∈ TE(X)
3: WRTG G = (N, Σ, P, n0, M, G) in normal form,
with no chain productions
4: n ∈ N
5: outputs
6: set Π of pairs (0, w) where 0 is a mapping
pos(y) → N and w ∈ R∞� , each pair indicating
a successful run on y by productions in G, starting
from n, and w is the weight of the run.
7: complexity
8: O( |P|size (y))
9: Πlast ← {({(a, n)}, 1)}
10: for all v ∈ pos(y) such that y(v) ∈6 X in prefix order
do
11: Πv ← ∅
12: for all (0, w) ∈ Πlast do
13: if M =6 ∅ and G =6 ∅ then
14: G ← PRODUCE(G, 0(v))
15: for all (0(v) w�
−→ y(v)(n1, ... , nk)) ∈ P do
16: Πv ← Πv∪{(0∪{(vi, ni), 1 ≤ i ≤ k}, w·w0)}
17: Πlast ← Πv
18: return Πlast
Algorithm 5 MAKE-EXPLICIT
1: inputs
2: WRTG G = (N, Σ, P, n0, M, G) in normal form
3: outputs
4: WRTG G0 = (N0, Σ, P0, n0, M, G), in normal form,
such that if M =6 ∅ and G =6 ∅, LGi = LM(G)p, and
otherwise G0 = G.
5: complexity
6: O(|P0|)
7: G0 ← G
8: Ξ ← {n0} {seen nonterminals}
9: Ψ ← {n0} {pending nonterminals}
10: while Ψ =6 ∅ do
11: n ←any element of Ψ
12: Ψ ← Ψ \ {n}
13: if M =6 ∅ and G =6 ∅ then
14: G0 ← PRODUCE(G0, n)
15: for all (n w−→ u(n1, ... , nk)) ∈ P0 do
16: fori=1tokdo
17: if ni ∈6 Ξ then
18: Ξ ← Ξ ∪ {ni}
19: Ψ ← Ψ ∪ {ni}
20: return G0
</listItem>
<equation confidence="0.970739037037037">
1063
g0
g0 −−→σ(g0, g1)
w1
g0 −−→ α g1
w2
(a) Input WRTG G
a0
a0.σ(x1, x2) w4
−−→ σ(a0.x1, a1.x2)
a0.σ(x1, x2) w5
−−→ ψ(a2.x1, a1.x2)
a0.α −−→ α a1.α 7 w8
w6 −−→w α a2.α −−→ ρ
(b) First transducer MA in the cascade
b0
b0.σ(x1, x2) w9
−−→ σ(b0.x1, b0.x2)
b0.α w10
−−→ α
(c) Second transducer MB in the cascade
−−−−→ σ(g0a0, g1a1)
w1·w4
−−−−→ ψ(g0a2, g1a1)
w1·w5
w2·w6
g0a0 −−−−→ α g1a1
</equation>
<bodyText confidence="0.585298">
(d) Productions of MA(G). built as a consequence
of building the complete MB(MA(G).).
</bodyText>
<equation confidence="0.6324024">
g0a0b0
g0a0b0
w1·w4·w9 −−−−−−→ σ(g0a0b0, g1a1b0)
w2·w6·w10 w3·w7·w10
g0a0b0 −−−−−−−→ α g1a1b0 −−−−−−−→ α
</equation>
<figure confidence="0.701149">
(e) Complete MB(MA(G).).
</figure>
<figureCaption confidence="0.995436">
Figure 2: Forward application through a cascade
of tree transducers using an on-the-fly method.
</figureCaption>
<bodyText confidence="0.929348461538462">
tion in an on-the-fly manner.11 It makes calls to
REPLACE, which is presented in Algorithm 4, as
well as to a NORM algorithm that ensures normal
form by replacing a single production not in nor-
mal form with several normal-form productions
that can be combined together (Alexandrakis and
Bozapalidis, 1987) and a CHAIN-REM algorithm
that replaces a WRTG containing chain productions
with an equivalent WRTG that does not (Mohri,
2009).
As an example of stand-in construction, con-
sider the invocation PRODUCE(G1, g0a0), where
G1 = ({g0a0}, ta, o, a, p}, ∅, g0a0, MA, G), G
is in Figure 2a,1 and MA is in 2b. The stand-in
WRTG that is output contains the first three of the
four productions in Figure 2d.
To demonstrate the use of on-the-fly application
in a cascade, we next show the effect of PRO-
DUCE when used with the cascade G ◦ MA ◦ MB,
where MB is in Figure 2c. Our driving al-
gorithm in this case is Algorithm 5, MAKE-
11Note further that it allows forward application of class
wxLNT, something the embed-compose-project approach did
not allow.
12By convention the initial nonterminal and state are listed
first in graphical depictions of WRTGs and WXTTs.
</bodyText>
<equation confidence="0.960137307692308">
rJJ.JJ(x1, x2, x3) →− JJ(rDT.x1, rJJ.x2, rVB.x3)
rVB.VB(x1, x2, x3) →− VB(rNNPS.x1, rNN.x3, rVB.x2)
t.”gentle” →− ”gentle”(a) Rotation rules
iVB.NN(x1, x2) →− NN(INS iNN.x1, iNN.x2)
iVB.NN(x1, x2) →− NN(iNN.x1, iNN.x2)
iVB.NN(x1, x2) →− NN(iNN.x1, iNN.x2, INS)
(b) Insertion rules
t.VB(x1, x2, x3) →− X(t.x1, t.x2, t.x3)
t.”gentleman” →− j1
t.”gentleman” →− EPS
t.INS →− j1
t.INS →− j2
(c) Translation rules
</equation>
<figureCaption confidence="0.817594">
Figure 3: Example rules from transducers used
</figureCaption>
<bodyText confidence="0.98622665625">
in decoding experiment. j1 and j2 are Japanese
words.
EXPLICIT, which simply generates the full ap-
plication WRTG using calls to PRODUCE. The
input to MAKE-EXPLICIT is G2 = ({g0a0b0},
{a, a}, ∅, g0a0b0, MB, G1).13 MAKE-EXPLICIT
calls PRODUCE(G2, g0a0b0). PRODUCE then
seeks to cover b0.a(x1, x2) W�
−→ Q(b0.x1, b0.x2)
with productions from G1, which is a stand-in for
MA(G)°. At line 14 of REPLACE, G1 is im-
proved so that it has the appropriate productions.
The productions of MA(G)° that must be built
to form the complete MB(MA(G)°)° are shown
in Figure 2d. The complete MB(MA(G)°)° is
shown in Figure 2e. Note that because we used
this on-the-fly approach, we were able to avoid
building all the productions in MA(G)°; in par-
ticular we did not build g0a2 −−−−→ p, while a
W2�W8
bucket brigade approach would have built this pro-
duction. We have also designed an analogous on-
the-fly PRODUCE algorithm for backward appli-
cation on linear WTT.
We have now defined several on-the-fly and
bucket brigade algorithms, and also discussed the
possibility of embed-compose-project and offline
composition strategies to application of cascades
of tree transducers. Tables 2a and 2b summa-
rize the available methods of forward and back-
ward application of cascades for recognizability-
preserving tree transducer classes.
</bodyText>
<sectionHeader confidence="0.99614" genericHeader="method">
5 Decoding Experiments
</sectionHeader>
<bodyText confidence="0.970983888888889">
The main purpose of this paper has been to
present novel algorithms for performing applica-
tion. However, it is important to demonstrate these
algorithms on real data. We thus demonstrate
bucket-brigade and on-the-fly backward applica-
tion on a typical NLP task cast as a cascade of
wLNT. We adapt the Japanese-to-English transla-
13Note that G2 is the initial stand-in for MB(MA(G).).,
since G1 is the initial stand-in for MA(G)..
</bodyText>
<figure confidence="0.975944333333333">
−−→ α
w3
g0a0
g0a0
−−−−→ α
w3·w7
</figure>
<page confidence="0.939049">
1064
</page>
<table confidence="0.976206111111111">
method WST wxLNT wLNT
oc x �
bb x �
otf �
method WST wxLT wLT wxLNT wLNT
oc � x x x �
bb � � � � �
otf � � � �
(a) Forward application (b) Backward application
</table>
<tableCaption confidence="0.978727">
Table 2: Transducer types and available methods of forward and backward application of a cascade.
oc = offline composition, bb = bucket brigade, otf = on the fly.
</tableCaption>
<bodyText confidence="0.989912957446809">
tion model of Yamada and Knight (2001) by trans-
forming it from an English-tree-to-Japanese-string
model to an English-tree-to-Japanese-tree model.
The Japanese trees are unlabeled, meaning they
have syntactic structure but all nodes are labeled
“X”. We then cast this modified model as a cas-
cade of LNT tree transducers. Space does not per-
mit a detailed description, but some example rules
are in Figure 3. The rotation transducer R, a sam-
ple of which is in Figure 3a, has 6,453 rules, the
insertion transducer Z, Figure 3b, has 8,122 rules,
and the translation transducer, T, Figure 3c, has
37,311 rules.
We add an English syntax language model L to
the cascade of transducers just described to bet-
ter simulate an actual machine translation decod-
ing task. The language model is cast as an iden-
tity WTT and thus fits naturally into the experimen-
tal framework. In our experiments we try several
different language models to demonstrate varying
performance of the application algorithms. The
most realistic language model is a PCFG. Each
rule captures the probability of a particular se-
quence of child labels given a parent label. This
model has 7,765 rules.
To demonstrate more extreme cases of the use-
fulness of the on-the-fly approach, we build a lan-
guage model that recognizes exactly the 2,087
trees in the training corpus, each with equal
weight. It has 39,455 rules. Finally, to be ultra-
specific, we include a form of the “specific” lan-
guage model just described, but only allow the
English counterpart of the particular Japanese sen-
tence being decoded in the language.
The goal in our experiments is to apply a single
tree t backward through the cascade LoRoZoT ot
and find the 1-best path in the application WRTG.
We evaluate the speed of each approach: bucket
brigade and on-the-fly. The algorithm we use to
obtain the 1-best path is a modification of the k-
best algorithm of Pauls and Klein (2009). Our al-
gorithm finds the 1-best path in a WRTG and ad-
mits an on-the-fly approach.
The results of the experiments are shown in
Table 3. As can be seen, on-the-fly application
is generally faster than the bucket brigade, about
double the speed per sentence in the traditional
</bodyText>
<table confidence="0.999330857142857">
LM type method time/sentence
pcfg bucket 28s
pcfg otf 17s
exact bucket &gt;1m
exact otf 24s
1-sent bucket 2.5s
1-sent otf .06s
</table>
<tableCaption confidence="0.995515">
Table 3: Timing results to obtain 1-best from ap-
</tableCaption>
<bodyText confidence="0.997470966666667">
plication through a weighted tree transducer cas-
cade, using on-the-fly vs. bucket brigade back-
ward application techniques. pcfg = model rec-
ognizes any tree licensed by a pcfg built from
observed data, exact = model recognizes each of
2,000+ trees with equal weight, 1-sent = model
recognizes exactly one tree.
experiment that uses an English PCFG language
model. The results for the other two language
models demonstrate more keenly the potential ad-
vantage that an on-the-fly approach provides—the
simultaneous incorporation of information from
all models allows application to be done more ef-
fectively than if each information source is consid-
ered in sequence. In the “exact” case, where a very
large language model that simply recognizes each
of the 2,087 trees in the training corpus is used,
the final application is so large that it overwhelms
the resources of a 4gb MacBook Pro, while the
on-the-fly approach does not suffer from this prob-
lem. The “1-sent” case is presented to demonstrate
the ripple effect caused by using on-the fly. In the
other two cases, a very large language model gen-
erally overwhelms the timing statistics, regardless
of the method being used. But a language model
that represents exactly one sentence is very small,
and thus the effects of simultaneous inference are
readily apparent—the time to retrieve the 1-best
sentence is reduced by two orders of magnitude in
this experiment.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999937666666667">
We have presented algorithms for forward and
backward application of weighted tree trans-
ducer cascades, including on-the-fly variants, and
demonstrated the benefit of an on-the-fly approach
to application. We note that a more formal ap-
proach to application of WTTs is being developed,
</bodyText>
<page confidence="0.950727">
1065
</page>
<bodyText confidence="0.9980975">
independent from these efforts, by Ful¨op et al.
(2010).
</bodyText>
<sectionHeader confidence="0.995197" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999413625">
We are grateful for extensive discussions with
Andreas Maletti. We also appreciate the in-
sights and advice of David Chiang, Steve De-
Neefe, and others at ISI in the preparation of
this work. Jonathan May and Kevin Knight were
supported by NSF grants IIS-0428020 and IIS-
0904684. Heiko Vogler was supported by DFG
VO 1011/5-1.
</bodyText>
<sectionHeader confidence="0.999036" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999627662337662">
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene’s theorem.
Information Processing Letters, 24(1):1–4.
Brenda S. Baker. 1979. Composition of top-down and
bottom-up tree transductions. Information and Con-
trol, 41(2):186–213.
Zolt´an ´Esik and Werner Kuich. 2003. Formal tree se-
ries. Journal of Automata, Languages and Combi-
natorics, 8(2):219–285.
Zolt´an Ful¨op and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9, pages 313–404.
Springer-Verlag.
Zolt´an Ful¨op, Andreas Maletti, and Heiko Vogler.
2010. Backward and forward application of
weighted extended tree transducers. Unpublished
manuscript.
Ferenc G´ecseg and Magnus Steinby. 1984. Tree Au-
tomata. Akad´emiai Kiad´o, Budapest.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Harry Bunt, Robert Malouf, and Alon
Lavie, editors, Proceedings of the Ninth Interna-
tional Workshop on Parsing Technologies (IWPT),
pages 53–64, Vancouver, October. Association for
Computational Linguistics.
Werner Kuich. 1998. Formal power series over trees.
In Symeon Bozapalidis, editor, Proceedings of the
3rd International Conference on Developments in
Language Theory (DLT), pages 61–101, Thessa-
loniki, Greece. Aristotle University of Thessaloniki.
Werner Kuich. 1999. Tree transducers and formal tree
series. Acta Cybernetica, 14:135–149.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-
down tree transducers. SIAM Journal on Comput-
ing, 39(2):410–430.
Andreas Maletti. 2006. Compositions of tree se-
ries transformations. Theoretical Computer Science,
366:248–271.
Andreas Maletti. 2008. Compositions of extended top-
down tree transducers. Information and Computa-
tion, 206(9–10):1187–1196.
Andreas Maletti. 2009. Personal Communication.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The design principles of a weighted
finite-state transducer library. Theoretical Computer
Science, 231:17–32.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269–312.
Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, chapter 6, pages 213–254. Springer-Verlag.
Adam Pauls and Dan Klein. 2009. K-best A* parsing.
In Keh-Yih Su, Jian Su, Janyce Wiebe, and Haizhou
Li, editors, Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 958–966, Suntec,
Singapore, August. Association for Computational
Linguistics.
Fernando Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In Emmanuel Roche and Yves Schabes, ed-
itors, Finite-State Language Processing, chapter 15,
pages 431–453. MIT Press, Cambridge, MA.
William A. Woods. 1980. Cascaded ATN gram-
mars. American Journal of Computational Linguis-
tics, 6(1):1–12.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523–530, Toulouse,
France, July. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.975096">
1066
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416830">
<title confidence="0.999987">Efficient Inference Through Cascades of Weighted Tree Transducers</title>
<author confidence="0.999979">Jonathan May</author>
<author confidence="0.999979">Kevin Knight</author>
<affiliation confidence="0.999614">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.999273">Marina del Rey, CA 90292</address>
<author confidence="0.971597">Heiko Vogler</author>
<affiliation confidence="0.9790155">Technische Universit¨at Dresden Institut f¨ur Theoretische Informatik</affiliation>
<address confidence="0.999862">01062 Dresden, Germany</address>
<email confidence="0.999142">heiko.vogler@tu-dresden.de</email>
<abstract confidence="0.996132743034056">Weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations. We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). 1 Motivation Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the given some input, which we call foror the set of possible weighted given some output, which we call After application we can do some inon this result, such as determining its highest weighted elements. We may also want to divide up our problems into manageable chunks, each represented by a transducer. As noted by Woods (1980), it is easier for designers to write several small transducers where each performs a simple transformation, rather than painstakingly construct a single complicated device. We would like to know, then, the result of transformation of input or output by transducers, one operating after the other. As we will see, there are various strategies for approaching this problem. We will conbrigade applicaand Application of cascades of weighted string has been well-studied (Mohri, 1997). Less well-studied but of more recent inis application of cascades of weighted We tackle application of cascades in this work, presenting: explicit algorithms for application of cascades • novel algorithms for on-the-fly application of and • experiments comparing the performance of these algorithms. 2 Strategies for the string case we discuss application of it is helpto recall the solution to this problem in the domain. We recall previous formal presentations (Mohri, 1997) and note informally that they may be represented as directed graphs with designated start and end states and edges labeled input symbols, output symbols, and the solution for is practically trivial—we achieve application through a series and operations. Embedding is simply the act of representing a string or regular string language as an iden- Composition of that is, generata single captures the transformations two input used in sequence, is not at all trivial, but has been well covered in, e.g., (Mohri, 2009), where directly implementable algorithms can be found. Finally, projection is another trivial operation—the domain or range language can obtained from a ignoring the output or input symbols, respectively, on its arcs, and summing weights on otherwise identical arcs. By embedding an input, composing the result with the and projecting the result, forward apis We are then left with weighted string acceptor essentially a weighted, labeled graph, which can be traversed assume throughout this paper that weights are in that the weight of a path is calculated as the product of the weights of its edges, and that the weight of a necessarily finite) set paths is calculated as the sum the weights of the paths of backward applications, the roles of input and output are simply exchanged. 1058 of the 48th Annual Meeting of the Association for Computational pages 1058–1066, Sweden, 11-16 July 2010. Association for Computational Linguistics Input string “a a” embedded in an (b) first cascade (c) second cascade (d) Offline composition approach: (e) Bucket brigade approach: (f) Result of offline or bucket application the transducers Apply to after projection (g) Initial on-the-fly (h) On-the-fly stand-in after exploring (i) On-the-fly stand-in after best path has been found stand-in for (f) outgoing edges of state ADF 1: Three different approaches to application through cascades of by well-known algorithms to efficiently find the kbest paths. can be freely composed, extendapplication to operate on a is fairly trivial. The only question is one of composition order: whether to initially compose the cascade into a single transducer (an approach we or to compose the initial embedding with the first transducer, trim useless states, compose the result with the second, and so (an approach we call The appropriate strategy generally depends on the structure of the individual transducers. A third approach builds the result incrementally, as dictated by some algorithm that requests information about it. Such an approach, which we was described in (Pereira and Riley, 1997; Mohri, 2009; Mohri et al., 2000). If we can efficiently calculate the outgoing edges of state of the result demand, without calculating all edges in the entire machine, we can a the result structure, a machine consisting at first of only the start state of the true result. As a calling algorithm (e.g., an implementation of Dijkstra’s algorithm) requests information about the result graph, such as the set of outgoing edges from a state, we replace the current stand-in with a richer version by adding the result of the request. The on-the-fly approach has a distinct advantage over the other two methods in that the entire result graph need not be built. A graphical representation of all three methods is presented in Figure 1. 3 Application of tree transducers Now let us revisit these strategies in the setting of trees and tree transducers. Imagine we have a tree or set of trees as input that can be represented a weighted regular tree and can transform that input with some weight. We would like to know the k-best trees the produce as output for that input, along with their weights. We already know of several for acquiring k-best trees from a (Huang and Chiang, 2005; Pauls and Klein, 2009), so we then must ask if, analogously to the string preserve and we can an application Before we begin, howwe must define and alphabet a finite set that evmember a rank We k set of those that = The set of deis assumed to be disjoint from any ranked alphabet used in this paper. use denote a symbol of rank 0 that is not any ranked alphabet used in this paper. A denoted ... , and ... , For we generates the same class of weighted tree languages weighted tree automata, the direct analogue of and is more useful for our purposes. weighted tree language is recognizable iff it can be represented by a wrtg. following formal definitions and notations are needed for understanding and reimplementation of the presented algorithms, but can be safely skipped on first reading and consulted when encountering an unfamiliar term. 1059 shorthand for For every set from let = where, for = define the a tree ... , for ... , as a set such that = ∪ v set of leaf positions those that for no We presume standard lexicographic s The position denoted by the denoted by and the by are defined as follows: For every = = and = For every ... , that = = = For every = and = ... , a tree the carof its position set. The set a tree the set of labels of its leaves: for a tree sets. Let a mapping. We extend the mapping that for for and ... , ... , ... , We indisuch extensions by describing a substimapping then using further comment. use denote the set denote 3.1 (Alexandrakis and Bozapa- A weighted regular tree grammar is a 4-tuple P, N a finite set of with the start nonterminal. E a ranked alphabet of input symbols, where P a tuple where is a finite set each production the form and → is a weight function of the productions. We will to a finite set of weighted produceach production the form production a production it is the form where w (Alexandrakis and Bozapalidis, 1987), chain productions are forbidden in order to avoid infinite summations. We explicitly allow such summations. in form each production is either a chain production or is of the ... , and ... , P, and the form we a step replacing leaf nonterminal in Fort there exists some that = = We say this step is for all ∈ &lt; We henceforth assume all derivation steps are leftmost. If, for and all we say sequence ... , a in that the weight of = The weighted language recognized by the mapping that for every is the sum of the weights of all (possibly infinitely derivations of A weighted tree lanthere is a that define a partial ordering that for we say where the weights are preserved. Definition 3.2 (cf. Def. 1 of (Maletti, 2008)) A weighted extended top-down tree transducer is a 5-tuple R, Q a finite set of states. E the ranked alphabets of input and output symbols, respectively, where R a tuple where is a finite set each rule the form and further require that no variable apmore than once in and that each variappearing in also in Moreover, → a weight function of the As for we refer to a finite of weighted rules, each rule the form for each rule the form each at most once (respecat least once) in We denote the class all as wxT and add the letters L and N to signify the subclasses of linear and nondeleting respectively. Additionally, if of the form ... , we remove the letter “x” to signify q.y 1060 the transducer is not extended (i.e., it is a “tradiand Vogler, 2009)). R, and the form we obtain step replacing some of with a tree matching a of where each instance of a variable has been replaced by a corresponding subtree the tree. Formally, there a position a substitution mapping and a rule that = where substitution mapping such that = all We say this derivation step for all E We henceforth assume all derivation are leftmost. If, for some and all &lt;_ we say the sequence ... , a weight of = · tree transformation by the mapping such for every the sum of the weights of all (possibly infinitely many) of The two tree transformations the weighted tree transfor every = · 3.2 Applicable classes We now consider transducer classes where recognizability is preserved under application. Table 1 presents known results for the top-down tree transducer classes described in Section 3.1. Unlike the string case, preservation of recognizability is not universal or symmetric. This is important for because we can only construct an i.e., a the result of application, if we can ensure that the language generated by application is in fact recognizable. Of the types under consideration, only wxLNT and wLNT preserve forward recognizability. The two classes marked as open questions and the other classes, which are superclasses of wNT, do not or are presumed not to. All subclasses of wxLT prebackward do not consider cases where recognizability is not preserved the remainder of this paper. If a transducer of a class that preserves forward recognizability is to a we can call the forward ap-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Athanasios Alexandrakis</author>
<author>Symeon Bozapalidis</author>
</authors>
<title>Weighted grammars and Kleene’s theorem.</title>
<date>1987</date>
<journal>Information Processing Letters,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="9743" citStr="Alexandrakis and Bozapalidis, 1987" startWordPosition="1721" endWordPosition="1725">size(t) is |pos(t)|, the cardinality of its position set. The yield set of a tree is the set of labels of its leaves: for a tree t, yd (t) = {t(v) |v ∈ lv(t)}. Let A and B be sets. Let co : A → TΣ(B) be a mapping. We extend co to the mapping co : TΣ(A) → TΣ(B) such that for a ∈ A, co(a) = co(a) and for k ≥ 0, Q ∈ E(k), and t1, ... , tk ∈ TΣ(A), co(Q(t1, ... , tk)) = Q(co(t1), ... , co(tk)). We indicate such extensions by describing co as a substitution mapping and then using co without further comment. We use R+ to denote the set {w ∈ R |w ≥ 0} and R +to denote R+ ∪ {+∞}. Definition 3.1 (cf. (Alexandrakis and Bozapalidis, 1987)) A weighted regular tree grammar (WRTG) is a 4-tuple G = (N, E, P, n0) where: 1. N is a finite set of nonterminals, with n0 ∈ N the start nonterminal. 2. E is a ranked alphabet of input symbols, where E ∩ N = ∅. 3. P is a tuple (P&apos;, 7r), where P&apos; is a finite set of productions, each production p of the form n →− u, n ∈ N, u ∈ TΣ(N), and 7r : P&apos; → R+ is a weight function of the productions. We will refer to P as a finite set of weighted productions, each production p of the form nπ(p)→ u. A production p is a chain production if it is of the form ni −→ nj, where ni, nj ∈ N.6 w 6In (Alexandrakis</context>
<context position="18106" citStr="Alexandrakis and Bozapalidis, 1987" startWordPosition="3407" endWordPosition="3410">right side of a single rule in the first transducer. Since the embedded WRTG is of type wLNT, it may be either the first or second argument provided to Algorithm 1, depending on whether the application is forward or backward. We can thus use the embed-compose-project strategy for forward application of wLNT and backward application of wxLT and wxLNT. Note that we cannot use this strategy for forward applica8Without loss of generality we assume this is so, since standard algorithms exist to remove chain productions (Kuich, 1998; ´Esik and Kuich, 2003; Mohri, 2009) and convert into normal form (Alexandrakis and Bozapalidis, 1987). 9Finitely many such productions may be formed. 1061 tion of wxLNT, even though that class preserves recognizability. Algorithm 1 COMPOSE 1: inputs 2: wxLT M1 = (Q1, Σ, Δ, R1, q10) 3: wLNT M2 = (Q2, Δ, Γ, R2, q20) 4: outputs 5: wxLT M3 = ((Q1 ×Q2), Σ, Γ, R3, (q10, q20)) such that M3 = (τM1; τM2). 6: complexity 7: O(|R1 |max(|R2|9ize(˜u), |Q2|)), where u˜ is the largest right side tree in any rule in R1 8: Let R3 be of the form (R&apos;3, π) 9: R3 ← (∅, ∅) 10: Ξ ← {(q10, q20)} {seen states} 11: Ψ ← {(q10, q20)} {pending states} 12: while Ψ =6 ∅ do 13: (q1, q2) ←any element of Ψ 14: Ψ ← Ψ \ {(q1, q2</context>
<context position="26997" citStr="Alexandrakis and Bozapalidis, 1987" startWordPosition="5226" endWordPosition="5229">g0a0 −−−−→ α g1a1 (d) Productions of MA(G). built as a consequence of building the complete MB(MA(G).). g0a0b0 g0a0b0 w1·w4·w9 −−−−−−→ σ(g0a0b0, g1a1b0) w2·w6·w10 w3·w7·w10 g0a0b0 −−−−−−−→ α g1a1b0 −−−−−−−→ α (e) Complete MB(MA(G).). Figure 2: Forward application through a cascade of tree transducers using an on-the-fly method. tion in an on-the-fly manner.11 It makes calls to REPLACE, which is presented in Algorithm 4, as well as to a NORM algorithm that ensures normal form by replacing a single production not in normal form with several normal-form productions that can be combined together (Alexandrakis and Bozapalidis, 1987) and a CHAIN-REM algorithm that replaces a WRTG containing chain productions with an equivalent WRTG that does not (Mohri, 2009). As an example of stand-in construction, consider the invocation PRODUCE(G1, g0a0), where G1 = ({g0a0}, ta, o, a, p}, ∅, g0a0, MA, G), G is in Figure 2a,1 and MA is in 2b. The stand-in WRTG that is output contains the first three of the four productions in Figure 2d. To demonstrate the use of on-the-fly application in a cascade, we next show the effect of PRODUCE when used with the cascade G ◦ MA ◦ MB, where MB is in Figure 2c. Our driving algorithm in this case is A</context>
</contexts>
<marker>Alexandrakis, Bozapalidis, 1987</marker>
<rawString>Athanasios Alexandrakis and Symeon Bozapalidis. 1987. Weighted grammars and Kleene’s theorem. Information Processing Letters, 24(1):1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brenda S Baker</author>
</authors>
<title>Composition of top-down and bottom-up tree transductions.</title>
<date>1979</date>
<journal>Information and Control,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="17367" citStr="Baker (1979)" startWordPosition="3286" endWordPosition="3287">ir associated states from the right side. So, the rule q1.σ(γ(x1), x2) w�—* δ(q2.x2, β(α, q3.x1)) would yield the production q1 �—* σ(γ(q3), q2) in w the domain projection. However, a deleting rule such as q1.σ(x1, x2) w�—* γ(q2.x2) necessitates the introduction of a new nonterminal L that can generate all of TE with weight 1. The only missing piece in our embed-composeproject strategy is composition. Algorithm 1, which is based on the declarative construction of Maletti (2006), generates the syntactic composition of a wxLT and a wLNT, a generalization of the basic composition construction of Baker (1979). It calls Algorithm 2, which determines the sequences of rules in the second transducer that match the right side of a single rule in the first transducer. Since the embedded WRTG is of type wLNT, it may be either the first or second argument provided to Algorithm 1, depending on whether the application is forward or backward. We can thus use the embed-compose-project strategy for forward application of wLNT and backward application of wxLT and wxLNT. Note that we cannot use this strategy for forward applica8Without loss of generality we assume this is so, since standard algorithms exist to r</context>
<context position="19454" citStr="Baker, 1979" startWordPosition="3687" endWordPosition="3688">q ∈6 Ξ then 19: Ξ ← Ξ ∪ {q} 20: Ψ ← Ψ ∪ {q} 21: r ← ((q1, q2).y →− z) 22: R&apos;3 ← R&apos;3 ∪ {r} 23: π(r) ← π(r) + (w1 · w2) 24: return M3 4 Application of tree transducer cascades What about the case of an input WRTG and a cascade of tree transducers? We will revisit the three strategies for accomplishing application discussed above for the string case. In order for offline composition to be a viable strategy, the transducers in the cascade must be closed under composition. Unfortunately, of the classes that preserve recognizability, only wLNT is closed under composition (G´ecseg and Steinby, 1984; Baker, 1979; Maletti et al., 2009; F¨ul¨op and Vogler, 2009). However, the general lack of composability of tree transducers does not preclude us from conducting forward application of a cascade. We revisit the bucket brigade approach, which in Section 2 appeared to be little more than a choice of composition order. As discussed previously, application of a single transducer involves an embedding, a composition, and a projection. The embedded WRTG is in the class wLNT, and the projection forms another WRTG. As long as every transducer in the cascade can be composed with a wLNT to its left or right, depen</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Brenda S. Baker. 1979. Composition of top-down and bottom-up tree transductions. Information and Control, 41(2):186–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an ´Esik</author>
<author>Werner Kuich</author>
</authors>
<title>Formal tree series.</title>
<date>2003</date>
<journal>Journal of Automata, Languages and Combinatorics,</journal>
<volume>8</volume>
<issue>2</issue>
<marker>´Esik, Kuich, 2003</marker>
<rawString>Zolt´an ´Esik and Werner Kuich. 2003. Formal tree series. Journal of Automata, Languages and Combinatorics, 8(2):219–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an Ful¨op</author>
<author>Heiko Vogler</author>
</authors>
<title>Weighted tree automata and tree transducers.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, chapter 9,</booktitle>
<pages>313--404</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<marker>Ful¨op, Vogler, 2009</marker>
<rawString>Zolt´an Ful¨op and Heiko Vogler. 2009. Weighted tree automata and tree transducers. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, chapter 9, pages 313–404. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an Ful¨op</author>
<author>Andreas Maletti</author>
<author>Heiko Vogler</author>
</authors>
<title>Backward and forward application of weighted extended tree transducers.</title>
<date>2010</date>
<note>Unpublished manuscript.</note>
<marker>Ful¨op, Maletti, Vogler, 2010</marker>
<rawString>Zolt´an Ful¨op, Andreas Maletti, and Heiko Vogler. 2010. Backward and forward application of weighted extended tree transducers. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc G´ecseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree Automata. Akad´emiai Kiad´o,</title>
<date>1984</date>
<location>Budapest.</location>
<marker>G´ecseg, Steinby, 1984</marker>
<rawString>Ferenc G´ecseg and Magnus Steinby. 1984. Tree Automata. Akad´emiai Kiad´o, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>53--64</pages>
<editor>In Harry Bunt, Robert Malouf, and Alon Lavie, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Vancouver,</location>
<contexts>
<context position="7006" citStr="Huang and Chiang, 2005" startWordPosition="1124" endWordPosition="1127">in that the entire result graph need not be built. A graphical representation of all three methods is presented in Figure 1. 3 Application of tree transducers Now let us revisit these strategies in the setting of trees and tree transducers. Imagine we have a tree or set of trees as input that can be represented as a weighted regular tree grammar3 (WRTG) and a WTT that can transform that input with some weight. We would like to know the k-best trees the WTT can produce as output for that input, along with their weights. We already know of several methods for acquiring k-best trees from a WRTG (Huang and Chiang, 2005; Pauls and Klein, 2009), so we then must ask if, analogously to the string case, WTTs preserve recognizability4 and we can form an application WRTG. Before we begin, however, we must define WTTs and WRTGs. 3.1 Preliminaries5 A ranked alphabet is a finite set E such that every member Q E E has a rank rk(Q) E N. We call E(k) C_ E, k E N the set of those Q E E such that rk(Q) = k. The set of variables is denoted X = {x1, x2,...I and is assumed to be disjoint from any ranked alphabet used in this paper. We use L to denote a symbol of rank 0 that is not in any ranked alphabet used in this paper. A</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Harry Bunt, Robert Malouf, and Alon Lavie, editors, Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 53–64, Vancouver, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Kuich</author>
</authors>
<title>Formal power series over trees.</title>
<date>1998</date>
<booktitle>Proceedings of the 3rd International Conference on Developments in Language Theory (DLT),</booktitle>
<pages>61--101</pages>
<editor>In Symeon Bozapalidis, editor,</editor>
<institution>Thessaloniki, Greece. Aristotle University of Thessaloniki.</institution>
<contexts>
<context position="18003" citStr="Kuich, 1998" startWordPosition="3393" endWordPosition="3394">hich determines the sequences of rules in the second transducer that match the right side of a single rule in the first transducer. Since the embedded WRTG is of type wLNT, it may be either the first or second argument provided to Algorithm 1, depending on whether the application is forward or backward. We can thus use the embed-compose-project strategy for forward application of wLNT and backward application of wxLT and wxLNT. Note that we cannot use this strategy for forward applica8Without loss of generality we assume this is so, since standard algorithms exist to remove chain productions (Kuich, 1998; ´Esik and Kuich, 2003; Mohri, 2009) and convert into normal form (Alexandrakis and Bozapalidis, 1987). 9Finitely many such productions may be formed. 1061 tion of wxLNT, even though that class preserves recognizability. Algorithm 1 COMPOSE 1: inputs 2: wxLT M1 = (Q1, Σ, Δ, R1, q10) 3: wLNT M2 = (Q2, Δ, Γ, R2, q20) 4: outputs 5: wxLT M3 = ((Q1 ×Q2), Σ, Γ, R3, (q10, q20)) such that M3 = (τM1; τM2). 6: complexity 7: O(|R1 |max(|R2|9ize(˜u), |Q2|)), where u˜ is the largest right side tree in any rule in R1 8: Let R3 be of the form (R&apos;3, π) 9: R3 ← (∅, ∅) 10: Ξ ← {(q10, q20)} {seen states} 11: Ψ </context>
</contexts>
<marker>Kuich, 1998</marker>
<rawString>Werner Kuich. 1998. Formal power series over trees. In Symeon Bozapalidis, editor, Proceedings of the 3rd International Conference on Developments in Language Theory (DLT), pages 61–101, Thessaloniki, Greece. Aristotle University of Thessaloniki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Kuich</author>
</authors>
<title>Tree transducers and formal tree series.</title>
<date>1999</date>
<journal>Acta Cybernetica,</journal>
<pages>14--135</pages>
<contexts>
<context position="22554" citStr="Kuich, 1999" startWordPosition="4307" endWordPosition="4308">-fly approach we describe algorithms that only generate those productions in a WRTG that have a given left nonterminal. In this section we extend Definition 3.1 as follows: a WRTG is a 6- tuple G = (N, E, P, no, M, G) where N, E, P, and no are defined as in Definition 3.1, and either M = G = 0,10 or M is a wxLNT and G is a normal form, chain production-free WRTG such that 10In which case the definition is functionally unchanged from before. 1062 type preserved? source w[x]T No See w[x]NT w[x]LT OQ (Maletti, 2009) w[x]NT No (G´ecseg and Steinby, 1984) wxLNT Yes (F¨ul¨op et al., 2010) wLNT Yes (Kuich, 1999) type preserved? source w[x]T No See w[x]NT w[x]LT Yes (F¨ul¨op et al., 2010) w[x]NT No (Maletti, 2009) w[x]LNT Yes See w[x]LT (a) Preservation of forward recognizability (b) Preservation of backward recognizability Table 1: Preservation of forward and backward recognizability for various classes of top-down tree transducers. Here and elsewhere, the following abbreviations apply: w = weighted, x = extended LHS, L = linear, N = nondeleting, OQ = open question. Square brackets include a superposition of classes. For example, w[x]T signifies both wxT and wT. Algorithm 3 PRODUCE 1: inputs 2: WRTG </context>
</contexts>
<marker>Kuich, 1999</marker>
<rawString>Werner Kuich. 1999. Tree transducers and formal tree series. Acta Cybernetica, 14:135–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
<author>Jonathan Graehl</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
</authors>
<title>The power of extended topdown tree transducers.</title>
<date>2009</date>
<journal>SIAM Journal on Computing,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="19476" citStr="Maletti et al., 2009" startWordPosition="3689" endWordPosition="3692">9: Ξ ← Ξ ∪ {q} 20: Ψ ← Ψ ∪ {q} 21: r ← ((q1, q2).y →− z) 22: R&apos;3 ← R&apos;3 ∪ {r} 23: π(r) ← π(r) + (w1 · w2) 24: return M3 4 Application of tree transducer cascades What about the case of an input WRTG and a cascade of tree transducers? We will revisit the three strategies for accomplishing application discussed above for the string case. In order for offline composition to be a viable strategy, the transducers in the cascade must be closed under composition. Unfortunately, of the classes that preserve recognizability, only wLNT is closed under composition (G´ecseg and Steinby, 1984; Baker, 1979; Maletti et al., 2009; F¨ul¨op and Vogler, 2009). However, the general lack of composability of tree transducers does not preclude us from conducting forward application of a cascade. We revisit the bucket brigade approach, which in Section 2 appeared to be little more than a choice of composition order. As discussed previously, application of a single transducer involves an embedding, a composition, and a projection. The embedded WRTG is in the class wLNT, and the projection forms another WRTG. As long as every transducer in the cascade can be composed with a wLNT to its left or right, depending on the applicatio</context>
</contexts>
<marker>Maletti, Graehl, Hopkins, Knight, 2009</marker>
<rawString>Andreas Maletti, Jonathan Graehl, Mark Hopkins, and Kevin Knight. 2009. The power of extended topdown tree transducers. SIAM Journal on Computing, 39(2):410–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Compositions of tree series transformations.</title>
<date>2006</date>
<journal>Theoretical Computer Science,</journal>
<pages>366--248</pages>
<contexts>
<context position="17237" citStr="Maletti (2006)" startWordPosition="3265" endWordPosition="3266">projection for wxLT is best explained by way of example. The left side of a rule is preserved, with variables leaves replaced by their associated states from the right side. So, the rule q1.σ(γ(x1), x2) w�—* δ(q2.x2, β(α, q3.x1)) would yield the production q1 �—* σ(γ(q3), q2) in w the domain projection. However, a deleting rule such as q1.σ(x1, x2) w�—* γ(q2.x2) necessitates the introduction of a new nonterminal L that can generate all of TE with weight 1. The only missing piece in our embed-composeproject strategy is composition. Algorithm 1, which is based on the declarative construction of Maletti (2006), generates the syntactic composition of a wxLT and a wLNT, a generalization of the basic composition construction of Baker (1979). It calls Algorithm 2, which determines the sequences of rules in the second transducer that match the right side of a single rule in the first transducer. Since the embedded WRTG is of type wLNT, it may be either the first or second argument provided to Algorithm 1, depending on whether the application is forward or backward. We can thus use the embed-compose-project strategy for forward application of wLNT and backward application of wxLT and wxLNT. Note that we </context>
</contexts>
<marker>Maletti, 2006</marker>
<rawString>Andreas Maletti. 2006. Compositions of tree series transformations. Theoretical Computer Science, 366:248–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Compositions of extended topdown tree transducers.</title>
<date>2008</date>
<journal>Information and Computation,</journal>
<pages>206--9</pages>
<contexts>
<context position="11763" citStr="Maletti, 2008" startWordPosition="2185" endWordPosition="2186"> ... , pm) is a derivation of tm in G and that n0 ⇒* tm; the weight of d is wt(d) = 7r(p1) · ... · 7r(pm). The weighted tree language recognized by G is the mapping LG : TΣ → R+ such that for every t ∈ TΣ, LG(t) is the sum of the weights of all (possibly infinitely many) derivations of t in G. A weighted tree language f : TΣ → R+ is recognizable if there is a WRTG G such that f = LG. We define a partial ordering :� on WRTGs such that for WRTGs G1 = (N1, E, P1, n0) and G2 = (N2, E, P2, n0), we say G1 -� G2 iff N1 ⊆ N2 and P1 ⊆ P2, where the weights are preserved. Definition 3.2 (cf. Def. 1 of (Maletti, 2008)) A weighted extended top-down tree transducer (WXTT) is a 5-tuple M = (Q, E, A, R, q0) where: 1. Q is a finite set of states. 2. E and A are the ranked alphabets of input and output symbols, respectively, where (E ∪ A) ∩ Q = ∅. 3. R is a tuple (R&apos;, 7r), where R&apos; is a finite set of rules, each rule r of the form q.y →− u for q ∈ Q, y ∈ TΣ(X), and u ∈ TΔ(Q × X). We further require that no variable x ∈ X appears more than once in y, and that each variable appearing in u is also in y. Moreover, 7r : R&apos; → R+ is a weight function of the rules. As for WRTGs, we refer to R as a finite set of weighted</context>
</contexts>
<marker>Maletti, 2008</marker>
<rawString>Andreas Maletti. 2008. Compositions of extended topdown tree transducers. Information and Computation, 206(9–10):1187–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Personal Communication. Mehryar Mohri, Fernando</title>
<date>2009</date>
<journal>Theoretical Computer Science,</journal>
<pages>231--17</pages>
<contexts>
<context position="22460" citStr="Maletti, 2009" startWordPosition="4291" endWordPosition="4292">es an entire application WRTG before any inference algorithm is run. In order to admit an on-the-fly approach we describe algorithms that only generate those productions in a WRTG that have a given left nonterminal. In this section we extend Definition 3.1 as follows: a WRTG is a 6- tuple G = (N, E, P, no, M, G) where N, E, P, and no are defined as in Definition 3.1, and either M = G = 0,10 or M is a wxLNT and G is a normal form, chain production-free WRTG such that 10In which case the definition is functionally unchanged from before. 1062 type preserved? source w[x]T No See w[x]NT w[x]LT OQ (Maletti, 2009) w[x]NT No (G´ecseg and Steinby, 1984) wxLNT Yes (F¨ul¨op et al., 2010) wLNT Yes (Kuich, 1999) type preserved? source w[x]T No See w[x]NT w[x]LT Yes (F¨ul¨op et al., 2010) w[x]NT No (Maletti, 2009) w[x]LNT Yes See w[x]LT (a) Preservation of forward recognizability (b) Preservation of backward recognizability Table 1: Preservation of forward and backward recognizability for various classes of top-down tree transducers. Here and elsewhere, the following abbreviations apply: w = weighted, x = extended LHS, L = linear, N = nondeleting, OQ = open question. Square brackets include a superposition of</context>
</contexts>
<marker>Maletti, 2009</marker>
<rawString>Andreas Maletti. 2009. Personal Communication. Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2000. The design principles of a weighted finite-state transducer library. Theoretical Computer Science, 231:17–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="1067" citStr="Mohri, 1997" startWordPosition="142" endWordPosition="143"> language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations. We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). 1 Motivation Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward application. After application we can do some inference on this result, such as determining its k highest weighted elements. We may also want to divide up our problems into manageable chunks, each represented by a transducer. As noted by Woods (1980), it is easier for designers to write several small transduc</context>
<context position="2747" citStr="Mohri, 1997" startWordPosition="413" endWordPosition="414">ducers (WSTs) has been well-studied (Mohri, 1997). Less well-studied but of more recent interest is application of cascades of weighted tree transducers (WTTs). We tackle application of WTT cascades in this work, presenting: • explicit algorithms for application of WTT cascades • novel algorithms for on-the-fly application of WTT cascades, and • experiments comparing the performance of these algorithms. 2 Strategies for the string case Before we discuss application of WTTs, it is helpful to recall the solution to this problem in the WST domain. We recall previous formal presentations of WSTs (Mohri, 1997) and note informally that they may be represented as directed graphs with designated start and end states and edges labeled with input symbols, output symbols, and weights.1 Fortunately, the solution for WSTs is practically trivial—we achieve application through a series of embedding, composition, and projection operations. Embedding is simply the act of representing a string or regular string language as an identity WST. Composition of WSTs, that is, generating a single WST that captures the transformations of two input WSTs used in sequence, is not at all trivial, but has been well covered i</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23(2):269–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted automata algorithms.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, chapter 6,</booktitle>
<pages>213--254</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3369" citStr="Mohri, 2009" startWordPosition="514" endWordPosition="515">informally that they may be represented as directed graphs with designated start and end states and edges labeled with input symbols, output symbols, and weights.1 Fortunately, the solution for WSTs is practically trivial—we achieve application through a series of embedding, composition, and projection operations. Embedding is simply the act of representing a string or regular string language as an identity WST. Composition of WSTs, that is, generating a single WST that captures the transformations of two input WSTs used in sequence, is not at all trivial, but has been well covered in, e.g., (Mohri, 2009), where directly implementable algorithms can be found. Finally, projection is another trivial operation—the domain or range language can be obtained from a WST by ignoring the output or input symbols, respectively, on its arcs, and summing weights on otherwise identical arcs. By embedding an input, composing the result with the given WST, and projecting the result, forward application is accomplished.2 We are then left with a weighted string acceptor (WSA), essentially a weighted, labeled graph, which can be traversed 1We assume throughout this paper that weights are in R+ U {+oo1, that the w</context>
<context position="5757" citStr="Mohri, 2009" startWordPosition="902" endWordPosition="903"> is one of composition order: whether to initially compose the cascade into a single transducer (an approach we call offline composition) or to compose the initial embedding with the first transducer, trim useless states, compose the result with the second, and so on (an approach we call bucket brigade). The appropriate strategy generally depends on the structure of the individual transducers. A third approach builds the result incrementally, as dictated by some algorithm that requests information about it. Such an approach, which we call on-the-fly, was described in (Pereira and Riley, 1997; Mohri, 2009; Mohri et al., 2000). If we can efficiently calculate the outgoing edges of a state of the result WSA on demand, without calculating all edges in the entire machine, we can maintain a stand-in for the result structure, a machine consisting at first of only the start state of the true result. As a calling algorithm (e.g., an implementation of Dijkstra’s algorithm) requests information about the result graph, such as the set of outgoing edges from a state, we replace the current stand-in with a richer version by adding the result of the request. The on-the-fly approach has a distinct advantage </context>
<context position="18040" citStr="Mohri, 2009" startWordPosition="3399" endWordPosition="3400">es in the second transducer that match the right side of a single rule in the first transducer. Since the embedded WRTG is of type wLNT, it may be either the first or second argument provided to Algorithm 1, depending on whether the application is forward or backward. We can thus use the embed-compose-project strategy for forward application of wLNT and backward application of wxLT and wxLNT. Note that we cannot use this strategy for forward applica8Without loss of generality we assume this is so, since standard algorithms exist to remove chain productions (Kuich, 1998; ´Esik and Kuich, 2003; Mohri, 2009) and convert into normal form (Alexandrakis and Bozapalidis, 1987). 9Finitely many such productions may be formed. 1061 tion of wxLNT, even though that class preserves recognizability. Algorithm 1 COMPOSE 1: inputs 2: wxLT M1 = (Q1, Σ, Δ, R1, q10) 3: wLNT M2 = (Q2, Δ, Γ, R2, q20) 4: outputs 5: wxLT M3 = ((Q1 ×Q2), Σ, Γ, R3, (q10, q20)) such that M3 = (τM1; τM2). 6: complexity 7: O(|R1 |max(|R2|9ize(˜u), |Q2|)), where u˜ is the largest right side tree in any rule in R1 8: Let R3 be of the form (R&apos;3, π) 9: R3 ← (∅, ∅) 10: Ξ ← {(q10, q20)} {seen states} 11: Ψ ← {(q10, q20)} {pending states} 12: w</context>
<context position="27125" citStr="Mohri, 2009" startWordPosition="5248" endWordPosition="5249">) w2·w6·w10 w3·w7·w10 g0a0b0 −−−−−−−→ α g1a1b0 −−−−−−−→ α (e) Complete MB(MA(G).). Figure 2: Forward application through a cascade of tree transducers using an on-the-fly method. tion in an on-the-fly manner.11 It makes calls to REPLACE, which is presented in Algorithm 4, as well as to a NORM algorithm that ensures normal form by replacing a single production not in normal form with several normal-form productions that can be combined together (Alexandrakis and Bozapalidis, 1987) and a CHAIN-REM algorithm that replaces a WRTG containing chain productions with an equivalent WRTG that does not (Mohri, 2009). As an example of stand-in construction, consider the invocation PRODUCE(G1, g0a0), where G1 = ({g0a0}, ta, o, a, p}, ∅, g0a0, MA, G), G is in Figure 2a,1 and MA is in 2b. The stand-in WRTG that is output contains the first three of the four productions in Figure 2d. To demonstrate the use of on-the-fly application in a cascade, we next show the effect of PRODUCE when used with the cascade G ◦ MA ◦ MB, where MB is in Figure 2c. Our driving algorithm in this case is Algorithm 5, MAKE11Note further that it allows forward application of class wxLNT, something the embed-compose-project approach d</context>
</contexts>
<marker>Mohri, 2009</marker>
<rawString>Mehryar Mohri. 2009. Weighted automata algorithms. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, chapter 6, pages 213–254. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>K-best A* parsing. In Keh-Yih Su,</title>
<date>2009</date>
<booktitle>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>958--966</pages>
<editor>Su, Janyce Wiebe, and Haizhou Li, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Jian</location>
<contexts>
<context position="7030" citStr="Pauls and Klein, 2009" startWordPosition="1128" endWordPosition="1131">t graph need not be built. A graphical representation of all three methods is presented in Figure 1. 3 Application of tree transducers Now let us revisit these strategies in the setting of trees and tree transducers. Imagine we have a tree or set of trees as input that can be represented as a weighted regular tree grammar3 (WRTG) and a WTT that can transform that input with some weight. We would like to know the k-best trees the WTT can produce as output for that input, along with their weights. We already know of several methods for acquiring k-best trees from a WRTG (Huang and Chiang, 2005; Pauls and Klein, 2009), so we then must ask if, analogously to the string case, WTTs preserve recognizability4 and we can form an application WRTG. Before we begin, however, we must define WTTs and WRTGs. 3.1 Preliminaries5 A ranked alphabet is a finite set E such that every member Q E E has a rank rk(Q) E N. We call E(k) C_ E, k E N the set of those Q E E such that rk(Q) = k. The set of variables is denoted X = {x1, x2,...I and is assumed to be disjoint from any ranked alphabet used in this paper. We use L to denote a symbol of rank 0 that is not in any ranked alphabet used in this paper. A tree t E TE is denoted </context>
<context position="32327" citStr="Pauls and Klein (2009)" startWordPosition="6129" endWordPosition="6132">tly the 2,087 trees in the training corpus, each with equal weight. It has 39,455 rules. Finally, to be ultraspecific, we include a form of the “specific” language model just described, but only allow the English counterpart of the particular Japanese sentence being decoded in the language. The goal in our experiments is to apply a single tree t backward through the cascade LoRoZoT ot and find the 1-best path in the application WRTG. We evaluate the speed of each approach: bucket brigade and on-the-fly. The algorithm we use to obtain the 1-best path is a modification of the kbest algorithm of Pauls and Klein (2009). Our algorithm finds the 1-best path in a WRTG and admits an on-the-fly approach. The results of the experiments are shown in Table 3. As can be seen, on-the-fly application is generally faster than the bucket brigade, about double the speed per sentence in the traditional LM type method time/sentence pcfg bucket 28s pcfg otf 17s exact bucket &gt;1m exact otf 24s 1-sent bucket 2.5s 1-sent otf .06s Table 3: Timing results to obtain 1-best from application through a weighted tree transducer cascade, using on-the-fly vs. bucket brigade backward application techniques. pcfg = model recognizes any tr</context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>Adam Pauls and Dan Klein. 2009. K-best A* parsing. In Keh-Yih Su, Jian Su, Janyce Wiebe, and Haizhou Li, editors, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 958–966, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, chapter 15,</booktitle>
<pages>431--453</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5744" citStr="Pereira and Riley, 1997" startWordPosition="897" endWordPosition="901">rivial. The only question is one of composition order: whether to initially compose the cascade into a single transducer (an approach we call offline composition) or to compose the initial embedding with the first transducer, trim useless states, compose the result with the second, and so on (an approach we call bucket brigade). The appropriate strategy generally depends on the structure of the individual transducers. A third approach builds the result incrementally, as dictated by some algorithm that requests information about it. Such an approach, which we call on-the-fly, was described in (Pereira and Riley, 1997; Mohri, 2009; Mohri et al., 2000). If we can efficiently calculate the outgoing edges of a state of the result WSA on demand, without calculating all edges in the entire machine, we can maintain a stand-in for the result structure, a machine consisting at first of only the start state of the true result. As a calling algorithm (e.g., an implementation of Dijkstra’s algorithm) requests information about the result graph, such as the set of outgoing edges from a state, we replace the current stand-in with a richer version by adding the result of the request. The on-the-fly approach has a distin</context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>Fernando Pereira and Michael Riley. 1997. Speech recognition by composition of weighted finite automata. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, chapter 15, pages 431–453. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Cascaded ATN grammars.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="1607" citStr="Woods (1980)" startWordPosition="233" endWordPosition="234">cers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward application. After application we can do some inference on this result, such as determining its k highest weighted elements. We may also want to divide up our problems into manageable chunks, each represented by a transducer. As noted by Woods (1980), it is easier for designers to write several small transducers where each performs a simple transformation, rather than painstakingly construct a single complicated device. We would like to know, then, the result of transformation of input or output by a cascade of transducers, one operating after the other. As we will see, there are various strategies for approaching this problem. We will consider offline composition, bucket brigade application, and on-the-fly application. Application of cascades of weighted string transducers (WSTs) has been well-studied (Mohri, 1997). Less well-studied but</context>
</contexts>
<marker>Woods, 1980</marker>
<rawString>William A. Woods. 1980. Cascaded ATN grammars. American Journal of Computational Linguistics, 6(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse, France,</location>
<contexts>
<context position="951" citStr="Yamada and Knight, 2001" startWordPosition="123" endWordPosition="126">r@tu-dresden.de Abstract Weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations. We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). 1 Motivation Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward application. After application we can do some inference on this result, such as determining its k highest weighted elements. We may also want to divide up our problems into manageable chunks, ea</context>
<context position="30460" citStr="Yamada and Knight (2001)" startWordPosition="5813" endWordPosition="5816">nd on-the-fly backward application on a typical NLP task cast as a cascade of wLNT. We adapt the Japanese-to-English transla13Note that G2 is the initial stand-in for MB(MA(G).)., since G1 is the initial stand-in for MA(G).. −−→ α w3 g0a0 g0a0 −−−−→ α w3·w7 1064 method WST wxLNT wLNT oc x � bb x � otf � method WST wxLT wLT wxLNT wLNT oc � x x x � bb � � � � � otf � � � � (a) Forward application (b) Backward application Table 2: Transducer types and available methods of forward and backward application of a cascade. oc = offline composition, bb = bucket brigade, otf = on the fly. tion model of Yamada and Knight (2001) by transforming it from an English-tree-to-Japanese-string model to an English-tree-to-Japanese-tree model. The Japanese trees are unlabeled, meaning they have syntactic structure but all nodes are labeled “X”. We then cast this modified model as a cascade of LNT tree transducers. Space does not permit a detailed description, but some example rules are in Figure 3. The rotation transducer R, a sample of which is in Figure 3a, has 6,453 rules, the insertion transducer Z, Figure 3b, has 8,122 rules, and the translation transducer, T, Figure 3c, has 37,311 rules. We add an English syntax languag</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530, Toulouse, France, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>