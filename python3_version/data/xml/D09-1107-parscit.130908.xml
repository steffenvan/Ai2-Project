<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001504">
<title confidence="0.9984095">
Sinuhe — Statistical Machine Translation using a Globally Trained
Conditional Exponential Family Translation Model
</title>
<author confidence="0.991871">
Matti K¨a¨ari¨ainen
</author>
<affiliation confidence="0.996126">
Department of Computer Science
FI-00014 University of Helsinki, Finland
</affiliation>
<email confidence="0.998507">
matti.kaariainen@cs.helsinki.fi
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99973037037037">
We present a new phrase-based con-
ditional exponential family translation
model for statistical machine translation.
The model operates on a feature repre-
sentation in which sentence level transla-
tions are represented by enumerating all
the known phrase level translations that
occur inside them. This makes the model
a good match with the commonly used
phrase extraction heuristics. The model’s
predictions are properly normalized prob-
abilities. In addition, the model automati-
cally takes into account information pro-
vided by phrase overlaps, and does not
suffer from reference translation reacha-
bility problems.
We have implemented an open source
translation system Sinuhe based on the
proposed translation model. Our experi-
ments on Europarl and GigaFrEn corpora
demonstrate that finding the unique MAP
parameters for the model on large scale
data is feasible with simple stochastic gra-
dient methods. Sinuhe is fast and mem-
ory efficient, and the BLEU scores ob-
tained by it are only slightly inferior to
those of Moses.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997915">
In current phrase-based statistical machine transla-
tion systems such as Moses1 (Koehn et al., 2007),
the translation model is defined in terms of phrase
pairs (biphrases) extracted from a bilingual cor-
pus as follows. The corpus is first word-aligned
using a word alignment heuristic (Och and Ney,
</bodyText>
<footnote confidence="0.927457">
1Throughout this paper, we refer to Moses for concrete-
ness, but most of the discussion applies to other standard
phrase-based statistical machine translation systems as well.
</footnote>
<bodyText confidence="0.999875829268293">
2003). The phrase extraction heuristic then ex-
tracts all the biphrases that are compatible with
the word alignment (Och et al., 1999). This way,
each sentence pair may generate any number of
potentially overlapping biphrases. However, when
defining the phrase-based sentence level transla-
tion model, phrase overlaps are explicitly disal-
lowed: The source sentence is segmented into dis-
joint phrases, which are translated independently
using conditional phrase-level translation models
that have been estimated from extracted biphrase
counts.
The disparity between the phrase extraction
heuristic and the use of the extracted biphrases
can be addressed in at least three ways. One
approach is to simply ignore the disparity as is
done, e.g., in Moses. While empirically succes-
ful, this approach is hard to justify theoretically,
and begs the question of whether more principled
methods might lead to better translation results.
The other extensively studied approach is to re-
place the phrase extraction heuristic with a method
that better matches the use of the extracted phrases
(see, e.g., (Marcu and Wong, 2002; DeNero et al.,
2008) and the references therein). While theo-
retically sound, this approach is computationally
challenging both in practice (DeNero et al., 2008)
and in theory (DeNero and Klein, 2008), may suf-
fer from reference reachability problems (DeNero
et al., 2006), and in the end may lead to inferior
translation quality (Koehn et al., 2003).
In this paper, we study a third alternative. We
propose a new translation model that is compati-
ble with the phrase extraction heuristic. The pro-
posed machine learning inspired translation model
takes the form of a conditional exponential family
probability distribution over a feature representa-
tion for word-aligned sentence pairs. The feature
representation represents a word-aligned sentence
pair by essentially enumerating the (multi)set of
biphrases that would have been extracted from it,
</bodyText>
<page confidence="0.965152">
1027
</page>
<note confidence="0.996592">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1027–1036,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999954105263158">
together with the source positions at which the
biphrases occur. The model’s predictions are con-
ditional probabilities for such sets of biphrases
given the source sentence.
The chosen feature representation has many ad-
vantages. Since all word-aligned sentence pairs
can be represented, reference reachability prob-
lems are automatically circumvented. For exam-
ple, if the translation of a sentence consisted solely
of words that do not occur in the phrase table, then
the feature vector for the translation would be the
all zero vector. As the training data receives non-
zero probability, maximum likelihood or maxi-
mum a posteriori (MAP) parameters for the model
can be estimated in a principled way without re-
sorting to pseudo-references. The fact that the
model is not restricted to using disjoint biphrases
means that the information in biphrase overlaps is
automatically taken into account. This may help
in smoothing the model’s predictions on long and
rare phrases, and in enhancing fluency at places
that otherwise would be phrase boundaries. Also,
the model can be extended in a principled way by
introducing additional features (e.g., translations
from a dictionary, biphrases with gaps, biphrases
over POS tags,...).
The proposed model has one parameter per
biphrase feature, so the total number of parame-
ters is easily millions or more. Still, the model
structure is designed so that feature expectations
and related quantities can be computed efficiently
by dynamic programming. It is thus feasible to
compute the gradient of the MAP objective, and
simple gradient ascent can be used to efficiently
find the globally optimal model parameters (with
respect to a suitably scaled Gaussian prior used for
regularization). Exact inference is also possible by
dynamic programming when translations are pre-
dicted by the translation model alone. When other
features like a language model are included, one
needs to resort to beam search type approximate
dynamic programming for decoding.
We have implemented a translation system
called Sinuhe based on the proposed translation
model. The system has been released under the
GPLv3 open source license (K¨a¨ari¨ainen, 2009).
Our experiments on Europarl and GigaFrEn cor-
pora demonstrate that the proposed translation
model scales well to large data, and offers trans-
lation quality that is only slightly worse than that
of the baseline system Moses. In terms of trans-
lation speed, Sinuhe is already clearly better.
The rest of this paper is organized as follows.
After briefly reviewing related work in Section 2,
we describe the proposed translation model in Sec-
tion 3. Finally, experimental results are presented
in Section 4, and conclusions in Section 5.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9996684">
The proposed translation model is strongly influ-
enced by machine learning techniques for solv-
ing sequence prediction tasks, most notably the
work on conditional random fields (Lafferty et al.,
2001). The modelling task in machine translation
is, however, more complicated than sequence la-
belling (not one-to-one, reorderings), so the stan-
dard methods cannot be directly applied here.
The model we propose is also related to standard
phrase-based translation models through the use of
the same phrase-level translation features. How-
ever, the way we use the features is quite different.
There exists a number of discriminative ap-
proaches whose model structure, training crite-
ria, or both, are similar to ours. However, to our
knowledge, none of the other systems operates
directly on biphrase features, scales up to bilin-
gual corpora with millions of sentence pairs, and
achieves translation quality comparable to fully
tuned standard phrase-based systems. The ap-
proach most closely resembling ours is the in-
dependently developed global discriminative log-
linear model based on synchronous context-free
grammars (Blunsom and Osborne, 2008; Blun-
som et al., 2008). The version presented in (Blun-
som and Osborne, 2008) operates on millions of
rule count features analogous to our biphrase fea-
tures, and integrates a language model into train-
ing and decoding. The system can be trained on
tens of thousands of short sentences yielding bet-
ter translations than a baseline system Hiero on
this data. The version presented in (Blunsom et
al., 2008) scales to more than a hundred thousand
short training sentences, but does not integrate a
language model and thus has performance that im-
proves upon Hiero without a language model
only. Both versions deal with derivational ambi-
guity by treating derivations as a latent variables
that are integrated out to get conditional proba-
bilities for translations2. The downside of this
</bodyText>
<footnote confidence="0.9967855">
2In our translation model, coping with multiple deriva-
tions is not needed as there is just one derivation for each
word-aligned sentence pair. However, dealing with alterna-
tive word alignments might be beneficial, though as argued
</footnote>
<page confidence="0.997703">
1028
</page>
<bodyText confidence="0.999490535714286">
is that approximations are needed in computing
the maximum probability translation in decoding,
and also in computing model expectations in train-
ing when a language model is used. In addi-
tion, since the models operate directly on transla-
tions, using probabilistic training criteria for learn-
ing the model parameters is possible only if all
reference translations in the training data can be
generated by the model. In practice, this problem
can be circumvented by discarding the training
sentence pairs with unreachable reference transla-
tions, but this may mean a significant reduction in
the amount of training data (24% in (Blunsom et
al., 2008)).
Another closely related approach is the in-
dependently developed discriminative block bi-
gram prediction model presented in (Tillmann
and Zhang, 2007). This work proposes a global
phrase-based translation model very similar to
ours, but due to computational reasons, resorts to a
localized approximation thereof, and is restricted
to biphrases of length at most two. In (Liang et
al., 2006) a standard phrase-based model is aug-
mented with more than a million features whose
weights are trained discriminatively by a variant
of the perceptron algorithm. Reference reachabil-
ity is again a problem, and the method has not been
scaled up to use biphrase features directly.
</bodyText>
<sectionHeader confidence="0.931836" genericHeader="method">
3 The proposed translation model
</sectionHeader>
<subsectionHeader confidence="0.999625">
3.1 Biphrase extraction
</subsectionHeader>
<bodyText confidence="0.9888623">
The biphrases used in Sinuhe are extracted from
the training data with the Moses phrase extraction
heuristics. The sentence-aligned training corpus S
is first word-aligned by running Giza++ in both
directions and then symmetrizing the alignments.
This maps the original aligned sentence pairs
(x, y) into word-aligned sentence pairs (x, a, y),
where a is a many-to-many alignment between the
words in x and y. Second, using a heuristic pro-
posed in (Och et al., 1999), all the aligned phrase
</bodyText>
<listItem confidence="0.6861035">
pairs (x&apos;, a&apos;, y&apos;) satisfying the following criteria
are extracted: (1) x&apos; and y&apos; consist of consecutive
words of x and y, and both have length at most k,
(2) a&apos; is the alignment between words of x&apos; and y&apos;
induced by a, (3) a&apos; contains at least one link, and
(4) there are no links in a that have just one end in
x&apos; or y&apos;. Each aligned training sentence (x, a, y)
thus generates a number of potentially overlapping
</listItem>
<bodyText confidence="0.994715842105263">
in (DeNero et al., 2006), the ambiguity in word alignment is
less prevalent than in phrase segmentation.
aligned biphrase features (x&apos;, a&apos;, y&apos;). In our exper-
iments, we chose k = 7 which is the default in
Moses. Unlike in Moses, we do not map the
aligned biphrases (x&apos;, a&apos;, y&apos;) back to non-aligned
biphrases (x&apos;, y&apos;).
To reduce the number of extracted biphrases, for
each source phrase x&apos;, only biphrases (x&apos;, a&apos;, y&apos;)
whose occurrence count is among the top K = 20
in the training data are retained (rank ties bro-
ken by including all biphrases with rank equal to
the limit K). For technical reasons related to our
dynamic programming algorithms, we also drop
biphrases whose source phrase begins or ends with
unlinked words. Finally, we drop all biphrases
that occur only once in the training data. This can
be motivated by a leave-one-out argument (cf the
derivation of Good-Turing estimates): Dropping
the biphrases that occur only once in the train-
ing data means that the feature representation for
a training sentence pair (see Section 3.2) contains
only biphrases that occur also in other training ex-
amples. Without the leave-one-out pruning, the
feature vectors for training sentence pairs would
be maximally dense, whereas such feature density
cannot be expected on test data. Our system can
also be used without the leave-one-out pruning,
but according to our preliminary experiments this
has little effect on translation quality. An excep-
tion seems to be morphologically rich languages
with scarce training data on which pruning seems
to reduce translation quality.
All the pruning steps combined reduce the
phrase table size considerably, but in our experi-
ments, millions of biphrases per language pair still
remain (2-4 million for Europarl data and over 95
million for GigaFrEn data).
</bodyText>
<subsectionHeader confidence="0.994056">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.965834">
Our primary feature representation is a binary fea-
ture vector that indicates which aligned biphrases
in the phrase table occur in an aligned sentence
pair and where. More specifically, a source sen-
tence x aligned to a target sentence y by an align-
ment a is represented by a binary feature vector
φ(x, a, y) whose component φ(x, a, y)(x/,a/,y/),i is
1 iff the aligned biphrase (x&apos;, a&apos;, y&apos;) occurs at
source position i in (x, a, y), and 0 otherwise.
Here, (x&apos;, a&apos;, y&apos;) occurs in (x, a, y) at source posi-
tion i iff the phrase extraction process described in
Section 3.1 would have extracted it from (x, a, y)
at source position i.
</bodyText>
<page confidence="0.985376">
1029
</page>
<bodyText confidence="0.999959636363636">
The weights for the aligned biphrases are tied
together by mapping the binary feature vector O
(indexed by pairs of an aligned biphrase and a
source position) to an integral feature vector O˜ (in-
dexed by aligned biphrases only) using the for-
mula ˜O(x0,a0,y0) = Ei O(x0,a0,y0),i. The “real” fea-
tures that drive the translation process are thus the
lowest level binary features O, whereas the higher
level representation O˜ is convenient in defining the
conditional probabilities given by the translation
model.
</bodyText>
<subsectionHeader confidence="0.998406">
3.3 The model
</subsectionHeader>
<bodyText confidence="0.999589555555556">
Instead of modelling the conditional distribution
P(y|x) directly, we model the conditional dis-
tribution P(O(x, a, y)|x) by the following condi-
tional exponential model:
Here, w is a parameter vector with one component
for each aligned biphrase feature in the phrase ta-
ble. The set Φx defines the set of possible predic-
tions given x, and includes all feature vectors O
satisfying the following criteria:
</bodyText>
<listItem confidence="0.867531">
1. There exists a translation y0 and an alignment
a0 such that all active features in O occur in
(x, a0, y0)
2. Features corresponding to aligned biphrases
that occur inside aligned biphrases whose
features are active in O are also active in O.
</listItem>
<bodyText confidence="0.9988520625">
Thus, the set Φx has a feature representation for
all possible aligned sentence pairs (x, a0, y0) that
have x as the source side, so all reference trans-
lations y0 word-aligned to x in any way a0 are
representable by features in Φx. By condition
1, the predictions given by the model never con-
tain conflicting biphrases, so given any prediction
of the model, there always exists a translation y0
where all the predicted biphrases do occur. How-
ever, since our dynamic programming algorithms
can only force active super-phrases implying ac-
tive sub-phrases (condition 2) but not active sub-
phrases implying active super-phrases, the set Φx
also contains some feature vectors in which the lat-
ter type of implications are not enforced. Having
such redundant representations for some transla-
tions is a waste of probability mass, but we hope it
has little effect in practice.
The choice of modelling P(O(x, a, y)|x) in-
stead of modelling P(y|x) directly is crucial, both
from a modelling and from a computational per-
spective. From the modelling perspective, the cru-
cial point is that in our approach, any aligned sen-
tence pair (x, a, y) has an associated feature vec-
tor O(x, a, y) ∈ Φx that is reachable (i.e., re-
ceives non-zero probability) by the model. This
means it is straightforward to use probabilistic cri-
teria in learning the model parameters. In con-
trast, systems modelling P(y|x) directly are often
plagued by the reference reachability problem. To
use probabilistic training criteria for such systems
one needs to circumvent the reference reachability
problem, e.g., by using pseudo-references or by
dropping out the non-reachable portion of training
data.
Working with the feature vectors O(x, a, y) in-
stead of working with a and y directly means that
we model the ordering and choice of words in y
only partially. This way, when computing the nor-
malizing constants and feature expectations, we
can partition the unbounded set of potential trans-
lations y and alignments a into a smaller set of
equivalence classes given by O(x, a, y). Though
the number of feature vectors O ∈ Φx may be
large (exponential in length of x), all the necessary
computations can be done exactly and efficiently
by dynamic programming. For more details, see
Section 3.4.3.
</bodyText>
<subsectionHeader confidence="0.9883">
3.4 Learning the model parameters
3.4.1 The objective
</subsectionHeader>
<bodyText confidence="0.998682388888889">
We use maximum a posteriori (MAP) estimation
to estimate the model parameters w. To con-
trol overfitting, we regularize the parameters by a
suitably scaled Gaussian prior. This can be also
viewed as L2 regularization. The prior guarantees
that the MAP parameters are unique, and mod-
els our belief that the observed feature occurrence
counts randomly deviate from their “true” values
roughly proportionally to the standard deviations
of the occurrence count distributions. The prior
variance σ2(x0,a0,y0) for feature (x0, a0, y0) is given
by the formula σ2a0 y0) = α/ ρ(x0 a 0 y0) , where
α &gt; 0 is a free regularization parameter, and
ρ2(x0,a0,y0) is an empirical estimate of the variance
of the occurrence count of (x0, a0, y0) in the train-
ing data. This is similar to (Chen and Rosenfeld,
2000), except that we use standard deviations in
place of variances. As the estimate for the vari-
</bodyText>
<equation confidence="0.964649333333333">
exp(w ·
P (O(x, a, y)|x) =
E
φ∈Φx exp(w ·
˜O(x, a, y))
˜O) .
</equation>
<page confidence="0.865187">
1030
</page>
<bodyText confidence="0.997944461538461">
ance of a feature we use the occurrence count of
the corresponding biphrase in the training data.
This could be justified by assuming that the oc-
currence counts follow a Poisson distribution. We
have also run preliminary experiments with other
forms of regularization (different ways of comput-
ing σ(x0,a0,y0), exponential priors corresponding to
L1 regularization, no regularization), and it looks
like the system is not very sensitive to the chosen
prior.
Combining the prior with the model, we see that
the negative log-posterior L(w) is given by the for-
mula
</bodyText>
<table confidence="0.853819571428571">
X 2 log P(φ(x, a, y)|x) + C,
(x0,a0,y0) w(0 0 y0)
x,a,
2
2σ(x0,a0, y0)
X−
(x,a,y)ES
</table>
<bodyText confidence="0.999972">
where the sum over (x&apos;, a&apos;, y&apos;) is understood to
go over all aligned biphrase features in the model.
This is our criterion for learning w.
</bodyText>
<subsectionHeader confidence="0.747782">
3.4.2 Optimization
</subsectionHeader>
<bodyText confidence="0.978560611111111">
We solve the optimization problem related to
learning w by first order gradient ascent methods.
The gradient ∇L(w) of L(w) with respect to w
can be written as
P(x, a, y)−Ew[˜φ|x]],
where Ew[˜φ|x] denotes the conditional expecta-
tion of the aligned biphrase occurrence count fea-
tures given x with respect to model parameters w.
Feature expectations can be computed by combin-
ing the results of a left-to-right and right-to-left
dynamic programming sweep over the source sen-
tence. For more details, see Section 3.4.3.
Inspired by the empirical results in (Vish-
wanathan et al., 2006), we use classic stochas-
tic gradient ascent to solve the optimization prob-
lem. At each step t, we sample with replacement
a batch St of b examples from S. We start from
w0 = 0, and use the update rule
</bodyText>
<equation confidence="0.997281">
wt+1 = wt − ηt∇Lt(wt), (1)
</equation>
<bodyText confidence="0.9899805">
where ηt &gt; 0 is the learning rate, and ∇Lt(w)
is the stochastic gradient of the negative log-
</bodyText>
<equation confidence="0.850923666666667">
posterior log P(φ(x, a, y)|x)
w2 i
Lt(w) = |St |X
|S |2σ2
i i X−
(x,a,y)ESt
</equation>
<bodyText confidence="0.99989940625">
restricted to batch St. The second term of the
stochastic gradient involves only biphrases whose
source sides match the source sentences in the
batch. Though the gradient of the regularizer is
non-zero for all non-zero biphrase features, the
updates of features that are not active in the sec-
ond term of the gradient can be postponed until
they become active again. Due to feature sparsity,
the number of features that are active in a small
batch is small, and thus also the updates are sparse.
Hence, it is possible to handle even feature vectors
that do not fit into memory.
Another advantage of the stochastic gradient
method is that many processes can apply updates
(1) to a weight vector asynchronously in parallel.
We have implemented two strategies for dealing
with this. The simpler one is to store the weight
vector in a database that takes care of the neces-
sary concurrency control. This way, no process
needs to store the entire weight vector in memory.
The downside is that all training processes must
be able to mmap() to the common file-system
due to limitations in the underlying Berkeley DB
database system. We have also implemented a
client-server architecture in which a server process
stores w in memory and manages read and update
requests to its components that come from train-
ing clients. In this approach, the degree of paral-
lelism is limited only by the number of available
machines and server capacity. The server could
be further distributed for managing models that do
not fit into the memory of a single server.
</bodyText>
<subsectionHeader confidence="0.960751">
3.4.3 Computing gradients etc
</subsectionHeader>
<bodyText confidence="0.999938454545454">
The computationally most challenging part in
learning the model parameters is computing
∇ log P(φ(x, a, y)|x), i.e., the vector of differ-
ences between the observed occurrence counts of
biphrase features in (x, a, y) and their conditional
expectations under the current model parameters.
The conditional feature expectations can be
computed by a dynamic programming procedure
similar to the one used in training conditional ran-
dom fields. We combine the results of a left-
to-right and right-to-left dynamic programming
</bodyText>
<equation confidence="0.778672">
X w(x0,a0,y0) X−
(x0,a0,y0) σ2 (x,a,y)ES
(x0,a0,y0)
</equation>
<page confidence="0.963827">
1031
</page>
<bodyText confidence="0.999418363636364">
sweep over x. In the left-to-right sweep, we
have for each biphrase feature (x&apos;, a&apos;, y&apos;), i a state
s(x0,a0,y0),i for translations starting from the be-
ginning of x and ending in an occurrence of the
biphrase (x&apos;, a&apos;, y&apos;) at source position i. This state
records the contribution of all partial translations
whose right-most active biphrase feature on the
source side is (x&apos;, a&apos;, y&apos;), i to the conditional ex-
pectation of feature (x&apos;, a&apos;, y&apos;), i (in log scale).
The score for sempty,0 = 0, and s(x0,a0,y0),i is ob-
tained from the recurrence
</bodyText>
<equation confidence="0.9985412">
score (s(x0,a0,y0),i) =
�
(x00,a00,y00),i00EA((x0,a0,y0),i)
+ � iw(x000,a000,y000),i000
(x000,a000,y000),i000EB
</equation>
<bodyText confidence="0.99459625">
Here, A((x&apos;, a&apos;, y&apos;), i) is the set of prede-
cessor states of s(x0,a0,y0),i and includes all
states s(x00,a00,y00),i00 such that a proper suffix of
(x&apos;&apos;, a&apos;&apos;, y&apos;&apos;), i&apos;&apos; (i.e., a biphrase whose source and
target are proper suffices of x&apos;&apos; and y&apos;&apos; respec-
tively) is equal to a prefix of (x&apos;, a&apos;, y&apos;), i. As
a special case, A includes all states s(x00,a00,y00),i00
for which (x&apos;&apos;, a&apos;&apos;, y&apos;&apos;), i&apos;&apos; ends before or at posi-
tion i. This takes care of translation paths that
leave some words in x untranslated. Since the
starting position of a proper suffix of a biphrase
is always after the biphrase’s original starting po-
sition, going through the states in order of in-
creasing i guarantees that the scores for biphrases
in A((x&apos;, a&apos;, y&apos;), i) are available when computing
score (s(x0,a0,y0),i).
</bodyText>
<equation confidence="0.656888666666667">
The set B that depends on ((x&apos;, a&apos;, y&apos;), i) and
((x&apos;&apos;, a&apos;&apos;, y&apos;&apos;), i&apos;&apos;) is defined by the formula
B = sub ((x&apos;, a&apos;, y&apos;), i) \ sub ((x&apos;&apos;, a&apos;&apos;, y&apos;&apos;), i&apos;&apos;) ,
</equation>
<bodyText confidence="0.999899177777778">
where sub ((x&apos;, a&apos;, y&apos;), i) denotes the set of sub-
biphrases of (x&apos;, a&apos;, y&apos;), i (including the biphrase
(x&apos;, a&apos;, y&apos;), i itself). Thus, summing over the
weights of biphrases in B adds the contribution of
features introduced by extending translation paths
ending in (x&apos;&apos;, a&apos;&apos;, y&apos;&apos;), i&apos;&apos; by (x&apos;, a&apos;, y&apos;), i&apos;.
From the right-to-left dynamic programming,
we get analogously the contribution of right-to-
left partial translations whose left-most active
biphrase is (x&apos;, a&apos;, y&apos;), i. The partition function
used for normalizing the expectations can be ob-
tained as a side product of either of the sweeps.
In conditional random fields, the (unnormal-
ized) expectations for the feature can be ob-
tained by multiplying the scores of the states
corresponding to the same feature in the left-
to-right and right-to-left dynamic programming
memories. In our case, combining the two val-
ues stored in the states for a feature (x&apos;, a&apos;, y&apos;), i
only gives the contribution of the translation paths
where (x&apos;, a&apos;, y&apos;), i is active but not covered by
any longer biphrase that extends (x&apos;, a&apos;, y&apos;), i both
left and right. To include the contribution of
the remaining translation paths, we need to go
through states corresponding to super-biphrases of
(x&apos;, a&apos;, y&apos;), i. Special care has to be taken in order
to include the contribution of all feature vectors
in which such super-biphrases are active exactly
once. An efficient way to do this is to process the
states for super-biphrases in topological order with
respect to biphrase inclusion, and to include only
the contributions of states for super-biphrases that
extend the previously included states both left and
right.
Another complication in the dynamic program-
ming is that a biphrase can extend the source side
of another overlapping biphrase to the right, but
the target side to the left, or visa versa. Such
overlaps are not directly covered by our dynamic
programming. To deal with them, we construct
new virtual biphrases that correspond to the re-
sults of such overlaps in a pre-processing step. The
number of such virtual combinations can in theory
grow exponentially, but in practice only a small
number of virtual biphrases seems to suffice.
</bodyText>
<subsectionHeader confidence="0.99555">
3.5 Prediction with translation model alone
</subsectionHeader>
<bodyText confidence="0.999928">
Prediction is done in two phases. First, we find (by
a dynamic programming procedure similar to the
one outlined in Section 3.4.3) the highest proba-
bility feature vector ˆO(x) defined by
</bodyText>
<equation confidence="0.9973245">
ˆO(x) = arg max P(O|x).
φEΦx : x covered by biphrases in φ
</equation>
<bodyText confidence="0.9997984">
Note that we restrict the search to feature vec-
tors that cover the whole of x, i.e., to feature vec-
tors O(x, a, y) in which each word in x is covered
by at least one active aligned biphrase (x&apos;, a&apos;, y&apos;).
This forces the system to translate all words in the
source sentence even if the translation model pre-
dicts that none of the translations are very likely.
To translate words that are not covered by any
aligned biphrase feature in the model, we use the
following strategy: If the word is found from an
</bodyText>
<equation confidence="0.79397">
[score (s(x00,a00,y00),i00)
</equation>
<page confidence="0.96241">
1032
</page>
<bodyText confidence="0.999962608695652">
optional out-of-vocabulary dictionary, we use the
translation from the dictionary, and otherwise re-
sort to an implicit zero weight aligned biphrase
that copies the input word to the output as is.
In our experiments, the out-of-vocabulary dictio-
nary is constructed from the word translations that
occur once in the training data, so the out-of-
vocabulary dictionary only compensates for the
word translations lost in phrase table pruning. If
available, a real dictionary could be used as well.
The second step in predicting a translation is
solving the pre-image problem, i.e., constructing
a translation y from the predicted feature vector
ˆO(x). Since ˆO(x) E Φx, there always exists an
alignment a and a translation y such that all the
aligned biphrases in ˆO(x) occur in O(x, a, y), but
the a and y may not be unique. We choose the a
and y given by concatenating the target sides of the
biphrases active in ˆO(x) in the order induced by
their positions in the source sentence. Thus, there
is no phrase-level reordering, and the fluency of
the target language output is induced by the phrase
overlaps only.
</bodyText>
<subsectionHeader confidence="0.999768">
3.6 Predicting with an integrated LM
</subsectionHeader>
<bodyText confidence="0.999097642857143">
The prediction strategy outlined in the previous
section is simple and conceptually clean. How-
ever, biphrase overlaps alone may not be enough to
enforce fluent output, especially given that bilin-
gual data is typically more scarce than monolin-
gual data. Also, the lack of a reverse transla-
tion model means the system is unable to iden-
tify phrase extraction errors in which rarely seen
source phrases are translated to common target
phrases by chance.
To address these shortcomings, we augment the
translation model with the following additional
features that have been observed to enhance trans-
lation quality in other SMT systems.
</bodyText>
<listItem confidence="0.996126625">
1. Language model: log P(y), where P(y) is
given by a smoothed n-gram language model
2. Lexical translation model (reverse direc-
tion): log P (x|y, a) given by a word-level re-
verse translation model
3. Translation length: number of words in y
4. Distortion: number of source words in
phrases with swapped translations
</listItem>
<bodyText confidence="0.999544916666667">
The final score driving the translation process
is given by a linear combination of the trans-
lation model score log P (O(x, a, y)|x) and these
features. Besides the translation model, the lan-
guage model feature is clearly the most influential,
while the lexical translation feature has only a mi-
nor positive effect on translation quality.
We use an approximate dynamic program-
ming variant of the commonly used beam search
procedure to find the highest scoring candidate
translation. We compute the translation model
log-probability log P(O(x, a, y)|x) incrementally
while building up the corresponding candidate
translation y and word alignment a from left to
right. We allow phrase-level distortions given
by swapping the order of translations of consec-
utive non-overlapping source phrases. Unlike in
Moses, our beam search is structured around state
transitions, not around states. This means that we
apply each biphrase (state transition) simultane-
ously to all applicable partial translations (states).
This strategy is in our experience more efficient,
does not rely on future score estimates, and is im-
plementationally very similar to the dynamic pro-
gramming procedures that we use in training the
model parameters and in prediction with a lan-
guage model alone.
The weights of the features are tuned by opti-
mizing the BLEU score of development set trans-
lations with amoeba search. This simplistic strat-
egy is feasible given our system’s fast translation
speed, and extends easily to cover non-linear fea-
ture combinations. The reason for using amoeba is
that it is simpler to implement — we do not believe
amoeba yields any better values for the parameters
in the end.
</bodyText>
<sectionHeader confidence="0.999761" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999349">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999937">
Our experiments are on the Europarl translation
tasks following the setup used in the shared trans-
lation task of the ACL 2008 Third Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2008), and on the French-to-English transla-
tion task of the EACL 2009 Fourth Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2009). The size of the Europarl training cor-
pora is about 1M sentence pairs per language pair,
while the larger GigaFrEn corpus contains about
22M sentence pairs. The corpora were used for
biphrase extraction and translation model training.
Decoder feature weights were tuned on the pro-
vided development sets. In case of Europarl, lan-
guage models were trained on the target sides of
</bodyText>
<page confidence="0.909006">
1033
</page>
<table confidence="0.999554">
es-en en-es fr-en en-fr de-en en-de time
Sinuhe 31.38 30.94 31.50 28.91 25.03 19.26 338.0
Moses 32.18 31.88 32.63 29.92 27.30 20.57 3729.5
Sinuhetrans 29.14 27.12 28.74 26.06 22.38 17.14 44.2
Mosestrans 24.32 22.75 23.84 21.22 19.62 13.59 1321.5
</table>
<tableCaption confidence="0.871117666666667">
Table 1: Left: The translation quality of the SMT systems as measured by the BLEU score. Translations
were detokenized but not recased before evaluating their quality against lowercased reference translations
by the mteval-v11b.pl script. Right: Average total translation time in seconds.
</tableCaption>
<bodyText confidence="0.99952828">
the bilingual corpora. In the GigaFrEn experi-
ments we used the provided monolingual news do-
main data. All data was tokenized and lowercased
using the tools in the Moses distribution.
We experimented with four translation sys-
tems: Sinuhetrans, Sinuhe, Mosestrans, and
Moses. Sinuhetrans uses only the translation
model in producing translations (see Section 3.5),
while the full system Sinuhe uses also a lan-
guage model and some additional features (see
Section 3.6). As a baseline, we used the Moses
translation system, which is known to be very
competitive on the Europarl translation tasks as
evidenced by the University of Edinburgh entries
in the translation challenge (Callison-Burch et al.,
2008). The other comparison point Mosestrans
was obtained from Moses by disabling distortions
and setting setting the weights of all features ex-
cept the forward translation model to 0. By com-
paring Sinuhetrans and Mosestrans, we hope to
indirectly compare the performance of the under-
lying translation models. A more direct compar-
ison was not possible as it is not feasible to nor-
malize the “probabilities” predicted by the Moses
translation model.
</bodyText>
<subsectionHeader confidence="0.901633">
4.1.1 Training the models
</subsectionHeader>
<bodyText confidence="0.9999879">
We trained Moses exactly as suggested
in (Callison-Burch et al., 2008), except that
we used the -unk option for SRILM in training
the language models (both for Sinuhe and
Moses). The translation model for Sinuhe
(and Sinuhetrans) was built from the phrases
extracted by Moses as described in Section 3.1.
We chose α = 1.0 and set batch size to 1. The
learning rate was initially set to 0.1, and decayed
proportional to 1/t after 2M or 100M iterations
of training for Europarl and GigaFrEn tasks,
respectively. These choices may not be optimal
as we did not experiment with other choices
yet. In case of Europarl, training was run for
70-100M iterations using the Berkeley DB based
distribution strategy (4 CPU cores per language
pair). This took 10 days. For GigaFrEn, we
used the client-server architecture, and trained the
model for 620M stochastic gradient iterations on
about 200 CPUs. This took 2 days, which is a lot
less than the time needed to run (parallel) Giza
on this data. The number of biphrase features in
Sinuhe’s model was 2-4 million on the Europarl
tasks, and about 95 million on the GigaFrEn task.
The decoder parameters for Sinuhe were
tuned on the development sets by amoeba, and for
Moses by MERT. As both amoeba and MERT try
to solve the same optimization problem, we be-
lieve the difference in optimization methods has
little influence on the results.
</bodyText>
<subsubsectionHeader confidence="0.521127">
4.1.2 Translation results
</subsubsectionHeader>
<bodyText confidence="0.998419304347826">
Europarl tasks The systems were tested on
the 2000 sentence Europarl domain develop-
ment test sets provided for the shared translation
task (Callison-Burch et al., 2008). The resulting
BLEU scores and total translation times averaged
over the datasets are reported in Table 1. While
Moses has the highest BLEU score for all the
language pairs, the BLEU score for Sinuhe is
worse by only at most 1.31 BLEU points except
on the de-en task, where the difference is 2.27.
Sinuhetrans is clearly inferior to Sinuhe but
equally clearly superior to Mosestrans.
It takes less than a minute to translate the
development test set by the fastest system
Sinuhetrans. The slowest system Moses needs
around an hour for the same task. Memory usage
follows a similar pattern. For example, Sinuhe
requires roughly one tenth of the memory used
by Moses. Thus, in terms of resource usage,
Sinuhetrans and Sinuhe seem clearly supe-
rior to Moses. The quantitative results would
change if the systems’ parameters were optimized
for speed rather than quality, but the differences
</bodyText>
<page confidence="0.98857">
1034
</page>
<bodyText confidence="0.999945239130435">
are so clear that the general pattern would proba-
bly remain the same. For example, Moses with
no distortion is still clearly slower than Sinuhe.
GigaFrEn task The fr-en model was tested on
the 2525 sentence news domain test data used in
the preliminary evaluation of the translation chal-
lenge results. The BLEU scores for Sinuhe and
Moses were 26.32 and 26.98, respectively. The
total translation time was 13m 50s with Sinuhe,
whereas Moses needed 82m 28s. Thus, the pat-
tern that was observed on Europarl tasks is re-
peated here: The translation quality of Moses is
slightly better, but Sinuhe is significantly faster.
Surprisingly, both Sinuhe and Moses fare well
in comparison to the participants of the actual
challenge: According to the preliminary results
on the same test data we used (Koehn, 2009),
the Moses baseline would have been beaten only
by Google, and Sinuhe would have been sixth
among the 23 participating systems with a differ-
ence of only 0.57 BLEU points to the second best
entry. A partial explanation for the good relative
performance could be that the challenge partici-
pants had only a week to train their models on
the full version of GigaFrEn data, so they may not
have had time to take full advantage of it. On the
other hand, many of the top ranked systems relied
on external resources that were not available for
us.
Based on an informal human evaluation of the
outputs of Sinuhe and Moses, it looks like the
translations of Sinuhe are slightly more accu-
rate in conveying the meaning of the original sen-
tences, but especially the translations of long rare
expressions (e.g., multi-word names of institu-
tions) are less fluent. This hints that the parame-
ters for (rare) biphrases may have been regularized
too heavily — it looks like Sinuhe is underfitting
rather than overfitting. We will conduct more ex-
periments to see how much the translation quality
can be improved by a better choice of α or by us-
ing a different prior for regularization. Of course,
there is room for tuning elsewhere, too. For exam-
ple, it would be a surprise if the phrase extraction
pipeline that has been optimized for Moses would
be optimal for Sinuhe.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99997723255814">
In this paper, we have shown that phrase-based
SMT can be viewed as an instance of structural
prediction. The word alignment and phrase ex-
traction heuristics serve as a strategy for feature
extraction, and the translation task can be mod-
elled as a structural prediction problems over these
features. Our methods scale to large corpora and
are fast at predicting translations. While speed is
not the primary goal, the faster translation times
may be a key to success in applications where the
amount of text that needs to be translated is large.
In terms of BLEU scores, the results do not im-
prove the state-of-the-art so far. However, fine-
tuning the standard phrase-based approach over
the years has increased its performance signifi-
cantly, and we see no reason why the same would
not happen with the proposed approach, especially
if the model is augmented with additional features
like gapped biphrases and biphrases over POS
tags.
The fact that our translation model is a prop-
erly normalized conditional probability distribu-
tion opens up many new possibilities. For in-
stance, instead of predicting translations, it is pos-
sible to efficiently compute the expected num-
ber of times each word would appear in them.
Such output might be useful, e.g., if the transla-
tions are to be post-processed by models relying
on bag-of-words representation. Another research
direction we are currently looking into is train-
ing the proposed translation model in the reverse
direction, and then predicting translations using
the noisy channel approach, i.e., by maximizing
P(xly)P(y). The key difference to previous work
here is that since P(x y) is properly normalized,
the noisy channel approach would not in our case
suffer from the potentially negative effects caused
by ignoring the normalizer that depends on y. Be-
sides being a viable (though computationally de-
manding) alternative criterion for predicting trans-
lations, the noisy channel approach could easily be
used for, e.g., reranking n-best lists and for system
combination.
</bodyText>
<sectionHeader confidence="0.998236" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999911333333333">
This work has been partially funded by the
SMART EU project. We wish to thank the anony-
mous reviewers for their constructive comments
and especially for pointing out the related pa-
per (Blunsom and Osborne, 2008) that we had
missed. Special thanks to Vladimir Poroshin for
all the bugs he found in beta-testing and to Esther
Galbrun for her comments on a draft version of
this paper.
</bodyText>
<page confidence="0.991902">
1035
</page>
<sectionHeader confidence="0.993825" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999711815384615">
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In EMNLP.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Josh Schroeder, and Cameron Shaw Fordyce.
2008. ACL 2008 third workshop on statistical ma-
chine translation. http://www.statmt.org/
wmt08.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. EACL 2009 fourth
workshop on statistical machine translation. http:
//www.statmt.org/wmt09.
Stanley Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions Speech and Audio Processing,
8(1):37–50.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In ACL.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models un-
derperform surface heuristics. In Workshop on SMT
at NAACL.
John DeNero, Alex Bouchard, and Dan Klein. 2008.
Sampling alignment structure under a bayesian
translation model. In EMNLP.
Matti K¨a¨ari¨ainen. 2009. Sinuhe source code dis-
tribution (v1.2). Website. http://www.cs.
helsinki.fi/u/mtkaaria/sinuhe.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, and
Chris Callison-Burch et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL.
Philipp Koehn. 2009. BLEU/NIST scores for sub-
missions. http://groups.google.com/
group/WMT09/browse thread/thread/
bfbce7b219648a4c.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In ACL.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In EMNLP.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:2003.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statisti-
cal machine translation. In EMNLP, pages 20–28.
Cristoph Tillmann and Tong Zhang. 2007. A block bi-
gram prediction model for statistical machine trans-
lation. ACM Transacions on Speech and Language
Processing, 4(3).
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In ICML.
</reference>
<page confidence="0.993078">
1036
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.555695">
<title confidence="0.947517">Statistical Machine Translation using a Globally Conditional Exponential Family Translation Model</title>
<author confidence="0.963849">Matti</author>
<affiliation confidence="0.986194">Department of Computer FI-00014 University of Helsinki,</affiliation>
<email confidence="0.989509">matti.kaariainen@cs.helsinki.fi</email>
<abstract confidence="0.998128296296297">We present a new phrase-based conditional exponential family translation model for statistical machine translation. The model operates on a feature representation in which sentence level translations are represented by enumerating all the known phrase level translations that occur inside them. This makes the model a good match with the commonly used phrase extraction heuristics. The model’s predictions are properly normalized probabilities. In addition, the model automatically takes into account information provided by phrase overlaps, and does not suffer from reference translation reachability problems. We have implemented an open source system on the proposed translation model. Our experiments on Europarl and GigaFrEn corpora demonstrate that finding the unique MAP parameters for the model on large scale data is feasible with simple stochastic gramethods. fast and memory efficient, and the BLEU scores obtained by it are only slightly inferior to</abstract>
<intro confidence="0.66152">of</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Probabilistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7726" citStr="Blunsom and Osborne, 2008" startWordPosition="1174" endWordPosition="1177">slation features. However, the way we use the features is quite different. There exists a number of discriminative approaches whose model structure, training criteria, or both, are similar to ours. However, to our knowledge, none of the other systems operates directly on biphrase features, scales up to bilingual corpora with millions of sentence pairs, and achieves translation quality comparable to fully tuned standard phrase-based systems. The approach most closely resembling ours is the independently developed global discriminative loglinear model based on synchronous context-free grammars (Blunsom and Osborne, 2008; Blunsom et al., 2008). The version presented in (Blunsom and Osborne, 2008) operates on millions of rule count features analogous to our biphrase features, and integrates a language model into training and decoding. The system can be trained on tens of thousands of short sentences yielding better translations than a baseline system Hiero on this data. The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. Both versi</context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>Phil Blunsom and Miles Osborne. 2008. Probabilistic inference for machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7749" citStr="Blunsom et al., 2008" startWordPosition="1178" endWordPosition="1182">the way we use the features is quite different. There exists a number of discriminative approaches whose model structure, training criteria, or both, are similar to ours. However, to our knowledge, none of the other systems operates directly on biphrase features, scales up to bilingual corpora with millions of sentence pairs, and achieves translation quality comparable to fully tuned standard phrase-based systems. The approach most closely resembling ours is the independently developed global discriminative loglinear model based on synchronous context-free grammars (Blunsom and Osborne, 2008; Blunsom et al., 2008). The version presented in (Blunsom and Osborne, 2008) operates on millions of rule count features analogous to our biphrase features, and integrates a language model into training and decoding. The system can be trained on tens of thousands of short sentences yielding better translations than a baseline system Hiero on this data. The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. Both versions deal with derivatio</context>
<context position="9389" citStr="Blunsom et al., 2008" startWordPosition="1443" endWordPosition="1446"> needed in computing the maximum probability translation in decoding, and also in computing model expectations in training when a language model is used. In addition, since the models operate directly on translations, using probabilistic training criteria for learning the model parameters is possible only if all reference translations in the training data can be generated by the model. In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). Another closely related approach is the independently developed discriminative block bigram prediction model presented in (Tillmann and Zhang, 2007). This work proposes a global phrase-based translation model very similar to ours, but due to computational reasons, resorts to a localized approximation thereof, and is restricted to biphrases of length at most two. In (Liang et al., 2006) a standard phrase-based model is augmented with more than a million features whose weights are trained discriminatively by a variant of the perceptron algorithm. Reference reachability is again a problem, and</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
<author>Cameron Shaw Fordyce</author>
</authors>
<title>third workshop on statistical machine translation.</title>
<date>2008</date>
<publisher>ACL</publisher>
<note>http://www.statmt.org/ wmt08.</note>
<contexts>
<context position="30416" citStr="Callison-Burch et al., 2008" startWordPosition="4978" endWordPosition="4981">eatures are tuned by optimizing the BLEU score of development set translations with amoeba search. This simplistic strategy is feasible given our system’s fast translation speed, and extends easily to cover non-linear feature combinations. The reason for using amoeba is that it is simpler to implement — we do not believe amoeba yields any better values for the parameters in the end. 4 Experiments 4.1 Experimental setup Our experiments are on the Europarl translation tasks following the setup used in the shared translation task of the ACL 2008 Third Workshop on Statistical Machine Translation (Callison-Burch et al., 2008), and on the French-to-English translation task of the EACL 2009 Fourth Workshop on Statistical Machine Translation (Callison-Burch et al., 2009). The size of the Europarl training corpora is about 1M sentence pairs per language pair, while the larger GigaFrEn corpus contains about 22M sentence pairs. The corpora were used for biphrase extraction and translation model training. Decoder feature weights were tuned on the provided development sets. In case of Europarl, language models were trained on the target sides of 1033 es-en en-es fr-en en-fr de-en en-de time Sinuhe 31.38 30.94 31.50 28.91 </context>
<context position="32181" citStr="Callison-Burch et al., 2008" startWordPosition="5251" endWordPosition="5254">nolingual news domain data. All data was tokenized and lowercased using the tools in the Moses distribution. We experimented with four translation systems: Sinuhetrans, Sinuhe, Mosestrans, and Moses. Sinuhetrans uses only the translation model in producing translations (see Section 3.5), while the full system Sinuhe uses also a language model and some additional features (see Section 3.6). As a baseline, we used the Moses translation system, which is known to be very competitive on the Europarl translation tasks as evidenced by the University of Edinburgh entries in the translation challenge (Callison-Burch et al., 2008). The other comparison point Mosestrans was obtained from Moses by disabling distortions and setting setting the weights of all features except the forward translation model to 0. By comparing Sinuhetrans and Mosestrans, we hope to indirectly compare the performance of the underlying translation models. A more direct comparison was not possible as it is not feasible to normalize the “probabilities” predicted by the Moses translation model. 4.1.1 Training the models We trained Moses exactly as suggested in (Callison-Burch et al., 2008), except that we used the -unk option for SRILM in training </context>
<context position="34221" citStr="Callison-Burch et al., 2008" startWordPosition="5591" endWordPosition="5594">needed to run (parallel) Giza on this data. The number of biphrase features in Sinuhe’s model was 2-4 million on the Europarl tasks, and about 95 million on the GigaFrEn task. The decoder parameters for Sinuhe were tuned on the development sets by amoeba, and for Moses by MERT. As both amoeba and MERT try to solve the same optimization problem, we believe the difference in optimization methods has little influence on the results. 4.1.2 Translation results Europarl tasks The systems were tested on the 2000 sentence Europarl domain development test sets provided for the shared translation task (Callison-Burch et al., 2008). The resulting BLEU scores and total translation times averaged over the datasets are reported in Table 1. While Moses has the highest BLEU score for all the language pairs, the BLEU score for Sinuhe is worse by only at most 1.31 BLEU points except on the de-en task, where the difference is 2.27. Sinuhetrans is clearly inferior to Sinuhe but equally clearly superior to Mosestrans. It takes less than a minute to translate the development test set by the fastest system Sinuhetrans. The slowest system Moses needs around an hour for the same task. Memory usage follows a similar pattern. For examp</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, Fordyce, 2008</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Josh Schroeder, and Cameron Shaw Fordyce. 2008. ACL 2008 third workshop on statistical machine translation. http://www.statmt.org/ wmt08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>EACL</title>
<date>2009</date>
<note>http: //www.statmt.org/wmt09.</note>
<contexts>
<context position="30561" citStr="Callison-Burch et al., 2009" startWordPosition="4999" endWordPosition="5002"> system’s fast translation speed, and extends easily to cover non-linear feature combinations. The reason for using amoeba is that it is simpler to implement — we do not believe amoeba yields any better values for the parameters in the end. 4 Experiments 4.1 Experimental setup Our experiments are on the Europarl translation tasks following the setup used in the shared translation task of the ACL 2008 Third Workshop on Statistical Machine Translation (Callison-Burch et al., 2008), and on the French-to-English translation task of the EACL 2009 Fourth Workshop on Statistical Machine Translation (Callison-Burch et al., 2009). The size of the Europarl training corpora is about 1M sentence pairs per language pair, while the larger GigaFrEn corpus contains about 22M sentence pairs. The corpora were used for biphrase extraction and translation model training. Decoder feature weights were tuned on the provided development sets. In case of Europarl, language models were trained on the target sides of 1033 es-en en-es fr-en en-fr de-en en-de time Sinuhe 31.38 30.94 31.50 28.91 25.03 19.26 338.0 Moses 32.18 31.88 32.63 29.92 27.30 20.57 3729.5 Sinuhetrans 29.14 27.12 28.74 26.06 22.38 17.14 44.2 Mosestrans 24.32 22.75 23</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. EACL 2009 fourth workshop on statistical machine translation. http: //www.statmt.org/wmt09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for ME models.</title>
<date>2000</date>
<journal>IEEE Transactions Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="17737" citStr="Chen and Rosenfeld, 2000" startWordPosition="2843" endWordPosition="2846">. This can be also viewed as L2 regularization. The prior guarantees that the MAP parameters are unique, and models our belief that the observed feature occurrence counts randomly deviate from their “true” values roughly proportionally to the standard deviations of the occurrence count distributions. The prior variance σ2(x0,a0,y0) for feature (x0, a0, y0) is given by the formula σ2a0 y0) = α/ ρ(x0 a 0 y0) , where α &gt; 0 is a free regularization parameter, and ρ2(x0,a0,y0) is an empirical estimate of the variance of the occurrence count of (x0, a0, y0) in the training data. This is similar to (Chen and Rosenfeld, 2000), except that we use standard deviations in place of variances. As the estimate for the variexp(w · P (O(x, a, y)|x) = E φ∈Φx exp(w · ˜O(x, a, y)) ˜O) . 1030 ance of a feature we use the occurrence count of the corresponding biphrase in the training data. This could be justified by assuming that the occurrence counts follow a Poisson distribution. We have also run preliminary experiments with other forms of regularization (different ways of computing σ(x0,a0,y0), exponential priors corresponding to L1 regularization, no regularization), and it looks like the system is not very sensitive to the</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 2000. A survey of smoothing techniques for ME models. IEEE Transactions Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3066" citStr="DeNero and Klein, 2008" startWordPosition="457" endWordPosition="460">s to simply ignore the disparity as is done, e.g., in Moses. While empirically succesful, this approach is hard to justify theoretically, and begs the question of whether more principled methods might lead to better translation results. The other extensively studied approach is to replace the phrase extraction heuristic with a method that better matches the use of the extracted phrases (see, e.g., (Marcu and Wong, 2002; DeNero et al., 2008) and the references therein). While theoretically sound, this approach is computationally challenging both in practice (DeNero et al., 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al., 2006), and in the end may lead to inferior translation quality (Koehn et al., 2003). In this paper, we study a third alternative. We propose a new translation model that is compatible with the phrase extraction heuristic. The proposed machine learning inspired translation model takes the form of a conditional exponential family probability distribution over a feature representation for word-aligned sentence pairs. The feature representation represents a word-aligned sentence pair by essentially enumerating the (multi)set of biph</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Workshop on SMT at NAACL.</booktitle>
<contexts>
<context position="3137" citStr="DeNero et al., 2006" startWordPosition="468" endWordPosition="471">ally succesful, this approach is hard to justify theoretically, and begs the question of whether more principled methods might lead to better translation results. The other extensively studied approach is to replace the phrase extraction heuristic with a method that better matches the use of the extracted phrases (see, e.g., (Marcu and Wong, 2002; DeNero et al., 2008) and the references therein). While theoretically sound, this approach is computationally challenging both in practice (DeNero et al., 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al., 2006), and in the end may lead to inferior translation quality (Koehn et al., 2003). In this paper, we study a third alternative. We propose a new translation model that is compatible with the phrase extraction heuristic. The proposed machine learning inspired translation model takes the form of a conditional exponential family probability distribution over a feature representation for word-aligned sentence pairs. The feature representation represents a word-aligned sentence pair by essentially enumerating the (multi)set of biphrases that would have been extracted from it, 1027 Proceedings of the 2</context>
<context position="11046" citStr="DeNero et al., 2006" startWordPosition="1721" endWordPosition="1724">aligned sentence pairs (x, a, y), where a is a many-to-many alignment between the words in x and y. Second, using a heuristic proposed in (Och et al., 1999), all the aligned phrase pairs (x&apos;, a&apos;, y&apos;) satisfying the following criteria are extracted: (1) x&apos; and y&apos; consist of consecutive words of x and y, and both have length at most k, (2) a&apos; is the alignment between words of x&apos; and y&apos; induced by a, (3) a&apos; contains at least one link, and (4) there are no links in a that have just one end in x&apos; or y&apos;. Each aligned training sentence (x, a, y) thus generates a number of potentially overlapping in (DeNero et al., 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. aligned biphrase features (x&apos;, a&apos;, y&apos;). In our experiments, we chose k = 7 which is the default in Moses. Unlike in Moses, we do not map the aligned biphrases (x&apos;, a&apos;, y&apos;) back to non-aligned biphrases (x&apos;, y&apos;). To reduce the number of extracted biphrases, for each source phrase x&apos;, only biphrases (x&apos;, a&apos;, y&apos;) whose occurrence count is among the top K = 20 in the training data are retained (rank ties broken by including all biphrases with rank equal to the limit K). For technical reasons related to our dynamic pro</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In Workshop on SMT at NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alex Bouchard</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a bayesian translation model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2887" citStr="DeNero et al., 2008" startWordPosition="430" endWordPosition="433"> extracted biphrase counts. The disparity between the phrase extraction heuristic and the use of the extracted biphrases can be addressed in at least three ways. One approach is to simply ignore the disparity as is done, e.g., in Moses. While empirically succesful, this approach is hard to justify theoretically, and begs the question of whether more principled methods might lead to better translation results. The other extensively studied approach is to replace the phrase extraction heuristic with a method that better matches the use of the extracted phrases (see, e.g., (Marcu and Wong, 2002; DeNero et al., 2008) and the references therein). While theoretically sound, this approach is computationally challenging both in practice (DeNero et al., 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al., 2006), and in the end may lead to inferior translation quality (Koehn et al., 2003). In this paper, we study a third alternative. We propose a new translation model that is compatible with the phrase extraction heuristic. The proposed machine learning inspired translation model takes the form of a conditional exponential family probability distribution </context>
</contexts>
<marker>DeNero, Bouchard, Klein, 2008</marker>
<rawString>John DeNero, Alex Bouchard, and Dan Klein. 2008. Sampling alignment structure under a bayesian translation model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matti K¨a¨ari¨ainen</author>
</authors>
<title>Sinuhe source code distribution</title>
<date>2009</date>
<marker>K¨a¨ari¨ainen, 2009</marker>
<rawString>Matti K¨a¨ari¨ainen. 2009. Sinuhe source code distribution (v1.2). Website. http://www.cs. helsinki.fi/u/mtkaaria/sinuhe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="3215" citStr="Koehn et al., 2003" startWordPosition="482" endWordPosition="485">estion of whether more principled methods might lead to better translation results. The other extensively studied approach is to replace the phrase extraction heuristic with a method that better matches the use of the extracted phrases (see, e.g., (Marcu and Wong, 2002; DeNero et al., 2008) and the references therein). While theoretically sound, this approach is computationally challenging both in practice (DeNero et al., 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al., 2006), and in the end may lead to inferior translation quality (Koehn et al., 2003). In this paper, we study a third alternative. We propose a new translation model that is compatible with the phrase extraction heuristic. The proposed machine learning inspired translation model takes the form of a conditional exponential family probability distribution over a feature representation for word-aligned sentence pairs. The feature representation represents a word-aligned sentence pair by essentially enumerating the (multi)set of biphrases that would have been extracted from it, 1027 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1027</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1381" citStr="Koehn et al., 2007" startWordPosition="198" endWordPosition="201">rovided by phrase overlaps, and does not suffer from reference translation reachability problems. We have implemented an open source translation system Sinuhe based on the proposed translation model. Our experiments on Europarl and GigaFrEn corpora demonstrate that finding the unique MAP parameters for the model on large scale data is feasible with simple stochastic gradient methods. Sinuhe is fast and memory efficient, and the BLEU scores obtained by it are only slightly inferior to those of Moses. 1 Introduction In current phrase-based statistical machine translation systems such as Moses1 (Koehn et al., 2007), the translation model is defined in terms of phrase pairs (biphrases) extracted from a bilingual corpus as follows. The corpus is first word-aligned using a word alignment heuristic (Och and Ney, 1Throughout this paper, we refer to Moses for concreteness, but most of the discussion applies to other standard phrase-based statistical machine translation systems as well. 2003). The phrase extraction heuristic then extracts all the biphrases that are compatible with the word alignment (Och et al., 1999). This way, each sentence pair may generate any number of potentially overlapping biphrases. H</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, and Chris Callison-Burch et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>BLEU/NIST scores for submissions. http://groups.google.com/ group/WMT09/browse thread/thread/ bfbce7b219648a4c.</title>
<date>2009</date>
<contexts>
<context position="35916" citStr="Koehn, 2009" startWordPosition="5875" endWordPosition="5876">e 2525 sentence news domain test data used in the preliminary evaluation of the translation challenge results. The BLEU scores for Sinuhe and Moses were 26.32 and 26.98, respectively. The total translation time was 13m 50s with Sinuhe, whereas Moses needed 82m 28s. Thus, the pattern that was observed on Europarl tasks is repeated here: The translation quality of Moses is slightly better, but Sinuhe is significantly faster. Surprisingly, both Sinuhe and Moses fare well in comparison to the participants of the actual challenge: According to the preliminary results on the same test data we used (Koehn, 2009), the Moses baseline would have been beaten only by Google, and Sinuhe would have been sixth among the 23 participating systems with a difference of only 0.57 BLEU points to the second best entry. A partial explanation for the good relative performance could be that the challenge participants had only a week to train their models on the full version of GigaFrEn data, so they may not have had time to take full advantage of it. On the other hand, many of the top ranked systems relied on external resources that were not available for us. Based on an informal human evaluation of the outputs of Sin</context>
</contexts>
<marker>Koehn, 2009</marker>
<rawString>Philipp Koehn. 2009. BLEU/NIST scores for submissions. http://groups.google.com/ group/WMT09/browse thread/thread/ bfbce7b219648a4c.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="6789" citStr="Lafferty et al., 2001" startWordPosition="1033" endWordPosition="1036"> and offers translation quality that is only slightly worse than that of the baseline system Moses. In terms of translation speed, Sinuhe is already clearly better. The rest of this paper is organized as follows. After briefly reviewing related work in Section 2, we describe the proposed translation model in Section 3. Finally, experimental results are presented in Section 4, and conclusions in Section 5. 2 Related work The proposed translation model is strongly influenced by machine learning techniques for solving sequence prediction tasks, most notably the work on conditional random fields (Lafferty et al., 2001). The modelling task in machine translation is, however, more complicated than sequence labelling (not one-to-one, reorderings), so the standard methods cannot be directly applied here. The model we propose is also related to standard phrase-based translation models through the use of the same phrase-level translation features. However, the way we use the features is quite different. There exists a number of discriminative approaches whose model structure, training criteria, or both, are similar to ours. However, to our knowledge, none of the other systems operates directly on biphrase feature</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cote</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9780" citStr="Liang et al., 2006" startWordPosition="1502" endWordPosition="1505">ractice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). Another closely related approach is the independently developed discriminative block bigram prediction model presented in (Tillmann and Zhang, 2007). This work proposes a global phrase-based translation model very similar to ours, but due to computational reasons, resorts to a localized approximation thereof, and is restricted to biphrases of length at most two. In (Liang et al., 2006) a standard phrase-based model is augmented with more than a million features whose weights are trained discriminatively by a variant of the perceptron algorithm. Reference reachability is again a problem, and the method has not been scaled up to use biphrase features directly. 3 The proposed translation model 3.1 Biphrase extraction The biphrases used in Sinuhe are extracted from the training data with the Moses phrase extraction heuristics. The sentence-aligned training corpus S is first word-aligned by running Giza++ in both directions and then symmetrizing the alignments. This maps the ori</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrasebased, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2865" citStr="Marcu and Wong, 2002" startWordPosition="426" endWordPosition="429">ve been estimated from extracted biphrase counts. The disparity between the phrase extraction heuristic and the use of the extracted biphrases can be addressed in at least three ways. One approach is to simply ignore the disparity as is done, e.g., in Moses. While empirically succesful, this approach is hard to justify theoretically, and begs the question of whether more principled methods might lead to better translation results. The other extensively studied approach is to replace the phrase extraction heuristic with a method that better matches the use of the extracted phrases (see, e.g., (Marcu and Wong, 2002; DeNero et al., 2008) and the references therein). While theoretically sound, this approach is computationally challenging both in practice (DeNero et al., 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al., 2006), and in the end may lead to inferior translation quality (Koehn et al., 2003). In this paper, we study a third alternative. We propose a new translation model that is compatible with the phrase extraction heuristic. The proposed machine learning inspired translation model takes the form of a conditional exponential family pro</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrasebased, joint probability model for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--2003</pages>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In EMNLP,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="1887" citStr="Och et al., 1999" startWordPosition="278" endWordPosition="281">ntroduction In current phrase-based statistical machine translation systems such as Moses1 (Koehn et al., 2007), the translation model is defined in terms of phrase pairs (biphrases) extracted from a bilingual corpus as follows. The corpus is first word-aligned using a word alignment heuristic (Och and Ney, 1Throughout this paper, we refer to Moses for concreteness, but most of the discussion applies to other standard phrase-based statistical machine translation systems as well. 2003). The phrase extraction heuristic then extracts all the biphrases that are compatible with the word alignment (Och et al., 1999). This way, each sentence pair may generate any number of potentially overlapping biphrases. However, when defining the phrase-based sentence level translation model, phrase overlaps are explicitly disallowed: The source sentence is segmented into disjoint phrases, which are translated independently using conditional phrase-level translation models that have been estimated from extracted biphrase counts. The disparity between the phrase extraction heuristic and the use of the extracted biphrases can be addressed in at least three ways. One approach is to simply ignore the disparity as is done,</context>
<context position="10582" citStr="Och et al., 1999" startWordPosition="1631" endWordPosition="1634"> is again a problem, and the method has not been scaled up to use biphrase features directly. 3 The proposed translation model 3.1 Biphrase extraction The biphrases used in Sinuhe are extracted from the training data with the Moses phrase extraction heuristics. The sentence-aligned training corpus S is first word-aligned by running Giza++ in both directions and then symmetrizing the alignments. This maps the original aligned sentence pairs (x, y) into word-aligned sentence pairs (x, a, y), where a is a many-to-many alignment between the words in x and y. Second, using a heuristic proposed in (Och et al., 1999), all the aligned phrase pairs (x&apos;, a&apos;, y&apos;) satisfying the following criteria are extracted: (1) x&apos; and y&apos; consist of consecutive words of x and y, and both have length at most k, (2) a&apos; is the alignment between words of x&apos; and y&apos; induced by a, (3) a&apos; contains at least one link, and (4) there are no links in a that have just one end in x&apos; or y&apos;. Each aligned training sentence (x, a, y) thus generates a number of potentially overlapping in (DeNero et al., 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. aligned biphrase features (x&apos;, a&apos;, y&apos;). In our experime</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In EMNLP, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A block bigram prediction model for statistical machine translation.</title>
<date>2007</date>
<journal>ACM Transacions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="9540" citStr="Tillmann and Zhang, 2007" startWordPosition="1464" endWordPosition="1467"> used. In addition, since the models operate directly on translations, using probabilistic training criteria for learning the model parameters is possible only if all reference translations in the training data can be generated by the model. In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). Another closely related approach is the independently developed discriminative block bigram prediction model presented in (Tillmann and Zhang, 2007). This work proposes a global phrase-based translation model very similar to ours, but due to computational reasons, resorts to a localized approximation thereof, and is restricted to biphrases of length at most two. In (Liang et al., 2006) a standard phrase-based model is augmented with more than a million features whose weights are trained discriminatively by a variant of the perceptron algorithm. Reference reachability is again a problem, and the method has not been scaled up to use biphrase features directly. 3 The proposed translation model 3.1 Biphrase extraction The biphrases used in Si</context>
</contexts>
<marker>Tillmann, Zhang, 2007</marker>
<rawString>Cristoph Tillmann and Tong Zhang. 2007. A block bigram prediction model for statistical machine translation. ACM Transacions on Speech and Language Processing, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Nicol N Schraudolph</author>
<author>Mark W Schmidt</author>
<author>Kevin P Murphy</author>
</authors>
<title>Accelerated training of conditional random fields with stochastic gradient methods.</title>
<date>2006</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="19276" citStr="Vishwanathan et al., 2006" startWordPosition="3105" endWordPosition="3109">ur criterion for learning w. 3.4.2 Optimization We solve the optimization problem related to learning w by first order gradient ascent methods. The gradient ∇L(w) of L(w) with respect to w can be written as P(x, a, y)−Ew[˜φ|x]], where Ew[˜φ|x] denotes the conditional expectation of the aligned biphrase occurrence count features given x with respect to model parameters w. Feature expectations can be computed by combining the results of a left-to-right and right-to-left dynamic programming sweep over the source sentence. For more details, see Section 3.4.3. Inspired by the empirical results in (Vishwanathan et al., 2006), we use classic stochastic gradient ascent to solve the optimization problem. At each step t, we sample with replacement a batch St of b examples from S. We start from w0 = 0, and use the update rule wt+1 = wt − ηt∇Lt(wt), (1) where ηt &gt; 0 is the learning rate, and ∇Lt(w) is the stochastic gradient of the negative logposterior log P(φ(x, a, y)|x) w2 i Lt(w) = |St |X |S |2σ2 i i X− (x,a,y)ESt restricted to batch St. The second term of the stochastic gradient involves only biphrases whose source sides match the source sentences in the batch. Though the gradient of the regularizer is non-zero fo</context>
</contexts>
<marker>Vishwanathan, Schraudolph, Schmidt, Murphy, 2006</marker>
<rawString>S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W. Schmidt, and Kevin P. Murphy. 2006. Accelerated training of conditional random fields with stochastic gradient methods. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>