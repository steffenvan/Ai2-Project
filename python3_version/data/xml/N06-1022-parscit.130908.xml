<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000053">
<title confidence="0.986066">
Multilevel Coarse-to-fine PCFG Parsing
</title>
<author confidence="0.997046666666667">
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil,
David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths,
Jeremy Moore, Michael Pozar, and Theresa Vu
</author>
<affiliation confidence="0.9758215">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.956547">
Providence, RI 02912
</address>
<email confidence="0.999685">
ec@cs.brown.edu
</email>
<sectionHeader confidence="0.993918" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984111111112">
We present a PCFG parsing algorithm
that uses a multilevel coarse-to-fine
(mlctf) scheme to improve the effi-
ciency of search for the best parse.
Our approach requires the user to spec-
ify a sequence of nested partitions or
equivalence classes of the PCFG non-
terminals. We define a sequence of
PCFGs corresponding to each parti-
tion, where the nonterminals of each
PCFG are clusters of nonterminals of
the original source PCFG. We use the
results of parsing at a coarser level
(i.e., grammar defined in terms of a
coarser partition) to prune the next
finer level. We present experiments
showing that with our algorithm the
work load (as measured by the total
number of constituents processed) is
decreased by a factor of ten with no de-
crease in parsing accuracy compared to
standard CKY parsing with the origi-
nal PCFG. We suggest that the search
space over mlctf algorithms is almost
totally unexplored so that future work
should be able to improve significantly
on these results.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999765575">
Reasonably accurate constituent-based parsing
is fairly quick these days, if fairly quick means
about a second per sentence. Unfortunately, this
is still too slow for many applications. In some
cases researchers need large quantities of parsed
data and do not have the hundreds of machines
necessary to parse gigaword corpora in a week
or two. More pressingly, in real-time applica-
tions such as speech recognition, a parser would
be only a part of a much larger system, and
the system builders are not keen on giving the
parser one of the ten seconds available to pro-
cess, say, a thirty-word sentence. Even worse,
some applications require the parsing of multi-
ple candidate strings per sentence (Johnson and
Charniak, 2004) or parsing from a lattice (Hall
and Johnson, 2004), and in these applications
parsing efficiency is even more important.
We present here a multilevel coarse-to-fine
(mlctf) PCFG parsing algorithm that reduces
the complexity of the search involved in find-
ing the best parse. It defines a sequence of in-
creasingly more complex PCFGs, and uses the
parse forest produced by one PCFG to prune
the search of the next more complex PCFG.
We currently use four levels of grammars in our
mlctf algorithm. The simplest PCFG, which we
call the level-0 grammar, contains only one non-
trivial nonterminal and is so simple that min-
imal time is needed to parse a sentence using
it. Nonetheless, we demonstrate that it identi-
fies the locations of correct constituents of the
parse tree (the “gold constituents”) with high
recall. Our level-1 grammar distinguishes only
argument from modifier phrases (i.e., it has two
nontrivial nonterminals), while our level-2 gram-
mar distinguishes the four major phrasal cate-
gories (verbal, nominal, adjectival and preposi-
tional phrases), and level 3 distinguishes all of
the standard categories of the Penn treebank.
</bodyText>
<page confidence="0.965867">
168
</page>
<note confidence="0.995432">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 168–175,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.970624555555555">
first appearance of this idea we are aware of is
in Maxwell and Kaplan (1993), where a cover-
ing CFG is automatically extracted from a more
detailed unification grammar and used to iden-
tify the possible locations of constituents in the
more detailed parses of the sentence. Maxwell
and Kaplan use their covering CFG to prune the
search of their unification grammar parser in es-
sentially the same manner as we do here, and
demonstrate significant performance improve-
ments by using their coarse-to-fine approach.
The basic theory of coarse-to-fine approxima-
tions and dynamic programming in a stochastic
framework is laid out in Geman and Kochanek
(2001). This paper describes the multilevel
dynamic programming algorithm needed for
coarse-to-fine analysis (which they apply to de-
coding rather than parsing), and show how
to perform exact coarse-to-fine computation,
rather than the heuristic search we perform here.
A paper closely related to ours is Goodman
(1997). In our terminology, Goodman’s parser
is a two-stage ctf parser. The second stage is a
standard tree-bank parser while the first stage is
a regular-expression approximation of the gram-
mar. Again, the second stage is constrained by
the parses found in the first stage. Neither stage
is smoothed. The parser of Charniak (2000)
is also a two-stage ctf model, where the first
stage is a smoothed Markov grammar (it uses
up to three previous constituents as context),
and the second stage is a lexicalized Markov
grammar with extra annotations about parents
and grandparents. The second stage explores
all of the constituents not pruned out after the
first stage. Related approaches are used in Hall
(2004) and Charniak and Johnson (2005).
A quite different approach to parsing effi-
ciency is taken in Caraballo and Charniak (1998)
(and refined in Charniak et al. (1998)). Here
efficiency is gained by using a standard chart-
parsing algorithm and pulling constituents off
the agenda according to (an estimate of) their
probability given the sentence. This probability
is computed by estimating Equation 1:
</bodyText>
<equation confidence="0.982493">
��,) . (1)
p(n� ��, s) = α(n� ��,)β(n�
p(s)
</equation>
<bodyText confidence="0.999981023809524">
The nonterminal categories in these grammars
can be regarded as clusters or equivalence classes
of the original Penn treebank nonterminal cat-
egories. (In fact, we obtain these grammars by
relabeling the node labels in the treebank and
extracting a PCFG from this relabelled treebank
in the standard fashion, but we discuss other ap-
proaches below.) We require that the partition
of the nonterminals defined by the equivalence
classes at level l + 1 be a refinement of the par-
tition defined at level l. This means that each
nonterminal category at level l+1 is mapped to a
unique nonterminal category at level l (although
in general the mapping is many to one, i.e., each
nonterminal category at level l corresponds to
several nonterminal categories at level l + 1).
We use the correspondence between categories
at different levels to prune possible constituents.
A constituent is considered at level l + 1 only
if the corresponding constituent at level l has
a probability exceeding some threshold.. Thus
parsing a sentence proceeds as follows. We first
parse the sentence with the level-0 grammar to
produce a parse forest using the CKY parsing
algorithm. Then for each level l + 1 we reparse
the sentence with the level l + 1 grammar us-
ing the level l parse forest to prune as described
above. As we demonstrate, this leads to consid-
erable efficiency improvements.
The paper proceeds as follows. We next dis-
cuss previous work (Section 2). Section 3 out-
lines the algorithm in more detail. Section
4 presents some experiments showing that the
work load (as measured by the total number of
constituents processed) is decreased by a fac-
tor of ten over standard CKY parsing at the
final level. We also discuss some fine points of
the results therein. Finally in section 5 we sug-
gest that because the search space of mlctf al-
gorithms is, at this point, almost totally unex-
plored, future work should be able to improve
significantly on these results.
</bodyText>
<sectionHeader confidence="0.776429" genericHeader="method">
2 Previous Research
</sectionHeader>
<bodyText confidence="0.677817">
Coarse-to-fine search is an idea that has ap-
peared several times in the literature of com-
putational linguistics and related areas. The
</bodyText>
<page confidence="0.998275">
169
</page>
<bodyText confidence="0.999462591836735">
It must be estimated because during the
bottom-up chart-parsing algorithm, the true
outside probability cannot be computed. The
results cited in Caraballo and Charniak (1998)
cannot be compared directly to ours, but are
roughly in the same equivalence class. Those
presented in Charniak et al. (1998) are superior,
but in Section 5 below we suggest that a com-
bination of the techniques could yield better re-
sults still.
Klein and Manning (2003a) describe efficient
A* for the most likely parse, where pruning is
accomplished by using Equation 1 and a true
upper bound on the outside probability. While
their maximum is a looser estimate of the out-
side probability, it is an admissible heuristic and
together with an A* search is guaranteed to find
the best parse first. One question is if the guar-
antee is worth the extra search required by the
looser estimate of the true outside probability.
Tsuruoka and Tsujii (2004) explore the frame-
work developed in Klein and Manning (2003a),
and seek ways to minimize the time required
by the heap manipulations necessary in this
scheme. They describe an iterative deepening
algorithm that does not require a heap. They
also speed computation by precomputing more
accurate upper bounds on the outside proba-
bilities of various kinds of constituents. They
are able to reduce by half the number of con-
stituents required to find the best parse (com-
pared to CKY).
Most recently, McDonald et al. (2005) have
implemented a dependency parser with good
accuracy (it is almost as good at dependency
parsing as Charniak (2000)) and very impres-
sive speed (it is about ten times faster than
Collins (1997) and four times faster than Char-
niak (2000)). It achieves its speed in part be-
cause it uses the Eisner and Satta (1999) algo-
rithm for n3 bilexical parsing, but also because
dependency parsing has a much lower grammar
constant than does standard PCFG parsing —
after all, there are no phrasal constituents to
consider. The current paper can be thought of
as a way to take the sting out of the grammar
constant for PCFGs by parsing first with very
few phrasal constituents and adding them only
Level: 0 1 2 3
</bodyText>
<figure confidence="0.536540551724138">
S1 { S1 { S1 { S1
S
VP
UCP
SQ
SBAR
SBARQ
SINV
NP
NAC
NX
LST
X
UCP
FRAG
ADJP
QP
CONJP
ADVP
INTJ
PRN
PRT
PP
PRT
RRC
WHADJP
WHADVP
WHNP
WHPP
</figure>
<figureCaption confidence="0.99967">
Figure 1: The levels of nonterminal labels
</figureCaption>
<bodyText confidence="0.916294">
after most constituents have been pruned away.
</bodyText>
<sectionHeader confidence="0.940272" genericHeader="method">
3 Multilevel Course-to-fine Parsing
</sectionHeader>
<bodyText confidence="0.999927705882353">
We use as the underlying parsing algorithm a
reasonably standard CKY parser, modified to
allow unary branching rules.
The complete nonterminal clustering is given
in Figure 1. We do not cluster preterminals.
These remain fixed at all levels to the standard
Penn-tree-bank set Marcus et al. (1993).
Level-0 makes two distinctions, the root node
and everybody else. At level 1 we make one
further distinction, between phrases that tend
to be heads of constituents (NPs, VPs, and Ss)
and those that tend to be modifiers (ADJPs,
PPs, etc.). Level-2 has a total of five categories:
root, things that are typically headed by nouns,
those headed by verbs, things headed by prepo-
sitions, and things headed by classical modifiers
(adjectives, adverbs, etc.). Finally, level 3 is the
</bodyText>
<figure confidence="0.983545310344828">
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
HP
MP
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
⎧
⎨⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎩
⎧
⎨⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎩
⎧
⎨⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎩
⎧
⎨⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎩
S
N
P
A
P
170
</figure>
<figureCaption confidence="0.999669">
Figure 2: A tree represented at levels 0 to 3
</figureCaption>
<bodyText confidence="0.985792588235294">
classical tree-bank set. As an example, Figure 2
shows the parse for the sentence “He ate at the
mall.” at levels 0 to 3.
During training we create four grammars, one
for each level of granularity. So, for example, at
level 1 the tree-bank rule
S →NP VP .
would be translated into the rule
HP →HP HP .
That is, each constituent type found in “S →NP
VP .” is mapped into its generalization at level 1.
The probabilities of all rules are computed us-
ing maximum likelihood for constituents at that
level.
The grammar used by the parser can best be
described as being influenced by four compo-
nents:
</bodyText>
<listItem confidence="0.989777285714286">
1. the nonterminals defined at that level of
parsing,
2. the binarization scheme,
3. the generalizations defined over the bina-
rization, and
4. extra annotation to improve parsing accu-
racy.
</listItem>
<bodyText confidence="0.993697625">
The first of these has already been covered. We
discuss the other three in turn.
In anticipation of eventually lexicalizing the
grammar we binarize from the head out. For
example, consider the rule
A →a b c d e
where c is the head constituent. We binarize
this as follows:
</bodyText>
<equation confidence="0.8289775">
A →A1 e
A1 →A2 d
A2 →a A3
A3 →b c
</equation>
<bodyText confidence="0.922960962962963">
Grammars induced in this way tend to be
too specific, as the binarization introduce a very
large number of very specialized phrasal cat-
egories (the Ai). Following common practice
Johnson (1998; Klein and Manning (2003b) we
Markovize by replacing these nonterminals with
ones that remember less of the immediate rule
context. In our version we keep track of only the
parent, the head constituent and the constituent
immediately to the right or left, depending on
which side of the constituent we are processing.
With this scheme the above rules now look like
this:
A →Ad,c e
Ad,c →Aa,c d
Aa,c →a Ab,c
Ab,c →b c
So, for example, the rule “A →Ad,c e” would
have a high probability if constituents of type
A, with c as their head, often have d followed
by e at their end.
Lastly, we add parent annotation to phrasal
categories to improve parsing accuracy. If we
assume that in this case we are expanding a rule
for an A used as a child of Q, and a, b, c, d, and
e are all phrasal categories, then the above rules
become:
</bodyText>
<footnote confidence="0.4577775">
AQ →Ad,c eA
Ad,c →Aa,c dA
Aa,c →aA Ab,c
Ab,c →bA cA
</footnote>
<figure confidence="0.988359406779661">
S1 S1
P HP
P
PRP
He
IN
at
P
P
DT
.
P
NN
HP
PRP
He
IN
at
HP
MP
DT
.
.
NN
VBD
VBD
ate
ate
.
HP
the mall the mall
S_
N_
PRP
PP
PRP
P_
.
VBD
.
VBD
VP
NP
He
ate
IN
N_
He
IN
NP
.
ate
.
at DT NN at DT NN
the mall the mall
S1
S
S1
S_
</figure>
<page confidence="0.759185">
171
</page>
<figureCaption confidence="0.987727166666667">
Figure 3: Probability of a gold constituent being
pruned as a function of pruning thresholds for
the first 100 sentences of the development corpus
Figure 4: Fraction of incorrect constituents kept
as a function of pruning thresholds for the first
100 sentences of the development corpus
</figureCaption>
<figure confidence="0.968532222222222">
0.1
Level 0
Level 1
Level 2
Level 3
0.01
0.001
0.0001
10−8 10−7 10−6 10−5 10−4 10−3
1
0.1
0.01
Level 0
Level 1
Level 2
Level 3
0.001
10−8 10−7 10−6 10−5 10−4 10−3
</figure>
<bodyText confidence="0.9998263">
Once we have parsed at a level, we evaluate
the probability of a constituent p(nki,j  |s) ac-
cording to the standard inside-outside formula
of Equation 1. In this equation nki,j is a con-
stituent of type k spanning the words i to j, and
α(·) and β(·) are the outside and inside proba-
bilities of the constituent, respectively. Because
we prune at the end each granularity level, we
can evaluate the equation exactly; no approxi-
mations are needed (as in, e.g., Charniak et al.
(1998)).
During parsing, instead of building each con-
stituent allowed by the grammar, we first test
if the probability of the corresponding coarser
constituent (which we have from Equation 1 in
the previous round of parsing) is greater than
a threshold. (The threshold is set empirically
based upon the development data.) If it is below
the threshold, we do not put the constituent in
the chart. For example, before we can use a NP
and a VP to create a S (using the rule above),
we would first need to check that the probability
in the coarser grammar of using the same span
HP and HP to create a HP is above the thresh-
old. We use the standard inside-outside for-
mula to calculate this probability (Equation 1).
The empirical results below justify our conjec-
ture that there are thresholds that allow signifi-
cant pruning while leaving the gold constituents
untouched.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999847413793104">
In all experiments the system is trained on the
Penn tree-bank sections 2-21. Section 23 is used
for testing and section 24 for development. The
input to the parser are the gold-standard parts
of speech, not the words.
The point of parsing at multiple levels of gran-
ularity is to prune the results of rough levels be-
fore going on to finer levels. In particular, it is
necessary for any pruning scheme to retain the
true (gold-standard WSJ) constituents in the
face of the pruning. To gain an idea of what
is possible, consider Figure 3. According to the
graph, at the zeroth level of parsing and a the
pruning level 10−4 the probability that a gold
constituent is deleted due to pruning is slightly
more than 0.001 (or 0.1%). At level three it is
slightly more that 0.01 (or 1.0%).
The companion figure, Figure 4 shows the
retention rate of the non-gold (incorrect) con-
stituents. Again, at pruning level 10−4 and pars-
ing level 0 we retain about .3 (30%) of the bad
constituents (so we pruned 70%), whereas at
level 3 we retain about .004 (0.4%). Note that
in the current paper we do not actually prune
at level 3, instead return the Viterbi parse. We
include pruning results here in anticipation of
future work in which level 3 would be a precur-
sor to still more fine-grained parsing.
As noted in Section 2, there is some (implicit)
</bodyText>
<page confidence="0.991204">
172
</page>
<table confidence="0.998679333333333">
Level Constits Constits % Pruned
Produced Pruned
*106 *106
0 8.82 7.55 86.5
1 9.18 6.51 70.8
2 11.2 9.48 84.4
3 11,8 0 0.0
total 40.4 – –
3-only 392.0 0 0
</table>
<figureCaption confidence="0.9069525">
Figure 5: Total constituents pruned at all levels
for WSJ section 23, sentences of length G 100
</figureCaption>
<bodyText confidence="0.99757">
debate in the literature on using estimates of
the outside probability in Equation 1, or instead
computing the exact upper bound. The idea is
that an exact upper bound gives one an admis-
sible search heuristic but at a cost, since it is a
less accurate estimator of the true outside prob-
ability. (Note that even the upper bound does
not, in general, keep all of the gold constituents,
since a non-perfect model will assign some of
them low probability.) As is clear from Figure
3, the estimate works very well indeed.
On the basis of this graph, we set the lowest
allowable constituent probability at &gt; 5 · 10−4,
&gt; 10−5, and &gt; 10−4 for levels 0,1, and 2, re-
spectively. No pruning is done at level 3, since
there is no level 4. After setting the pruning
parameters on the development set we proceed
to parse the test set (WSJ section 23). Figure 5
shows the resulting pruning statistics. The to-
tal number of constituents created at level 0, for
all sentences combined, is 8.82 · 106. Of those
7.55 · 106 (or 86.5%) are pruned before going on
to level 1. At level 1, the 1.3 million left over
from level 0 expanded to a total of 9.18 · 106.
70.8% of these in turn are pruned, and so forth.
The percent pruned at, e.g., level 1 in Figure 3
is much higher than that shown here because it
considers all of the possible level-1 constituents,
not just those left unpruned after level 0.
There is no pruning at level 3. There we sim-
ply return the Viterbi parse. We also show that
with pruning we generate a total of 40.4 · 106
constituents. For comparison exhaustively pars-
ing using the tree-bank grammar yields a total
of 392·106 constituents. This is the factor-of-10
Level Time for Level Running Total
</bodyText>
<figure confidence="0.9976708">
0 1598 1598
1 2570 4168
2 4303 8471
3 1527 9998
3-only 114654 –
</figure>
<figureCaption confidence="0.806273333333333">
Figure 6: Running times in seconds on WSJ sec-
tion 23, with and without pruning
workload reduction mentioned in Section 1.
</figureCaption>
<bodyText confidence="0.999961305555556">
There are two points of interest. The first is
that each level of pruning is worthwhile. We do
not get most of the effect from one or the other
level. The second point is that we get signif-
icant pruning at level 0. The reader may re-
member that level 0 distinguishes only between
the root node and the rest. We initially ex-
pected that it would be too coarse to distinguish
good from bad constituents at this level, but it
proved as useful as the other levels. The expla-
nation is that this level does use the full tree-
bank preterminal tags, and in many cases these
alone are sufficient to make certain constituents
very unlikely. For example, what is the proba-
bility of any constituent of length two or greater
ending in a preposition? The answer is: very
low. Similarly for constituents of length two or
greater ending in modal verbs, and determiners.
Not quite so improbable, but nevertheless less
likely than most, would be constituents ending
in verbs, or ending just short of the end of the
sentence.
Figure 6 shows how much time is spent at each
level of the algorithm, along with a running to-
tal of the time spent to that point. (This is for
all sentences in the test set, length G 100.) The
number for the unpruned parser is again about
ten times that for the pruned version, but the
number for the standard CKY version is prob-
ably too high. Because our CKY implementa-
tion is quite slow, we ran the unpruned version
on many machines and summed the results. In
all likelihood at least some of these machines
were overloaded, a fact that our local job dis-
tributer would not notice. We suspect that the
real number is significantly lower, though still
</bodyText>
<page confidence="0.995556">
173
</page>
<figure confidence="0.487557333333333">
No pruning 77.9
With pruning 77.9
Klein and Manning (2003b) 77.4
</figure>
<figureCaption confidence="0.859349">
Figure 7: Labeled precision/recall f-measure,
WSJ section 23, all sentences of length ≤ 100
</figureCaption>
<bodyText confidence="0.987854518518518">
much higher than the pruned version.
Finally Figure 7 shows that our pruning is ac-
complished without loss of accuracy. The results
with pruning include four sentences that did not
receive any parses at all. These sentences re-
ceived zeros for both precision and recall and
presumably lowered the results somewhat. We
allowed ourselves to look at the first of these,
which turned out to contain the phrase:
(NP ... (INTJ (UH oh) (UH yes)) ...)
The training data does not include interjections
consisting of two “UH”s, and thus a gold parse
cannot be constructed. Note that a different
binarization scheme (e.g. the one used in Klein
and Manning (2003b) would have smoothed over
this problem. In our case the unpruned version
is able to patch together a lot of very unlikely
constituents to produce a parse, but not a very
good one. Thus we attribute the problem not to
pruning, but to binarization.
We also show the results for the most similar
Klein and Manning (2003b) experiment. Our
results are slightly better. We attribute the dif-
ference to the fact that we have the gold tags
and they do not, but their binarization scheme
does not run into the problems that we encoun-
tered.
</bodyText>
<sectionHeader confidence="0.993516" genericHeader="conclusions">
5 Conclusion and Future Research
</sectionHeader>
<bodyText confidence="0.999990206896552">
We have presented a novel parsing algorithm
based upon the coarse-to-fine processing model.
Several aspects of the method recommend it.
First, unlike methods that depend on best-first
search, the method is “holistic” in its evalua-
tion of constituents. For example, consider the
impact of parent labeling. It has been repeat-
edly shown to improve parsing accuracy (John-
son, 1998; Charniak, 2000; Klein and Manning,
2003b), but it is difficult if not impossible to
integrate with best-first search in bottom-up
chart-parsing (as in Charniak et al. (1998)). The
reason is that when working bottom up it is diffi-
cult to determine if, say, ssbar is any more or less
likely than ss, as the evidence, working bottom
up, is negligible. Since our method computes
the exact outside probability of constituents (al-
beit at a coarser level) all of the top down in-
formation is available to the system. Or again,
another very useful feature in English parsing
is the knowledge that a constituent ends at the
right boundary (minus punctuation) of a string.
This can be included only in an ad-hoc way when
working bottom up, but could be easily added
here.
Many aspects of the current implementation
that are far from optimal. It seems clear to
us that extracting the maximum benefit from
our pruning would involve taking the unpruned
constituents and specifying them in all possible
ways allowed by the next level of granularity.
What we actually did is to propose all possi-
ble constituents at the next level, and immedi-
ately rule out those lacking a corresponding con-
stituent remaining at the previous level. This
was dictated by ease of implementation. Before
using mlctf parsing in a production parser, the
other method should be evaluated to see if our
intuitions of greater efficiency are correct.
It is also possible to combine mlctf parsing
with queue reordering methods. The best-first
search method of Charniak et al. (1998) esti-
mates Equation 1. Working bottom up, estimat-
ing the inside probability is easy (we just sum
the probability of all the trees found to build
this constituent). All the cleverness goes into
estimating the outside probability. Quite clearly
the current method could be used to provide a
more accurate estimate of the outside probabil-
ity, namely the outside probability at the coarser
level of granularity.
There is one more future-research topic to add
before we stop, possibly the most interesting of
all. The particular tree of coarser to finer con-
stituents that governs our mlctf algorithm (Fig-
ure 1) was created by hand after about 15 min-
utes of reflection and survives, except for typos,
with only two modifications. There is no rea-
</bodyText>
<page confidence="0.993375">
174
</page>
<bodyText confidence="0.999958333333333">
son to think it is anywhere close to optimal. It
should be possible to define “optimal” formally
and search for the best mlctf constituent tree.
This would be a clustering problem, and, for-
tunately, one thing statistical NLP researchers
know how to do is cluster.
</bodyText>
<sectionHeader confidence="0.998114" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999505">
This paper is the class project for Computer
Science 241 at Brown University in fall 2005.
The faculty involved were supported in part
by DARPA GALE contract HR0011-06-2-0001.
The graduate students were mostly supported
by Brown University fellowships. The under-
graduates were mostly supported by their par-
ents. Our thanks to all.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905135135135">
Sharon Caraballo and Eugene Charniak. 1998. Fig-
ures of merit for best-first probabalistic parsing.
Computational Linguistics, 24(2):275–298.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 2005 Meeting of
the Association for Computational Linguistics.
Eugene Charniak, Sharon Goldwater, and Mark
Johnson. 1998. Edge-based best-first chart pars-
ing. In Proceedings of the Sixth Workshop on
Very Large Corpora, pages 127–133. Morgan Kauf-
mann.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 132–139.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, San Francisco. Mor-
gan Kaufmann.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 457–464.
Stuart Geman and Kevin Kochanek. 2001. Dy-
namic programming and the representation of
soft-decodable codes. IEEE Transactions on In-
formation Theory, 47:549–568.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 1997).
Keith Hall and Mark Johnson. 2004. Attention shift-
ing for parsing speech. In The Proceedings of the
42th Annual Meeting of the Association for Com-
putational Linguistics, pages 40–46.
Keith Hall. 2004. Best-first Word-lattice Pars-
ing: Techniques for Integrated Syntactic Language
Modeling. Ph.D. thesis, Brown University.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 33–
39.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613–632.
Dan Klein and Chris Manning. 2003a. A* parsing:
Fast exact viterbi parse selection. In Proceedings
of HLT-NAACL’03.
Dan Klein and Christopher Manning. 2003b. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313–
330.
John T. Maxwell and Ronald M. Kaplan. 1993.
The interface between phrasal and functional con-
straints. Computational Linguistics, 19(4):571–
590.
Ryan McDonald, Toby Crammer, and Fernando
Pereira. 2005. Online large margin training of
dependency parsers. In Proceedings of the 43rd
Meeting of the Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2004. It-
erative cky parsing for probabilistic context-free
grammars. In International Joint Conference on
Natural-Language Processing.
</reference>
<page confidence="0.998715">
175
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.648081">
<title confidence="0.999643">Multilevel Coarse-to-fine PCFG Parsing</title>
<author confidence="0.947109666666667">Eugene Charniak</author>
<author confidence="0.947109666666667">Mark Johnson</author>
<author confidence="0.947109666666667">Micha Elsner</author>
<author confidence="0.947109666666667">Joseph David Ellis</author>
<author confidence="0.947109666666667">Isaac Haxton</author>
<author confidence="0.947109666666667">Catherine Hill</author>
<author confidence="0.947109666666667">R Jeremy Moore</author>
<author confidence="0.947109666666667">Michael Pozar</author>
<author confidence="0.947109666666667">Theresa</author>
<affiliation confidence="0.9351025">Laboratory for Linguistic Information Processing Brown</affiliation>
<address confidence="0.836855">Providence, RI</address>
<email confidence="0.99947">ec@cs.brown.edu</email>
<abstract confidence="0.999877714285714">We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse. Our approach requires the user to specify a sequence of nested partitions or equivalence classes of the PCFG nonterminals. We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG. We use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level. We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG. We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sharon Caraballo</author>
<author>Eugene Charniak</author>
</authors>
<title>Figures of merit for best-first probabalistic parsing.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="5158" citStr="Caraballo and Charniak (1998)" startWordPosition="821" endWordPosition="824">the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: ��,) . (1) p(n� ��, s) = α(n� ��,)β(n� p(s) The nonterminal categories in these grammars can be regarded as clusters or equivalence classes of the original Penn treebank nonterminal categories. (In fact, we obtain these grammars by relabeling the node labels in the treebank and extracting a PCFG from this relabelled treebank</context>
<context position="7754" citStr="Caraballo and Charniak (1998)" startWordPosition="1257" endWordPosition="1260">standard CKY parsing at the final level. We also discuss some fine points of the results therein. Finally in section 5 we suggest that because the search space of mlctf algorithms is, at this point, almost totally unexplored, future work should be able to improve significantly on these results. 2 Previous Research Coarse-to-fine search is an idea that has appeared several times in the literature of computational linguistics and related areas. The 169 It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A* for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A* search is guaranteed to find the best parse first. One </context>
</contexts>
<marker>Caraballo, Charniak, 1998</marker>
<rawString>Sharon Caraballo and Eugene Charniak. 1998. Figures of merit for best-first probabalistic parsing. Computational Linguistics, 24(2):275–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5066" citStr="Charniak and Johnson (2005)" startWordPosition="806" endWordPosition="809">parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: ��,) . (1) p(n� ��, s) = α(n� ��,)β(n� p(s) The nonterminal categories in these grammars can be regarded as clusters or equivalence classes of the original Penn treebank nonterminal categories. (In fact, we obtain these grammars by re</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 2005 Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Edge-based best-first chart parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>127--133</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="5197" citStr="Charniak et al. (1998)" startWordPosition="828" endWordPosition="831">found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: ��,) . (1) p(n� ��, s) = α(n� ��,)β(n� p(s) The nonterminal categories in these grammars can be regarded as clusters or equivalence classes of the original Penn treebank nonterminal categories. (In fact, we obtain these grammars by relabeling the node labels in the treebank and extracting a PCFG from this relabelled treebank in the standard fashion, but we discus</context>
<context position="7880" citStr="Charniak et al. (1998)" startWordPosition="1278" endWordPosition="1281">t because the search space of mlctf algorithms is, at this point, almost totally unexplored, future work should be able to improve significantly on these results. 2 Previous Research Coarse-to-fine search is an idea that has appeared several times in the literature of computational linguistics and related areas. The 169 It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A* for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A* search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruo</context>
<context position="14375" citStr="Charniak et al. (1998)" startWordPosition="2460" endWordPosition="2463"> Level 2 Level 3 0.01 0.001 0.0001 10−8 10−7 10−6 10−5 10−4 10−3 1 0.1 0.01 Level 0 Level 1 Level 2 Level 3 0.001 10−8 10−7 10−6 10−5 10−4 10−3 Once we have parsed at a level, we evaluate the probability of a constituent p(nki,j |s) according to the standard inside-outside formula of Equation 1. In this equation nki,j is a constituent of type k spanning the words i to j, and α(·) and β(·) are the outside and inside probabilities of the constituent, respectively. Because we prune at the end each granularity level, we can evaluate the equation exactly; no approximations are needed (as in, e.g., Charniak et al. (1998)). During parsing, instead of building each constituent allowed by the grammar, we first test if the probability of the corresponding coarser constituent (which we have from Equation 1 in the previous round of parsing) is greater than a threshold. (The threshold is set empirically based upon the development data.) If it is below the threshold, we do not put the constituent in the chart. For example, before we can use a NP and a VP to create a S (using the rule above), we would first need to check that the probability in the coarser grammar of using the same span HP and HP to create a HP is abo</context>
<context position="22295" citStr="Charniak et al. (1998)" startWordPosition="3875" endWordPosition="3878">nto the problems that we encountered. 5 Conclusion and Future Research We have presented a novel parsing algorithm based upon the coarse-to-fine processing model. Several aspects of the method recommend it. First, unlike methods that depend on best-first search, the method is “holistic” in its evaluation of constituents. For example, consider the impact of parent labeling. It has been repeatedly shown to improve parsing accuracy (Johnson, 1998; Charniak, 2000; Klein and Manning, 2003b), but it is difficult if not impossible to integrate with best-first search in bottom-up chart-parsing (as in Charniak et al. (1998)). The reason is that when working bottom up it is difficult to determine if, say, ssbar is any more or less likely than ss, as the evidence, working bottom up, is negligible. Since our method computes the exact outside probability of constituents (albeit at a coarser level) all of the top down information is available to the system. Or again, another very useful feature in English parsing is the knowledge that a constituent ends at the right boundary (minus punctuation) of a string. This can be included only in an ad-hoc way when working bottom up, but could be easily added here. Many aspects</context>
<context position="23660" citStr="Charniak et al. (1998)" startWordPosition="4107" endWordPosition="4110">ve taking the unpruned constituents and specifying them in all possible ways allowed by the next level of granularity. What we actually did is to propose all possible constituents at the next level, and immediately rule out those lacking a corresponding constituent remaining at the previous level. This was dictated by ease of implementation. Before using mlctf parsing in a production parser, the other method should be evaluated to see if our intuitions of greater efficiency are correct. It is also possible to combine mlctf parsing with queue reordering methods. The best-first search method of Charniak et al. (1998) estimates Equation 1. Working bottom up, estimating the inside probability is easy (we just sum the probability of all the trees found to build this constituent). All the cleverness goes into estimating the outside probability. Quite clearly the current method could be used to provide a more accurate estimate of the outside probability, namely the outside probability at the coarser level of granularity. There is one more future-research topic to add before we stop, possibly the most interesting of all. The particular tree of coarser to finer constituents that governs our mlctf algorithm (Figu</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998. Edge-based best-first chart parsing. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 127–133. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="4657" citStr="Charniak (2000)" startWordPosition="741" endWordPosition="742"> the multilevel dynamic programming algorithm needed for coarse-to-fine analysis (which they apply to decoding rather than parsing), and show how to perform exact coarse-to-fine computation, rather than the heuristic search we perform here. A paper closely related to ours is Goodman (1997). In our terminology, Goodman’s parser is a two-stage ctf parser. The second stage is a standard tree-bank parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsin</context>
<context position="9136" citStr="Charniak (2000)" startWordPosition="1493" endWordPosition="1494">k developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only Level: 0 1 2 3 S1 { S1 { S1 { S</context>
<context position="22136" citStr="Charniak, 2000" startWordPosition="3852" endWordPosition="3853">s are slightly better. We attribute the difference to the fact that we have the gold tags and they do not, but their binarization scheme does not run into the problems that we encountered. 5 Conclusion and Future Research We have presented a novel parsing algorithm based upon the coarse-to-fine processing model. Several aspects of the method recommend it. First, unlike methods that depend on best-first search, the method is “holistic” in its evaluation of constituents. For example, consider the impact of parent labeling. It has been repeatedly shown to improve parsing accuracy (Johnson, 1998; Charniak, 2000; Klein and Manning, 2003b), but it is difficult if not impossible to integrate with best-first search in bottom-up chart-parsing (as in Charniak et al. (1998)). The reason is that when working bottom up it is difficult to determine if, say, ssbar is any more or less likely than ss, as the evidence, working bottom up, is negligible. Since our method computes the exact outside probability of constituents (albeit at a coarser level) all of the top down information is available to the system. Or again, another very useful feature in English parsing is the knowledge that a constituent ends at the </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="9213" citStr="Collins (1997)" startWordPosition="1507" endWordPosition="1508">equired by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only Level: 0 1 2 3 S1 { S1 { S1 { S1 S VP UCP SQ SBAR SBARQ SINV NP NAC NX LST X UCP FRAG ADJP QP CONJP ADVP INT</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>457--464</pages>
<contexts>
<context position="9332" citStr="Eisner and Satta (1999)" startWordPosition="1528" endWordPosition="1531">t does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only Level: 0 1 2 3 S1 { S1 { S1 { S1 S VP UCP SQ SBAR SBARQ SINV NP NAC NX LST X UCP FRAG ADJP QP CONJP ADVP INTJ PRN PRT PP PRT RRC WHADJP WHADVP WHNP WHPP Figure 1: The levels of nonterminal labels after most constituents have be</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 457–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Kevin Kochanek</author>
</authors>
<title>Dynamic programming and the representation of soft-decodable codes.</title>
<date>2001</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>47--549</pages>
<contexts>
<context position="4020" citStr="Geman and Kochanek (2001)" startWordPosition="640" endWordPosition="643">idea we are aware of is in Maxwell and Kaplan (1993), where a covering CFG is automatically extracted from a more detailed unification grammar and used to identify the possible locations of constituents in the more detailed parses of the sentence. Maxwell and Kaplan use their covering CFG to prune the search of their unification grammar parser in essentially the same manner as we do here, and demonstrate significant performance improvements by using their coarse-to-fine approach. The basic theory of coarse-to-fine approximations and dynamic programming in a stochastic framework is laid out in Geman and Kochanek (2001). This paper describes the multilevel dynamic programming algorithm needed for coarse-to-fine analysis (which they apply to decoding rather than parsing), and show how to perform exact coarse-to-fine computation, rather than the heuristic search we perform here. A paper closely related to ours is Goodman (1997). In our terminology, Goodman’s parser is a two-stage ctf parser. The second stage is a standard tree-bank parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is sm</context>
</contexts>
<marker>Geman, Kochanek, 2001</marker>
<rawString>Stuart Geman and Kevin Kochanek. 2001. Dynamic programming and the representation of soft-decodable codes. IEEE Transactions on Information Theory, 47:549–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple-pass parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="4332" citStr="Goodman (1997)" startWordPosition="688" endWordPosition="689">ication grammar parser in essentially the same manner as we do here, and demonstrate significant performance improvements by using their coarse-to-fine approach. The basic theory of coarse-to-fine approximations and dynamic programming in a stochastic framework is laid out in Geman and Kochanek (2001). This paper describes the multilevel dynamic programming algorithm needed for coarse-to-fine analysis (which they apply to decoding rather than parsing), and show how to perform exact coarse-to-fine computation, rather than the heuristic search we perform here. A paper closely related to ours is Goodman (1997). In our terminology, Goodman’s parser is a two-stage ctf parser. The second stage is a standard tree-bank parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores al</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and multiple-pass parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Mark Johnson</author>
</authors>
<title>Attention shifting for parsing speech.</title>
<date>2004</date>
<booktitle>In The Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--46</pages>
<contexts>
<context position="2097" citStr="Hall and Johnson, 2004" startWordPosition="333" endWordPosition="336">ll too slow for many applications. In some cases researchers need large quantities of parsed data and do not have the hundreds of machines necessary to parse gigaword corpora in a week or two. More pressingly, in real-time applications such as speech recognition, a parser would be only a part of a much larger system, and the system builders are not keen on giving the parser one of the ten seconds available to process, say, a thirty-word sentence. Even worse, some applications require the parsing of multiple candidate strings per sentence (Johnson and Charniak, 2004) or parsing from a lattice (Hall and Johnson, 2004), and in these applications parsing efficiency is even more important. We present here a multilevel coarse-to-fine (mlctf) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse. It defines a sequence of increasingly more complex PCFGs, and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG. We currently use four levels of grammars in our mlctf algorithm. The simplest PCFG, which we call the level-0 grammar, contains only one nontrivial nonterminal and is so simple that minimal time is needed to parse a sent</context>
</contexts>
<marker>Hall, Johnson, 2004</marker>
<rawString>Keith Hall and Mark Johnson. 2004. Attention shifting for parsing speech. In The Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics, pages 40–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
</authors>
<title>Best-first Word-lattice Parsing: Techniques for Integrated Syntactic Language Modeling.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="5034" citStr="Hall (2004)" startWordPosition="803" endWordPosition="804">ndard tree-bank parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: ��,) . (1) p(n� ��, s) = α(n� ��,)β(n� p(s) The nonterminal categories in these grammars can be regarded as clusters or equivalence classes of the original Penn treebank nonterminal categories. (In fact</context>
</contexts>
<marker>Hall, 2004</marker>
<rawString>Keith Hall. 2004. Best-first Word-lattice Parsing: Techniques for Integrated Syntactic Language Modeling. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A TAGbased noisy-channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--39</pages>
<contexts>
<context position="2046" citStr="Johnson and Charniak, 2004" startWordPosition="324" endWordPosition="327">about a second per sentence. Unfortunately, this is still too slow for many applications. In some cases researchers need large quantities of parsed data and do not have the hundreds of machines necessary to parse gigaword corpora in a week or two. More pressingly, in real-time applications such as speech recognition, a parser would be only a part of a much larger system, and the system builders are not keen on giving the parser one of the ten seconds available to process, say, a thirty-word sentence. Even worse, some applications require the parsing of multiple candidate strings per sentence (Johnson and Charniak, 2004) or parsing from a lattice (Hall and Johnson, 2004), and in these applications parsing efficiency is even more important. We present here a multilevel coarse-to-fine (mlctf) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse. It defines a sequence of increasingly more complex PCFGs, and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG. We currently use four levels of grammars in our mlctf algorithm. The simplest PCFG, which we call the level-0 grammar, contains only one nontrivial nonterminal and is so</context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A TAGbased noisy-channel model of speech repairs. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 33– 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="12340" citStr="Johnson (1998" startWordPosition="2055" endWordPosition="2056">cheme, 3. the generalizations defined over the binarization, and 4. extra annotation to improve parsing accuracy. The first of these has already been covered. We discuss the other three in turn. In anticipation of eventually lexicalizing the grammar we binarize from the head out. For example, consider the rule A →a b c d e where c is the head constituent. We binarize this as follows: A →A1 e A1 →A2 d A2 →a A3 A3 →b c Grammars induced in this way tend to be too specific, as the binarization introduce a very large number of very specialized phrasal categories (the Ai). Following common practice Johnson (1998; Klein and Manning (2003b) we Markovize by replacing these nonterminals with ones that remember less of the immediate rule context. In our version we keep track of only the parent, the head constituent and the constituent immediately to the right or left, depending on which side of the constituent we are processing. With this scheme the above rules now look like this: A →Ad,c e Ad,c →Aa,c d Aa,c →a Ab,c Ab,c →b c So, for example, the rule “A →Ad,c e” would have a high probability if constituents of type A, with c as their head, often have d followed by e at their end. Lastly, we add parent an</context>
<context position="22120" citStr="Johnson, 1998" startWordPosition="3849" endWordPosition="3851">ent. Our results are slightly better. We attribute the difference to the fact that we have the gold tags and they do not, but their binarization scheme does not run into the problems that we encountered. 5 Conclusion and Future Research We have presented a novel parsing algorithm based upon the coarse-to-fine processing model. Several aspects of the method recommend it. First, unlike methods that depend on best-first search, the method is “holistic” in its evaluation of constituents. For example, consider the impact of parent labeling. It has been repeatedly shown to improve parsing accuracy (Johnson, 1998; Charniak, 2000; Klein and Manning, 2003b), but it is difficult if not impossible to integrate with best-first search in bottom-up chart-parsing (as in Charniak et al. (1998)). The reason is that when working bottom up it is difficult to determine if, say, ssbar is any more or less likely than ss, as the evidence, working bottom up, is negligible. Since our method computes the exact outside probability of constituents (albeit at a coarser level) all of the top down information is available to the system. Or again, another very useful feature in English parsing is the knowledge that a constitu</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>A* parsing: Fast exact viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL’03.</booktitle>
<contexts>
<context position="8023" citStr="Klein and Manning (2003" startWordPosition="1304" endWordPosition="1307">ly on these results. 2 Previous Research Coarse-to-fine search is an idea that has appeared several times in the literature of computational linguistics and related areas. The 169 It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A* for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A* search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap mani</context>
<context position="12365" citStr="Klein and Manning (2003" startWordPosition="2057" endWordPosition="2060">eneralizations defined over the binarization, and 4. extra annotation to improve parsing accuracy. The first of these has already been covered. We discuss the other three in turn. In anticipation of eventually lexicalizing the grammar we binarize from the head out. For example, consider the rule A →a b c d e where c is the head constituent. We binarize this as follows: A →A1 e A1 →A2 d A2 →a A3 A3 →b c Grammars induced in this way tend to be too specific, as the binarization introduce a very large number of very specialized phrasal categories (the Ai). Following common practice Johnson (1998; Klein and Manning (2003b) we Markovize by replacing these nonterminals with ones that remember less of the immediate rule context. In our version we keep track of only the parent, the head constituent and the constituent immediately to the right or left, depending on which side of the constituent we are processing. With this scheme the above rules now look like this: A →Ad,c e Ad,c →Aa,c d Aa,c →a Ab,c Ab,c →b c So, for example, the rule “A →Ad,c e” would have a high probability if constituents of type A, with c as their head, often have d followed by e at their end. Lastly, we add parent annotation to phrasal categ</context>
<context position="20431" citStr="Klein and Manning (2003" startWordPosition="3564" endWordPosition="3567">g total of the time spent to that point. (This is for all sentences in the test set, length G 100.) The number for the unpruned parser is again about ten times that for the pruned version, but the number for the standard CKY version is probably too high. Because our CKY implementation is quite slow, we ran the unpruned version on many machines and summed the results. In all likelihood at least some of these machines were overloaded, a fact that our local job distributer would not notice. We suspect that the real number is significantly lower, though still 173 No pruning 77.9 With pruning 77.9 Klein and Manning (2003b) 77.4 Figure 7: Labeled precision/recall f-measure, WSJ section 23, all sentences of length ≤ 100 much higher than the pruned version. Finally Figure 7 shows that our pruning is accomplished without loss of accuracy. The results with pruning include four sentences that did not receive any parses at all. These sentences received zeros for both precision and recall and presumably lowered the results somewhat. We allowed ourselves to look at the first of these, which turned out to contain the phrase: (NP ... (INTJ (UH oh) (UH yes)) ...) The training data does not include interjections consistin</context>
<context position="22161" citStr="Klein and Manning, 2003" startWordPosition="3854" endWordPosition="3857">etter. We attribute the difference to the fact that we have the gold tags and they do not, but their binarization scheme does not run into the problems that we encountered. 5 Conclusion and Future Research We have presented a novel parsing algorithm based upon the coarse-to-fine processing model. Several aspects of the method recommend it. First, unlike methods that depend on best-first search, the method is “holistic” in its evaluation of constituents. For example, consider the impact of parent labeling. It has been repeatedly shown to improve parsing accuracy (Johnson, 1998; Charniak, 2000; Klein and Manning, 2003b), but it is difficult if not impossible to integrate with best-first search in bottom-up chart-parsing (as in Charniak et al. (1998)). The reason is that when working bottom up it is difficult to determine if, say, ssbar is any more or less likely than ss, as the evidence, working bottom up, is negligible. Since our method computes the exact outside probability of constituents (albeit at a coarser level) all of the top down information is available to the system. Or again, another very useful feature in English parsing is the knowledge that a constituent ends at the right boundary (minus pun</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003a. A* parsing: Fast exact viterbi parse selection. In Proceedings of HLT-NAACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8023" citStr="Klein and Manning (2003" startWordPosition="1304" endWordPosition="1307">ly on these results. 2 Previous Research Coarse-to-fine search is an idea that has appeared several times in the literature of computational linguistics and related areas. The 169 It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A* for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A* search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap mani</context>
<context position="12365" citStr="Klein and Manning (2003" startWordPosition="2057" endWordPosition="2060">eneralizations defined over the binarization, and 4. extra annotation to improve parsing accuracy. The first of these has already been covered. We discuss the other three in turn. In anticipation of eventually lexicalizing the grammar we binarize from the head out. For example, consider the rule A →a b c d e where c is the head constituent. We binarize this as follows: A →A1 e A1 →A2 d A2 →a A3 A3 →b c Grammars induced in this way tend to be too specific, as the binarization introduce a very large number of very specialized phrasal categories (the Ai). Following common practice Johnson (1998; Klein and Manning (2003b) we Markovize by replacing these nonterminals with ones that remember less of the immediate rule context. In our version we keep track of only the parent, the head constituent and the constituent immediately to the right or left, depending on which side of the constituent we are processing. With this scheme the above rules now look like this: A →Ad,c e Ad,c →Aa,c d Aa,c →a Ab,c Ab,c →b c So, for example, the rule “A →Ad,c e” would have a high probability if constituents of type A, with c as their head, often have d followed by e at their end. Lastly, we add parent annotation to phrasal categ</context>
<context position="20431" citStr="Klein and Manning (2003" startWordPosition="3564" endWordPosition="3567">g total of the time spent to that point. (This is for all sentences in the test set, length G 100.) The number for the unpruned parser is again about ten times that for the pruned version, but the number for the standard CKY version is probably too high. Because our CKY implementation is quite slow, we ran the unpruned version on many machines and summed the results. In all likelihood at least some of these machines were overloaded, a fact that our local job distributer would not notice. We suspect that the real number is significantly lower, though still 173 No pruning 77.9 With pruning 77.9 Klein and Manning (2003b) 77.4 Figure 7: Labeled precision/recall f-measure, WSJ section 23, all sentences of length ≤ 100 much higher than the pruned version. Finally Figure 7 shows that our pruning is accomplished without loss of accuracy. The results with pruning include four sentences that did not receive any parses at all. These sentences received zeros for both precision and recall and presumably lowered the results somewhat. We allowed ourselves to look at the first of these, which turned out to contain the phrase: (NP ... (INTJ (UH oh) (UH yes)) ...) The training data does not include interjections consistin</context>
<context position="22161" citStr="Klein and Manning, 2003" startWordPosition="3854" endWordPosition="3857">etter. We attribute the difference to the fact that we have the gold tags and they do not, but their binarization scheme does not run into the problems that we encountered. 5 Conclusion and Future Research We have presented a novel parsing algorithm based upon the coarse-to-fine processing model. Several aspects of the method recommend it. First, unlike methods that depend on best-first search, the method is “holistic” in its evaluation of constituents. For example, consider the impact of parent labeling. It has been repeatedly shown to improve parsing accuracy (Johnson, 1998; Charniak, 2000; Klein and Manning, 2003b), but it is difficult if not impossible to integrate with best-first search in bottom-up chart-parsing (as in Charniak et al. (1998)). The reason is that when working bottom up it is difficult to determine if, say, ssbar is any more or less likely than ss, as the evidence, working bottom up, is negligible. Since our method computes the exact outside probability of constituents (albeit at a coarser level) all of the top down information is available to the system. Or again, another very useful feature in English parsing is the knowledge that a constituent ends at the right boundary (minus pun</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003b. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>330</pages>
<contexts>
<context position="10280" citStr="Marcus et al. (1993)" startWordPosition="1698" endWordPosition="1701">ew phrasal constituents and adding them only Level: 0 1 2 3 S1 { S1 { S1 { S1 S VP UCP SQ SBAR SBARQ SINV NP NAC NX LST X UCP FRAG ADJP QP CONJP ADVP INTJ PRN PRT PP PRT RRC WHADJP WHADVP WHNP WHPP Figure 1: The levels of nonterminal labels after most constituents have been pruned away. 3 Multilevel Course-to-fine Parsing We use as the underlying parsing algorithm a reasonably standard CKY parser, modified to allow unary branching rules. The complete nonterminal clustering is given in Figure 1. We do not cluster preterminals. These remain fixed at all levels to the standard Penn-tree-bank set Marcus et al. (1993). Level-0 makes two distinctions, the root node and everybody else. At level 1 we make one further distinction, between phrases that tend to be heads of constituents (NPs, VPs, and Ss) and those that tend to be modifiers (ADJPs, PPs, etc.). Level-2 has a total of five categories: root, things that are typically headed by nouns, those headed by verbs, things headed by prepositions, and things headed by classical modifiers (adjectives, adverbs, etc.). Finally, level 3 is the ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ HP MP ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪ ⎪⎪⎪⎪⎪⎪⎪</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Michell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313– 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John T Maxwell</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The interface between phrasal and functional constraints.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>590</pages>
<contexts>
<context position="3447" citStr="Maxwell and Kaplan (1993)" startWordPosition="549" endWordPosition="552">old constituents”) with high recall. Our level-1 grammar distinguishes only argument from modifier phrases (i.e., it has two nontrivial nonterminals), while our level-2 grammar distinguishes the four major phrasal categories (verbal, nominal, adjectival and prepositional phrases), and level 3 distinguishes all of the standard categories of the Penn treebank. 168 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 168–175, New York, June 2006. c�2006 Association for Computational Linguistics first appearance of this idea we are aware of is in Maxwell and Kaplan (1993), where a covering CFG is automatically extracted from a more detailed unification grammar and used to identify the possible locations of constituents in the more detailed parses of the sentence. Maxwell and Kaplan use their covering CFG to prune the search of their unification grammar parser in essentially the same manner as we do here, and demonstrate significant performance improvements by using their coarse-to-fine approach. The basic theory of coarse-to-fine approximations and dynamic programming in a stochastic framework is laid out in Geman and Kochanek (2001). This paper describes the </context>
</contexts>
<marker>Maxwell, Kaplan, 1993</marker>
<rawString>John T. Maxwell and Ronald M. Kaplan. 1993. The interface between phrasal and functional constraints. Computational Linguistics, 19(4):571– 590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Toby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9017" citStr="McDonald et al. (2005)" startWordPosition="1472" endWordPosition="1475"> extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar cons</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Toby Crammer, and Fernando Pereira. 2005. Online large margin training of dependency parsers. In Proceedings of the 43rd Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Iterative cky parsing for probabilistic context-free grammars.</title>
<date>2004</date>
<booktitle>In International Joint Conference on Natural-Language Processing.</booktitle>
<contexts>
<context position="8500" citStr="Tsuruoka and Tsujii (2004)" startWordPosition="1386" endWordPosition="1389">(1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A* for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A* search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at d</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2004</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2004. Iterative cky parsing for probabilistic context-free grammars. In International Joint Conference on Natural-Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>