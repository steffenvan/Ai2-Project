<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995917">
Combining Statistical and Knowledge-based Spoken Language
Understanding in Conditional Models
</title>
<author confidence="0.994764">
Ye-Yi Wang, Alex Acero, Milind Mahajan
</author>
<affiliation confidence="0.967759">
Microsoft Research
</affiliation>
<address confidence="0.935422">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.999194">
{yeyiwang,alexac,milindm}@microsoft.com
</email>
<author confidence="0.936293">
John Lee
</author>
<affiliation confidence="0.900116">
Spoken Language Systems
</affiliation>
<address confidence="0.780958">
MIT CSAIL
Cambridge, MA 02139, USA
</address>
<email confidence="0.999348">
jsylee@csail.mit.edu
</email>
<sectionHeader confidence="0.997381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998398379310345">
Spoken Language Understanding (SLU)
addresses the problem of extracting semantic
meaning conveyed in an utterance. The
traditional knowledge-based approach to this
problem is very expensive -- it requires joint
expertise in natural language processing and
speech recognition, and best practices in
language engineering for every new domain.
On the other hand, a statistical learning
approach needs a large amount of annotated
data for model training, which is seldom
available in practical applications outside of
large research labs. A generative HMM/CFG
composite model, which integrates easy-to-
obtain domain knowledge into a data-driven
statistical learning framework, has previously
been introduced to reduce data requirement.
The major contribution of this paper is the
investigation of integrating prior knowledge
and statistical learning in a conditional model
framework. We also study and compare
conditional random fields (CRFs) with
perceptron learning for SLU. Experimental
results show that the conditional models
achieve more than 20% relative reduction in
slot error rate over the HMM/CFG model,
which had already achieved an SLU accuracy
at the same level as the best results reported
on the ATIS data.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999820928571428">
Spoken Language Understanding (SLU)
addresses the problem of extracting meaning
conveyed in an utterance. Traditionally, the
problem is solved with a knowledge-based
approach, which requires joint expertise in
natural language processing and speech
recognition, and best practices in language
engineering for every new domain. In the past
decade many statistical learning approaches have
been proposed, most of which exploit generative
models, as surveyed in (Wang, Deng et al.,
2005). While the data-driven approach addresses
the difficulties in knowledge engineering, it
requires a large amount of labeled data for model
training, which is seldom available in practical
applications outside of large research labs. To
alleviate the problem, a generative HMM/CFG
composite model has previously been introduced
(Wang, Deng et al., 2005). It integrates a
knowledge-based approach into a statistical
learning framework, utilizing prior knowledge to
compensate for the dearth of training data. In the
ATIS evaluation (Price, 1990), this model
achieves the same level of understanding
accuracy (5.3% error rate on standard ATIS
evaluation) as the best system (5.5% error rate),
which is a semantic parsing system based on a
manually developed grammar.
Discriminative training has been widely used
for acoustic modeling in speech recognition
(Bahl, Brown et al., 1986; Juang, Chou et al.,
1997; Povey and Woodland, 2002). Most of the
methods use the same generative model
framework, exploit the same features, and apply
discriminative training for parameter
optimization. Along the same lines, we have
recently exploited conditional models by directly
porting the HMM/CFG model to Hidden
Conditional Random Fields (HCRFs)
(Gunawardana, Mahajan et al., 2005), but failed
to obtain any improvement. This is mainly due to
the vast parameter space, with the parameters
settling at local optima. We then simplified the
original model structure by removing the hidden
variables, and introduced a number of important
overlapping and non-homogeneous features. The
resulting Conditional Random Fields (CRFs)
(Lafferty, McCallum et al., 2001) yielded a 21%
relative improvement in SLU accuracy. We also
applied a much simpler perceptron learning
algorithm on the conditional model and observed
improved SLU accuracy as well.
In this paper, we will first introduce the
generative HMM/CFG composite model, then
discuss the problem of directly porting the model
to HCRFs, and finally introduce the CRFs and
</bodyText>
<page confidence="0.961407">
882
</page>
<note confidence="0.7261165">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 882–889,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.991723666666667">
the features that obtain the best SLU result on
ATIS test data. We compare the CRF and
perceptron training performances on the task.
</bodyText>
<sectionHeader confidence="0.99927" genericHeader="method">
2 Generative Models
</sectionHeader>
<bodyText confidence="0.993840333333333">
The HMM/CFG composite model (Wang, Deng
et al., 2005) adopts a pattern recognition
approach to SLU. Given a word sequence W , an
SLU component needs to find the semantic
representation of the meaning M that has the
maximum a posteriori probability Pr (M  |W) :
</bodyText>
<equation confidence="0.869793333333333">
(M|W)
(W  |M)⋅ Pr(M)
M
</equation>
<bodyText confidence="0.98542868">
The composite model integrates domain
knowledge by setting the topology of the prior
model, Pr(M), according to the domain
semantics; and by using PCFG rules as part of
the lexicalization model Pr( W  |M) .
The domain semantics define an application’s
semantic structure with semantic frames.
Figure 1 shows a simplified example of three
semantic frames in the ATIS domain. The two
frames with the “toplevel” attribute are also
known as commands. The “filler” attribute of a
slot specifies the semantic object that can fill it.
Each slot may be associated with a CFG rule,
and the filler semantic object must be
instantiated by a word string that is covered by
that rule. For example, the string “Seattle” is
covered by the “City” rule in a CFG. It can
therefore fill the ACity (ArrivalCity) or the
DCity (DepartureCity) slot, and instantiate a
Flight frame. This frame can then fill the Flight
slot of a ShowFlight frame. Figure 2 shows a
semantic representation according to these
frames.
&lt; frame name=“ShowFlight” toplevel=“1”&gt;
&lt;slot name=“Flight” filler=“Flight”/&gt;
</bodyText>
<figure confidence="0.670385625">
&lt; /frame &gt;
&lt; frame name=“GroundTrans” toplevel=“1”&gt;
&lt; slot name=“City” filler=“City”/ &gt;
&lt; /frame &gt;
&lt; frame name=“Flight” &gt;
&lt;slot name=“DCity” filler=“City”/&gt;
&lt; slot name=“ACity” filler=“City”/ &gt;
&lt; /frame &gt;
</figure>
<figureCaption confidence="0.997648">
Figure 1. Simplified domain semantics for the ATIS
domain.
</figureCaption>
<bodyText confidence="0.997931411764706">
The semantic prior model comprises the
HMM topology and state transition probabilities.
The topology is determined by the domain
semantics, and the transition probabilities can be
estimated from training data. Figure 3 shows the
topology of the underlying states in the statistical
model for the semantic frames in Figure 1. On
top is the transition network for the two top-level
commands. At the bottom is a zoomed-in view
for the “Flight” sub-network. State 1 and state 4
are called precommands. State 3 and state 6 are
called postcommands. States 2, 5, 8 and 9
represent slots. A slot is actually a three-state
sequence — the slot state is preceded by a
preamble state and followed by a postamble
state, both represented by black circles. They
provide contextual clues for the slot’s identity.
</bodyText>
<listItem confidence="0.9855675">
&lt; ShowFlight &gt;
&lt; Flight &gt;
&lt; DCity filler=“City” &gt; Seattle&lt; /DCity &gt;
&lt; ACity filler=“City” &gt; Boston&lt; /ACity &gt;
&lt; /Flight &gt;
&lt; /ShowFlight &gt;
</listItem>
<figureCaption confidence="0.9992844">
Figure 2. The semantic representation for “Show me
the flights departing from Seattle arriving at Boston”
is an instantiation of the semantic frames in Figure 1.
Figure 3. The HMM/CFG model’s state topology, as
determined by the semantic frames in Figure 1.
</figureCaption>
<bodyText confidence="0.997950333333333">
The lexicalization model, Pr( W  |M) , depicts
the process of sentence generation from the
topology by estimating the distribution of words
emitted by a state. It uses state-dependent n-
grams to model the precommands,
postcommands, preambles and postambles, and
uses knowledge-based CFG rules to model the
slot fillers. These rules help compensate for the
dearth of domain-specific data. In the remainder
of this paper we will say a string is “covered by a
CFG non-terminal (NT)”, or equivalently, is
“CFG-covered for s” if the string can be parsed
by the CFG rule corresponding to the slot s.
Given the semantic representation in Figure 2,
the state sequence through the model topology in
</bodyText>
<equation confidence="0.974011">
ˆ
M= arg max Pr
M
= arg max Pr
</equation>
<page confidence="0.98702">
883
</page>
<bodyText confidence="0.997542666666667">
Figure 3 is deterministic, as shown in Figure 4.
However, the words are not aligned to the states
in the shaded boxes. The parameters in their
corresponding n-gram models can be estimated
with an EM algorithm that treats the alignments
as hidden variables.
</bodyText>
<figureCaption confidence="0.9775565">
Figure 4. Word/state alignments. The segmentation
of the word sequences in the shaded region is hidden.
</figureCaption>
<bodyText confidence="0.999781529411765">
The HMM/CFG composite model was
evaluated in the ATIS domain (Price, 1990). The
model was trained with ATIS3 category A
training data (~1700 annotated sentences) and
tested with the 1993 ATIS3 category A test
sentences (470 sentences with 1702 reference
slots). The slot insertion-deletion-substitution
error rate (SER) of the test set is 5.0%, leading to
a 5.3% semantic error rate in the standard end-to-
end ATIS evaluation, which is slightly better
than the best manually developed system (5.5%).
Moreover, a steep drop in the error rate is
observed after training with only the first two
hundred sentences. This demonstrates that the
inclusion of prior knowledge in the statistical
model helps alleviate the data sparseness
problem.
</bodyText>
<sectionHeader confidence="0.997357" genericHeader="method">
3 Conditional Models
</sectionHeader>
<bodyText confidence="0.980231363636364">
We investigated the application of conditional
models to SLU. The problem is formulated as
assigning a label l to each element in an
observation o. Here, o consists of a word
sequence oτ1 and a list of CFG non-terminals
(NT) that cover its subsequences, as illustrated in
Figure 5. The task is to label “two” as the “Num-
of-tickets” slot of the “ShowFlight” command,
and “Washington D.C.” as the ArrivalCity slot
for the same command. To do so, the model must
be able to resolve several kinds of ambiguities:
</bodyText>
<listItem confidence="0.992554636363636">
1. Filler/non-filler ambiguity, e.g., “two” can
either fill a Num-of-tickets slot, or its
homonym “to” can form part of the preamble
of an ArrivalCity slot.
2. CFG ambiguity, e.g., “Washington” can be
CFG-covered as either City or State.
3. Segmentation ambiguity, e.g., [Washington]
[D.C.] vs. [Washington D.C.].
4. Semantic label ambiguity, e.g., “Washington
D.C.” can fill either an ArrivalCity or
DepartureCity slot.
</listItem>
<figureCaption confidence="0.9928635">
Figure 5. The observation includes a word sequence
and the subsequences covered by CFG non-terminals.
</figureCaption>
<subsectionHeader confidence="0.963809">
3.1 CRFs and HCRFs
</subsectionHeader>
<footnote confidence="0.6139876">
Conditional Random Fields (CRFs) (Lafferty,
McCallum et al., 2001) are undirected
conditional graphical models that assign the
conditional probability of a state (label) sequence
1sτ with respect to a vector of features f (sτ 1 o1 τ )
</footnote>
<equation confidence="0.962267157894737">
, .
They are of the following form:
1
1
( 1 )
p (sτ  |; =
τ
o ) exp ( )
λ⋅ ,
λ s
f o
o; λ
z( )
Here
( ) exp ( 1 )
o; λ = ∑ λ ⋅ f , o normalizes
( )
s τ
zsτ1
</equation>
<bodyText confidence="0.99144515">
The parameter vector
λ is trained conditionally
(discriminatively). If we assume that 1sτ is a
Markov chain given o and the feature functions
only depend on two adjacent states, then
exp ⎞
k ⎠⎟
fk (s(
)
s(
)
τ⎛
∑λ
∑
t−1
,
t
, o
, t)
⎝ k t=1
</bodyText>
<equation confidence="0.9916002">
( 1) ( )
t t
(s s
− , , ,
o t)
1 if
=FlightInit = flights;
( )
t ∧ ot
s
In this case, the state sequence
is only
partially observed in the meaning representation
M : M(s5)
M(s8)
</equation>
<bodyText confidence="0.976938">
&amp;quot;ACity&amp;quot; for the
words
and
The states for the
remaining words are hidden. Let
</bodyText>
<equation confidence="0.5986968">
1sτ
=&amp;quot;DCity&amp;quot;∧
=
“Seattle”
“Boston”.
</equation>
<bodyText confidence="0.999324">
Γ(M ) represent
the set of all state sequences that satisfy the
constraints imposed by M. To obtain the
conditional probability of M, we need to sum
over all possible labels for the hidden states:
</bodyText>
<equation confidence="0.855632933333334">
fFlightInit,flights
⎧
⎨
⎩
=
(2)
the distribution over all possible state sequences.
p s τ
(  |;
o λ )
1
1
=
z ( )
o ; λ
</equation>
<bodyText confidence="0.986815888888889">
In some cases, it may be natural to exploit
features on variables that are not directly
observed. For example, a feature for the Flight
preamble
defined in terms of an
may be
observed
word and an unobserved state in the shaded
region in Figure 4:
</bodyText>
<equation confidence="0.986464">
0 otherwise
(3)
. (1)
884
p M
(  |o; λ)
1
τ⎞
( 1) ( )
−
∑ ∑ ∑
⎛
exp λ f s s
t
⎜ t
( , , , ⎟
o t)
k k
z ( )
o; ⎝
s τ
λ ( )
M k t 1
1 ∈Γ = ⎠
</equation>
<bodyText confidence="0.8984765">
CRFs with features dependent on hidden state
variables are called Hidden Conditional Random
Fields (HCRFs). They have been applied to tasks
such as phonetic classification (Gunawardana,
Mahajan et al., 2005) and object recognition
(Quattoni, Collins et al., 2004).
</bodyText>
<subsectionHeader confidence="0.986934">
3.2 Conditional Model Training
</subsectionHeader>
<bodyText confidence="0.99686125">
We train CRFs and HCRFs with gradient-based
optimization algorithms that maximize the log
posterior. The gradient of the objective function
is
</bodyText>
<equation confidence="0.960128333333333">
τ
∇λL (λ) = EP(l o)P(s, |l o)⎣ f (s1 , o);
− ⎡ , ; ⎤
τ
E s
( ) λ
� ( ) ( ) 1
P P s τ 1  |⎣ f o ⎦
o o
</equation>
<bodyText confidence="0.999934">
which is the difference between the conditional
expectation of the feature vector given the
observation sequence and label sequence, and the
conditional expectation given the observation
sequence alone. With the Markov assumption in
Eq. (2), these expectations can be computed
using a forward-backward-like dynamic
programming algorithm. For CRFs, whose
features do not depend on hidden state
sequences, the first expectation is simply the
feature counts given the observation and label
sequences. In this work, we applied stochastic
gradient descent (SGD) (Kushner and Yin, 1997)
for parameter optimization. In our experiments
on several different tasks, it is faster than L-
BFGS (Nocedal and Wright, 1999), a quasi-
Newton optimization algorithm.
</bodyText>
<subsectionHeader confidence="0.998244">
3.3 CRFs and Perceptron Learning
</subsectionHeader>
<bodyText confidence="0.999800846153846">
Perceptron training for conditional models
(Collins, 2002) is an approximation to the SGD
algorithm, using feature counts from the Viterbi
label sequence in lieu of expected feature counts.
It eliminates the need of a forward-backward
algorithm to collect the expected counts, hence
greatly speeds up model training. This algorithm
can be viewed as using the minimum margin of a
training example (i.e., the difference in the log
conditional probability of the reference label
sequence and the Viterbi label sequence) as the
objective function instead of the conditional
probability:
</bodyText>
<equation confidence="0.906486">
L&apos; (λ) = log P (l  |o; λ) − max log P (l&apos;  |o; λ )
l&apos;
</equation>
<bodyText confidence="0.999719416666667">
Here again, o is the observation and l is its
reference label sequence. In perceptron training,
the parameter updating stops when the Viterbi
label sequence is the same as the reference label
sequence. In contrast, the optimization based on
the log posterior probability objective function
keeps pulling probability mass from all incorrect
label sequences to the reference label sequence
until convergence.
In both perceptron and CRF training, we
average the parameters over training iterations
(Collins, 2002).
</bodyText>
<sectionHeader confidence="0.998164" genericHeader="method">
4 Porting HMM/CFG Model to HCRFs
</sectionHeader>
<bodyText confidence="0.999991142857143">
In our first experiment, we would like to exploit
the discriminative training capability of a
conditional model without changing the
HMM/CFG model’s topology and feature set.
Since the state sequence is only partially labeled,
an HCRF is used to model the conditional
distribution of the labels.
</bodyText>
<subsectionHeader confidence="0.759894">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.9922182">
We used the same state topology and features as
those in the HMM/CFG composite model. The
following indicator features are included:
Command prior features capture the a priori
likelihood of different top-level commands:
</bodyText>
<equation confidence="0.95207">
(s(t−1),s(t) , ,
o t)
1 if =0 C( )
( )
t s t = c
= ⎧ ⎨ ∧
⎩
</equation>
<bodyText confidence="0.9993414">
Here C(s) stands for the name of the command
that corresponds to the transition network
containing state s.
State Transition features capture the likelihood
of transition from one state to another:
</bodyText>
<equation confidence="0.969384846153846">
⎧ 1 if s s s s
t − ( )
( 1)
t
= , =
f TR ( 1) ( )
− , , , = ⎨
t 1 2
t o t ) ,
(s s
, ⎩ 0 otherwise
s s
1 2
</equation>
<bodyText confidence="0.99892175">
where s1 → s2 is a legal transition according to the
state topology.
Unigram and Bigram features capture the
likelihoods of words emitted by a state:
</bodyText>
<figure confidence="0.876349666666667">
=
fcPR
0 otherwise
, CommandSet
∀ ∈
c
</figure>
<page confidence="0.939264">
885
</page>
<equation confidence="0.999515352941176">
( 1) ( )
− , , ,
t
( s s
t τ
o t )
1
( 1)
t − ( ) 1
⎧ 1 if = ∧ = ∧ = ∧ =
t −
o t o t
s s s s w w
1 2
⎨
0 otherwise ,
∀s  |¬isFiller(s);∀w,w1w2 ∈ TrainingData
</equation>
<bodyText confidence="0.9996985">
The condition isFiller(s1) restricts s1 to be a slot
state and not a pre- or postamble state.
</bodyText>
<subsectionHeader confidence="0.776277">
4.2 Experiments
</subsectionHeader>
<bodyText confidence="0.989021615384615">
The model is trained with SGD with the
parameters initialized in two ways. The flat start
initialization sets all parameters to 0. The
generative model initialization uses the
parameters trained by the HMM/CFG model.
Figure 6 shows the test set slot error rates
(SER) at different training iterations. With the
flat start initialization (top curve), the error rate
never comes close to the 5% baseline error rate
of the HMM/CFG model. With the generative
model initialization, the error rate is reduced to
4.8% at the second iteration, but the model
quickly gets over-trained afterwards.
</bodyText>
<figure confidence="0.964381666666667">
35
30
25
20
15
10
5
0
0 20 40 60 80 100 120
</figure>
<figureCaption confidence="0.9839195">
Figure 6. Test set slot error rates (in %) at different
training iterations. The top curve is for the flat start
initialization, the bottom for the generative model
initialization.
</figureCaption>
<bodyText confidence="0.999422">
The failure of the direct porting of the
generative model to the conditional model can be
attributed to the following reasons:
</bodyText>
<listItem confidence="0.94534035">
• The conditional log-likelihood function is
no longer a convex function due to the
summation over hidden variables. This
makes the model highly likely to settle on
a local optimum. The fact that the flat start
initialization failed to achieve the accuracy
of the generative model initialization is a
clear indication of the problem.
• In order to account for words in the test
data, the n-grams in the generative model
are properly smoothed with back-offs to
the uniform distribution over the
vocabulary. This results in a huge number
of parameters, many of which cannot be
estimated reliably in the conditional
model, given that model regularization is
not as well studied as in n-grams.
• The hidden variables make parameter
estimation less reliable, given only a small
amount of training data.
</listItem>
<sectionHeader confidence="0.993377" genericHeader="method">
5 CRFs for SLU
</sectionHeader>
<bodyText confidence="0.999894117647059">
An important lesson we have learned from the
previous experiment is that we should not think
generatively when applying conditional models.
While it is important to find cues that help
identify the slots, there is no need to exhaustively
model the generation of every word in a
sentence. Hence, the distinctions between pre-
and postcommands, and pre- and postambles are
no longer necessary. Every word that appears
between two slots is labeled as the preamble state
of the second slot, as illustrated in Figure 7. This
labeling scheme effectively removes the hidden
variables and simplifies the model to a CRF. It
not only expedites model training, but also
prevents parameters from settling at a local
optimum, because the log conditional probability
is now a convex function.
</bodyText>
<figureCaption confidence="0.637949">
Figure 7. Once the slots are marked in the
simplified model topology, the state sequence is fully
marked, leaving no hidden variables and resulting in a
CRF. Here, PAC stands for “preamble for arrival
city,” and PDC for “preamble for departure city.”
</figureCaption>
<bodyText confidence="0.999988333333333">
The command prior and state transition
features (with fewer states) are the same as in the
HCRF model. For unigrams and bigrams, only
those that occur in front of a CFG-covered string
are considered. If the string is CFG-covered for
slot s, then the unigram and bigram features for
the preamble state of s are included. Suppose the
words “that departs” occur at positions
t −1 and t in front of the word “Seattle,” which
is CFG-covered by the non-terminal City. Since
City can fill a DepartureCity or ArrivalCity slot,
the four following features are introduced:
</bodyText>
<equation confidence="0.771513666666667">
( 1) ( )
− , , , = ⎨
t
( s s
t τ
o t )
</equation>
<figure confidence="0.979794722222222">
1
⎩
f UG s w
,
( )
t
⎧ 1 if ot
s s
= ∧ =w
,
0 otherwise
1 2
, w
f BG
s w
,
=
⎩
</figure>
<page confidence="0.995022">
886
</page>
<bodyText confidence="0.988625555555556">
fUG , (s(t −1) s(t) oτ1, t) = fUG (s (t−1) s(t) oτ t) = 1
PDC,that , PAC,that , , 1,
And
previous slot is ArrivalCity, so the state
transition features are not helpful for
disambiguation. The identity of the time slot
depends not on the ArrivalCity slot, but on its
preamble. Our second feature set, previous-slot
context, introduces this dependency to the model:
</bodyText>
<equation confidence="0.947892422222222">
f BG
PDC,that,departs
f BG
PAC,that,departs
( 1) ( )
− , , ,
t
( s s
t τ
o t )
1
( 1) ( )
− , , , =
t
(s s
t τ
o t) 1
1
=
Formally,
( )
t t
⎧ s s
1 if = ∧ =
o w
s s
( 1) ( )
t −
( , , , = ⎨
t o τ t)
1
⎩
( 1) ( )
− , , ,
t
( s s
t τ
o t )
1
=
∀s  |¬isFiller(s);
∀w,w1w2  |in the training data, wand w1w2
appears in front of sequence that is CFG-covered
for .
s
</equation>
<subsectionHeader confidence="0.993112">
5.1 Additional Features
</subsectionHeader>
<bodyText confidence="0.999934333333333">
One advantage of CRFs over generative models
is the ease with which overlapping features can
be incorporated. In this section, we describe
three additional feature sets.
The first set addresses a side effect of not
modeling the generation of every word in a
sentence. Suppose a preamble state has never
occurred in a position that is confusable with a
slot state s, and a word that is CFG-covered for s
has never occurred as part of the preamble state
in the training data. Then, the unigram feature of
the word for that preamble state has weight 0,
and there is thus no penalty for mislabeling the
word as the preamble. This is one of the most
common errors observed in the development set.
The chunk coverage for preamble words feature
introduced to model the likelihood of a CFG-
covered word being labeled as a preamble:
</bodyText>
<equation confidence="0.982735272727273">
( 1) ( )
−
(s s t
t , , ,
t o )
1 if C( ) covers( , ) isPre( )
( )
t ( )
= ∧ ot t
s c NT ∧ s
0 otherwise
</equation>
<bodyText confidence="0.999955782608695">
where isPre(s) indicates that s is a preamble
state.
Often, the identity of a slot depends on the
preambles of the previous slot. For example, “at
two PM” is a DepartureTime in “flight from
Seattle to Boston at two PM”, but it is an
ArrivalTime in “flight departing from Seattle
arriving in Boston at two PM.” In both cases, the
Here Slot(s) stands for the slot associated with
the state s, which can be a filler state or a
preamble state, as shown in Figure 7.
Θ(s1, o, t −1) is the set of k words (where k is an
adjustable window size) in front of the longest
sequence that ends at position t −1 and that is
CFG-covered by Slot(s1) .
The third feature set is intended to penalize
erroneous segmentation, such as segmenting
“Washington D.C.” into two separate City slots.
The chunk coverage for slot boundary feature is
activated when a slot boundary is covered by a
CFG non-terminal NT, i.e., when words in two
consecutive slots (“Washington” and “D.C.”) can
also be covered by one single slot:
</bodyText>
<equation confidence="0.995756722222222">
( 1) ( )
−
( s s t
t , , ,
t o )
if C( ) covers( , )
s ( )
t ot
NT
= ∧
c t −1
isFiller( ) isFiller( )
( 1)
t − ( )
∧ t
s ∧ s
∧ s(t −1) ≠ s(t)
0 otherwise
</equation>
<bodyText confidence="0.991410666666667">
This feature set shares its weights with the
chunk coverage features for preamble words,
and does not introduce any new parameters.
</bodyText>
<table confidence="0.9952785">
Features # of Param. SER
Command Prior 6
+State Transition +1377 18.68%
+Unigrams +14433 7.29%
+Bigrams +58191 7.23%
+Chunk Cov Preamble Word +156 6.87%
+Previous-Slot Context +290 5.46%
+Chunk Cov Slot Boundaries +0 3.94%
</table>
<tableCaption confidence="0.9610155">
Table 1. Number of additional parameters and the
slot error rate after each new feature set is introduced.
</tableCaption>
<subsectionHeader confidence="0.983656">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999733333333333">
Since the objective function is convex, the
optimization algorithm does not make any
significant difference on SLU accuracy. We
</bodyText>
<figure confidence="0.984934166666667">
fUG
s ,w
,
0 otherwise
fBG
s w w
, ,
1 2
⎩ 0 otherwise ,
CC
f cNT
,
⎧⎪⎨
⎪⎩
=
fPC
s1,s2,w
⎧
= ⎪⎨ ∧ isFiller( ) Slot( ) Slot( )
s ∧ s ≠ s
1 1 2
⎪⎩
0 otherwise
( 1) ( )
t
( s s t
t − , , ,
o )
( 1)
t − = ∧ = ∧ ∈Θ
( )
t
1 if s s s s w s t
( , , 1)
o −
1 2 1
fSB
c,NT
⎧
⎪
⎪⎨
⎪
⎪
⎪⎩
=
1
( 1) ( )
t − 1
⎧ 1 if = = ∧ = ∧ =
t −
o t ot
s s s w w
1 2
⎨
</figure>
<page confidence="0.967952">
887
</page>
<bodyText confidence="0.999924451612903">
trained the model with SGD. Other optimization
algorithm like Stochastic Meta-Decent
(Vishwanathan, Schraudolph et al., 2006) can be
used to speed up the convergence. The training
stopping criterion is cross-validated with the
development set.
Table 1 shows the number of new parameters
and the slot error rate (SER) on the test data,
after each new feature set is introduced. The new
features improve the prediction of slot identities
and reduce the SER by 21%, relative to the
generative HMM/CFG composite model.
The figures below show in detail the impact of
the n-gram, previous-slot context and chunk
coverage features. The chunk coverage feature
has three settings: 0 stands for no chunk
coverage features; 1 for chunk coverage features
for preamble words only; and 2 for both words
and slot boundaries.
Figure 8 shows the impact of the order of n-
gram features. Zero-order means no lexical
features for preamble states are included. As the
figure illustrates, the inclusion of CFG rules for
slot filler states and domain-specific knowledge
about command priors and slot transitions have
already produced a reasonable SER under 15%.
Unigram features for preamble states cut the
error by more than 50%, while the impact of
bigram features is not consistent -- it yields a
small positive or negative difference depending
on other experimental parameter settings.
</bodyText>
<figure confidence="0.977841">
0 1 2
Ngram Order
</figure>
<figureCaption confidence="0.998619">
Figure 8. Effects of the order of n-grams on SER.
The window size for the previous-slot context features
is 2.
</figureCaption>
<bodyText confidence="0.9894815">
Figure 9 shows the impact of the CFG chunk
coverage feature. Coverage for both preamble
words and slot boundaries help improve the SLU
accuracy.
Figure 10 shows the impact of the window
size for the previous-slot context feature. Here, 0
means that the previous-slot context feature is
not used. When the window size is k, the k words
in front of the longest previous CFG-covered
word sequence are included as the previous-slot
unigram context features. As the figure
illustrates, this feature significantly reduces SER,
while the window size does not make any
significant difference.
</bodyText>
<figure confidence="0.9939745">
0 1 2
Chunk Coverage
</figure>
<figureCaption confidence="0.848727">
Figure 9. Effects of the chunk coverage feature. The
window size for the previous-slot context feature is 2.
The three lines correspond to different n-gram orders,
where 0-gram indicates that no preamble lexical
features are used.
</figureCaption>
<bodyText confidence="0.975693333333333">
It is important to note that overlapping
features like f CC, fSB and f PC could not be easily
incorporated into a generative model.
</bodyText>
<figure confidence="0.9806495">
0 1 2
Window Size
</figure>
<figureCaption confidence="0.8893964">
Figure 10. Effects of the window size of the
previous-slot context feature. The three lines represent
different orders of n-grams (0, 1, and 2). Chunk
coverage features for both preamble words and slot
boundaries are used.
</figureCaption>
<subsectionHeader confidence="0.975088">
5.3 CRFs vs. Perceptrons
</subsectionHeader>
<bodyText confidence="0.9965585">
Table 2 compares the perceptron and CRF
training algorithms, using chunk coverage
features for both preamble words and slot
boundaries, with which the best accuracy results
</bodyText>
<figure confidence="0.998641432432433">
16%
14%
Slot Error Rate
12%
10%
8%
6%
4%
2%
0%
ChunkCoverage=0
ChunkCoverage=1
ChunkCoverage=2
Slot Error Rate
16%
14%
12%
10%
4%
8%
6%
2%
0%
n=0
n=1
n=2
Slot Error Rate
12%
10%
4%
8%
6%
2%
0%
n=0
n=1
n=2
</figure>
<page confidence="0.99094">
888
</page>
<bodyText confidence="0.999863866666667">
are achieved. Both improve upon the 5%
baseline SER from the generative HMM/CFG
model. CRF training outperforms the perceptron
in most settings, except for the one with unigram
features for preamble states and with window
size 1 -- the model with the fewest parameters.
One possible explanation is as follows. The
objective function in CRFs is a convex function,
and so SGD can find the single global optimum
for it. In contrast, the objective function for the
perceptron, which is the difference between two
convex functions, is not convex. The gradient
ascent approach in perceptron training is hence
more likely to settle on a local optimum as the
model becomes more complicated.
</bodyText>
<table confidence="0.992553">
PSWSize=1 PSWSize=2
Perceptron CRFs Perceptron CRFs
n=1 3.76% 4.11% 4.23% 3.94%
n=2 4.76% 4.41% 4.58% 3.94%
</table>
<tableCaption confidence="0.782716">
Table 2. Perceptron vs. CRF training. Chunk
coverage features are used for both preamble words
</tableCaption>
<bodyText confidence="0.993573357142857">
and slot boundaries. PSWSize stands for the window
size of the previous-slot context feature. N is the order
of the n-gram features.
The biggest advantage of perceptron learning
is its speed. It directly counts the occurrence of
features given an observation and its reference
label sequence and Viterbi label sequence, with
no need to collect expected feature counts with a
forward-backward-like algorithm. Not only is
each iteration faster, but fewer iterations are
required, when using SLU accuracy on a cross-
validation set as the stopping criterion. Overall,
perceptron training is 5 to 8 times faster than
CRF training.
</bodyText>
<sectionHeader confidence="0.999754" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999990125">
This paper has introduced a conditional model
framework that integrates statistical learning
with a knowledge-based approach to SLU. We
have shown that a conditional model reduces
SLU slot error rate by more than 20% over the
generative HMM/CFG composite model. The
improvement was mostly due to the introduction
of new overlapping features into the model. We
have also discussed our experience in directly
porting a generative model to a conditional
model, and demonstrated that it may not be
beneficial at all if we still think generatively in
conditional modeling; more specifically,
replicating the feature set of a generative model
in a conditional model may not help much. The
key benefit of conditional models is the ease with
which they can incorporate overlapping and non-
homogeneous features. This is consistent with
the finding in the application of conditional
models for POS tagging (Lafferty, McCallum et
al., 2001). The paper also compares different
training algorithms for conditional models. In
most cases, CRF training is more accurate,
however, perceptron training is much faster.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918953488372">
Bahl, L., P. Brown, et al. 1986. Maximum mutual
information estimation of hidden Markov model
parameters for speech recognition. IEEE
International Conference on Acoustics, Speech,
and Signal Processing.
Collins, M. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and
Experiments with Perceptron Algorithms. EMNLP,
Philadelphia, PA.
Gunawardana, A., M. Mahajan, et al. 2005. Hidden
conditional random fields for phone classification.
Eurospeech, Lisbon, Portugal.
Juang, B.-H., W. Chou, et al. 1997. &amp;quot;Minimum
classification error rate methods for speech
recognition.&amp;quot; IEEE Transactions on Speech and
Audio Processing 5(3): 257-265.
Kushner, H. J. and G. G. Yin. 1997. Stochastic
approximation algorithms and applications,
Springer-Verlag.
Lafferty, J., A. McCallum, et al. 2001. Conditional
random fields: probabilistic models for segmenting
and labeling sequence data. ICML.
Nocedal, J. and S. J. Wright. 1999. Numerical
optimization, Springer-Verlag.
Povey, D. and P. C. Woodland. 2002. Minimum
phone error and I-smoothing for improved
discriminative training. IEEE International
Conference on Acoustics, Speech, and Signal
Processing.
Price, P. 1990. Evaluation of spoken language system:
the ATIS domain. DARPA Speech and Natural
Language Workshop, Hidden Valley, PA.
Quattoni, A., M. Collins and T. Darrell. 2004.
Conditional Random Fields for Object
Recognition. NIPS.
Vishwanathan, S. V. N., N. N. Schraudolph, et al.
2006. Accelerated Training of conditional random
fields with stochastic meta-descent. The Learning
Workshop, Snowbird, Utah.
Wang, Y.-Y., L. Deng, et al. 2005. &amp;quot;Spoken language
understanding --- an introduction to the statistical
framework.&amp;quot; IEEE Signal Processing Magazine
22(5): 16-31.
</reference>
<page confidence="0.998966">
889
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.612761">
<title confidence="0.998892">Combining Statistical and Knowledge-based Spoken Language Understanding in Conditional Models</title>
<author confidence="0.999349">Ye-Yi Wang</author>
<author confidence="0.999349">Alex Acero</author>
<author confidence="0.999349">Milind Mahajan</author>
<affiliation confidence="0.999796">Microsoft Research</affiliation>
<address confidence="0.994453">One Microsoft Way Redmond, WA 98052, USA</address>
<email confidence="0.999475">yeyiwang@microsoft.com</email>
<email confidence="0.999475">alexac@microsoft.com</email>
<email confidence="0.999475">milindm@microsoft.com</email>
<author confidence="0.99828">John Lee</author>
<affiliation confidence="0.8105215">Spoken Language Systems MIT CSAIL</affiliation>
<address confidence="0.999927">Cambridge, MA 02139, USA</address>
<email confidence="0.99986">jsylee@csail.mit.edu</email>
<abstract confidence="0.999615866666667">Spoken Language Understanding (SLU) addresses the problem of extracting semantic meaning conveyed in an utterance. The traditional knowledge-based approach to this problem is very expensive -it requires joint expertise in natural language processing and speech recognition, and best practices in language engineering for every new domain. On the other hand, a statistical learning approach needs a large amount of annotated data for model training, which is seldom available in practical applications outside of large research labs. A generative HMM/CFG composite model, which integrates easy-toobtain domain knowledge into a data-driven statistical learning framework, has previously been introduced to reduce data requirement. The major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework. We also study and compare conditional random fields (CRFs) with perceptron learning for SLU. Experimental results show that the conditional models achieve more than 20% relative reduction in slot error rate over the HMM/CFG model, which had already achieved an SLU accuracy at the same level as the best results reported on the ATIS data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>P Brown</author>
</authors>
<title>Maximum mutual information estimation of hidden Markov model parameters for speech recognition.</title>
<date>1986</date>
<booktitle>IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<marker>Bahl, Brown, 1986</marker>
<rawString>Bahl, L., P. Brown, et al. 1986. Maximum mutual information estimation of hidden Markov model parameters for speech recognition. IEEE International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<location>EMNLP, Philadelphia, PA.</location>
<contexts>
<context position="13059" citStr="Collins, 2002" startWordPosition="2163" endWordPosition="2164">mption in Eq. (2), these expectations can be computed using a forward-backward-like dynamic programming algorithm. For CRFs, whose features do not depend on hidden state sequences, the first expectation is simply the feature counts given the observation and label sequences. In this work, we applied stochastic gradient descent (SGD) (Kushner and Yin, 1997) for parameter optimization. In our experiments on several different tasks, it is faster than LBFGS (Nocedal and Wright, 1999), a quasiNewton optimization algorithm. 3.3 CRFs and Perceptron Learning Perceptron training for conditional models (Collins, 2002) is an approximation to the SGD algorithm, using feature counts from the Viterbi label sequence in lieu of expected feature counts. It eliminates the need of a forward-backward algorithm to collect the expected counts, hence greatly speeds up model training. This algorithm can be viewed as using the minimum margin of a training example (i.e., the difference in the log conditional probability of the reference label sequence and the Viterbi label sequence) as the objective function instead of the conditional probability: L&apos; (λ) = log P (l |o; λ) − max log P (l&apos; |o; λ ) l&apos; Here again, o is the ob</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, M. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. EMNLP, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gunawardana</author>
<author>M Mahajan</author>
</authors>
<title>Hidden conditional random fields for phone classification.</title>
<date>2005</date>
<location>Eurospeech, Lisbon, Portugal.</location>
<marker>Gunawardana, Mahajan, 2005</marker>
<rawString>Gunawardana, A., M. Mahajan, et al. 2005. Hidden conditional random fields for phone classification. Eurospeech, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B-H Juang</author>
<author>W Chou</author>
</authors>
<title>Minimum classification error rate methods for speech recognition.&amp;quot;</title>
<date>1997</date>
<journal>IEEE Transactions on Speech and Audio Processing</journal>
<volume>5</volume>
<issue>3</issue>
<pages>257--265</pages>
<marker>Juang, Chou, 1997</marker>
<rawString>Juang, B.-H., W. Chou, et al. 1997. &amp;quot;Minimum classification error rate methods for speech recognition.&amp;quot; IEEE Transactions on Speech and Audio Processing 5(3): 257-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Kushner</author>
<author>G G Yin</author>
</authors>
<title>Stochastic approximation algorithms and applications,</title>
<date>1997</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="12802" citStr="Kushner and Yin, 1997" startWordPosition="2124" endWordPosition="2127"> ( ) λ � ( ) ( ) 1 P P s τ 1 |⎣ f o ⎦ o o which is the difference between the conditional expectation of the feature vector given the observation sequence and label sequence, and the conditional expectation given the observation sequence alone. With the Markov assumption in Eq. (2), these expectations can be computed using a forward-backward-like dynamic programming algorithm. For CRFs, whose features do not depend on hidden state sequences, the first expectation is simply the feature counts given the observation and label sequences. In this work, we applied stochastic gradient descent (SGD) (Kushner and Yin, 1997) for parameter optimization. In our experiments on several different tasks, it is faster than LBFGS (Nocedal and Wright, 1999), a quasiNewton optimization algorithm. 3.3 CRFs and Perceptron Learning Perceptron training for conditional models (Collins, 2002) is an approximation to the SGD algorithm, using feature counts from the Viterbi label sequence in lieu of expected feature counts. It eliminates the need of a forward-backward algorithm to collect the expected counts, hence greatly speeds up model training. This algorithm can be viewed as using the minimum margin of a training example (i.e.</context>
</contexts>
<marker>Kushner, Yin, 1997</marker>
<rawString>Kushner, H. J. and G. G. Yin. 1997. Stochastic approximation algorithms and applications, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<publisher>ICML.</publisher>
<marker>Lafferty, McCallum, 2001</marker>
<rawString>Lafferty, J., A. McCallum, et al. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S J Wright</author>
</authors>
<date>1999</date>
<note>Numerical optimization, Springer-Verlag.</note>
<contexts>
<context position="12928" citStr="Nocedal and Wright, 1999" startWordPosition="2144" endWordPosition="2147"> given the observation sequence and label sequence, and the conditional expectation given the observation sequence alone. With the Markov assumption in Eq. (2), these expectations can be computed using a forward-backward-like dynamic programming algorithm. For CRFs, whose features do not depend on hidden state sequences, the first expectation is simply the feature counts given the observation and label sequences. In this work, we applied stochastic gradient descent (SGD) (Kushner and Yin, 1997) for parameter optimization. In our experiments on several different tasks, it is faster than LBFGS (Nocedal and Wright, 1999), a quasiNewton optimization algorithm. 3.3 CRFs and Perceptron Learning Perceptron training for conditional models (Collins, 2002) is an approximation to the SGD algorithm, using feature counts from the Viterbi label sequence in lieu of expected feature counts. It eliminates the need of a forward-backward algorithm to collect the expected counts, hence greatly speeds up model training. This algorithm can be viewed as using the minimum margin of a training example (i.e., the difference in the log conditional probability of the reference label sequence and the Viterbi label sequence) as the obj</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Nocedal, J. and S. J. Wright. 1999. Numerical optimization, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>P C Woodland</author>
</authors>
<title>Minimum phone error and I-smoothing for improved discriminative training.</title>
<date>2002</date>
<booktitle>IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="2979" citStr="Povey and Woodland, 2002" startWordPosition="423" endWordPosition="426">usly been introduced (Wang, Deng et al., 2005). It integrates a knowledge-based approach into a statistical learning framework, utilizing prior knowledge to compensate for the dearth of training data. In the ATIS evaluation (Price, 1990), this model achieves the same level of understanding accuracy (5.3% error rate on standard ATIS evaluation) as the best system (5.5% error rate), which is a semantic parsing system based on a manually developed grammar. Discriminative training has been widely used for acoustic modeling in speech recognition (Bahl, Brown et al., 1986; Juang, Chou et al., 1997; Povey and Woodland, 2002). Most of the methods use the same generative model framework, exploit the same features, and apply discriminative training for parameter optimization. Along the same lines, we have recently exploited conditional models by directly porting the HMM/CFG model to Hidden Conditional Random Fields (HCRFs) (Gunawardana, Mahajan et al., 2005), but failed to obtain any improvement. This is mainly due to the vast parameter space, with the parameters settling at local optima. We then simplified the original model structure by removing the hidden variables, and introduced a number of important overlappin</context>
</contexts>
<marker>Povey, Woodland, 2002</marker>
<rawString>Povey, D. and P. C. Woodland. 2002. Minimum phone error and I-smoothing for improved discriminative training. IEEE International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Price</author>
</authors>
<title>Evaluation of spoken language system: the ATIS domain. DARPA Speech and Natural Language Workshop,</title>
<date>1990</date>
<location>Hidden Valley, PA.</location>
<contexts>
<context position="2591" citStr="Price, 1990" startWordPosition="364" endWordPosition="365">h exploit generative models, as surveyed in (Wang, Deng et al., 2005). While the data-driven approach addresses the difficulties in knowledge engineering, it requires a large amount of labeled data for model training, which is seldom available in practical applications outside of large research labs. To alleviate the problem, a generative HMM/CFG composite model has previously been introduced (Wang, Deng et al., 2005). It integrates a knowledge-based approach into a statistical learning framework, utilizing prior knowledge to compensate for the dearth of training data. In the ATIS evaluation (Price, 1990), this model achieves the same level of understanding accuracy (5.3% error rate on standard ATIS evaluation) as the best system (5.5% error rate), which is a semantic parsing system based on a manually developed grammar. Discriminative training has been widely used for acoustic modeling in speech recognition (Bahl, Brown et al., 1986; Juang, Chou et al., 1997; Povey and Woodland, 2002). Most of the methods use the same generative model framework, exploit the same features, and apply discriminative training for parameter optimization. Along the same lines, we have recently exploited conditional</context>
<context position="8326" citStr="Price, 1990" startWordPosition="1289" endWordPosition="1290"> by the CFG rule corresponding to the slot s. Given the semantic representation in Figure 2, the state sequence through the model topology in ˆ M= arg max Pr M = arg max Pr 883 Figure 3 is deterministic, as shown in Figure 4. However, the words are not aligned to the states in the shaded boxes. The parameters in their corresponding n-gram models can be estimated with an EM algorithm that treats the alignments as hidden variables. Figure 4. Word/state alignments. The segmentation of the word sequences in the shaded region is hidden. The HMM/CFG composite model was evaluated in the ATIS domain (Price, 1990). The model was trained with ATIS3 category A training data (~1700 annotated sentences) and tested with the 1993 ATIS3 category A test sentences (470 sentences with 1702 reference slots). The slot insertion-deletion-substitution error rate (SER) of the test set is 5.0%, leading to a 5.3% semantic error rate in the standard end-toend ATIS evaluation, which is slightly better than the best manually developed system (5.5%). Moreover, a steep drop in the error rate is observed after training with only the first two hundred sentences. This demonstrates that the inclusion of prior knowledge in the s</context>
</contexts>
<marker>Price, 1990</marker>
<rawString>Price, P. 1990. Evaluation of spoken language system: the ATIS domain. DARPA Speech and Natural Language Workshop, Hidden Valley, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Quattoni</author>
<author>M Collins</author>
<author>T Darrell</author>
</authors>
<title>Conditional Random Fields for Object Recognition.</title>
<date>2004</date>
<publisher>NIPS.</publisher>
<marker>Quattoni, Collins, Darrell, 2004</marker>
<rawString>Quattoni, A., M. Collins and T. Darrell. 2004. Conditional Random Fields for Object Recognition. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>N N Schraudolph</author>
</authors>
<title>Accelerated Training of conditional random fields with stochastic meta-descent. The Learning Workshop,</title>
<date>2006</date>
<location>Snowbird, Utah.</location>
<marker>Vishwanathan, Schraudolph, 2006</marker>
<rawString>Vishwanathan, S. V. N., N. N. Schraudolph, et al. 2006. Accelerated Training of conditional random fields with stochastic meta-descent. The Learning Workshop, Snowbird, Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-Y Wang</author>
<author>L Deng</author>
</authors>
<title>Spoken language understanding --- an introduction to the statistical framework.&amp;quot;</title>
<date>2005</date>
<journal>IEEE Signal Processing Magazine</journal>
<volume>22</volume>
<issue>5</issue>
<pages>16--31</pages>
<marker>Wang, Deng, 2005</marker>
<rawString>Wang, Y.-Y., L. Deng, et al. 2005. &amp;quot;Spoken language understanding --- an introduction to the statistical framework.&amp;quot; IEEE Signal Processing Magazine 22(5): 16-31.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>