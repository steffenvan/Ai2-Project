<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000712">
<title confidence="0.960265">
Hierarchical Dirichlet Trees for Information Retrieval
</title>
<author confidence="0.986711">
Gholamreza Haffari Yee Whye Teh
</author>
<affiliation confidence="0.997761">
School of Computing Sciences Gatsby Computational Neuroscience
Simon Fraser University University College London
</affiliation>
<email confidence="0.991158">
ghaffar1@cs.sfu.ca ywteh@gatsby.ucl.ac.uk
</email>
<sectionHeader confidence="0.99852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999178384615385">
We propose a principled probabilisitc frame-
work which uses trees over the vocabulary to
capture similarities among terms in an infor-
mation retrieval setting. This allows the re-
trieval of documents based not just on occur-
rences of specific query terms, but also on sim-
ilarities between terms (an effect similar to
query expansion). Additionally our principled
generative model exhibits an effect similar to
inverse document frequency. We give encour-
aging experimental evidence of the superiority
of the hierarchical Dirichlet tree compared to
standard baselines.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942942307692">
Information retrieval (IR) is the task of retrieving,
given a query, the documents relevant to the user
from a large quantity of documents (Salton and
McGill, 1983). IR has become very important in
recent years, with the proliferation of large quanti-
ties of documents on the world wide web. Many IR
systems are based on some relevance score function
R(j, q) which returns the relevance of document j to
query q. Examples of such relevance score functions
include term frequency-inverse document frequency
(tf-idf) and Okapi BM25 (Robertson et al., 1992).
Besides the effect that documents containing
more query terms should be more relevant (term fre-
quency), the main effect that many relevance scores
try to capture is that of inverse document frequency:
the importance of a term is inversely related to the
number of documents that it appears in, i.e. the
popularity of the term. This is because popular
terms, e.g. common and stop words, are often un-
informative, while rare terms are often very infor-
mative. Another important effect is that related or
co-occurring terms are often useful in determining
the relevance of documents. Because most relevance
scores do not capture this effect, IR systems resort to
techniques like query expansion which includes syn-
onyms and other morphological forms of the origi-
nal query terms in order to improve retrieval results;
e.g. (Riezler et al., 2007; Metzler and Croft, 2007).
In this paper we explore a probabilistic model for
IR that simultaneously handles both effects in a prin-
cipled manner. It builds upon the work of (Cow-
ans, 2004) who proposed a hierarchical Dirichlet
document model. In this model, each document is
modeled using a multinomial distribution (making
the bag-of-words assumption) whose parameters are
given Dirichlet priors. The common mean of the
Dirichlet priors is itself assumed random and given
a Dirichlet hyperprior. (Cowans, 2004) showed that
the shared mean parameter induces sharing of infor-
mation across documents in the corpus, and leads to
an inverse document frequency effect.
We generalize the model of (Cowans, 2004) by re-
placing the Dirichlet distributions with Dirichlet tree
distributions (Minka, 2003), thus we call our model
the hierarchical Dirichlet tree. Related terms are
placed close by in the vocabulary tree, allowing the
model to take this knowledge into account when de-
termining document relevance. This makes it unnec-
essary to use ad-hoc query expansion methods, as re-
lated words such as synonyms will be taken into ac-
count by the retrieval rule. The structure of the tree
is learned from data in an unsupervised fashion, us-
</bodyText>
<page confidence="0.986894">
173
</page>
<note confidence="0.8907915">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 173–181,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.997995">
ing a variety of agglomerative clustering techniques.
We review the hierarchical Dirichlet document
(HDD) model in section 2, and present our proposed
hierarchical Dirichlet tree (HDT) document model
in section 3. We describe three algorithms for con-
structing the vocabulary tree in section 4, and give
encouraging experimental evidence of the superi-
ority of the hierarchical Dirichlet tree compared to
standard baselines in section 5. We conclude the pa-
per in section 6.
</bodyText>
<sectionHeader confidence="0.999244" genericHeader="method">
2 Hierarchical Dirichlet Document Model
</sectionHeader>
<bodyText confidence="0.9993415">
The probabilistic approach to IR assumes that each
document in a collection can be modeled probabilis-
tically. Given a query q, it is further assumed that
relevant documents j are those with highest gener-
ative probability p(q|j) for the query. Thus given q
the relevance score is R(j, q) = p(q|j) and the doc-
uments with highest relevance are returned.
Assume that each document is a bag of words,
with document j modeled as a multinomial distri-
bution over the words in j. Let V be the terms in
the vocabulary, njw be the number of occurrences
of term w E V in document j, and 0flat
jw be the proba-
bility of w occurring in document j (the superscript
“flat” denotes a flat Dirichlet as opposed to our pro-
posed Dirichlet tree). (Cowans, 2004) assumes the
following hierarchical Bayesian model for the docu-
ment collection:
</bodyText>
<equation confidence="0.980291125">
0flat 0= (0flat
0w)wEV — Dirichlet(γu) (1)
0flat
j = (0flat
jw)wEV — Dirichlet(α0flat
0 )
nj = (njw)wEV — Multinomial(0flat
j )
</equation>
<bodyText confidence="0.969904882352941">
In the above, bold face a = (aw)wEV means that a
is a vector with |V  |entries indexed by w E V , and
u is a uniform distribution over V . The generative
process is as follows (Figure 1(a)). First a vector
0flat
0 is drawn from a symmetric Dirichlet distribution
with concentration parameter γ. Then we draw the
parameters 0flat
j for each document j from a common
Dirichlet distribution with mean 0flat
0 and concentra-
tion parameter α. Finally, the term frequencies of
the document are drawn from a multinomial distri-
bution with parameters 0flat
j .
The insight of (Cowans, 2004) is that because
the common mean parameter 0flat
</bodyText>
<footnote confidence="0.6207815">
0 is random, it in-
duces dependencies across the document models in
</footnote>
<figureCaption confidence="0.884932166666667">
Figure 1: (a) The graphical model representation of the
hierarchical Dirichlet document model. (b) The global
tree and local trees in hierarchical Dirichlet tree docu-
ment model. Triangles stand for trees with the same
structure, but different parameters at each node. The gen-
eration of words in each document is not shown.
</figureCaption>
<bodyText confidence="0.998008333333333">
the collection, and this in turn is the mechanism for
information sharing among documents. (Cowans,
2004) proposed a good estimate of 0flat
</bodyText>
<equation confidence="0.999463333333333">
0 :
0flat _ γ/ |V  |+ n0w /2)
0w — γ + PwEV n0w l
</equation>
<bodyText confidence="0.99921">
where n0w is simply the number of documents con-
taining term w, i.e. the document frequency. Inte-
grating out the document parameters 0flat
j , we see that
the probability of query q being generated from doc-
ument j is:
</bodyText>
<equation confidence="0.990547666666667">
α0flat
0x + njx
α + PwEV njw
Const + njx
γ/|V |+n0x
α + PwEV njw
</equation>
<bodyText confidence="0.999566454545455">
Where Const are terms not depending on j. We see
that njx is term frequency, its denominator γ/|V  |+
n0x is an inverse document frequency factor, and
α + PwEV njw normalizes for document length.
The inverse document frequency factor is directly
related to the shared mean parameter, in that popular
terms x will have high 0flat
0x value, causing all docu-
ments to assign higher probability to x, and down
weighting the term frequency. This effect will be
inherited by our model in the next section.
</bodyText>
<figure confidence="0.997743208333333">
Uk
θflat γ
0
α
njw
nj
J
θflat
j
(a) (b)
b
αflat
k
γk
θk0
θkj
αk
J
Y
p(q|j) =
xEq
(3)
Y= Const ·
xEq
</figure>
<page confidence="0.991204">
174
</page>
<sectionHeader confidence="0.995459" genericHeader="method">
3 Hierarchical Dirichlet Trees
</sectionHeader>
<bodyText confidence="0.999975125">
Apart from the constraint that the parameters should
sum to one, the Dirichlet priors in the HDD model
do not impose any dependency among the param-
eters of the resulting multinomial. In other words,
the document models cannot capture the notion that
related terms tend to co-occur together. For exam-
ple, this model cannot incorporate the knowledge
that if the word ‘computer’ is seen in a document, it
is likely to observe the word ‘software’ in the same
document. We relax the independence assump-
tion of the Dirichlet distribution by using Dirichlet
tree distributions (Minka, 2003), which can capture
some dependencies among the resulting parameters.
This allows relationships among terms to be mod-
eled, and we will see that it improves retrieval per-
formance.
</bodyText>
<subsectionHeader confidence="0.996816">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.9997275">
Let us assume that we have a tree over the vocab-
ulary whose leaves correspond to vocabulary terms.
Each internal node k of the tree has a multinomial
distribution over its children C(k). Words are drawn
by starting at the root of the tree, recursively picking
a child l ∈ C(k) whenever we are in an internal node
k, until we reach a leaf of the tree which corresponds
to a vocabulary term (see Figure 2(b)). The Dirich-
let tree distribution is the product of Dirichlet dis-
tributions placed over the child probabilities of each
internal node, and serves as a (dependent) prior over
the parameters of multinomial distributions over the
vocabulary (the leaves).
Our model generalizes the HDD model by replac-
ing the Dirichlet distributions in (1) by Dirichlet tree
distributions. At each internal node k, define a hier-
archical Dirichlet prior over the choice of the chil-
dren:
</bodyText>
<equation confidence="0.9994665">
θ0k = (00l)lEC(k) ∼ Dirichlet(γkuk) (4)
θjk = (0jl)lEC(k) ∼ Dirichlet(αkθ0k)
</equation>
<bodyText confidence="0.999947111111111">
where uk is a uniform distribution over the children
of node k, and each internal node has its own hy-
perparameters γk and αk. 0jl is the probability of
choosing child l if we are at internal node k. If the
tree is degenerate with just one internal node (the
root) and all leaves are direct children of the root we
recover the “flat” HDD model in the previous sec-
tion. We call our model the hierarchical Dirichlet
tree (HDT).
</bodyText>
<subsectionHeader confidence="0.995612">
3.2 Inference and Learning
</subsectionHeader>
<bodyText confidence="0.9973304">
Given a term, the path from the root to the corre-
sponding leaf is unique. Thus given the term fre-
quencies nj of document j as defined in (1), the
number of times njl child l ∈ C(k) was picked at
node k is known and fixed. The probability of all
words in document j, given the parameters, is then
a product of multinomials probabilities over internal
nodes k:
The probability of the documents, integrating out the
θjk’s, is:
</bodyText>
<equation confidence="0.99027075">
p({nj}|{θ0k}) = (6)
Y Y 7T r(αk) TT r(αkθ0l+njl)
j k njk! r(αk+njk) 11 r(αkθ0l)
REC(k) njl! lEC(k)
</equation>
<bodyText confidence="0.9561385">
The probability of a query q under document j, i.e.
the relevance score, follows from (3):
</bodyText>
<equation confidence="0.913084333333333">
Y
p(q|j) =
xEq
</equation>
<bodyText confidence="0.9999565625">
where the second product is over pairs (kl) where k
is a parent of l on the path from the root to x.
The hierarchical Dirichlet tree model we pro-
posed has a large number of parameters and hy-
perparameters (even after integrating out the θjk’s),
since the vocabulary trees we will consider later typ-
ically have large numbers of internal nodes. This
over flexibility might lead to overfitting or to param-
eter regimes that do not aid in the actual task of IR.
To avoid both issues, we constrain the hierarchical
Dirichlet tree to be centered over the flat hierarchi-
cal Dirichlet document model, and allow it to learn
only the αk hyperparameters, integrating out the θjk
parameters.
We set {θ0k}, the hyperparameters of the global
tree, so that it induces the same distribution over vo-
</bodyText>
<equation confidence="0.9686441875">
cabulary terms as θflat
0 :
X
00l = 0flat 00k =
0l
lEC(k)
Y 7T 0njl
p(nj|{θjk}) = &apos;&apos; nj k ! jl (5)
k rI1EC(k) njl! Y
lEC(k)
αkθ0l+njl
αk+njk
Y
(kl)
(7)
00l (8)
</equation>
<page confidence="0.986434">
175
</page>
<bodyText confidence="0.9975506">
The hyperparameters of the local trees αk’s are es-
timated using maximum a posteriori learning with
likelihood given by (6), and a gamma prior with
informative parameters. The density function of a
Gamma(a, b) distribution is
</bodyText>
<equation confidence="0.983761666666667">
xa−1bae−bx
αk — Gamma(bαflat
k + 1, b)
</equation>
<bodyText confidence="0.874000307692308">
and b &gt; 0 is an inverse scale hyperparameter to be
tuned, with large values giving a sharp peak around
αflat
k . We tried a few values1 of b and have found that
the results we report in the next section are not sen-
sitive to b. This prior is constructed such that if there
is insufficient information in (6) the MAP value will
simply default back to the hierarchical Dirichlet doc-
ument model.
We used LBFGS2 which is a gradient based opti-
mization method to find the MAP values, where the
gradient of the likelihood part of the objective func-
tion (6) is:
</bodyText>
<table confidence="0.9210712">
∂ log p(Jnj}|Jθ0j}) �= Ψ(αk) − Ψ(αk + njk)
∂αk j
� ( I
+ 90l Ψ(αk90l + njl) − Ψ(αk90l)
lEC(k)
</table>
<bodyText confidence="0.9998112">
where Ψ(x) := ∂ log r(x)/∂x is the digamma func-
tion. Because each αk can be optimized separately,
the optimization is very fast (approximately 15-30
minutes in the experiments to follow on a Linux ma-
chine with 1.8 GH CPU speed).
</bodyText>
<sectionHeader confidence="0.99727" genericHeader="method">
4 Vocabulary Tree Structure Learning
</sectionHeader>
<bodyText confidence="0.999483">
The structure of the vocabulary tree plays an impor-
tant role in the quality of the HDT document model,
</bodyText>
<footnote confidence="0.994849">
1Of the form 10&apos; for i E {−2, −1, 0, 1}.
2We used a C++ re-implementation of Jorge Nocedal’s
LBFGS library (Nocedal, 1980) from the ALGLIB website:
http://www.alglib.net.
</footnote>
<construct confidence="0.557235">
Algorithm 1 Greedy Agglomerative Clustering
</construct>
<listItem confidence="0.992061333333333">
1: Place m words into m singleton clusters
2: repeat
3: Merge the two clusters with highest similarity, re-
sulting in one less cluster
4: If there still are unincluded words, pick one and
place it in a singleton cluster, resulting in one more
cluster
5: until all words have been included and there is only
one cluster left
</listItem>
<bodyText confidence="0.997424416666667">
since it encapsulates the similarities among words
captured by the model. In this paper we explored
using trees learned in an unsupervised fashion from
the training corpus.
The three methods are all agglomerative cluster-
ing algorithms (Duda et al., 2000) with different
similarity functions. Initially each vocabulary word
is placed in its own cluster; each iteration of the al-
gorithm finds the pair of clusters with highest sim-
ilarity and merges them, continuing until only one
cluster is left. The sequence of merges determines a
binary tree with vocabulary words as its leaves.
Using a heap data structure, this basic agglom-
erative clustering algorithm requires O(n2 log(n) +
sn2) computations where n is the size of the vocab-
ulary and s is the amount of computation needed to
compute the similarity between two clusters. Typi-
cally the vocabulary size n is large; to speed up the
algorithm, we use a greedy version described in Al-
gorithm 1 which restricts the number of cluster can-
didates to at most m « n. This greedy version is
faster with complexity O(nm(log m + s)). In the
experiments we used m = 500.
Distributional clustering (Dcluster) (Pereira et
al., 1993) measures similarity among words in terms
of the similarity among their local contexts. Each
word is represented by the frequencies of various
words in a window around each occurrence of the
word. The similarity between two words is com-
puted to be a symmetrized KL divergence between
the distributions over neighboring words associated
with the two words. For a cluster of words the neigh-
boring words are the union of those associated with
each word in the cluster. Dcluster has been used
extensively in text classification (Baker and McCal-
lum, 1998).
</bodyText>
<subsectionHeader confidence="0.471702">
Probabilistic hierarchical clustering (Pcluster)
</subsectionHeader>
<bodyText confidence="0.874444">
g(xe a, b) = r(a)
where the mode happens at x = a−1
</bodyText>
<figure confidence="0.883717166666667">
b . We set the
mode of the prior such that the hierarchical Dirichlet
tree reduces to the hierarchical Dirichlet document
model at these values:
flat flat flpt � αflat
αl = α90l ak = lEC(k) l (9)
</figure>
<page confidence="0.995568">
176
</page>
<bodyText confidence="0.999713857142857">
(Friedman, 2003). Dcluster associates each word
with its local context, as a result it captures both
semantic and syntactic relationships among words.
Pcluster captures more relevant semantic relation-
ships by instead associating each word with the doc-
uments in which it appears. Specifically, each word
is associated with a binary vector indexed by doc-
uments in the corpus, where a 1 means the word
appears in the corresponding document. Pcluster
models a cluster of words probabilistically, with the
binary vectors being iid draws from a product of
Bernoulli distributions. The similarity of two clus-
ters c1 and c2 of words is P(c1 U c2)/P(c1)P(c2),
i.e. two clusters of words are similar if their union
can be effectively modeled using one cluster, rela-
tive to modeling each separately. Conjugate beta pri-
ors are placed over the parameters of the Bernoulli
distributions and integrated out so that the similarity
scores are comparable.
Brown’s algorithm (Bcluster) (Brown et al.,
1990) was originally proposed to build class-based
language models. In the 2-gram case, words are
clustered such that the class of the previous word
is most predictive of the class of the current word.
Thus the similarity between two clusters of words
is defined to be the resulting mutual information be-
tween adjacent classes corrresponding to a sequence
of words.
</bodyText>
<subsectionHeader confidence="0.996225">
4.1 Operations to Simplify Trees
</subsectionHeader>
<bodyText confidence="0.999913375">
Trees constructed using the agglomerative hierarchi-
cal clustering algorithms described in this section
suffer from a few drawbacks. Firstly, because they
are binary trees they have large numbers of internal
nodes. Secondly, many internal nodes are simply not
informative in that the two clusters of words below
a node are indistinguishable. Thirdly, Pcluster and
Dcluster tend to produce long chain-like branches
which significantly slows down the computation of
the relevance score.
To address these issues, we considered operations
to simplify trees by contracting internal edges of the
tree while preserving as much of the word relation-
ship information as possible. Let L be the set of tree
leaves and T(a) be the distance from node or edge a
to the leaves:
</bodyText>
<equation confidence="0.703998">
T(a) := min#{edges between a and l} (10)
l∈L
</equation>
<figureCaption confidence="0.993939333333333">
Figure 2: T(root) = 2, while T(v) = 1 for shaded ver-
tices v. Contracting a and b results in both child of b
being direct children of a while b is removed.
</figureCaption>
<bodyText confidence="0.999399235294118">
In the experiments we considered either contracting
edges3 close to the leaves T(a) = 1 (thus remov-
ing many of the long branches described above), or
edges further up the tree T(a) ≥ 2 (preserving the
informative subtrees closer to the leaves while re-
moving many internal nodes). See Figure 2.
(Miller et al., 2004) cut the BCluster tree at a cer-
tain depth k to simplify the tree, meaning every leaf
descending from a particular internal node at level
k is made an immediate child of that node. They
use the tree to get extra features for a discrimina-
tive model to tackle the problem of sparsity—the
features obtained from the new tree do not suffer
from sparsity since each node has several words as
its leaves. This technique did not work well for our
application so we will not report results using it in
our experiments.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9996708">
In this section we present experimental results on
two IR datasets: Cranfield and Medline4. The Cran-
field dataset consists of 1,400 documents and 225
queries; its vocabulary size after stemming and re-
moving stop words is 4,227. The Medline dataset
contains 1,033 documents and 30 queries with the
vocabulary size of 8,800 after stemming and remov-
ing stop words. We compare HDT with the flat
HDD model and Okapi BM25 (Robertson et al.,
1992). Since one of our motivations has been to
</bodyText>
<footnote confidence="0.99864275">
3Contracting an edge means removing the edge and the adja-
cent child node and connecting the grandchildren to the parent.
4Both datasets can be downloaded from
http://www.dcs.gla.ac.uk/idom/ir resources/test collections.
</footnote>
<figure confidence="0.997011">
a
b
</figure>
<page confidence="0.981179">
177
</page>
<table confidence="0.999841866666667">
Tree Depth Statistics Performance
Cranfield Medline Cranfield Medline
avg / max total avg / max total avg-pr top10-pr avg-pr top10-pr
BCluster 16.7 / 24 4226 16.4 / 22 8799 0.2675 0.3218 0.2131 0.6433
BC contract T &gt; 2 6.2 / 16 3711 5.3 / 14 7473 0.2685 0.3147 0.2079 0.6533
BC contract T = 1 16.1 / 23 3702 15.8 / 22 7672 0.2685 0.3204 0.1975 0.6400
DCluster 41.2 / 194 4226 38.1 / 176 8799 0.2552 0.3120 0.1906 0.6300
DC contract T &gt; 2 2.3 / 8 2469 3.3 / 9 5091 0.2555 0.3156 0.1906 0.6167
DC contract T = 1 40.9 / 194 3648 38.1 / 176 8799 0.2597 0.3129 0.1848 0.6300
PCluster 50.2 / 345 4226 37.1 / 561 8799 0.2613 0.3231 0.1681 0.6633
PC contract T &gt; 2 35.2 / 318 3741 20.4 / 514 7280 0.2624 0.3213 0.1792 0.6767
PC contract T = 1 33.6 / 345 2246 34.1 / 561 4209 0.2588 0.3240 0.1880 0.6633
flat model 1 / 1 1 1 / 1 1 0.2506 0.3089 0.1381 0.6133
BM25 – – – – 0.2566 0.3124 0.1804 0.6567
BM25QueryExp – – – – 0.2097 0.3191 0.2121 0.7366
</table>
<tableCaption confidence="0.982062333333333">
Table 1: Average precision and Top-10 precision scores of HDT with different trees versus flat model and BM25. The
statistics for each tree shows its average/maximum depth of its leaf nodes as well as the number of its total internal
nodes. The bold numbers highlight the best results in the corresponding columns.
</tableCaption>
<bodyText confidence="0.999905714285714">
get away from query expansion, we also compare
against Okapi BM25 with query expansion. The
new terms to expand each query are chosen based
on Robertson-Sparck Jones weights (Robertson and
Sparck Jones, 1976) from the pseudo relevant docu-
ments. The comparison criteria are (i) top-10 preci-
sion, and (ii) average precision.
</bodyText>
<subsectionHeader confidence="0.997609">
5.1 HDT vs Baselines
</subsectionHeader>
<bodyText confidence="0.999932341463415">
All the hierarchical clustering algorithms mentioned
in section 4 are used to generate trees, each of which
is further post-processed by tree simplification op-
erators described in section 4.1. We consider (i)
contracting nodes at higher levels of the hierarchy
(T &gt; 2), and (ii) contracting nodes right above the
leaves (T = 1).
The statistics of the trees before and after post-
processing are shown in Table 1. Roughly, the
Dcluster and BCluster trees do not have long chains
with leaves hanging directly off them, which is why
their average depths are reduced significantly by the
T &gt; 2 simplification, but not by the T = 1 sim-
plification. The converse is true for Pcluster: the
trees have many chains with leaves hanging directly
off them, which is why average depth is not reduced
as much as the previous trees based on the T &gt; 2
simplification. However the average depth is still re-
duced significantly compared to the original trees.
Table 1 presents the performance of HDT with
different trees against the baselines in terms of the
top-10 and average precision (we have bold faced
the performance values which are the maximum
of each column). HDT with every tree outper-
forms significantly the flat model in both datasets.
More specifically, HDT with (original) BCluster and
PCluster trees significantly outperforms the three
baselines in terms of both performance measure for
the Cranfield. Similar trends are observed on the
Medline except here the baseline Okapi BM25 with
query expansion is pretty strong5, which is still out-
performed by HDT with BCluster tree.
To further highlight the differences among the
methods, we have shown the precision at particular
recall points on Medline dataset in Figure 4 for HDT
with PCluster tree vs the baselines. As the recall
increases, the precision of the PCluster tree signifi-
cantly outperforms the flat model and BM25. We at-
tribute this to the ability of PCluster tree to give high
scores to documents which have words relevant to a
query word (an effect similar to query expansion).
</bodyText>
<subsectionHeader confidence="0.999343">
5.2 Analysis
</subsectionHeader>
<bodyText confidence="0.750932166666667">
It is interesting to contrast the learned αk’s for each
of the clustering methods. These αk’s impose cor-
5Note that we tuned the parameters of the baselines BM25
with/without query expansion with respect to their performance
on the actual retrieval task, which in a sense makes them appear
better than they should.
</bodyText>
<page confidence="0.976704">
178
</page>
<figure confidence="0.999003814814815">
PCluster
103
102
101
100
10−1
10−2
10−2 10−1 100 101 102 103
θ0k αparent(k)
αk
BCluster
DCluster
103
103
102
102
101
101
αk
αk
100
100
10−1
10−1
10−2
10−1 100 101 102 103 10−2 10−1 100 101 102 103
θ0k αparent(k) θ0k αparent(k)
</figure>
<figureCaption confidence="0.9893185">
Figure 3: The plots showing the contribution of internal nodes in trees constructed by the three clustering algorithms
for the Cranfield dataset. In each plot, a point represent an internal node showing a positive exponent in the node’s
contribution (i.e. positive correlation among its children) if the point is below x = y line. From left to the right plots,
the fraction of nodes below the line is 0.9044, 0.7977, and 0.3344 for a total of 4,226 internal nodes.
Figure 4: The precision of all methods at particular recall
points for the Medline dataset.
</figureCaption>
<bodyText confidence="0.999962325">
relations on the probabilities of the children under k
in an interesting fashion. In particular, if we com-
pare αk to 00kαparent(k), then a larger value of αk
implies that the probabilities of picking one of the
children of k (from among all nodes) are positively
correlated, while a smaller value of αk implies neg-
ative correlation. Roughly speaking, this is because
drawn values of 0pl for l ∈ C(k) are more likely
to be closer to uniform (relative to the flat Dirichlet)
thus if we had picked one child of k we will likely
pick another child of k.
Figure 3 shows scatter plots of αk values ver-
sus 00kαparent(k) for the internal nodes of the trees.
Firstly, smaller values for both tend to be associ-
ated with lower levels of the trees, while large val-
ues are with higher levels of the trees. Thus we
see that PCluster tend to have subtrees of vocabu-
lary terms that are positively correlated with each
other—i.e. they tend to co-occur in the same docu-
ments. The converse is true of DCluster and BClus-
ter because they tend to put words with the same
meaning together, thus to express a particular con-
cept it is enough to select one of the words and not
to choose the rest. Figure 5 show some fragments
of the actual trees including the words they placed
together and αk parameters learned by HDT model
for their internal nodes. Moreover, visual inspection
of the trees shows that DCluster can easily misplace
words in the tree, which explains its lower perfor-
mance compared to the other tree construction meth-
ods.
Secondly, we observed that for higher nodes of
the tree (corresponding generally to larger values of
αk and 00kαparent(k)) PCluster αk’s are smaller, thus
higher levels of the tree exhibit negative correlation.
This is reasonable, since if the subtrees capture pos-
itively correlated words, then higher up the tree the
different subtrees correspond to clusters of words
that do not co-occur together, i.e. negatively corre-
lated.
</bodyText>
<sectionHeader confidence="0.998257" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999682">
We presented a hierarchical Dirichlet tree model for
information retrieval which can inject (semantical or
syntactical) word relationships as the domain knowl-
edge into a probabilistic model for information re-
trieval. Using trees to capture word relationships,
the model is highly efficient while making use of
both prior information about words and their occur-
rence statistics in the corpus. Furthermore, we inves-
tigated the effect of different tree construction algo-
rithms on the model performance.
On the Cranfield dataset, HDT achieves 26.85%
for average-precision and 32.40% for top-10 preci-
</bodyText>
<figure confidence="0.9967984">
Precision at particular recall points
1
PCluster
Flat model
BM25
BM25 Query Expansion
0.7
0.6
0.5
0.4
0.3
0.2
0 .1 .2 .3 .4 .5 .6 .7 .8 .9
0.9
0.8
</figure>
<page confidence="0.766547">
179
</page>
<figureCaption confidence="0.9905055">
Figure 5: Small parts of the trees learned by clustering algorithms for the Cranfield dataset where the learned αk for
each internal node is written close to it.
</figureCaption>
<bodyText confidence="0.9999911875">
sion, and outperforms all baselines including BM25
which gets 25.66% and 31.24% for these two mea-
sures. On the Medline dataset, HDT is competi-
tive with BM25 with Query Expansion and outper-
forms all other baselines. These encouraging results
show the benefits of HDT as a principled probabilis-
tic model for information retrieval.
An interesting avenue of research is to construct
the vocabulary tree based on WordNet, as a way to
inject independent prior knowledge into the model.
However WordNet has a low coverage problem, i.e.
there are some words in the data which do not ex-
ist in it. One solution to this low coverage problem
is to combine trees generated by the clustering algo-
rithms mentioned in this paper and WordNet, which
we leave as a future work.
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999818035714286">
L. Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In SIGIR ’98: Proceedings of the
21st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 96–103.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1990. Class-based n-gram models
of natural language. Computational Linguistics.
P. J. Cowans. 2004. Information retrieval using hierar-
chical dirichlet processes. In Proceedings of the 27th
Annual International Conference on Research and De-
velopment in Information Retrieval (SIGIR).
R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pattern
Classification. Wiley-Interscience Publication.
N. Friedman. 2003. Pcluster: Probabilistic agglomera-
tive clustering of gene expression profiles. Available
from http://citeseer.ist.psu.edu/668029.html.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In Pro-
ceedings of the 30th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In Proceedings of North American Chapter of the As-
sociation for Computational Linguistics -Human Lan-
guage Technologies conference (NAACL HLT).
</reference>
<page confidence="0.973438">
180
</page>
<reference confidence="0.999207304347826">
T. Minka. 2003. The dirichlet-tree distribu-
tion. Available from http://research.microsoft.com/
minka/papers/dirichlet/minka-dirtree.pdf.
J. Nocedal. 1980. Updating quasi-newton matrices with
limited storage. Mathematics of Computation, 35.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In 31st
Annual Meeting of the Association for Computational
Linguistics, pages 183–190.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics.
S. E. Robertson and K. Sparck Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27(3):129–146.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at trec. In Text
REtrieval Conference, pages 21–30.
G. Salton and M.J. McGill. 1983. An Introduction to
Modern Information Retrieval. McGraw-Hill, New
York.
</reference>
<page confidence="0.998362">
181
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672498">
<title confidence="0.999519">Hierarchical Dirichlet Trees for Information Retrieval</title>
<author confidence="0.955964">Gholamreza Haffari Yee Whye Teh</author>
<affiliation confidence="0.850146">School of Computing Sciences Gatsby Computational Neuroscience Simon Fraser University University College London</affiliation>
<email confidence="0.940516">ghaffar1@cs.sfu.caywteh@gatsby.ucl.ac.uk</email>
<abstract confidence="0.998123">We propose a principled probabilisitc framework which uses trees over the vocabulary to capture similarities among terms in an information retrieval setting. This allows the retrieval of documents based not just on occurrences of specific query terms, but also on similarities between terms (an effect similar to query expansion). Additionally our principled generative model exhibits an effect similar to inverse document frequency. We give encouraging experimental evidence of the superiority of the hierarchical Dirichlet tree compared to standard baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Douglas Baker</author>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Distributional clustering of words for text classification.</title>
<date>1998</date>
<booktitle>In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="14507" citStr="Baker and McCallum, 1998" startWordPosition="2479" endWordPosition="2483">e used m = 500. Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. Each word is represented by the frequencies of various words in a window around each occurrence of the word. The similarity between two words is computed to be a symmetrized KL divergence between the distributions over neighboring words associated with the two words. For a cluster of words the neighboring words are the union of those associated with each word in the cluster. Dcluster has been used extensively in text classification (Baker and McCallum, 1998). Probabilistic hierarchical clustering (Pcluster) g(xe a, b) = r(a) where the mode happens at x = a−1 b . We set the mode of the prior such that the hierarchical Dirichlet tree reduces to the hierarchical Dirichlet document model at these values: flat flat flpt � αflat αl = α90l ak = lEC(k) l (9) 176 (Friedman, 2003). Dcluster associates each word with its local context, as a result it captures both semantic and syntactic relationships among words. Pcluster captures more relevant semantic relationships by instead associating each word with the documents in which it appears. Specifically, each</context>
</contexts>
<marker>Baker, McCallum, 1998</marker>
<rawString>L. Douglas Baker and Andrew Kachites McCallum. 1998. Distributional clustering of words for text classification. In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1990</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="15797" citStr="Brown et al., 1990" startWordPosition="2693" endWordPosition="2696">rpus, where a 1 means the word appears in the corresponding document. Pcluster models a cluster of words probabilistically, with the binary vectors being iid draws from a product of Bernoulli distributions. The similarity of two clusters c1 and c2 of words is P(c1 U c2)/P(c1)P(c2), i.e. two clusters of words are similar if their union can be effectively modeled using one cluster, relative to modeling each separately. Conjugate beta priors are placed over the parameters of the Bernoulli distributions and integrated out so that the similarity scores are comparable. Brown’s algorithm (Bcluster) (Brown et al., 1990) was originally proposed to build class-based language models. In the 2-gram case, words are clustered such that the class of the previous word is most predictive of the class of the current word. Thus the similarity between two clusters of words is defined to be the resulting mutual information between adjacent classes corrresponding to a sequence of words. 4.1 Operations to Simplify Trees Trees constructed using the agglomerative hierarchical clustering algorithms described in this section suffer from a few drawbacks. Firstly, because they are binary trees they have large numbers of internal</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1990</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1990. Class-based n-gram models of natural language. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Cowans</author>
</authors>
<title>Information retrieval using hierarchical dirichlet processes.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th Annual International Conference on Research and Development in Information Retrieval (SIGIR).</booktitle>
<contexts>
<context position="2408" citStr="Cowans, 2004" startWordPosition="371" endWordPosition="373">hile rare terms are often very informative. Another important effect is that related or co-occurring terms are often useful in determining the relevance of documents. Because most relevance scores do not capture this effect, IR systems resort to techniques like query expansion which includes synonyms and other morphological forms of the original query terms in order to improve retrieval results; e.g. (Riezler et al., 2007; Metzler and Croft, 2007). In this paper we explore a probabilistic model for IR that simultaneously handles both effects in a principled manner. It builds upon the work of (Cowans, 2004) who proposed a hierarchical Dirichlet document model. In this model, each document is modeled using a multinomial distribution (making the bag-of-words assumption) whose parameters are given Dirichlet priors. The common mean of the Dirichlet priors is itself assumed random and given a Dirichlet hyperprior. (Cowans, 2004) showed that the shared mean parameter induces sharing of information across documents in the corpus, and leads to an inverse document frequency effect. We generalize the model of (Cowans, 2004) by replacing the Dirichlet distributions with Dirichlet tree distributions (Minka,</context>
<context position="4895" citStr="Cowans, 2004" startWordPosition="776" endWordPosition="777"> it is further assumed that relevant documents j are those with highest generative probability p(q|j) for the query. Thus given q the relevance score is R(j, q) = p(q|j) and the documents with highest relevance are returned. Assume that each document is a bag of words, with document j modeled as a multinomial distribution over the words in j. Let V be the terms in the vocabulary, njw be the number of occurrences of term w E V in document j, and 0flat jw be the probability of w occurring in document j (the superscript “flat” denotes a flat Dirichlet as opposed to our proposed Dirichlet tree). (Cowans, 2004) assumes the following hierarchical Bayesian model for the document collection: 0flat 0= (0flat 0w)wEV — Dirichlet(γu) (1) 0flat j = (0flat jw)wEV — Dirichlet(α0flat 0 ) nj = (njw)wEV — Multinomial(0flat j ) In the above, bold face a = (aw)wEV means that a is a vector with |V |entries indexed by w E V , and u is a uniform distribution over V . The generative process is as follows (Figure 1(a)). First a vector 0flat 0 is drawn from a symmetric Dirichlet distribution with concentration parameter γ. Then we draw the parameters 0flat j for each document j from a common Dirichlet distribution with </context>
<context position="6226" citStr="Cowans, 2004" startWordPosition="1003" endWordPosition="1004"> distribution with parameters 0flat j . The insight of (Cowans, 2004) is that because the common mean parameter 0flat 0 is random, it induces dependencies across the document models in Figure 1: (a) The graphical model representation of the hierarchical Dirichlet document model. (b) The global tree and local trees in hierarchical Dirichlet tree document model. Triangles stand for trees with the same structure, but different parameters at each node. The generation of words in each document is not shown. the collection, and this in turn is the mechanism for information sharing among documents. (Cowans, 2004) proposed a good estimate of 0flat 0 : 0flat _ γ/ |V |+ n0w /2) 0w — γ + PwEV n0w l where n0w is simply the number of documents containing term w, i.e. the document frequency. Integrating out the document parameters 0flat j , we see that the probability of query q being generated from document j is: α0flat 0x + njx α + PwEV njw Const + njx γ/|V |+n0x α + PwEV njw Where Const are terms not depending on j. We see that njx is term frequency, its denominator γ/|V |+ n0x is an inverse document frequency factor, and α + PwEV njw normalizes for document length. The inverse document frequency factor i</context>
</contexts>
<marker>Cowans, 2004</marker>
<rawString>P. J. Cowans. 2004. Information retrieval using hierarchical dirichlet processes. In Proceedings of the 27th Annual International Conference on Research and Development in Information Retrieval (SIGIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
<author>D G Stork</author>
</authors>
<title>Pattern Classification. Wiley-Interscience Publication.</title>
<date>2000</date>
<contexts>
<context position="13042" citStr="Duda et al., 2000" startWordPosition="2232" endWordPosition="2235">orithm 1 Greedy Agglomerative Clustering 1: Place m words into m singleton clusters 2: repeat 3: Merge the two clusters with highest similarity, resulting in one less cluster 4: If there still are unincluded words, pick one and place it in a singleton cluster, resulting in one more cluster 5: until all words have been included and there is only one cluster left since it encapsulates the similarities among words captured by the model. In this paper we explored using trees learned in an unsupervised fashion from the training corpus. The three methods are all agglomerative clustering algorithms (Duda et al., 2000) with different similarity functions. Initially each vocabulary word is placed in its own cluster; each iteration of the algorithm finds the pair of clusters with highest similarity and merges them, continuing until only one cluster is left. The sequence of merges determines a binary tree with vocabulary words as its leaves. Using a heap data structure, this basic agglomerative clustering algorithm requires O(n2 log(n) + sn2) computations where n is the size of the vocabulary and s is the amount of computation needed to compute the similarity between two clusters. Typically the vocabulary size</context>
</contexts>
<marker>Duda, Hart, Stork, 2000</marker>
<rawString>R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pattern Classification. Wiley-Interscience Publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friedman</author>
</authors>
<title>Pcluster: Probabilistic agglomerative clustering of gene expression profiles. Available from http://citeseer.ist.psu.edu/668029.html.</title>
<date>2003</date>
<contexts>
<context position="14826" citStr="Friedman, 2003" startWordPosition="2540" endWordPosition="2541">etrized KL divergence between the distributions over neighboring words associated with the two words. For a cluster of words the neighboring words are the union of those associated with each word in the cluster. Dcluster has been used extensively in text classification (Baker and McCallum, 1998). Probabilistic hierarchical clustering (Pcluster) g(xe a, b) = r(a) where the mode happens at x = a−1 b . We set the mode of the prior such that the hierarchical Dirichlet tree reduces to the hierarchical Dirichlet document model at these values: flat flat flpt � αflat αl = α90l ak = lEC(k) l (9) 176 (Friedman, 2003). Dcluster associates each word with its local context, as a result it captures both semantic and syntactic relationships among words. Pcluster captures more relevant semantic relationships by instead associating each word with the documents in which it appears. Specifically, each word is associated with a binary vector indexed by documents in the corpus, where a 1 means the word appears in the corresponding document. Pcluster models a cluster of words probabilistically, with the binary vectors being iid draws from a product of Bernoulli distributions. The similarity of two clusters c1 and c2 </context>
</contexts>
<marker>Friedman, 2003</marker>
<rawString>N. Friedman. 2003. Pcluster: Probabilistic agglomerative clustering of gene expression profiles. Available from http://citeseer.ist.psu.edu/668029.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>Latent concept expansion using markov random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="2246" citStr="Metzler and Croft, 2007" startWordPosition="341" endWordPosition="344">related to the number of documents that it appears in, i.e. the popularity of the term. This is because popular terms, e.g. common and stop words, are often uninformative, while rare terms are often very informative. Another important effect is that related or co-occurring terms are often useful in determining the relevance of documents. Because most relevance scores do not capture this effect, IR systems resort to techniques like query expansion which includes synonyms and other morphological forms of the original query terms in order to improve retrieval results; e.g. (Riezler et al., 2007; Metzler and Croft, 2007). In this paper we explore a probabilistic model for IR that simultaneously handles both effects in a principled manner. It builds upon the work of (Cowans, 2004) who proposed a hierarchical Dirichlet document model. In this model, each document is modeled using a multinomial distribution (making the bag-of-words assumption) whose parameters are given Dirichlet priors. The common mean of the Dirichlet priors is itself assumed random and given a Dirichlet hyperprior. (Cowans, 2004) showed that the shared mean parameter induces sharing of information across documents in the corpus, and leads to </context>
</contexts>
<marker>Metzler, Croft, 2007</marker>
<rawString>Donald Metzler and W. Bruce Croft. 2007. Latent concept expansion using markov random fields. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>J Guinness</author>
<author>A Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics -Human Language Technologies conference (NAACL HLT).</booktitle>
<contexts>
<context position="17467" citStr="Miller et al., 2004" startWordPosition="2975" endWordPosition="2978">as possible. Let L be the set of tree leaves and T(a) be the distance from node or edge a to the leaves: T(a) := min#{edges between a and l} (10) l∈L Figure 2: T(root) = 2, while T(v) = 1 for shaded vertices v. Contracting a and b results in both child of b being direct children of a while b is removed. In the experiments we considered either contracting edges3 close to the leaves T(a) = 1 (thus removing many of the long branches described above), or edges further up the tree T(a) ≥ 2 (preserving the informative subtrees closer to the leaves while removing many internal nodes). See Figure 2. (Miller et al., 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. They use the tree to get extra features for a discriminative model to tackle the problem of sparsity—the features obtained from the new tree do not suffer from sparsity since each node has several words as its leaves. This technique did not work well for our application so we will not report results using it in our experiments. 5 Experiments In this section we present experimental results on two IR datasets: Cranfiel</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>S. Miller, J. Guinness, and A. Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proceedings of North American Chapter of the Association for Computational Linguistics -Human Language Technologies conference (NAACL HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Minka</author>
</authors>
<title>The dirichlet-tree distribution.</title>
<date>2003</date>
<note>Available from http://research.microsoft.com/ minka/papers/dirichlet/minka-dirtree.pdf.</note>
<contexts>
<context position="3014" citStr="Minka, 2003" startWordPosition="461" endWordPosition="462"> 2004) who proposed a hierarchical Dirichlet document model. In this model, each document is modeled using a multinomial distribution (making the bag-of-words assumption) whose parameters are given Dirichlet priors. The common mean of the Dirichlet priors is itself assumed random and given a Dirichlet hyperprior. (Cowans, 2004) showed that the shared mean parameter induces sharing of information across documents in the corpus, and leads to an inverse document frequency effect. We generalize the model of (Cowans, 2004) by replacing the Dirichlet distributions with Dirichlet tree distributions (Minka, 2003), thus we call our model the hierarchical Dirichlet tree. Related terms are placed close by in the vocabulary tree, allowing the model to take this knowledge into account when determining document relevance. This makes it unnecessary to use ad-hoc query expansion methods, as related words such as synonyms will be taken into account by the retrieval rule. The structure of the tree is learned from data in an unsupervised fashion, us173 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 173–181, Boulder, Colorado, June 2009. c�2009 Association </context>
<context position="7808" citStr="Minka, 2003" startWordPosition="1299" endWordPosition="1300">Hierarchical Dirichlet Trees Apart from the constraint that the parameters should sum to one, the Dirichlet priors in the HDD model do not impose any dependency among the parameters of the resulting multinomial. In other words, the document models cannot capture the notion that related terms tend to co-occur together. For example, this model cannot incorporate the knowledge that if the word ‘computer’ is seen in a document, it is likely to observe the word ‘software’ in the same document. We relax the independence assumption of the Dirichlet distribution by using Dirichlet tree distributions (Minka, 2003), which can capture some dependencies among the resulting parameters. This allows relationships among terms to be modeled, and we will see that it improves retrieval performance. 3.1 Model Let us assume that we have a tree over the vocabulary whose leaves correspond to vocabulary terms. Each internal node k of the tree has a multinomial distribution over its children C(k). Words are drawn by starting at the root of the tree, recursively picking a child l ∈ C(k) whenever we are in an internal node k, until we reach a leaf of the tree which corresponds to a vocabulary term (see Figure 2(b)). The</context>
</contexts>
<marker>Minka, 2003</marker>
<rawString>T. Minka. 2003. The dirichlet-tree distribution. Available from http://research.microsoft.com/ minka/papers/dirichlet/minka-dirtree.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
</authors>
<title>Updating quasi-newton matrices with limited storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<volume>35</volume>
<contexts>
<context position="12372" citStr="Nocedal, 1980" startWordPosition="2125" endWordPosition="2126">e objective function (6) is: ∂ log p(Jnj}|Jθ0j}) �= Ψ(αk) − Ψ(αk + njk) ∂αk j � ( I + 90l Ψ(αk90l + njl) − Ψ(αk90l) lEC(k) where Ψ(x) := ∂ log r(x)/∂x is the digamma function. Because each αk can be optimized separately, the optimization is very fast (approximately 15-30 minutes in the experiments to follow on a Linux machine with 1.8 GH CPU speed). 4 Vocabulary Tree Structure Learning The structure of the vocabulary tree plays an important role in the quality of the HDT document model, 1Of the form 10&apos; for i E {−2, −1, 0, 1}. 2We used a C++ re-implementation of Jorge Nocedal’s LBFGS library (Nocedal, 1980) from the ALGLIB website: http://www.alglib.net. Algorithm 1 Greedy Agglomerative Clustering 1: Place m words into m singleton clusters 2: repeat 3: Merge the two clusters with highest similarity, resulting in one less cluster 4: If there still are unincluded words, pick one and place it in a singleton cluster, resulting in one more cluster 5: until all words have been included and there is only one cluster left since it encapsulates the similarities among words captured by the model. In this paper we explored using trees learned in an unsupervised fashion from the training corpus. The three m</context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>J. Nocedal. 1980. Updating quasi-newton matrices with limited storage. Mathematics of Computation, 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="13957" citStr="Pereira et al., 1993" startWordPosition="2389" endWordPosition="2392"> vocabulary words as its leaves. Using a heap data structure, this basic agglomerative clustering algorithm requires O(n2 log(n) + sn2) computations where n is the size of the vocabulary and s is the amount of computation needed to compute the similarity between two clusters. Typically the vocabulary size n is large; to speed up the algorithm, we use a greedy version described in Algorithm 1 which restricts the number of cluster candidates to at most m « n. This greedy version is faster with complexity O(nm(log m + s)). In the experiments we used m = 500. Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. Each word is represented by the frequencies of various words in a window around each occurrence of the word. The similarity between two words is computed to be a symmetrized KL divergence between the distributions over neighboring words associated with the two words. For a cluster of words the neighboring words are the union of those associated with each word in the cluster. Dcluster has been used extensively in text classification (Baker and McCallum, 1998). Probabilistic hierarchical clustering (Pcluster)</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In 31st Annual Meeting of the Association for Computational Linguistics, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="2220" citStr="Riezler et al., 2007" startWordPosition="337" endWordPosition="340">f a term is inversely related to the number of documents that it appears in, i.e. the popularity of the term. This is because popular terms, e.g. common and stop words, are often uninformative, while rare terms are often very informative. Another important effect is that related or co-occurring terms are often useful in determining the relevance of documents. Because most relevance scores do not capture this effect, IR systems resort to techniques like query expansion which includes synonyms and other morphological forms of the original query terms in order to improve retrieval results; e.g. (Riezler et al., 2007; Metzler and Croft, 2007). In this paper we explore a probabilistic model for IR that simultaneously handles both effects in a principled manner. It builds upon the work of (Cowans, 2004) who proposed a hierarchical Dirichlet document model. In this model, each document is modeled using a multinomial distribution (making the bag-of-words assumption) whose parameters are given Dirichlet priors. The common mean of the Dirichlet priors is itself assumed random and given a Dirichlet hyperprior. (Cowans, 2004) showed that the shared mean parameter induces sharing of information across documents in</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>K Sparck Jones</author>
</authors>
<title>Relevance weighting of search terms.</title>
<date>1976</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>27</volume>
<issue>3</issue>
<marker>Robertson, Jones, 1976</marker>
<rawString>S. E. Robertson and K. Sparck Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information Science, 27(3):129–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>M Hancock-Beaulieu</author>
<author>A Gull</author>
<author>M Lau</author>
</authors>
<title>Okapi at trec.</title>
<date>1992</date>
<booktitle>In Text REtrieval Conference,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="1381" citStr="Robertson et al., 1992" startWordPosition="200" endWordPosition="203">rchical Dirichlet tree compared to standard baselines. 1 Introduction Information retrieval (IR) is the task of retrieving, given a query, the documents relevant to the user from a large quantity of documents (Salton and McGill, 1983). IR has become very important in recent years, with the proliferation of large quantities of documents on the world wide web. Many IR systems are based on some relevance score function R(j, q) which returns the relevance of document j to query q. Examples of such relevance score functions include term frequency-inverse document frequency (tf-idf) and Okapi BM25 (Robertson et al., 1992). Besides the effect that documents containing more query terms should be more relevant (term frequency), the main effect that many relevance scores try to capture is that of inverse document frequency: the importance of a term is inversely related to the number of documents that it appears in, i.e. the popularity of the term. This is because popular terms, e.g. common and stop words, are often uninformative, while rare terms are often very informative. Another important effect is that related or co-occurring terms are often useful in determining the relevance of documents. Because most releva</context>
<context position="18431" citStr="Robertson et al., 1992" startWordPosition="3144" endWordPosition="3147">parsity since each node has several words as its leaves. This technique did not work well for our application so we will not report results using it in our experiments. 5 Experiments In this section we present experimental results on two IR datasets: Cranfield and Medline4. The Cranfield dataset consists of 1,400 documents and 225 queries; its vocabulary size after stemming and removing stop words is 4,227. The Medline dataset contains 1,033 documents and 30 queries with the vocabulary size of 8,800 after stemming and removing stop words. We compare HDT with the flat HDD model and Okapi BM25 (Robertson et al., 1992). Since one of our motivations has been to 3Contracting an edge means removing the edge and the adjacent child node and connecting the grandchildren to the parent. 4Both datasets can be downloaded from http://www.dcs.gla.ac.uk/idom/ir resources/test collections. a b 177 Tree Depth Statistics Performance Cranfield Medline Cranfield Medline avg / max total avg / max total avg-pr top10-pr avg-pr top10-pr BCluster 16.7 / 24 4226 16.4 / 22 8799 0.2675 0.3218 0.2131 0.6433 BC contract T &gt; 2 6.2 / 16 3711 5.3 / 14 7473 0.2685 0.3147 0.2079 0.6533 BC contract T = 1 16.1 / 23 3702 15.8 / 22 7672 0.2685</context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, Gull, Lau, 1992</marker>
<rawString>S. E. Robertson, S. Walker, M. Hancock-Beaulieu, A. Gull, and M. Lau. 1992. Okapi at trec. In Text REtrieval Conference, pages 21–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>An Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="992" citStr="Salton and McGill, 1983" startWordPosition="137" endWordPosition="140">rms in an information retrieval setting. This allows the retrieval of documents based not just on occurrences of specific query terms, but also on similarities between terms (an effect similar to query expansion). Additionally our principled generative model exhibits an effect similar to inverse document frequency. We give encouraging experimental evidence of the superiority of the hierarchical Dirichlet tree compared to standard baselines. 1 Introduction Information retrieval (IR) is the task of retrieving, given a query, the documents relevant to the user from a large quantity of documents (Salton and McGill, 1983). IR has become very important in recent years, with the proliferation of large quantities of documents on the world wide web. Many IR systems are based on some relevance score function R(j, q) which returns the relevance of document j to query q. Examples of such relevance score functions include term frequency-inverse document frequency (tf-idf) and Okapi BM25 (Robertson et al., 1992). Besides the effect that documents containing more query terms should be more relevant (term frequency), the main effect that many relevance scores try to capture is that of inverse document frequency: the impo</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. An Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>