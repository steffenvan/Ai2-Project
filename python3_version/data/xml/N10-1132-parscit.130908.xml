<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.987436">
Interpretation and Transformation for Abstracting Conversations
</title>
<author confidence="0.864229">
Gabriel Murray Giuseppe Carenini Raymond Ng
</author>
<affiliation confidence="0.587991">
gabrielm@cs.ubc.ca carenini@cs.ubc.ca rng@cs.ubc.ca
Department of Computer Science, University of British Columbia
Vancouver, Canada
</affiliation>
<sectionHeader confidence="0.964374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996174">
We address the challenge of automatically ab-
stracting conversations such as face-to-face
meetings and emails. We focus here on
the stages of interpretation, where sentences
are mapped to a conversation ontology, and
transformation, where the summary content
is selected. Our approach is fully developed
and tested on meeting speech, and we subse-
quently explore its application to email con-
versations.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962784615385">
The dominant approach to the challenge of auto-
matic summarization has been extraction, where in-
formative sentences in a document are identified and
concatenated to form a condensed version of the
original document. Extractive summarization has
been popular at least in part because it is a binary
classification task that lends itself well to machine
learning techniques, and does not require a natural
language generation (NLG) component. There is ev-
idence that human abstractors at times use sentences
from the source documents nearly verbatim in their
own summaries, justifying this approach to some ex-
tent (Kupiec et al., 1995). Extrinsic evaluations have
also shown that, while extractive summaries may be
less coherent than human abstracts, users still find
them to be valuable tools for browsing documents
(He et al., 1999; McKeown et al., 2005; Murray et
al., 2008).
However, these same evaluations also indicate
that concise abstracts are generally preferred by
users and lead to higher objective task scores. The
limitation of a cut-and-paste summary is that the
end-user does not know why the selected sentences
are important; this can often only be discerned by
exploring the context in which each sentence origi-
nally appeared. One possible improvement is to cre-
ate structured extracts that represent an increased
level of abstraction, where selected sentences are
grouped according to phenomena such as decisions,
action items and problems, thereby giving the user
more information on why the sentences are being
highlighted. For example, the sentence Let’s go with
a simple chip represents a decision. An even higher
level of abstraction can be provided by generating
new text that synthesizes or extrapolates on the in-
formation contained in the structured summary. For
example, the sentence Sandra and Sue expressed
negative opinions about the remote control design
can be coupled with extracted sentences containing
these negative opinions, forming a hybrid summary.
Our summarization system ultimately performs both
types of abstraction, grouping sentences according
to various sentence-level phenomena, and generat-
ing novel text that describes this content at a higher
level.
In this work we describe the first two components
of our abstractive summarization system. In the in-
terpretation stage, sentences are mapped to nodes
in a conversation ontology by utilizing classifiers
relating to a variety of sentence-level phenomena
such as decisions, action items and subjective sen-
tences. These classifiers achieve high accuracy by
using a very large feature set integrating conversa-
tion structure, lexical patterns, part-of-speech (POS)
tags and character n-grams. In the transformation
stage, we select the most informative sentences by
maximizing a function based on the derived ontol-
ogy mapping and the coverage of weighted enti-
ties mentioned in the conversation. This transforma-
tion component utilizes integer linear programming
(ILP) and we compare its performance with several
greedy selection algorithms.
We do not discuss the generation compo-
nent of our summarization system in this pa-
per. The transformation component is still ex-
</bodyText>
<page confidence="0.980917">
894
</page>
<note confidence="0.752495">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 894–902,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940730769231">
tractive in nature, but the sentences that are se-
lected in the transformation stage correspond to
objects in the ontology and the properties link-
ing them. Specifically, these are triples of the
form &lt; participant, relation, entity &gt; where a
participant is a person in the conversation, an
entity is an item under discussion, and a relation
such as positive opinion or action item links the two.
This intermediate output enables us to create struc-
tured extracts as described above, with the triples
also acting as input to the downstream NLG com-
ponent.
We have tested our approach in summarization
experiments on both meeting and email conversa-
tions, where the quality of a sentence is measured
by how effectively it conveys information in a model
abstract summary according to human annotators.
On meetings the ILP approach consistently outper-
forms several greedy summarization methods. A
key finding is that emails exhibit markedly varying
conversation structures, and the email threads yield-
ing the best summarization results are those that are
structured similarly to meetings. Other email con-
versation structures are less amenable to the current
treatment and require further investigation and pos-
sibly domain adaptation.
</bodyText>
<sectionHeader confidence="0.99694" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.999968456521739">
The view that summarization consists of stages of
interpretation, transformation and generation was
laid out by Sparck-Jones (1999). Popular ap-
proaches to text extraction essentially collapse inter-
pretation and transformation into one step, with gen-
eration either being ignored or consisting of post-
processing techniques such as sentence compres-
sion (Knight and Marcu, 2000; Clarke and Lapata,
2006) or sentence merging (Barzilay and McKeown,
2005). In contrast, in this work we clearly separate
interpretation from transformation.
The most relevant research to ours is by Klein-
bauer et al. (2007), similarly focused on meet-
ing abstraction. They create an ontology for the
AMI scenario meeting corpus (Carletta et al., 2005),
described in Section 5.1. The system uses topic
segments and topic labels, and for each topic seg-
ment in the meeting a sentence is generated that de-
scribes the most frequently mentioned content items
in that topic. Our systems differ in two major re-
spects: their summarization process uses human
gold-standard annotations of topic segments, topic
labels and content items from the ontology, while
our summarizer is fully automatic; and the ontology
used by Kleinbauer et al. is specific not just to meet-
ings but to the AMI scenario meetings, while our
ontology applies to conversations in general.
While the work by Kleinbauer et al. is among
the earliest research on abstracting multi-party dia-
logues, much attention in recent years has been paid
to extractive summarization of such conversations,
including meetings (Galley, 2006), emails (Rambow
et al., 2004; Carenini et al., 2007), telephone con-
versations (Zhu and Penn, 2006) and internet relay
chats (Zhou and Hovy, 2005).
Recent research has addressed the challenges of
detecting decisions (Hsueh et al., 2007), action items
(Purver et al., 2007; Murray and Renals, 2008) and
subjective sentences (Raaijmakers et al., 2008). In
our work we perform all of these tasks but rely on
general conversational features without recourse to
meeting-specific or email-specific features.
Our approach to transformation is an adaptation
of an ILP sentence selection algorithm described by
Xie et al. (2009). We describe both ILP approaches
in Section 4.
</bodyText>
<sectionHeader confidence="0.996359" genericHeader="method">
3 Interpretation - Ontology Mapping
</sectionHeader>
<bodyText confidence="0.998908076923077">
Source document interpretation in our system re-
lies on a simple conversation ontology. The ontol-
ogy is written in OWL/RDF and contains two core
upper-level classes: Participant and Entity. When
additional information is available about participant
roles in a given domain, Participant subclasses such
as ProjectManager can be utilized. The ontology
also contains six properties that express relations be-
tween the participants and the entities. For example,
the following snippet of the ontology indicates that
hasActionItem is a relationship between a meeting
participant (the property domain) and a discussed
entity (the property range).
</bodyText>
<equation confidence="0.5491">
&lt;owl:ObjectProperty rdf:ID=&amp;quot;hasActionItem&amp;quot;&gt;
&lt;rdfs:domain rdf:resource=&amp;quot;#Participant&amp;quot;/&gt;
&lt;rdfs:range rdf:resource=&amp;quot;#Entity&amp;quot;/&gt;
&lt;/owl:ObjectProperty&gt;
</equation>
<bodyText confidence="0.9313305">
Similar properties exist for decisions, actions,
problems, positive-subjective sentences, negative-
</bodyText>
<page confidence="0.998126">
895
</page>
<bodyText confidence="0.997382487804879">
subjective sentences and general extractive sen-
tences (important sentences that may not match the
other categories), all connecting conversation par-
ticipants and entities. The goal is to populate the
ontology with participant and entity instances from
a given conversation and determine their relation-
ships. This involves identifying the important en-
tities and classifying the sentences in which they
occur as being decision sentences, action item sen-
tences, etc.
Our current definition of entity is simple. The en-
tities in a conversation are noun phrases with mid-
range document frequency. This is similar to the
definition of concept as defined by Xie et al. (Xie
et al., 2009), where n-grams are weighted by tf.idf
scores, except that we use noun phrases rather than
any n-grams because we want to refer to the enti-
ties in the generated text. We use mid-range doc-
ument frequency instead of idf (Church and Gale,
1995), where the entities occur in between 10% and
90% of the documents in the collection. In Section 4
we describe how we use the entity’s term frequency
to detect the most informative entities. We do not
currently attempt coreference resolution for entities;
recent work has investigated coreference resolution
for multi-party dialogues (Muller, 2007; Gupta et
al., 2007), but the challenge of resolution on such
noisy data is highlighted by low accuracy (e.g. F-
measure of 21.21) compared with using well-formed
text (e.g. monologues).
We map sentences to our ontology’s object prop-
erties by building numerous supervised classifiers
trained on labeled decision sentences, action sen-
tences, etc. A general extractive classifier is also
trained on sentences simply labeled as important.
After predicting these sentence-level properties, we
consider a participant to be linked to an entity if
the participant mentioned the entity in a sentence in
which one of these properties is predicted. We give a
specific example of the ontology mapping using this
excerpt from the AMI corpus:
</bodyText>
<listItem confidence="0.981716571428571">
1. A: And you two are going to work together on
a prototype using modelling clay.
2. A: You’ll get specific instructions from your
personal coach.
3. C: Cool.
4. A: Um did we decide on a chip?
5. A: Let’s go with a simple chip.
</listItem>
<bodyText confidence="0.997432125">
Example entities are italicized. Sentences 1 and
2 are classified as action items. Sentence 3 is clas-
sified as positive-subjective, but because it contains
no entities, no &lt; participant, relation, entity &gt;
triple can be added to the ontology. Sentence
4 is classified as a decision sentence, and Sen-
tence 5 is both a decision sentence and a positive-
subjective sentence (because the participant is advo-
cating a particular position). The ontology is pop-
ulated by adding all of the sentence entities as in-
stances of the Entity class, all of the participants
as instances of the Participant class, and adding
&lt; participant, relation, entity &gt; triples for Sen-
tences 1, 2, 4 and 5. For example, Sentence 5 results
in the following two triples being added to the on-
tology:
</bodyText>
<figure confidence="0.684620333333333">
&lt;ProjectManager rdf:ID=&amp;quot;participant-A&amp;quot;&gt;
&lt;hasDecision rdf:resource=&amp;quot;#simple-chip&amp;quot;/&gt;
&lt;/ProjectManager&gt;
&lt;ProjectManager rdf:ID=&amp;quot;participant-A&amp;quot;&gt;
&lt;hasPos rdf:resource=&amp;quot;#simple-chip&amp;quot;/&gt;
&lt;/ProjectManager&gt;
</figure>
<bodyText confidence="0.999916166666667">
Elements in the ontology are associated with lin-
guistic annotations used by the generation compo-
nent of our system; since we do not discuss the gen-
eration task here, we presently skip the details of this
aspect of the ontology. In the following section we
describe the features used for the ontology mapping.
</bodyText>
<subsectionHeader confidence="0.998971">
3.1 Feature Set
</subsectionHeader>
<bodyText confidence="0.999859888888889">
The interpretation component uses general features
that are applicable to any conversation domain. The
first set of features we use for ontology mapping are
features relating to conversational structure. These
are listed and briefly described in Table 1. The
5prob and Tprob features measure how terms clus-
ter between conversation participants and conver-
sation turns. There are simple features measur-
ing sentence length (SLEN, SLEN2) and position
(TLOC, CLOC). Pause-style features indicate how
much time transpires between the previous turn, the
current turn and the subsequent turn (PPAU, SPAU).
For email conversations, pause features are based on
the timestamps between consecutive emails. Lexical
features capture cohesion (CWS) and cosine sim-
ilarity between the sentence and the conversation
(CENT1, CENT2). All structural features are nor-
malized by document length.
</bodyText>
<page confidence="0.996971">
896
</page>
<table confidence="0.785424571428571">
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
</table>
<tableCaption confidence="0.985167833333333">
COS1 cos. of conv. splits, w/ Sprob
COS2 cos. of conv. splits, w/ Tprob
PENT entro. of conv. up to sentence
SENT entro. of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore
CENT1 cos. of sentence &amp; conv., w/ Sprob
CENT2 cos. of sentence &amp; conv., w/ Tprob
Table 1: Features Key
</tableCaption>
<bodyText confidence="0.97554125">
While these features have been found to work
well for generic extractive summarization, we use
additional features for capturing the more specific
sentence-level phenomena of this research.
</bodyText>
<listItem confidence="0.988717318181818">
• Character trigrams We derive all of the char-
acter trigrams in the collected corpora and in-
clude features indicating the presence or ab-
sence of each trigram in a given sentence.
• Word bigrams We similarly derive all of the
word bigrams in the collected corpora.
• POS bigrams We similarly derive all of the
POS-tag bigrams in the collected corpora.
• Word pairs We consider w1, w2 to be a word
pair if they occur in the same sentence and w1
precedes w2. We derive all of the word pairs
in the collected corpora and includes features
indicating the presence or absence of each word
pair in the given sentence. This is essentially a
skip bigram where any amount of intervening
material is allowed as long as the words occur
in the same sentence.
• POS pairs We calculate POS pairs in the same
manner as word pairs, above. These are essen-
tially skip bigrams for POS tags.
• Varying instantiation ngrams We derive a
simplified set of VIN features for these exper-
</listItem>
<bodyText confidence="0.99944675">
iments. For each word bigram w1, w2, we fur-
ther represent the bigram as p1, w2 and w1, p2
so that each pattern consists of a word and a
POS tag. We include a feature indicating the
presence or absence of each of these varying
instantiation bigrams.
After removing features that occur fewer than five
times, we end up with 218,957 total features.
</bodyText>
<sectionHeader confidence="0.980297" genericHeader="method">
4 Transformation - ILP Content Selection
</sectionHeader>
<bodyText confidence="0.999478076923077">
In the previous section we described how we
identify sentences that link participants and enti-
ties through a variety of sentence-level phenom-
ena. Having populated our ontology with these
triples to form a source representation, we now turn
to the task of transforming the source representa-
tion to a summary representation, identifying the &lt;
participant, relation, entity &gt; triples for which
we want to generate text. We adapt a method pro-
posed by Xie et al. (2009) for extractive sentence
selection. They propose an ILP approach that cre-
ates a summary by maximizing a global objective
function:
</bodyText>
<equation confidence="0.832152">
�maximize (1 − A) � �wici + A � ujsj (1)
i j
�subject to ljsj &lt; L (2)
</equation>
<bodyText confidence="0.916840722222222">
j
where wi is the tf.idf score for concept i, uj is the
weight for sentence j using the cosine similarity to
the entire document, ci is a binary variable indicat-
ing whether concept i is selected (with the concept
represented by a unique weighted n-gram), sj is a
binary variable indicating whether sentence j is se-
lected, lj is the length of sentence j and L is the
desired summary length. The A term is used to bal-
ance concept and sentence weights. This method se-
lects sentences that are weighted strongly and which
cover as many important concepts as possible. As
described by Gillick et al. (2009), concepts and
sentences are tied together by two additional con-
straints:
� sjoij ? ci bi (3)
j
sjoij :5 ci bi,j (4)
</bodyText>
<page confidence="0.988397">
897
</page>
<bodyText confidence="0.999971181818182">
where ozj is the occurence of concept i in sentence
j. These constraints state that a concept can only be
selected if it occurs in a sentence that is selected,
and that a sentence can only be selected if all of its
concepts have been selected.
We adapt their method in several ways. As men-
tioned in the previous section, we use weighted noun
phrases as our entities instead of n-grams. In our
version of Equation 1, wz is the tf score of en-
tity i (the idf was already used to identify entities
as described previously). More importantly, our
sentence weight uj is the sum of all the posterior
probabilities for sentence j derived from the various
sentence-level classifiers. In other words, sentences
are weighted highly if they correspond to multiple
object properties in the ontology. To continue the
example from Section 3, the sentence Let’s go with
the simple chip may be selected because it represents
both a decision and a positive-subjective opinion, as
well as containing the entity simple chip which is
mentioned frequently in the conversation.
We include constraint 3 but not 4; it is possi-
ble for a sentence to be extracted even if not all
of its entities are. We know that all the sentences
under consideration will contain at least one en-
tity because sentences with no entities would not
have been mapped to the ontology in the form of
&lt; participant, relation, entity &gt; triples in the
first place. To begin with, we set the A term at 0.75
as we are mostly concerned with identifying impor-
tant sentences containing multiple links to the on-
tology. In our case L is 20% of the total document
word count.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999658666666667">
In this section we describe our conversation cor-
pora, the statistical classifiers used, and the evalu-
ation metrics employed.
</bodyText>
<subsectionHeader confidence="0.921937">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.9995415">
These experiments are conducted on both meeting
and email conversations, which we describe in turn.
</bodyText>
<subsubsectionHeader confidence="0.906952">
5.1.1 The AMI Meetings Corpus
</subsubsectionHeader>
<bodyText confidence="0.999982085714285">
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Carletta
et al., 2005), where groups of four participants take
part in a series of four meetings and play roles within
a fictitious company. There are 140 of these meet-
ings in total, including a 20 meeting test set contain-
ing multiple human summary annotations per meet-
ing (the others are annotated by a single individual).
We report results on both manual and ASR tran-
scripts. The word error rate for the ASR transcripts
is 38.9%.
For the summary annotation, annotators wrote ab-
stract summaries of each meeting and extracted sen-
tences that best conveyed or supported the informa-
tion in the abstracts. The human-authored abstracts
each contain a general abstract summary and three
subsections for “decisions,” “actions” and “prob-
lems” from the meeting. A many-to-many mapping
between transcript sentences and sentences from the
human abstract was obtained for each annotator. Ap-
proximately 13% of the total transcript sentences are
ultimately labeled as extracted sentences. A sen-
tence is considered a decision item if it is linked to
the decision portion of the abstract, and action and
problem sentences are derived similarly.
For the subjectivity annotation, we use annota-
tions of positive-subjective and negative-subjective
utterances on a subset of 20 AMI meetings (Wil-
son, 2008). Such subjective utterances involve
the expression of a private state, such as a pos-
itive/negative opinion, positive/negative argument,
and agreement/disagreement. Of the roughly 20,000
total sentences in the 20 AMI meetings, nearly 4000
are labeled as positive-subjective and nearly 1300 as
negative-subjective.
</bodyText>
<subsubsectionHeader confidence="0.744491">
5.1.2 The BC3 Email Corpus
</subsubsectionHeader>
<bodyText confidence="0.999966133333333">
While our main experiments focus on the AMI
meeting corpus, we follow these up with an inves-
tigation into applying our abstractive techniques to
email data. The BC3 corpus (Ulrich et al., 2008)
contains email threads from the World Wide Web
Consortium (W3C) mailing list. The threads fea-
ture a variety of topics such as web accessibility and
planning face-to-face meetings. The annotated por-
tion of the mailing list consists of 40 threads. The
threads are annotated in the same manner as the AMI
corpus, with three human annotators per thread first
authoring abstracts and then linking email thread
sentences to the abstract sentences. The corpus also
contains speech act annotations. Unlike the AMI
corpus, however, there are no annotations for deci-
</bodyText>
<page confidence="0.99682">
898
</page>
<bodyText confidence="0.994344">
sions, actions and problems, an issue addressed later.
</bodyText>
<subsectionHeader confidence="0.986535">
5.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.999994833333333">
For these experiments we use a maximum entropy
classifier using the liblinear toolkit1 (Fan et al.,
2008). For each of the AMI and BC3 corpora, we
perform 10-fold cross-validation on the data. In all
experiments we apply a 20% compression rate in
terms of the total document word count.
</bodyText>
<subsectionHeader confidence="0.980268">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.99944225">
We evaluate the various classifiers described in Sec-
tion 3 using the ROC curve and the area under the
curve (AUROC), where a baseline AUROC is 0.5
and an ideal classifier approaches 1.
To evaluate the content selection in the transfor-
mation stage, we use weighted recall.This evaluation
metric is based on the links between extracted sen-
tences and the human gold-standard abstracts, with
the underlying motivation being that sentences with
more links to the human abstract are generally more
informative, as they provide the content on which an
effective abstract summary should be built. If M is
the number of sentences selected in the transforma-
tion step, O is the total number of sentences in the
document, and N is the number of annotators, then
Weighted Recall is given by
</bodyText>
<equation confidence="0.89543325">
M N
recall = Ei=1 Ej=1 L(si, aj )
O N
Ei=1 Ej=1 L(si, aj )
</equation>
<bodyText confidence="0.9999955">
where L(si, aj) is the number of links for a sen-
tence si according to annotator aj. We can com-
pare machine performance with human performance
in the following way. For each annotator, we rank
their sentences from most-linked to least-linked and
select the best sentences until we reach the same
word count as our selections. We then calculate their
weighted recall score by using the other N-1 annota-
tions, and then average over all N annotators to get
an average human performance. We report all trans-
formation scores normalized by human performance
for that dataset.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.9714695">
In this section we present results for our interpreta-
tion and transformation components.
</bodyText>
<footnote confidence="0.996341">
1http://www.csie.ntu.edu.tw/ cjlin/liblinear/
</footnote>
<subsectionHeader confidence="0.900605">
6.1 Interpretation: Meetings
</subsectionHeader>
<bodyText confidence="0.999873047619048">
Figure 1 shows the ROC curves for the sentence-
level classifiers applied to manual transcripts. On
both manual and ASR transcripts, the classifiers
with the largest AUROCs are the action item and
general extractive classifiers. Action item sentences
can be detected very well with this feature set, with
the classifier having an AUROC of 0.92 on man-
ual transcripts and 0.93 on ASR, a result compa-
rable to previous findings of 0.91 and 0.93 (Mur-
ray and Renals, 2008) obtained using a speech-
specific feature set. General extractive classification
is also similar to other state-of-the-art extraction ap-
proaches on spoken data using speech features (Zhu
and Penn, 2006)2 with an AUROC of 0.87 on man-
ual and 0.85 on ASR. Decision sentences can also
be detected quite well, with AUROCs of 0.81 and
0.77. Positive-subjective, negative-subjective and
problem sentences are the most difficult to detect,
but the classifiers still give credible performance
with AUROCs of approximately 0.76 for manual
and 0.70-0.72 for ASR.
</bodyText>
<figure confidence="0.8519625">
0 0.2 0.4 0.6 0.8 1
FP
</figure>
<figureCaption confidence="0.9607835">
Figure 1: ROC Curves for Ontology Mapping Classifiers
(Manual Transcripts)
</figureCaption>
<subsectionHeader confidence="0.996754">
6.2 Transformation: Meetings
</subsectionHeader>
<bodyText confidence="0.999919428571429">
In this section we present the weighted recall scores
for the sentences selected using the ILP method de-
scribed in Section 4. Remember, weighted recall
measures how useful these sentences would be in
generating sentences for an abstract summary. We
also assess the performance of three baseline sum-
marizers operating at the same compression level.
</bodyText>
<footnote confidence="0.743486">
2Based on visual inspection of their reported best ROC curve
</footnote>
<figure confidence="0.8106495">
TP
0.8
0.6
0.4
0.2
0
1
actions
decisions
problems
positive-subjective
negative-subjective
extractive
random
</figure>
<page confidence="0.99743">
899
</page>
<bodyText confidence="0.998602842105263">
The simplest baseline (GREEDY) selects sentences
by ranking the posterior probabilites output by the
general extractive classifier. The second baseline
(CLASS COMBO) averages the posterior proba-
bilites output by all the classifiers and ranks sen-
tences from best to worst. The third baseline (RE-
TRAIN) uses the posterior probability outputs of all
the classifiers (except for the extractive classifier) as
new feature inputs for the general extractive classi-
fier.
ROC curves displayed in Figure 3. Both the general
extractive and negative-subjective classifiers have
AUROCs of around 0.75. The positive-subjective
classifier initially has the worst performance with
an AUROC of 0.66, but we found that positive-
subjective performance increased dramatically to an
AUROC of 0.77 when we used only conversational
features and not word bigrams, character trigrams or
POS tags.
</bodyText>
<figure confidence="0.997732909090909">
Weighted Recall, Normalized
0.9
0.8
0.7
0.6
0.5
1
manual
ASR
TP
0.8
0.6
0.4
0.2
0
1
positive-subjective
negative-subjective
extractive
random
0 0.2 0.4 0.6 0.8 1
FP
</figure>
<figureCaption confidence="0.999643">
Figure 2: Weighted Recall Scores for AMI Meetings
</figureCaption>
<bodyText confidence="0.960974692307692">
Figure 2 shows the weighted recall scores, nor-
malized by human performance, for all approaches
on both manual and ASR transcripts. On man-
ual transcripts, the ILP approach (0.76) is better
than GREEDY (0.71) with a marginally significant
difference (p=0.07) and is significantly better than
CLASS COMBO and RETRAIN (both 0.68) ac-
cording to t-test (p &lt; 0.05) . For ASR transcripts,
the ILP approach is significantly better than all other
approaches (p &lt; 0.05). Xie et al. (2009) reported
ROUGE-1 F-measures on a different meeting cor-
pus, and our ROUGE-1 scores are in the same range
of 0.64-0.69 (they used 18% compression ratio).
</bodyText>
<subsectionHeader confidence="0.991978">
6.3 Interpretation: Emails
</subsectionHeader>
<bodyText confidence="0.9999815">
We applied the same summarization method to the
40 BC3 email threads, with contrasting results. Be-
cause the BC3 corpus does not currently contain an-
notations for decisions, actions and problems, we
simply ran the AMI-trained models over the data
for those three phenomena. We can assess the
performance of the extractive, positive-subjective
and negative-subjective classifiers by examining the
</bodyText>
<figureCaption confidence="0.9116965">
Figure 3: ROC Curves for Ontology Mapping Classifiers
(BC3 Corpus)
</figureCaption>
<subsectionHeader confidence="0.986433">
6.4 Transformation: Emails
</subsectionHeader>
<bodyText confidence="0.999910727272727">
If we examine the weighted recall scores in Fig-
ure 4 we see that the ILP approach is worse than
the greedy summarizers on the BC3 dataset. How-
ever, the differences are not significant between ILP
and COMBO CLASS (p=0.15) and only marginally
significant compared with RETRAIN and GREEDY
(both p=0.08). The performance of the ILP approach
varies greatly across email threads. The top 15
threads (out of 40) yield ILP weighted recall scores
that are on par with human performance, while the
worst 15 are half that.
</bodyText>
<sectionHeader confidence="0.534056" genericHeader="method">
6.4.1 Email Corpus Analysis
</sectionHeader>
<bodyText confidence="0.99987775">
Due to the large discrepancy in performance on
BC3 emails, we conducted additional experiments
for error analysis. We first explored whether we
could build a classifier that could discriminate the
best 15 emails from the worst 15 emails in terms of
weighted recall scores with the ILP approach, to de-
termine whether there are certain features that cor-
relate with good performance. Using the same fea-
</bodyText>
<page confidence="0.991264">
900
</page>
<figureCaption confidence="0.999799">
Figure 4: Weighted Recall Scores for BC3 Threads
</figureCaption>
<bodyText confidence="0.999930928571429">
tures described in Section 3.1, we built a logistic re-
gression classifier on the two classes and found that
they can be discriminated quite well (80% accuracy
on an approximately balanced dataset) and that the
conversation structure features are the most useful
for discerning them. Table 2 shows the weighted
recall scores and several conversation features that
were weighted most highly by the logistic regres-
sion model. In particular, we found that the email
threads that yielded good performance tended to fea-
ture more active participants (# Participants), were
not dominated by a single individual (BEGAUTH),
and featured a higher number of turns (# Turns)
that followed each other in quick succession without
long pauses (PPAU, pause as percentage of conver-
sation length). In other words, these emails were
structured more similarly to meetings. Note that
since we normalize weighted recall by human per-
formance, it is possible to have a weighted recall
score higher than 1. On the 15 best threads, our sys-
tem achieves human-level performance. Because we
used AMI-trained models for detecting decisions,
actions and problems in the BC3 data, it is not sur-
prising that performance was better on those emails
structured similarly to meetings. All of this indicates
that there are many different types of emails and that
we will have to focus on improving performance on
emails that differ markedly in structure.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999214666666667">
We have presented two components of an abstractive
conversation summarization system. The interpreta-
tion component is used to populate a simple conver-
</bodyText>
<table confidence="0.999506833333333">
Metric Worst 15 Best 15
Weighted Recall 0.49 1.05
# Turns 6.27 6.73
# Participants 4.67 5.4
PPAU 0.18 0.12
BEGAUTH 0.31 0.18
</table>
<tableCaption confidence="0.999392">
Table 2: Selected Email Features, Averaged
</tableCaption>
<bodyText confidence="0.999774394736842">
sation ontology where conversation participants and
entities are linked by object properties such as deci-
sions, actions and subjective opinions. For this step
we show that highly accurate classifiers can be built
using a large set of features not specific to any con-
versation modality.
In the transformation step, a summary is cre-
ated by maximizing a function relating sentence
weights and entity weights, with the sentence
weights determined by the sentence-ontology map-
ping. Our evaluation shows that the sentences we
select are highly informative to generate abstract
summaries, and that our content selection method
outperforms several greedy selection approaches.
The system described thus far may appear extrac-
tive in nature, as the transformation step is iden-
tifying informative sentences in the conversation.
However, these selected sentences correspond to
&lt; participant, relation, entity &gt; triples in the
ontology, for which we can subsequently gener-
ate novel text by creating linguistic annotations of
the conversation ontology (Galanis and Androut-
sopolous, 2007). Even without the generation step,
the approach described above allows us to create
structured extracts by grouping sentences according
to specific phenomena such as action items and de-
cisions. The knowledge represented by the ontology
enables us to significantly improve sentence selec-
tion according to intrinsic measures and to generate
structured output that we hypothesize will be more
useful to an end user compared with a generic un-
structured extract.
Future work will focus on the generation compo-
nent and on applying the summarization system to
conversations in other modalities such as blogs and
instant messages. Based on the email error analysis,
we plan to pursue domain adaptation techniques to
improve performance on different types of emails.
</bodyText>
<figure confidence="0.993410333333333">
Weighted Recall, Normalized 1
0.9
0.8
0.7
0.6
0.5
</figure>
<page confidence="0.989935">
901
</page>
<sectionHeader confidence="0.989358" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938171428572">
R. Barzilay and K. McKeown. 2005. Sentence fusion for
multidocument news summarization. Computational
Linguistics, 31(3):297–328.
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proc. ofACM
WWW 07, Banff, Canada.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Well-
ner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28–39.
K. Church and W. Gale. 1995. Inverse document fre-
quency IDF: A measure of deviation from poisson. In
Proc. of the Third Workshop on Very Large Corpora,
pages 121–130.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proc. of COLING/ACL 2006, pages 144–
151.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large linear
classification. Journal ofMachine Learning Research,
9:1871–1874.
D. Galanis and I. Androutsopolous. 2007. Generating
multilingual descriptions from linguistically annotated
owl ontologies: the naturalowl system. In Proc. of
ENLG 2007, Schloss Dagstuhl, Germany.
M. Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP 2006, Sydney, Australia, pages 364–
372.
D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-
T¨ur. 2009. A global optimization framework for meet-
ing summarization. In Proc. of ICASSP 2009, Taipei,
Taiwan.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky. 2007.
Resolving ”You” in multi-party dialog. In Proc. of
SIGdial2007, Antwerp, Belgium.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-
summarization of audio-video presentations. In Proc.
ofACMMULTIMEDIA ’99, Orlando, FL, USA, pages
489–498.
P-Y. Hsueh, J. Kilgour, J. Carletta, J. Moore, and S. Re-
nals. 2007. Automatic decision detection in meeting
speech. In Proc. of MLMI 2007, Brno, Czech Repub-
lic.
K. Sp¨arck Jones. 1999. Automatic summarizing: Factors
and directions. In I. Mani and M. Maybury, editors,
Advances in Automatic Text Summarization, pages 1–
12. MITP.
T. Kleinbauer, S. Becker, and T. Becker. 2007. Com-
bining multiple information layers for the automatic
generation of indicative meeting abstracts. In Proc. of
ENLG 2007, Dagstuhl, Germany.
K. Knight and D. Marcu. 2000. Statistics-based summa-
rization - step one: Sentence compression. In Proc. of
AAAI2000, Austin, Texas, USA, pages 703–710.
J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable
document summarizer. In Proc. of the 18th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval. Seattle, Wash-
ington, USA, pages 68–73.
K. McKeown, J. Hirschberg, M. Galley, and S. Maskey.
2005. From text to speech summarization. In Proc. of
ICASSP 2005, Philadelphia, USA, pages 997–1000.
C. Muller. 2007. Resolving It, This and That in un-
restricted multi-party dialog. In Proc. of ACL 2007,
Prague, Czech Republic.
G. Murray and S. Renals. 2008. Detecting action items
in meetings. In Proc. of MLMI 2008, Utrecht, the
Netherlands.
G. Murray, T. Kleinbauer, P. Poller, S. Renals, T. Becker,
and J. Kilgour. 2008. Extrinsic summarization evalu-
ation: A decision audit task. In Proc. of MLMI 2008,
Utrecht, the Netherlands.
M. Purver, J. Dowding, J. Niekrasz, P. Ehlen, and
S. Noorbaloochi. 2007. Detecting and summariz-
ing action items in multi-party dialogue. In Proc. of
the 9th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Multi-
modal subjectivity analysis of multiparty conversation.
In Proc. ofEMNLP 2008, Honolulu, HI, USA.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proc. of HLT-
NAACL 2004, Boston, USA.
J. Ulrich, G. Murray, and G. Carenini. 2008. A publicly
available annotated corpus for supervised email sum-
marization. In Proc. ofAAAI EMAIL-2008 Workshop,
Chicago, USA.
T. Wilson. 2008. Annotating subjective content in meet-
ings. In Proc. ofLREC 2008, Marrakech, Morocco.
S. Xie, B. Favre, D. Hakkani-T¨ur, and Y. Liu. 2009.
Leveraging sentence weights in a concept-based op-
timization framework for extractive meeting summa-
rization. In Proc. ofInterspeech 2009, Brighton, Eng-
land.
L. Zhou and E. Hovy. 2005. Digesting virtual ”geek”
culture: The summarization of technical internet relay
chats. In Proc. ofACL 2005, Ann Arbor, MI, USA.
X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proc. of Interspeech 2006,
Pittsburgh, USA, pages 1531–1534.
</reference>
<page confidence="0.99765">
902
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.676341">
<title confidence="0.999922">Interpretation and Transformation for Abstracting Conversations</title>
<author confidence="0.999228">Gabriel Murray Giuseppe Carenini Raymond Ng</author>
<email confidence="0.838541">gabrielm@cs.ubc.cacarenini@cs.ubc.carng@cs.ubc.ca</email>
<affiliation confidence="0.999829">Department of Computer Science, University of British Columbia</affiliation>
<address confidence="0.979457">Vancouver, Canada</address>
<abstract confidence="0.983313181818182">We address the challenge of automatically abstracting conversations such as face-to-face meetings and emails. We focus here on stages of where sentences are mapped to a conversation ontology, and where the summary content is selected. Our approach is fully developed and tested on meeting speech, and we subsequently explore its application to email conversations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="5715" citStr="Barzilay and McKeown, 2005" startWordPosition="853" endWordPosition="856">y to meetings. Other email conversation structures are less amenable to the current treatment and require further investigation and possibly domain adaptation. 2 Related Research The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), described in Section 5.1. The system uses topic segments and topic labels, and for each topic segment in the meeting a sentence is generated that describes the most frequently mentioned content items in that topic. Our systems differ in two major respects: their summarization process uses human gold-standard annotations </context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>R. Barzilay and K. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>R Ng</author>
<author>X Zhou</author>
</authors>
<title>Summarizing email conversations with clue words.</title>
<date>2007</date>
<booktitle>In Proc. ofACM WWW 07,</booktitle>
<location>Banff, Canada.</location>
<contexts>
<context position="6878" citStr="Carenini et al., 2007" startWordPosition="1039" endWordPosition="1042">summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches </context>
</contexts>
<marker>Carenini, Ng, Zhou, 2007</marker>
<rawString>G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing email conversations with clue words. In Proc. ofACM WWW 07, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>W Kraaij</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>I McCowan</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI meeting corpus: A preannouncement.</title>
<date>2005</date>
<booktitle>In Proc. of MLMI 2005,</booktitle>
<pages>28--39</pages>
<location>Edinburgh, UK,</location>
<contexts>
<context position="5991" citStr="Carletta et al., 2005" startWordPosition="897" endWordPosition="900">id out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), described in Section 5.1. The system uses topic segments and topic labels, and for each topic segment in the meeting a sentence is generated that describes the most frequently mentioned content items in that topic. Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. </context>
<context position="18547" citStr="Carletta et al., 2005" startWordPosition="2949" endWordPosition="2952">n the first place. To begin with, we set the A term at 0.75 as we are mostly concerned with identifying important sentences containing multiple links to the ontology. In our case L is 20% of the total document word count. 5 Experimental Setup In this section we describe our conversation corpora, the statistical classifiers used, and the evaluation metrics employed. 5.1 Corpora These experiments are conducted on both meeting and email conversations, which we describe in turn. 5.1.1 The AMI Meetings Corpus For our meeting summarization experiments, we use the scenario portion of the AMI corpus (Carletta et al., 2005), where groups of four participants take part in a series of four meetings and play roles within a fictitious company. There are 140 of these meetings in total, including a 20 meeting test set containing multiple human summary annotations per meeting (the others are annotated by a single individual). We report results on both manual and ASR transcripts. The word error rate for the ASR transcripts is 38.9%. For the summary annotation, annotators wrote abstract summaries of each meeting and extracted sentences that best conveyed or supported the information in the abstracts. The human-authored a</context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kraaij, Kronenthal, Lathoud, Lincoln, Lisowska, McCowan, Post, Reidsma, Wellner, 2005</marker>
<rawString>J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI meeting corpus: A preannouncement. In Proc. of MLMI 2005, Edinburgh, UK, pages 28–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>Inverse document frequency IDF: A measure of deviation from poisson.</title>
<date>1995</date>
<booktitle>In Proc. of the Third Workshop on Very Large Corpora,</booktitle>
<pages>121--130</pages>
<contexts>
<context position="9334" citStr="Church and Gale, 1995" startWordPosition="1399" endWordPosition="1402">tionships. This involves identifying the important entities and classifying the sentences in which they occur as being decision sentences, action item sentences, etc. Our current definition of entity is simple. The entities in a conversation are noun phrases with midrange document frequency. This is similar to the definition of concept as defined by Xie et al. (Xie et al., 2009), where n-grams are weighted by tf.idf scores, except that we use noun phrases rather than any n-grams because we want to refer to the entities in the generated text. We use mid-range document frequency instead of idf (Church and Gale, 1995), where the entities occur in between 10% and 90% of the documents in the collection. In Section 4 we describe how we use the entity’s term frequency to detect the most informative entities. We do not currently attempt coreference resolution for entities; recent work has investigated coreference resolution for multi-party dialogues (Muller, 2007; Gupta et al., 2007), but the challenge of resolution on such noisy data is highlighted by low accuracy (e.g. Fmeasure of 21.21) compared with using well-formed text (e.g. monologues). We map sentences to our ontology’s object properties by building nu</context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>K. Church and W. Gale. 1995. Inverse document frequency IDF: A measure of deviation from poisson. In Proc. of the Third Workshop on Very Large Corpora, pages 121–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Constraint-based sentence compression: An integer programming approach.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<pages>144--151</pages>
<contexts>
<context position="5666" citStr="Clarke and Lapata, 2006" startWordPosition="846" endWordPosition="849">results are those that are structured similarly to meetings. Other email conversation structures are less amenable to the current treatment and require further investigation and possibly domain adaptation. 2 Related Research The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), described in Section 5.1. The system uses topic segments and topic labels, and for each topic segment in the meeting a sentence is generated that describes the most frequently mentioned content items in that topic. Our systems differ in two major respects: their summarizat</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>J. Clarke and M. Lapata. 2006. Constraint-based sentence compression: An integer programming approach. In Proc. of COLING/ACL 2006, pages 144– 151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="21080" citStr="Fan et al., 2008" startWordPosition="3347" endWordPosition="3350">uch as web accessibility and planning face-to-face meetings. The annotated portion of the mailing list consists of 40 threads. The threads are annotated in the same manner as the AMI corpus, with three human annotators per thread first authoring abstracts and then linking email thread sentences to the abstract sentences. The corpus also contains speech act annotations. Unlike the AMI corpus, however, there are no annotations for deci898 sions, actions and problems, an issue addressed later. 5.2 Classifiers For these experiments we use a maximum entropy classifier using the liblinear toolkit1 (Fan et al., 2008). For each of the AMI and BC3 corpora, we perform 10-fold cross-validation on the data. In all experiments we apply a 20% compression rate in terms of the total document word count. 5.3 Evaluation We evaluate the various classifiers described in Section 3 using the ROC curve and the area under the curve (AUROC), where a baseline AUROC is 0.5 and an ideal classifier approaches 1. To evaluate the content selection in the transformation stage, we use weighted recall.This evaluation metric is based on the links between extracted sentences and the human gold-standard abstracts, with the underlying </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. 2008. Liblinear: A library for large linear classification. Journal ofMachine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>I Androutsopolous</author>
</authors>
<title>Generating multilingual descriptions from linguistically annotated owl ontologies: the naturalowl system.</title>
<date>2007</date>
<booktitle>In Proc. of ENLG 2007, Schloss Dagstuhl,</booktitle>
<location>Germany.</location>
<contexts>
<context position="30563" citStr="Galanis and Androutsopolous, 2007" startWordPosition="4859" endWordPosition="4863">ned by the sentence-ontology mapping. Our evaluation shows that the sentences we select are highly informative to generate abstract summaries, and that our content selection method outperforms several greedy selection approaches. The system described thus far may appear extractive in nature, as the transformation step is identifying informative sentences in the conversation. However, these selected sentences correspond to &lt; participant, relation, entity &gt; triples in the ontology, for which we can subsequently generate novel text by creating linguistic annotations of the conversation ontology (Galanis and Androutsopolous, 2007). Even without the generation step, the approach described above allows us to create structured extracts by grouping sentences according to specific phenomena such as action items and decisions. The knowledge represented by the ontology enables us to significantly improve sentence selection according to intrinsic measures and to generate structured output that we hypothesize will be more useful to an end user compared with a generic unstructured extract. Future work will focus on the generation component and on applying the summarization system to conversations in other modalities such as blog</context>
</contexts>
<marker>Galanis, Androutsopolous, 2007</marker>
<rawString>D. Galanis and I. Androutsopolous. 2007. Generating multilingual descriptions from linguistically annotated owl ontologies: the naturalowl system. In Proc. of ENLG 2007, Schloss Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP 2006,</booktitle>
<pages>364--372</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6825" citStr="Galley, 2006" startWordPosition="1032" endWordPosition="1033">systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described b</context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>M. Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proc. of EMNLP 2006, Sydney, Australia, pages 364– 372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gillick</author>
<author>K Riedhammer</author>
<author>B Favre</author>
<author>D HakkaniT¨ur</author>
</authors>
<title>A global optimization framework for meeting summarization.</title>
<date>2009</date>
<booktitle>In Proc. of ICASSP 2009,</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Gillick, Riedhammer, Favre, HakkaniT¨ur, 2009</marker>
<rawString>D. Gillick, K. Riedhammer, B. Favre, and D. HakkaniT¨ur. 2009. A global optimization framework for meeting summarization. In Proc. of ICASSP 2009, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>J Niekrasz</author>
<author>M Purver</author>
<author>D Jurafsky</author>
</authors>
<title>Resolving ”You” in multi-party dialog.</title>
<date>2007</date>
<booktitle>In Proc. of SIGdial2007,</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="9702" citStr="Gupta et al., 2007" startWordPosition="1456" endWordPosition="1459">al., 2009), where n-grams are weighted by tf.idf scores, except that we use noun phrases rather than any n-grams because we want to refer to the entities in the generated text. We use mid-range document frequency instead of idf (Church and Gale, 1995), where the entities occur in between 10% and 90% of the documents in the collection. In Section 4 we describe how we use the entity’s term frequency to detect the most informative entities. We do not currently attempt coreference resolution for entities; recent work has investigated coreference resolution for multi-party dialogues (Muller, 2007; Gupta et al., 2007), but the challenge of resolution on such noisy data is highlighted by low accuracy (e.g. Fmeasure of 21.21) compared with using well-formed text (e.g. monologues). We map sentences to our ontology’s object properties by building numerous supervised classifiers trained on labeled decision sentences, action sentences, etc. A general extractive classifier is also trained on sentences simply labeled as important. After predicting these sentence-level properties, we consider a participant to be linked to an entity if the participant mentioned the entity in a sentence in which one of these properti</context>
</contexts>
<marker>Gupta, Niekrasz, Purver, Jurafsky, 2007</marker>
<rawString>S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky. 2007. Resolving ”You” in multi-party dialog. In Proc. of SIGdial2007, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L He</author>
<author>E Sanocki</author>
<author>A Gupta</author>
<author>J Grudin</author>
</authors>
<title>Autosummarization of audio-video presentations.</title>
<date>1999</date>
<booktitle>In Proc. ofACMMULTIMEDIA ’99,</booktitle>
<pages>489--498</pages>
<location>Orlando, FL, USA,</location>
<contexts>
<context position="1495" citStr="He et al., 1999" startWordPosition="214" endWordPosition="217"> Extractive summarization has been popular at least in part because it is a binary classification task that lends itself well to machine learning techniques, and does not require a natural language generation (NLG) component. There is evidence that human abstractors at times use sentences from the source documents nearly verbatim in their own summaries, justifying this approach to some extent (Kupiec et al., 1995). Extrinsic evaluations have also shown that, while extractive summaries may be less coherent than human abstracts, users still find them to be valuable tools for browsing documents (He et al., 1999; McKeown et al., 2005; Murray et al., 2008). However, these same evaluations also indicate that concise abstracts are generally preferred by users and lead to higher objective task scores. The limitation of a cut-and-paste summary is that the end-user does not know why the selected sentences are important; this can often only be discerned by exploring the context in which each sentence originally appeared. One possible improvement is to create structured extracts that represent an increased level of abstraction, where selected sentences are grouped according to phenomena such as decisions, ac</context>
</contexts>
<marker>He, Sanocki, Gupta, Grudin, 1999</marker>
<rawString>L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Autosummarization of audio-video presentations. In Proc. ofACMMULTIMEDIA ’99, Orlando, FL, USA, pages 489–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-Y Hsueh</author>
<author>J Kilgour</author>
<author>J Carletta</author>
<author>J Moore</author>
<author>S Renals</author>
</authors>
<title>Automatic decision detection in meeting speech.</title>
<date>2007</date>
<booktitle>In Proc. of MLMI 2007,</booktitle>
<location>Brno, Czech Republic.</location>
<contexts>
<context position="7061" citStr="Hsueh et al., 2007" startWordPosition="1067" endWordPosition="1070">used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping Source document interpretation in our system relies on a simple conversation ontology. The ontology is written in OWL/RDF and contain</context>
</contexts>
<marker>Hsueh, Kilgour, Carletta, Moore, Renals, 2007</marker>
<rawString>P-Y. Hsueh, J. Kilgour, J. Carletta, J. Moore, and S. Renals. 2007. Automatic decision detection in meeting speech. In Proc. of MLMI 2007, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sp¨arck Jones</author>
</authors>
<title>Automatic summarizing: Factors and directions.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>1--12</pages>
<editor>In I. Mani and M. Maybury, editors,</editor>
<publisher>MITP.</publisher>
<contexts>
<context position="5398" citStr="Jones (1999)" startWordPosition="808" endWordPosition="809">according to human annotators. On meetings the ILP approach consistently outperforms several greedy summarization methods. A key finding is that emails exhibit markedly varying conversation structures, and the email threads yielding the best summarization results are those that are structured similarly to meetings. Other email conversation structures are less amenable to the current treatment and require further investigation and possibly domain adaptation. 2 Related Research The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), descr</context>
</contexts>
<marker>Jones, 1999</marker>
<rawString>K. Sp¨arck Jones. 1999. Automatic summarizing: Factors and directions. In I. Mani and M. Maybury, editors, Advances in Automatic Text Summarization, pages 1– 12. MITP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kleinbauer</author>
<author>S Becker</author>
<author>T Becker</author>
</authors>
<title>Combining multiple information layers for the automatic generation of indicative meeting abstracts.</title>
<date>2007</date>
<booktitle>In Proc. of ENLG</booktitle>
<location>Dagstuhl, Germany.</location>
<contexts>
<context position="5864" citStr="Kleinbauer et al. (2007)" startWordPosition="876" endWordPosition="880">tation. 2 Related Research The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), described in Section 5.1. The system uses topic segments and topic labels, and for each topic segment in the meeting a sentence is generated that describes the most frequently mentioned content items in that topic. Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et </context>
</contexts>
<marker>Kleinbauer, Becker, Becker, 2007</marker>
<rawString>T. Kleinbauer, S. Becker, and T. Becker. 2007. Combining multiple information layers for the automatic generation of indicative meeting abstracts. In Proc. of ENLG 2007, Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. of AAAI2000,</booktitle>
<pages>703--710</pages>
<location>Austin, Texas, USA,</location>
<contexts>
<context position="5640" citStr="Knight and Marcu, 2000" startWordPosition="842" endWordPosition="845"> the best summarization results are those that are structured similarly to meetings. Other email conversation structures are less amenable to the current treatment and require further investigation and possibly domain adaptation. 2 Related Research The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), described in Section 5.1. The system uses topic segments and topic labels, and for each topic segment in the meeting a sentence is generated that describes the most frequently mentioned content items in that topic. Our systems differ in two major </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-based summarization - step one: Sentence compression. In Proc. of AAAI2000, Austin, Texas, USA, pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pederson</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proc. of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<pages>68--73</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1297" citStr="Kupiec et al., 1995" startWordPosition="183" endWordPosition="186">t approach to the challenge of automatic summarization has been extraction, where informative sentences in a document are identified and concatenated to form a condensed version of the original document. Extractive summarization has been popular at least in part because it is a binary classification task that lends itself well to machine learning techniques, and does not require a natural language generation (NLG) component. There is evidence that human abstractors at times use sentences from the source documents nearly verbatim in their own summaries, justifying this approach to some extent (Kupiec et al., 1995). Extrinsic evaluations have also shown that, while extractive summaries may be less coherent than human abstracts, users still find them to be valuable tools for browsing documents (He et al., 1999; McKeown et al., 2005; Murray et al., 2008). However, these same evaluations also indicate that concise abstracts are generally preferred by users and lead to higher objective task scores. The limitation of a cut-and-paste summary is that the end-user does not know why the selected sentences are important; this can often only be discerned by exploring the context in which each sentence originally a</context>
</contexts>
<marker>Kupiec, Pederson, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable document summarizer. In Proc. of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Seattle, Washington, USA, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>M Galley</author>
<author>S Maskey</author>
</authors>
<title>From text to speech summarization.</title>
<date>2005</date>
<booktitle>In Proc. of ICASSP 2005,</booktitle>
<pages>997--1000</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="1517" citStr="McKeown et al., 2005" startWordPosition="218" endWordPosition="221">rization has been popular at least in part because it is a binary classification task that lends itself well to machine learning techniques, and does not require a natural language generation (NLG) component. There is evidence that human abstractors at times use sentences from the source documents nearly verbatim in their own summaries, justifying this approach to some extent (Kupiec et al., 1995). Extrinsic evaluations have also shown that, while extractive summaries may be less coherent than human abstracts, users still find them to be valuable tools for browsing documents (He et al., 1999; McKeown et al., 2005; Murray et al., 2008). However, these same evaluations also indicate that concise abstracts are generally preferred by users and lead to higher objective task scores. The limitation of a cut-and-paste summary is that the end-user does not know why the selected sentences are important; this can often only be discerned by exploring the context in which each sentence originally appeared. One possible improvement is to create structured extracts that represent an increased level of abstraction, where selected sentences are grouped according to phenomena such as decisions, action items and problem</context>
</contexts>
<marker>McKeown, Hirschberg, Galley, Maskey, 2005</marker>
<rawString>K. McKeown, J. Hirschberg, M. Galley, and S. Maskey. 2005. From text to speech summarization. In Proc. of ICASSP 2005, Philadelphia, USA, pages 997–1000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Muller</author>
</authors>
<title>Resolving It, This and That in unrestricted multi-party dialog.</title>
<date>2007</date>
<booktitle>In Proc. of ACL 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9681" citStr="Muller, 2007" startWordPosition="1454" endWordPosition="1455">t al. (Xie et al., 2009), where n-grams are weighted by tf.idf scores, except that we use noun phrases rather than any n-grams because we want to refer to the entities in the generated text. We use mid-range document frequency instead of idf (Church and Gale, 1995), where the entities occur in between 10% and 90% of the documents in the collection. In Section 4 we describe how we use the entity’s term frequency to detect the most informative entities. We do not currently attempt coreference resolution for entities; recent work has investigated coreference resolution for multi-party dialogues (Muller, 2007; Gupta et al., 2007), but the challenge of resolution on such noisy data is highlighted by low accuracy (e.g. Fmeasure of 21.21) compared with using well-formed text (e.g. monologues). We map sentences to our ontology’s object properties by building numerous supervised classifiers trained on labeled decision sentences, action sentences, etc. A general extractive classifier is also trained on sentences simply labeled as important. After predicting these sentence-level properties, we consider a participant to be linked to an entity if the participant mentioned the entity in a sentence in which </context>
</contexts>
<marker>Muller, 2007</marker>
<rawString>C. Muller. 2007. Resolving It, This and That in unrestricted multi-party dialog. In Proc. of ACL 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
</authors>
<title>Detecting action items in meetings.</title>
<date>2008</date>
<booktitle>In Proc. of MLMI</booktitle>
<location>Utrecht, the Netherlands.</location>
<contexts>
<context position="7122" citStr="Murray and Renals, 2008" startWordPosition="1077" endWordPosition="1080">gs but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping Source document interpretation in our system relies on a simple conversation ontology. The ontology is written in OWL/RDF and contains two core upper-level classes: Participant and Entity. When </context>
<context position="23321" citStr="Murray and Renals, 2008" startWordPosition="3722" endWordPosition="3726">his section we present results for our interpretation and transformation components. 1http://www.csie.ntu.edu.tw/ cjlin/liblinear/ 6.1 Interpretation: Meetings Figure 1 shows the ROC curves for the sentencelevel classifiers applied to manual transcripts. On both manual and ASR transcripts, the classifiers with the largest AUROCs are the action item and general extractive classifiers. Action item sentences can be detected very well with this feature set, with the classifier having an AUROC of 0.92 on manual transcripts and 0.93 on ASR, a result comparable to previous findings of 0.91 and 0.93 (Murray and Renals, 2008) obtained using a speechspecific feature set. General extractive classification is also similar to other state-of-the-art extraction approaches on spoken data using speech features (Zhu and Penn, 2006)2 with an AUROC of 0.87 on manual and 0.85 on ASR. Decision sentences can also be detected quite well, with AUROCs of 0.81 and 0.77. Positive-subjective, negative-subjective and problem sentences are the most difficult to detect, but the classifiers still give credible performance with AUROCs of approximately 0.76 for manual and 0.70-0.72 for ASR. 0 0.2 0.4 0.6 0.8 1 FP Figure 1: ROC Curves for O</context>
</contexts>
<marker>Murray, Renals, 2008</marker>
<rawString>G. Murray and S. Renals. 2008. Detecting action items in meetings. In Proc. of MLMI 2008, Utrecht, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>T Kleinbauer</author>
<author>P Poller</author>
<author>S Renals</author>
<author>T Becker</author>
<author>J Kilgour</author>
</authors>
<title>Extrinsic summarization evaluation: A decision audit task.</title>
<date>2008</date>
<booktitle>In Proc. of MLMI 2008,</booktitle>
<location>Utrecht, the Netherlands.</location>
<contexts>
<context position="1539" citStr="Murray et al., 2008" startWordPosition="222" endWordPosition="225">lar at least in part because it is a binary classification task that lends itself well to machine learning techniques, and does not require a natural language generation (NLG) component. There is evidence that human abstractors at times use sentences from the source documents nearly verbatim in their own summaries, justifying this approach to some extent (Kupiec et al., 1995). Extrinsic evaluations have also shown that, while extractive summaries may be less coherent than human abstracts, users still find them to be valuable tools for browsing documents (He et al., 1999; McKeown et al., 2005; Murray et al., 2008). However, these same evaluations also indicate that concise abstracts are generally preferred by users and lead to higher objective task scores. The limitation of a cut-and-paste summary is that the end-user does not know why the selected sentences are important; this can often only be discerned by exploring the context in which each sentence originally appeared. One possible improvement is to create structured extracts that represent an increased level of abstraction, where selected sentences are grouped according to phenomena such as decisions, action items and problems, thereby giving the </context>
</contexts>
<marker>Murray, Kleinbauer, Poller, Renals, Becker, Kilgour, 2008</marker>
<rawString>G. Murray, T. Kleinbauer, P. Poller, S. Renals, T. Becker, and J. Kilgour. 2008. Extrinsic summarization evaluation: A decision audit task. In Proc. of MLMI 2008, Utrecht, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>J Dowding</author>
<author>J Niekrasz</author>
<author>P Ehlen</author>
<author>S Noorbaloochi</author>
</authors>
<title>Detecting and summarizing action items in multi-party dialogue.</title>
<date>2007</date>
<booktitle>In Proc. of the 9th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="7096" citStr="Purver et al., 2007" startWordPosition="1073" endWordPosition="1076">ic not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping Source document interpretation in our system relies on a simple conversation ontology. The ontology is written in OWL/RDF and contains two core upper-level classes: Par</context>
</contexts>
<marker>Purver, Dowding, Niekrasz, Ehlen, Noorbaloochi, 2007</marker>
<rawString>M. Purver, J. Dowding, J. Niekrasz, P. Ehlen, and S. Noorbaloochi. 2007. Detecting and summarizing action items in multi-party dialogue. In Proc. of the 9th SIGdial Workshop on Discourse and Dialogue, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raaijmakers</author>
<author>K Truong</author>
<author>T Wilson</author>
</authors>
<title>Multimodal subjectivity analysis of multiparty conversation.</title>
<date>2008</date>
<booktitle>In Proc. ofEMNLP 2008,</booktitle>
<location>Honolulu, HI, USA.</location>
<contexts>
<context position="7174" citStr="Raaijmakers et al., 2008" startWordPosition="1084" endWordPosition="1087">logy applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping Source document interpretation in our system relies on a simple conversation ontology. The ontology is written in OWL/RDF and contains two core upper-level classes: Participant and Entity. When additional information is available about participan</context>
</contexts>
<marker>Raaijmakers, Truong, Wilson, 2008</marker>
<rawString>S. Raaijmakers, K. Truong, and T. Wilson. 2008. Multimodal subjectivity analysis of multiparty conversation. In Proc. ofEMNLP 2008, Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>L Shrestha</author>
<author>J Chen</author>
<author>C Lauridsen</author>
</authors>
<title>Summarizing email threads.</title>
<date>2004</date>
<booktitle>In Proc. of HLTNAACL 2004,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="6854" citStr="Rambow et al., 2004" startWordPosition="1035" endWordPosition="1038">ajor respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We descr</context>
</contexts>
<marker>Rambow, Shrestha, Chen, Lauridsen, 2004</marker>
<rawString>O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen. 2004. Summarizing email threads. In Proc. of HLTNAACL 2004, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ulrich</author>
<author>G Murray</author>
<author>G Carenini</author>
</authors>
<title>A publicly available annotated corpus for supervised email summarization.</title>
<date>2008</date>
<booktitle>In Proc. ofAAAI EMAIL-2008 Workshop,</booktitle>
<location>Chicago, USA.</location>
<contexts>
<context position="20343" citStr="Ulrich et al., 2008" startWordPosition="3230" endWordPosition="3233">ctive and negative-subjective utterances on a subset of 20 AMI meetings (Wilson, 2008). Such subjective utterances involve the expression of a private state, such as a positive/negative opinion, positive/negative argument, and agreement/disagreement. Of the roughly 20,000 total sentences in the 20 AMI meetings, nearly 4000 are labeled as positive-subjective and nearly 1300 as negative-subjective. 5.1.2 The BC3 Email Corpus While our main experiments focus on the AMI meeting corpus, we follow these up with an investigation into applying our abstractive techniques to email data. The BC3 corpus (Ulrich et al., 2008) contains email threads from the World Wide Web Consortium (W3C) mailing list. The threads feature a variety of topics such as web accessibility and planning face-to-face meetings. The annotated portion of the mailing list consists of 40 threads. The threads are annotated in the same manner as the AMI corpus, with three human annotators per thread first authoring abstracts and then linking email thread sentences to the abstract sentences. The corpus also contains speech act annotations. Unlike the AMI corpus, however, there are no annotations for deci898 sions, actions and problems, an issue a</context>
</contexts>
<marker>Ulrich, Murray, Carenini, 2008</marker>
<rawString>J. Ulrich, G. Murray, and G. Carenini. 2008. A publicly available annotated corpus for supervised email summarization. In Proc. ofAAAI EMAIL-2008 Workshop, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
</authors>
<title>Annotating subjective content in meetings.</title>
<date>2008</date>
<booktitle>In Proc. ofLREC 2008,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="19809" citStr="Wilson, 2008" startWordPosition="3151" endWordPosition="3153">and three subsections for “decisions,” “actions” and “problems” from the meeting. A many-to-many mapping between transcript sentences and sentences from the human abstract was obtained for each annotator. Approximately 13% of the total transcript sentences are ultimately labeled as extracted sentences. A sentence is considered a decision item if it is linked to the decision portion of the abstract, and action and problem sentences are derived similarly. For the subjectivity annotation, we use annotations of positive-subjective and negative-subjective utterances on a subset of 20 AMI meetings (Wilson, 2008). Such subjective utterances involve the expression of a private state, such as a positive/negative opinion, positive/negative argument, and agreement/disagreement. Of the roughly 20,000 total sentences in the 20 AMI meetings, nearly 4000 are labeled as positive-subjective and nearly 1300 as negative-subjective. 5.1.2 The BC3 Email Corpus While our main experiments focus on the AMI meeting corpus, we follow these up with an investigation into applying our abstractive techniques to email data. The BC3 corpus (Ulrich et al., 2008) contains email threads from the World Wide Web Consortium (W3C) m</context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>T. Wilson. 2008. Annotating subjective content in meetings. In Proc. ofLREC 2008, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Xie</author>
<author>B Favre</author>
<author>D Hakkani-T¨ur</author>
<author>Y Liu</author>
</authors>
<title>Leveraging sentence weights in a concept-based optimization framework for extractive meeting summarization.</title>
<date>2009</date>
<booktitle>In Proc. ofInterspeech 2009,</booktitle>
<location>Brighton, England.</location>
<marker>Xie, Favre, Hakkani-T¨ur, Liu, 2009</marker>
<rawString>S. Xie, B. Favre, D. Hakkani-T¨ur, and Y. Liu. 2009. Leveraging sentence weights in a concept-based optimization framework for extractive meeting summarization. In Proc. ofInterspeech 2009, Brighton, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>E Hovy</author>
</authors>
<title>Digesting virtual ”geek” culture: The summarization of technical internet relay chats.</title>
<date>2005</date>
<booktitle>In Proc. ofACL 2005,</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="6971" citStr="Zhou and Hovy, 2005" startWordPosition="1054" endWordPosition="1057">content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping Source document interpretation in our syste</context>
</contexts>
<marker>Zhou, Hovy, 2005</marker>
<rawString>L. Zhou and E. Hovy. 2005. Digesting virtual ”geek” culture: The summarization of technical internet relay chats. In Proc. ofACL 2005, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>G Penn</author>
</authors>
<title>Summarization of spontaneous conversations.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech</booktitle>
<pages>1531--1534</pages>
<location>Pittsburgh, USA,</location>
<contexts>
<context position="6924" citStr="Zhu and Penn, 2006" startWordPosition="1046" endWordPosition="1049">notations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapp</context>
<context position="23522" citStr="Zhu and Penn, 2006" startWordPosition="3752" endWordPosition="3755">level classifiers applied to manual transcripts. On both manual and ASR transcripts, the classifiers with the largest AUROCs are the action item and general extractive classifiers. Action item sentences can be detected very well with this feature set, with the classifier having an AUROC of 0.92 on manual transcripts and 0.93 on ASR, a result comparable to previous findings of 0.91 and 0.93 (Murray and Renals, 2008) obtained using a speechspecific feature set. General extractive classification is also similar to other state-of-the-art extraction approaches on spoken data using speech features (Zhu and Penn, 2006)2 with an AUROC of 0.87 on manual and 0.85 on ASR. Decision sentences can also be detected quite well, with AUROCs of 0.81 and 0.77. Positive-subjective, negative-subjective and problem sentences are the most difficult to detect, but the classifiers still give credible performance with AUROCs of approximately 0.76 for manual and 0.70-0.72 for ASR. 0 0.2 0.4 0.6 0.8 1 FP Figure 1: ROC Curves for Ontology Mapping Classifiers (Manual Transcripts) 6.2 Transformation: Meetings In this section we present the weighted recall scores for the sentences selected using the ILP method described in Section </context>
</contexts>
<marker>Zhu, Penn, 2006</marker>
<rawString>X. Zhu and G. Penn. 2006. Summarization of spontaneous conversations. In Proc. of Interspeech 2006, Pittsburgh, USA, pages 1531–1534.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>