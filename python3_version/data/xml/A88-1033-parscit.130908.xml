<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.8367126">
COMBINATORIAL DISAMBIGUATION
P. S. Newman
IBM Los Angeles Scientific Center
11601 Wilshire Boulevard
Los Angeles, CA 90025-1738
</note>
<sectionHeader confidence="0.967418" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997920181818182">
The disambiguation of sentences is a combinatorial
problem. This paper describes a method for treating
it as such, directly, by adapting standard combinatorial
search optimizations. Traditional disambiguation heu-
ristics are applied but, instead of being embedded in
individual decision procedures for specific types of
ambiguities, they contribute to numerical weights that
are considered by a single global optimizer. The result
is increased power and simpler code. The method is
being implemented for a machine translation project,
but could be adapted to any natural language system.
</bodyText>
<sectionHeader confidence="0.997376" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.986069333333333">
The disambiguation of sentences is a combinatorial
problem. Identification of one word sense interacts
with the identification of other word senses,
</bodyText>
<listItem confidence="0.992579">
(1) Ile addressed the chair
and with constituent attachment,
(2) He shot some bucks with a rifle
</listItem>
<bodyText confidence="0.951716">
Moreover, the attachment of one constituent interacts
with the attachment of other constituents:
</bodyText>
<listItem confidence="0.955205">
(3) She put the vase on the table in the living room
</listItem>
<bodyText confidence="0.999769888888889">
This paper describes a method of addressing the prob-
lem directly, by adapting standard search optimization
techniques. In the first section we describe the core of
the method, which applies a version of best-first search
to a uniform representation of the set of possibilities.
In the second section we relate the work to other
approaches to preference-based disambiguation. The
final sections describe how the representation may be
obtained from a lexicon.
</bodyText>
<sectionHeader confidence="0.995404" genericHeader="introduction">
2. The Search Method
</sectionHeader>
<bodyText confidence="0.998933484848485">
In the machine translation project for which this tech-
nique is being developed, disambiguation begins after
a parser, specifically the PEN1.13 English Grammar
by Jensen (1986), has identified one or more parses
based primarily on syntactic considerations (including
subcategorization). Words are disambiguated up to
part-of-speech, but word senses are not identified. In-
dividual parses may indicate alternative attachments
of many kinds of constituents including prepositional
phrases and relative clauses.
Beginning disambiguation only after a general parse
has the advantage of making clear what all the possi-
bilities are, thus allowing their investigation in an ef-
ficient order. Performing a full parse before
disambiguation need not consume an inordinate
amount of space or time; techniques such as those
used by Jensen (default rightmost prepositional phrase
attachment), and Tomita (1985) (parse forests) ade-
quately control resource requirements.
The parser output is first transformed so that it is
represented as a set of of semantic choice points. Each
choice point generally represents a constituent. It con-
tains a group of weighted semantic alternatives that
represent the different ways in which word-senses of
the constituent head can be associated semantically
with word-senses of a higher level head. This allows
word-sense and attachment alternatives to be treated
uniformly.
Combinatorial disambiguation then selects the consis-
tent combination of alternatives, one from each choice
point, that yields the highest total weight. To illustrate
the method, we use the extension of the classic example
mentioned above:
</bodyText>
<listItem confidence="0.791537">
(2) Ile shot some bucks with a rifle
</listItem>
<bodyText confidence="0.8500595">
A decomposition of the sentence into choice points cl ,
c2, and c3 is shown in Figure I. (The illustration
</bodyText>
<page confidence="0.997626">
243
</page>
<bodyText confidence="0.999949419354839">
assumes that &amp;quot;shot&amp;quot; and &amp;quot;bucks&amp;quot; have two meanings
each, and ignores determiners.) Choice point &amp;quot;cl&amp;quot;
gives the alternative syntactic and semantic possibilities
for the attachment of the constituent &amp;quot;he&amp;quot; to its only
possible head &amp;quot;shot&amp;quot;. Alternative el 1 is that &amp;quot;he&amp;quot; is
the subject of &amp;quot;shootr, with the semantic function
&amp;quot;agent&amp;quot;, and is given (for reasons which will be dis-
cussed later) the weight &amp;quot;3&amp;quot;. Alternative c12 is similar,
but the meaning used for &amp;quot;shoot&amp;quot; is &amp;quot;shoot2&amp;quot;. Similar
alternatives are used for the attachment (c2) of the
object &amp;quot;some bucks&amp;quot; to its only possible head, &amp;quot;shoot&amp;quot;.
Alternative c23 represents the unlikely combinations.
Choice point c3 reflects the different possible attach-
ments of &amp;quot;with a rifle&amp;quot;; the highest weight (3) is given
to its attachmcnt as an instrumental modifier of
&amp;quot;shootl&amp;quot;. The other possibilities range from barely
plausible to implausible and are weighted accordingly.
I laving obtained this repesentation (whose construction
is described in later sections), the next step is to es-
tablish the single-alternative choice points as given and
to propagate any associated word-sense constraints to
narrow or eliminate other alternatives. (This does not
occur in our example.)
Then combinations of the remaining choices are
searched using a variation of the A* best-first search
method. See Nilsson (1980) for a thorough description
of A*. Briefly, A* views combinatorial search as a
problem of finding an optimal path from the root to
a leaf of a tree whose nodes are the weighted alterna-
tives to be combined. At any point in the process, the
next node n to be added is one for which the potential
</bodyText>
<equation confidence="0.998723">
F(n) = G(n) + 11(n)
</equation>
<bodyText confidence="0.999083793103449">
is maximal over all possible additions. G(n) represents
the value of the path up to and including n. 11(n)
represents an estimated upper bound for the value of
the path below n (i.e., for the additional value which
can be obtained)&apos;. When a complete path is found by
this method, it must be optimal. The efficiency of the
method, i.e., the speed of finding an optimal path,
depends on the accuracy of the estimates II(n).
To apply the method in our context, the search tree
is defined in terms of levels, with each level corre-
sponding to a choice point. Choice points are assigned
to levels so that those which would probably be re-
sponsible for the greatest difference between estimated
and actual 11(n) in an arbitrary assignment are exam-
ined first. Looked at in another way, the assignment
of choice points to levels is made so that those which
will best differentiate among potential path scores are
examined first.
This is done by (partially) ordering the choice points
in descending order of their difference potential Dc,
the difference in weight between their highest weighted
alternative and the next alternative. If the highest
weight is represented by two different alternatives, Dc
= 0. Within this ordering the choice points are further
ordered by weight differences between the 2nd and
3rd highest weighted alternatives, etc. For our example
this results in choice point c3 (&amp;quot;with a rifle&amp;quot;) being
assigned to the highest level in the tree, followed by
choice points c2 and then cl .
</bodyText>
<table confidence="0.87937125">
cl.He
cll agt (he shootl) 3 (i.e., fired-at)
c12 agt (he shoot2) 3 (i.e., wasted)
c2.some bucks
c21 goal (buckl shootl) 3 (i.e., male deer)
c22 goal (buck2 shoot2) 3 (i.e., dollar)
c23 goal (buckl shoot2),(buck2 shootl) 0
c3.with a rifle
c31 inst (rifle shootl) 3
c32 inst (rifle shoot2) 2
c33 togw (rifle (buckl,buck2)) 0 (i.e., together-with)
c34 accm (rifle (shootl,shoot2)) 0 (i.e., accompanied-by)
</table>
<figureCaption confidence="0.951667">
Figure 1: Choice points and alternatives
</figureCaption>
<page confidence="0.995761">
244
</page>
<bodyText confidence="0.996802895833334">
We also associate with each level= choice point the
value 1 lc, which is the sum of the maximum weights
that can be added by contributions below that choice
point. This is needed by the algorithm to estimate
potential path scores below a given node. For this
example, the Ilc values are:
HO: top=9 H3: rifle=6
H2: buck=3 Hl: he=0
Then the best-first search is carried out. At each point
in the search, the next node to be added is that which
(a) is consistent in word-sense selection with previous
choices, and (b) has the highest potential. The potential
is calculated as the accumulated weight down to (and
including) that node plus FIc for that level. A diagram
of the procedure, as applied to the example, is shown
in Figure 2. The first node to be added is with rifle
shoot!&apos;, which has the highest potential. At that point,
the highest weighted consistent alternative is c21, etc.
While the set of choice points implies that there are
(4 x 3 x 2) = 24 paths to be searched, only one is
pursued to any distance. Thus while the approach
takes a combinatorial view of the problem, it does so
without loss of efficiency.
When a full path is found, it is examined for semantic
consistency (beyond word-sense consistency). The
checks made include: (a) ensuring that the interpreta-
tion includes all required semantic functions for a
word-sense (specified in the lexicon), and (13) ensuring
that non-repeatable functions (e.g., the goal of an ac-
tion) are not duplicated.
Even if the full path is found to be consistent, the
search does not terminate immediately, but continues
until it is certain that no other path can have an equal
score. This will be true whenever the maximum po-
tential for open nodes is less than the score for an
already-found path. A more precise description of the
algorithm is given in the Appendix.
When more than one path is found with the same
high score, additional tests are applied. These tests
include comparisons of surface proximity and, as this
work is situated within a multi-target translation sys-
tem, user queries in the source language, as outlined
by Tomita (1985).
An extended version of the method is used in com-
paring alternate parses which differ in constituent com-
position, and thus are more easily analyzed as different
parse trees, each with its own set of choice points. An
example is the classic:
</bodyText>
<listItem confidence="0.450681">
(4) Time flies like an arrow
</listItem>
<bodyText confidence="0.999825066666667">
(where the main verb can be any one of the first three
words). In such cases, one set of choice points is
constructed per parse tree. In general, the search
alternates among trees, with the next node to be added
being that with the greatest potential across trees. If
such trees always had the same number of choice
points, this would he the only revision needed. How-
ever, the number of choice points may differ, for one
thing because the parser may have detected and con-
densed non-compositional compounds (e.g., &amp;quot;all the
same&amp;quot;) in one parse but not in another. For this
reason the algorithm changes to evaluate paths not by
total scores, but by average scores (i.e., the total scores
divided by the number of choice points in the particular
parse).
</bodyText>
<figure confidence="0.6468785">
top
314H*********
&amp;quot;rifle&amp;quot; c31
*********
&amp;quot;buck&amp;quot; c21 c22 (inconsistent)
cll
</figure>
<figureCaption confidence="0.998213">
Figure 2: Reduced Tree Search
</figureCaption>
<bodyText confidence="0.96428">
I The basic A&amp;quot; algorithm is usually described as &amp;quot;expanding&apos; (i.e., adding all immediate successors 01) the most promising node. The variant
described here, which is more appropriate to our situation (and also mentioned by Nilsson), adds a single node at each step.
</bodyText>
<page confidence="0.984814">
245
</page>
<sectionHeader confidence="0.550257" genericHeader="related work">
3. Related Work implications of the results of other kinds of decision
</sectionHeader>
<bodyText confidence="0.979877939759036">
procedures.
There seems to be little work which directly addresses
the combinatorial problem. First, there is considerable
work in preference-related disambiguation that as-
sumes, at least for purposes of discussion, that indi-
vidual disambiguation problems can be addressed in
isolation. For example, treatments of prepositional
phrase attachment by by Dahlgren and McDowell
(1986) and Wilks et. al. (1985) propose methods of
finding the &amp;quot;best&amp;quot; resolution of a single attachment
problem by finding the first preference which is satisfied
in some recommended order of rule application. Other
types of ambiguity, and other instances of the same
type, are assumed to have been resolved. This type
of work contributes interesting insights, but cannot be
used directly.
One type of more realistic treatment, which might be
called the deferred decision approach, is exemplified
by !first (1983). When, in the course of a parse, an
immediate decision about a word sense or attachment
cannot be made, a set of alternative possibilities is
developed. The possibility sets are gradually narrowed
by propagating the implications of both decisions and
narrowings of other possibility sets.
This approach has a number of problems. First, it is
susceptible to error in semantic &amp;quot;garden path&amp;quot; situa-
tions, as early decisions may not be justifiable in the
context. For example, in processing
(S) He shot a hundred bucks with one rifle.
a particular expert might decide on &amp;quot;dollars&amp;quot; for bucks,
because of the modifier &amp;quot;hundred&apos;, before the prepo-
sitional phrase is processed. Also, it is difficult to see
how versions of this method could be extended to deal
with comparing alternate parses where the alternatives
are not just ones of attaching constituents, but of de-
ciding what the constituents are in the first place.
A full-scale deferred-decision approach also has the
potential of significant design complexity (the Hirst
version is explicitly limited), as each type of decision
procedure (for words and for different kinds of attach-
ments) must be able to understand and process the
Underlying these problems is the lack of quantification
of alternatives, which allows for comparison of com-
binations.
There are, however, early and more recent approaches
which do apply numeric weights to sentence analysis.
Early approaches using weights applied them primarily
to judge alternative parses. Syntactically-oriented ap-
proaches in this vein attached weights to phrase struc-
ture grammar rules (Robinson 1975, 1980) or ATN
arcs (Bates 1976). Some approaches of this period
focussing on semantic expectations were those of Wilks
(1975) and Maxwell and Tuggle (1975), which em-
ployed scores expressing the number of expected de-
pendents present in an interpretation. An ambitious
approach combining different kinds of weights and
cumulative scores, described by Walker and Paxton
Ct. al. (1977), included heuristics to select the most
promising subtrecs for earlier development, to avoid
running out of space before a &amp;quot;best&amp;quot; tree can be found.
However, except for this type of provision, none of
the early approaches using weights seem to address
the combinatorial problem.2
A contemporary approach for thorough syntactic and
semantic disambiguation using weights is described by
L. Schubert (1986). During a lett-to-right parse, in-
dividual attachments are weighted based on a list of
considerations including preference, relative sentential
position, frames/scripts, and salience in the current
context. The multiple evolving parse trees are rated
by summing their contained weights, and the combi-
natorial problem is controlled by retaining only the
two highest scoring parses of any complete phrases.
This approach is interesting, although some details are
vague3. However, the post-parse application of A*
described in this paper obtains the benefits of such a
within-parse approach without its deficiences in that:
(a) combinatorial computations of weights and word-
sense consistencies are avoided except when warranted
by total sentence information, and (11) there is no pos-
sibility of early erroneous discarding of alternatives.
2 Heidom (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions
based on syntactic considerations only.
</bodyText>
<footnote confidence="0.75934775">
3 No examples are given, so it is unclear whether a parse for a phrase or part thereof represents only one interpretation, or all interpretations
having the same structure, scored by the most likely interpretation. The former is obviously inadequate (e.g., for highly ambiguous subject NPs
like &apos;The stands&apos;), while the latter seems to require either the calculation of all alternative cumulative scores, or recalculation of scores if an
interpretation fails.
</footnote>
<page confidence="0.988766">
246
</page>
<figure confidence="0.611403333333333">
he bucks with a rifle
subj he shoot obj buck shoot with rifle shoot
with rifle buck
</figure>
<figureCaption confidence="0.998276">
Figure 3: Syntactic Choice Points
</figureCaption>
<bodyText confidence="0.999951606060606">
One other parser-based work should be noted, that of
Wittenburg (1986), as it is explicitly based on A. The
intent and content of the method is quite different from
that described here. It is situated within a chart-parser
for a categorial grammar, and the intent is to limit
parsing expense by selecting that rule for next appli-
cation which has the least likelihood of leading to an
incomplete tree. While selectional preference is men-
tioned in passing as a possible heuristic, the heuristics
discussed in any depth are grammar oriented, and
operate on the immediate facts of the situation, rather
than on informed estimates of total parse scores.
It should be also be mentioned that the representation
of alternatives in schemes which combine syntactic
and semantic disambiguation is rarely discussed, al-
though maintaining a consistent representation of the
relationships among word-sense and attachment alter-
natives is fundamental to a systematic treatment of the
problem. An exception is the discussion by K.
Schubert (1986), who describes a representation for
alternatives with some affinities to that described here.
The information limitations of disambiguation during
parsing are not found in spreading-activation ap-
proaches, exemplified by Charniak (1986), Cottrell
and Small (1984), and Waltz and Pollack (1985).
These approaches are still in the experimental stage,
and are primarily intended for parallel hardware, while
the A* algorithm used in this paper is designed for
conventional serial hardware. But, in a sense, these
approaches reinforce the main point of this paper:
they argue for a single global technique for optimized
combinatorial disambiguation based on all available
information.
</bodyText>
<sectionHeader confidence="0.867446" genericHeader="method">
4. Preparing Semantic Choices
</sectionHeader>
<bodyText confidence="0.999204842105263">
having described how the choice points are used, we
address their development. Two steps are involved:
(1) the development of syntactic choice points, and (2)
the development of semantic choice points. The first
step transforms the parse-level syntactic functions into
a form appropriate to the second step, which is the
application of the lexicon to those functions to obtain
the semantic alternatives.
In our example, the first step is a simple one. Syntactic
relationships among constituents are transformed into
syntactic relationships among head words, and the
syntactic relationships are refined, so that &amp;quot;ppmod&amp;quot; is
replaced by the actual prepositions used. &apos;the result
of this step is shown in Figure 3. The development of
syntactic choice points for some other types of con-
stituents is more complex. Before discussing these
situations, we discuss step 2: application of the lexicon
to the syntactic choice points to obtain the semantic
choice points, i.e., those shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.992152">
4.1 The Lexicon
</subsectionHeader>
<bodyText confidence="0.985583176470588">
The lexicon contains entries for word sterns (distin-
guished by part-of-speech), linked to word-sense en-
tries, which arc the lowest level &amp;quot;concepts&amp;quot;. Concept
entries are linked to superordinatc concept entries,
forming a lattice. Concept entries include a set of
concept features (e.g. a list of superordinate concepts),
and one or more rules for each syntactic function
associated with the concept. The more relevant parts
of the lexicon entries for the concepts used in the
ongoing example are shown in Figure 4. The -classes&amp;quot;
are lists of superordinate concepts. The syntactic func-
tion rules have the form:
synfun dependent head semfun weight
Thus the first rule under &amp;quot;shoot&apos; - indicates that word-
senses falling into the class &amp;quot;animate&amp;quot; are its preferred
objects, with the weight 3, and the combination is
given the semantic function &amp;quot;goal&amp;quot;. The concept entry
</bodyText>
<page confidence="0.608616">
4 However, the weighting scheme is different, and rather interesting. The reference does not discuss the selection of a particular combination of
</page>
<tableCaption confidence="0.38159">
alternatives in any detail, but it appears to be based on the presence in a combination of one (or more?) highly weighted alternative (or alternatives?).
</tableCaption>
<page confidence="0.960894">
247
</page>
<figure confidence="0.97250625">
shootl classes (humanact, transv)
(object animate shootl goal 3)
(with firearm shootl inst 3)
humanact classes (act...)
(subj human humanact agt 3)
(with human humanact accm 3)
buckl classes (animate...)
buck2 classes (money—)
shoot2 classes (humanact, transv)
(object money shoot2 goal 3)
transv
(object thing transv goal 0)
act
(with tool act inst 2)
rifle classes (firearm...)
firearm classes (tool...)
</figure>
<figureCaption confidence="0.99998">
Figure 4: Lexicon Entries
</figureCaption>
<bodyText confidence="0.99458625">
Thumanacr contains other rules applicable to shootl
and other verb-senses taking human actors.
Weights are actually assigned symbolically, to allow
experimentation. Current settings are as follows:
</bodyText>
<listItem confidence="0.933635125">
• Rules for idioms (e.g., kick the bucket), 4.
• Rules for more general selectional preferences, 3.
• Rules for acceptable but not preferred alternatives
(e.g., locative prepositional phrases attached to ar-
bitrary actions), 2.
• Very general attachments (e.g., &amp;quot;nounmod nounl
noun2), 0. These allow for uninterpreted metaphoric
usage.s
</listItem>
<bodyText confidence="0.823168">
One major objective in assigning weights to ensure
that combinations of very high (idiom) weights together
with very low weights do not outscore more balanced
combinations. Thus, for:
</bodyText>
<listItem confidence="0.737626">
(6) He kicked the ugly bucket
</listItem>
<bodyText confidence="0.9977196">
weights such as:
subj he kickedl 3
obj bucketl kickedl 4
adjm ugly bucket1 0
subj he kicked2 3
obi bucket2 kicked2 3
adjm ugly bucket2 2
provide the necessary balance. (Here kicked&apos; is the
idiomatic interpretation, and bucket&apos; is a word-sense
of bucket used only in that interpretation.)
By convention, rules for syntactic functions are as-
signed, by class, to entries for specific kinds of concepts.
Thus rules for verb-attached arguments or preposi-
tional phrases arc stored with verbs or verb classes.
Adjective-noun rules are generally associated with ad-
jectives, and noun-noun rules with the right-hand
nouns (the next section discusses the treatment of
noun-phrase choice points in somewhat greater detail).
Lexicon entries also contain additional information.
First, a list of required syntactic functions is generally
associated with word-senses. Also, syntactic function
rules may contain additional conditions limiting their
applicability. For example, a combination Thounl. IN
nounr might be limited to apply only lithe second
concept denotes an object larger than the first.
</bodyText>
<subsectionHeader confidence="0.992689">
4.2 Lexicon Application
</subsectionHeader>
<bodyText confidence="0.999950777777778">
Given these lexicon entries, the set of semantic alter-
natives corresponding to each syntactic alternative
-synfun wordl wordr may be derived. The goal of
the derivation process is to account for all possible
combinations of word-senses for wordl and word2
related by the syntactic function &amp;quot;synfun&amp;quot;. To do this,
all concept entries containing potentially applicable
rules are searched. For each satisfied rule found, an
alternative is created of the form:
</bodyText>
<subsectionHeader confidence="0.57147">
semantic-relation sensepairs weight
</subsectionHeader>
<bodyText confidence="0.826572166666667">
where -sensepairs is a list of pairs. Each pair is in
the form ((di,dj....) (httn,lin,...)), where the di&apos;s arc
senses of the dependent participant of the function,
Obtaining the necessary lexicon information is of course a major problem. Rut there is significant contemporary work in the automatic or
semi-automatic derivation of that information. For example, the aproached described by Jensen and Ilinot (19/k() obtains attachment preference
information by parsing dictionary definitions.
</bodyText>
<page confidence="0.994589">
248
</page>
<bodyText confidence="0.9901972">
syntactic choice identification step is to re-express, or
&amp;quot;normalize&amp;quot; input syntactic relationsips in terms of the
relationships assumed by the lexicon. For example,
passive constructions such as:
and the hi&apos;s are senses of the headword. The semantic
relationship is stated to apply to all combinations of
word-senses in the cross-products of those lists. For
the example sentence, this process would obtain es-
sentially the alternatives shown in Figure 1, except
that alternative c23 would first be expressed as:
(7) The bucks were shot with a rifle
obj ( buckl ,buck2)(shootl , shoot 2 1 1 0
The last step in the process reduces this result. If
some of the word-sense combinations are also found
in an alternative of higher weight, the &amp;quot;dominated&amp;quot;
combinations are deleted. And if all word sense com-
binations are so dominated, the alternative is deleted.
In this way alternative c23 is reduced to its final form.
After the semantic choice point list is completed, the
search algorithm is applied as described above.
</bodyText>
<sectionHeader confidence="0.75588" genericHeader="method">
5. Preparing Syntactic Choices
</sectionHeader>
<bodyText confidence="0.999979">
In the example above, the preparation of syntactic
choice points from parser output was very simple.
Assuming an input choice point for a constituent to
be a list of (one or more) parser-provided alternative
relationships with an immediately containing constitu-
ent, the process consisted of obtaining the headwords
of each constituent, and of substituting literal prepo-
sitions for the general syntactic function &amp;quot;ppmod&amp;quot;.
I lowever, in other cases this step is a more significant
one. In the lexicon, selectional preferences are ex-
pressed in terms of the syntactic functions of some
basic constituent types. For example, verb preferences
are expressed in terms of the syntactic functions of
active-voice, declarative main clauses, with dependents
in unmarked positions. Adjective preferences are ex-
pressed in terms of classes of nouns occurring in the
relationship &amp;quot;adjective noun&amp;quot;. But there are many
other syntactic relationships whose disambiguation de-
pends on this information. The major function of the
are normalized by replacing the choice &amp;quot;subj buck
shoot&amp;quot; with &amp;quot;object buck shoot&amp;quot;. (A lexicon condition
barring or lowering preference for the &amp;quot;gambling&amp;quot; in-
terpretation in the passive voice is also needed here.)
In ditransitive cases both indirect and direct object
functions arc used as alternatives.
Thus the sequence of deriving semantic choice points
consists of two significant steps, which may be depicted
in terms of results as shown in Figure 5.
The transformation of input syntactic choice points to
normalized syntactic choice points is governed by de-
clarative specifications indicating, for each syntactic
function, how its choice points arc to be transformed.
&apos;The changes are specified as replacements for one or
more positions of the choice triple. For example, some
of the &amp;quot;subj&amp;quot; rules are:
</bodyText>
<figure confidence="0.7053386">
(subj
(test (voicepassivel
synfun &apos;obj))
(test (not (voicepassive))
synfun &apos;subj)
</figure>
<bodyText confidence="0.999935625">
stating that for the input function &amp;quot;subj&amp;quot;, if the specified
test (voicepassive) succeeds, then &amp;quot;obj&amp;quot; is used for the
synfun part of the normalized choice. The additional
rule is used to ensure that the &amp;quot;subject&amp;quot; function is
retained only for the active voice.
Additional applications of these transformations in-
clude those for modifiers of nominalized verbs, attrib-
utive clauses, and relative clauses.
</bodyText>
<figure confidence="0.993105">
Input Syn Chptl Normalized Syn Chptl Semantic Chptl
Choice Cll Choice C111 Choice C1111
Choice C1112
Choice C112 Choice C1121
Choice C1122
</figure>
<figureCaption confidence="0.999577">
Figure 5: Steps in Semantic Choice Point Derivation
</figureCaption>
<page confidence="0.995407">
249
</page>
<bodyText confidence="0.940891818181818">
Noun phrases whose heads are nominalized verbs are
addressed by adding choice points corresponding to
verb arguments. Thus for
(8) The bucks&apos; shooting
the alternative &amp;quot;nounmod bucks shooting&amp;quot; is expanded
to include the alternatives &amp;quot;subj bucks shooting&amp;quot; and
&amp;quot;obj bucks shooting&amp;quot;. Then, during lexical processing,
rules for word-senses of the noun &amp;quot;shooting&amp;quot; having
an associated verb are understood as expanded to
include the expected verb arguments.
Attributive clauses such as:
</bodyText>
<subsubsectionHeader confidence="0.494769">
(9) The bucks were handsome.
</subsubsectionHeader>
<bodyText confidence="0.990953696969697">
are transformed to allow the application of adjective
information. !fere &amp;quot;obj handsome were is transformed
to &amp;quot;adjmod handsome bucks&amp;quot;.
For relative clauses, the core of the transformation
expresses alternative attachments of the relative clause
as alternative connections between the head of the
relative clause, and the possible fillers of the gap po-
sition. (Relative clauses with multiple gaps are gen-
erally handled in separate parses.) Thus for:
(/0) The rifle above the mantle that the bucks were
shot with...
transformations produce the alternatives with shoot
rifle and with shoot mantle&amp;quot;.
The handling of relative clauses, however, is more
complicated than this, as it is desirable to also use
information from the relative pronoun (if present) for
the disambiguation. Two initial choice points are in-
volved, one attaching the relative clause to its higher
level head, and one attaching the gap to its head. The
first is expanded to to obtain relationships &amp;quot;relp that
rifle, &amp;quot;relp that mantle, and the other to obtain the
with ....&amp;quot; relationships. And an additional consistency
check is made during the tree search, beyond word-
sense consistency, to keep the choices consistent.
It should be noted that the transformation rules for
syntactic choice points also include &amp;quot;fixup specifica-
tions&amp;quot; (not shown above) indicating how result semantic
functions and attachments are to be modified if the
transformed alternatives are used in the final interpre-
tation. For example, to &amp;quot;fixup&amp;quot; the results of trans-
forming attributive clauses, the noun-modifier semantic
role is replaced with one suitable to a direct role in
the clause.
</bodyText>
<sectionHeader confidence="0.967051" genericHeader="method">
6. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.914052837209302">
This paper has summarized a three-step method for
optimized combinatorial preference based
disambiguation:
1. obtaining syntactic choice points, with alternatives
stated as syntactic functions relating words.
2. transformation into semantic choice points with
alternatives stated as weighted semantic functions
relating word-senses, via lexicon search.
3. application of M to search alternative combina-
tions.
This method, currently being implemented in the con-
text of a multi-target machine translation system, is
more powerful and systematic than approaches using
isolated or interacting decision procedures, and thus
is easier to modify and extend with new heuristics as
desired.
The method is applicable to any post-parse
disambiguation situation, and can be taken as an ar-
gument for that approach. To demonstrate this, we
first note that aspects of the method are useful for
within-parse disambiguation. In any realistic scheme,
decisions must often be deferred, making two aspects
of the method relevant: (a) the unified way of repre-
senting word sense and attachment alternatives and
their interdependency, and (b) the explicit, additive
weights. Explicit additive weights substitute for elab-
orate, case-specific rules, and also make possible a
systematic treatment of alternative parses which differ
in more than word-senses and attachments.
I lowever, using weighted attachments for within-parse
disambiguation requires calculating the summed
weights of, arid examining the consistency of, all com-
binations encountered whose elements cannot be dis-
carded (assuming some good criteria for discarding
can be found). Deferring disambiguation until after
the parse allows for optimized searching of alternatives,
as described above, to significantly limit the number
of combinations examined.
Future work in this direction will include refining the
weighting criteria, extending the method to deal with
anaphoric references (using considerations developed
by, for example, Jensen and 7.adrozny (1987), and
integrating a treatment of non-frozen metaphor.
</bodyText>
<page confidence="0.982545">
250
</page>
<bodyText confidence="0.9002618">
7.Acknowledgement5
I thank John Sowa, Maria Fuenmayor, and Shelley
Smith for their careful reviews and many helpful sug-
gestions. Also, I thank Peter Woon for his patient
managerial support of this project.
</bodyText>
<sectionHeader confidence="0.986621" genericHeader="method">
8. References
</sectionHeader>
<reference confidence="0.9997515">
1. Bates, Madeleine 1976. &amp;quot;Syntax in Automatic
Speech Understanding&amp;quot;, Am. J. Comp. Ling. Mi-
crofiche 45.
2. Charniak, Eugene 1986. &amp;quot;A Neat Theory of
Marker Passing&amp;quot; Proc. AAAI-86 584-588
3. Cottrell, Garrison W. and Steven L. Small 1984.
&amp;quot;Viewing Parsing as Word Sense Discrimination:
A Connectionist Approach&amp;quot;, in B.G. Bara and G.
Guida (eds), Computation Models of Natural Lan-
guage Processing, Elsevier Science Publishers B.V.
4. Dahlgren, Kathleen and Joyce McDowell 1986.
&amp;quot;Using Commonsense Knowledge to
Disambiguate Prepositional Phrase Modifiers&amp;quot;,
Proc. AAAI-86, 589-593
5. Heidorn, George 1982. &amp;quot;Experience with an Easily
Computed Metric for Ranking Alternative Parses&amp;quot;
Proc. 20th Annual Meeting of the ACL, June 1982
6. Hirst, Graeme 1983. &amp;quot;Semantic Interpretation
Against Ambiguity&amp;quot;, Technical Report CS-83-25,
Brown University, December 1983
7. Jensen, Karen 1986. &amp;quot;Parsing Strategies in a
Broad-coverage Grammar of English&amp;quot;, IBM Re-
search Report RC 12147, 1986
8. Jensen, Karen and Jean-Louis Binot 1987. &amp;quot;A
Semantic Expert Using an Online Standard Dic-
tionary&amp;quot;, Proc. !JCA!-87, 709-714
9. Jensen, Karen and Wlodzimierz Zadrozny 1987.
The Semantics of Paragraphs&amp;quot;, presented at Logic
and Linguistics, Stanford, July 1987.
10. Maxwell, B.D and F. D. Tuggle 1975. &amp;quot;Toward
a Natural Language Question Answering Facility&amp;quot;,
Am. J. Comp. Ling., Microfiche 61.
11. Nilsson, Nils J. 1980. Principles of Artificial In-
telligence, Tioga Publishing Co.
12. Robinson, Jane J. 1982. &amp;quot;DIAGRAM: A Gram-
mar for Dialogues&amp;quot;, Comm. ACM Vol 25 No 1,
27-47
13. Schubert, Klaus 1986. &amp;quot;Linguistic and Extra-
Linguistic Knowledge Computers and Translation
Vol 1, No 3, July-September 1986
14. Schubert, Lenhart K. 1986. &amp;quot;Are There Preference
Trade-offs in Attachment Decisions&amp;quot; , Proc.
AAAI-86, 601-605
15. Tomita, Masan&apos; 1984. &amp;quot;Disambiguating Gram-
matically Ambiguous Sentences by Asking&amp;quot; , Proc.
COLING 84
16. Tomita, Masaru 1985. An Efficient Context-Free
Parsing Algorithm for Natural Languages&amp;quot;, Proc.
IJCAI-85,756-763
17. Walker, Donald E. and William II. Paxton with
Gary G. Ilendrix, Ann E. Robinson, Jane J. Rob-
inson, Jonathan Slocum 1977. &amp;quot;Procedures for
Integrating Knowledge in a Speech Understanding
System&amp;quot;, SRI Technical Note 143.
18. Waltz, David L. and J. B. Pollack 1985. &amp;quot;Mas-
sively Parallel Parsing: A Strongly Interactive
Model of Natural Language Interpretation&amp;quot;, Cog-
nitive Science Vol 9,No 1, January-March 1985,
51-74
19. Wilks, Yorick, Xiuming Huang, and Dan Fass
1985. &amp;quot;Syntax, Preference and Right Attachment&amp;quot;,
Proc. IJCA I-85 779-784
20. Wilks, Yorick 1975. &amp;quot;An Intelligent Analyzer and
Understander of English&amp;quot;, Comm. ACM, Vol 18
No 5, 264-274
21. Wittenhurg, Kent 1986. &amp;quot;A Parser for Portable
NI. Interfaces Using Graph-Unification-Based
Grammars&amp;quot;, Proc. AA A 1-86, 1053-1058
</reference>
<sectionHeader confidence="0.451695" genericHeader="method">
9. Appendix: Search Algorithm
</sectionHeader>
<bodyText confidence="0.511275">
Figure 6 describes the step by step application of A*
to searching semantic choices.
</bodyText>
<page confidence="0.98995">
251
</page>
<bodyText confidence="0.6505635">
Assume an &amp;quot;open list&amp;quot; containing, for each node n with an unexamined
child, the following information:
</bodyText>
<sectionHeader confidence="0.541698" genericHeader="method">
1. The list of choices made on the path up &apos;Co and including n.
2. The set of constraints on word senses imposed by nodes on the path.
3. The index of the highest weighted unexamined child
</sectionHeader>
<bodyText confidence="0.418227142857143">
(choice at next level) of n, where choices within levels are sorted by
descending weight.
4. The potential Fc = An + Plc + He for that child, where An is the
accumulated weight on the path up to and including n, We is the
weight of the child, and Hc is the upper bound on the cumulative
potential for paths below the child.
Than the following algorithm is used to search the tree.
</bodyText>
<listItem confidence="0.554356">
1. Put an entry for the dummy &amp;quot;top&amp;quot; node in the open list,
with path=self, index of first child, and its potential Fc.
2. Find the node n in the &amp;quot;open list&amp;quot; which has the highest
potential Fc for a child node.
3. If a full path has already been found, and Fc is lower than the
total weight for that path, the search is over.
4. Otherwise check the consistency of the designated child in
entry n. If it is consistent, add an open list entry for the child,
including a new, more constrained consistency requirement.
5. Whether or not the designated child is consistent, determine
if there are any unexamined children of node n. If so, modify
entry n accordingly. Otherwise remove entry n from the open list.
6. If there is a new entry, and it represents a completed path,
remove it from the open list and perform additional consistency
checks. If the checks fail, ignore the new path. If they succeed,
record the path and its score as a competing alternative.
7. Return to Step 2.
</listItem>
<figureCaption confidence="0.901621">
Figure 6: Search Algorithm
</figureCaption>
<page confidence="0.992374">
252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975678">
<title confidence="0.998767">COMBINATORIAL DISAMBIGUATION</title>
<author confidence="0.999976">S Newman</author>
<affiliation confidence="0.999881">IBM Los Angeles Scientific Center</affiliation>
<address confidence="0.996428">11601 Wilshire Boulevard Los Angeles, CA 90025-1738</address>
<abstract confidence="0.998662666666667">The disambiguation of sentences is a combinatorial problem. This paper describes a method for treating it as such, directly, by adapting standard combinatorial search optimizations. Traditional disambiguation heuristics are applied but, instead of being embedded in individual decision procedures for specific types of ambiguities, they contribute to numerical weights that are considered by a single global optimizer. The result is increased power and simpler code. The method is being implemented for a machine translation project, but could be adapted to any natural language system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Madeleine Bates</author>
</authors>
<title>Syntax in Automatic Speech Understanding&amp;quot;,</title>
<date>1976</date>
<journal>Am. J. Comp. Ling. Microfiche</journal>
<volume>45</volume>
<contexts>
<context position="892" citStr="(1)" startWordPosition="126" endWordPosition="126">atorial search optimizations. Traditional disambiguation heuristics are applied but, instead of being embedded in individual decision procedures for specific types of ambiguities, they contribute to numerical weights that are considered by a single global optimizer. The result is increased power and simpler code. The method is being implemented for a machine translation project, but could be adapted to any natural language system. 1. Introduction The disambiguation of sentences is a combinatorial problem. Identification of one word sense interacts with the identification of other word senses, (1) Ile addressed the chair and with constituent attachment, (2) He shot some bucks with a rifle Moreover, the attachment of one constituent interacts with the attachment of other constituents: (3) She put the vase on the table in the living room This paper describes a method of addressing the problem directly, by adapting standard search optimization techniques. In the first section we describe the core of the method, which applies a version of best-first search to a uniform representation of the set of possibilities. In the second section we relate the work to other approaches to preference-bas</context>
<context position="17283" citStr="(1)" startWordPosition="2750" endWordPosition="2750">ied by Charniak (1986), Cottrell and Small (1984), and Waltz and Pollack (1985). These approaches are still in the experimental stage, and are primarily intended for parallel hardware, while the A* algorithm used in this paper is designed for conventional serial hardware. But, in a sense, these approaches reinforce the main point of this paper: they argue for a single global technique for optimized combinatorial disambiguation based on all available information. 4. Preparing Semantic Choices having described how the choice points are used, we address their development. Two steps are involved: (1) the development of syntactic choice points, and (2) the development of semantic choice points. The first step transforms the parse-level syntactic functions into a form appropriate to the second step, which is the application of the lexicon to those functions to obtain the semantic alternatives. In our example, the first step is a simple one. Syntactic relationships among constituents are transformed into syntactic relationships among head words, and the syntactic relationships are refined, so that &amp;quot;ppmod&amp;quot; is replaced by the actual prepositions used. &apos;the result of this step is shown in Figur</context>
</contexts>
<marker>1.</marker>
<rawString>Bates, Madeleine 1976. &amp;quot;Syntax in Automatic Speech Understanding&amp;quot;, Am. J. Comp. Ling. Microfiche 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Neat Theory of Marker Passing&amp;quot;</title>
<date>1986</date>
<booktitle>Proc. AAAI-86</booktitle>
<pages>584--588</pages>
<contexts>
<context position="953" citStr="(2)" startWordPosition="135" endWordPosition="135">istics are applied but, instead of being embedded in individual decision procedures for specific types of ambiguities, they contribute to numerical weights that are considered by a single global optimizer. The result is increased power and simpler code. The method is being implemented for a machine translation project, but could be adapted to any natural language system. 1. Introduction The disambiguation of sentences is a combinatorial problem. Identification of one word sense interacts with the identification of other word senses, (1) Ile addressed the chair and with constituent attachment, (2) He shot some bucks with a rifle Moreover, the attachment of one constituent interacts with the attachment of other constituents: (3) She put the vase on the table in the living room This paper describes a method of addressing the problem directly, by adapting standard search optimization techniques. In the first section we describe the core of the method, which applies a version of best-first search to a uniform representation of the set of possibilities. In the second section we relate the work to other approaches to preference-based disambiguation. The final sections describe how the repres</context>
<context position="3249" citStr="(2)" startWordPosition="481" endWordPosition="481">f semantic choice points. Each choice point generally represents a constituent. It contains a group of weighted semantic alternatives that represent the different ways in which word-senses of the constituent head can be associated semantically with word-senses of a higher level head. This allows word-sense and attachment alternatives to be treated uniformly. Combinatorial disambiguation then selects the consistent combination of alternatives, one from each choice point, that yields the highest total weight. To illustrate the method, we use the extension of the classic example mentioned above: (2) Ile shot some bucks with a rifle A decomposition of the sentence into choice points cl , c2, and c3 is shown in Figure I. (The illustration 243 assumes that &amp;quot;shot&amp;quot; and &amp;quot;bucks&amp;quot; have two meanings each, and ignores determiners.) Choice point &amp;quot;cl&amp;quot; gives the alternative syntactic and semantic possibilities for the attachment of the constituent &amp;quot;he&amp;quot; to its only possible head &amp;quot;shot&amp;quot;. Alternative el 1 is that &amp;quot;he&amp;quot; is the subject of &amp;quot;shootr, with the semantic function &amp;quot;agent&amp;quot;, and is given (for reasons which will be discussed later) the weight &amp;quot;3&amp;quot;. Alternative c12 is similar, but the meaning used for </context>
<context position="17335" citStr="(2)" startWordPosition="2758" endWordPosition="2758">nd Waltz and Pollack (1985). These approaches are still in the experimental stage, and are primarily intended for parallel hardware, while the A* algorithm used in this paper is designed for conventional serial hardware. But, in a sense, these approaches reinforce the main point of this paper: they argue for a single global technique for optimized combinatorial disambiguation based on all available information. 4. Preparing Semantic Choices having described how the choice points are used, we address their development. Two steps are involved: (1) the development of syntactic choice points, and (2) the development of semantic choice points. The first step transforms the parse-level syntactic functions into a form appropriate to the second step, which is the application of the lexicon to those functions to obtain the semantic alternatives. In our example, the first step is a simple one. Syntactic relationships among constituents are transformed into syntactic relationships among head words, and the syntactic relationships are refined, so that &amp;quot;ppmod&amp;quot; is replaced by the actual prepositions used. &apos;the result of this step is shown in Figure 3. The development of syntactic choice points for </context>
</contexts>
<marker>2.</marker>
<rawString>Charniak, Eugene 1986. &amp;quot;A Neat Theory of Marker Passing&amp;quot; Proc. AAAI-86 584-588</rawString>
</citation>
<citation valid="true">
<authors>
<author>Garrison W Cottrell</author>
<author>Steven L Small</author>
</authors>
<title>Viewing Parsing as Word Sense Discrimination: A Connectionist Approach&amp;quot;,</title>
<date>1984</date>
<booktitle>in B.G. Bara and G. Guida (eds), Computation Models of Natural Language Processing,</booktitle>
<publisher>Elsevier Science Publishers B.V.</publisher>
<contexts>
<context position="1086" citStr="(3)" startWordPosition="156" endWordPosition="156">e to numerical weights that are considered by a single global optimizer. The result is increased power and simpler code. The method is being implemented for a machine translation project, but could be adapted to any natural language system. 1. Introduction The disambiguation of sentences is a combinatorial problem. Identification of one word sense interacts with the identification of other word senses, (1) Ile addressed the chair and with constituent attachment, (2) He shot some bucks with a rifle Moreover, the attachment of one constituent interacts with the attachment of other constituents: (3) She put the vase on the table in the living room This paper describes a method of addressing the problem directly, by adapting standard search optimization techniques. In the first section we describe the core of the method, which applies a version of best-first search to a uniform representation of the set of possibilities. In the second section we relate the work to other approaches to preference-based disambiguation. The final sections describe how the representation may be obtained from a lexicon. 2. The Search Method In the machine translation project for which this technique is being de</context>
<context position="4142" citStr="(3)" startWordPosition="628" endWordPosition="628">possibilities for the attachment of the constituent &amp;quot;he&amp;quot; to its only possible head &amp;quot;shot&amp;quot;. Alternative el 1 is that &amp;quot;he&amp;quot; is the subject of &amp;quot;shootr, with the semantic function &amp;quot;agent&amp;quot;, and is given (for reasons which will be discussed later) the weight &amp;quot;3&amp;quot;. Alternative c12 is similar, but the meaning used for &amp;quot;shoot&amp;quot; is &amp;quot;shoot2&amp;quot;. Similar alternatives are used for the attachment (c2) of the object &amp;quot;some bucks&amp;quot; to its only possible head, &amp;quot;shoot&amp;quot;. Alternative c23 represents the unlikely combinations. Choice point c3 reflects the different possible attachments of &amp;quot;with a rifle&amp;quot;; the highest weight (3) is given to its attachmcnt as an instrumental modifier of &amp;quot;shootl&amp;quot;. The other possibilities range from barely plausible to implausible and are weighted accordingly. I laving obtained this repesentation (whose construction is described in later sections), the next step is to establish the single-alternative choice points as given and to propagate any associated word-sense constraints to narrow or eliminate other alternatives. (This does not occur in our example.) Then combinations of the remaining choices are searched using a variation of the A* best-first search method. See Nilsson (1980) for</context>
</contexts>
<marker>3.</marker>
<rawString>Cottrell, Garrison W. and Steven L. Small 1984. &amp;quot;Viewing Parsing as Word Sense Discrimination: A Connectionist Approach&amp;quot;, in B.G. Bara and G. Guida (eds), Computation Models of Natural Language Processing, Elsevier Science Publishers B.V.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen Dahlgren</author>
<author>Joyce McDowell</author>
</authors>
<title>Using Commonsense Knowledge to Disambiguate Prepositional Phrase Modifiers&amp;quot;,</title>
<date>1986</date>
<booktitle>Proc. AAAI-86,</booktitle>
<pages>589--593</pages>
<contexts>
<context position="9343" citStr="(4)" startWordPosition="1514" endWordPosition="1514">nd path. A more precise description of the algorithm is given in the Appendix. When more than one path is found with the same high score, additional tests are applied. These tests include comparisons of surface proximity and, as this work is situated within a multi-target translation system, user queries in the source language, as outlined by Tomita (1985). An extended version of the method is used in comparing alternate parses which differ in constituent composition, and thus are more easily analyzed as different parse trees, each with its own set of choice points. An example is the classic: (4) Time flies like an arrow (where the main verb can be any one of the first three words). In such cases, one set of choice points is constructed per parse tree. In general, the search alternates among trees, with the next node to be added being that with the greatest potential across trees. If such trees always had the same number of choice points, this would he the only revision needed. However, the number of choice points may differ, for one thing because the parser may have detected and condensed non-compositional compounds (e.g., &amp;quot;all the same&amp;quot;) in one parse but not in another. For this rea</context>
</contexts>
<marker>4.</marker>
<rawString>Dahlgren, Kathleen and Joyce McDowell 1986. &amp;quot;Using Commonsense Knowledge to Disambiguate Prepositional Phrase Modifiers&amp;quot;, Proc. AAAI-86, 589-593</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Heidorn</author>
</authors>
<title>Experience with an Easily Computed Metric for Ranking Alternative Parses&amp;quot;</title>
<date>1982</date>
<booktitle>Proc. 20th Annual Meeting of the ACL,</booktitle>
<marker>5.</marker>
<rawString>Heidorn, George 1982. &amp;quot;Experience with an Easily Computed Metric for Ranking Alternative Parses&amp;quot; Proc. 20th Annual Meeting of the ACL, June 1982</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Semantic Interpretation Against Ambiguity&amp;quot;,</title>
<date>1983</date>
<tech>Technical Report CS-83-25,</tech>
<institution>Brown University,</institution>
<contexts>
<context position="20503" citStr="(6)" startWordPosition="3245" endWordPosition="3245">signed symbolically, to allow experimentation. Current settings are as follows: • Rules for idioms (e.g., kick the bucket), 4. • Rules for more general selectional preferences, 3. • Rules for acceptable but not preferred alternatives (e.g., locative prepositional phrases attached to arbitrary actions), 2. • Very general attachments (e.g., &amp;quot;nounmod nounl noun2), 0. These allow for uninterpreted metaphoric usage.s One major objective in assigning weights to ensure that combinations of very high (idiom) weights together with very low weights do not outscore more balanced combinations. Thus, for: (6) He kicked the ugly bucket weights such as: subj he kickedl 3 obj bucketl kickedl 4 adjm ugly bucket1 0 subj he kicked2 3 obi bucket2 kicked2 3 adjm ugly bucket2 2 provide the necessary balance. (Here kicked&apos; is the idiomatic interpretation, and bucket&apos; is a word-sense of bucket used only in that interpretation.) By convention, rules for syntactic functions are assigned, by class, to entries for specific kinds of concepts. Thus rules for verb-attached arguments or prepositional phrases arc stored with verbs or verb classes. Adjective-noun rules are generally associated with adjectives, and nou</context>
</contexts>
<marker>6.</marker>
<rawString>Hirst, Graeme 1983. &amp;quot;Semantic Interpretation Against Ambiguity&amp;quot;, Technical Report CS-83-25, Brown University, December 1983</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Jensen</author>
</authors>
<title>Parsing Strategies in a Broad-coverage Grammar of English&amp;quot;,</title>
<date>1986</date>
<journal>IBM Research Report RC</journal>
<volume>12147</volume>
<contexts>
<context position="23143" citStr="(7)" startWordPosition="3641" endWordPosition="3641">9/k() obtains attachment preference information by parsing dictionary definitions. 248 syntactic choice identification step is to re-express, or &amp;quot;normalize&amp;quot; input syntactic relationsips in terms of the relationships assumed by the lexicon. For example, passive constructions such as: and the hi&apos;s are senses of the headword. The semantic relationship is stated to apply to all combinations of word-senses in the cross-products of those lists. For the example sentence, this process would obtain essentially the alternatives shown in Figure 1, except that alternative c23 would first be expressed as: (7) The bucks were shot with a rifle obj ( buckl ,buck2)(shootl , shoot 2 1 1 0 The last step in the process reduces this result. If some of the word-sense combinations are also found in an alternative of higher weight, the &amp;quot;dominated&amp;quot; combinations are deleted. And if all word sense combinations are so dominated, the alternative is deleted. In this way alternative c23 is reduced to its final form. After the semantic choice point list is completed, the search algorithm is applied as described above. 5. Preparing Syntactic Choices In the example above, the preparation of syntactic choice points fro</context>
</contexts>
<marker>7.</marker>
<rawString>Jensen, Karen 1986. &amp;quot;Parsing Strategies in a Broad-coverage Grammar of English&amp;quot;, IBM Research Report RC 12147, 1986</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Jensen</author>
<author>Jean-Louis Binot</author>
</authors>
<title>A Semantic Expert Using an Online Standard Dictionary&amp;quot;,</title>
<date>1987</date>
<booktitle>Proc. !JCA!-87,</booktitle>
<pages>709--714</pages>
<contexts>
<context position="26298" citStr="(8)" startWordPosition="4133" endWordPosition="4133">part of the normalized choice. The additional rule is used to ensure that the &amp;quot;subject&amp;quot; function is retained only for the active voice. Additional applications of these transformations include those for modifiers of nominalized verbs, attributive clauses, and relative clauses. Input Syn Chptl Normalized Syn Chptl Semantic Chptl Choice Cll Choice C111 Choice C1111 Choice C1112 Choice C112 Choice C1121 Choice C1122 Figure 5: Steps in Semantic Choice Point Derivation 249 Noun phrases whose heads are nominalized verbs are addressed by adding choice points corresponding to verb arguments. Thus for (8) The bucks&apos; shooting the alternative &amp;quot;nounmod bucks shooting&amp;quot; is expanded to include the alternatives &amp;quot;subj bucks shooting&amp;quot; and &amp;quot;obj bucks shooting&amp;quot;. Then, during lexical processing, rules for word-senses of the noun &amp;quot;shooting&amp;quot; having an associated verb are understood as expanded to include the expected verb arguments. Attributive clauses such as: (9) The bucks were handsome. are transformed to allow the application of adjective information. !fere &amp;quot;obj handsome were is transformed to &amp;quot;adjmod handsome bucks&amp;quot;. For relative clauses, the core of the transformation expresses alternative attachments</context>
</contexts>
<marker>8.</marker>
<rawString>Jensen, Karen and Jean-Louis Binot 1987. &amp;quot;A Semantic Expert Using an Online Standard Dictionary&amp;quot;, Proc. !JCA!-87, 709-714</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Jensen</author>
<author>Wlodzimierz Zadrozny</author>
</authors>
<title>The Semantics of Paragraphs&amp;quot;, presented at Logic and Linguistics,</title>
<date>1987</date>
<location>Stanford,</location>
<contexts>
<context position="26651" citStr="(9)" startWordPosition="4184" endWordPosition="4184">Choice C1111 Choice C1112 Choice C112 Choice C1121 Choice C1122 Figure 5: Steps in Semantic Choice Point Derivation 249 Noun phrases whose heads are nominalized verbs are addressed by adding choice points corresponding to verb arguments. Thus for (8) The bucks&apos; shooting the alternative &amp;quot;nounmod bucks shooting&amp;quot; is expanded to include the alternatives &amp;quot;subj bucks shooting&amp;quot; and &amp;quot;obj bucks shooting&amp;quot;. Then, during lexical processing, rules for word-senses of the noun &amp;quot;shooting&amp;quot; having an associated verb are understood as expanded to include the expected verb arguments. Attributive clauses such as: (9) The bucks were handsome. are transformed to allow the application of adjective information. !fere &amp;quot;obj handsome were is transformed to &amp;quot;adjmod handsome bucks&amp;quot;. For relative clauses, the core of the transformation expresses alternative attachments of the relative clause as alternative connections between the head of the relative clause, and the possible fillers of the gap position. (Relative clauses with multiple gaps are generally handled in separate parses.) Thus for: (/0) The rifle above the mantle that the bucks were shot with... transformations produce the alternatives with shoot rifle an</context>
</contexts>
<marker>9.</marker>
<rawString>Jensen, Karen and Wlodzimierz Zadrozny 1987. The Semantics of Paragraphs&amp;quot;, presented at Logic and Linguistics, Stanford, July 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B D Maxwell</author>
<author>F D Tuggle</author>
</authors>
<title>Toward a Natural Language Question Answering Facility&amp;quot;,</title>
<date>1975</date>
<journal>Am. J. Comp. Ling., Microfiche</journal>
<volume>61</volume>
<marker>10.</marker>
<rawString>Maxwell, B.D and F. D. Tuggle 1975. &amp;quot;Toward a Natural Language Question Answering Facility&amp;quot;, Am. J. Comp. Ling., Microfiche 61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils J Nilsson</author>
</authors>
<date>1980</date>
<journal>Principles of Artificial Intelligence, Tioga Publishing Co.</journal>
<contexts>
<context position="14602" citStr="(11)" startWordPosition="2336" endWordPosition="2336">/scripts, and salience in the current context. The multiple evolving parse trees are rated by summing their contained weights, and the combinatorial problem is controlled by retaining only the two highest scoring parses of any complete phrases. This approach is interesting, although some details are vague3. However, the post-parse application of A* described in this paper obtains the benefits of such a within-parse approach without its deficiences in that: (a) combinatorial computations of weights and wordsense consistencies are avoided except when warranted by total sentence information, and (11) there is no possibility of early erroneous discarding of alternatives. 2 Heidom (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. 3 No examples are given, so it is unclear whether a parse for a phrase or part thereof represents only one interpretation, or all interpretations having the same structure, scored by the most likely interpretation. The former is obviously inadequate (e.g., for highly ambiguous subject NPs like &apos;The stands&apos;), while the latter seems to require ei</context>
</contexts>
<marker>11.</marker>
<rawString>Nilsson, Nils J. 1980. Principles of Artificial Intelligence, Tioga Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>DIAGRAM: A Grammar for Dialogues&amp;quot;,</title>
<date>1982</date>
<journal>Comm. ACM Vol</journal>
<volume>25</volume>
<pages>27--47</pages>
<marker>12.</marker>
<rawString>Robinson, Jane J. 1982. &amp;quot;DIAGRAM: A Grammar for Dialogues&amp;quot;, Comm. ACM Vol 25 No 1, 27-47</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Schubert</author>
</authors>
<title>Linguistic and ExtraLinguistic</title>
<date>1986</date>
<journal>Knowledge Computers and Translation</journal>
<volume>1</volume>
<contexts>
<context position="8375" citStr="(13)" startWordPosition="1347" endWordPosition="1347">oot!&apos;, which has the highest potential. At that point, the highest weighted consistent alternative is c21, etc. While the set of choice points implies that there are (4 x 3 x 2) = 24 paths to be searched, only one is pursued to any distance. Thus while the approach takes a combinatorial view of the problem, it does so without loss of efficiency. When a full path is found, it is examined for semantic consistency (beyond word-sense consistency). The checks made include: (a) ensuring that the interpretation includes all required semantic functions for a word-sense (specified in the lexicon), and (13) ensuring that non-repeatable functions (e.g., the goal of an action) are not duplicated. Even if the full path is found to be consistent, the search does not terminate immediately, but continues until it is certain that no other path can have an equal score. This will be true whenever the maximum potential for open nodes is less than the score for an already-found path. A more precise description of the algorithm is given in the Appendix. When more than one path is found with the same high score, additional tests are applied. These tests include comparisons of surface proximity and, as this w</context>
</contexts>
<marker>13.</marker>
<rawString>Schubert, Klaus 1986. &amp;quot;Linguistic and ExtraLinguistic Knowledge Computers and Translation Vol 1, No 3, July-September 1986</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart K Schubert</author>
</authors>
<title>Are There Preference Trade-offs in Attachment Decisions&amp;quot;</title>
<date>1986</date>
<booktitle>Proc. AAAI-86,</booktitle>
<pages>601--605</pages>
<marker>14.</marker>
<rawString>Schubert, Lenhart K. 1986. &amp;quot;Are There Preference Trade-offs in Attachment Decisions&amp;quot; , Proc. AAAI-86, 601-605</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masan&apos; Tomita</author>
</authors>
<title>Disambiguating Grammatically Ambiguous Sentences by Asking&amp;quot; ,</title>
<date>1984</date>
<booktitle>Proc. COLING</booktitle>
<volume>84</volume>
<marker>15.</marker>
<rawString>Tomita, Masan&apos; 1984. &amp;quot;Disambiguating Grammatically Ambiguous Sentences by Asking&amp;quot; , Proc. COLING 84</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm for Natural Languages&amp;quot;,</title>
<date>1985</date>
<booktitle>Proc.</booktitle>
<pages>85--756</pages>
<marker>16.</marker>
<rawString>Tomita, Masaru 1985. An Efficient Context-Free Parsing Algorithm for Natural Languages&amp;quot;, Proc. IJCAI-85,756-763</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paxton with Gary G Ilendrix</author>
<author>Ann E Robinson</author>
<author>Jane J Robinson</author>
</authors>
<title>Procedures for Integrating Knowledge in a Speech Understanding System&amp;quot;,</title>
<date>1977</date>
<tech>SRI Technical Note 143.</tech>
<location>Jonathan Slocum</location>
<marker>17.</marker>
<rawString>Walker, Donald E. and William II. Paxton with Gary G. Ilendrix, Ann E. Robinson, Jane J. Robinson, Jonathan Slocum 1977. &amp;quot;Procedures for Integrating Knowledge in a Speech Understanding System&amp;quot;, SRI Technical Note 143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Waltz</author>
<author>J B Pollack</author>
</authors>
<title>Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation&amp;quot;, Cognitive Science Vol 9,No 1, January-March</title>
<date>1985</date>
<pages>51--74</pages>
<marker>18.</marker>
<rawString>Waltz, David L. and J. B. Pollack 1985. &amp;quot;Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation&amp;quot;, Cognitive Science Vol 9,No 1, January-March 1985, 51-74</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Xiuming Huang,</title>
<location>and Dan Fass</location>
<marker>19.</marker>
<rawString>Wilks, Yorick, Xiuming Huang, and Dan Fass</rawString>
</citation>
<citation valid="false">
<title>Syntax, Preference and Right Attachment&amp;quot;,</title>
<booktitle>Proc. IJCA I-85</booktitle>
<pages>779--784</pages>
<contexts>
<context position="2509" citStr="(1985)" startWordPosition="370" endWordPosition="370">rds are disambiguated up to part-of-speech, but word senses are not identified. Individual parses may indicate alternative attachments of many kinds of constituents including prepositional phrases and relative clauses. Beginning disambiguation only after a general parse has the advantage of making clear what all the possibilities are, thus allowing their investigation in an efficient order. Performing a full parse before disambiguation need not consume an inordinate amount of space or time; techniques such as those used by Jensen (default rightmost prepositional phrase attachment), and Tomita (1985) (parse forests) adequately control resource requirements. The parser output is first transformed so that it is represented as a set of of semantic choice points. Each choice point generally represents a constituent. It contains a group of weighted semantic alternatives that represent the different ways in which word-senses of the constituent head can be associated semantically with word-senses of a higher level head. This allows word-sense and attachment alternatives to be treated uniformly. Combinatorial disambiguation then selects the consistent combination of alternatives, one from each ch</context>
<context position="9098" citStr="(1985)" startWordPosition="1471" endWordPosition="1471">und to be consistent, the search does not terminate immediately, but continues until it is certain that no other path can have an equal score. This will be true whenever the maximum potential for open nodes is less than the score for an already-found path. A more precise description of the algorithm is given in the Appendix. When more than one path is found with the same high score, additional tests are applied. These tests include comparisons of surface proximity and, as this work is situated within a multi-target translation system, user queries in the source language, as outlined by Tomita (1985). An extended version of the method is used in comparing alternate parses which differ in constituent composition, and thus are more easily analyzed as different parse trees, each with its own set of choice points. An example is the classic: (4) Time flies like an arrow (where the main verb can be any one of the first three words). In such cases, one set of choice points is constructed per parse tree. In general, the search alternates among trees, with the next node to be added being that with the greatest potential across trees. If such trees always had the same number of choice points, this </context>
<context position="10969" citStr="(1985)" startWordPosition="1780" endWordPosition="1780"> described here, which is more appropriate to our situation (and also mentioned by Nilsson), adds a single node at each step. 245 3. Related Work implications of the results of other kinds of decision procedures. There seems to be little work which directly addresses the combinatorial problem. First, there is considerable work in preference-related disambiguation that assumes, at least for purposes of discussion, that individual disambiguation problems can be addressed in isolation. For example, treatments of prepositional phrase attachment by by Dahlgren and McDowell (1986) and Wilks et. al. (1985) propose methods of finding the &amp;quot;best&amp;quot; resolution of a single attachment problem by finding the first preference which is satisfied in some recommended order of rule application. Other types of ambiguity, and other instances of the same type, are assumed to have been resolved. This type of work contributes interesting insights, but cannot be used directly. One type of more realistic treatment, which might be called the deferred decision approach, is exemplified by !first (1983). When, in the course of a parse, an immediate decision about a word sense or attachment cannot be made, a set of alte</context>
<context position="16759" citStr="(1985)" startWordPosition="2671" endWordPosition="2671">ives in schemes which combine syntactic and semantic disambiguation is rarely discussed, although maintaining a consistent representation of the relationships among word-sense and attachment alternatives is fundamental to a systematic treatment of the problem. An exception is the discussion by K. Schubert (1986), who describes a representation for alternatives with some affinities to that described here. The information limitations of disambiguation during parsing are not found in spreading-activation approaches, exemplified by Charniak (1986), Cottrell and Small (1984), and Waltz and Pollack (1985). These approaches are still in the experimental stage, and are primarily intended for parallel hardware, while the A* algorithm used in this paper is designed for conventional serial hardware. But, in a sense, these approaches reinforce the main point of this paper: they argue for a single global technique for optimized combinatorial disambiguation based on all available information. 4. Preparing Semantic Choices having described how the choice points are used, we address their development. Two steps are involved: (1) the development of syntactic choice points, and (2) the development of sema</context>
</contexts>
<marker>1985.</marker>
<rawString>&amp;quot;Syntax, Preference and Right Attachment&amp;quot;, Proc. IJCA I-85 779-784</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>An Intelligent Analyzer and Understander of English&amp;quot;,</title>
<date>1975</date>
<journal>Comm. ACM, Vol</journal>
<volume>18</volume>
<pages>264--274</pages>
<marker>20.</marker>
<rawString>Wilks, Yorick 1975. &amp;quot;An Intelligent Analyzer and Understander of English&amp;quot;, Comm. ACM, Vol 18 No 5, 264-274</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kent Wittenhurg</author>
</authors>
<title>A Parser for Portable NI. Interfaces Using Graph-Unification-Based Grammars&amp;quot;,</title>
<date>1986</date>
<booktitle>Proc. AA A</booktitle>
<pages>1--86</pages>
<marker>21.</marker>
<rawString>Wittenhurg, Kent 1986. &amp;quot;A Parser for Portable NI. Interfaces Using Graph-Unification-Based Grammars&amp;quot;, Proc. AA A 1-86, 1053-1058</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>