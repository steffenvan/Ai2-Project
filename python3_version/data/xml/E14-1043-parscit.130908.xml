<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.990529">
Predicting and Characterising User Impact on Twitter
</title>
<author confidence="0.998913">
Vasileios Lampos1, Nikolaos Aletras2, Daniel Preot¸iuc-Pietro2 and Trevor Cohn3
</author>
<affiliation confidence="0.907430333333333">
1 Department of Computer Science, University College London
2 Department of Computer Science, University of Sheffield
3 Computing and Information Systems, The University of Melbourne
</affiliation>
<email confidence="0.98609">
v.lampos@ucl.ac.uk, {n.aletras,d.preotiuc}@dcs.shef.ac.uk, trevor.cohn@gmail.com
</email>
<sectionHeader confidence="0.993896" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874210526316">
The open structure of online social net-
works and their uncurated nature give rise
to problems of user credibility and influ-
ence. In this paper, we address the task of
predicting the impact of Twitter users based
only on features under their direct control,
such as usage statistics and the text posted
in their tweets. We approach the problem as
regression and apply linear as well as non-
linear learning methods to predict a user
impact score, estimated by combining the
numbers of the user’s followers, followees
and listings. The experimental results point
out that a strong prediction performance is
achieved, especially for models based on
the Gaussian Processes framework. Hence,
we can interpret various modelling com-
ponents, transforming them into indirect
‘suggestions’ for impact boosting.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964355932204">
Online social networks have become a wide spread
medium for information dissemination and inter-
action between millions of users (Huberman et al.,
2009; Kwak et al., 2010), turning, at the same
time, into a popular subject for interdisciplinary
research, involving domains such as Computer Sci-
ence (Sakaki et al., 2010), Health (Lampos and
Cristianini, 2012) and Psychology (Boyd et al.,
2010). Open access along with the property of struc-
tured content retrieval for publicly posted data have
brought the microblogging platform of Twitter into
the spotlight.
Vast quantities of human-generated text from
a range of themes, including opinions, news and
everyday activities, spread over a social network.
Naturally, issues arise, like user credibility (Castillo
et al., 2011) and content attractiveness (Suh et al.,
2010), and quite often trustful or appealing informa-
tion transmitters are identified by an impact assess-
ment.1 Intuitively, it is expected that user impact
cannot be defined by a single attribute, but depends
on multiple user actions, such as posting frequency
and quality, interaction strategies, and the text or
topics of the written communications.
In this paper, we start by predicting user impact
as a statistical learning task (regression). For that
purpose, we firstly define an impact score function
for Twitter users driven by basic account proper-
ties. Afterwards, from a set of accounts, we mea-
sure several publicly available attributes, such as
the quantity of posts or interaction figures. Textual
attributes are also modelled either by word frequen-
cies or, more generally, by clusters of related words
which quantify a topic-oriented participation. The
main hypothesis being tested is whether textual
and non textual attributes encapsulate patterns that
affect the impact of an account.
To model this data, we present a method based
on nonlinear regression using Gaussian Processes,
a Bayesian non-parametric class of methods (Ras-
mussen and Williams, 2006), proven more effec-
tive in capturing the multimodal user features. The
modelling choice of excluding components that
are not under an account’s direct control (e.g. re-
ceived retweets) combined with a significant user
impact prediction performance (r = .78) enabled
us to investigate further how specific aspects of a
user’s behaviour relate to impact, by examining the
parameters of the inferred model.
Among our findings, we identify relevant fea-
tures for this task and confirm that consistent ac-
tivity and broad interaction are deciding impact
factors. Informativeness, estimated by computing
a joint user-topic entropy, contributes well to the
separation between low and high impact accounts.
Use case scenarios based on combinations of fea-
tures are also explored, leading to findings such as
that engaging about ‘serious’ or more ‘light’ topics
may not register a differentiation in impact.
</bodyText>
<footnote confidence="0.997887">
1For example, the influence assessment metric of Klout —
http://www.klout.com.
</footnote>
<page confidence="0.950001">
405
</page>
<note confidence="0.9943675">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 405–413,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.995241153846154">
Probability Density
0.15
0.05
0.1
0
−5 0 5 10 15 20 25 30
@spam?
@guardian
@David_Cameron
@PaulMasonNews
@lampos
@nikaletras
Impact Score (S)
</figure>
<figureCaption confidence="0.86757">
Figure 1: Histogram of the user impact scores in
our data set. The solid black line represents a gen-
</figureCaption>
<bodyText confidence="0.970561142857143">
eralised extreme value probability distribution fit-
ted in our data, and the dashed line denotes the
mean impact score (= 6.776). User @spam? is a
sample account with φin = 10, φout = 1000 and
φa = 0; @lampos is a very active account, whereas
@nikaletras is a regular user.
interest. Indeed, Pearson’s correlation between φin
and φa for all the accounts in our data set is equal
to .765 (p &lt; .001); the two metrics are correlated,
but not entirely and on those grounds, it would be
reasonable to use both for quantifying impact.
Consequently, we have chosen to represent user
impact (S) as a log function of the number of fol-
lowers, followees and listings, given by
</bodyText>
<equation confidence="0.988722666666667">
S(φin, φout, φa) = ln (φa + θ) (φin + θ)2
φout + θ ) ,
(1)
</equation>
<bodyText confidence="0.999762875">
where θ is a smoothing constant set equal to 1 so
that the natural logarithm is always applied on a
real positive number. Figure 1 shows the impact
score distribution for all the users in our sample,
including some pointers to less or more popular
Twitter accounts. The depicted user impact scores
form the response variable in the regression models
presented in the following sections.
</bodyText>
<sectionHeader confidence="0.919599" genericHeader="introduction">
4 User Account Features
</sectionHeader>
<bodyText confidence="0.9996776">
This section presents the features used in the user
impact prediction task. They are divided into two
categories: non-textual and text-based. All features
have the joint characteristic of being under the
user’s direct control, something essential for char-
acterising impact based on the actions of a user.
Attributes such as the number of received retweets
or @-mentions (of a user in the tweets of others)
were not considered as they are not controlled by
the account itself.
</bodyText>
<sectionHeader confidence="0.979814" genericHeader="method">
2 Data
</sectionHeader>
<bodyText confidence="0.998282952380952">
For the experimental process of this paper, we
formed a Twitter data set (D1) of more than 48 mil-
lion tweets produced by |U |= 38, 020 users geolo-
cated in the UK in the period between 14/04/2011
and 12/04/2012 (both dates included, At = 365
days). D1 is a temporal subset of the data set used
for modelling UK voting intentions in (Lampos et
al., 2013). Geolocation of users was carried out
by matching the location field in their profile with
UK city names on DBpedia as well as by check-
ing that the user’s timezone is set to G.M.T. (Rout
et al., 2013). The use of a common greater geo-
graphical area (UK) was essential in order to derive
a data set with language and topic homogeneity.
A distinct Twitter data set (D2) consisting of ap-
prox. 400 million tweets was formed for learning
term clusters (Section 4.2). D2 was retrieved from
Twitter’s Gardenhose stream (a 10% sample of the
entire stream) from 02/01 to 28/02/2011. D1 and
D2 were processed using TrendMiner’s pipeline
(Preot¸iuc-Pietro et al., 2012).
</bodyText>
<sectionHeader confidence="0.997376" genericHeader="method">
3 User Impact Definition
</sectionHeader>
<bodyText confidence="0.999862703703704">
On the microblogging platform of Twitter, user –
or, in general, account – popularity is usually quan-
tified by the raw number of followers (φin &gt; 0),
i.e. other users interested in this account. Likewise,
a user can follow others, which we denote as his set
of followees (φout &gt; 0). It is expected that users
with high numbers of followers are also popular
in the real world, being well-known artists, politi-
cians, brands and so on. However, non popular
entities, the majority in the social network, can also
gain a great number of followers, by exploiting,
for example, a follow-back strategy.2 Therefore,
using solely the number of followers to quantify
impact may lead to inaccurate outcomes (Cha et al.,
2010). A natural alternative, the ratio of φin/φout
is not a reliable metric, as it is invariant to scal-
ing, i.e. it cannot differentiate accounts of the type
{φin, φout} = {m, n} and {γ x m, γ x n}. We
resolve this problem by squaring the number of
followers (φ2in/φout); note that the previous expres-
sion is equal to (φin − φout) x (φin/φout) + φin and
thus, it incorporates the ratio as well as the differ-
ence between followers and followees.
An additional impact indicator is the number of
times an account has been listed by others (φa &gt; 0).
Lists provide a way to curate content on Twitter;
thus, users included in many lists are attractors of
</bodyText>
<footnote confidence="0.9416085">
2An account follows other accounts randomly expecting
that they will follow back.
</footnote>
<page confidence="0.997582">
406
</page>
<tableCaption confidence="0.705463421052632">
# of tweets
proportion of retweets
proportion of non-duplicate tweets
proportion of tweets with hashtags
hashtag-tokens ratio in tweets
proportion of tweets with @-mentions
# of unique @-mentions in tweets
proportion of tweets with @-replies
links ratio in tweets
# of favourites the account made
total # of tweets (entire history)
using default profile background (binary)
using default profile image (binary)
enabled geolocation (binary)
population of account’s location
account’s location latitude
account’s location longitude
proportion of days with nonzero tweets
Table 1: Non textual attributes for a Twitter account
</tableCaption>
<bodyText confidence="0.9906186">
used in the modelling process. All attributes refer
to a set of 365 days (At) with the exception of a11,
the total number of tweets in the entire history of an
account. Attributes az, i E 12 − 6, 8, 91 are ratios
of a1, whereas attribute a18 is a proportion of At.
</bodyText>
<subsectionHeader confidence="0.999599">
4.1 Non textual attributes
</subsectionHeader>
<bodyText confidence="0.99992075">
The non-textual attributes (a) are derived either
from general user behaviour statistics or directly
from the account’s profile. Table 1 presents the 18
attributes we extracted and used in our models.
</bodyText>
<subsectionHeader confidence="0.994938">
4.2 Text features
</subsectionHeader>
<bodyText confidence="0.999927580645161">
We process the text in the tweets of D1 and com-
pute daily unigram frequencies. By discarding
terms that appear less than 100 times, we form
a vocabulary of size |V  |= 71, 555. We then form
a user term-frequency matrix of size |U|x|V  |with
the mean term frequencies per user during the time
interval At. All term frequencies are normalised
with the total number of tweets posted by the user.
Apart from single word frequencies, we are also
interested in deriving a more abstract representa-
tion for each user. To achieve this, we learn word
clusters from a distinct reference corpus (D2) that
could potentially represent specific domains of
discussion (or topics). From a multitude of pro-
posed techniques, we have chosen to apply spec-
tral clustering (Shi and Malik, 2000; Ng et al.,
2002), a hard-clustering method appropriate for
high-dimensional data and non-convex clusters
(von Luxburg, 2007). Spectral clustering performs
graph partitioning on the word-by-word similar-
ity matrix. In our case, tweet-term similarity is
reflected by the Normalised Pointwise Mutual In-
formation (NPMI), an information theoretic mea-
sure indicating which words co-occur in the same
context (Bouma, 2009). We use the random walk
graph Laplacian and only keep the largest compo-
nent of the resulting graph, eliminating most stop
words in the process. The number of clusters needs
to be specified in advance and each cluster’s most
representative words are identified by the following
metric of centrality:
</bodyText>
<equation confidence="0.998029">
Cw (C) = Ev∈c NPMI(w, v)
l |c |− 1
</equation>
<bodyText confidence="0.999898">
where w is the target word and c the cluster it be-
longs (|c |denotes the cluster’s size). Examples of
extracted word clusters are illustrated in Table 4.
Other techniques were also applied, such as online
LDA (Hoffman et al., 2010), but we found that
the results were not satisfactory, perhaps due to
the short message length and the foreign terms co-
occuring within a tweet. After forming the clusters
using D2, we compute a topic score (T) for each
user-topic pair in D1, representing a normalised
user-word frequency sum per topic.
</bodyText>
<sectionHeader confidence="0.997029" genericHeader="method">
5 Methods
</sectionHeader>
<bodyText confidence="0.99997125">
This section presents the various modelling ap-
proaches for the underlying inference task, the im-
pact score (S) prediction of Twitter users based on
a set of their actions.
</bodyText>
<subsectionHeader confidence="0.999224">
5.1 Learning functions for regression
</subsectionHeader>
<bodyText confidence="0.9999806">
We formulate this problem as a regression task,
i.e. we infer a real numbered value based on a set
of observed features. As a simple baseline, we ap-
ply Ridge Regression (RR) (Hoerl and Kennard,
1970), a reguralised version of the ordinary least
squares. Most importantly, we focus on nonlinear
methods for the impact score prediction task given
the multimodality of the feature space. Recently, it
was shown by Cohn and Specia (2013) that Sup-
port Vector Machines for Regression (SVR) (Vap-
nik, 1998; Smola and Sch¨olkopf, 2004), commonly
considered the state-of-the-art for NLP regression
tasks, can be outperformed by Gaussian Processes
(GPs), a kernelised, probabilistic approach to learn-
ing (Rasmussen and Williams, 2006). Their setting
is close to ours, in that they had few (17) features
and were also aiming to predict a complex con-
tinuous phenomenon (human post-editing time).
The initial stages of our experimental process con-
firmed that GPs performed better than SVR; thus,
</bodyText>
<figure confidence="0.996974315789474">
a1
a2
a3
a4
a5
a6
a7
a8
a9
a10
a11
a12
a13
a14
a15
a16
a17
a18
, (2)
</figure>
<page confidence="0.991041">
407
</page>
<bodyText confidence="0.9983746">
we based our modelling around them, including
RR for comparison.
In GP regression, for the inputs x ∈ Rd we want
to learn a function f: Rd → R that is drawn from
a GP prior
</bodyText>
<equation confidence="0.996152">
f(x) ∼ GP (m(x), k(x,x0)) , (3)
</equation>
<bodyText confidence="0.999932636363636">
where m(x) and k(x,x0) denote the mean (set to
0 in our experiments) and covariance (or kernel)
functions respectively. The GP kernel function rep-
resents the covariance between pairs of input. We
wish to limit f to smooth functions over the inputs,
with different smoothness in each input dimension,
assuming that some features are more useful than
others. This can be accommodated by a squared ex-
ponential covariance function with Automatic Rele-
vance Determination (ARD) (Neal, 1996; Williams
and Rasmussen, 1996):
</bodyText>
<equation confidence="0.999638333333333">
kard(x, x0) = σ2 exp I � (xi 2`2 i)2 (4)
i
P(y∗ |x∗, O) = if P(y∗ |x∗, f )P(f |O) , (5)
</equation>
<bodyText confidence="0.999835923076923">
where y∗ is the response variable, O a labelled train-
ing set and x∗ the current observation. We learn the
hyperparameters of the model by maximising the
log marginal likelihood P(y|O) using gradient as-
cent. However, inference becomes intractable when
many training instances (n) are present as the num-
ber of computations needed is O(n3) (Qui˜nonero-
Candela and Rasmussen, 2005). Since our training
samples are tens of thousands, we apply a sparse
approximation method (FITC), which bases param-
eter learning on a few inducing points in the train-
ing set (Qui˜nonero-Candela and Rasmussen, 2005;
Snelson and Ghahramani, 2006).
</bodyText>
<subsectionHeader confidence="0.997427">
5.2 Models
</subsectionHeader>
<bodyText confidence="0.99991925">
For predicting user impact on Twitter, we develop
three regression models that build on each other.
The first and simplest one (A) uses only the non-
textual attributes as features; the performance of A
is tested using RR,3 SVR as well as a GP model.
For SVR we used an RBF kernel (equivalent to
kiso), whereas for the GP we applied the following
covariance function
</bodyText>
<equation confidence="0.999887">
k(a,a0) = kard(a,a0) + knoise(a,a0) + β, (6)
</equation>
<bodyText confidence="0.997933666666667">
where knoise(a,a0) = σ2 × δ(a,a0), δ is a Kro-
necker delta function and β is the regression bias;
this function consists of (|a |+ 3) hyperparame-
ters. Note that the sum of covariance functions is
also a valid covariance function (Rasmussen and
Williams, 2006).
The second model (AW) extends model A by
adding word-frequencies as features. The 500 most
frequent terms in D1 are discarded as stop words
and we use the following 2, 000 ones (denoted by
w). Setting x = {a,w}, the covariance function
becomes
</bodyText>
<equation confidence="0.954857">
+ knoise(x,x0) + β,
</equation>
<bodyText confidence="0.999953">
where we apply kiso on the term-frequencies due to
their high dimensionality; the number of hyperpa-
rameters is (|a |+ 5). This is an intermediate model
aiming to evaluate whether the incorporation of
text improves prediction performance.
Finally, in the third model (AC) instead of rely-
ing on the high dimensional space of single words,
we use topic-oriented collections of terms extracted
by applying spectral clustering (see Section 4.2).
By denoting the set of different clusters or topics
as τ and the entire feature space as x = {a,τ}, the
covariance function now becomes
</bodyText>
<equation confidence="0.999804">
k(x,x0) = kard(x,x0) + knoise(x,x0) + β. (8)
</equation>
<bodyText confidence="0.97978">
The number of hyperparameters is equal to (|a |+
|τ |+ 3) and this model is applied for |τ |= 50 and
100.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999849666666667">
Here we present the experimental results for the
user impact prediction task and then investigate the
factors that can affect it.
</bodyText>
<subsectionHeader confidence="0.997347">
6.1 Predictive Accuracy
</subsectionHeader>
<bodyText confidence="0.999731666666667">
We evaluated the performance of the proposed
models via 10-fold cross-validation. Results are
presented in Table 2; Root Mean Squared Error
</bodyText>
<footnote confidence="0.785233666666667">
3Given that the representation of attributes a16 and a17
(latitude, longitude) is ambiguous in a linear model, they were
not included in the RR-based models.
</footnote>
<equation confidence="0.682805">
L i
</equation>
<bodyText confidence="0.999869">
where σ2 denotes the overall variance and `i is
the length-scale parameter for feature xi; all hy-
perparameters are learned from data during model
inference. Parameter `i is inversely proportional to
the feature’s relevancy in the model, i.e. high val-
ues of `i indicate a low degree of relevance for the
corresponding xi. By setting `i = ` in Eq. 4, we
learn a common length-scale for all the dimensions
– this is known as the isotropic squared exponen-
tial function (kiso) since it is based purely on the
difference |x − x0|. kiso is a preferred choice when
the dimensionality of the input space is high. Hav-
ing set our covariance functions, predictions are
conducted using Bayesian integration
</bodyText>
<equation confidence="0.99922">
k(x,x0) = kard(a,a0) + kiso(w,w0) (7)
</equation>
<page confidence="0.991873">
408
</page>
<table confidence="0.999916833333333">
Linear (RR) Nonlinear (GP)
Model r RMSE r RMSE
A .667 2.642 .759 2.298
AW .712 2.529 .768 2.263
AC, ITI = 50 .703 2.518 .774 2.234
AC, ITI = 100 .714 2.480 .780 2.210
</table>
<tableCaption confidence="0.893283">
Table 2: Average performance (RMSE and Pear-
son’s r) derived from 10-fold cross-validation for
the task of user impact score prediction.
</tableCaption>
<figure confidence="0.736303904761905">
Tweets in entire history (α,,)
Unique @-mentions (α7)
Links (α9)
H
100
0
L
H
100
0
L
Model
Top relevant features
a~13, a11, a7, a1, a9, a8, a18, a4, a6, a3
a7, a1, a11, a~13, a9, a8, a18, a4, a6, a15
A
AW
AC, T = 50
AC, T = 100
o 0
a13,a11,a7,T1,a1,a9,a8,T2,a6,T3
</figure>
<tableCaption confidence="0.714908">
a13, a11, a7, a1, a9, T1, T2, T3, a18, a8
Table 3: The 10 most relevant features in descend-
</tableCaption>
<bodyText confidence="0.9978219375">
ing relevance order for all GP models. T0 i and Ti
denote word clusters (may vary in each model).6
(RMSE) and Pearson’s correlation (r) between pre-
dictions and responses were used as the perfor-
mance metrics. Overall, the best performance in
terms of both RMSE (2.21 impact points) and lin-
ear correlation (r = .78, p &lt; .001) is achieved
by the GP model (AC) that combines non-textual
attributes with a 100 topic clusters; the difference
in performance with all other models is statistically
significant.4 The linear baseline (RR) follows the
same pattern of improvement through the differ-
ent models, but never manages to reach the perfor-
mance of the nonlinear alternative. As mentioned
previously, we have also tried SVR with an RBF
kernel for model A (parameters were optimised on
a held-out development set) and the performance
(RMSE: 2.33, r = .75, p &lt; .001) was significantly
worse than the one achieved by the GP model.4
Notice that when word-based features are intro-
duced in model AW, performance improves. This
was one of the motivations for including text in the
modelling, apart from the notion that the posted
content should also affect general impact. Lastly,
turning this problem from regression to classifi-
cation by creating 3 impact score pseudo-classes
based on the .25 and the .9 quantiles of the re-
sponse variable (4.3 and 11.4 impact score points
respectively) and by using the outputs of model
AC (T = 100) in each phase of the 10-fold cross-
validation, we achieve a 75.86% classification ac-
curacy.5
</bodyText>
<footnote confidence="0.993898666666667">
4 Indicated by performing a t-test (5% significance level).
5Similar performance scores can be estimated for different
class threshold settings.
</footnote>
<figure confidence="0.322622">
Days with nonzero tweets (α18)
</figure>
<figureCaption confidence="0.822449">
Figure 2: User impact distribution (x-axis: impact
points, y-axis: # of user accounts) for users with a
low (L) or a high (H) participation in a selection
of relevant non-textual attributes. Dot-dashed lines
denote the respective mean impact score; the red
line is the mean of the entire sample (= 6.776).
</figureCaption>
<subsectionHeader confidence="0.99956">
6.2 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.950149857142857">
Given the model’s strong performance, we now
conduct a more thorough analysis to identify and
characterise the properties that affect aspects of
the user impact. GP’s length-scale parameters (i)
– which are inversely proportional to feature rele-
vancy – are used for ranking feature importance.
Note that since our data set consists of UK users,
some results may be biased towards specific cul-
tural properties.
Non-textual attributes. Table 3 lists the 10 most
relevant attributes (or topics, where applicable) as
extracted in each GP model. Ranking is determined
by the mean value of the length-scale parameter for
each feature in the 10-fold cross-validation process.
We do not show feature ranking derived from the
RR models as we focus on the models with the best
performance. Despite this, it is worth mentioning
6Length-scales are comparable for features of the same
variance (z-scored). Binary features (denoted by o) are not
z-scored, but for comparison purposes we have rescaled their
length-scale using the feature’s variance.
</bodyText>
<figure confidence="0.9990608">
0 10 20 30
100
0
0 10 20 30
H
L
H
100
0
L
@-replies (αa)
100
0
L
H
</figure>
<page confidence="0.996546">
409
</page>
<table confidence="0.98853845">
Label A(�) ± σ(�)
71: Weather 3.73 ± 1.80
72: Healthcare 5.44 ± 1.55
Finance
Housing
73: Politics 6.07 ± 2.86
74: Showbiz 7.36 ± 2.25
Movies
TV
75: Commerce 7.83 ± 2.77
76: Twitter 8.22 ± 2.98
Hashtags
77: Social 8.37 ± 5.52
Unrest
78: Non English 8.45 ± 3.80
79: Horoscope 9.11 ± 3.07
Gambling
710: Religion 10.29 ± 6.27
Sports
Cluster’s words ranked by centrality |S|
</table>
<bodyText confidence="0.991146826086957">
mph, humidity, barometer, gust, winds, hpa, temperature, kt, #weather [...]
nursing, nurse, rn, registered, bedroom, clinical, #news, estate, #hospital,
rent, healthcare, therapist, condo, investment, furnished, medical, #nyc,
occupational, investors, #ny, litigation, tutors, spacious, foreclosure [...]
senate, republican, gop, police, arrested, voters, robbery, democrats, presi-
dential, elections, charged, election, charges, #religion, arrest, repeal, dems,
#christian, reform, democratic, pleads, #jesus, #atheism [...]
damon, potter, #tvd, harry, elena, kate, portman, pattinson, hermione, jen-
nifer, kristen, stefan, robert, catholic, stewart, katherine, lois, jackson, vam-
pire, natalie, #vampirediaries, tempah, tinie, weasley, turner, rowland [...]
chevrolet, inventory, coupon, toyota, mileage, sedan, nissan, adde, jeep, 4x4,
2002, #coupon, enhanced, #deal, dodge, gmc, 20%, suv, 15%, 2005, 2003,
2006, coupons, discount, hatchback, purchase, #ebay, 10% [...]
#teamfollowback, #500aday, #tfb, #instantfollowback, #ifollowback, #in-
stantfollow, #followback, #teamautofollow, #autofollow, #mustfollow [...]
#egypt, #tunisia, #iran, #israel, #palestine, tunisia, arab, #jan25, iran, israel,
protests, egypt, #yemen, #iranelection, israeli, #jordan, regime, yemen,
#gaza, protesters, #lebanon, #syria, egyptian, #protest, #iraq [...]
yg, nak, gw, gue, kalo, itu, aku, aja, ini, gak, klo, sih, tak, mau, buat [...]
horoscope, astrology, zodiac, aries, libra, aquarius, pisces, taurus, virgo,
capricorn, horoscopes, sagitarius, comprehensive, lottery, jackpot [...]
#jesustweeters, psalm, christ, #nhl, proverbs, unto, salvation, psalms, lord,
kjv, righteousness, niv, bible, pastor, #mlb, romans, awards, nhl [...]
</bodyText>
<figure confidence="0.9235599">
309
1281
950
1943
608
194
321
469
1354
1610
</figure>
<tableCaption confidence="0.547523333333333">
Table 4: The 10 most relevant topics (for model AC, 17-1 = 100) in the prediction of a user’s impact score
together with their most central words. The topics are ranked by their mean length-scale, µ(e), in the
10-fold cross-validation process (u(e) is the respective standard deviation).
</tableCaption>
<bodyText confidence="0.999856525423729">
that RR’s outputs also followed similar ranking pat-
terns, e.g. the top 5 features in model A were a18,
a7, a3, a11 and a9. Notice that across all models,
among the strongest features are the total number
of posts either in the entire account’s history (a11)
or within the 365-day interval of our experiment
(a1) and the number of unique @-mentions (a7),
good indicators of user activity and user interaction
respectively. Feature a13 is also a very good predic-
tor, but is of limited utility for modelling our data
set because very few accounts maintain the default
profile photo (0.4%). Less relevant attributes (not
shown) are the ones related to the location of a
user (a16, a17) signalling that the whereabouts of a
user may not necessarily relate to impact. Another
low relevance attribute is the number of favourites
that an account did (a10), something reasonable, as
those weak endorsements are not affecting the main
stream of content updates in the social network.
In Figure 2, we present the distribution of user
impact for accounts with low (left-side) and high
(right-side) participation in a selection of non-
textual attributes. Low (L) and high (H) participa-
tions are defined by selecting the 500 accounts with
lowest and highest scores for this specific attribute.
The means of (L) and (H) are compared with the
mean impact score in our sample. As anticipated,
accounts with low activity (a11) are likely to be
assigned impact scores far below the mean, while
very active accounts may follow a quite opposite
pattern. Avoiding mentioning (a7) or replying (a8)
to others may not affect (on average) an impact
score positively or negatively; however, accounts
that do many unique @-mentions are distributed
around a clearly higher impact score. On the other
hand, users that overdo @-replies are distributed be-
low the mean impact score. Furthermore, accounts
that post irregularly with gaps longer than a day
(a18) or avoid using links in their tweets (a9) will
probably appear in the low impact score range.
Topics. Regarding prediction accuracy (Table 2),
performance improves when topics are included.
In turn, some of the topics replace non-textual at-
tributes in the relevancy ranking (Table 3). Table 4
presents the 10 most relevant topic word-clusters
based on their mean length-scale µ(e) in the 10-
fold cross-validation process for the best perform-
ing GP model (AC, 17-1 = 100). We see that clusters
with their most central words representing topics
such as ‘Weather’, ‘Healthcare/Finance’, ‘Politics’
and ‘Showbiz’ come up on top.
Contrary to the non-textual attributes, accounts
with low participation in a topic (for the vast major-
ity of topics) were distributed along impact score
values lower than the mean. Based on the fact that
word clusters are not small in size, this is a rational
outcome indicating that accounts with small word-
frequency sums (i.e. the ones that do not tweet
much) will more likely be users with small impact
</bodyText>
<page confidence="0.991663">
410
</page>
<figure confidence="0.998208625">
100
100
0
0
0 10 20 30
τ1
τ6
τ2
τ3
τ7
τ6
τ4
τ9
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
τ5
τ10
</figure>
<figureCaption confidence="0.993375333333333">
Figure 3: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for accounts with a
high participation in the 10 most relevant topics. Dot-dashed lines denote mean impact scores; the red line
is the mean of the entire sample (= 6.776).
</figureCaption>
<subsectionHeader confidence="0.380073">
Impact Score (S)
</subsectionHeader>
<bodyText confidence="0.91392385">
Figure 4: User impact distribution for accounts with
high (blue) and low (dark grey) topic entropy. Lines
denote the respective mean impact scores.
scores. Hence, in Figure 3 we only show the user
impact distribution for the 500 accounts with the
top participation in each topic. Informally, this is a
way to quantify the contribution of each domain or
topic of discussion in the impact score. Notice that
the topics which ‘push’ users towards the highest
impact scores fall into the domains of ‘Politics’ (T3)
and ‘Showbiz’ (T4). An equally interesting observa-
tion is that engaging a lot about a specific topic will
more likely result to a higher than average impact;
the only exception is T8 which does not deviate
from the mean, but T8 rather represents the use of a
non-English language (Indonesian) and therefore,
does not form an actual topic of discussion.
To further understand how participation in the
10 most relevant topics relates to impact, we also
computed the joint user-topic entropy defined by
</bodyText>
<equation confidence="0.998051333333333">
M
H(ui, T) = − P(ui, Tj) x log2 P(ui, Tj), (9)
j=1
</equation>
<bodyText confidence="0.99996596969697">
where ui is a user and M = 10 (Shannon, 2001).
This is a measure of user pseudo-informativeness,
meaning that users with high entropy are consid-
ered as more informative (without assessing the
quality of the information). Figure 4 shows the im-
pact score distributions for the 500 accounts with
the lowest and highest entropy. Low and high en-
tropies are separated, with the former being placed
clearly below the mean user impact score and the
latter above. This pictorial assessment suggests that
a connection between informativeness and impact
may exist, at least in their extremes (their correla-
tion in the entire sample is r = .35, p &lt; .001).
Use case scenarios. Most of the previous analysis
focused on the properties of single features. How-
ever, the user impact prediction models we learn
depend on feature combinations. For that reason,
it is of interest to investigate use case scenarios
that bring various attributes together. To reduce
notation in this paragraph, we use x+i (x is ei-
ther a non-textual attribute a or a topic T) to ex-
press xi &gt; µ(xi), the set of users for which the
value of feature xi is above the mean; equivalently
x−i : xi &lt; µ(xi). We also use T∗A to express the
more complex set {T+A n T−j n ... n T−z }, an inter-
section of users that are active in one topic (TA),
but not very active in the rest. Figure 5 depicts the
user impact distributions for five use case scenarios.
Scenario A compares interactive to non interac-
tive users, represented by P(a1 , a+6 , a+7 , a+8 ) and
P(a1 , a−6 ,a−7 , a− 8 ) respectively; interactivity, de-
fined by an intersection of accounts that tweet regu-
larly, do many @-mentions and @-replies, but also
</bodyText>
<figure confidence="0.994341862068965">
0 10 20 30
Number of Accounts
100
50
0
All
Low Entropy
High Entropy
411
0 10 20 30
A
IA
NIA
900
750
600
450
300
150
0
400
300
200
100
0
0 10 20 30
0 10 20 30
0 10 20 30
0 10 20 30
B
IA
IAC
400
200
500
300
100
0
C
L
NL
400
500
D
TO
TF
200
E
LT
ST
150
300
100
200
100
50
0
0
</figure>
<figureCaption confidence="0.985668">
Figure 5: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for five Twitter
use scenarios based on subsets of the most relevant attributes and topics – IA: Interactive, IAC: Clique
Interactive, L: Using many links, TO: Topic-Overall, TF: Topic-Focused, LT: ‘Light’ topics, ST: ‘Serious’
topics. (N) denotes negation and lines the respective mean impact scores.
</figureCaption>
<bodyText confidence="0.999300130434783">
mention many different users, seems to be rewarded
on average with higher impact scores. Interactive
users gain more impact than clique-interactive ac-
counts represented by P(a+1 , a+6 , a−7 , a+8 ), i.e. users
who interact, but do not mention many differ-
ent accounts, possibly because they are conduct-
ing discussions with a specific circle only (sce-
nario B). The use of links when writing about
the most prevalent topics (‘Politics’ and ‘Show-
biz’) appears to be an important impact-wise fac-
tor (scenario C); the compared probability distri-
butions in that case were P(a+1 , (τ+3 U τ+4 ), a9 )
against P(a+1 , (τ+3 U τ4), ay) . Surprisingly, when
links were replaced by hashtags in the previous
distributions, a clear class separation was not
achieved. In scenario D, topic-focused accounts,
i.e. users that write about one topic consistently,
represented by P(a+ 10)),
1 , (τ∗ 2 U τ∗ 3 U τ∗ 4 U τ∗ 7 U τ∗
have on average slightly worse impact scores when
compared to accounts tweeting about many top-
ics, P(a+1 , τ+2 , τ+3 , τ+4 , τ+7 , τ+10). Finally, scenario
E shows thats users engaging about more ‘seri-
</bodyText>
<equation confidence="0.820854">
ous’ topics, P(a+ 7 )), were
1 , τ− 4 , τ− 5 , τ− 9 , (τ+ 3 U τ+
</equation>
<bodyText confidence="0.9773555">
not differentiated from the ones posting about more
‘light’ topics, P(ai (τ+4 U τ+ U τ+9 ), τ−3 ,τ7−)
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999943148148148">
The task of user-impact prediction based on a ma-
chine learning approach that incorporates text fea-
tures is novel, to the best of our knowledge. De-
spite this fact, our work is partly related to research
approaches for quantifying and analysing user in-
fluence in online social networks. For example,
Cha et al. (2010) compared followers, retweets
and @-mentions received as measures of influ-
ence. Bakshy et al. (2011) aggregated all posts by
each user, computed an individual-level influence
and then tried to predict it by modelling user at-
tributes (# of followers, followees, tweets and date
of joining) together with past user influence. Their
method, based on classification and regression trees
(Breiman, 1984), achieved a modest performance
(r = .34). Furthermore, Romero et al. (2011) pro-
posed an algorithm for determining user influence
and passivity based on information-forwarding ac-
tivity, and Luo et al. (2013) exploited user attributes
to predict retweet occurrences. The primary differ-
ence with all the works described above is that we
aim to predict user impact by exploiting features
under the user’s direct control. Hence, our findings
can be used as indirect insights for strategies that in-
dividual users may follow to increase their impact
score. In addition, we incorporate the actual text
posted by the users in the entire modelling process.
</bodyText>
<sectionHeader confidence="0.995512" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999980833333333">
We have introduced the task of user impact pre-
diction on the microblogging platform of Twitter
based on user-controlled textual and non-textual
attributes. Nonlinear methods, in particular Gaus-
sian Processes, were more suitable than linear ap-
proaches for this problem, providing a strong per-
formance (r = .78). That result motivated the anal-
ysis of specific characteristics in the inferred model
to further define and understand the elements that
affect impact. In a nutshell, activity, non clique-
oriented interactivity and engagement on a diverse
set of topics are among the most decisive impact
factors. In future work, we plan to improve various
modelling components and gain a deeper under-
standing of the derived outcomes in collaboration
with domain experts. For more general conclusions,
the consideration of different cultures and media
sources is essential.
</bodyText>
<sectionHeader confidence="0.998893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.90845625">
This research was supported by EU-FP7-ICT
project n.287863 (“TrendMiner”). Lampos also ac-
knowledges the support from EPSRC IRC project
EP/K031953/1.
</bodyText>
<page confidence="0.998036">
412
</page>
<sectionHeader confidence="0.995881" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999933452830189">
Eytan Bakshy, Jake M. Hofman, Winter A. Mason, and Dun-
can J. Watts. 2011. Everyone’s an influencer: quantifying
influence on Twitter. In 4th International Conference on
Web Search and Data Mining, WSDM’11, pages 65–74.
Gerlof Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Biennial GSCL
Conference, pages 31–40.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010. Tweet,
Tweet, Retweet: Conversational Aspects of Retweeting on
Twitter. In System Sciences, HICSS’10, pages 1–10.
Leo Breiman. 1984. Classification and regression trees.
Chapman &amp; Hall.
Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011.
Information credibility on Twitter. In 20th International
Conference on World Wide Web, WWW’11, pages 675–
684.
Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and
Krishna P. Gummadi. 2010. Measuring User Influence in
Twitter: The Million Follower Fallacy. In 4th International
Conference on Weblogs and Social Media, ICWSM’10,
pages 10–17.
Trevor Cohn and Lucia Specia. 2013. Modelling Annotator
Bias with Multi-task Gaussian Processes: An Application
to Machine Translation Quality Estimation. In 51st Annual
Meeting of the Association for Computational Linguistics,
ACL’13, pages 32–42.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge Re-
gression: Biased Estimation for Nonorthogonal Problems.
Technometrics, 12(1):55–67.
Matthew Hoffman, David Blei, and Francis Bach. 2010. On-
line Learning for Latent Dirichlet Allocation. In Advances
in Neural Information Processing Systems, NIPS’10, pages
856–864.
Bernardo A. Huberman, Daniel M. Romero, and Fang Wu.
2009. Social Networks that Matter: Twitter Under the
Microscope. First Monday, 14(1).
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a news
media? In 19th International Conference on World Wide
Web, WWW’10, pages 591–600.
Vasileios Lampos and Nello Cristianini. 2012. Nowcast-
ing Events from the Social Web with Statistical Learning.
ACM Transactions on Intelligent Systems and Technology,
3(4):72:1–72:22.
Vasileios Lampos, Daniel Preot¸iuc-Pietro, and Trevor Cohn.
2013. A user-centric model of voting intention from Social
Media. In 51st Annual Meeting of the Association for
Computational Linguistics, ACL’13, pages 993–1003.
Zhunchen Luo, Miles Osborne, Jintao Tang, and Ting Wang.
2013. Who will retweet me?: finding retweeters in Twit-
ter. In 36th International Conference on Research and
Development in Information Retrieval, SIGIR’13, pages
869–872.
Radford M. Neal. 1996. Bayesian Learning for Neural Net-
works. Springer.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2002. On
spectral clustering: Analysis and an algorithm. In Advances
in Neural Information Processing Systems, NIPS’02, pages
849–856.
Daniel Preot¸iuc-Pietro, Sina Samangooei, Trevor Cohn,
Nicholas Gibbins, and Mahesan Niranjan. 2012. Trend-
miner: An Architecture for Real Time Analysis of Social
Media Text. In 6th International Conference on Weblogs
and Social Media, ICWSM’12, pages 38–42.
Joaquin Qui˜nonero-Candela and Carl E. Rasmussen. 2005.
A unifying view of sparse approximate Gaussian Process
regression. Journal ofMachine Learning Research, 6:1939–
1959.
Carl E. Rasmussen and Christopher K. I. Williams. 2006.
Gaussian Processes for Machine Learning. MIT Press.
Daniel M. Romero, Wojciech Galuba, Sitaram Asur, and
Bernardo A. Huberman. 2011. Influence and Passivity
in Social Media. In Machine Learning and Knowledge
Discovery in Databases, volume 6913, pages 18–33.
Dominic Rout, Daniel Preot¸iuc-Pietro, Bontcheva Kalina, and
Trevor Cohn. 2013. Where’s @wally: A classification
approach to geolocating users based on their social ties. In
24th Conference on Hypertext and Social Media, HT’13,
pages 11–20.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010.
Earthquake shakes Twitter users: real-time event detection
by social sensors. In 19th International Conference on
World Wide Web, WWW’10, pages 851–860.
Claude E. Shannon. 2001. A mathematical theory of com-
munication. SIGMOBILE Mob. Comput. Commun. Rev.,
5(1):3–55 (reprint with corrections).
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and
image segmentation. Transactions on Pattern Analysis and
Machine Intelligence, 22(8):888–905.
Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial
on support vector regression. Statistics and Computing,
14(3):199–222.
Edward Snelson and Zoubin Ghahramani. 2006. Sparse
Gaussian Processes using Pseudo-inputs. In Advances in
Neural Information Processing Systems, NIPS’06, pages
1257–1264.
Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H. Chi.
2010. Want to be Retweeted? Large Scale Analytics on
Factors Impacting Retweet in Twitter Network. In Social
Computing, SocialCom’10, pages 177–184.
Vladimir N. Vapnik. 1998. Statistical learning theory. Wiley.
Ulrike von Luxburg. 2007. A tutorial on spectral clustering.
Statistics and computing, 17(4):395–416.
Christopher K. I. Williams and Carl E. Rasmussen. 1996.
Gaussian Processes for Regression. In Advances in Neural
Information Processing Systems, NIPS’96, pages 514–520.
</reference>
<page confidence="0.998987">
413
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957684">
<title confidence="0.999952">Predicting and Characterising User Impact on Twitter</title>
<author confidence="0.999707">Nikolaos Daniel</author>
<affiliation confidence="0.996932666666667">of Computer Science, University College of Computer Science, University of Sheffield and Information Systems, The University of Melbourne</affiliation>
<abstract confidence="0.99825235">The open structure of online social networks and their uncurated nature give rise to problems of user credibility and influence. In this paper, we address the task of predicting the impact of Twitter users based only on features under their direct control, such as usage statistics and the text posted in their tweets. We approach the problem as regression and apply linear as well as nonlinear learning methods to predict a user impact score, estimated by combining the numbers of the user’s followers, followees and listings. The experimental results point out that a strong prediction performance is achieved, especially for models based on the Gaussian Processes framework. Hence, we can interpret various modelling components, transforming them into indirect ‘suggestions’ for impact boosting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eytan Bakshy</author>
<author>Jake M Hofman</author>
<author>Winter A Mason</author>
<author>Duncan J Watts</author>
</authors>
<title>Everyone’s an influencer: quantifying influence on Twitter.</title>
<date>2011</date>
<booktitle>In 4th International Conference on Web Search and Data Mining, WSDM’11,</booktitle>
<pages>65--74</pages>
<contexts>
<context position="32140" citStr="Bakshy et al. (2011)" startWordPosition="5362" endWordPosition="5365">gaging about more ‘serious’ topics, P(a+ 7 )), were 1 , τ− 4 , τ− 5 , τ− 9 , (τ+ 3 U τ+ not differentiated from the ones posting about more ‘light’ topics, P(ai (τ+4 U τ+ U τ+9 ), τ−3 ,τ7−) 7 Related Work The task of user-impact prediction based on a machine learning approach that incorporates text features is novel, to the best of our knowledge. Despite this fact, our work is partly related to research approaches for quantifying and analysing user influence in online social networks. For example, Cha et al. (2010) compared followers, retweets and @-mentions received as measures of influence. Bakshy et al. (2011) aggregated all posts by each user, computed an individual-level influence and then tried to predict it by modelling user attributes (# of followers, followees, tweets and date of joining) together with past user influence. Their method, based on classification and regression trees (Breiman, 1984), achieved a modest performance (r = .34). Furthermore, Romero et al. (2011) proposed an algorithm for determining user influence and passivity based on information-forwarding activity, and Luo et al. (2013) exploited user attributes to predict retweet occurrences. The primary difference with all the </context>
</contexts>
<marker>Bakshy, Hofman, Mason, Watts, 2011</marker>
<rawString>Eytan Bakshy, Jake M. Hofman, Winter A. Mason, and Duncan J. Watts. 2011. Everyone’s an influencer: quantifying influence on Twitter. In 4th International Conference on Web Search and Data Mining, WSDM’11, pages 65–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>In Biennial GSCL Conference,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="11050" citStr="Bouma, 2009" startWordPosition="1782" endWordPosition="1783">e corpus (D2) that could potentially represent specific domains of discussion (or topics). From a multitude of proposed techniques, we have chosen to apply spectral clustering (Shi and Malik, 2000; Ng et al., 2002), a hard-clustering method appropriate for high-dimensional data and non-convex clusters (von Luxburg, 2007). Spectral clustering performs graph partitioning on the word-by-word similarity matrix. In our case, tweet-term similarity is reflected by the Normalised Pointwise Mutual Information (NPMI), an information theoretic measure indicating which words co-occur in the same context (Bouma, 2009). We use the random walk graph Laplacian and only keep the largest component of the resulting graph, eliminating most stop words in the process. The number of clusters needs to be specified in advance and each cluster’s most representative words are identified by the following metric of centrality: Cw (C) = Ev∈c NPMI(w, v) l |c |− 1 where w is the target word and c the cluster it belongs (|c |denotes the cluster’s size). Examples of extracted word clusters are illustrated in Table 4. Other techniques were also applied, such as online LDA (Hoffman et al., 2010), but we found that the results we</context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. In Biennial GSCL Conference, pages 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danah Boyd</author>
<author>Scott Golder</author>
<author>Gilad Lotan</author>
</authors>
<title>Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter.</title>
<date>2010</date>
<booktitle>In System Sciences, HICSS’10,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="1612" citStr="Boyd et al., 2010" startWordPosition="230" endWordPosition="233">performance is achieved, especially for models based on the Gaussian Processes framework. Hence, we can interpret various modelling components, transforming them into indirect ‘suggestions’ for impact boosting. 1 Introduction Online social networks have become a wide spread medium for information dissemination and interaction between millions of users (Huberman et al., 2009; Kwak et al., 2010), turning, at the same time, into a popular subject for interdisciplinary research, involving domains such as Computer Science (Sakaki et al., 2010), Health (Lampos and Cristianini, 2012) and Psychology (Boyd et al., 2010). Open access along with the property of structured content retrieval for publicly posted data have brought the microblogging platform of Twitter into the spotlight. Vast quantities of human-generated text from a range of themes, including opinions, news and everyday activities, spread over a social network. Naturally, issues arise, like user credibility (Castillo et al., 2011) and content attractiveness (Suh et al., 2010), and quite often trustful or appealing information transmitters are identified by an impact assessment.1 Intuitively, it is expected that user impact cannot be defined by a </context>
</contexts>
<marker>Boyd, Golder, Lotan, 2010</marker>
<rawString>Danah Boyd, Scott Golder, and Gilad Lotan. 2010. Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter. In System Sciences, HICSS’10, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Classification and regression trees.</title>
<date>1984</date>
<publisher>Chapman &amp; Hall.</publisher>
<contexts>
<context position="32438" citStr="Breiman, 1984" startWordPosition="5409" endWordPosition="5410">ures is novel, to the best of our knowledge. Despite this fact, our work is partly related to research approaches for quantifying and analysing user influence in online social networks. For example, Cha et al. (2010) compared followers, retweets and @-mentions received as measures of influence. Bakshy et al. (2011) aggregated all posts by each user, computed an individual-level influence and then tried to predict it by modelling user attributes (# of followers, followees, tweets and date of joining) together with past user influence. Their method, based on classification and regression trees (Breiman, 1984), achieved a modest performance (r = .34). Furthermore, Romero et al. (2011) proposed an algorithm for determining user influence and passivity based on information-forwarding activity, and Luo et al. (2013) exploited user attributes to predict retweet occurrences. The primary difference with all the works described above is that we aim to predict user impact by exploiting features under the user’s direct control. Hence, our findings can be used as indirect insights for strategies that individual users may follow to increase their impact score. In addition, we incorporate the actual text poste</context>
</contexts>
<marker>Breiman, 1984</marker>
<rawString>Leo Breiman. 1984. Classification and regression trees. Chapman &amp; Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Castillo</author>
<author>Marcelo Mendoza</author>
<author>Barbara Poblete</author>
</authors>
<title>Information credibility on Twitter.</title>
<date>2011</date>
<booktitle>In 20th International Conference on World Wide Web, WWW’11,</booktitle>
<pages>675--684</pages>
<contexts>
<context position="1992" citStr="Castillo et al., 2011" startWordPosition="286" endWordPosition="289">; Kwak et al., 2010), turning, at the same time, into a popular subject for interdisciplinary research, involving domains such as Computer Science (Sakaki et al., 2010), Health (Lampos and Cristianini, 2012) and Psychology (Boyd et al., 2010). Open access along with the property of structured content retrieval for publicly posted data have brought the microblogging platform of Twitter into the spotlight. Vast quantities of human-generated text from a range of themes, including opinions, news and everyday activities, spread over a social network. Naturally, issues arise, like user credibility (Castillo et al., 2011) and content attractiveness (Suh et al., 2010), and quite often trustful or appealing information transmitters are identified by an impact assessment.1 Intuitively, it is expected that user impact cannot be defined by a single attribute, but depends on multiple user actions, such as posting frequency and quality, interaction strategies, and the text or topics of the written communications. In this paper, we start by predicting user impact as a statistical learning task (regression). For that purpose, we firstly define an impact score function for Twitter users driven by basic account propertie</context>
</contexts>
<marker>Castillo, Mendoza, Poblete, 2011</marker>
<rawString>Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on Twitter. In 20th International Conference on World Wide Web, WWW’11, pages 675– 684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meeyoung Cha</author>
<author>Hamed Haddadi</author>
<author>Fabricio Benevenuto</author>
<author>Krishna P Gummadi</author>
</authors>
<title>Measuring User Influence in Twitter: The Million Follower Fallacy.</title>
<date>2010</date>
<booktitle>In 4th International Conference on Weblogs and Social Media, ICWSM’10,</booktitle>
<pages>10--17</pages>
<contexts>
<context position="8001" citStr="Cha et al., 2010" startWordPosition="1278" endWordPosition="1281"> quantified by the raw number of followers (φin &gt; 0), i.e. other users interested in this account. Likewise, a user can follow others, which we denote as his set of followees (φout &gt; 0). It is expected that users with high numbers of followers are also popular in the real world, being well-known artists, politicians, brands and so on. However, non popular entities, the majority in the social network, can also gain a great number of followers, by exploiting, for example, a follow-back strategy.2 Therefore, using solely the number of followers to quantify impact may lead to inaccurate outcomes (Cha et al., 2010). A natural alternative, the ratio of φin/φout is not a reliable metric, as it is invariant to scaling, i.e. it cannot differentiate accounts of the type {φin, φout} = {m, n} and {γ x m, γ x n}. We resolve this problem by squaring the number of followers (φ2in/φout); note that the previous expression is equal to (φin − φout) x (φin/φout) + φin and thus, it incorporates the ratio as well as the difference between followers and followees. An additional impact indicator is the number of times an account has been listed by others (φa &gt; 0). Lists provide a way to curate content on Twitter; thus, us</context>
<context position="32040" citStr="Cha et al. (2010)" startWordPosition="5347" endWordPosition="5350">bout many topics, P(a+1 , τ+2 , τ+3 , τ+4 , τ+7 , τ+10). Finally, scenario E shows thats users engaging about more ‘serious’ topics, P(a+ 7 )), were 1 , τ− 4 , τ− 5 , τ− 9 , (τ+ 3 U τ+ not differentiated from the ones posting about more ‘light’ topics, P(ai (τ+4 U τ+ U τ+9 ), τ−3 ,τ7−) 7 Related Work The task of user-impact prediction based on a machine learning approach that incorporates text features is novel, to the best of our knowledge. Despite this fact, our work is partly related to research approaches for quantifying and analysing user influence in online social networks. For example, Cha et al. (2010) compared followers, retweets and @-mentions received as measures of influence. Bakshy et al. (2011) aggregated all posts by each user, computed an individual-level influence and then tried to predict it by modelling user attributes (# of followers, followees, tweets and date of joining) together with past user influence. Their method, based on classification and regression trees (Breiman, 1984), achieved a modest performance (r = .34). Furthermore, Romero et al. (2011) proposed an algorithm for determining user influence and passivity based on information-forwarding activity, and Luo et al. (</context>
</contexts>
<marker>Cha, Haddadi, Benevenuto, Gummadi, 2010</marker>
<rawString>Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and Krishna P. Gummadi. 2010. Measuring User Influence in Twitter: The Million Follower Fallacy. In 4th International Conference on Weblogs and Social Media, ICWSM’10, pages 10–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics, ACL’13,</booktitle>
<pages>32--42</pages>
<contexts>
<context position="12572" citStr="Cohn and Specia (2013)" startWordPosition="2040" endWordPosition="2043"> the various modelling approaches for the underlying inference task, the impact score (S) prediction of Twitter users based on a set of their actions. 5.1 Learning functions for regression We formulate this problem as a regression task, i.e. we infer a real numbered value based on a set of observed features. As a simple baseline, we apply Ridge Regression (RR) (Hoerl and Kennard, 1970), a reguralised version of the ordinary least squares. Most importantly, we focus on nonlinear methods for the impact score prediction task given the multimodality of the feature space. Recently, it was shown by Cohn and Specia (2013) that Support Vector Machines for Regression (SVR) (Vapnik, 1998; Smola and Sch¨olkopf, 2004), commonly considered the state-of-the-art for NLP regression tasks, can be outperformed by Gaussian Processes (GPs), a kernelised, probabilistic approach to learning (Rasmussen and Williams, 2006). Their setting is close to ours, in that they had few (17) features and were also aiming to predict a complex continuous phenomenon (human post-editing time). The initial stages of our experimental process confirmed that GPs performed better than SVR; thus, a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 a15 </context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation. In 51st Annual Meeting of the Association for Computational Linguistics, ACL’13, pages 32–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur E Hoerl</author>
<author>Robert W Kennard</author>
</authors>
<title>Ridge Regression: Biased Estimation for Nonorthogonal Problems.</title>
<date>1970</date>
<journal>Technometrics,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="12338" citStr="Hoerl and Kennard, 1970" startWordPosition="2003" endWordPosition="2006"> the foreign terms cooccuring within a tweet. After forming the clusters using D2, we compute a topic score (T) for each user-topic pair in D1, representing a normalised user-word frequency sum per topic. 5 Methods This section presents the various modelling approaches for the underlying inference task, the impact score (S) prediction of Twitter users based on a set of their actions. 5.1 Learning functions for regression We formulate this problem as a regression task, i.e. we infer a real numbered value based on a set of observed features. As a simple baseline, we apply Ridge Regression (RR) (Hoerl and Kennard, 1970), a reguralised version of the ordinary least squares. Most importantly, we focus on nonlinear methods for the impact score prediction task given the multimodality of the feature space. Recently, it was shown by Cohn and Specia (2013) that Support Vector Machines for Regression (SVR) (Vapnik, 1998; Smola and Sch¨olkopf, 2004), commonly considered the state-of-the-art for NLP regression tasks, can be outperformed by Gaussian Processes (GPs), a kernelised, probabilistic approach to learning (Rasmussen and Williams, 2006). Their setting is close to ours, in that they had few (17) features and wer</context>
</contexts>
<marker>Hoerl, Kennard, 1970</marker>
<rawString>Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1):55–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hoffman</author>
<author>David Blei</author>
<author>Francis Bach</author>
</authors>
<title>Online Learning for Latent Dirichlet Allocation.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS’10,</booktitle>
<pages>856--864</pages>
<contexts>
<context position="11616" citStr="Hoffman et al., 2010" startWordPosition="1880" endWordPosition="1883"> which words co-occur in the same context (Bouma, 2009). We use the random walk graph Laplacian and only keep the largest component of the resulting graph, eliminating most stop words in the process. The number of clusters needs to be specified in advance and each cluster’s most representative words are identified by the following metric of centrality: Cw (C) = Ev∈c NPMI(w, v) l |c |− 1 where w is the target word and c the cluster it belongs (|c |denotes the cluster’s size). Examples of extracted word clusters are illustrated in Table 4. Other techniques were also applied, such as online LDA (Hoffman et al., 2010), but we found that the results were not satisfactory, perhaps due to the short message length and the foreign terms cooccuring within a tweet. After forming the clusters using D2, we compute a topic score (T) for each user-topic pair in D1, representing a normalised user-word frequency sum per topic. 5 Methods This section presents the various modelling approaches for the underlying inference task, the impact score (S) prediction of Twitter users based on a set of their actions. 5.1 Learning functions for regression We formulate this problem as a regression task, i.e. we infer a real numbered</context>
</contexts>
<marker>Hoffman, Blei, Bach, 2010</marker>
<rawString>Matthew Hoffman, David Blei, and Francis Bach. 2010. Online Learning for Latent Dirichlet Allocation. In Advances in Neural Information Processing Systems, NIPS’10, pages 856–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo A Huberman</author>
<author>Daniel M Romero</author>
<author>Fang Wu</author>
</authors>
<date>2009</date>
<journal>Social Networks that Matter: Twitter Under the Microscope. First Monday,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="1370" citStr="Huberman et al., 2009" startWordPosition="192" endWordPosition="195">as regression and apply linear as well as nonlinear learning methods to predict a user impact score, estimated by combining the numbers of the user’s followers, followees and listings. The experimental results point out that a strong prediction performance is achieved, especially for models based on the Gaussian Processes framework. Hence, we can interpret various modelling components, transforming them into indirect ‘suggestions’ for impact boosting. 1 Introduction Online social networks have become a wide spread medium for information dissemination and interaction between millions of users (Huberman et al., 2009; Kwak et al., 2010), turning, at the same time, into a popular subject for interdisciplinary research, involving domains such as Computer Science (Sakaki et al., 2010), Health (Lampos and Cristianini, 2012) and Psychology (Boyd et al., 2010). Open access along with the property of structured content retrieval for publicly posted data have brought the microblogging platform of Twitter into the spotlight. Vast quantities of human-generated text from a range of themes, including opinions, news and everyday activities, spread over a social network. Naturally, issues arise, like user credibility (</context>
</contexts>
<marker>Huberman, Romero, Wu, 2009</marker>
<rawString>Bernardo A. Huberman, Daniel M. Romero, and Fang Wu. 2009. Social Networks that Matter: Twitter Under the Microscope. First Monday, 14(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haewoon Kwak</author>
<author>Changhyun Lee</author>
<author>Hosung Park</author>
<author>Sue Moon</author>
</authors>
<title>What is Twitter, a social network or a news media?</title>
<date>2010</date>
<booktitle>In 19th International Conference on World Wide Web, WWW’10,</booktitle>
<pages>591--600</pages>
<contexts>
<context position="1390" citStr="Kwak et al., 2010" startWordPosition="196" endWordPosition="199"> linear as well as nonlinear learning methods to predict a user impact score, estimated by combining the numbers of the user’s followers, followees and listings. The experimental results point out that a strong prediction performance is achieved, especially for models based on the Gaussian Processes framework. Hence, we can interpret various modelling components, transforming them into indirect ‘suggestions’ for impact boosting. 1 Introduction Online social networks have become a wide spread medium for information dissemination and interaction between millions of users (Huberman et al., 2009; Kwak et al., 2010), turning, at the same time, into a popular subject for interdisciplinary research, involving domains such as Computer Science (Sakaki et al., 2010), Health (Lampos and Cristianini, 2012) and Psychology (Boyd et al., 2010). Open access along with the property of structured content retrieval for publicly posted data have brought the microblogging platform of Twitter into the spotlight. Vast quantities of human-generated text from a range of themes, including opinions, news and everyday activities, spread over a social network. Naturally, issues arise, like user credibility (Castillo et al., 201</context>
</contexts>
<marker>Kwak, Lee, Park, Moon, 2010</marker>
<rawString>Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. 2010. What is Twitter, a social network or a news media? In 19th International Conference on World Wide Web, WWW’10, pages 591–600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nello Cristianini</author>
</authors>
<title>Nowcasting Events from the Social Web with Statistical Learning.</title>
<date>2012</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1577" citStr="Lampos and Cristianini, 2012" startWordPosition="224" endWordPosition="227">al results point out that a strong prediction performance is achieved, especially for models based on the Gaussian Processes framework. Hence, we can interpret various modelling components, transforming them into indirect ‘suggestions’ for impact boosting. 1 Introduction Online social networks have become a wide spread medium for information dissemination and interaction between millions of users (Huberman et al., 2009; Kwak et al., 2010), turning, at the same time, into a popular subject for interdisciplinary research, involving domains such as Computer Science (Sakaki et al., 2010), Health (Lampos and Cristianini, 2012) and Psychology (Boyd et al., 2010). Open access along with the property of structured content retrieval for publicly posted data have brought the microblogging platform of Twitter into the spotlight. Vast quantities of human-generated text from a range of themes, including opinions, news and everyday activities, spread over a social network. Naturally, issues arise, like user credibility (Castillo et al., 2011) and content attractiveness (Suh et al., 2010), and quite often trustful or appealing information transmitters are identified by an impact assessment.1 Intuitively, it is expected that </context>
</contexts>
<marker>Lampos, Cristianini, 2012</marker>
<rawString>Vasileios Lampos and Nello Cristianini. 2012. Nowcasting Events from the Social Web with Statistical Learning. ACM Transactions on Intelligent Systems and Technology, 3(4):72:1–72:22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A user-centric model of voting intention from Social Media.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics, ACL’13,</booktitle>
<pages>993--1003</pages>
<marker>Lampos, Preot¸iuc-Pietro, Cohn, 2013</marker>
<rawString>Vasileios Lampos, Daniel Preot¸iuc-Pietro, and Trevor Cohn. 2013. A user-centric model of voting intention from Social Media. In 51st Annual Meeting of the Association for Computational Linguistics, ACL’13, pages 993–1003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhunchen Luo</author>
<author>Miles Osborne</author>
<author>Jintao Tang</author>
<author>Ting Wang</author>
</authors>
<title>Who will retweet me?: finding retweeters in Twitter.</title>
<date>2013</date>
<booktitle>In 36th International Conference on Research and Development in Information Retrieval, SIGIR’13,</booktitle>
<pages>869--872</pages>
<contexts>
<context position="32645" citStr="Luo et al. (2013)" startWordPosition="5439" endWordPosition="5442">t al. (2010) compared followers, retweets and @-mentions received as measures of influence. Bakshy et al. (2011) aggregated all posts by each user, computed an individual-level influence and then tried to predict it by modelling user attributes (# of followers, followees, tweets and date of joining) together with past user influence. Their method, based on classification and regression trees (Breiman, 1984), achieved a modest performance (r = .34). Furthermore, Romero et al. (2011) proposed an algorithm for determining user influence and passivity based on information-forwarding activity, and Luo et al. (2013) exploited user attributes to predict retweet occurrences. The primary difference with all the works described above is that we aim to predict user impact by exploiting features under the user’s direct control. Hence, our findings can be used as indirect insights for strategies that individual users may follow to increase their impact score. In addition, we incorporate the actual text posted by the users in the entire modelling process. 8 Conclusions and Future Work We have introduced the task of user impact prediction on the microblogging platform of Twitter based on user-controlled textual a</context>
</contexts>
<marker>Luo, Osborne, Tang, Wang, 2013</marker>
<rawString>Zhunchen Luo, Miles Osborne, Jintao Tang, and Ting Wang. 2013. Who will retweet me?: finding retweeters in Twitter. In 36th International Conference on Research and Development in Information Retrieval, SIGIR’13, pages 869–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Bayesian Learning for Neural Networks.</title>
<date>1996</date>
<publisher>Springer.</publisher>
<contexts>
<context position="13882" citStr="Neal, 1996" startWordPosition="2267" endWordPosition="2268">ion, for the inputs x ∈ Rd we want to learn a function f: Rd → R that is drawn from a GP prior f(x) ∼ GP (m(x), k(x,x0)) , (3) where m(x) and k(x,x0) denote the mean (set to 0 in our experiments) and covariance (or kernel) functions respectively. The GP kernel function represents the covariance between pairs of input. We wish to limit f to smooth functions over the inputs, with different smoothness in each input dimension, assuming that some features are more useful than others. This can be accommodated by a squared exponential covariance function with Automatic Relevance Determination (ARD) (Neal, 1996; Williams and Rasmussen, 1996): kard(x, x0) = σ2 exp I � (xi 2`2 i)2 (4) i P(y∗ |x∗, O) = if P(y∗ |x∗, f )P(f |O) , (5) where y∗ is the response variable, O a labelled training set and x∗ the current observation. We learn the hyperparameters of the model by maximising the log marginal likelihood P(y|O) using gradient ascent. However, inference becomes intractable when many training instances (n) are present as the number of computations needed is O(n3) (Qui˜noneroCandela and Rasmussen, 2005). Since our training samples are tens of thousands, we apply a sparse approximation method (FITC), whic</context>
</contexts>
<marker>Neal, 1996</marker>
<rawString>Radford M. Neal. 1996. Bayesian Learning for Neural Networks. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>Yair Weiss</author>
</authors>
<title>On spectral clustering: Analysis and an algorithm.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS’02,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="10652" citStr="Ng et al., 2002" startWordPosition="1726" endWordPosition="1729"> 555. We then form a user term-frequency matrix of size |U|x|V |with the mean term frequencies per user during the time interval At. All term frequencies are normalised with the total number of tweets posted by the user. Apart from single word frequencies, we are also interested in deriving a more abstract representation for each user. To achieve this, we learn word clusters from a distinct reference corpus (D2) that could potentially represent specific domains of discussion (or topics). From a multitude of proposed techniques, we have chosen to apply spectral clustering (Shi and Malik, 2000; Ng et al., 2002), a hard-clustering method appropriate for high-dimensional data and non-convex clusters (von Luxburg, 2007). Spectral clustering performs graph partitioning on the word-by-word similarity matrix. In our case, tweet-term similarity is reflected by the Normalised Pointwise Mutual Information (NPMI), an information theoretic measure indicating which words co-occur in the same context (Bouma, 2009). We use the random walk graph Laplacian and only keep the largest component of the resulting graph, eliminating most stop words in the process. The number of clusters needs to be specified in advance a</context>
</contexts>
<marker>Ng, Jordan, Weiss, 2002</marker>
<rawString>Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2002. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems, NIPS’02, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Sina Samangooei</author>
<author>Trevor Cohn</author>
<author>Nicholas Gibbins</author>
<author>Mahesan Niranjan</author>
</authors>
<title>Trendminer: An Architecture for Real Time Analysis of Social Media Text.</title>
<date>2012</date>
<booktitle>In 6th International Conference on Weblogs and Social Media, ICWSM’12,</booktitle>
<pages>38--42</pages>
<marker>Preot¸iuc-Pietro, Samangooei, Cohn, Gibbins, Niranjan, 2012</marker>
<rawString>Daniel Preot¸iuc-Pietro, Sina Samangooei, Trevor Cohn, Nicholas Gibbins, and Mahesan Niranjan. 2012. Trendminer: An Architecture for Real Time Analysis of Social Media Text. In 6th International Conference on Weblogs and Social Media, ICWSM’12, pages 38–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joaquin Qui˜nonero-Candela</author>
<author>Carl E Rasmussen</author>
</authors>
<title>A unifying view of sparse approximate Gaussian Process regression.</title>
<date>2005</date>
<journal>Journal ofMachine Learning Research,</journal>
<volume>6</volume>
<marker>Qui˜nonero-Candela, Rasmussen, 2005</marker>
<rawString>Joaquin Qui˜nonero-Candela and Carl E. Rasmussen. 2005. A unifying view of sparse approximate Gaussian Process regression. Journal ofMachine Learning Research, 6:1939– 1959.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl E Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<title>Gaussian Processes for Machine Learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3202" citStr="Rasmussen and Williams, 2006" startWordPosition="471" endWordPosition="475">c account properties. Afterwards, from a set of accounts, we measure several publicly available attributes, such as the quantity of posts or interaction figures. Textual attributes are also modelled either by word frequencies or, more generally, by clusters of related words which quantify a topic-oriented participation. The main hypothesis being tested is whether textual and non textual attributes encapsulate patterns that affect the impact of an account. To model this data, we present a method based on nonlinear regression using Gaussian Processes, a Bayesian non-parametric class of methods (Rasmussen and Williams, 2006), proven more effective in capturing the multimodal user features. The modelling choice of excluding components that are not under an account’s direct control (e.g. received retweets) combined with a significant user impact prediction performance (r = .78) enabled us to investigate further how specific aspects of a user’s behaviour relate to impact, by examining the parameters of the inferred model. Among our findings, we identify relevant features for this task and confirm that consistent activity and broad interaction are deciding impact factors. Informativeness, estimated by computing a joi</context>
<context position="12862" citStr="Rasmussen and Williams, 2006" startWordPosition="2081" endWordPosition="2084"> set of observed features. As a simple baseline, we apply Ridge Regression (RR) (Hoerl and Kennard, 1970), a reguralised version of the ordinary least squares. Most importantly, we focus on nonlinear methods for the impact score prediction task given the multimodality of the feature space. Recently, it was shown by Cohn and Specia (2013) that Support Vector Machines for Regression (SVR) (Vapnik, 1998; Smola and Sch¨olkopf, 2004), commonly considered the state-of-the-art for NLP regression tasks, can be outperformed by Gaussian Processes (GPs), a kernelised, probabilistic approach to learning (Rasmussen and Williams, 2006). Their setting is close to ours, in that they had few (17) features and were also aiming to predict a complex continuous phenomenon (human post-editing time). The initial stages of our experimental process confirmed that GPs performed better than SVR; thus, a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 a15 a16 a17 a18 , (2) 407 we based our modelling around them, including RR for comparison. In GP regression, for the inputs x ∈ Rd we want to learn a function f: Rd → R that is drawn from a GP prior f(x) ∼ GP (m(x), k(x,x0)) , (3) where m(x) and k(x,x0) denote the mean (set to 0 in our experim</context>
<context position="15305" citStr="Rasmussen and Williams, 2006" startWordPosition="2511" endWordPosition="2514">we develop three regression models that build on each other. The first and simplest one (A) uses only the nontextual attributes as features; the performance of A is tested using RR,3 SVR as well as a GP model. For SVR we used an RBF kernel (equivalent to kiso), whereas for the GP we applied the following covariance function k(a,a0) = kard(a,a0) + knoise(a,a0) + β, (6) where knoise(a,a0) = σ2 × δ(a,a0), δ is a Kronecker delta function and β is the regression bias; this function consists of (|a |+ 3) hyperparameters. Note that the sum of covariance functions is also a valid covariance function (Rasmussen and Williams, 2006). The second model (AW) extends model A by adding word-frequencies as features. The 500 most frequent terms in D1 are discarded as stop words and we use the following 2, 000 ones (denoted by w). Setting x = {a,w}, the covariance function becomes + knoise(x,x0) + β, where we apply kiso on the term-frequencies due to their high dimensionality; the number of hyperparameters is (|a |+ 5). This is an intermediate model aiming to evaluate whether the incorporation of text improves prediction performance. Finally, in the third model (AC) instead of relying on the high dimensional space of single word</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>Carl E. Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Romero</author>
<author>Wojciech Galuba</author>
<author>Sitaram Asur</author>
<author>Bernardo A Huberman</author>
</authors>
<title>Influence and Passivity in Social Media.</title>
<date>2011</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<volume>6913</volume>
<pages>18--33</pages>
<contexts>
<context position="32514" citStr="Romero et al. (2011)" startWordPosition="5419" endWordPosition="5422">rk is partly related to research approaches for quantifying and analysing user influence in online social networks. For example, Cha et al. (2010) compared followers, retweets and @-mentions received as measures of influence. Bakshy et al. (2011) aggregated all posts by each user, computed an individual-level influence and then tried to predict it by modelling user attributes (# of followers, followees, tweets and date of joining) together with past user influence. Their method, based on classification and regression trees (Breiman, 1984), achieved a modest performance (r = .34). Furthermore, Romero et al. (2011) proposed an algorithm for determining user influence and passivity based on information-forwarding activity, and Luo et al. (2013) exploited user attributes to predict retweet occurrences. The primary difference with all the works described above is that we aim to predict user impact by exploiting features under the user’s direct control. Hence, our findings can be used as indirect insights for strategies that individual users may follow to increase their impact score. In addition, we incorporate the actual text posted by the users in the entire modelling process. 8 Conclusions and Future Wor</context>
</contexts>
<marker>Romero, Galuba, Asur, Huberman, 2011</marker>
<rawString>Daniel M. Romero, Wojciech Galuba, Sitaram Asur, and Bernardo A. Huberman. 2011. Influence and Passivity in Social Media. In Machine Learning and Knowledge Discovery in Databases, volume 6913, pages 18–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Rout</author>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Bontcheva Kalina</author>
<author>Trevor Cohn</author>
</authors>
<title>Where’s @wally: A classification approach to geolocating users based on their social ties.</title>
<date>2013</date>
<booktitle>In 24th Conference on Hypertext and Social Media, HT’13,</booktitle>
<pages>11--20</pages>
<marker>Rout, Preot¸iuc-Pietro, Kalina, Cohn, 2013</marker>
<rawString>Dominic Rout, Daniel Preot¸iuc-Pietro, Bontcheva Kalina, and Trevor Cohn. 2013. Where’s @wally: A classification approach to geolocating users based on their social ties. In 24th Conference on Hypertext and Social Media, HT’13, pages 11–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes Twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In 19th International Conference on World Wide Web, WWW’10,</booktitle>
<pages>851--860</pages>
<contexts>
<context position="1538" citStr="Sakaki et al., 2010" startWordPosition="219" endWordPosition="222">s and listings. The experimental results point out that a strong prediction performance is achieved, especially for models based on the Gaussian Processes framework. Hence, we can interpret various modelling components, transforming them into indirect ‘suggestions’ for impact boosting. 1 Introduction Online social networks have become a wide spread medium for information dissemination and interaction between millions of users (Huberman et al., 2009; Kwak et al., 2010), turning, at the same time, into a popular subject for interdisciplinary research, involving domains such as Computer Science (Sakaki et al., 2010), Health (Lampos and Cristianini, 2012) and Psychology (Boyd et al., 2010). Open access along with the property of structured content retrieval for publicly posted data have brought the microblogging platform of Twitter into the spotlight. Vast quantities of human-generated text from a range of themes, including opinions, news and everyday activities, spread over a social network. Naturally, issues arise, like user credibility (Castillo et al., 2011) and content attractiveness (Suh et al., 2010), and quite often trustful or appealing information transmitters are identified by an impact assessm</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: real-time event detection by social sensors. In 19th International Conference on World Wide Web, WWW’10, pages 851–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>2001</date>
<journal>SIGMOBILE Mob. Comput. Commun. Rev.,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="28159" citStr="Shannon, 2001" startWordPosition="4628" endWordPosition="4629">f ‘Politics’ (T3) and ‘Showbiz’ (T4). An equally interesting observation is that engaging a lot about a specific topic will more likely result to a higher than average impact; the only exception is T8 which does not deviate from the mean, but T8 rather represents the use of a non-English language (Indonesian) and therefore, does not form an actual topic of discussion. To further understand how participation in the 10 most relevant topics relates to impact, we also computed the joint user-topic entropy defined by M H(ui, T) = − P(ui, Tj) x log2 P(ui, Tj), (9) j=1 where ui is a user and M = 10 (Shannon, 2001). This is a measure of user pseudo-informativeness, meaning that users with high entropy are considered as more informative (without assessing the quality of the information). Figure 4 shows the impact score distributions for the 500 accounts with the lowest and highest entropy. Low and high entropies are separated, with the former being placed clearly below the mean user impact score and the latter above. This pictorial assessment suggests that a connection between informativeness and impact may exist, at least in their extremes (their correlation in the entire sample is r = .35, p &lt; .001). U</context>
</contexts>
<marker>Shannon, 2001</marker>
<rawString>Claude E. Shannon. 2001. A mathematical theory of communication. SIGMOBILE Mob. Comput. Commun. Rev., 5(1):3–55 (reprint with corrections).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianbo Shi</author>
<author>Jitendra Malik</author>
</authors>
<title>Normalized cuts and image segmentation.</title>
<date>2000</date>
<journal>Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>22</volume>
<issue>8</issue>
<contexts>
<context position="10634" citStr="Shi and Malik, 2000" startWordPosition="1722" endWordPosition="1725">ary of size |V |= 71, 555. We then form a user term-frequency matrix of size |U|x|V |with the mean term frequencies per user during the time interval At. All term frequencies are normalised with the total number of tweets posted by the user. Apart from single word frequencies, we are also interested in deriving a more abstract representation for each user. To achieve this, we learn word clusters from a distinct reference corpus (D2) that could potentially represent specific domains of discussion (or topics). From a multitude of proposed techniques, we have chosen to apply spectral clustering (Shi and Malik, 2000; Ng et al., 2002), a hard-clustering method appropriate for high-dimensional data and non-convex clusters (von Luxburg, 2007). Spectral clustering performs graph partitioning on the word-by-word similarity matrix. In our case, tweet-term similarity is reflected by the Normalised Pointwise Mutual Information (NPMI), an information theoretic measure indicating which words co-occur in the same context (Bouma, 2009). We use the random walk graph Laplacian and only keep the largest component of the resulting graph, eliminating most stop words in the process. The number of clusters needs to be spec</context>
</contexts>
<marker>Shi, Malik, 2000</marker>
<rawString>Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation. Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Snelson</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Sparse Gaussian Processes using Pseudo-inputs.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS’06,</booktitle>
<pages>1257--1264</pages>
<contexts>
<context position="14624" citStr="Snelson and Ghahramani, 2006" startWordPosition="2391" endWordPosition="2394">) , (5) where y∗ is the response variable, O a labelled training set and x∗ the current observation. We learn the hyperparameters of the model by maximising the log marginal likelihood P(y|O) using gradient ascent. However, inference becomes intractable when many training instances (n) are present as the number of computations needed is O(n3) (Qui˜noneroCandela and Rasmussen, 2005). Since our training samples are tens of thousands, we apply a sparse approximation method (FITC), which bases parameter learning on a few inducing points in the training set (Qui˜nonero-Candela and Rasmussen, 2005; Snelson and Ghahramani, 2006). 5.2 Models For predicting user impact on Twitter, we develop three regression models that build on each other. The first and simplest one (A) uses only the nontextual attributes as features; the performance of A is tested using RR,3 SVR as well as a GP model. For SVR we used an RBF kernel (equivalent to kiso), whereas for the GP we applied the following covariance function k(a,a0) = kard(a,a0) + knoise(a,a0) + β, (6) where knoise(a,a0) = σ2 × δ(a,a0), δ is a Kronecker delta function and β is the regression bias; this function consists of (|a |+ 3) hyperparameters. Note that the sum of covari</context>
</contexts>
<marker>Snelson, Ghahramani, 2006</marker>
<rawString>Edward Snelson and Zoubin Ghahramani. 2006. Sparse Gaussian Processes using Pseudo-inputs. In Advances in Neural Information Processing Systems, NIPS’06, pages 1257–1264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bongwon Suh</author>
<author>Lichan Hong</author>
<author>Peter Pirolli</author>
<author>Ed H Chi</author>
</authors>
<title>Want to be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network.</title>
<date>2010</date>
<booktitle>In Social Computing, SocialCom’10,</booktitle>
<pages>177--184</pages>
<contexts>
<context position="2038" citStr="Suh et al., 2010" startWordPosition="293" endWordPosition="296">to a popular subject for interdisciplinary research, involving domains such as Computer Science (Sakaki et al., 2010), Health (Lampos and Cristianini, 2012) and Psychology (Boyd et al., 2010). Open access along with the property of structured content retrieval for publicly posted data have brought the microblogging platform of Twitter into the spotlight. Vast quantities of human-generated text from a range of themes, including opinions, news and everyday activities, spread over a social network. Naturally, issues arise, like user credibility (Castillo et al., 2011) and content attractiveness (Suh et al., 2010), and quite often trustful or appealing information transmitters are identified by an impact assessment.1 Intuitively, it is expected that user impact cannot be defined by a single attribute, but depends on multiple user actions, such as posting frequency and quality, interaction strategies, and the text or topics of the written communications. In this paper, we start by predicting user impact as a statistical learning task (regression). For that purpose, we firstly define an impact score function for Twitter users driven by basic account properties. Afterwards, from a set of accounts, we meas</context>
</contexts>
<marker>Suh, Hong, Pirolli, Chi, 2010</marker>
<rawString>Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H. Chi. 2010. Want to be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network. In Social Computing, SocialCom’10, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical learning theory. Wiley. Ulrike von Luxburg.</title>
<date>1998</date>
<pages>17--4</pages>
<contexts>
<context position="12636" citStr="Vapnik, 1998" startWordPosition="2052" endWordPosition="2054">mpact score (S) prediction of Twitter users based on a set of their actions. 5.1 Learning functions for regression We formulate this problem as a regression task, i.e. we infer a real numbered value based on a set of observed features. As a simple baseline, we apply Ridge Regression (RR) (Hoerl and Kennard, 1970), a reguralised version of the ordinary least squares. Most importantly, we focus on nonlinear methods for the impact score prediction task given the multimodality of the feature space. Recently, it was shown by Cohn and Specia (2013) that Support Vector Machines for Regression (SVR) (Vapnik, 1998; Smola and Sch¨olkopf, 2004), commonly considered the state-of-the-art for NLP regression tasks, can be outperformed by Gaussian Processes (GPs), a kernelised, probabilistic approach to learning (Rasmussen and Williams, 2006). Their setting is close to ours, in that they had few (17) features and were also aiming to predict a complex continuous phenomenon (human post-editing time). The initial stages of our experimental process confirmed that GPs performed better than SVR; thus, a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 a15 a16 a17 a18 , (2) 407 we based our modelling around them, includ</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical learning theory. Wiley. Ulrike von Luxburg. 2007. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher K I Williams</author>
<author>Carl E Rasmussen</author>
</authors>
<title>Gaussian Processes for Regression.</title>
<date>1996</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS’96,</booktitle>
<pages>514--520</pages>
<contexts>
<context position="13913" citStr="Williams and Rasmussen, 1996" startWordPosition="2269" endWordPosition="2272"> inputs x ∈ Rd we want to learn a function f: Rd → R that is drawn from a GP prior f(x) ∼ GP (m(x), k(x,x0)) , (3) where m(x) and k(x,x0) denote the mean (set to 0 in our experiments) and covariance (or kernel) functions respectively. The GP kernel function represents the covariance between pairs of input. We wish to limit f to smooth functions over the inputs, with different smoothness in each input dimension, assuming that some features are more useful than others. This can be accommodated by a squared exponential covariance function with Automatic Relevance Determination (ARD) (Neal, 1996; Williams and Rasmussen, 1996): kard(x, x0) = σ2 exp I � (xi 2`2 i)2 (4) i P(y∗ |x∗, O) = if P(y∗ |x∗, f )P(f |O) , (5) where y∗ is the response variable, O a labelled training set and x∗ the current observation. We learn the hyperparameters of the model by maximising the log marginal likelihood P(y|O) using gradient ascent. However, inference becomes intractable when many training instances (n) are present as the number of computations needed is O(n3) (Qui˜noneroCandela and Rasmussen, 2005). Since our training samples are tens of thousands, we apply a sparse approximation method (FITC), which bases parameter learning on a</context>
</contexts>
<marker>Williams, Rasmussen, 1996</marker>
<rawString>Christopher K. I. Williams and Carl E. Rasmussen. 1996. Gaussian Processes for Regression. In Advances in Neural Information Processing Systems, NIPS’96, pages 514–520.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>