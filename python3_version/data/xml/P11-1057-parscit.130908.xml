<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.961146">
Together We Can: Bilingual Bootstrapping for WSD
</title>
<author confidence="0.99017">
Mitesh M. Khapra Salil Joshi Arindam Chatterjee Pushpak Bhattacharyya
</author>
<affiliation confidence="0.997925">
Department Of Computer Science and Engineering,
</affiliation>
<address confidence="0.554377">
IIT Bombay,
Powai,
Mumbai, 400076.
</address>
<email confidence="0.998003">
{miteshk,salilj,arindam,pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.997374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999607037037037">
Recent work on bilingual Word Sense Disam-
biguation (WSD) has shown that a resource
deprived language (L1) can benefit from the
annotation work done in a resource rich lan-
guage (L2) via parameter projection. How-
ever, this method assumes the presence of suf-
ficient annotated data in one resource rich lan-
guage which may not always be possible. In-
stead, we focus on the situation where there
are two resource deprived languages, both
having a very small amount of seed annotated
data and a large amount of untagged data. We
then use bilingual bootstrapping, wherein, a
model trained using the seed annotated data
of L1 is used to annotate the untagged data of
L2 and vice versa using parameter projection.
The untagged instances of L1 and L2 which
get annotated with high confidence are then
added to the seed data of the respective lan-
guages and the above process is repeated. Our
experiments show that such a bilingual boot-
strapping algorithm when evaluated on two
different domains with small seed sizes using
Hindi (L1) and Marathi (L2) as the language
pair performs better than monolingual boot-
strapping and significantly reduces annotation
cost.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998882720930233">
The high cost of collecting sense annotated data for
supervised approaches (Ng and Lee, 1996; Lee et
al., 2004) has always remained a matter of concern
for some of the resource deprived languages of the
world. The problem is even more hard-hitting for
multilingual regions (e.g., India which has more than
20 constitutionally recognized languages). To cir-
cumvent this problem, unsupervised and knowledge
based approaches (Lesk, 1986; Walker and Amsler,
1986; Agirre and Rigau, 1996; McCarthy et al.,
2004; Mihalcea, 2005) have been proposed as an al-
ternative but they have failed to deliver good accura-
cies. Semi-supervised approaches (Yarowsky, 1995)
which use a small amount of annotated data and a
large amount of untagged data have shown promise
albeit for a limited set of target words. The above
situation highlights the need for high accuracy re-
source conscious approaches to all-words multilin-
gual WSD.
Recent work by Khapra et al. (2010) in this di-
rection has shown that it is possible to perform cost
effective WSD in a target language (L2) without
compromising much on accuracy by leveraging on
the annotation work done in another language (L1).
This is achieved with the help of a novel synset-
aligned multilingual dictionary which facilitates the
projection of parameters learned from the Wordnet
and annotated corpus of L1 to L2. This approach
thus obviates the need for collecting large amounts
of annotated corpora in multiple languages by rely-
ing on sufficient annotated corpus in one resource
rich language. However, in many situations such a
pivot resource rich language itself may not be avail-
able. Instead, we might have two or more languages
having a small amount of annotated corpus and a
large amount of untagged corpus. Addressing such
situations is the main focus of this work. Specifi-
cally, we address the following question:
In the absence of a pivot resource rich lan-
guage is it possible for two resource de-
prived languages to mutually benefit from
each other’s annotated data?
While addressing the above question we assume that
</bodyText>
<page confidence="0.967267">
561
</page>
<note confidence="0.9795285">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 561–569,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99822532">
even though it is hard to obtain large amounts of
annotated data in multiple languages, it should be
fairly easy to obtain a large amount of untagged data
in these languages. We leverage on such untagged
data by employing a bootstrapping strategy. The
idea is to train an initial model using a small amount
of annotated data in both the languages and itera-
tively expand this seed data by including untagged
instances which get tagged with a high confidence
in successive iterations. Instead of using monolin-
gual bootstrapping, we use bilingual bootstrapping
via parameter projection. In other words, the pa-
rameters learned from the annotated data of L1 (and
L2 respectively) are projected to L2 (and L1 respec-
tively) and the projected model is used to tag the un-
tagged instances of L2 (and L1 respectively).
Such a bilingual bootstrapping strategy when
tested on two domains, viz., Tourism and Health us-
ing Hindi (L1) and Marathi (L2) as the language
pair, consistently does better than a baseline strat-
egy which uses only seed data for training without
performing any bootstrapping. Further, it consis-
tently performs better than monolingual bootstrap-
ping. A simple and intuitive explanation for this is
as follows. In monolingual bootstrapping a language
can benefit only from its own seed data and hence
can tag only those instances with high confidence
which it has already seen. On the other hand, in
bilingual bootstrapping a language can benefit from
the seed data available in the other language which
was not previously seen in its self corpus. This is
very similar to the process of co-training (Blum and
Mitchell, 1998) wherein the annotated data in the
two languages can be seen as two different views of
the same data. Hence, the classifier trained on one
view can be improved by adding those untagged in-
stances which are tagged with a high confidence by
the classifier trained on the other view.
The remainder of this paper is organized as fol-
lows. In section 2 we present related work. Section
3 describes the Synset aligned multilingual dictio-
nary which facilitates parameter projection. Section
4 discusses the work of Khapra et al. (2009) on pa-
rameter projection. In section 5 we discuss bilin-
gual bootstrapping which is the main focus of our
work followed by a brief discussion on monolingual
bootstrapping. Section 6 describes the experimental
setup. In section 7 we present the results followed
by discussion in section 8. Section 9 concludes the
paper.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999953790697675">
Bootstrapping for Word Sense Disambiguation was
first discussed in (Yarowsky, 1995). Starting with a
very small number of seed collocations an initial de-
cision list is created. This decisions list is then ap-
plied to untagged data and the instances which get
tagged with a high confidence are added to the seed
data. This algorithm thus proceeds iteratively in-
creasing the seed size in successive iterations. This
monolingual bootstrapping method showed promise
when tested on a limited set of target words but was
not tried for all-words WSD.
The failure of monolingual approaches (Ng and
Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and
Amsler, 1986; Agirre and Rigau, 1996; McCarthy
et al., 2004; Mihalcea, 2005) to deliver high accura-
cies for all-words WSD at low costs created interest
in bilingual approaches which aim at reducing the
annotation effort. Recent work in this direction by
Khapra et al. (2009) aims at reducing the annotation
effort in multiple languages by leveraging on exist-
ing resources in a pivot language. They showed that
it is possible to project the parameters learned from
the annotation work of one language to another lan-
guage provided aligned Wordnets for the two lan-
guages are available. However, they do not address
situations where two resource deprived languages
have aligned Wordnets but neither has sufficient an-
notated data. In such cases bilingual bootstrapping
can be used so that the two languages can mutually
benefit from each other’s small annotated data.
Li and Li (2004) proposed a bilingual bootstrap-
ping approach for the more specific task of Word
Translation Disambiguation (WTD) as opposed to
the more general task of WSD. This approach does
not need parallel corpora (just like our approach)
and relies only on in-domain corpora from two lan-
guages. However, their work was evaluated only on
a handful of target words (9 nouns) for WTD as op-
posed to the broader task of WSD. Our work instead
focuses on improving the performance of all words
WSD for two resource deprived languages using
bilingual bootstrapping. At the heart of our work lies
parameter projection facilitated by a synset aligned
</bodyText>
<page confidence="0.995262">
562
</page>
<bodyText confidence="0.927387">
multilingual dictionary described in the next section.
</bodyText>
<sectionHeader confidence="0.98731" genericHeader="method">
3 Synset Aligned Multilingual Dictionary
</sectionHeader>
<bodyText confidence="0.9991294">
A novel and effective method of storage and use of
dictionary in a multilingual setting was proposed by
Mohanty et al. (2008). For the purpose of current
discussion, we will refer to this multilingual dictio-
nary framework as MultiDict. One important de-
parture in this framework from the traditional dic-
tionary is that synsets are linked, and after that
the words inside the synsets are linked. The ba-
sic mapping is thus between synsets and thereafter
between the words.
</bodyText>
<table confidence="0.979567375">
Concepts L1 L2 L3
(English) (Hindi) (Marathi)
04321: {male {lwкA {m lgA
a youth- child, (ladkaa), (mulgaa),
ful male boy} bAlк porgA
person (baalak), (porgaa),
bQcA por (por)}
(bachchaa)}
</table>
<tableCaption confidence="0.998741">
Table 1: Multilingual Dictionary Framework
</tableCaption>
<bodyText confidence="0.999247956521739">
Table 1 shows the structure of MultiDict, with one
example row standing for the concept of boy. The
first column is the pivot describing a concept with a
unique ID. The subsequent columns show the words
expressing the concept in respective languages (in
the example table, English, Hindi and Marathi). Af-
ter the synsets are linked, cross linkages are set up
manually from the words of a synset to the words
of a linked synset of the pivot language. For exam-
ple, for the Marathi word m lgA (mulgaa), “a youth-
ful male person”, the correct lexical substitute from
the corresponding Hindi synset is ff3-q-5T (ladkaa).
The average number of such links per synset per lan-
guage pair is approximately 3. However, since our
work takes place in a semi-supervised setting, we
do not assume the presence of these manual cross
linkages between synset members. Instead, in the
above example, we assume that all the words in
the Hindi synset are equally probable translations
of every word in the corresponding Marathi synset.
Such cross-linkages between synset members facil-
itate parameter projection as explained in the next
section.
</bodyText>
<sectionHeader confidence="0.991052" genericHeader="method">
4 Parameter Projection
</sectionHeader>
<bodyText confidence="0.98121025">
Khapra et al. (2009) proposed that the various
parameters essential for domain-specific Word
Sense Disambiguation can be broadly classified into
two categories:
</bodyText>
<listItem confidence="0.962699285714286">
Wordnet-dependent parameters:
• belongingness-to-dominant-concept
• conceptual distance
• semantic distance
Corpus-dependent parameters:
• sense distributions
• corpus co-occurrence
</listItem>
<bodyText confidence="0.989946666666667">
They proposed a scoring function (Equation (1))
which combines these parameters to identify the cor-
rect sense of a word in a context:
</bodyText>
<equation confidence="0.967319416666667">
� Wij * Vi * Vj) (1)
S* = arg max(θiVi +
i
jEJ
where,
i E Candidate Synsets
J = Set of disambiguated words
θi = BelongingnessToDominantConcept(Si)
Vi = P(Si|word)
Wij = CorpusCooccurrence(Si, Sj)
* VWNConceptualDistance(Si, Sj)
* VWNSemanticGraphDistance(Si, Sj)
</equation>
<bodyText confidence="0.9999515">
The first component θiVi of Equation (1) captures
influence of the corpus specific sense of a word in a
domain. The other component Wij *Vi *Vj captures
the influence of interaction of the candidate sense
with the senses of context words weighted by factors
of co-occurrence, conceptual distance and semantic
distance.
Wordnet-dependent parameters depend on the
structure of the Wordnet whereas the Corpus-
dependent parameters depend on various statistics
learned from a sense marked corpora. Both the
tasks of (a) constructing a Wordnet from scratch and
(b) collecting sense marked corpora for multiple
languages are tedious and expensive. Khapra et
</bodyText>
<page confidence="0.991846">
563
</page>
<bodyText confidence="0.96958">
al. (2009) observed that by projecting relations
from the Wordnet of a language and by projecting
corpus statistics from the sense marked corpora
of the language to those of the target language,
the effort required in constructing semantic graphs
for multiple Wordnets and collecting sense marked
corpora for multiple languages can be avoided
or reduced. At the heart of their work lies the
MultiDict described in previous section which
facilitates parameter projection in the following
manner:
</bodyText>
<listItem confidence="0.988777142857143">
1. By linking with the synsets of a pivot resource
rich language (Hindi, in our case), the cost of build-
ing Wordnets of other languages is partly reduced
(semantic relations are inherited). The Wordnet pa-
rameters of Hindi Wordnet now become projectable
to other languages.
2. For calculating corpus specific sense distribu-
tions, P(Sense Si|Word W), we need the counts,
#(Si, W). By using cross linked words in the
synsets, these counts become projectable to the tar-
get language (Marathi, in our case) as they can be
approximated by the counts of the cross linked Hindi
words calculated from the Hindi sense marked cor-
pus as follows:
</listItem>
<equation confidence="0.9901988">
P(Si|W) = #(Si,marathi word)
Ej #(Sj, marathi word)
Algorithm 1 Bilingual Bootstrapping
LD1 := Seed Labeled Data from L1
LD2 := Seed Labeled Data from L2
UD1 := Unlabeled Data from L1
UD2 := Unlabeled Data from L2
repeat
01 := model trained using LD1
02 := model trained using LD2
{Project models from L1/L2 to L2/L1}
02 := project(01, L2)
�01 := project(02, L1)
for all u1 E UD1 do
s := sense assigned by �01 to u1
if confidence(s) &gt; ǫ then
LD1 := LD1 + u1
UD1 := UD1 - u1
end if
end for
for all u2 E UD2 do
s := sense assigned by �02 to u2
if confidence(s) &gt; ǫ then
LD2 := LD2 + u2
UD2 := UD2 - u2
</equation>
<tableCaption confidence="0.3198885">
end if
end for
</tableCaption>
<table confidence="0.78934275">
until convergence
#(Si, cross linked hindi word)
Ej #(Sj, cross linked hindi word)
P(Si|W) ,:
</table>
<bodyText confidence="0.999638">
The rationale behind the above approximation is the
observation that within a domain the counts of cross-
linked words will remain the same across languages.
This parameter projection strategy as explained
above lies at the heart of our work and allows us
to perform bilingual bootstrapping by projecting the
models learned from one language to another.
</bodyText>
<sectionHeader confidence="0.991144" genericHeader="method">
5 Bilingual Bootstrapping
</sectionHeader>
<bodyText confidence="0.999976454545455">
We now come to the main contribution of our work,
i.e., bilingual bootstrapping. As shown in Algorithm
1, we start with a small amount of seed data (LD1
and LD2) in the two languages. Using this data we
learn the parameters described in the previous sec-
tion. We collectively refer to the parameters learned
from the seed data as models 01 and 02 for L1 and L2
respectively. The parameter projection strategy de-
scribed in the previous section is then applied to 01
and 02 to obtain the projected models 02 and 01 re-
spectively. These projected models are then applied
to the untagged data of L1 and L2 and the instances
which get labeled with a high confidence are added
to the labeled data of the respective languages. This
process is repeated till we reach convergence, i.e.,
till it is no longer possible to move any data from
UD1 (and UD2) to LD1 (and LD2 respectively).
We compare our algorithm with monolingual
bootstrapping where the self models 01 and 02 are
directly used to annotate the unlabeled instances in
L1 and L2 respectively instead of using the projected
models 01 and 02. The process of monolingual boot-
</bodyText>
<page confidence="0.977514">
564
</page>
<bodyText confidence="0.333175">
Algorithm 2 Monolingual Bootstrapping
</bodyText>
<equation confidence="0.942831789473684">
LD1 := Seed Labeled Data from L1
LD2 := Seed Labeled Data from L2
UD1 := Unlabeled Data from L1
UD2 := Unlabeled Data from L2
repeat
01 := model trained using LD1
02 := model trained using LD2
for all u1 E UD1 do
s := sense assigned by 01 to u1
if confidence(s) &gt; c then
LD1 := LD1 + u1
UD1 := UD1 - u1
end if
end for
for all u2 E UD2 do
s := sense assigned by 02 to u2
if confidence(s) &gt; c then
LD2 := LD2 + u2
UD2 := UD2 - u2
</equation>
<listItem confidence="0.390281">
end if
end for
</listItem>
<bodyText confidence="0.9426115">
until convergence
strapping is shown in Algorithm 2.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9991925">
We used the publicly available dataset1 described
in Khapra et al. (2010) for all our experiments.
The data was collected from two domains, viz.,
Tourism and Health. The data for Tourism domain
was collected by manually translating English doc-
uments downloaded from Indian Tourism websites
into Hindi and Marathi. Similarly, English docu-
ments for Health domain were obtained from two
doctors and were manually translated into Hindi and
Marathi. The entire data was then manually an-
notated by three lexicographers adept in Hindi and
Marathi. The various statistics pertaining to the total
number of words, number of words per POS cate-
gory and average degree of polysemy are described
in Tables 2 to 5.
Although Tables 2 and 3 also report the num-
</bodyText>
<footnote confidence="0.998134">
1http://www.cfilt.iitb.ac.in/wsd/annotated corpus
</footnote>
<table confidence="0.999788857142857">
Category Polysemous words Monosemous words
Tourism Health Tourism Health
Noun 62336 24089 35811 18923
Verb 6386 1401 3667 5109
Adjective 18949 8773 28998 12138
Adverb 4860 2527 13699 7152
All 92531 36790 82175 43322
</table>
<tableCaption confidence="0.9114695">
Table 2: Polysemous and Monosemous words per cate-
gory in each domain for Hindi
</tableCaption>
<table confidence="0.999942142857143">
Polysemous words Monosemous words
Category Tourism Health Tourism Health
Noun 45589 17482 27386 11383
Verb 7879 3120 2672 1500
Adjective 13107 4788 16725 6032
Adverb 4036 1727 5023 1874
All 70611 27117 51806 20789
</table>
<tableCaption confidence="0.9222755">
Table 3: Polysemous and Monosemous words per cate-
gory in each domain for Marathi
</tableCaption>
<table confidence="0.97348025">
Avg. degree of Wordnet polysemy
for polysemous words
Category Tourism Health
Noun 3.02 3.17
Verb 5.05 6.58
Adjective 2.66 2.75
Adverb 2.52 2.57
All 3.09 3.23
</table>
<tableCaption confidence="0.987044">
Table 4: Average degree of Wordnet polysemy per cate-
gory in the 2 domains for Hindi
</tableCaption>
<table confidence="0.96066275">
Avg. degree of Wordnet polysemy
for polysemous words
Category Tourism Health
Noun 3.06 3.18
Verb 4.96 5.18
Adjective 2.60 2.72
Adverb 2.44 2.45
All 3.14 3.29
</table>
<tableCaption confidence="0.942531">
Table 5: Average degree of Wordnet polysemy per cate-
gory in the 2 domains for Marathi
</tableCaption>
<page confidence="0.987303">
565
</page>
<figure confidence="0.998743419354839">
Seed Size v/s F-score Seed Size v/s F-score
F-score (%)
40
20
80
70
60
50
30
10
0
OnlySeed
WFS
BiBoot
MonoBoot
F-score (%)
40
20
80
70
60
50
30
10
0
OnlySeed
WFS
BiBoot
MonoBoot
0 1000 2000 3000 4000 5000
Seed Size (words)
</figure>
<figureCaption confidence="0.972126333333333">
Figure 1: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Hindi Health
data
</figureCaption>
<figure confidence="0.9900365">
0 1000 2000 3000 4000 5000
Seed Size (words)
</figure>
<figureCaption confidence="0.821867666666667">
Figure 2: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Hindi
Tourism data
</figureCaption>
<figure confidence="0.998476903225806">
Seed Size v/s F-score Seed Size v/s F-score
F-score (%)
40
20
80
70
60
50
30
10
0
OnlySeed
WFS
BiBoot
MonoBoot
F-score (%)
40
20
80
70
60
50
30
10
0
OnlySeed
WFS
BiBoot
MonoBoot
0 1000 2000 3000 4000 5000
Seed Size (words)
</figure>
<figureCaption confidence="0.922109666666667">
Figure 3: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Marathi
Health data
</figureCaption>
<figure confidence="0.982216">
0 1000 2000 3000 4000 5000
Seed Size (words)
</figure>
<figureCaption confidence="0.915862666666667">
Figure 4: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Marathi
Tourism data
</figureCaption>
<bodyText confidence="0.999778125">
ber of monosemous words, we would like to clearly
state that we do not consider monosemous words
while evaluating the performance of our algorithms
(as monosemous words do not need any disambigua-
tion).
We did a 4-fold cross validation of our algorithm
using the above described corpora. Note that even
though the corpora were parallel we did not use this
property in any way in our experiments or algorithm.
In fact, the documents in the two languages were
randomly split into 4 folds without ensuring that the
parallel documents remain in the same folds for the
two languages. We experimented with different seed
sizes varying from 0 to 5000 in steps of 250. The
seed annotated data and untagged instances for boot-
strapping are extracted from 3 folds of the data and
the final evaluation is done on the held-out data in
the 4th fold.
We ran both the bootstrapping algorithms (i.e.,
monolingual bootstrapping and bilingual boot-
strapping) for 10 iterations but, we observed
that after 1-2 iterations the algorithms converge.
In each iteration only those words for which
P(assigned senselword) &gt; 0.6 get moved to the
labeled data. Ideally, this threshold (0.6) should
have been selected using a development set. How-
ever, since our work focuses on resource scarce lan-
guages we did not want to incur the additional cost
of using a development set. Hence, we used a fixed
threshold of 0.6 so that in each iteration only those
words get moved to the labeled data for which the
assigned sense is clearly a majority sense (P &gt; 0.6).
</bodyText>
<page confidence="0.995114">
566
</page>
<table confidence="0.999245916666667">
Language- Algorithm F-score(%) No. of tagged % Reduction in annotation
Domain words needed to cost
achieve this
F-score
Hindi-Health Biboot 57.70 1250 (2250+2250)−(1250+1750) * 100 = 33.33%
OnlySeed 57.99 2250
Marathi-Health Biboot 64.97 1750 (2250+2250)
OnlySeed 64.51 2250
Hindi-Tourism Biboot 60.67 1000 (2000+2000)−(1000+1250) * 100 = 43.75%
OnlySeed 59.83 2000
Marathi-Tourism Biboot 61.90 1250 (2000+2000)
OnlySeed 61.68 2000
</table>
<tableCaption confidence="0.999808">
Table 6: Reduction in annotation cost achieved using Bilingual Bootstrapping
</tableCaption>
<sectionHeader confidence="0.999626" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.964619125">
The results of our experiments are summarized in
Figures 1 to 4. The x-axis represents the amount of
seed data used and the y-axis represents the F-scores
obtained. The different curves in each graph are as
follows:
a. BiBoot: This curve represents the F-score ob-
tained after 10 iterations by using bilingual boot-
strapping with different amounts of seed data.
</bodyText>
<listItem confidence="0.671710666666667">
b. MonoBoot: This curve represents the F-score ob-
tained after 10 iterations by using monolingual
bootstrapping with different amounts of seed data.
c. OnlySeed: This curve represents the F-score ob-
tained by training on the seed data alone without
using any bootstrapping.
d. WFS: This curve represents the F-score obtained
by simply selecting the first sense from Wordnet,
a typically reported baseline.
</listItem>
<sectionHeader confidence="0.997936" genericHeader="evaluation">
8 Discussions
</sectionHeader>
<bodyText confidence="0.9983845">
In this section we discuss the important observations
made from Figures 1 to 4.
</bodyText>
<subsectionHeader confidence="0.998343">
8.1 Performance of Bilingual bootstrapping
</subsectionHeader>
<bodyText confidence="0.99994025">
For small seed sizes, the F-score of bilingual boot-
strapping is consistently better than the F-score ob-
tained by training only on the seed data without us-
ing any bootstrapping. This is true for both the lan-
guages in both the domains. Further, bilingual boot-
strapping also does better than monolingual boot-
strapping for small seed sizes. As explained earlier,
this better performance can be attributed to the fact
that in monolingual bootstrapping the algorithm can
tag only those instances with high confidence which
it has already seen in the training data. Hence, in
successive iterations, very little new information be-
comes available to the algorithm. This is clearly
evident from the fact that the curve of monolin-
gual bootstrapping (MonoBoot) is always close to
the curve of OnlySeed.
</bodyText>
<subsectionHeader confidence="0.999262">
8.2 Effect of seed size
</subsectionHeader>
<bodyText confidence="0.999894454545455">
The benefit of bilingual bootstrapping is clearly felt
for small seed sizes. However, as the seed size in-
creases the performance of the 3 algorithms, viz.,
MonoBoot, BiBoot and OnlySeed is more or less the
same. This is intuitive, because, as the seed size in-
creases the algorithm is able to see more and more
tagged instances in its self corpora and hence does
not need any assistance from the other language. In
other words, the annotated data in L1 is not able to
add any new information to the training process of
L2 and vice versa.
</bodyText>
<subsectionHeader confidence="0.931721">
8.3 Bilingual bootstrapping reduces annotation
cost
</subsectionHeader>
<bodyText confidence="0.999939125">
The performance boost obtained at small seed sizes
suggests that bilingual bootstrapping helps to reduce
the overall annotation costs for both the languages.
To further illustrate this, we take some sample points
from the graph and compare the number of tagged
words needed by BiBoot and OnlySeed to reach the
same (or nearly the same) F-score. We present this
comparison in Table 6.
</bodyText>
<page confidence="0.98875">
567
</page>
<bodyText confidence="0.999689944444444">
The rows for Hindi-Health and Marathi-Health in
Table 6 show that when BiBoot is employed we
need 1250 tagged words in Hindi and 1750 tagged
words in Marathi to attain F-scores of 57.70% and
64.97% respectively. On the other hand, in the ab-
sence of bilingual bootstrapping, (i.e., using Only-
Seed) we need 2250 tagged words each in Hindi and
Marathi to achieve similar F-scores. BiBoot thus
gives a reduction of 33.33% in the overall annota-
tion cost ( {1250 + 1750} v/s {2250 + 2250}) while
achieving similar F-scores. Similarly, the results for
Hindi-Tourism and Marathi-Tourism show that Bi-
Boot gives a reduction of 43.75% in the overall an-
notation cost while achieving similar F-scores. Fur-
ther, since the results of MonoBoot are almost the
same as OnlySeed, the above numbers indicate that
BiBoot provides a reduction in cost when compared
to MonoBoot also.
</bodyText>
<subsectionHeader confidence="0.9592105">
8.4 Contribution of monosemous words in the
performance of BiBoot
</subsectionHeader>
<bodyText confidence="0.999886142857143">
As mentioned earlier, monosemous words in the test
set are not considered while evaluating the perfor-
mance of our algorithm but, we add monosemous
words to the seed data. However, we do not count
monosemous words while calculating the seed size
as there is no manual annotation cost associated with
monosemous words (they can be tagged automati-
cally by fetching their singleton sense id from the
wordnet). We observed that the monosemous words
of L1 help in boosting the performance of L2 and
vice versa. This is because for a given monose-
mous word in L2 (or L1 respectively) the corre-
sponding cross-linked word in L1 (or L2 respec-
tively) need not necessarily be monosemous. In such
cases, the cross-linked polysemous word in L2 (or
L1 respectively) benefits from the projected statis-
tics of a monosemous word in L1 (or L2 respec-
tively). This explains why BiBoot gives an F-score
of 35-52% even at zero seed size even though the
F-score of OnlySeed is only 2-5% (see Figures 1 to
4).
</bodyText>
<sectionHeader confidence="0.998825" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999948083333333">
We presented a bilingual bootstrapping algorithm
for Word Sense Disambiguation which allows two
resource deprived languages to mutually benefit
from each other’s data via parameter projection. The
algorithm consistently performs better than mono-
lingual bootstrapping. It also performs better than
using only monolingual seed data without using any
bootstrapping. The benefit of bilingual bootstrap-
ping is felt prominently when the seed size in the two
languages is very small thus highlighting the useful-
ness of this algorithm in highly resource constrained
scenarios.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999696">
We acknowledge the support of Microsoft Re-
search India in the form of an International Travel
Grant, which enabled one of the authors (Mitesh M.
Khapra) to attend this conference.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999795147058824">
Eneko Agirre and German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING).
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. pages 92–
100. Morgan Kaufmann Publishers.
Mitesh M. Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2009. Projecting parameters for
multilingual word sense disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 459–467, Singa-
pore, August. Association for Computational Linguis-
tics.
Mitesh Khapra, Saurabh Sohoney, Anup Kulkarni, and
Pushpak Bhattacharyya. 2010. Value for money: Bal-
ancing annotation effort, lexicon building and accu-
racy for multilingual wsd. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics.
Yoong Keok Lee, Hwee Tou Ng, and Tee Kiah Chia.
2004. Supervised word sense disambiguation with
support vector machines and multiple knowledge
sources. In Proceedings of Senseval-3: Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 137–140.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In In Proceedings of the
5th annual international conference on Systems docu-
mentation.
Hang Li and Cong Li. 2004. Word translation disam-
biguation using bilingual bootstrapping. Comput. Lin-
guist., 30:1–22, March.
</reference>
<page confidence="0.976399">
568
</page>
<reference confidence="0.99957015625">
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In ACL ’04: Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 279, Morristown, NJ, USA.
Association for Computational Linguistics.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In In Proceedings of
the Joint Human Language Technology and Empirical
Methods in Natural Language Processing Conference
(HLT/EMNLP), pages 411–418.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dictionary:
Insights, applications and challenges. In Global Word-
net Conference.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Proceed-
ings of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 40–47.
D. Walker and R. Amsler. 1986. The use of machine
readable dictionaries in sublanguage analysis. In In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pages 69–83.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, pages 189–196, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.998595">
569
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.547499">
<title confidence="0.999214">We Bilingual Bootstrapping for WSD</title>
<author confidence="0.999588">Mitesh M Khapra Salil Joshi Arindam Chatterjee Pushpak</author>
<affiliation confidence="0.994344">Department Of Computer Science and IIT</affiliation>
<address confidence="0.615199">Mumbai,</address>
<abstract confidence="0.995377535714286">Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource language can benefit from the annotation work done in a resource rich lanvia parameter projection. However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We use wherein, a model trained using the seed annotated data used to annotate the untagged data of vice versa using parameter projection. untagged instances of get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two domains with seed sizes and Marathi as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>German Rigau</author>
</authors>
<title>Word sense disambiguation using conceptual density. In</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="1899" citStr="Agirre and Rigau, 1996" startWordPosition="294" endWordPosition="297">d Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2) without compromising much on a</context>
<context position="6834" citStr="Agirre and Rigau, 1996" startWordPosition="1106" endWordPosition="1109">iscussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. Recent work in this direction by Khapra et al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. However, they do not address situations where two res</context>
</contexts>
<marker>Agirre, Rigau, 1996</marker>
<rawString>Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In In Proceedings of the 16th International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<pages>92--100</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="5297" citStr="Blum and Mitchell, 1998" startWordPosition="850" endWordPosition="853">y which uses only seed data for training without performing any bootstrapping. Further, it consistently performs better than monolingual bootstrapping. A simple and intuitive explanation for this is as follows. In monolingual bootstrapping a language can benefit only from its own seed data and hence can tag only those instances with high confidence which it has already seen. On the other hand, in bilingual bootstrapping a language can benefit from the seed data available in the other language which was not previously seen in its self corpus. This is very similar to the process of co-training (Blum and Mitchell, 1998) wherein the annotated data in the two languages can be seen as two different views of the same data. Hence, the classifier trained on one view can be improved by adding those untagged instances which are tagged with a high confidence by the classifier trained on the other view. The remainder of this paper is organized as follows. In section 2 we present related work. Section 3 describes the Synset aligned multilingual dictionary which facilitates parameter projection. Section 4 discusses the work of Khapra et al. (2009) on parameter projection. In section 5 we discuss bilingual bootstrapping </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. pages 92– 100. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh M Khapra</author>
<author>Sapan Shah</author>
<author>Piyush Kedia</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Projecting parameters for multilingual word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>459--467</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5823" citStr="Khapra et al. (2009)" startWordPosition="940" endWordPosition="943"> its self corpus. This is very similar to the process of co-training (Blum and Mitchell, 1998) wherein the annotated data in the two languages can be seen as two different views of the same data. Hence, the classifier trained on one view can be improved by adding those untagged instances which are tagged with a high confidence by the classifier trained on the other view. The remainder of this paper is organized as follows. In section 2 we present related work. Section 3 describes the Synset aligned multilingual dictionary which facilitates parameter projection. Section 4 discusses the work of Khapra et al. (2009) on parameter projection. In section 5 we discuss bilingual bootstrapping which is the main focus of our work followed by a brief discussion on monolingual bootstrapping. Section 6 describes the experimental setup. In section 7 we present the results followed by discussion in section 8. Section 9 concludes the paper. 2 Related Work Bootstrapping for Word Sense Disambiguation was first discussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged w</context>
<context position="7072" citStr="Khapra et al. (2009)" startWordPosition="1145" endWordPosition="1148">ed to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. Recent work in this direction by Khapra et al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. However, they do not address situations where two resource deprived languages have aligned Wordnets but neither has sufficient annotated data. In such cases bilingual bootstrapping can be used so that the two languages can mutually benefit from each other’s small annotated data. Li and Li (</context>
<context position="10272" citStr="Khapra et al. (2009)" startWordPosition="1668" endWordPosition="1671"> substitute from the corresponding Hindi synset is ff3-q-5T (ladkaa). The average number of such links per synset per language pair is approximately 3. However, since our work takes place in a semi-supervised setting, we do not assume the presence of these manual cross linkages between synset members. Instead, in the above example, we assume that all the words in the Hindi synset are equally probable translations of every word in the corresponding Marathi synset. Such cross-linkages between synset members facilitate parameter projection as explained in the next section. 4 Parameter Projection Khapra et al. (2009) proposed that the various parameters essential for domain-specific Word Sense Disambiguation can be broadly classified into two categories: Wordnet-dependent parameters: • belongingness-to-dominant-concept • conceptual distance • semantic distance Corpus-dependent parameters: • sense distributions • corpus co-occurrence They proposed a scoring function (Equation (1)) which combines these parameters to identify the correct sense of a word in a context: � Wij * Vi * Vj) (1) S* = arg max(θiVi + i jEJ where, i E Candidate Synsets J = Set of disambiguated words θi = BelongingnessToDominantConcept(</context>
</contexts>
<marker>Khapra, Shah, Kedia, Bhattacharyya, 2009</marker>
<rawString>Mitesh M. Khapra, Sapan Shah, Piyush Kedia, and Pushpak Bhattacharyya. 2009. Projecting parameters for multilingual word sense disambiguation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459–467, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Saurabh Sohoney</author>
<author>Anup Kulkarni</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Value for money: Balancing annotation effort, lexicon building and accuracy for multilingual wsd.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2364" citStr="Khapra et al. (2010)" startWordPosition="371" endWordPosition="374">recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2) without compromising much on accuracy by leveraging on the annotation work done in another language (L1). This is achieved with the help of a novel synsetaligned multilingual dictionary which facilitates the projection of parameters learned from the Wordnet and annotated corpus of L1 to L2. This approach thus obviates the need for collecting large amounts of annotated corpora in multiple languages by relying on sufficient annotated corpus in one resource rich language. However, in many situ</context>
<context position="15614" citStr="Khapra et al. (2010)" startWordPosition="2588" endWordPosition="2591">m 2 Monolingual Bootstrapping LD1 := Seed Labeled Data from L1 LD2 := Seed Labeled Data from L2 UD1 := Unlabeled Data from L1 UD2 := Unlabeled Data from L2 repeat 01 := model trained using LD1 02 := model trained using LD2 for all u1 E UD1 do s := sense assigned by 01 to u1 if confidence(s) &gt; c then LD1 := LD1 + u1 UD1 := UD1 - u1 end if end for for all u2 E UD2 do s := sense assigned by 02 to u2 if confidence(s) &gt; c then LD2 := LD2 + u2 UD2 := UD2 - u2 end if end for until convergence strapping is shown in Algorithm 2. 6 Experimental Setup We used the publicly available dataset1 described in Khapra et al. (2010) for all our experiments. The data was collected from two domains, viz., Tourism and Health. The data for Tourism domain was collected by manually translating English documents downloaded from Indian Tourism websites into Hindi and Marathi. Similarly, English documents for Health domain were obtained from two doctors and were manually translated into Hindi and Marathi. The entire data was then manually annotated by three lexicographers adept in Hindi and Marathi. The various statistics pertaining to the total number of words, number of words per POS category and average degree of polysemy are </context>
</contexts>
<marker>Khapra, Sohoney, Kulkarni, Bhattacharyya, 2010</marker>
<rawString>Mitesh Khapra, Saurabh Sohoney, Anup Kulkarni, and Pushpak Bhattacharyya. 2010. Value for money: Balancing annotation effort, lexicon building and accuracy for multilingual wsd. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
<author>Tee Kiah Chia</author>
</authors>
<title>Supervised word sense disambiguation with support vector machines and multiple knowledge sources.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>137--140</pages>
<contexts>
<context position="1529" citStr="Lee et al., 2004" startWordPosition="238" endWordPosition="241">2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amou</context>
<context position="6773" citStr="Lee et al., 2004" startWordPosition="1096" endWordPosition="1099">Bootstrapping for Word Sense Disambiguation was first discussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. Recent work in this direction by Khapra et al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are ava</context>
</contexts>
<marker>Lee, Ng, Chia, 2004</marker>
<rawString>Yoong Keok Lee, Hwee Tou Ng, and Tee Kiah Chia. 2004. Supervised word sense disambiguation with support vector machines and multiple knowledge sources. In Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 137–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation.</booktitle>
<contexts>
<context position="1850" citStr="Lesk, 1986" startWordPosition="288" endWordPosition="289"> small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a ta</context>
<context position="6785" citStr="Lesk, 1986" startWordPosition="1100" endWordPosition="1101">Word Sense Disambiguation was first discussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. Recent work in this direction by Khapra et al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. Howe</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In In Proceedings of the 5th annual international conference on Systems documentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Cong Li</author>
</authors>
<title>Word translation disambiguation using bilingual bootstrapping.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<pages>30--1</pages>
<contexts>
<context position="7677" citStr="Li and Li (2004)" startWordPosition="1243" endWordPosition="1246"> al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. However, they do not address situations where two resource deprived languages have aligned Wordnets but neither has sufficient annotated data. In such cases bilingual bootstrapping can be used so that the two languages can mutually benefit from each other’s small annotated data. Li and Li (2004) proposed a bilingual bootstrapping approach for the more specific task of Word Translation Disambiguation (WTD) as opposed to the more general task of WSD. This approach does not need parallel corpora (just like our approach) and relies only on in-domain corpora from two languages. However, their work was evaluated only on a handful of target words (9 nouns) for WTD as opposed to the broader task of WSD. Our work instead focuses on improving the performance of all words WSD for two resource deprived languages using bilingual bootstrapping. At the heart of our work lies parameter projection fa</context>
</contexts>
<marker>Li, Li, 2004</marker>
<rawString>Hang Li and Cong Li. 2004. Word translation disambiguation using bilingual bootstrapping. Comput. Linguist., 30:1–22, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1922" citStr="McCarthy et al., 2004" startWordPosition="298" endWordPosition="301">nguage pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2) without compromising much on accuracy by leveraging o</context>
<context position="6857" citStr="McCarthy et al., 2004" startWordPosition="1110" endWordPosition="1113">995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. Recent work in this direction by Khapra et al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. However, they do not address situations where two resource deprived language</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 279, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Large vocabulary unsupervised word sense disambiguation with graph-based algorithms for sequence data labeling. In</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Human Language Technology and Empirical Methods in Natural Language Processing Conference (HLT/EMNLP),</booktitle>
<pages>411--418</pages>
<contexts>
<context position="1939" citStr="Mihalcea, 2005" startWordPosition="302" endWordPosition="303">tter than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2) without compromising much on accuracy by leveraging on the annotation </context>
<context position="6874" citStr="Mihalcea, 2005" startWordPosition="1114" endWordPosition="1115">ery small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. Recent work in this direction by Khapra et al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. However, they do not address situations where two resource deprived languages have aligned Wo</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Large vocabulary unsupervised word sense disambiguation with graph-based algorithms for sequence data labeling. In In Proceedings of the Joint Human Language Technology and Empirical Methods in Natural Language Processing Conference (HLT/EMNLP), pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Mohanty</author>
<author>Pushpak Bhattacharyya</author>
<author>Prabhakar Pande</author>
<author>Shraddha Kalele</author>
<author>Mitesh Khapra</author>
<author>Aditya Sharma</author>
</authors>
<title>Synset based multilingual dictionary: Insights, applications and challenges.</title>
<date>2008</date>
<booktitle>In Global Wordnet Conference.</booktitle>
<contexts>
<context position="8532" citStr="Mohanty et al. (2008)" startWordPosition="1383" endWordPosition="1386">ies only on in-domain corpora from two languages. However, their work was evaluated only on a handful of target words (9 nouns) for WTD as opposed to the broader task of WSD. Our work instead focuses on improving the performance of all words WSD for two resource deprived languages using bilingual bootstrapping. At the heart of our work lies parameter projection facilitated by a synset aligned 562 multilingual dictionary described in the next section. 3 Synset Aligned Multilingual Dictionary A novel and effective method of storage and use of dictionary in a multilingual setting was proposed by Mohanty et al. (2008). For the purpose of current discussion, we will refer to this multilingual dictionary framework as MultiDict. One important departure in this framework from the traditional dictionary is that synsets are linked, and after that the words inside the synsets are linked. The basic mapping is thus between synsets and thereafter between the words. Concepts L1 L2 L3 (English) (Hindi) (Marathi) 04321: {male {lwкA {m lgA a youth- child, (ladkaa), (mulgaa), ful male boy} bAlк porgA person (baalak), (porgaa), bQcA por (por)} (bachchaa)} Table 1: Multilingual Dictionary Framework Table 1 shows the struct</context>
</contexts>
<marker>Mohanty, Bhattacharyya, Pande, Kalele, Khapra, Sharma, 2008</marker>
<rawString>Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar Pande, Shraddha Kalele, Mitesh Khapra, and Aditya Sharma. 2008. Synset based multilingual dictionary: Insights, applications and challenges. In Global Wordnet Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1510" citStr="Ng and Lee, 1996" startWordPosition="234" endWordPosition="237">untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated da</context>
<context position="6755" citStr="Ng and Lee, 1996" startWordPosition="1092" endWordPosition="1095">r. 2 Related Work Bootstrapping for Word Sense Disambiguation was first discussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. Recent work in this direction by Khapra et al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Walker</author>
<author>R Amsler</author>
</authors>
<title>The use of machine readable dictionaries in sublanguage analysis.</title>
<date>1986</date>
<booktitle>In In Analyzing Language in Restricted Domains, Grishman and Kittredge (eds),</booktitle>
<pages>69--83</pages>
<publisher>LEA Press,</publisher>
<contexts>
<context position="1875" citStr="Walker and Amsler, 1986" startWordPosition="290" endWordPosition="293">sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2) withou</context>
<context position="6810" citStr="Walker and Amsler, 1986" startWordPosition="1102" endWordPosition="1105">isambiguation was first discussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. Recent work in this direction by Khapra et al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. However, they do not address </context>
</contexts>
<marker>Walker, Amsler, 1986</marker>
<rawString>D. Walker and R. Amsler. 1986. The use of machine readable dictionaries in sublanguage analysis. In In Analyzing Language in Restricted Domains, Grishman and Kittredge (eds), LEA Press, pages 69–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2069" citStr="Yarowsky, 1995" startWordPosition="322" endWordPosition="323">notated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2) without compromising much on accuracy by leveraging on the annotation work done in another language (L1). This is achieved with the help of a novel synsetaligned multilingual dictionary which facilita</context>
<context position="6240" citStr="Yarowsky, 1995" startWordPosition="1008" endWordPosition="1009">ows. In section 2 we present related work. Section 3 describes the Synset aligned multilingual dictionary which facilitates parameter projection. Section 4 discusses the work of Khapra et al. (2009) on parameter projection. In section 5 we discuss bilingual bootstrapping which is the main focus of our work followed by a brief discussion on monolingual bootstrapping. Section 6 describes the experimental setup. In section 7 we present the results followed by discussion in section 8. Section 9 concludes the paper. 2 Related Work Bootstrapping for Word Sense Disambiguation was first discussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data. This algorithm thus proceeds iteratively increasing the seed size in successive iterations. This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD. The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCa</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 189–196, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>