<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013969">
<title confidence="0.999226">
The Fixed-Size Ordinally-Forgetting Encoding Method
for Neural Network Language Models
</title>
<author confidence="0.999586">
Shiliang Zhang&apos;, Hui Jiang2, Mingbin Xu2, Junfeng Hou&apos;, Lirong Dai&apos;
</author>
<affiliation confidence="0.9123775">
&apos;National Engineering Laboratory for Speech and Language Information Processing
University of Science and Technology of China, Hefei, Anhui, China
2Department of Electrical Engineering and Computer Science
York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada
</affiliation>
<email confidence="0.994102">
{zsl2008,hjf176}@mail.ustc.edu.cn, {hj,xmb}@cse.yorku.ca, lrdai@ustc.edu.cn
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895294117647">
In this paper, we propose the new fixed-
size ordinally-forgetting encoding (FOFE)
method, which can almost uniquely en-
code any variable-length sequence of
words into a fixed-size representation.
FOFE can model the word order in a se-
quence using a simple ordinally-forgetting
mechanism according to the positions of
words. In this work, we have applied
FOFE to feedforward neural network lan-
guage models (FNN-LMs). Experimental
results have shown that without using any
recurrent feedbacks, FOFE based FNN-
LMs can significantly outperform not only
the standard fixed-input FNN-LMs but
also the popular recurrent neural network
(RNN) LMs.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965573770492">
Language models play an important role in many
applications like speech recognition, machine
translation, information retrieval and nature lan-
guage understanding. Traditionally, the back-off
n-gram models (Katz, 1987; Kneser, 1995) are
the standard approach to language modeling. Re-
cently, neural networks have been successfully ap-
plied to language modeling, yielding the state-
of-the-art performance in many tasks. In neural
network language models (NNLM), the feedfor-
ward neural networks (FNN) and recurrent neu-
ral networks (RNN) (Elman, 1990) are two pop-
ular architectures. The basic idea of NNLMs is
to use a projection layer to project discrete words
into a continuous space and estimate word con-
ditional probabilities in this space, which may be
smoother to better generalize to unseen contexts.
FNN language models (FNN-LM) (Bengio and
Ducharme, 2001; Bengio, 2003) usually use a lim-
ited history within a fixed-size context window
to predict the next word. RNN language mod-
els (RNN-LM) (Mikolov, 2010; Mikolov, 2012)
adopt a time-delayed recursive architecture for the
hidden layers to memorize the long-term depen-
dency in language. Therefore, it is widely re-
ported that RNN-LMs usually outperform FNN-
LMs in language modeling. While RNNs are the-
oretically powerful, the learning of RNNs needs to
use the so-called back-propagation through time
(BPTT) (Werbos, 1990) due to the internal recur-
rent feedback cycles. The BPTT significantly in-
creases the computational complexity of the learn-
ing algorithms and it may cause many problems
in learning, such as gradient vanishing and ex-
ploding (Bengio, 1994). More recently, some
new architectures have been proposed to solve
these problems. For example, the long short
term memory (LSTM) RNN (Hochreiter, 1997) is
an enhanced architecture to implement the recur-
rent feedbacks using various learnable gates, and
it has obtained promising results on handwriting
recognition (Graves, 2009) and sequence model-
ing (Graves, 2013).
Comparing with RNN-LMs, FNN-LMs can be
learned in a simpler and more efficient way. How-
ever, FNN-LMs can not model the long-term de-
pendency in language due to the fixed-size input
window. In this paper, we propose a novel encod-
ing method for discrete sequences, named fixed-
size ordinally-forgetting encoding (FOFE), which
can almost uniquely encode any variable-length
word sequence into a fixed-size code. Relying
on a constant forgetting factor, FOFE can model
the word order in a sequence based on a sim-
ple ordinally-forgetting mechanism, which uses
the position of each word in the sequence. Both
the theoretical analysis and the experimental sim-
ulation have shown that FOFE can provide al-
most unique codes for variable-length word se-
quences as long as the forgetting factor is prop-
erly selected. In this work, we apply FOFE to
</bodyText>
<page confidence="0.956164">
495
</page>
<bodyText confidence="0.924778588235294">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 495–500,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
neural network language models, where the fixed-
size FOFE codes are fed to FNNs as input to
predict next word, enabling FNN-LMs to model
long-term dependency in language. Experiments
on two benchmark tasks, Penn Treebank Corpus
(PTB) and Large Text Compression Benchmark
(LTCB), have shown that FOFE-based FNN-LMs
can not only significantly outperform the stan-
dard fixed-input FNN-LMs but also achieve better
performance than the popular RNN-LMs with or
without using LSTM. Moreover, our implementa-
tion also shows that FOFE based FNN-LMs can
be learned very efficiently on GPUs without the
complex BPTT procedure.
</bodyText>
<sectionHeader confidence="0.989945" genericHeader="method">
2 Our Approach: FOFE
</sectionHeader>
<bodyText confidence="0.99893">
Assume vocabulary size is K, NNLMs adopt the
1-of-K encoding vectors as input. In this case,
each word in vocabulary is represented as a one-
hot vector e E RK. The 1-of-K representation is a
context independent encoding method. When the
1-of-K representation is used to model a word in a
sequence, it can not model its history or context.
</bodyText>
<subsectionHeader confidence="0.986425">
2.1 Fixed-size Ordinally Forgetting Encoding
</subsectionHeader>
<bodyText confidence="0.9946158">
We propose a simple context-dependent encoding
method for any sequence consisting of discrete
symbols, namely fixed-size ordinally-forgetting
encoding (FOFE). Given a sequence of words (or
any discrete symbols), 5 = {w1, w2, · · · , wT},
each word wt is first represented by a 1-of-K rep-
resentation et, from the first word t = 1 to the end
of the sequence t = T, FOFE encodes each par-
tial sequence (history) based on a simple recursive
formula (with z0 = 0) as:
</bodyText>
<equation confidence="0.992177">
zt = α · zt−1 + et (1 &lt; t &lt; T) (1)
</equation>
<bodyText confidence="0.996300857142857">
where zt denotes the FOFE code for the partial
sequence up to wt, and α (0 &lt; α &lt; 1) is a con-
stant forgetting factor to control the influence of
the history on the current position. Let’s take a
simple example here, assume we have three sym-
bols in vocabulary, e.g., A, B, C, whose 1-of-
K codes are [1, 0, 0], [0, 1, 0] and [0, 0,1] respec-
tively. In this case, the FOFE code for the se-
quence {ABC} is [α2, α, 1], and that of {ABCBC}
is [α4, α + α3,1 + α2].
Obviously, FOFE can encode any variable-
length discrete sequence into a fixed-size code.
Moreover, it is a recursive context dependent en-
coding method that smartly models the order in-
</bodyText>
<figureCaption confidence="0.991209">
Figure 1: The FOFE-based FNN language model.
</figureCaption>
<bodyText confidence="0.999657166666667">
formation by various powers of the forgetting fac-
tor. Furthermore, FOFE has an appealing property
in modeling natural languages that the far-away
context will be gradually forgotten due to α &lt; 1
and the nearby contexts play much larger role in
the resultant FOFE codes.
</bodyText>
<subsectionHeader confidence="0.999713">
2.2 Uniqueness of FOFE codes
</subsectionHeader>
<bodyText confidence="0.999182153846154">
Given the vocabulary (of K symbols), for any se-
quence 5 with a length of T, based on the FOFE
code zT computed as above, if we can always de-
code the original sequence 5 unambiguously (per-
fectly recovering 5 from zT), we say FOFE is
unique.
Theorem 1 If the forgetting factor α satisfies 0 &lt;
α &lt; 0.5, FOFE is unique for any K and T.
The proof is simple because if the FOFE code
has a value αt in its i-th element, we may de-
termine the word wz occurs in the position t of
5 without ambiguity since no matter how many
times wz occurs in the far-away contexts (&lt; t),
they do not sum to αt (due to α &lt; 0.5). If wz ap-
pears in any closer context (&gt; t), the i-th element
must be larger than αt.
Theorem 2 For 0.5 &lt; α &lt; 1, given any finite
values of K and T, FOFE is almost unique every-
where for α E (0.5, 1.0), except only a finite set of
countable choices of α.
Refer to (Zhang et. al., 2015a) for the complete
proof. Based on Theorem 2, FOFE is unique al-
most everywhere between (0.5, 1.0) only except a
countable set of isolated choices of α. In practice,
the chance to exactly choose these isolated values
between (0.5, 1.0) is extremely slim, realistically
</bodyText>
<page confidence="0.994432">
496
</page>
<figureCaption confidence="0.999142">
Figure 2: Numbers of collisions in simulation.
</figureCaption>
<bodyText confidence="0.999987761904762">
almost impossible due to quantization errors in the
system. To verify this, we have run simulation ex-
periments for all possible sequences up to T = 20
symbols to count the number of collisions. Each
collision is defined as the maximum element-wise
difference between two FOFE codes (generated
from two different sequences) is less than a small
threshold c. In Figure 2, we have shown the num-
ber of collisions (out of the total 220 tested cases)
for various α values when c = 0.01, 0.001 and
0.0001.1 The simulation experiments have shown
that the chance of collision is extremely small even
when we allow a word to appear any times in the
context. Obviously, in a natural language, a word
normally does not appear repeatedly within a near
context. Moreover, we have run the simulation to
examine whether collisions actually occur in two
real text corpora, namely PTB (1M words) and
LTCB (160M words), using c = 0.01, we have
not observed a single collision for nine different α
values between [0.55, 1.0] (incremental 0.05).
</bodyText>
<subsectionHeader confidence="0.99482">
2.3 Implement FOFE for FNN-LMs
</subsectionHeader>
<bodyText confidence="0.996087090909091">
The architecture of a FOFE based neural network
language model (FOFE-FNNLM) is shown in Fig-
ure 1. It is similar to regular bigram FNN-LMs ex-
cept that it uses a FOFE code to feed into neural
network LM at each time. Moreover, the FOFE
can be easily scaled to higher orders like n-gram
NNLMs. For example, Figure 3 is an illustration
of a second order FOFE-based neural network lan-
guage model.
FOFE is a simple recursive encoding method
but a direct sequential implementation may not be
</bodyText>
<footnote confidence="0.84119575">
1When we use a bigger value for α, the magnitudes of the
resultant FOFE codes become much larger. As a result, the
number of collisions (as measured by a fixed absolute thresh-
old e) becomes smaller.
</footnote>
<figureCaption confidence="0.99973">
Figure 3: Diagram of 2nd-order FOFE FNN-LM.
</figureCaption>
<bodyText confidence="0.921584370370371">
efficient for the parallel computation platform like
GPUs. Here, we will show that the FOFE compu-
tation can be efficiently implemented as sentence-
by-sentence matrix multiplications, which are
suitable for the mini-batch based stochastic gra-
dient descent (SGD) method running on GPUs.
Given a sentence, S = {w1, w2, · · · , wT},
where each word is represented by a 1-of-K code
as et (1 ≤ t ≤ T). The FOFE codes for all par-
tial sequences in S can be computed based on the
following matrix multiplication:
1
α 1
α2 α 1
... ...1
αT−1 ··· α 1
where V is a matrix arranging all 1-of-K codes
of the words in the sentence row by row, and M
is a T-th order lower triangular matrix. Each row
vector of S represents a FOFE code of the partial
sequence up to each position in the sentence.
This matrix formulation can be easily extended
to a mini-batch consisting of several sentences.
Assume that a mini-batch is composed of N se-
quences, L = {S1 S2 · · · SN}, we can compute
the FOFE codes for all sentences in the mini-batch
as follows:
</bodyText>
<equation confidence="0.950944777777778">
M1
M2
= MV.
...
MN
�
� � � � � � � �
S =
I e1 1 = MV
</equation>
<table confidence="0.806576363636364">
e2
e3
...
eT
�
� � � � �
S=
1[ V1 1
V2
...
VN
</table>
<page confidence="0.995692">
497
</page>
<bodyText confidence="0.99997225">
When feeding the FOFE codes to FNN as
shown in Figure 1, we can compute the activation
signals (assume f is the activation function) in the
first hidden layer for all histories in S as follows:
</bodyText>
<equation confidence="0.99728">
( ) ( )
H = f ( M¯ ¯V)UW+b = f ¯M( ¯VU)W+b
</equation>
<bodyText confidence="0.999997785714286">
where U denotes the word embedding matrix that
projects the word indices onto a continuous low-
dimensional continuous space. As above, ¯VU
can be done efficiently by looking up the embed-
ding matrix. Therefore, for the computational ef-
ficiency purpose, we may apply FOFE to the word
embedding vectors instead of the original high-
dimensional one-hot vectors. In the backward
pass, we can calculate the gradients with the stan-
dard back-propagation (BP) algorithm rather than
BPTT. As a result, FOFE based FNN-LMs are the
same as the standard FNN-LMs in terms of com-
putational complexity in training, which is much
more efficient than RNN-LMs.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9999958">
We have evaluated the FOFE method for NNLMs
on two benchmark tasks: i) the Penn Treebank
(PTB) corpus of about 1M words, following the
same setup as (Mikolov, 2011). The vocabu-
lary size is limited to 10k. The preprocess-
ing method and the way to split data into train-
ing/validation/test sets are the same as (Mikolov,
2011). ii) The Large Text Compression Bench-
mark (LTCB) (Mahoney, 2011). In LTCB, we use
the enwik9 dataset, which is composed of the first
109 bytes of enwiki-20060303-pages-articles.xml.
We split it into three parts: training (153M), val-
idation (8.9M) and test (8.9M) sets. We limit the
vocabulary size to 80k for LTCB and replace all
out-of-vocabulary words by &lt;UNK&gt;. 2
</bodyText>
<subsectionHeader confidence="0.997747">
3.1 Experimental results on PTB
</subsectionHeader>
<bodyText confidence="0.999848888888889">
We have first evaluated the performance of the
traditional FNN-LMs, taking the previous several
words as input, denoted as n-gram FNN-LMs here.
We have trained neural networks with a linear pro-
jection layer (of 200 hidden nodes) and two hid-
den layers (of 400 nodes per layer). All hidden
units in networks use the rectified linear activation
function, i.e., f(x) = max(0, x). The nets are
initialized based on the normalized initialization
</bodyText>
<footnote confidence="0.981326666666667">
2Matlab codes are available at https://wiki.eecs.
yorku.ca/lab/MLL/projects:fofe:start for
readers to reproduce all results reported in this paper.
</footnote>
<figureCaption confidence="0.8610865">
Figure 4: Perplexities of FOFE FNNLMs as a
function of the forgetting factor.
</figureCaption>
<bodyText confidence="0.9999345">
in (Glorot, 2010), without using any pre-training.
We use SGD with a mini-batch size of 200 and an
initial learning rate of 0.4. The learning rate is kept
fixed as long as the perplexity on the validation set
decreases by at least 1. After that, we continue six
more epochs of training, where the learning rate
is halved after each epoch. The performance (in
perplexity) of several n-gram FNN-LMs (from bi-
gram to 6-gram) is shown in Table 1.
For the FOFE-FNNLMs, the net architecture
and the parameter setting are the same as above.
The mini-batch size is also 200 and each mini-
batch is composed of several sentences up to 200
words (the last sentence may be truncated). All
sentences in the corpus are randomly shuffled at
the beginning of each epoch. In this experiment,
we first investigate how the forgetting factor α
may affect the performance of LMs. We have
trained two FOFE-FNNLMs: i) 1st-order (using
zt as input to FNN for each time t; ii) 2nd-order
(using both zt and zt−1 as input for each time t,
with a forgetting factor varying between [0.0, 1.0].
Experimental results in Figure 4 have shown that
a good choice of α lies between [0.5, 0.8]. Us-
ing a too large or too small forgetting factor will
hurt the performance. A too small forgetting fac-
tor may limit the memory of the encoding while a
too large α may confuse LM with a far-away his-
tory. In the following experiments, we set α = 0.7
for the rest experiments in this paper.
In Table 1, we have summarized the perplexi-
ties on the PTB test set for various models. The
proposed FOFE-FNNLMs can significantly out-
perform the baseline FNN-LMs using the same
architecture. For example, the perplexity of the
baseline bigram FNNLM is 176, while the FOFE-
</bodyText>
<page confidence="0.999376">
498
</page>
<tableCaption confidence="0.99983">
Table 1: Perplexities on PTB for various LMs.
</tableCaption>
<table confidence="0.999902">
Model Test PPL
KN 5-gram (Mikolov, 2011) 141
FNNLM (Mikolov, 2012) 140
RNNLM (Mikolov, 2011) 123
LSTM (Graves, 2013) 117
bigram FNNLM 176
trigram FNNLM 131
4-gram FNNLM 118
5-gram FNNLM 114
6-gram FNNLM 113
1st-order FOFE-FNNLM 116
2nd-order FOFE-FNNLM 108
</table>
<tableCaption confidence="0.946373333333333">
Table 2: Perplexities on LTCB for various lan-
guage models. [M*N] denotes the sizes of the in-
put context window and projection layer.
</tableCaption>
<table confidence="0.999532846153846">
Model Architecture Test PPL
KN 3-gram - 156
KN 5-gram - 132
[1 *200]- 400 -400 -80k 241
[2 *200]- 400 -400 -80k 155
FNN-LM [2 *200]- 600 -600 -80k 150
[3 *200]- 400 -400 -80k 131
[4 *200]- 400 -400 -80k 125
RNN-LM [1*600] -80k 112
[1 *200]- 400 -400 -80k 120
FOFE [1 *200]- 600 -600 -80k 115
FNN-LM [2 *200]- 400 -400 -80k 112
[2 *200]- 600 -600 -80k 107
</table>
<bodyText confidence="0.991177076923077">
FNNLM can improve to 116. Moreover, the
FOFE-FNNLMs can even overtake a well-trained
RNNLM (400 hidden units) in (Mikolov, 2011)
and an LSTM in (Graves, 2013). It indicates
FOFE-FNNLMs can effectively model the long-
term dependency in language without using any
recurrent feedback. At last, the 2nd-order FOFE-
FNNLM can provide further improvement, yield-
ing the perplexity of 108 on PTB. It also outper-
forms all higher-order FNN-LMs (4-gram, 5-gram
and 6-gram), which are bigger in model size. To
our knowledge, this is one of the best reported re-
sults on PTB without model combination.
</bodyText>
<subsectionHeader confidence="0.999444">
3.2 Experimental results on LTCB
</subsectionHeader>
<bodyText confidence="0.999987821428571">
We have further examined the FOFE based FNN-
LMs on a much larger text corpus, i.e. LTCB,
which contains articles from Wikipedia. We have
trained several baseline systems: i) two n-gram
LMs (3-gram and 5-gram) using the modified
Kneser-Ney smoothing without count cutoffs; ii)
several traditional FNN-LMs with different model
sizes and input context windows (bigram, trigram,
4-gram and 5-gram); iii) an RNN-LM with one
hidden layer of 600 nodes using the toolkit in
(Mikolov, 2010), in which we have further used
a spliced sentence bunch in (Chen et al. 2014)
to speed up the training on GPUs. Moreover, we
have examined four FOFE based FNN-LMs with
various model sizes and input window sizes (two
1st-order FOFE models and two 2nd-order ones).
For all NNLMs, we have used an output layer of
the full vocabulary (80k words). In these exper-
iments, we have used an initial learning rate of
0.01, and a bigger mini-batch of 500 for FNN-
LMMs and of 256 sentences for the RNN and
FOFE models. Experimental results in Table 2
have shown that the FOFE-based FNN-LMs can
significantly outperform the baseline FNN-LMs
(including some larger higher-order models) and
also slightly overtake the popular RNN-based LM,
yielding the best result (perplexity of 107) on the
test set.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999104">
In this paper, we propose the fixed-size ordinally-
forgetting encoding (FOFE) method to almost
uniquely encode any variable-length sequence into
a fixed-size code. In this work, FOFE has been
successfully applied to neural network language
modeling. Next, FOFE may be combined with
neural networks (Zhang and Jiang, 2015; Zhang
et. al., 2015b) for other NLP tasks, such as sen-
tence modeling/matching, paraphrase detection,
machine translation, question and answer and etc.
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999926">
This work was supported in part by the Science
and Technology Development of Anhui Province,
China (Grants No. 2014z02006) and the Funda-
mental Research Funds for the Central Universi-
ties from China, as well as an NSERC Discov-
ery grant from Canadian federal government. We
appreciate Dr. Barlas Oguz at Microsoft for his
insightful comments and constructive suggestions
on Theorem 2.
</bodyText>
<page confidence="0.998932">
499
</page>
<sectionHeader confidence="0.992871" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.965326884615384">
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing (ASSP), Volume 35,
no 3, pages 400-401.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc.
of International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pages 181-184.
Paul Werbos. 1990. Back-propagation through time:
what it does and how to do it. Proceedings of the
IEEE, volume 78, no 10, pages 1550-1560.
Yoshua Bengio, Patrice Simard and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neural
Networks volume 5, no 2, pages 157-166.
Yoshua Bengio and Rejean Ducharme. 2001. A neural
probabilistic language model. In Proc. of NIPS, vol-
ume 13.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent,
and Christian Jauvin. 2003. A neural probabilistic
language model. Journal of Machine Learning Re-
search, volume 3, no 2, pages 1137-1155.
Jeffery Elman. 1990. Finding structure in time. Cogni-
tive science, volume 14, no 2, pages 179-211.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proc. of
Interspeech, pages 1045-1048.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In Proc. of International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
5528-5531.
Tomas Mikolov and Geoffrey Zweig. 2012. Context de-
pendent recurrent neural network language model.
In Proc. of SLT, pages 234-239.
X. Chen, Y. Wang, X. Liu, et al. 2014. Efficient GPU-
based training of recurrent neural network language
models using spliced sentence bunch. In Proc. of In-
terspeech.
Ilya Sutskever and Geoffrey Hinton. 2010. Temporal-
kernel recurrent neural networks. Neural Networks.
pages 239-243.
Yong-Zhe Shi, Wei-Qiang Zhang, Meng Cai and Jia
Liu. 2013. Temporal kernel neural network language
model. In Proc. of International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
pages 8247-8251.
Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long
short-term memory. Neural computation, volume 9,
no 8, pages 1735-1780.
Alex Graves and Jurgen Schmidhuber. 2009. Offline
handwriting recognition with multidimensional re-
current neural networks. In Proc. of NIPS. pages
545-552.
Alex Graves. 2013. Generating sequences with
recurrent neural networks. arXiv preprint
arXiv:1308.0850.
Glorot Xavier and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proc. of AISTATS.
Matt Mahoney. 2011. Large Text Compression Bench-
mark. In http://mattmahoney.net/dc/textdata.html.
Barlas Oguz. 2015. Personal Communications.
Shiling Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou
and LiRong Dai. 2015a. A Fixed-Size Encoding
Method for Variable-Length Sequences with its
Application to Neural Network Language Models.
arXiv:1505.01504.
Shiliang Zhang and Hui Jiang. 2015. Hybrid Orthog-
onal Projection and Estimation (HOPE): A New
Framework to Probe and Learn Neural Networks.
arXiv:1502.00702.
Shiliang Zhang, Hui Jiang and Lirong Dai. 2015b. The
New HOPE Way to Learn Neural Networks. Proc.
of Deep Learning Workshop at ICML 2015.
</reference>
<page confidence="0.995559">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444014">
<title confidence="0.9996085">The Fixed-Size Ordinally-Forgetting Encoding for Neural Network Language Models</title>
<author confidence="0.91626">Hui Mingbin Junfeng Lirong</author>
<affiliation confidence="0.85081">Engineering Laboratory for Speech and Language Information University of Science and Technology of China, Hefei, Anhui, of Electrical Engineering and Computer</affiliation>
<address confidence="0.978517">York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada</address>
<email confidence="0.991766">lrdai@ustc.edu.cn</email>
<abstract confidence="0.9921735">In this paper, we propose the new fixedsize ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN- LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular recurrent neural network (RNN) LMs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing (ASSP),</journal>
<volume>35</volume>
<pages>400--401</pages>
<contexts>
<context position="1382" citStr="Katz, 1987" startWordPosition="184" endWordPosition="185">inally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNNLMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular recurrent neural network (RNN) LMs. 1 Introduction Language models play an important role in many applications like speech recognition, machine translation, information retrieval and nature language understanding. Traditionally, the back-off n-gram models (Katz, 1987; Kneser, 1995) are the standard approach to language modeling. Recently, neural networks have been successfully applied to language modeling, yielding the stateof-the-art performance in many tasks. In neural network language models (NNLM), the feedforward neural networks (FNN) and recurrent neural networks (RNN) (Elman, 1990) are two popular architectures. The basic idea of NNLMs is to use a projection layer to project discrete words into a continuous space and estimate word conditional probabilities in this space, which may be smoother to better generalize to unseen contexts. FNN language mo</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing (ASSP), Volume 35, no 3, pages 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>181--184</pages>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 181-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Werbos</author>
</authors>
<title>Back-propagation through time: what it does and how to do it.</title>
<date>1990</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>78</volume>
<pages>1550--1560</pages>
<contexts>
<context position="2537" citStr="Werbos, 1990" startWordPosition="364" endWordPosition="365">r to better generalize to unseen contexts. FNN language models (FNN-LM) (Bengio and Ducharme, 2001; Bengio, 2003) usually use a limited history within a fixed-size context window to predict the next word. RNN language models (RNN-LM) (Mikolov, 2010; Mikolov, 2012) adopt a time-delayed recursive architecture for the hidden layers to memorize the long-term dependency in language. Therefore, it is widely reported that RNN-LMs usually outperform FNNLMs in language modeling. While RNNs are theoretically powerful, the learning of RNNs needs to use the so-called back-propagation through time (BPTT) (Werbos, 1990) due to the internal recurrent feedback cycles. The BPTT significantly increases the computational complexity of the learning algorithms and it may cause many problems in learning, such as gradient vanishing and exploding (Bengio, 1994). More recently, some new architectures have been proposed to solve these problems. For example, the long short term memory (LSTM) RNN (Hochreiter, 1997) is an enhanced architecture to implement the recurrent feedbacks using various learnable gates, and it has obtained promising results on handwriting recognition (Graves, 2009) and sequence modeling (Graves, 201</context>
</contexts>
<marker>Werbos, 1990</marker>
<rawString>Paul Werbos. 1990. Back-propagation through time: what it does and how to do it. Proceedings of the IEEE, volume 78, no 10, pages 1550-1560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult.</title>
<date>1994</date>
<journal>IEEE Transactions on Neural Networks</journal>
<volume>5</volume>
<pages>157--166</pages>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks volume 5, no 2, pages 157-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2001</date>
<booktitle>In Proc. of NIPS,</booktitle>
<volume>13</volume>
<contexts>
<context position="2022" citStr="Bengio and Ducharme, 2001" startWordPosition="282" endWordPosition="285"> are the standard approach to language modeling. Recently, neural networks have been successfully applied to language modeling, yielding the stateof-the-art performance in many tasks. In neural network language models (NNLM), the feedforward neural networks (FNN) and recurrent neural networks (RNN) (Elman, 1990) are two popular architectures. The basic idea of NNLMs is to use a projection layer to project discrete words into a continuous space and estimate word conditional probabilities in this space, which may be smoother to better generalize to unseen contexts. FNN language models (FNN-LM) (Bengio and Ducharme, 2001; Bengio, 2003) usually use a limited history within a fixed-size context window to predict the next word. RNN language models (RNN-LM) (Mikolov, 2010; Mikolov, 2012) adopt a time-delayed recursive architecture for the hidden layers to memorize the long-term dependency in language. Therefore, it is widely reported that RNN-LMs usually outperform FNNLMs in language modeling. While RNNs are theoretically powerful, the learning of RNNs needs to use the so-called back-propagation through time (BPTT) (Werbos, 1990) due to the internal recurrent feedback cycles. The BPTT significantly increases the </context>
</contexts>
<marker>Bengio, Ducharme, 2001</marker>
<rawString>Yoshua Bengio and Rejean Ducharme. 2001. A neural probabilistic language model. In Proc. of NIPS, volume 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, volume 3, no 2, pages 1137-1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffery Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<pages>179--211</pages>
<contexts>
<context position="1710" citStr="Elman, 1990" startWordPosition="233" endWordPosition="234">also the popular recurrent neural network (RNN) LMs. 1 Introduction Language models play an important role in many applications like speech recognition, machine translation, information retrieval and nature language understanding. Traditionally, the back-off n-gram models (Katz, 1987; Kneser, 1995) are the standard approach to language modeling. Recently, neural networks have been successfully applied to language modeling, yielding the stateof-the-art performance in many tasks. In neural network language models (NNLM), the feedforward neural networks (FNN) and recurrent neural networks (RNN) (Elman, 1990) are two popular architectures. The basic idea of NNLMs is to use a projection layer to project discrete words into a continuous space and estimate word conditional probabilities in this space, which may be smoother to better generalize to unseen contexts. FNN language models (FNN-LM) (Bengio and Ducharme, 2001; Bengio, 2003) usually use a limited history within a fixed-size context window to predict the next word. RNN language models (RNN-LM) (Mikolov, 2010; Mikolov, 2012) adopt a time-delayed recursive architecture for the hidden layers to memorize the long-term dependency in language. There</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffery Elman. 1990. Finding structure in time. Cognitive science, volume 14, no 2, pages 179-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
</authors>
<title>Cernock`y and Sanjeev Khudanpur.</title>
<date></date>
<booktitle>In Proc. of Interspeech,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, </marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proc. of Interspeech, pages 1045-1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>5528--5531</pages>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5528-5531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Context dependent recurrent neural network language model.</title>
<date>2012</date>
<booktitle>In Proc. of SLT,</booktitle>
<pages>234--239</pages>
<marker>Mikolov, Zweig, 2012</marker>
<rawString>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In Proc. of SLT, pages 234-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Chen</author>
<author>Y Wang</author>
<author>X Liu</author>
</authors>
<title>Efficient GPUbased training of recurrent neural network language models using spliced sentence bunch.</title>
<date>2014</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="16828" citStr="Chen et al. 2014" startWordPosition="2875" endWordPosition="2878">ithout model combination. 3.2 Experimental results on LTCB We have further examined the FOFE based FNNLMs on a much larger text corpus, i.e. LTCB, which contains articles from Wikipedia. We have trained several baseline systems: i) two n-gram LMs (3-gram and 5-gram) using the modified Kneser-Ney smoothing without count cutoffs; ii) several traditional FNN-LMs with different model sizes and input context windows (bigram, trigram, 4-gram and 5-gram); iii) an RNN-LM with one hidden layer of 600 nodes using the toolkit in (Mikolov, 2010), in which we have further used a spliced sentence bunch in (Chen et al. 2014) to speed up the training on GPUs. Moreover, we have examined four FOFE based FNN-LMs with various model sizes and input window sizes (two 1st-order FOFE models and two 2nd-order ones). For all NNLMs, we have used an output layer of the full vocabulary (80k words). In these experiments, we have used an initial learning rate of 0.01, and a bigger mini-batch of 500 for FNNLMMs and of 256 sentences for the RNN and FOFE models. Experimental results in Table 2 have shown that the FOFE-based FNN-LMs can significantly outperform the baseline FNN-LMs (including some larger higher-order models) and als</context>
</contexts>
<marker>Chen, Wang, Liu, 2014</marker>
<rawString>X. Chen, Y. Wang, X. Liu, et al. 2014. Efficient GPUbased training of recurrent neural network language models using spliced sentence bunch. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Temporalkernel recurrent neural networks. Neural Networks.</title>
<date>2010</date>
<pages>239--243</pages>
<marker>Sutskever, Hinton, 2010</marker>
<rawString>Ilya Sutskever and Geoffrey Hinton. 2010. Temporalkernel recurrent neural networks. Neural Networks. pages 239-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong-Zhe Shi</author>
<author>Wei-Qiang Zhang</author>
<author>Meng Cai</author>
<author>Jia Liu</author>
</authors>
<title>Temporal kernel neural network language model.</title>
<date>2013</date>
<booktitle>In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP).</booktitle>
<pages>8247--8251</pages>
<marker>Shi, Zhang, Cai, Liu, 2013</marker>
<rawString>Yong-Zhe Shi, Wei-Qiang Zhang, Meng Cai and Jia Liu. 2013. Temporal kernel neural network language model. In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP). pages 8247-8251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>Jurgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<journal>Neural computation,</journal>
<volume>9</volume>
<pages>1735--1780</pages>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural computation, volume 9, no 8, pages 1735-1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Jurgen Schmidhuber</author>
</authors>
<title>Offline handwriting recognition with multidimensional recurrent neural networks.</title>
<date>2009</date>
<booktitle>In Proc. of NIPS.</booktitle>
<pages>545--552</pages>
<marker>Graves, Schmidhuber, 2009</marker>
<rawString>Alex Graves and Jurgen Schmidhuber. 2009. Offline handwriting recognition with multidimensional recurrent neural networks. In Proc. of NIPS. pages 545-552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</title>
<date>2013</date>
<contexts>
<context position="3139" citStr="Graves, 2013" startWordPosition="456" endWordPosition="457">rbos, 1990) due to the internal recurrent feedback cycles. The BPTT significantly increases the computational complexity of the learning algorithms and it may cause many problems in learning, such as gradient vanishing and exploding (Bengio, 1994). More recently, some new architectures have been proposed to solve these problems. For example, the long short term memory (LSTM) RNN (Hochreiter, 1997) is an enhanced architecture to implement the recurrent feedbacks using various learnable gates, and it has obtained promising results on handwriting recognition (Graves, 2009) and sequence modeling (Graves, 2013). Comparing with RNN-LMs, FNN-LMs can be learned in a simpler and more efficient way. However, FNN-LMs can not model the long-term dependency in language due to the fixed-size input window. In this paper, we propose a novel encoding method for discrete sequences, named fixedsize ordinally-forgetting encoding (FOFE), which can almost uniquely encode any variable-length word sequence into a fixed-size code. Relying on a constant forgetting factor, FOFE can model the word order in a sequence based on a simple ordinally-forgetting mechanism, which uses the position of each word in the sequence. Bo</context>
<context position="15023" citStr="Graves, 2013" startWordPosition="2568" endWordPosition="2569">mit the memory of the encoding while a too large α may confuse LM with a far-away history. In the following experiments, we set α = 0.7 for the rest experiments in this paper. In Table 1, we have summarized the perplexities on the PTB test set for various models. The proposed FOFE-FNNLMs can significantly outperform the baseline FNN-LMs using the same architecture. For example, the perplexity of the baseline bigram FNNLM is 176, while the FOFE498 Table 1: Perplexities on PTB for various LMs. Model Test PPL KN 5-gram (Mikolov, 2011) 141 FNNLM (Mikolov, 2012) 140 RNNLM (Mikolov, 2011) 123 LSTM (Graves, 2013) 117 bigram FNNLM 176 trigram FNNLM 131 4-gram FNNLM 118 5-gram FNNLM 114 6-gram FNNLM 113 1st-order FOFE-FNNLM 116 2nd-order FOFE-FNNLM 108 Table 2: Perplexities on LTCB for various language models. [M*N] denotes the sizes of the input context window and projection layer. Model Architecture Test PPL KN 3-gram - 156 KN 5-gram - 132 [1 *200]- 400 -400 -80k 241 [2 *200]- 400 -400 -80k 155 FNN-LM [2 *200]- 600 -600 -80k 150 [3 *200]- 400 -400 -80k 131 [4 *200]- 400 -400 -80k 125 RNN-LM [1*600] -80k 112 [1 *200]- 400 -400 -80k 120 FOFE [1 *200]- 600 -600 -80k 115 FNN-LM [2 *200]- 400 -400 -80k 112</context>
</contexts>
<marker>Graves, 2013</marker>
<rawString>Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glorot Xavier</author>
<author>Yoshua Bengio</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In Proc. of AISTATS.</booktitle>
<marker>Xavier, Bengio, 2010</marker>
<rawString>Glorot Xavier and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proc. of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Mahoney</author>
</authors>
<title>Large Text Compression Benchmark. In http://mattmahoney.net/dc/textdata.html. Barlas Oguz.</title>
<date>2011</date>
<tech>Personal Communications.</tech>
<contexts>
<context position="12142" citStr="Mahoney, 2011" startWordPosition="2078" endWordPosition="2079"> back-propagation (BP) algorithm rather than BPTT. As a result, FOFE based FNN-LMs are the same as the standard FNN-LMs in terms of computational complexity in training, which is much more efficient than RNN-LMs. 3 Experiments We have evaluated the FOFE method for NNLMs on two benchmark tasks: i) the Penn Treebank (PTB) corpus of about 1M words, following the same setup as (Mikolov, 2011). The vocabulary size is limited to 10k. The preprocessing method and the way to split data into training/validation/test sets are the same as (Mikolov, 2011). ii) The Large Text Compression Benchmark (LTCB) (Mahoney, 2011). In LTCB, we use the enwik9 dataset, which is composed of the first 109 bytes of enwiki-20060303-pages-articles.xml. We split it into three parts: training (153M), validation (8.9M) and test (8.9M) sets. We limit the vocabulary size to 80k for LTCB and replace all out-of-vocabulary words by &lt;UNK&gt;. 2 3.1 Experimental results on PTB We have first evaluated the performance of the traditional FNN-LMs, taking the previous several words as input, denoted as n-gram FNN-LMs here. We have trained neural networks with a linear projection layer (of 200 hidden nodes) and two hidden layers (of 400 nodes p</context>
</contexts>
<marker>Mahoney, 2011</marker>
<rawString>Matt Mahoney. 2011. Large Text Compression Benchmark. In http://mattmahoney.net/dc/textdata.html. Barlas Oguz. 2015. Personal Communications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiling Zhang</author>
<author>Hui Jiang</author>
<author>Mingbin Xu</author>
</authors>
<title>Junfeng Hou and LiRong Dai. 2015a. A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models.</title>
<date>1505</date>
<marker>Zhang, Jiang, Xu, 1505</marker>
<rawString>Shiling Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou and LiRong Dai. 2015a. A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models. arXiv:1505.01504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiliang Zhang</author>
<author>Hui Jiang</author>
</authors>
<title>Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks.</title>
<date>2015</date>
<tech>arXiv:1502.00702.</tech>
<marker>Zhang, Jiang, 2015</marker>
<rawString>Shiliang Zhang and Hui Jiang. 2015. Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks. arXiv:1502.00702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiliang Zhang</author>
<author>Hui Jiang</author>
<author>Lirong Dai</author>
</authors>
<title>The New HOPE Way to Learn Neural Networks.</title>
<date>2015</date>
<booktitle>Proc. of Deep Learning Workshop at ICML</booktitle>
<marker>Zhang, Jiang, Dai, 2015</marker>
<rawString>Shiliang Zhang, Hui Jiang and Lirong Dai. 2015b. The New HOPE Way to Learn Neural Networks. Proc. of Deep Learning Workshop at ICML 2015.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>