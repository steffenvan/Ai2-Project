<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.997392">
Fast Online Training with Frequency-Adaptive Learning Rates for Chinese
Word Segmentation and New Word Detection
</title>
<author confidence="0.999618">
Xu Sun†, Houfeng Wang‡, Wenjie Li††Department of Computing, The Hong Kong Polytechnic University
</author>
<affiliation confidence="0.995608">
‡Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, China
</affiliation>
<email confidence="0.994391">
{csxsun, cswjli}@comp.polyu.edu.hk wanghf@pku.edu.cn
</email>
<sectionHeader confidence="0.993762" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999593">
We present a joint model for Chinese
word segmentation and new word detection.
We present high dimensional new features,
including word-based features and enriched
edge (label-transition) features, for the joint
modeling. As we know, training a word
segmentation system on large-scale datasets
is already costly. In our case, adding high
dimensional new features will further slow
down the training speed. To solve this
problem, we propose a new training method,
adaptive online gradient descent based on
feature frequency information, for very fast
online training of the parameters, even given
large-scale datasets with high dimensional
features. Compared with existing training
methods, our training method is an order
magnitude faster in terms of training time, and
can achieve equal or even higher accuracies.
The proposed fast training method is a general
purpose optimization method, and it is not
limited in the specific task discussed in this
paper.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941468085106">
Since Chinese sentences are written as continuous
sequences of characters, segmenting a character
sequence into words is normally the first step
in the pipeline of Chinese text processing. The
major problem of Chinese word segmentation
is the ambiguity. Chinese character sequences
are normally ambiguous, and new words (out-
of-vocabulary words) are a major source of the
ambiguity. A typical category of new words
is named entities, including organization names,
person names, location names, and so on.
In this paper, we present high dimensional
new features, including word-based features and
enriched edge (label-transition) features, for the
joint modeling of Chinese word segmentation
(CWS) and new word detection (NWD). While most
of the state-of-the-art CWS systems used semi-
Markov conditional random fields or latent variable
conditional random fields, we simply use a single
first-order conditional random fields (CRFs) for
the joint modeling. The semi-Markov CRFs and
latent variable CRFs relax the Markov assumption
of CRFs to express more complicated dependencies,
and therefore to achieve higher disambiguation
power. Alternatively, our plan is not to relax
Markov assumption of CRFs, but to exploit more
complicated dependencies via using refined high-
dimensional features. The advantage of our choice
is the simplicity of our model. As a result, our
CWS model can be more efficient compared with
the heavier systems, and with similar or even higher
accuracy because of using refined features.
As we know, training a word segmentation system
on large-scale datasets is already costly. In our
case, adding high dimensional new features will
further slow down the training speed. To solve this
challenging problem, we propose a new training
method, adaptive online gradient descent based on
feature frequency information (ADF), for very fast
word segmentation with new word detection, even
given large-scale datasets with high dimensional
features. In the proposed training method, we try
to use more refined learning rates. Instead of using
a single learning rate (a scalar) for all weights,
we extend the learning rate scalar to a learning
rate vector based on feature frequency information
in the updating. By doing so, each weight has
</bodyText>
<page confidence="0.982407">
253
</page>
<note confidence="0.985727">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253–262,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999718928571429">
its own learning rate adapted on feature frequency
information. We will show that this can significantly
improve the convergence speed of online learning.
We approximate the learning rate vector based
on feature frequency information in the updating
process. Our proposal is based on the intuition
that a feature with higher frequency in the training
process should be with a learning rate that is decayed
faster. Based on this intuition, we will show the
formalized training algorithm later. We will show in
experiments that our solution is an order magnitude
faster compared with exiting learning methods, and
can achieve equal or even higher accuracies.
The contribution of this work is as follows:
</bodyText>
<listItem confidence="0.996437555555556">
• We propose a general purpose fast online
training method, ADF. The proposed training
method requires only a few passes to complete
the training.
• We propose a joint model for Chinese word
segmentation and new word detection.
• Compared with prior work, our system
achieves better accuracies on both word
segmentation and new word detection.
</listItem>
<sectionHeader confidence="0.999107" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998305">
First, we review related work on word segmentation
and new word detection. Then, we review popular
online training methods, in particular stochastic
gradient descent (SGD).
</bodyText>
<subsectionHeader confidence="0.980439">
2.1 Word Segmentation and New Word
Detection
</subsectionHeader>
<bodyText confidence="0.999975894736842">
Conventional approaches to Chinese word
segmentation treat the problem as a sequential
labeling task (Xue, 2003; Peng et al., 2004; Tseng
et al., 2005; Asahara et al., 2005; Zhao et al.,
2010). To achieve high accuracy, most of the state-
of-the-art systems are heavy probabilistic systems
using semi-Markov assumptions or latent variables
(Andrew, 2006; Sun et al., 2009b). For example,
one of the state-of-the-art CWS system is the latent
variable conditional random field (Sun et al., 2008;
Sun and Tsujii, 2009) system presented in Sun et al.
(2009b). It is a heavy probabilistic model and it is
slow in training. A few other state-of-the-art CWS
systems are using semi-Markov perceptron methods
or voting systems based on multiple semi-Markov
perceptron segmenters (Zhang and Clark, 2007;
Sun, 2010). Those semi-Markov perceptron systems
are moderately faster than the heavy probabilistic
systems using semi-Markov conditional random
fields or latent variable conditional random fields.
However, a disadvantage of the perceptron style
systems is that they can not provide probabilistic
information.
On the other hand, new word detection is also one
of the important problems in Chinese information
processing. Many statistical approaches have been
proposed (J. Nie and Jin, 1995; Chen and Bai, 1998;
Wu and Jiang, 2000; Peng et al., 2004; Chen and
Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and
Luke, 2004; Wu et al., 2011). New word detection
is normally considered as a separate process from
segmentation. There were studies trying to solve this
problem jointly with CWS. However, the current
studies are limited. Integrating the two tasks would
benefit both segmentation and new word detection.
Our method provides a convenient framework for
doing this. Our new word detection is not a stand-
alone process, but an integral part of segmentation.
</bodyText>
<subsectionHeader confidence="0.997168">
2.2 Online Training
</subsectionHeader>
<bodyText confidence="0.999984166666667">
The most representative online training method
is the SGD method. The SGD uses a small
randomly-selected subset of the training samples to
approximate the gradient of an objective function.
The number of training samples used for this
approximation is called the batch size. By using a
smaller batch size, one can update the parameters
more frequently and speed up the convergence. The
extreme case is a batch size of 1, and it gives the
maximum frequency of updates, which we adopt in
this work. Then, the model parameters are updated
in such a way:
</bodyText>
<equation confidence="0.999452">
wt+1 = wt + γtVwtGstoch(xi,wt), (1)
</equation>
<bodyText confidence="0.999908625">
where t is the update counter, γt is the learning rate,
and Gstoch(xi,wt) is the stochastic loss function
based on a training sample xi.
There were accelerated versions of SGD,
including stochastic meta descent (Vishwanathan
et al., 2006) and periodic step-size adaptation
online learning (Hsu et al., 2009). Compared with
those two methods, our proposal is fundamentally
</bodyText>
<page confidence="0.996731">
254
</page>
<bodyText confidence="0.999931">
different. Those two methods are using 2nd-order
gradient (Hessian) information for accelerated
training, while our accelerated training method
does not need such 2nd-order gradient information,
which is costly and complicated. Our ADF training
method is based on feature frequency adaptation,
and there is no prior work on using feature frequency
information for accelerating online training.
Other online training methods includes averaged
SGD with feedback (Sun et al., 2010; Sun et al.,
2011), latent variable perceptron training (Sun et al.,
2009a), and so on. Those methods are less related to
this paper.
</bodyText>
<sectionHeader confidence="0.997572" genericHeader="method">
3 System Architecture
</sectionHeader>
<subsectionHeader confidence="0.998956">
3.1 A Joint Model Based on CRFs
</subsectionHeader>
<bodyText confidence="0.9981535">
First, we briefly review CRFs. CRFs are proposed
as a method for structured classification by solving
“the label bias problem” (Lafferty et al., 2001).
Assuming a feature function that maps a pair of
observation sequence x and label sequence y to a
global feature vector f, the probability of a label
sequence y conditioned on the observation sequence
x is modeled as follows (Lafferty et al., 2001):
</bodyText>
<equation confidence="0.960071666666667">
exp {wTf(y,x)}
P(y|x,w) = (2)
Evy′ exp {wT f ( y&amp;quot; x) }
</equation>
<bodyText confidence="0.9938228">
where w is a parameter vector.
Given a training set consisting of n labeled
sequences, zi = (xi,yi), for i = 1... n, parameter
estimation is performed by maximizing the objective
function,
</bodyText>
<equation confidence="0.996799">
n
L(w) = log P (yi|xi,w) − R(w). (3)
i=1
</equation>
<bodyText confidence="0.927525125">
The first term of this equation represents a
conditional log-likelihood of a training data. The
second term is a regularizer for reducing overfitting.
We employed an L2 prior, R(w) = ||�||2
2σ2 . In what
follows, we denote the conditional log-likelihood of
each sample log P(yi|xi,w) as ℓ(zi,w). The final
objective function is as follows:
</bodyText>
<equation confidence="0.947917">
ℓ(zi,w) − ||w||2
2σ2 .(4)
</equation>
<bodyText confidence="0.999893590909091">
Since no word list can be complete, new word
identification is an important task in Chinese NLP.
New words in input text are often incorrectly
segmented into single-character or other very short
words (Chen and Bai, 1998). This phenomenon
will also undermine the performance of Chinese
word segmentation. We consider here new word
detection as an integral part of segmentation,
aiming to improve both segmentation and new word
detection: detected new words are added to the
word list lexicon in order to improve segmentation.
Based on our CRF word segmentation system,
we can compute a probability for each segment.
When we find some word segments are of reliable
probabilities yet they are not in the existing word
list, we then treat those “confident” word segments
as new words and add them into the existing word
list. Based on preliminary experiments, we treat
a word segment as a new word if its probability
is larger than 0.5. Newly detected words are re-
incorporated into word segmentation for improving
segmentation accuracies.
</bodyText>
<subsectionHeader confidence="0.999549">
3.2 New Features
</subsectionHeader>
<bodyText confidence="0.999772">
Here, we will describe high dimensional new
features for the system.
</bodyText>
<subsectionHeader confidence="0.48603">
3.2.1 Word-based Features
</subsectionHeader>
<bodyText confidence="0.999983277777778">
There are two ideas in deriving the refined
features. The first idea is to exploit word features
for node features of CRFs. Note that, although our
model is a Markov CRF model, we can still use word
features to learn word information in the training
data. To derive word features, first of all, our system
automatically collect a list of word unigrams and
bigrams from the training data. To avoid overfitting,
we only collect the word unigrams and bigrams
whose frequency is larger than 2 in the training set.
This list of word unigrams and bigrams are then used
as a unigram-dictionary and a bigram-dictionary to
generate word-based unigram and bigram features.
The word-based features are indicator functions that
fire when the local character sequence matches a
word unigram or bigram occurred in the training
data. The word-based feature templates derived for
the label yi are as follows:
</bodyText>
<listItem confidence="0.7860515">
• unigram1(x, yi) ← [xj,i, yi], if the
character sequence xj,i matches a word w ∈ U,
</listItem>
<equation confidence="0.958664">
n
L(w) =
i=1
</equation>
<page confidence="0.975523">
255
</page>
<bodyText confidence="0.9988345">
with the constraint i − 6 &lt; j &lt; i. The item
xj,i represents the character sequence xj ... xi.
U represents the unigram-dictionary collected
from the training data.
</bodyText>
<listItem confidence="0.944116692307692">
• unigram2(x, yi) +— [xi,k, yi], if the
character sequence xi,k matches a word w E U,
with the constraint i &lt; k &lt; i + 6.
• bigram1(x, yi) — [xj,i−1, xi,k, yi], if
the word bigram candidate [xj,i−1, xi,k] hits
a word bigram [wi, wj] E B, and satisfies
the aforementioned constraints on j and k. B
represents the word bigram dictionary collected
from the training data.
• bigram2(x, yi) — [xj,i, xi+1,k, yi], if
the word bigram candidate [xj,i, xi+1,k] hits a
word bigram [wi, wj] E B, and satisfies the
aforementioned constraints on j and k.
</listItem>
<bodyText confidence="0.972287666666667">
We also employ the traditional character-based
features. For each label yi, we use the feature
templates as follows:
</bodyText>
<listItem confidence="0.968833">
• Character unigrams locating at positions i − 2,
i − 1, i, i + 1 and i + 2
• Character bigrams locating at positions i −
2, i − 1, i and i + 1
• Whether xj and xj+1 are identical, for j = i −
2, ..., i + 1
• Whether xj and xj+2 are identical, for j = i −
3, ..., i + 1
</listItem>
<bodyText confidence="0.999178">
The latter two feature templates are designed
to detect character or word reduplication, a
morphological phenomenon that can influence word
segmentation in Chinese.
</bodyText>
<subsectionHeader confidence="0.95726">
3.2.2 High Dimensional Edge Features
</subsectionHeader>
<bodyText confidence="0.9999931875">
The node features discussed above are based on
a single label yi. CRFs also have edge features
that are based on label transitions. The second idea
is to incorporate local observation information of
x in edge features. For traditional implementation
of CRF systems (e.g., the HCRF package), usually
the edges features contain only the information
of yi−1 and yi, and without the information of
the observation sequence (i.e., x). The major
reason for this simple realization of edge features
in traditional CRF implementation is for reducing
the dimension of features. Otherwise, there can
be an explosion of edge features in some tasks.
For example, in part-of-speech tagging tasks, there
can be more than 40 labels and more than 1,600
types of label transitions. Therefore, incorporating
local observation information into the edge feature
will result in an explosion of edge features, which
is 1,600 times larger than the number of feature
templates.
Fortunately, for our task, the label set is quite
small, Y = {B, T, E}1. There are only nine possible
label transitions: T = Y x Y and |T |= 9.2 As
a result, the feature dimension will have nine times
increase over the feature templates, if we incorporate
local observation information of x into the edge
features. In this way, we can effectively combine
observation information of x with label transitions
yi−1yi. We simply used the same templates of
node features for deriving the new edge features.
We found adding new edge features significantly
improves the disambiguation power of our model.
</bodyText>
<sectionHeader confidence="0.8443675" genericHeader="method">
4 Adaptive Online Gradient Descent based
on Feature Frequency Information
</sectionHeader>
<bodyText confidence="0.999983933333333">
As we will show in experiments, the training of the
CRF model with high-dimensional new features is
quite expensive, and the existing training method is
not good enough. To solve this issue, we propose a
fast online training method: adaptive online gradient
descent based on feature frequency information
(ADF). The proposed method is easy to implement.
For high convergence speed of online learning, we
try to use more refined learning rates than the SGD
training. Instead of using a single learning rate (a
scalar) for all weights, we extend the learning rate
scalar to a learning rate vector, which has the same
dimension of the weight vector w. The learning
rate vector is automatically adapted based on feature
frequency information. By doing so, each weight
</bodyText>
<footnote confidence="0.995876166666667">
1B means beginning of a word, I means inside a word, and
E means end of a word. The B, I, E labels have been widely
used in previous work of Chinese word segmentation (Sun et
al., 2009b).
2The operator x means a Cartesian product between two
sets.
</footnote>
<page confidence="0.990666">
256
</page>
<bodyText confidence="0.407221">
ADF learning algorithm
</bodyText>
<listItem confidence="0.958739181818182">
1: procedure ADF(q, c, α, β)
2: w ← 0, t ← 0, v ← 0, γ ← c
3: repeat until convergence
4: . Draw a sample zi at random
5: . v ← UPDATE(v, zi)
6: . if t &gt; 0 and t mod q = 0
7: . . γ ← UPDATE(γ, v)
8: . . v ← 0
9: . g ← ∇wLstoch(zi,w)
10: . w ← w + γ · g
11: . t ← t + 1
12: return w
14: procedure UPDATE(v, zi)
15: for k ∈ features used in sample zi
16: . vk ← vk + 1
17: return v
19: procedure UPDATE(γ, v)
20: for k ∈ all features
21: . u ← vk/q
22: . η ← α − u(α − β)
23: . γk ← ηγk
24: return γ
</listItem>
<figureCaption confidence="0.870098">
Figure 1: The proposed ADF online learning algorithm.
q, c, α, and 3 are hyper-parameters. q is an integer
</figureCaption>
<bodyText confidence="0.926312666666667">
representing window size. c is for initializing the learning
rates. α and 3 are the upper and lower bounds of a scalar,
with 0 &lt; 3 &lt; α &lt; 1.
has its own learning rate, and we will show that this
can significantly improve the convergence speed of
online learning.
In our proposed online learning method, the
update formula is as follows:
257 process. Our proposal is based on the intuition that a
</bodyText>
<equation confidence="0.998446">
wt+1 = wt + γt · gt. (5)
</equation>
<bodyText confidence="0.9881555">
The update term gt is the gradient term of a
randomly sampled instance:
</bodyText>
<equation confidence="0.966757666666667">
��wt��
J 2
gt = ∇,,,tLstoch(zi,wt) = ∇it1 f(zi,wt)−2nσ2 I
</equation>
<bodyText confidence="0.998356904761905">
In addition, γt ∈ ][8+ is a positive vector-
valued learning rate and · denotes component-wise
(Hadamard) product of two vectors.
We learn the learning rate vector γt based
on feature frequency information in the updating
feature with higher frequency in the training process
should be with a learning rate that decays faster. In
other words, we assume a high frequency feature
observed in the training process should have a small
learning rate, and a low frequency feature should
have a relatively larger learning rate in the training.
Our assumption is based on the intuition that a
weight with higher frequency is more adequately
trained, hence smaller learning rate is preferable for
fast convergence.
Given a window size q (number of samples in a
window), we use a vector v to record the feature
frequency. The
entry vk corresponds to the
frequency of the feature k in this window. Given
a feature k, we use u to record the normali
</bodyText>
<figure confidence="0.78281895">
k’th
zed
frequency:
=
For each feature, an adaptation factor
is calculated
based on the normali
u
vk/q.
η
zed frequency information, as
follows:
γk←ηγk.
based on sparse features
et al.,
2007). Similarly, the derivation of
can
(Shalev-Shwartz
γt
also
</figure>
<bodyText confidence="0.9853225">
perform efficiently via the optimization based on
sparse features.
where
and
</bodyText>
<equation confidence="0.754217307692308">
are the upper and lower bounds of
a scalar, with 0 &lt;
&lt;
&lt; 1. As we can see,
a feature with higher frequency corresponds to a
smaller scalar via linear approximation. Finally, the
learn
η = α − u(α − β),
α
β
β
α
ing rate is updated as follows:
</equation>
<bodyText confidence="0.994783777777778">
With this setting, different features will correspond
to different adaptation factors based on feature
frequency information. Our ADF algorithm is
summarized in Figure 1.
The ADF training method is efficient, because
the additional computation (compared with SGD) is
only the derivation of the learning rates, which is
simple and efficient. As we know, the regularization
of SGD can perform efficiently via the optimization
</bodyText>
<subsectionHeader confidence="0.998677">
4.1 Convergence Analysis
</subsectionHeader>
<bodyText confidence="0.598337">
Prior work on convergence analysis of existing
online learning algori
</bodyText>
<table confidence="0.961713285714286">
thms (Murata, 1998; Hsu et
Data Method Passes Train-Time (sec) NWD Rec Pre Rec CWS F-score
MSR Baseline 50 4.7e3 72.6 96.3 95.9 96.1
+ New features 50 1.2e4 75.3 97.2 97.0 97.1
+ New word detection 50 1.2e4 78.2 97.5 96.9 97.2
+ ADF training 10 2.3e3 77.5 97.6 97.2 97.4
CU Baseline 50 2.9e3 68.5 94.0 93.9 93.9
+ New features 50 7.5e3 68.0 94.4 94.5 94.4
+ New word detection 50 7.5e3 68.8 94.8 94.5 94.7
+ ADF training 10 1.5e3 68.8 94.8 94.7 94.8
PKU Baseline 50 2.2e3 77.2 95.0 94.0 94.5
+ New features 50 5.2e3 78.4 95.5 94.9 95.2
+ New word detection 50 5.2e3 79.1 95.8 94.9 95.3
+ ADF training 10 1.2e3 78.4 95.8 94.9 95.4
</table>
<tableCaption confidence="0.993592333333333">
Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge
features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is
decided by empirical convergence of the training methods.
</tableCaption>
<table confidence="0.9997185">
#W.T. #Word #C.T. #Char
MSR 8.8 x 104 2.4 x 106 5 x 103 4.1 x 106
CU 6.9 x 104 1.5 x 106 5 x 103 2.4 x 106
PKU 5.5 x 104 1.1 x 106 5 x 103 1.8 x 106
</table>
<tableCaption confidence="0.975645">
Table 1: Details of the datasets. W.T. represents word
types; C.T. represents character types.
</tableCaption>
<bodyText confidence="0.9984095">
al., 2009) can be extended to the proposed ADF
training method. We can show that the proposed
ADF learning algorithm has reasonable convergence
properties.
When we have the smallest learning rate γt+1 =
βγt, the expectation of the obtained wt is
</bodyText>
<equation confidence="0.949253666666667">
t
E(wt) = w* + H (I − γ0βmH(w*))(w0 − w*),
m=1
</equation>
<bodyText confidence="0.9572824">
where w* is the optimal weight vector, and H is the
Hessian matrix of the objective function. The rate of
convergence is governed by the largest eigenvalue of
the function Ct = 11tm=1(I − γ0βmH(w*)). Then,
we can derive a bound of rate of convergence.
</bodyText>
<construct confidence="0.83764275">
Theorem 1 Assume ϕ is the largest eigenvalue of
the function Ct = 11tm=1(I − γ0βmH(w*)). For
the proposed ADF training, its convergence rate is
bounded by ϕ, and we have
</construct>
<bodyText confidence="0.927927333333333">
ϕ G exp {γ0λβ },
β − 1
where λ is the minimum eigenvalue of H(w*).
</bodyText>
<sectionHeader confidence="0.999864" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99876">
5.1 Data and Metrics
</subsectionHeader>
<bodyText confidence="0.999923352941177">
We used benchmark datasets provided by the second
International Chinese Word Segmentation Bakeoff
to test our proposals. The datasets are from
Microsoft Research Asia (MSR), City University
of Hongkong (CU), and Peking University (PKU).
Details of the corpora are listed in Table 1. We
did not use any extra resources such as common
surnames, parts-of-speech, and semantics.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the
decoder), precision (P, the percentage of words in
the decoder output that are segmented correctly),
balanced F-score defined by 2PR/(P + R), and
recall of new word detection (NWD recall). For
more detailed information on the corpora, refer to
Emerson (2005).
</bodyText>
<subsectionHeader confidence="0.998955">
5.2 Features, Training, and Tuning
</subsectionHeader>
<bodyText confidence="0.999530375">
We employed the feature templates defined in
Section 3.2. The feature sets are huge. There are
2.4 x 107 features for the MSR data, 4.1 x 107
features for the CU data, and 4.7 x 107 features for
the PKU data. To generate word-based features, we
extracted high-frequency word-based unigram and
bigram lists from the training data.
As for training, we performed gradient descent
</bodyText>
<page confidence="0.982727">
258
</page>
<figure confidence="0.999197072727273">
PKU
Number of Passes Number of Passes
CU PKU
F−score 95 F−score 95.5
94.5 95
94 94.5
93.5 94
93
92.5
92
0 1000 2000 3000 4000
Training time (sec)
0 1000 2000 3000 4000
Training time (sec)
F−score
94.5
93.5
92.5
95
94
93
92
F−score
95.5
94.5
95
94
0 10 20 30 40 50
0 10 20 30 40 50
MSR
Number of Passes
MSR
0 2000 4000 6000
Training time (sec)
0 10 20 30 40 50
F−score
97.5
96.5
95.5
97
96
95
ADF
SGD
LBFGS (batch)
F−score
97.5
96.5
95.5
97
96
95
ADF
SGD
LBFGS (batch)
</figure>
<figureCaption confidence="0.999759">
Figure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods.
</figureCaption>
<bodyText confidence="0.999855052631579">
with our proposed training method. To compare
with existing methods, we chose two popular
training methods, a batch training one and an
online training one. The batch training method
is the Limited-Memory BFGS (LBFGS) method
(Nocedal and Wright, 1999). The online baseline
training method is the SGD method, which we have
introduced in Section 2.2.
For the ADF training method, we need to tune the
hyper-parameters q, c, α, and Q. Based on automatic
tuning within the training data (validation in the
training data), we found it is proper to set q = n/10
(n is the number of training samples), c = 0.1,
α = 0.995, and Q = 0.6. To reduce overfitting,
we employed an L2 Gaussian weight prior (Chen
and Rosenfeld, 1999) for all training methods. We
varied the σ with different values (e.g., 1.0, 2.0, and
5.0), and finally set the value to 1.0 for all training
methods.
</bodyText>
<subsectionHeader confidence="0.943171">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.987125457142857">
First, we performed incremental evaluation in this
order: Baseline (word segmentation model with
SGD training); Baseline + New features; Baseline
+ New features + New word detection; Baseline +
New features + New word detection + ADF training
(replacing SGD training). The results are shown in
Table 2.
As we can see, the new features improved
performance on both word segmentation and new
word detection. However, we also noticed that
the training cost became more expensive via
adding high dimensional new features. Adding
new word detection function further improved the
segmentation quality and the new word recognition
recall. Finally, by using the ADF training method,
the training speed is much faster than the SGD
training method. The ADF method can achieve
empirical optimum in only a few passes, yet
with better segmentation accuracies than the SGD
training with 50 passes.
To get more details of the proposed training
method, we compared it with SGD and LBFGS
training methods based on an identical platform,
by varying the number of passes. The comparison
was based on the same platform: Baseline + New
features + New word detection. The F-score curves
of the training methods are shown in Figure 2.
Impressively, the ADF training method reached
empirical convergence in only a few passes, while
the SGD and LBFGS training converged much
slower, requiring more than 50 passes. The ADF
training is about an order magnitude faster than
the SGD online training and more than an order
magnitude faster than the LBFGS batch training.
Finally, we compared our method with the state-
</bodyText>
<page confidence="0.99498">
259
</page>
<table confidence="0.999639294117647">
Data Method Prob. Pre Rec F-score
MSR Best05 (Tseng et al., 2005) √ 96.2 96.6 96.4
CRF + rule-system (Zhang et al., 2006) √ 97.2 96.9 97.1
Semi-Markov perceptron (Zhang and Clark, 2007) × N/A N/A 97.2
Semi-Markov CRF (Gao et al., 2007) √ N/A N/A 97.2
Latent-variable CRF (Sun et al., 2009b) √ 97.3 97.3 97.3
Our method (A Single CRF) √ 97.6 97.2 97.4
CU Best05 (Tseng et al., 2005) √ 94.1 94.6 94.3
CRF + rule-system (Zhang et al., 2006) √ 95.2 94.9 95.1
Semi-perceptron (Zhang and Clark, 2007) × N/A N/A 95.1
Latent-variable CRF (Sun et al., 2009b) √ 94.7 94.4 94.6
Our method (A Single CRF) √ 94.8 94.7 94.8
PKU Best05 (Chen et al., 2005) N/A 95.3 94.6 95.0
CRF + rule-system (Zhang et al., 2006) √ 94.7 95.5 95.1
semi-perceptron (Zhang and Clark, 2007) × N/A N/A 94.5
Latent-variable CRF (Sun et al., 2009b) √ 95.6 94.8 95.2
Our method (A Single CRF) √ 95.8 94.9 95.4
</table>
<tableCaption confidence="0.99991">
Table 3: Comparing our method with the state-of-the-art CWS systems.
</tableCaption>
<bodyText confidence="0.9998468125">
of-the-art systems reported in the previous papers.
The statistics are listed in Table 3. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; CRF + rule-system represents confidence-
based combination of CRF and rule-based models,
presented in Zhang et al. (2006). Prob. indicates
whether or not the system can provide probabilistic
information. As we can see, our method achieved
similar or even higher F-scores, compared with the
best systems reported in previous papers. Note that,
our system is a single Markov model, while most of
the state-of-the-art systems are complicated heavy
systems, with model-combinations (e.g., voting of
multiple segmenters), semi-Markov relaxations, or
latent-variables.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999971904761905">
In this paper, we presented a joint model for
Chinese word segmentation and new word detection.
We presented new features, including word-based
features and enriched edge features, for the joint
modeling. We showed that the new features can
improve the performance on the two tasks.
On the other hand, the training of the model,
especially with high-dimensional new features,
became quite expensive. To solve this problem,
we proposed a new training method, ADF training,
for very fast training of CRFs, even given large-
scale datasets with high dimensional features. We
performed experiments and showed that our new
training method is an order magnitude faster than
existing optimization methods. Our final system can
learn highly accurate models with only a few passes
in training. The proposed fast learning method
is a general algorithm that is not limited in this
specific task. As future work, we plan to apply
this fast learning method on other large-scale natural
language processing tasks.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99985825">
We thank Yaozhong Zhang and Weiwei Sun
for helpful discussions on word segmentation
techniques. The work described in this paper was
supported by a Hong Kong RGC Project (No. PolyU
5230/08E), National High Technology Research and
Development Program of China (863 Program) (No.
2012AA011101), and National Natural Science
Foundation of China (No.91024009, No.60973053).
</bodyText>
<sectionHeader confidence="0.995853" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7312745">
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
</reference>
<page confidence="0.98434">
260
</page>
<reference confidence="0.994745682242991">
In Proceedings of EMNLP’06, pages 465–472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of
machine learning methods for optimum chinese word
segmentation. In Proceedings of The Fourth SIGHAN
Workshop, pages 134–137.
K.J. Chen and M.H. Bai. 1998. Unknown word
detection for chinese by a corpus-based learning
method. Computational Linguistics and Chinese
Language Processing, 3(1):27–44.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word
extraction for chinese documents. In Proceedings of
COLING’02.
Stanley F. Chen and Ronald Rosenfeld. 1999. A
gaussian prior for smoothing maximum entropy
models. Technical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word
segmentation. In Proceedings of the fourth SIGHAN
workshop, pages 138–141.
Thomas Emerson. 2005. The second international
chinese word segmentation bakeoff. In Proceedings
of the fourth SIGHAN workshop, pages 123–133.
Guohong Fu and Kang-Kwong Luke. 2004. Chinese
unknown word identification using class-based lm. In
Proceedings of IJCNLP’04, volume 3248 of Lecture
Notes in Computer Science, pages 704–713. Springer.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL’07), pages 824–831.
Chooi-Ling Goh, Masayuki Asahara, and Yuji
Matsumoto. 2003. Chinese unknown word
identification using character-based tagging and
chunking. In Kotaro Funakoshi, Sandra Kbler, and
Jahna Otterbacher, editors, Proceedings of ACL
(Companion)’03, pages 197–200.
Chun-Nan Hsu, Han-Shen Huang, Yu-Ming Chang, and
Yuh-Jye Lee. 2009. Periodic step-size adaptation in
second-order gradient descent for single-pass on-line
structured learning. Machine Learning, 77(2-3):195–
224.
M. Hannan J. Nie and W. Jin. 1995. Unknown
word detection and segmentation of chinese using
statistical and heuristic knowledge. Communications
of the Chinese and Oriental Languages Information
Processing Society, 5:47C57.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proceedings of the 18th International Conference on
Machine Learning (ICML’01), pages 282–289.
Noboru Murata. 1998. A statistical study of on-line
learning. In On-line learning in neural networks,
Cambridge University Press, pages 63–92.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
Coling 2004, pages 562–568, Geneva, Switzerland,
Aug 23–Aug 27. COLING.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of ICML’07.
Xu Sun and Jun’ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm
and its efficient approximation. In Proceedings of
EACL’09, pages 772–780, Athens, Greece, March.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun’ichi Tsujii. 2008. Modeling latent-dynamic
in shallow parsing: A latent conditional model with
improved inference. In Proceedings of COLING’08,
pages 841–848, Manchester, UK.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun’ichi Tsujii. 2009a. Latent variable perceptron
algorithm for structured classification. In Proceedings
of the 21st International Joint Conference on Arti�cial
Intelligence (IJCAI 2009), pages 1236–1242.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshimasa
Tsuruoka, and Jun’ichi Tsujii. 2009b. A
discriminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of NAACL-HLT’09, pages 56–64, Boulder, Colorado,
June.
Xu Sun, Hisashi Kashima, Takuya Matsuzaki, and
Naonori Ueda. 2010. Averaged stochastic gradient
descent with feedback: An accurate, robust, and
fast training method. In Proceedings of the 10th
International Conference on Data Mining (ICDM’10),
pages 1067–1072.
Xu Sun, Hisashi Kashima, Ryota Tomioka, and Naonori
Ueda. 2011. Large scale real-life action recognition
using conditional random fields with stochastic
training. In Proceedings of the 15th Paci�c-Asia
Conf. on Knowledge Discovery and Data Mining
(PAKDD’11).
Weiwei Sun. 2010. Word-based and character-
based word segmentation models: Comparison and
combination. In Chu-Ren Huang and Dan Jurafsky,
editors, COLING’10 (Posters), pages 1211–1219.
Chinese Information Processing Society of China.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A
</reference>
<page confidence="0.963349">
261
</page>
<reference confidence="0.999797844444445">
conditional random field word segmenter for sighan
bakeoff 2005. In Proceedings of The Fourth SIGHAN
Workshop, pages 168–171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. In Proceedings ofICML’06, pages 969–
976.
A. Wu and Z. Jiang. 2000. Statistically-enhanced new
word identification in a rule-based chinese system.
In Proceedings of the Second Chinese Language
Processing Workshop, page 46C51, Hong Kong,
China.
Yi-Lun Wu, Chaio-Wen Hsieh, Wei-Hsuan Lin, Chun-
Yi Liu, and Liang-Chih Yu. 2011. Unknown
word extraction from multilingual code-switching
sentences (in chinese). In Proceedings of ROCLING
(Posters)’11, pages 349–360.
Nianwen Xue. 2003. Chinese word segmentation
as character tagging. International Journal of
Computational Linguistics and Chinese Language
Processing, 8(1):29–48.
Yue Zhang and Stephen Clark. 2007. Chinese
segmentation with a word-based perceptron algorithm.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 840–
847, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Companion Volume: Short Papers, pages
193–196, New York City, USA, June. Association for
Computational Linguistics.
Hai Zhao, Changning Huang, Mu Li, and Bao-Liang Lu.
2010. A unified character-based tagging framework
for chinese word segmentation. ACM Trans. Asian
Lang. Inf. Process., 9(2).
Guodong Zhou. 2005. A chunking strategy
towards unknown word detection in chinese word
segmentation. In Robert Dale, Kam-Fai Wong,
Jian Su, and Oi Yee Kwong, editors, Proceedings
of IJCNLP’05, volume 3651 of Lecture Notes in
Computer Science, pages 530–541. Springer.
</reference>
<page confidence="0.997295">
262
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.778172">
<title confidence="0.998851">Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</title>
<author confidence="0.90771">Houfeng Wenjie of Computing</author>
<author confidence="0.90771">The Hong Kong Polytechnic</author>
<affiliation confidence="0.997774">Laboratory of Computational Linguistics (Peking University), Ministry of Education,</affiliation>
<email confidence="0.958432">wanghf@pku.edu.cn</email>
<abstract confidence="0.994929208333333">We present a joint model for Chinese word segmentation and new word detection. We present high dimensional new features, including word-based features and enriched edge (label-transition) features, for the joint modeling. As we know, training a word segmentation system on large-scale datasets is already costly. In our case, adding high dimensional new features will further slow down the training speed. To solve this problem, we propose a new training method, adaptive online gradient descent based on feature frequency information, for very fast online training of the parameters, even given large-scale datasets with high dimensional features. Compared with existing training methods, our training method is an order magnitude faster in terms of training time, and can achieve equal or even higher accuracies. The proposed fast training method is a general purpose optimization method, and it is not limited in the specific task discussed in this paper.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
</authors>
<title>A hybrid markov/semi-markov conditional random field for sequence segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP’06,</booktitle>
<pages>465--472</pages>
<contexts>
<context position="5419" citStr="Andrew, 2006" startWordPosition="814" endWordPosition="815">ion and new word detection. 2 Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or late</context>
</contexts>
<marker>Andrew, 2006</marker>
<rawString>Galen Andrew. 2006. A hybrid markov/semi-markov conditional random field for sequence segmentation. In Proceedings of EMNLP’06, pages 465–472.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Masayuki Asahara</author>
</authors>
<location>Kenta Fukuoka, Ai Azuma, ChooiLing Goh, Yotaro Watanabe, Yuji Matsumoto, and</location>
<marker>Asahara, </marker>
<rawString>Masayuki Asahara, Kenta Fukuoka, Ai Azuma, ChooiLing Goh, Yotaro Watanabe, Yuji Matsumoto, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takahashi Tsuzuki</author>
</authors>
<title>Combination of machine learning methods for optimum chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of The Fourth SIGHAN Workshop,</booktitle>
<pages>134--137</pages>
<marker>Tsuzuki, 2005</marker>
<rawString>Takahashi Tsuzuki. 2005. Combination of machine learning methods for optimum chinese word segmentation. In Proceedings of The Fourth SIGHAN Workshop, pages 134–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Chen</author>
<author>M H Bai</author>
</authors>
<title>Unknown word detection for chinese by a corpus-based learning method.</title>
<date>1998</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="6369" citStr="Chen and Bai, 1998" startWordPosition="956" endWordPosition="959">eptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training The most representative online </context>
<context position="9840" citStr="Chen and Bai, 1998" startWordPosition="1529" endWordPosition="1532">log P (yi|xi,w) − R(w). (3) i=1 The first term of this equation represents a conditional log-likelihood of a training data. The second term is a regularizer for reducing overfitting. We employed an L2 prior, R(w) = ||�||2 2σ2 . In what follows, we denote the conditional log-likelihood of each sample log P(yi|xi,w) as ℓ(zi,w). The final objective function is as follows: ℓ(zi,w) − ||w||2 2σ2 .(4) Since no word list can be complete, new word identification is an important task in Chinese NLP. New words in input text are often incorrectly segmented into single-character or other very short words (Chen and Bai, 1998). This phenomenon will also undermine the performance of Chinese word segmentation. We consider here new word detection as an integral part of segmentation, aiming to improve both segmentation and new word detection: detected new words are added to the word list lexicon in order to improve segmentation. Based on our CRF word segmentation system, we can compute a probability for each segment. When we find some word segments are of reliable probabilities yet they are not in the existing word list, we then treat those “confident” word segments as new words and add them into the existing word list</context>
</contexts>
<marker>Chen, Bai, 1998</marker>
<rawString>K.J. Chen and M.H. Bai. 1998. Unknown word detection for chinese by a corpus-based learning method. Computational Linguistics and Chinese Language Processing, 3(1):27–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Wei-Yun Ma</author>
</authors>
<title>Unknown word extraction for chinese documents.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING’02.</booktitle>
<contexts>
<context position="6427" citStr="Chen and Ma, 2002" startWordPosition="968" endWordPosition="971">kov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training The most representative online training method is the SGD method. The SGD uses a small ra</context>
</contexts>
<marker>Chen, Ma, 2002</marker>
<rawString>Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word extraction for chinese documents. In Proceedings of COLING’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108, CMU.</tech>
<contexts>
<context position="23364" citStr="Chen and Rosenfeld, 1999" startWordPosition="3947" endWordPosition="3950">raining methods, a batch training one and an online training one. The batch training method is the Limited-Memory BFGS (LBFGS) method (Nocedal and Wright, 1999). The online baseline training method is the SGD method, which we have introduced in Section 2.2. For the ADF training method, we need to tune the hyper-parameters q, c, α, and Q. Based on automatic tuning within the training data (validation in the training data), we found it is proper to set q = n/10 (n is the number of training samples), c = 0.1, α = 0.995, and Q = 0.6. To reduce overfitting, we employed an L2 Gaussian weight prior (Chen and Rosenfeld, 1999) for all training methods. We varied the σ with different values (e.g., 1.0, 2.0, and 5.0), and finally set the value to 1.0 for all training methods. 5.3 Results and Discussion First, we performed incremental evaluation in this order: Baseline (word segmentation model with SGD training); Baseline + New features; Baseline + New features + New word detection; Baseline + New features + New word detection + ADF training (replacing SGD training). The results are shown in Table 2. As we can see, the new features improved performance on both word segmentation and new word detection. However, we also</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitao Chen</author>
<author>Yiping Zhou</author>
<author>Anne Zhang</author>
<author>Gordon Sun</author>
</authors>
<title>Unigram language model for chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the fourth SIGHAN workshop,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="25773" citStr="Chen et al., 2005" startWordPosition="4358" endWordPosition="4361">Rec F-score MSR Best05 (Tseng et al., 2005) √ 96.2 96.6 96.4 CRF + rule-system (Zhang et al., 2006) √ 97.2 96.9 97.1 Semi-Markov perceptron (Zhang and Clark, 2007) × N/A N/A 97.2 Semi-Markov CRF (Gao et al., 2007) √ N/A N/A 97.2 Latent-variable CRF (Sun et al., 2009b) √ 97.3 97.3 97.3 Our method (A Single CRF) √ 97.6 97.2 97.4 CU Best05 (Tseng et al., 2005) √ 94.1 94.6 94.3 CRF + rule-system (Zhang et al., 2006) √ 95.2 94.9 95.1 Semi-perceptron (Zhang and Clark, 2007) × N/A N/A 95.1 Latent-variable CRF (Sun et al., 2009b) √ 94.7 94.4 94.6 Our method (A Single CRF) √ 94.8 94.7 94.8 PKU Best05 (Chen et al., 2005) N/A 95.3 94.6 95.0 CRF + rule-system (Zhang et al., 2006) √ 94.7 95.5 95.1 semi-perceptron (Zhang and Clark, 2007) × N/A N/A 94.5 Latent-variable CRF (Sun et al., 2009b) √ 95.6 94.8 95.2 Our method (A Single CRF) √ 95.8 94.9 95.4 Table 3: Comparing our method with the state-of-the-art CWS systems. of-the-art systems reported in the previous papers. The statistics are listed in Table 3. Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidencebased combination of CRF and rule-based models, </context>
</contexts>
<marker>Chen, Zhou, Zhang, Sun, 2005</marker>
<rawString>Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun. 2005. Unigram language model for chinese word segmentation. In Proceedings of the fourth SIGHAN workshop, pages 138–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the fourth SIGHAN workshop,</booktitle>
<pages>123--133</pages>
<contexts>
<context position="21654" citStr="Emerson (2005)" startWordPosition="3632" endWordPosition="3633">MSR), City University of Hongkong (CU), and Peking University (PKU). Details of the corpora are listed in Table 1. We did not use any extra resources such as common surnames, parts-of-speech, and semantics. Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P, the percentage of words in the decoder output that are segmented correctly), balanced F-score defined by 2PR/(P + R), and recall of new word detection (NWD recall). For more detailed information on the corpora, refer to Emerson (2005). 5.2 Features, Training, and Tuning We employed the feature templates defined in Section 3.2. The feature sets are huge. There are 2.4 x 107 features for the MSR data, 4.1 x 107 features for the CU data, and 4.7 x 107 features for the PKU data. To generate word-based features, we extracted high-frequency word-based unigram and bigram lists from the training data. As for training, we performed gradient descent 258 PKU Number of Passes Number of Passes CU PKU F−score 95 F−score 95.5 94.5 95 94 94.5 93.5 94 93 92.5 92 0 1000 2000 3000 4000 Training time (sec) 0 1000 2000 3000 4000 Training time </context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the fourth SIGHAN workshop, pages 123–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guohong Fu</author>
<author>Kang-Kwong Luke</author>
</authors>
<title>Chinese unknown word identification using class-based lm.</title>
<date>2004</date>
<booktitle>In Proceedings of IJCNLP’04,</booktitle>
<volume>3248</volume>
<pages>704--713</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6476" citStr="Fu and Luke, 2004" startWordPosition="978" endWordPosition="981"> Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to</context>
</contexts>
<marker>Fu, Luke, 2004</marker>
<rawString>Guohong Fu and Kang-Kwong Luke. 2004. Chinese unknown word identification using class-based lm. In Proceedings of IJCNLP’04, volume 3248 of Lecture Notes in Computer Science, pages 704–713. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Galen Andrew</author>
<author>Mark Johnson</author>
<author>Kristina Toutanova</author>
</authors>
<title>A comparative study of parameter estimation methods for statistical natural language processing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL’07),</booktitle>
<pages>824--831</pages>
<contexts>
<context position="25368" citStr="Gao et al., 2007" startWordPosition="4280" endWordPosition="4283">2. Impressively, the ADF training method reached empirical convergence in only a few passes, while the SGD and LBFGS training converged much slower, requiring more than 50 passes. The ADF training is about an order magnitude faster than the SGD online training and more than an order magnitude faster than the LBFGS batch training. Finally, we compared our method with the state259 Data Method Prob. Pre Rec F-score MSR Best05 (Tseng et al., 2005) √ 96.2 96.6 96.4 CRF + rule-system (Zhang et al., 2006) √ 97.2 96.9 97.1 Semi-Markov perceptron (Zhang and Clark, 2007) × N/A N/A 97.2 Semi-Markov CRF (Gao et al., 2007) √ N/A N/A 97.2 Latent-variable CRF (Sun et al., 2009b) √ 97.3 97.3 97.3 Our method (A Single CRF) √ 97.6 97.2 97.4 CU Best05 (Tseng et al., 2005) √ 94.1 94.6 94.3 CRF + rule-system (Zhang et al., 2006) √ 95.2 94.9 95.1 Semi-perceptron (Zhang and Clark, 2007) × N/A N/A 95.1 Latent-variable CRF (Sun et al., 2009b) √ 94.7 94.4 94.6 Our method (A Single CRF) √ 94.8 94.7 94.8 PKU Best05 (Chen et al., 2005) N/A 95.3 94.6 95.0 CRF + rule-system (Zhang et al., 2006) √ 94.7 95.5 95.1 semi-perceptron (Zhang and Clark, 2007) × N/A N/A 94.5 Latent-variable CRF (Sun et al., 2009b) √ 95.6 94.8 95.2 Our met</context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>Jianfeng Gao, Galen Andrew, Mark Johnson, and Kristina Toutanova. 2007. A comparative study of parameter estimation methods for statistical natural language processing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL’07), pages 824–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chooi-Ling Goh</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chinese unknown word identification using character-based tagging and chunking.</title>
<date>2003</date>
<booktitle>In Kotaro Funakoshi, Sandra Kbler, and Jahna Otterbacher, editors, Proceedings of ACL (Companion)’03,</booktitle>
<pages>197--200</pages>
<contexts>
<context position="6457" citStr="Goh et al., 2003" startWordPosition="974" endWordPosition="977">g and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the </context>
</contexts>
<marker>Goh, Asahara, Matsumoto, 2003</marker>
<rawString>Chooi-Ling Goh, Masayuki Asahara, and Yuji Matsumoto. 2003. Chinese unknown word identification using character-based tagging and chunking. In Kotaro Funakoshi, Sandra Kbler, and Jahna Otterbacher, editors, Proceedings of ACL (Companion)’03, pages 197–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nan Hsu</author>
<author>Han-Shen Huang</author>
<author>Yu-Ming Chang</author>
<author>Yuh-Jye Lee</author>
</authors>
<title>Periodic step-size adaptation in second-order gradient descent for single-pass on-line structured learning.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="7832" citStr="Hsu et al., 2009" startWordPosition="1199" endWordPosition="1202"> using a smaller batch size, one can update the parameters more frequently and speed up the convergence. The extreme case is a batch size of 1, and it gives the maximum frequency of updates, which we adopt in this work. Then, the model parameters are updated in such a way: wt+1 = wt + γtVwtGstoch(xi,wt), (1) where t is the update counter, γt is the learning rate, and Gstoch(xi,wt) is the stochastic loss function based on a training sample xi. There were accelerated versions of SGD, including stochastic meta descent (Vishwanathan et al., 2006) and periodic step-size adaptation online learning (Hsu et al., 2009). Compared with those two methods, our proposal is fundamentally 254 different. Those two methods are using 2nd-order gradient (Hessian) information for accelerated training, while our accelerated training method does not need such 2nd-order gradient information, which is costly and complicated. Our ADF training method is based on feature frequency adaptation, and there is no prior work on using feature frequency information for accelerating online training. Other online training methods includes averaged SGD with feedback (Sun et al., 2010; Sun et al., 2011), latent variable perceptron traini</context>
</contexts>
<marker>Hsu, Huang, Chang, Lee, 2009</marker>
<rawString>Chun-Nan Hsu, Han-Shen Huang, Yu-Ming Chang, and Yuh-Jye Lee. 2009. Periodic step-size adaptation in second-order gradient descent for single-pass on-line structured learning. Machine Learning, 77(2-3):195– 224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hannan J Nie</author>
<author>W Jin</author>
</authors>
<title>Unknown word detection and segmentation of chinese using statistical and heuristic knowledge.</title>
<date>1995</date>
<booktitle>Communications of the Chinese and Oriental Languages Information Processing Society, 5:47C57.</booktitle>
<contexts>
<context position="6349" citStr="Nie and Jin, 1995" startWordPosition="952" endWordPosition="955">ng semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training The most re</context>
</contexts>
<marker>Nie, Jin, 1995</marker>
<rawString>M. Hannan J. Nie and W. Jin. 1995. Unknown word detection and segmentation of chinese using statistical and heuristic knowledge. Communications of the Chinese and Oriental Languages Information Processing Society, 5:47C57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning (ICML’01),</booktitle>
<pages>282--289</pages>
<contexts>
<context position="8717" citStr="Lafferty et al., 2001" startWordPosition="1334" endWordPosition="1337"> which is costly and complicated. Our ADF training method is based on feature frequency adaptation, and there is no prior work on using feature frequency information for accelerating online training. Other online training methods includes averaged SGD with feedback (Sun et al., 2010; Sun et al., 2011), latent variable perceptron training (Sun et al., 2009a), and so on. Those methods are less related to this paper. 3 System Architecture 3.1 A Joint Model Based on CRFs First, we briefly review CRFs. CRFs are proposed as a method for structured classification by solving “the label bias problem” (Lafferty et al., 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequence y conditioned on the observation sequence x is modeled as follows (Lafferty et al., 2001): exp {wTf(y,x)} P(y|x,w) = (2) Evy′ exp {wT f ( y&amp;quot; x) } where w is a parameter vector. Given a training set consisting of n labeled sequences, zi = (xi,yi), for i = 1... n, parameter estimation is performed by maximizing the objective function, n L(w) = log P (yi|xi,w) − R(w). (3) i=1 The first term of this equation represents a conditional log-lik</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (ICML’01), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noboru Murata</author>
</authors>
<title>A statistical study of on-line learning.</title>
<date>1998</date>
<booktitle>In On-line learning in neural networks, Cambridge</booktitle>
<pages>63--92</pages>
<publisher>University Press,</publisher>
<contexts>
<context position="18917" citStr="Murata, 1998" startWordPosition="3142" endWordPosition="3143">nally, the learn η = α − u(α − β), α β β α ing rate is updated as follows: With this setting, different features will correspond to different adaptation factors based on feature frequency information. Our ADF algorithm is summarized in Figure 1. The ADF training method is efficient, because the additional computation (compared with SGD) is only the derivation of the learning rates, which is simple and efficient. As we know, the regularization of SGD can perform efficiently via the optimization 4.1 Convergence Analysis Prior work on convergence analysis of existing online learning algori thms (Murata, 1998; Hsu et Data Method Passes Train-Time (sec) NWD Rec Pre Rec CWS F-score MSR Baseline 50 4.7e3 72.6 96.3 95.9 96.1 + New features 50 1.2e4 75.3 97.2 97.0 97.1 + New word detection 50 1.2e4 78.2 97.5 96.9 97.2 + ADF training 10 2.3e3 77.5 97.6 97.2 97.4 CU Baseline 50 2.9e3 68.5 94.0 93.9 93.9 + New features 50 7.5e3 68.0 94.4 94.5 94.4 + New word detection 50 7.5e3 68.8 94.8 94.5 94.7 + ADF training 10 1.5e3 68.8 94.8 94.7 94.8 PKU Baseline 50 2.2e3 77.2 95.0 94.0 94.5 + New features 50 5.2e3 78.4 95.5 94.9 95.2 + New word detection 50 5.2e3 79.1 95.8 94.9 95.3 + ADF training 10 1.2e3 78.4 95.</context>
</contexts>
<marker>Murata, 1998</marker>
<rawString>Noboru Murata. 1998. A statistical study of on-line learning. In On-line learning in neural networks, Cambridge University Press, pages 63–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<title>Numerical optimization.</title>
<date>1999</date>
<publisher>Springer.</publisher>
<contexts>
<context position="22899" citStr="Nocedal and Wright, 1999" startWordPosition="3861" endWordPosition="3864">3.5 92.5 95 94 93 92 F−score 95.5 94.5 95 94 0 10 20 30 40 50 0 10 20 30 40 50 MSR Number of Passes MSR 0 2000 4000 6000 Training time (sec) 0 10 20 30 40 50 F−score 97.5 96.5 95.5 97 96 95 ADF SGD LBFGS (batch) F−score 97.5 96.5 95.5 97 96 95 ADF SGD LBFGS (batch) Figure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods. with our proposed training method. To compare with existing methods, we chose two popular training methods, a batch training one and an online training one. The batch training method is the Limited-Memory BFGS (LBFGS) method (Nocedal and Wright, 1999). The online baseline training method is the SGD method, which we have introduced in Section 2.2. For the ADF training method, we need to tune the hyper-parameters q, c, α, and Q. Based on automatic tuning within the training data (validation in the training data), we found it is proper to set q = n/10 (n is the number of training samples), c = 0.1, α = 0.995, and Q = 0.6. To reduce overfitting, we employed an L2 Gaussian weight prior (Chen and Rosenfeld, 1999) for all training methods. We varied the σ with different values (e.g., 1.0, 2.0, and 5.0), and finally set the value to 1.0 for all tr</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Jorge Nocedal and Stephen J. Wright. 1999. Numerical optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>562--568</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5198" citStr="Peng et al., 2004" startWordPosition="778" endWordPosition="781">hod requires only a few passes to complete the training. • We propose a joint model for Chinese word segmentation and new word detection. • Compared with prior work, our system achieves better accuracies on both word segmentation and new word detection. 2 Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multi</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of Coling 2004, pages 562–568, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML’07.</booktitle>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proceedings of ICML’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Sequential labeling with latent variables: An exact inference algorithm and its efficient approximation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL’09,</booktitle>
<pages>772--780</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="5581" citStr="Sun and Tsujii, 2009" startWordPosition="839" endWordPosition="842">ning methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the othe</context>
</contexts>
<marker>Sun, Tsujii, 2009</marker>
<rawString>Xu Sun and Jun’ichi Tsujii. 2009. Sequential labeling with latent variables: An exact inference algorithm and its efficient approximation. In Proceedings of EACL’09, pages 772–780, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Louis-Philippe Morency</author>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Modeling latent-dynamic in shallow parsing: A latent conditional model with improved inference.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING’08,</booktitle>
<pages>841--848</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="5558" citStr="Sun et al., 2008" startWordPosition="835" endWordPosition="838">opular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic i</context>
</contexts>
<marker>Sun, Morency, Okanohara, Tsujii, 2008</marker>
<rawString>Xu Sun, Louis-Philippe Morency, Daisuke Okanohara, and Jun’ichi Tsujii. 2008. Modeling latent-dynamic in shallow parsing: A latent conditional model with improved inference. In Proceedings of COLING’08, pages 841–848, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Takuya Matsuzaki</author>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Latent variable perceptron algorithm for structured classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Arti�cial Intelligence (IJCAI</booktitle>
<pages>1236--1242</pages>
<contexts>
<context position="5437" citStr="Sun et al., 2009" startWordPosition="816" endWordPosition="819">rd detection. 2 Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable condit</context>
<context position="8452" citStr="Sun et al., 2009" startWordPosition="1289" endWordPosition="1292">mpared with those two methods, our proposal is fundamentally 254 different. Those two methods are using 2nd-order gradient (Hessian) information for accelerated training, while our accelerated training method does not need such 2nd-order gradient information, which is costly and complicated. Our ADF training method is based on feature frequency adaptation, and there is no prior work on using feature frequency information for accelerating online training. Other online training methods includes averaged SGD with feedback (Sun et al., 2010; Sun et al., 2011), latent variable perceptron training (Sun et al., 2009a), and so on. Those methods are less related to this paper. 3 System Architecture 3.1 A Joint Model Based on CRFs First, we briefly review CRFs. CRFs are proposed as a method for structured classification by solving “the label bias problem” (Lafferty et al., 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequence y conditioned on the observation sequence x is modeled as follows (Lafferty et al., 2001): exp {wTf(y,x)} P(y|x,w) = (2) Evy′ exp {wT f ( y&amp;quot; x) } where w is a parameter vector</context>
<context position="15630" citStr="Sun et al., 2009" startWordPosition="2520" endWordPosition="2523">is easy to implement. For high convergence speed of online learning, we try to use more refined learning rates than the SGD training. Instead of using a single learning rate (a scalar) for all weights, we extend the learning rate scalar to a learning rate vector, which has the same dimension of the weight vector w. The learning rate vector is automatically adapted based on feature frequency information. By doing so, each weight 1B means beginning of a word, I means inside a word, and E means end of a word. The B, I, E labels have been widely used in previous work of Chinese word segmentation (Sun et al., 2009b). 2The operator x means a Cartesian product between two sets. 256 ADF learning algorithm 1: procedure ADF(q, c, α, β) 2: w ← 0, t ← 0, v ← 0, γ ← c 3: repeat until convergence 4: . Draw a sample zi at random 5: . v ← UPDATE(v, zi) 6: . if t &gt; 0 and t mod q = 0 7: . . γ ← UPDATE(γ, v) 8: . . v ← 0 9: . g ← ∇wLstoch(zi,w) 10: . w ← w + γ · g 11: . t ← t + 1 12: return w 14: procedure UPDATE(v, zi) 15: for k ∈ features used in sample zi 16: . vk ← vk + 1 17: return v 19: procedure UPDATE(γ, v) 20: for k ∈ all features 21: . u ← vk/q 22: . η ← α − u(α − β) 23: . γk ← ηγk 24: return γ Figure 1: T</context>
<context position="25421" citStr="Sun et al., 2009" startWordPosition="4290" endWordPosition="4293">ical convergence in only a few passes, while the SGD and LBFGS training converged much slower, requiring more than 50 passes. The ADF training is about an order magnitude faster than the SGD online training and more than an order magnitude faster than the LBFGS batch training. Finally, we compared our method with the state259 Data Method Prob. Pre Rec F-score MSR Best05 (Tseng et al., 2005) √ 96.2 96.6 96.4 CRF + rule-system (Zhang et al., 2006) √ 97.2 96.9 97.1 Semi-Markov perceptron (Zhang and Clark, 2007) × N/A N/A 97.2 Semi-Markov CRF (Gao et al., 2007) √ N/A N/A 97.2 Latent-variable CRF (Sun et al., 2009b) √ 97.3 97.3 97.3 Our method (A Single CRF) √ 97.6 97.2 97.4 CU Best05 (Tseng et al., 2005) √ 94.1 94.6 94.3 CRF + rule-system (Zhang et al., 2006) √ 95.2 94.9 95.1 Semi-perceptron (Zhang and Clark, 2007) × N/A N/A 95.1 Latent-variable CRF (Sun et al., 2009b) √ 94.7 94.4 94.6 Our method (A Single CRF) √ 94.8 94.7 94.8 PKU Best05 (Chen et al., 2005) N/A 95.3 94.6 95.0 CRF + rule-system (Zhang et al., 2006) √ 94.7 95.5 95.1 semi-perceptron (Zhang and Clark, 2007) × N/A N/A 94.5 Latent-variable CRF (Sun et al., 2009b) √ 95.6 94.8 95.2 Our method (A Single CRF) √ 95.8 94.9 95.4 Table 3: Comparin</context>
</contexts>
<marker>Sun, Matsuzaki, Okanohara, Tsujii, 2009</marker>
<rawString>Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and Jun’ichi Tsujii. 2009a. Latent variable perceptron algorithm for structured classification. In Proceedings of the 21st International Joint Conference on Arti�cial Intelligence (IJCAI 2009), pages 1236–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Yaozhong Zhang</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative latent variable chinese segmenter with hybrid word/character information.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT’09,</booktitle>
<pages>56--64</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5437" citStr="Sun et al., 2009" startWordPosition="816" endWordPosition="819">rd detection. 2 Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable condit</context>
<context position="8452" citStr="Sun et al., 2009" startWordPosition="1289" endWordPosition="1292">mpared with those two methods, our proposal is fundamentally 254 different. Those two methods are using 2nd-order gradient (Hessian) information for accelerated training, while our accelerated training method does not need such 2nd-order gradient information, which is costly and complicated. Our ADF training method is based on feature frequency adaptation, and there is no prior work on using feature frequency information for accelerating online training. Other online training methods includes averaged SGD with feedback (Sun et al., 2010; Sun et al., 2011), latent variable perceptron training (Sun et al., 2009a), and so on. Those methods are less related to this paper. 3 System Architecture 3.1 A Joint Model Based on CRFs First, we briefly review CRFs. CRFs are proposed as a method for structured classification by solving “the label bias problem” (Lafferty et al., 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequence y conditioned on the observation sequence x is modeled as follows (Lafferty et al., 2001): exp {wTf(y,x)} P(y|x,w) = (2) Evy′ exp {wT f ( y&amp;quot; x) } where w is a parameter vector</context>
<context position="15630" citStr="Sun et al., 2009" startWordPosition="2520" endWordPosition="2523">is easy to implement. For high convergence speed of online learning, we try to use more refined learning rates than the SGD training. Instead of using a single learning rate (a scalar) for all weights, we extend the learning rate scalar to a learning rate vector, which has the same dimension of the weight vector w. The learning rate vector is automatically adapted based on feature frequency information. By doing so, each weight 1B means beginning of a word, I means inside a word, and E means end of a word. The B, I, E labels have been widely used in previous work of Chinese word segmentation (Sun et al., 2009b). 2The operator x means a Cartesian product between two sets. 256 ADF learning algorithm 1: procedure ADF(q, c, α, β) 2: w ← 0, t ← 0, v ← 0, γ ← c 3: repeat until convergence 4: . Draw a sample zi at random 5: . v ← UPDATE(v, zi) 6: . if t &gt; 0 and t mod q = 0 7: . . γ ← UPDATE(γ, v) 8: . . v ← 0 9: . g ← ∇wLstoch(zi,w) 10: . w ← w + γ · g 11: . t ← t + 1 12: return w 14: procedure UPDATE(v, zi) 15: for k ∈ features used in sample zi 16: . vk ← vk + 1 17: return v 19: procedure UPDATE(γ, v) 20: for k ∈ all features 21: . u ← vk/q 22: . η ← α − u(α − β) 23: . γk ← ηγk 24: return γ Figure 1: T</context>
<context position="25421" citStr="Sun et al., 2009" startWordPosition="4290" endWordPosition="4293">ical convergence in only a few passes, while the SGD and LBFGS training converged much slower, requiring more than 50 passes. The ADF training is about an order magnitude faster than the SGD online training and more than an order magnitude faster than the LBFGS batch training. Finally, we compared our method with the state259 Data Method Prob. Pre Rec F-score MSR Best05 (Tseng et al., 2005) √ 96.2 96.6 96.4 CRF + rule-system (Zhang et al., 2006) √ 97.2 96.9 97.1 Semi-Markov perceptron (Zhang and Clark, 2007) × N/A N/A 97.2 Semi-Markov CRF (Gao et al., 2007) √ N/A N/A 97.2 Latent-variable CRF (Sun et al., 2009b) √ 97.3 97.3 97.3 Our method (A Single CRF) √ 97.6 97.2 97.4 CU Best05 (Tseng et al., 2005) √ 94.1 94.6 94.3 CRF + rule-system (Zhang et al., 2006) √ 95.2 94.9 95.1 Semi-perceptron (Zhang and Clark, 2007) × N/A N/A 95.1 Latent-variable CRF (Sun et al., 2009b) √ 94.7 94.4 94.6 Our method (A Single CRF) √ 94.8 94.7 94.8 PKU Best05 (Chen et al., 2005) N/A 95.3 94.6 95.0 CRF + rule-system (Zhang et al., 2006) √ 94.7 95.5 95.1 semi-perceptron (Zhang and Clark, 2007) × N/A N/A 94.5 Latent-variable CRF (Sun et al., 2009b) √ 95.6 94.8 95.2 Our method (A Single CRF) √ 95.8 94.9 95.4 Table 3: Comparin</context>
</contexts>
<marker>Sun, Zhang, Matsuzaki, Tsuruoka, Tsujii, 2009</marker>
<rawString>Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshimasa Tsuruoka, and Jun’ichi Tsujii. 2009b. A discriminative latent variable chinese segmenter with hybrid word/character information. In Proceedings of NAACL-HLT’09, pages 56–64, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Hisashi Kashima</author>
<author>Takuya Matsuzaki</author>
<author>Naonori Ueda</author>
</authors>
<title>Averaged stochastic gradient descent with feedback: An accurate, robust, and fast training method.</title>
<date>2010</date>
<booktitle>In Proceedings of the 10th International Conference on Data Mining (ICDM’10),</booktitle>
<pages>1067--1072</pages>
<contexts>
<context position="8378" citStr="Sun et al., 2010" startWordPosition="1277" endWordPosition="1280">) and periodic step-size adaptation online learning (Hsu et al., 2009). Compared with those two methods, our proposal is fundamentally 254 different. Those two methods are using 2nd-order gradient (Hessian) information for accelerated training, while our accelerated training method does not need such 2nd-order gradient information, which is costly and complicated. Our ADF training method is based on feature frequency adaptation, and there is no prior work on using feature frequency information for accelerating online training. Other online training methods includes averaged SGD with feedback (Sun et al., 2010; Sun et al., 2011), latent variable perceptron training (Sun et al., 2009a), and so on. Those methods are less related to this paper. 3 System Architecture 3.1 A Joint Model Based on CRFs First, we briefly review CRFs. CRFs are proposed as a method for structured classification by solving “the label bias problem” (Lafferty et al., 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequence y conditioned on the observation sequence x is modeled as follows (Lafferty et al., 2001): exp {wTf(y</context>
</contexts>
<marker>Sun, Kashima, Matsuzaki, Ueda, 2010</marker>
<rawString>Xu Sun, Hisashi Kashima, Takuya Matsuzaki, and Naonori Ueda. 2010. Averaged stochastic gradient descent with feedback: An accurate, robust, and fast training method. In Proceedings of the 10th International Conference on Data Mining (ICDM’10), pages 1067–1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Hisashi Kashima</author>
<author>Ryota Tomioka</author>
<author>Naonori Ueda</author>
</authors>
<title>Large scale real-life action recognition using conditional random fields with stochastic training.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Paci�c-Asia Conf. on Knowledge Discovery and Data Mining (PAKDD’11).</booktitle>
<contexts>
<context position="8397" citStr="Sun et al., 2011" startWordPosition="1281" endWordPosition="1284">p-size adaptation online learning (Hsu et al., 2009). Compared with those two methods, our proposal is fundamentally 254 different. Those two methods are using 2nd-order gradient (Hessian) information for accelerated training, while our accelerated training method does not need such 2nd-order gradient information, which is costly and complicated. Our ADF training method is based on feature frequency adaptation, and there is no prior work on using feature frequency information for accelerating online training. Other online training methods includes averaged SGD with feedback (Sun et al., 2010; Sun et al., 2011), latent variable perceptron training (Sun et al., 2009a), and so on. Those methods are less related to this paper. 3 System Architecture 3.1 A Joint Model Based on CRFs First, we briefly review CRFs. CRFs are proposed as a method for structured classification by solving “the label bias problem” (Lafferty et al., 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequence y conditioned on the observation sequence x is modeled as follows (Lafferty et al., 2001): exp {wTf(y,x)} P(y|x,w) = (2)</context>
</contexts>
<marker>Sun, Kashima, Tomioka, Ueda, 2011</marker>
<rawString>Xu Sun, Hisashi Kashima, Ryota Tomioka, and Naonori Ueda. 2011. Large scale real-life action recognition using conditional random fields with stochastic training. In Proceedings of the 15th Paci�c-Asia Conf. on Knowledge Discovery and Data Mining (PAKDD’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and characterbased word segmentation models: Comparison and combination.</title>
<date>2010</date>
<booktitle>In Chu-Ren Huang and Dan Jurafsky, editors, COLING’10 (Posters),</booktitle>
<pages>1211--1219</pages>
<contexts>
<context position="5870" citStr="Sun, 2010" startWordPosition="886" endWordPosition="887">. To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke</context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Word-based and characterbased word segmentation models: Comparison and combination. In Chu-Ren Huang and Dan Jurafsky, editors, COLING’10 (Posters), pages 1211–1219. Chinese Information Processing Society of China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of The Fourth SIGHAN Workshop,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="5218" citStr="Tseng et al., 2005" startWordPosition="782" endWordPosition="785"> few passes to complete the training. • We propose a joint model for Chinese word segmentation and new word detection. • Compared with prior work, our system achieves better accuracies on both word segmentation and new word detection. 2 Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perc</context>
<context position="25198" citStr="Tseng et al., 2005" startWordPosition="4249" endWordPosition="4252">mber of passes. The comparison was based on the same platform: Baseline + New features + New word detection. The F-score curves of the training methods are shown in Figure 2. Impressively, the ADF training method reached empirical convergence in only a few passes, while the SGD and LBFGS training converged much slower, requiring more than 50 passes. The ADF training is about an order magnitude faster than the SGD online training and more than an order magnitude faster than the LBFGS batch training. Finally, we compared our method with the state259 Data Method Prob. Pre Rec F-score MSR Best05 (Tseng et al., 2005) √ 96.2 96.6 96.4 CRF + rule-system (Zhang et al., 2006) √ 97.2 96.9 97.1 Semi-Markov perceptron (Zhang and Clark, 2007) × N/A N/A 97.2 Semi-Markov CRF (Gao et al., 2007) √ N/A N/A 97.2 Latent-variable CRF (Sun et al., 2009b) √ 97.3 97.3 97.3 Our method (A Single CRF) √ 97.6 97.2 97.4 CU Best05 (Tseng et al., 2005) √ 94.1 94.6 94.3 CRF + rule-system (Zhang et al., 2006) √ 95.2 94.9 95.1 Semi-perceptron (Zhang and Clark, 2007) × N/A N/A 95.1 Latent-variable CRF (Sun et al., 2009b) √ 94.7 94.4 94.6 Our method (A Single CRF) √ 94.8 94.7 94.8 PKU Best05 (Chen et al., 2005) N/A 95.3 94.6 95.0 CRF +</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of The Fourth SIGHAN Workshop, pages 168–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Nicol N Schraudolph</author>
<author>Mark W Schmidt</author>
<author>Kevin P Murphy</author>
</authors>
<title>Accelerated training of conditional random fields with stochastic meta-descent.</title>
<date>2006</date>
<booktitle>In Proceedings ofICML’06,</booktitle>
<pages>969--976</pages>
<contexts>
<context position="7763" citStr="Vishwanathan et al., 2006" startWordPosition="1189" endWordPosition="1192">r of training samples used for this approximation is called the batch size. By using a smaller batch size, one can update the parameters more frequently and speed up the convergence. The extreme case is a batch size of 1, and it gives the maximum frequency of updates, which we adopt in this work. Then, the model parameters are updated in such a way: wt+1 = wt + γtVwtGstoch(xi,wt), (1) where t is the update counter, γt is the learning rate, and Gstoch(xi,wt) is the stochastic loss function based on a training sample xi. There were accelerated versions of SGD, including stochastic meta descent (Vishwanathan et al., 2006) and periodic step-size adaptation online learning (Hsu et al., 2009). Compared with those two methods, our proposal is fundamentally 254 different. Those two methods are using 2nd-order gradient (Hessian) information for accelerated training, while our accelerated training method does not need such 2nd-order gradient information, which is costly and complicated. Our ADF training method is based on feature frequency adaptation, and there is no prior work on using feature frequency information for accelerating online training. Other online training methods includes averaged SGD with feedback (S</context>
</contexts>
<marker>Vishwanathan, Schraudolph, Schmidt, Murphy, 2006</marker>
<rawString>S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W. Schmidt, and Kevin P. Murphy. 2006. Accelerated training of conditional random fields with stochastic meta-descent. In Proceedings ofICML’06, pages 969– 976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Wu</author>
<author>Z Jiang</author>
</authors>
<title>Statistically-enhanced new word identification in a rule-based chinese system.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Chinese Language Processing Workshop, page 46C51,</booktitle>
<location>Hong Kong, China.</location>
<contexts>
<context position="6389" citStr="Wu and Jiang, 2000" startWordPosition="960" endWordPosition="963">ting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training The most representative online training method is t</context>
</contexts>
<marker>Wu, Jiang, 2000</marker>
<rawString>A. Wu and Z. Jiang. 2000. Statistically-enhanced new word identification in a rule-based chinese system. In Proceedings of the Second Chinese Language Processing Workshop, page 46C51, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Lun Wu</author>
<author>Chaio-Wen Hsieh</author>
<author>Wei-Hsuan Lin</author>
<author>ChunYi Liu</author>
<author>Liang-Chih Yu</author>
</authors>
<title>Unknown word extraction from multilingual code-switching sentences (in chinese).</title>
<date>2011</date>
<booktitle>In Proceedings of ROCLING (Posters)’11,</booktitle>
<pages>349--360</pages>
<contexts>
<context position="6494" citStr="Wu et al., 2011" startWordPosition="982" endWordPosition="985">semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the g</context>
</contexts>
<marker>Wu, Hsieh, Lin, Liu, Yu, 2011</marker>
<rawString>Yi-Lun Wu, Chaio-Wen Hsieh, Wei-Hsuan Lin, ChunYi Liu, and Liang-Chih Yu. 2011. Unknown word extraction from multilingual code-switching sentences (in chinese). In Proceedings of ROCLING (Posters)’11, pages 349–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<journal>International Journal of Computational Linguistics and Chinese Language Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="5179" citStr="Xue, 2003" startWordPosition="776" endWordPosition="777">raining method requires only a few passes to complete the training. • We propose a joint model for Chinese word segmentation and new word detection. • Compared with prior work, our system achieves better accuracies on both word segmentation and new word detection. 2 Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting sys</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. International Journal of Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>840--847</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5858" citStr="Zhang and Clark, 2007" startWordPosition="882" endWordPosition="885">005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003;</context>
<context position="25318" citStr="Zhang and Clark, 2007" startWordPosition="4270" endWordPosition="4273">ore curves of the training methods are shown in Figure 2. Impressively, the ADF training method reached empirical convergence in only a few passes, while the SGD and LBFGS training converged much slower, requiring more than 50 passes. The ADF training is about an order magnitude faster than the SGD online training and more than an order magnitude faster than the LBFGS batch training. Finally, we compared our method with the state259 Data Method Prob. Pre Rec F-score MSR Best05 (Tseng et al., 2005) √ 96.2 96.6 96.4 CRF + rule-system (Zhang et al., 2006) √ 97.2 96.9 97.1 Semi-Markov perceptron (Zhang and Clark, 2007) × N/A N/A 97.2 Semi-Markov CRF (Gao et al., 2007) √ N/A N/A 97.2 Latent-variable CRF (Sun et al., 2009b) √ 97.3 97.3 97.3 Our method (A Single CRF) √ 97.6 97.2 97.4 CU Best05 (Tseng et al., 2005) √ 94.1 94.6 94.3 CRF + rule-system (Zhang et al., 2006) √ 95.2 94.9 95.1 Semi-perceptron (Zhang and Clark, 2007) × N/A N/A 95.1 Latent-variable CRF (Sun et al., 2009b) √ 94.7 94.4 94.6 Our method (A Single CRF) √ 94.8 94.7 94.8 PKU Best05 (Chen et al., 2005) N/A 95.3 94.6 95.0 CRF + rule-system (Zhang et al., 2006) √ 94.7 95.5 95.1 semi-perceptron (Zhang and Clark, 2007) × N/A N/A 94.5 Latent-variabl</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840– 847, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Subword-based tagging by conditional random fields for chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>193--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="25254" citStr="Zhang et al., 2006" startWordPosition="4260" endWordPosition="4263">tform: Baseline + New features + New word detection. The F-score curves of the training methods are shown in Figure 2. Impressively, the ADF training method reached empirical convergence in only a few passes, while the SGD and LBFGS training converged much slower, requiring more than 50 passes. The ADF training is about an order magnitude faster than the SGD online training and more than an order magnitude faster than the LBFGS batch training. Finally, we compared our method with the state259 Data Method Prob. Pre Rec F-score MSR Best05 (Tseng et al., 2005) √ 96.2 96.6 96.4 CRF + rule-system (Zhang et al., 2006) √ 97.2 96.9 97.1 Semi-Markov perceptron (Zhang and Clark, 2007) × N/A N/A 97.2 Semi-Markov CRF (Gao et al., 2007) √ N/A N/A 97.2 Latent-variable CRF (Sun et al., 2009b) √ 97.3 97.3 97.3 Our method (A Single CRF) √ 97.6 97.2 97.4 CU Best05 (Tseng et al., 2005) √ 94.1 94.6 94.3 CRF + rule-system (Zhang et al., 2006) √ 95.2 94.9 95.1 Semi-perceptron (Zhang and Clark, 2007) × N/A N/A 95.1 Latent-variable CRF (Sun et al., 2009b) √ 94.7 94.4 94.6 Our method (A Single CRF) √ 94.8 94.7 94.8 PKU Best05 (Chen et al., 2005) N/A 95.3 94.6 95.0 CRF + rule-system (Zhang et al., 2006) √ 94.7 95.5 95.1 semi-</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by conditional random fields for chinese word segmentation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 193–196, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Changning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>A unified character-based tagging framework for chinese word segmentation.</title>
<date>2010</date>
<journal>ACM Trans. Asian Lang. Inf. Process.,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="5260" citStr="Zhao et al., 2010" startWordPosition="790" endWordPosition="793"> propose a joint model for Chinese word segmentation and new word detection. • Compared with prior work, our system achieves better accuracies on both word segmentation and new word detection. 2 Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; </context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2010</marker>
<rawString>Hai Zhao, Changning Huang, Mu Li, and Bao-Liang Lu. 2010. A unified character-based tagging framework for chinese word segmentation. ACM Trans. Asian Lang. Inf. Process., 9(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
</authors>
<title>A chunking strategy towards unknown word detection in chinese word segmentation.</title>
<date>2005</date>
<booktitle>Proceedings of IJCNLP’05,</booktitle>
<volume>3651</volume>
<pages>530--541</pages>
<editor>In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="6439" citStr="Zhou, 2005" startWordPosition="972" endWordPosition="973">enters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training The most representative online training method is the SGD method. The SGD uses a small randomly-selec</context>
</contexts>
<marker>Zhou, 2005</marker>
<rawString>Guodong Zhou. 2005. A chunking strategy towards unknown word detection in chinese word segmentation. In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors, Proceedings of IJCNLP’05, volume 3651 of Lecture Notes in Computer Science, pages 530–541. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>