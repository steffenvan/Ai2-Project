<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031868">
<title confidence="0.9991765">
Automated Metrics That Agree With Human Judgements
On Generated Output for an Embodied Conversational Agent
</title>
<author confidence="0.90679">
Mary Ellen Foster
</author>
<affiliation confidence="0.7060665">
Informatik VI: Robotics and Embedded Systems
Technische Universit¨at M¨unchen
</affiliation>
<address confidence="0.915476">
Boltzmannstraße 3, D-85748 Garching bei M¨unchen, Germany
</address>
<email confidence="0.993614">
foster@in.tum.de
</email>
<sectionHeader confidence="0.993773" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99949116">
When evaluating a generation system, if a cor-
pus of target outputs is available, a common
and simple strategy is to compare the system
output against the corpus contents. However,
cross-validation metrics that test whether the
system makes exactly the same choices as the
corpus on each item have recently been shown
not to correlate well with human judgements
of quality. An alternative evaluation strategy
is to compute intrinsic, task-specific proper-
ties of the generated output; this requires more
domain-specific metrics, but can often pro-
duce a better assessment of the output. In this
paper, a range of metrics using both of these
techniques are used to evaluate three meth-
ods for selecting the facial displays of an em-
bodied conversational agent, and the predic-
tions of the metrics are compared with human
judgements of the same generated output. The
corpus-reproduction metrics show no relation-
ship with the human judgements, while the
intrinsic metrics that capture the number and
variety of facial displays show a significant
correlation with the preferences of the human
users.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970642857143">
Evaluating the output of a generation system is
known to be difficult: since generation is an open-
ended task, the criteria for success can be difficult
to define (cf. Mellish and Dale, 1998). In the cur-
rent state of the art, there are two main strategies
for evaluating the output of a generation system: the
behaviour or preferences of humans in response to
the output may be measured, or automated measures
may be computed on the output itself. A study in-
volving human judges is the most complete and con-
vincing evaluation of generated output. However,
such a study is not always practical, as recruiting
sufficient subjects can be time-consuming and ex-
pensive. So automated metrics are also used in ad-
dition to—or instead of—human studies.
When automatically evaluating generated output,
the goal is to find metrics that can easily be com-
puted and that can also be shown to correlate with
human judgements of quality. Such metrics have
been introduced in other fields, including PAR-
ADISE (Walker et al., 1997) for spoken dialogue
systems, BLEU (Papineni et al., 2002) for ma-
chine translation,1 and ROUGE (Lin, 2004) for sum-
marisation. Many automated generation evaluations
measure the similarity between the generated output
and a corpus of “gold-standard” target outputs, often
using measures such as precision and recall. Such
measures of corpus similarity are straightforward to
compute and easy to interpret; however, they are not
always appropriate for generation systems. One of
the main advantages of choosing dynamic genera-
tion over canned output is its flexibility and its abil-
ity to produce a range of different outputs; as pointed
out by Paris et al. (2007), “[e]valuation studies that
ignore the potential of the system to generate a range
of appropriate outputs will be necessarily limited.”
Indeed, several recent studies (Stent et al., 2005;
Belz and Reiter, 2006; Foster and White, 2007) have
shown that strict corpus-similarity measures tend to
favour repetitive generation strategies that do not di-
verge much, on average, from the corpus data, while
human judges often prefer output with more variety.
</bodyText>
<footnote confidence="0.8672345">
1Although Callison-Burch et al. (2006) have recently called
into question the utility of BLEU.
</footnote>
<page confidence="0.997787">
95
</page>
<bodyText confidence="0.996656157894737">
Automated metrics that take into account other
properties than strict corpus similarity have also
been used to evaluate the output of generation sys-
tems. Walker (2005) describes several evaluations
that used corpus data in a different way: each of the
corpus examples was associated with some reward
function (e.g., subjective user evaluation or task suc-
cess), and machine-learning techniques such as re-
inforcement learning or boosting were then used to
train the output planner. Foster and White (2007)
found that automated metrics based on factors other
than corpus similarity (e.g., the amount of variation
in the output) agreed better with user preferences
than did the corpus-similarity scores. Belz and Gatt
(2008) compare the predictions of a range of mea-
sures, both intrinsic and extrinsic, that were used
to evaluate the systems in a shared-task referring-
expression generation challenge. One main finding
from this comparison was that there was no signif-
icant correlation between the intrinsic and extrinsic
(task success) measures for this task.
All of the above studies considered only systems
that generate text, but many of the same factors also
apply to the generation of non-verbal behaviours for
an embodied conversational agent (ECA) (Cassell
et al., 2000). The behaviour of such an agent is nor-
mally based on recorded human behaviour, which
can provide targets similar to those used in corpus-
based evaluations of text-generation systems. How-
ever, just as in text generation, a multimodal system
that scores well on corpus similarity tends to pro-
duce highly repetitive non-verbal behaviours, so it
is equally important to gather human judgements to
accompany any automated evaluation.
This paper presents three corpus-driven methods
of selecting facial displays for an embodied conver-
sational agent and describes two studies comparing
the output of the different methods. All methods are
based on annotated data drawn from a corpus of hu-
man facial displays, and each uses the corpus data
in a different way. The first evaluation study uses
human judges to compare the output of the selection
methods against one another, while the second study
uses a range of automated metrics: several corpus-
reproduction measures, along with metrics based on
intrinsic properties of the outputs. The results of the
two studies are compared using multiple regression,
and the implications are discussed.
2 Corpus-based generation of facial
displays for an ECA
The experiments in this paper make use of the out-
put components of the COMIC multimodal dialogue
system (Foster et al., 2005), which adds a multi-
modal talking-head interface to a CAD-style system
for redesigning bathrooms. The studies focus on the
task of selecting appropriate ECA head and eyebrow
motions to accompany the turns in which the sys-
tem describes and compares the options for tiling
the room, as those are the parts of the output with
the most interesting and varied content.
The implementations were based on a corpus of
conversational facial displays derived from the be-
haviour of a single speaker reading approximately
450 scripted sentences generated by the COMIC
output-generation system. The OpenCCG syntac-
tic derivation trees (White, 2006) for the sentences
form the basis of the corpus. The leaf nodes in
these trees correspond to the individual words, while
the internal nodes correspond to multi-word con-
stituents. Every node in each tree was initially
labelled with all of the applicable contextual fea-
tures produced by the output planner: the user-
preference evaluation of the tile design being de-
scribed (positive/negative/neutral), the information
status (given/new) of each piece of information, and
the predicted speech-synthesiser prosody. The an-
notators then linked each facial display produced
by the speaker to the node or span of nodes in the
derivation tree covering the words temporally asso-
ciated with the display. Full details of this corpus are
given in Foster (2007a).
The most common display used by the speaker
was a downward nod, while the user-preference
evaluation had the single largest differential effect
on the displays used. When the speaker described
features of the design that the user was expected to
like, he was relatively more likely to turn to the right
and to raise his eyebrows (Figure 1(a)); on features
that the user was expected to dislike, on the other
hand, there was a higher probability of left leaning,
lowered eyebrows, and narrowed eyes (Figure 1(b)).
In a previous study, users were generally able to
recognise these “positive” and “negative” displays
when they were resynthesised on an embodied con-
versational agent (Foster, 2007b).
</bodyText>
<page confidence="0.991779">
96
</page>
<figure confidence="0.998931">
(a) Positive (b) Negative
</figure>
<figureCaption confidence="0.999974">
Figure 1: Characteristic facial displays from the corpus
</figureCaption>
<bodyText confidence="0.999977983333334">
Based on this corpus, three different strategies
were implemented for selecting facial displays to ac-
company the synthesised speech: one strategy us-
ing only the three characteristic displays described
above, along with two data-driven strategies drawing
on the full corpus data. All of the strategies use the
same basic process to select the displays to accom-
pany a sentence. Beginning with the contextually-
annotated syntactic tree for the sentence, the system
proceeds depth-first, selecting a face-display combi-
nation to accompany each node in turn. The main
difference among the strategies is the way that each
selects the displays for a node as it is encountered.
The rule-based strategy includes displays only on
derivation-tree nodes corresponding to specific tile-
design properties: that is, manufacturer and series
names, colours, and decorative motifs. The dis-
plays for such a node are entirely determined by
the user-preference evaluation of the property be-
ing described, and are based on the corpus patterns
described above: for every node associated with a
positive evaluation, this strategy selects a right turn
and brow raise; for a negative node, it selects a left
turn, brow lower, and eye squint; while for all other
design-property nodes, it chooses a downward nod.
While the rule-based strategy selects displays
only on nodes describing tile-design features, the
two data-driven strategies consider all nodes in the
syntactic tree for a sentence as possible sites for a fa-
cial display. To choose the displays for a given node,
the system considers the set of displays that occurred
on all nodes in the corpus with the same syntactic,
semantic, and pragmatic context, and then chooses a
display from this set in one of two ways. The ma-
jority strategy selects the most common option in all
cases, while the weighted strategy makes a stochas-
tic choice among all of the options based on the rel-
ative frequency. As a concrete example, consider a
hypothetical context in which the speaker made no
motion 80% of the time, a downward nod 15% of
the time, and a downward nod with a brow raise the
other 5% of the time. For nodes with this context,
the majority strategy would always choose no mo-
tion, while the weighted strategy would choose no
motion with probability 0.8, a downward nod with
probability 0.15, and a nod with a brow raise with
probability 0.05.
Table 1 shows a sample sentence from the corpus,
the original facial displays used by the speaker, and
the displays selected by each of the strategies. In the
figure, nd=d indicates a downward nod, bw=u and
bw=d a brow raise and lower, respectively, sq an eye
squint, ln=l a left lean, and tn=r a right turn. Most
of the displays in these schedules are associated with
leaf nodes in the derivation tree, and therefore with
single words in the output. However, both the left
lean in the original schedule and the right turn in
the weighted schedule are associated with internal
nodes in the tree, and therefore cover more than one
word in the surface string.
</bodyText>
<sectionHeader confidence="0.986439" genericHeader="method">
3 User-preference studies
</sectionHeader>
<bodyText confidence="0.999973833333333">
As a first comparison of the evaluation strategies,
human judges were asked to compare videos based
on the output of each of the generation strategies
to one another and to resynthesised versions of the
original displays from the corpus. This section gives
the details of a study in which the judges chose
</bodyText>
<page confidence="0.999118">
97
</page>
<table confidence="0.993400333333334">
Although it’s in the family style, the tiles are by Alessi.
Original nd=d nd=d ............................... nd=d ln=l nd=d,bw=u
........... nd=d
Rule-based ln=l,bw=d tn=r,bw=u
Majority nd=d nd=d
Weighted nd=d nd=d .. tn=r . .
</table>
<tableCaption confidence="0.999226">
Table 1: Face-display schedules for a sample sentence
</tableCaption>
<figureCaption confidence="0.9935">
Figure 2: RUTH talking head
</figureCaption>
<bodyText confidence="0.999703142857143">
among the original displays and the output of the
weighted and rule-based strategies. At the end of
the section, the results of this study are discussed to-
gether with the results of a similar previous study
comparing the two data-driven strategies to each
other; the full details of the earlier study are given
in Foster and Oberlander (2007).
</bodyText>
<subsectionHeader confidence="0.997893">
3.1 Subjects
</subsectionHeader>
<bodyText confidence="0.999986833333333">
Subjects were recruited for this experiment through
the Language Experiments Portal,2 a website dedi-
cated to online psycholinguistic experiments. There
were 36 subjects (20 female), 50% of whom identi-
fied themselves as native English speakers; most of
the subjects were between 20 and 29 years old.
</bodyText>
<subsectionHeader confidence="0.996227">
3.2 Materials
</subsectionHeader>
<bodyText confidence="0.9999198">
The materials for this experiment were based on 18
randomly-selected sentences from the corpus. For
each sentence, face-display schedules were gener-
ated using both the rule-based and the weighted
strategies. The Festival speech synthesiser (Clark
</bodyText>
<footnote confidence="0.747549">
2http://www.language-experiments.org/
</footnote>
<bodyText confidence="0.999791">
et al., 2004) and the RUTH animated talking head
(DeCarlo et al., 2004) (Figure 2) were used to create
video clips of the two generated schedules for each
sentence, along with a video clip showing the origi-
nal facial displays annotated in the corpus.
</bodyText>
<subsectionHeader confidence="0.999261">
3.3 Method
</subsectionHeader>
<bodyText confidence="0.999991">
Each subject saw a series of pairs of videos. Both
videos in a pair had identical spoken content, but the
face-display schedules differed: each trial included
two of rule-based, weighted, and original. For each
pair of videos, the subject was asked to select which
of the two versions they preferred. Subjects made
each pairwise comparison between schedule types
six times—three times in each order—for a total of
18 judgements. All subjects saw the same set of sen-
tences, in an individually randomly-selected order:
the pairwise choices between schedule types were
also allocated to items at random.
</bodyText>
<subsectionHeader confidence="0.644844">
3.4 Results and analysis
</subsectionHeader>
<bodyText confidence="0.968433666666667">
The overall pairwise preferences of the subjects in
this study are shown in Figure 3(a). A x2 goodness-
of-fit test can be used to evaluate the significance
of the choices made on each individual comparison.
For the comparison between original and rule-based
schedules, the preference is significant: x2(1,N =
216) = 4.17, p &lt; 0.05. The results are similar for the
original vs. weighted comparison: x2(1,N = 215) =
4.47, p &lt; 0.05. However, the preferences for the
weighted vs. rule-based comparison are not signifi-
cant: x2(1,N = 217) = 2.44, p ≈ 0.12.
Figure 3(b) shows the results from a similar pre-
vious study (Foster and Oberlander, 2007) in which
the subjects compared the two data-driven strate-
gies to the original displays, using a design identi-
cal to that used in the current study with 54 sub-
jects and 24 sentences. The responses given by
the subjects in this study also showed a signifi-
</bodyText>
<page confidence="0.988999">
98
</page>
<figure confidence="0.99887245">
125
100
75
50
25
0
123
93
Original vs. Rule-ba
153
Original vs. Majority
300
250
200
150
100
50
0
295
(a) Original, weighted, rule-based (b) Original, weighted, majority (Foster and Oberlander, 2007)
</figure>
<figureCaption confidence="0.999909">
Figure 3: Pairwise preferences from the user evaluations
</figureCaption>
<bodyText confidence="0.999916818181818">
cant preference for the original schedules over the
weighted ones (χ2(1,N = 448) = 6.51, p &lt; 0.05).
Both the weighted and the original schedules were
very strongly preferred over the majority schedules
(χ2(1,N = 448) = 45 and 26, respectively; p �
0.0001). The original vs. weighted comparison was
included in both studies (the rightmost pair of bars
on the two graphs in Figure 3), and the response pat-
terns across the two studies for this comparison did
not differ significantly from each other: χ2(1,N =
664) = 0.02, p ≈ 0.89.
</bodyText>
<sectionHeader confidence="0.568401" genericHeader="method">
3.5 Discussion
</sectionHeader>
<bodyText confidence="0.999991764705882">
Taken together, the results of these two studies sug-
gest a rough preference ordering among the dif-
ferent strategies for generating facial displays. In
both studies, the judges significantly preferred the
original displays from the corpus over any of the
automatically-generated alternatives. This suggests
that, for this generation task, the data in the cor-
pus can indeed be treated as a “gold standard”—
unlike, for example, the corpus used by Belz and
Reiter (2006), where the human judges sometimes
preferred generated output to the corpus data. The
schedules generated by the majority strategy, on the
other hand, were very obviously disliked by the
judges in the Foster and Oberlander (2007) study.
The ranking between the rule-based and weighted
schedules from the current study is less clear, al-
though there was a tendency to prefer the latter.
</bodyText>
<sectionHeader confidence="0.997014" genericHeader="method">
4 Automated evaluation
</sectionHeader>
<bodyText confidence="0.999969357142857">
Since the subjects in the user-preference studies gen-
erally selected the corpus schedules over any of
the alternatives, any automated metric for this task
should favour output that resembles the examples in
the corpus. The most obvious form of corpus simi-
larity is exact reproduction of the displays in the cor-
pus, which suggests using metrics such as precision
and recall that favour generation strategies whose
output on every item is as close as possible to what
was annotated in the corpus for that sentence. In
Section 4.1, several such corpus-reproduction met-
rics are described and their results presented.
For this type of open-ended generation task,
though, it can be overly restrictive to allow only
the displays that were annotated in the corpus for
a sentence and to penalise any deviation. Indeed, as
mentioned in the introduction, a number of previous
studies have found that the output of generation sys-
tems that score well on this type of metric is often
disliked in practice by users. Section 4.2 therefore
presents several intrinsic metrics that aim to cap-
ture corpus similarity of a different type: rather than
requiring the system to exactly reproduce the cor-
pus on each sentence, these metrics instead favour
strategies resulting in global behaviour that exhibits
similar patterns to those found in the corpus, without
necessarily agreeing exactly with the corpus on any
specific sentence.
</bodyText>
<page confidence="0.988464">
99
</page>
<figure confidence="0.999347793103449">
(a) Corpus-reproduction metrics
(b) Intrinsic metrics
0.31
0.29
1
0.8
0.6
0.52
0.4
0.22
0.2
0 Precision R
0
Major
Weig
Rule-
6
Tokens
5
4
3
2
1
0
3
4.58
5.38
3.15
2.07
</figure>
<figureCaption confidence="0.999964">
Figure 4: Results of the automated evaluations
</figureCaption>
<subsectionHeader confidence="0.966443">
4.1 Corpus-reproduction metrics
</subsectionHeader>
<bodyText confidence="0.999661964285714">
This first set of metrics compared the generated
schedules against the original schedules annotated in
the corpus, using 10-fold cross-validation. The first
three metrics that were tested are standard for this
sort of corpus-comparison task: recall, precision,
and F score. Recall was computed as the propor-
tion of the corpus displays for a sentence that were
reproduced exactly in the generated output, while
precision was the proportion of generated displays
that had exact matches in the corpus; the F score for
a sentence is then the harmonic mean of these two
values, as usual. The leftmost three columns in Ta-
ble 2 show the precision, recall, and F score for the
sample schedules in Table 1.
In addition to the above commonly-used metrics,
two other corpus-reproduction metrics were also
computed. The first, node accuracy, represents the
proportion of nodes in the derivation tree for a sen-
tence where the proposed displays were correct, in-
cluding those nodes where the system correctly se-
lected no motion—a baseline system that never pro-
poses any motion scores 0.79 on this measure. The
fourth column of Table 2 shows the node-accuracy
score for the sample sentences. The final corpus-
reproduction metric compared the proposed displays
to the annotated corpus displays using the R agree-
ment measure (Artstein and Poesio, 2005). R is a
weighted measure that permits different levels of
</bodyText>
<table confidence="0.9991172">
P R F NAcc Tok Typ TTR
Original – – – – 6 3 0.5
Rule-based 0 0 0 0.65 2 2 1
Majority 0.50 0.14 0.11 0.70 2 1 0.5
Weighted 0.67 0.29 0.20 0.74 3 2 0.67
</table>
<tableCaption confidence="0.999287">
Table 2: Automated evaluation of the sample schedules
</tableCaption>
<bodyText confidence="0.940287869565217">
agreement when annotations overlap, and that can
therefore capture a more fine-grained form of agree-
ment than other measures such as ic.
Figure 4(a) shows the results for all of these
corpus-reproduction measures, averaged across the
sentences in the corpus; the results for the weighted
and majority strategies are from Foster and Ober-
lander (2007). The majority strategy scored uni-
formly higher than the weighted strategy on all of
these measures—particularly on precision—while
the weighted strategy in turn scored higher than the
rule-based strategy on all measures except for node
accuracy. Using a Wilcoxon rank sum test with a
Bonferroni correction for multiple comparisons, all
of the differences among the strategies on precision,
recall, F score, and node accuracy are significant at
p &lt; 0.001. Significance cannot be assessed for the
differences in R scores, as noted by Artstein and Poe-
sio (2005), but the results are similar. Also, the node
accuracy score for the majority strategy is signifi-
cantly better than the no-motion baseline of 0.79,
while those for the weighted and rule-based strate-
gies are worse (also all p &lt; 0.001).
</bodyText>
<page confidence="0.982098">
100
</page>
<bodyText confidence="0.999996666666667">
As expected—and as noted by Foster and Ober-
lander (2007)—all of the corpus-reproduction met-
rics strongly favoured the weighted strategy over the
weighted strategy and generally penalised the rule-
based strategy. Since the majority strategy always
chooses the most probable option, it is not surpris-
ing that it agrees more often with the corpus than
do the other strategies, which deliberately select less
frequent options; this led to its relatively high scores
on the corpus-reproduction metrics. It is also not
surprising that the weighted strategy beat the rule-
based strategy on most of these metrics, as the for-
mer selects from the most frequent options, while
the latter uses the most marked options, which are
not generally the most frequent.
</bodyText>
<subsectionHeader confidence="0.902329">
4.2 Intrinsic metrics
</subsectionHeader>
<bodyText confidence="0.989232705128206">
The metrics in the preceding section compared the
displays selected for a sentence against the displays
found in the corpus for that sentence. This sec-
tion describes other measures that are computed di-
rectly on the generated schedules, without any ref-
erence to the corpus data. For each sentence, the
following values were counted: the total number of
face-display combinations (i.e., the number of dis-
play tokens), and the number of different combina-
tions (types). In addition to being used as metrics
themselves, these two counts were also used to com-
pute a third value: the type/token ratio (TTR) (i.e.,
# types ), which captures the diversity of the dis-
# tokens
plays selected for each sentence.
These intrinsic metrics were computed on each
sentence produced in the cross-validation study from
the preceding section and then averaged to produce
the final results. Since these metrics do not require
the original corpus data for comparison, they were
also computed on the original corpus schedules. The
rightmost columns in Table 2 show the intrinsic re-
sults for the sample schedules in Table 1.
The overall results for these metrics across the en-
tire corpus are shown in Figure 4(b). The original
corpus had both the most displays types and the most
tokens; the values for weighted choice were a fairly
close second, those for majority choice third, while
the rule-based strategy scored lowest on both of
these metrics. Except for the difference between ma-
jority and rule-based on the facial-display types—
which is not significant—all of the differences be-
tween schedule types on these two measures are sig-
nificant at p &lt; 0.001 on a Wilcoxon rank sum test
with Bonferroni correction. When it comes to the
type/token ratio, the value for the majority-choice
schedule is significantly lower than that for the other
three schedule types (all p &lt; 0.0001), while the
value for weighted choice is somewhat higher than
that for the original schedules (p &lt; 0.01); no other
differences are significant.
Since the original corpus schedules scored the
highest on the user study, these metrics should be
considered in the context of of how close the results
are to those of the corpus. Figure 4(b) shows that
the weighted strategy is most similar to the corpus in
both the number and the diversity of displays it se-
lects, while the other two strategies have much lower
diversity. However, even though the rule-based strat-
egy selects fewer displays all of the other strategies,
its TTR is more similar to that of the corpus and the
weighted strategy, while the majority strategy has a
much lower TTR. In fact, in the schedules generated
by the majority-choice strategy, nearly 90% of the
displays that were selected were downward nods.
5 Comparing the automated metrics with
human preferences
Qualitatively, the results of the corpus-reproduction
metrics differ greatly from the preferences of the
human judges. The users generally liked the ma-
jority schedules the least, while all of these met-
rics scored this strategy the highest. Among the
intrinsic metrics, the type and token counts placed
the weighted schedules closest to the corpus, while
the majority and rule-based strategies were further
away; this agrees with the human results for the
two data-driven strategies, but not for the rule-based
strategy. On the other hand, the TTR indicated that
the output of the rule-based and weighted schedules
was similar to the schedules found in the corpus,
while the majority-choice strategy produced sen-
tences with TTRs more different from the corpus,
generally agreeing with the human results.
To permit a more quantitative comparison be-
tween the predictions of the automated metrics and
the judges’ preferences, the pairwise preferences
from the user study were converted into a numeric
value called the selection ratio. The selection ratio
</bodyText>
<page confidence="0.996691">
101
</page>
<bodyText confidence="0.999988523809524">
for an item (i.e., a sentence with a particular set of
facial displays) was computed as the number of tri-
als on which that item was selected, divided by the
total number of trials on which that item was an op-
tion. For example, an item that was always preferred
over any of the alternatives on all trials would score
1.0 on this measure, while an item that was selected
a quarter of the time would score 0.25. The selec-
tion ratios of the items used in the human-preference
studies ranged from 0.13 to 0.85. As a concrete ex-
ample, when the sentence in Table 1 was used in the
Foster and Oberlander (2007) study, the selection ra-
tios were 0.43 for the original version, 0.33 for the
majority version, and 0.24 for the weighted version.
The relationship between the selection ratio and
the full set of automated metrics from the preced-
ing section was assessed through multiple linear re-
gression. An initial model including all of the auto-
mated metrics as predictor variables had an adjusted
R2 value of 0.413. Performing stepwise selection on
this initial model resulted in a final model with two
significant predictor variables—display tokens and
TTR—and an adjusted R2 of 0.422. The regression
coefficients for both of these predictor variables are
positive, with high significance (p &lt; 0.001). While
the R2 values indicate that neither the initial nor the
final model fully explains the selection ratios from
the user study, the details of the models themselves
are relevant to the overall goal of finding automated
metrics that agree with human preferences.
The results of the stepwise selection have backed
up the qualitative intuition that none of the corpus-
reproduction metrics had any relationship to the
users’ preferences, while the number and diversity
of displays per sentence appear to have contributed
much more strongly to the choices made by the hu-
man judges. This adds to the growing body of ev-
idence that intrinsic measures are the preferred op-
tion for evaluating the output of generation systems,
particularly those that are designed to incorporate
variation into their output, while measures based on
strict corpus similarity are less likely to be useful.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999991522727273">
This paper has presented three methods for using
corpus data to select facial displays for an embod-
ied agent and shown the results from two studies
comparing the output generated by these methods.
When human judges rated the output, they preferred
the original displays from the corpus and strongly
disliked the displays selected by a majority-choice
strategy, with the weighted and rule-based strategies
in between. In the automated evaluation, the metrics
that directly compared the generated output against
the corpus data favoured the majority strategy and
did not show any relationship with the user prefer-
ences. On the other hand, the number of displays
accompanying a sentence and the diversity of those
displays both had a positive relationship with the
rate at which users selected that display schedule.
These results confirm those of previous text-
generation evaluations and extend these results to
the multimodal-generation case. This adds to the
body of evidence that, even though direct corpus re-
production is often the easiest factor to analyse au-
tomatically, it is rarely an accurate reflection of user
reactions to generated output. If a system performs
well on this type of metric, its output tends to be con-
strained to a small space; for example, the majority-
choice strategy used in these studies nearly always
selected a nodding expression. For most generation
tasks, output options beyond those in the corpus are
often equally valid, and users seem to prefer a sys-
tem that makes use of this wider space of variations.
This suggests that corpus-based generation sys-
tems should use strategies that retain the full range
of variation, and—perhaps most importantly—that
metrics based on factors other than strict similarity
are more likely to capture human preferences when
evaluating generated output.
The user study described here was based only on
the preferences of human judges. In future, it would
be informative to include more task-based measures
such as task success and time taken, as user pref-
erences do not always correlate with performance
(Nielsen and Levy, 1994), to see if a different style
of automated measure agrees better with the results
of this sort of user study.
</bodyText>
<sectionHeader confidence="0.996551" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.98390575">
This work was partly supported by the EU projects
COMIC (IST-2001-32311) and JAST (FP6-003747-
IP). Thanks to Jon Oberlander, Jean Carletta, and the
INLG reviewers for helpful feedback.
</bodyText>
<page confidence="0.998461">
102
</page>
<sectionHeader confidence="0.995851" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999815206896552">
R. Artstein and M. Poesio. 2005. Kappa3 = al-
pha (or beta). Technical Report CSM-437,
University of Essex Department of Com-
puter Science. http://cswww.essex.ac.uk/
technical-reports/2005/csm-437.pdf.
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic evalu-
ation measures for referring expression generation. In
Proceedings of ACL/HLT 2008.
A. Belz and E. Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings of
EACL2006. acl:E06-1040.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine transla-
tion research. In Proceedings of EACL 2006. acl:
E06-1032.
J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, ed-
itors. 2000. Embodied Conversational Agents. MIT
Press.
R. A. J. Clark, K. Richmond, and S. King. 2004. Festi-
val 2 – build your own general purpose unit selection
speech synthesiser. In Proceedings of the 5th ISCA
Workshop on Speech Synthesis. http://www.ssw5.
org/papers/1047.pdf.
R. Dale and M. White, editors. 2007. Report
from the Workshop on Shared Tasks and Com-
parative Evaluation in Natural Language Genera-
tion. http://ling.ohio-state.edu/nlgeval07/
NLGEval07-Report.pdf.
D. DeCarlo, M. Stone, C. Revilla, and J. Venditti. 2004.
Specifying and animating facial signals for discourse
in embodied conversational agents. Computer Anima-
tion and Virtual Worlds, 15(1):27–38. doi:10.1002/
cav.5.
M. E. Foster. 2007a. Associating facial displays with syn-
tactic constituents for generation. In Proceedings of
the ACL 2007 Linguistic Annotation Workshop. acl:
W07-1504.
M. E. Foster. 2007b. Generating embodied descrip-
tions tailored to user preferences. In Proceedings
of Intelligent Virtual Agents 2007. doi:10.1007/
978-3-540-74997-4_24.
M. E. Foster and J. Oberlander. 2007. Corpus-based
generation of head and eyebrow motion for an em-
bodied conversational agent. Language Resources
and Evaluation, 41(3–4):305–323. doi:10.1007/
s10579-007-9055-3.
M. E. Foster and M. White. 2007. Avoiding repetition in
generated text. In Proceedings of ENLG 2007. acl:
W07-2305.
M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005.
Multimodal generation in the COMIC dialogue sys-
tem. In Proceedings of the ACL 2005 Demo Session.
acl:P05-3012.
C. Y. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Proceedings of the ACL 2004
Workshop on Text Summarization. acl:W04-1013.
C. Mellish and R. Dale. 1998. Evaluation in the con-
text of natural language generation. Computer Speech
and Language, 12(4):349–373. doi:10.1006/csla.
1998.0106.
J. Nielsen and J. Levy. 1994. Measuring usability: pref-
erence vs. performance. Communications of the ACM,
37(4):66–75. doi:10.1145/175276.175282.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL 2002. acl:
P02-1040.
C. Paris, D. Scott, N. Green, K. McCoy, , and D. Mc-
Donald. 2007. Desiderata for evaluation of natural lan-
guage generation. In Dale and White (2007), chapter 2.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Computational Linguistics and Intelligent
Text Processing, pages 341–351. Springer. doi:10.
1007/b105772.
M. A. Walker. 2005. Can we talk? Methods for evalu-
ation and training of spoken dialogue systems. Lan-
guage Resources and Evaluation, 39(1):65–75. doi:
10.1007/s10579-005-2696-1.
M. A. Walker, D. J. Litman, C. A. Kamm, and A. Abella.
1997. PARADISE: A general framework for eval-
uating spoken dialogue agents. In Proceedings of
ACL/EACL 1997. acl:P97-1035.
M. White. 2006. Efficient realization of coordinate struc-
tures in Combinatory Categorial Grammar. Research
on Language and Computation, 4(1):39–75. doi:
10.1007/s11168-006-9010-2.
</reference>
<page confidence="0.999288">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.265363">
<title confidence="0.996081">Automated Metrics That Agree With Human On Generated Output for an Embodied Conversational Agent</title>
<author confidence="0.971052">Mary Ellen</author>
<affiliation confidence="0.467166">Informatik VI: Robotics and Embedded Technische Universit¨at</affiliation>
<address confidence="0.716056">Boltzmannstraße 3, D-85748 Garching bei M¨unchen, Germany</address>
<email confidence="0.999646">foster@in.tum.de</email>
<abstract confidence="0.997789230769231">When evaluating a generation system, if a corpus of target outputs is available, a common and simple strategy is to compare the system output against the corpus contents. However, cross-validation metrics that test whether the system makes exactly the same choices as the corpus on each item have recently been shown not to correlate well with human judgements of quality. An alternative evaluation strategy is to compute intrinsic, task-specific properties of the generated output; this requires more domain-specific metrics, but can often produce a better assessment of the output. In this paper, a range of metrics using both of these techniques are used to evaluate three methods for selecting the facial displays of an embodied conversational agent, and the predictions of the metrics are compared with human judgements of the same generated output. The corpus-reproduction metrics show no relationship with the human judgements, while the intrinsic metrics that capture the number and variety of facial displays show a significant correlation with the preferences of the human users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Kappa3 = alpha (or beta).</title>
<date>2005</date>
<tech>Technical Report CSM-437,</tech>
<institution>University of Essex Department of Computer Science.</institution>
<note>http://cswww.essex.ac.uk/ technical-reports/2005/csm-437.pdf.</note>
<contexts>
<context position="19448" citStr="Artstein and Poesio, 2005" startWordPosition="3140" endWordPosition="3143">on to the above commonly-used metrics, two other corpus-reproduction metrics were also computed. The first, node accuracy, represents the proportion of nodes in the derivation tree for a sentence where the proposed displays were correct, including those nodes where the system correctly selected no motion—a baseline system that never proposes any motion scores 0.79 on this measure. The fourth column of Table 2 shows the node-accuracy score for the sample sentences. The final corpusreproduction metric compared the proposed displays to the annotated corpus displays using the R agreement measure (Artstein and Poesio, 2005). R is a weighted measure that permits different levels of P R F NAcc Tok Typ TTR Original – – – – 6 3 0.5 Rule-based 0 0 0 0.65 2 2 1 Majority 0.50 0.14 0.11 0.70 2 1 0.5 Weighted 0.67 0.29 0.20 0.74 3 2 0.67 Table 2: Automated evaluation of the sample schedules agreement when annotations overlap, and that can therefore capture a more fine-grained form of agreement than other measures such as ic. Figure 4(a) shows the results for all of these corpus-reproduction measures, averaged across the sentences in the corpus; the results for the weighted and majority strategies are from Foster and Ober</context>
</contexts>
<marker>Artstein, Poesio, 2005</marker>
<rawString>R. Artstein and M. Poesio. 2005. Kappa3 = alpha (or beta). Technical Report CSM-437, University of Essex Department of Computer Science. http://cswww.essex.ac.uk/ technical-reports/2005/csm-437.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>A Gatt</author>
</authors>
<title>Intrinsic vs. extrinsic evaluation measures for referring expression generation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT</booktitle>
<contexts>
<context position="4334" citStr="Belz and Gatt (2008)" startWordPosition="677" endWordPosition="680">en used to evaluate the output of generation systems. Walker (2005) describes several evaluations that used corpus data in a different way: each of the corpus examples was associated with some reward function (e.g., subjective user evaluation or task success), and machine-learning techniques such as reinforcement learning or boosting were then used to train the output planner. Foster and White (2007) found that automated metrics based on factors other than corpus similarity (e.g., the amount of variation in the output) agreed better with user preferences than did the corpus-similarity scores. Belz and Gatt (2008) compare the predictions of a range of measures, both intrinsic and extrinsic, that were used to evaluate the systems in a shared-task referringexpression generation challenge. One main finding from this comparison was that there was no significant correlation between the intrinsic and extrinsic (task success) measures for this task. All of the above studies considered only systems that generate text, but many of the same factors also apply to the generation of non-verbal behaviours for an embodied conversational agent (ECA) (Cassell et al., 2000). The behaviour of such an agent is normally ba</context>
</contexts>
<marker>Belz, Gatt, 2008</marker>
<rawString>A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic evaluation measures for referring expression generation. In Proceedings of ACL/HLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL2006.</booktitle>
<pages>06--1040</pages>
<contexts>
<context position="3274" citStr="Belz and Reiter, 2006" startWordPosition="514" endWordPosition="517">target outputs, often using measures such as precision and recall. Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems. One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al. (2007), “[e]valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited.” Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety. 1Although Callison-Burch et al. (2006) have recently called into question the utility of BLEU. 95 Automated metrics that take into account other properties than strict corpus similarity have also been used to evaluate the output of generation systems. Walker (2005) describes several evaluations that used corpus data in a different way: each of the corpus e</context>
<context position="16049" citStr="Belz and Reiter (2006)" startWordPosition="2585" endWordPosition="2588"> response patterns across the two studies for this comparison did not differ significantly from each other: χ2(1,N = 664) = 0.02, p ≈ 0.89. 3.5 Discussion Taken together, the results of these two studies suggest a rough preference ordering among the different strategies for generating facial displays. In both studies, the judges significantly preferred the original displays from the corpus over any of the automatically-generated alternatives. This suggests that, for this generation task, the data in the corpus can indeed be treated as a “gold standard”— unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data. The schedules generated by the majority strategy, on the other hand, were very obviously disliked by the judges in the Foster and Oberlander (2007) study. The ranking between the rule-based and weighted schedules from the current study is less clear, although there was a tendency to prefer the latter. 4 Automated evaluation Since the subjects in the user-preference studies generally selected the corpus schedules over any of the alternatives, any automated metric for this task should favour output that resembles t</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>A. Belz and E. Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of EACL2006. acl:E06-1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>06--1032</pages>
<contexts>
<context position="3554" citStr="Callison-Burch et al. (2006)" startWordPosition="556" endWordPosition="559">ation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al. (2007), “[e]valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited.” Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety. 1Although Callison-Burch et al. (2006) have recently called into question the utility of BLEU. 95 Automated metrics that take into account other properties than strict corpus similarity have also been used to evaluate the output of generation systems. Walker (2005) describes several evaluations that used corpus data in a different way: each of the corpus examples was associated with some reward function (e.g., subjective user evaluation or task success), and machine-learning techniques such as reinforcement learning or boosting were then used to train the output planner. Foster and White (2007) found that automated metrics based o</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In Proceedings of EACL 2006. acl: E06-1032.</rawString>
</citation>
<citation valid="true">
<title>Embodied Conversational Agents.</title>
<date>2000</date>
<editor>J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, editors.</editor>
<publisher>MIT Press.</publisher>
<marker>2000</marker>
<rawString>J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, editors. 2000. Embodied Conversational Agents. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A J Clark</author>
<author>K Richmond</author>
<author>S King</author>
</authors>
<title>Festival 2 – build your own general purpose unit selection speech synthesiser.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th ISCA Workshop on Speech Synthesis. http://www.ssw5. org/papers/1047.pdf.</booktitle>
<marker>Clark, Richmond, King, 2004</marker>
<rawString>R. A. J. Clark, K. Richmond, and S. King. 2004. Festival 2 – build your own general purpose unit selection speech synthesiser. In Proceedings of the 5th ISCA Workshop on Speech Synthesis. http://www.ssw5. org/papers/1047.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>M White</author>
<author>editors</author>
</authors>
<date>2007</date>
<booktitle>Report from the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation. http://ling.ohio-state.edu/nlgeval07/ NLGEval07-Report.pdf.</booktitle>
<marker>Dale, White, editors, 2007</marker>
<rawString>R. Dale and M. White, editors. 2007. Report from the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation. http://ling.ohio-state.edu/nlgeval07/ NLGEval07-Report.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D DeCarlo</author>
<author>M Stone</author>
<author>C Revilla</author>
<author>J Venditti</author>
</authors>
<title>Specifying and animating facial signals for discourse in embodied conversational agents. Computer Animation and Virtual Worlds,</title>
<date>2004</date>
<volume>15</volume>
<issue>1</issue>
<pages>10--1002</pages>
<contexts>
<context position="13067" citStr="DeCarlo et al., 2004" startWordPosition="2084" endWordPosition="2087">ugh the Language Experiments Portal,2 a website dedicated to online psycholinguistic experiments. There were 36 subjects (20 female), 50% of whom identified themselves as native English speakers; most of the subjects were between 20 and 29 years old. 3.2 Materials The materials for this experiment were based on 18 randomly-selected sentences from the corpus. For each sentence, face-display schedules were generated using both the rule-based and the weighted strategies. The Festival speech synthesiser (Clark 2http://www.language-experiments.org/ et al., 2004) and the RUTH animated talking head (DeCarlo et al., 2004) (Figure 2) were used to create video clips of the two generated schedules for each sentence, along with a video clip showing the original facial displays annotated in the corpus. 3.3 Method Each subject saw a series of pairs of videos. Both videos in a pair had identical spoken content, but the face-display schedules differed: each trial included two of rule-based, weighted, and original. For each pair of videos, the subject was asked to select which of the two versions they preferred. Subjects made each pairwise comparison between schedule types six times—three times in each order—for a tota</context>
</contexts>
<marker>DeCarlo, Stone, Revilla, Venditti, 2004</marker>
<rawString>D. DeCarlo, M. Stone, C. Revilla, and J. Venditti. 2004. Specifying and animating facial signals for discourse in embodied conversational agents. Computer Animation and Virtual Worlds, 15(1):27–38. doi:10.1002/ cav.5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
</authors>
<title>Associating facial displays with syntactic constituents for generation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Linguistic Annotation Workshop. acl:</booktitle>
<pages>07--1504</pages>
<contexts>
<context position="7579" citStr="Foster (2007" startWordPosition="1193" endWordPosition="1194"> nodes correspond to multi-word constituents. Every node in each tree was initially labelled with all of the applicable contextual features produced by the output planner: the userpreference evaluation of the tile design being described (positive/negative/neutral), the information status (given/new) of each piece of information, and the predicted speech-synthesiser prosody. The annotators then linked each facial display produced by the speaker to the node or span of nodes in the derivation tree covering the words temporally associated with the display. Full details of this corpus are given in Foster (2007a). The most common display used by the speaker was a downward nod, while the user-preference evaluation had the single largest differential effect on the displays used. When the speaker described features of the design that the user was expected to like, he was relatively more likely to turn to the right and to raise his eyebrows (Figure 1(a)); on features that the user was expected to dislike, on the other hand, there was a higher probability of left leaning, lowered eyebrows, and narrowed eyes (Figure 1(b)). In a previous study, users were generally able to recognise these “positive” and “n</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>M. E. Foster. 2007a. Associating facial displays with syntactic constituents for generation. In Proceedings of the ACL 2007 Linguistic Annotation Workshop. acl: W07-1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
</authors>
<title>Generating embodied descriptions tailored to user preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of Intelligent Virtual Agents</booktitle>
<volume>10</volume>
<pages>978--3</pages>
<contexts>
<context position="7579" citStr="Foster (2007" startWordPosition="1193" endWordPosition="1194"> nodes correspond to multi-word constituents. Every node in each tree was initially labelled with all of the applicable contextual features produced by the output planner: the userpreference evaluation of the tile design being described (positive/negative/neutral), the information status (given/new) of each piece of information, and the predicted speech-synthesiser prosody. The annotators then linked each facial display produced by the speaker to the node or span of nodes in the derivation tree covering the words temporally associated with the display. Full details of this corpus are given in Foster (2007a). The most common display used by the speaker was a downward nod, while the user-preference evaluation had the single largest differential effect on the displays used. When the speaker described features of the design that the user was expected to like, he was relatively more likely to turn to the right and to raise his eyebrows (Figure 1(a)); on features that the user was expected to dislike, on the other hand, there was a higher probability of left leaning, lowered eyebrows, and narrowed eyes (Figure 1(b)). In a previous study, users were generally able to recognise these “positive” and “n</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>M. E. Foster. 2007b. Generating embodied descriptions tailored to user preferences. In Proceedings of Intelligent Virtual Agents 2007. doi:10.1007/ 978-3-540-74997-4_24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>J Oberlander</author>
</authors>
<title>Corpus-based generation of head and eyebrow motion for an embodied conversational agent. Language Resources and Evaluation,</title>
<date>2007</date>
<pages>41--3</pages>
<contexts>
<context position="12383" citStr="Foster and Oberlander (2007)" startWordPosition="1985" endWordPosition="1988">ly style, the tiles are by Alessi. Original nd=d nd=d ............................... nd=d ln=l nd=d,bw=u ........... nd=d Rule-based ln=l,bw=d tn=r,bw=u Majority nd=d nd=d Weighted nd=d nd=d .. tn=r . . Table 1: Face-display schedules for a sample sentence Figure 2: RUTH talking head among the original displays and the output of the weighted and rule-based strategies. At the end of the section, the results of this study are discussed together with the results of a similar previous study comparing the two data-driven strategies to each other; the full details of the earlier study are given in Foster and Oberlander (2007). 3.1 Subjects Subjects were recruited for this experiment through the Language Experiments Portal,2 a website dedicated to online psycholinguistic experiments. There were 36 subjects (20 female), 50% of whom identified themselves as native English speakers; most of the subjects were between 20 and 29 years old. 3.2 Materials The materials for this experiment were based on 18 randomly-selected sentences from the corpus. For each sentence, face-display schedules were generated using both the rule-based and the weighted strategies. The Festival speech synthesiser (Clark 2http://www.language-expe</context>
<context position="14523" citStr="Foster and Oberlander, 2007" startWordPosition="2326" endWordPosition="2329"> pairwise preferences of the subjects in this study are shown in Figure 3(a). A x2 goodnessof-fit test can be used to evaluate the significance of the choices made on each individual comparison. For the comparison between original and rule-based schedules, the preference is significant: x2(1,N = 216) = 4.17, p &lt; 0.05. The results are similar for the original vs. weighted comparison: x2(1,N = 215) = 4.47, p &lt; 0.05. However, the preferences for the weighted vs. rule-based comparison are not significant: x2(1,N = 217) = 2.44, p ≈ 0.12. Figure 3(b) shows the results from a similar previous study (Foster and Oberlander, 2007) in which the subjects compared the two data-driven strategies to the original displays, using a design identical to that used in the current study with 54 subjects and 24 sentences. The responses given by the subjects in this study also showed a signifi98 125 100 75 50 25 0 123 93 Original vs. Rule-ba 153 Original vs. Majority 300 250 200 150 100 50 0 295 (a) Original, weighted, rule-based (b) Original, weighted, majority (Foster and Oberlander, 2007) Figure 3: Pairwise preferences from the user evaluations cant preference for the original schedules over the weighted ones (χ2(1,N = 448) = 6.5</context>
<context position="16278" citStr="Foster and Oberlander (2007)" startWordPosition="2621" endWordPosition="2624">ference ordering among the different strategies for generating facial displays. In both studies, the judges significantly preferred the original displays from the corpus over any of the automatically-generated alternatives. This suggests that, for this generation task, the data in the corpus can indeed be treated as a “gold standard”— unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data. The schedules generated by the majority strategy, on the other hand, were very obviously disliked by the judges in the Foster and Oberlander (2007) study. The ranking between the rule-based and weighted schedules from the current study is less clear, although there was a tendency to prefer the latter. 4 Automated evaluation Since the subjects in the user-preference studies generally selected the corpus schedules over any of the alternatives, any automated metric for this task should favour output that resembles the examples in the corpus. The most obvious form of corpus similarity is exact reproduction of the displays in the corpus, which suggests using metrics such as precision and recall that favour generation strategies whose output o</context>
<context position="20061" citStr="Foster and Oberlander (2007)" startWordPosition="3251" endWordPosition="3255">d Poesio, 2005). R is a weighted measure that permits different levels of P R F NAcc Tok Typ TTR Original – – – – 6 3 0.5 Rule-based 0 0 0 0.65 2 2 1 Majority 0.50 0.14 0.11 0.70 2 1 0.5 Weighted 0.67 0.29 0.20 0.74 3 2 0.67 Table 2: Automated evaluation of the sample schedules agreement when annotations overlap, and that can therefore capture a more fine-grained form of agreement than other measures such as ic. Figure 4(a) shows the results for all of these corpus-reproduction measures, averaged across the sentences in the corpus; the results for the weighted and majority strategies are from Foster and Oberlander (2007). The majority strategy scored uniformly higher than the weighted strategy on all of these measures—particularly on precision—while the weighted strategy in turn scored higher than the rule-based strategy on all measures except for node accuracy. Using a Wilcoxon rank sum test with a Bonferroni correction for multiple comparisons, all of the differences among the strategies on precision, recall, F score, and node accuracy are significant at p &lt; 0.001. Significance cannot be assessed for the differences in R scores, as noted by Artstein and Poesio (2005), but the results are similar. Also, the </context>
<context position="26072" citStr="Foster and Oberlander (2007)" startWordPosition="4246" endWordPosition="4249">. The selection ratio 101 for an item (i.e., a sentence with a particular set of facial displays) was computed as the number of trials on which that item was selected, divided by the total number of trials on which that item was an option. For example, an item that was always preferred over any of the alternatives on all trials would score 1.0 on this measure, while an item that was selected a quarter of the time would score 0.25. The selection ratios of the items used in the human-preference studies ranged from 0.13 to 0.85. As a concrete example, when the sentence in Table 1 was used in the Foster and Oberlander (2007) study, the selection ratios were 0.43 for the original version, 0.33 for the majority version, and 0.24 for the weighted version. The relationship between the selection ratio and the full set of automated metrics from the preceding section was assessed through multiple linear regression. An initial model including all of the automated metrics as predictor variables had an adjusted R2 value of 0.413. Performing stepwise selection on this initial model resulted in a final model with two significant predictor variables—display tokens and TTR—and an adjusted R2 of 0.422. The regression coefficien</context>
</contexts>
<marker>Foster, Oberlander, 2007</marker>
<rawString>M. E. Foster and J. Oberlander. 2007. Corpus-based generation of head and eyebrow motion for an embodied conversational agent. Language Resources and Evaluation, 41(3–4):305–323. doi:10.1007/ s10579-007-9055-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>M White</author>
</authors>
<title>Avoiding repetition in generated text.</title>
<date>2007</date>
<booktitle>In Proceedings of ENLG</booktitle>
<pages>07--2305</pages>
<contexts>
<context position="3299" citStr="Foster and White, 2007" startWordPosition="518" endWordPosition="521">sing measures such as precision and recall. Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems. One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al. (2007), “[e]valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited.” Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety. 1Although Callison-Burch et al. (2006) have recently called into question the utility of BLEU. 95 Automated metrics that take into account other properties than strict corpus similarity have also been used to evaluate the output of generation systems. Walker (2005) describes several evaluations that used corpus data in a different way: each of the corpus examples was associated wi</context>
</contexts>
<marker>Foster, White, 2007</marker>
<rawString>M. E. Foster and M. White. 2007. Avoiding repetition in generated text. In Proceedings of ENLG 2007. acl: W07-2305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>M White</author>
<author>A Setzer</author>
<author>R Catizone</author>
</authors>
<title>Multimodal generation in the COMIC dialogue system.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL 2005 Demo Session.</booktitle>
<pages>05--3012</pages>
<contexts>
<context position="6200" citStr="Foster et al., 2005" startWordPosition="973" endWordPosition="976">isplays, and each uses the corpus data in a different way. The first evaluation study uses human judges to compare the output of the selection methods against one another, while the second study uses a range of automated metrics: several corpusreproduction measures, along with metrics based on intrinsic properties of the outputs. The results of the two studies are compared using multiple regression, and the implications are discussed. 2 Corpus-based generation of facial displays for an ECA The experiments in this paper make use of the output components of the COMIC multimodal dialogue system (Foster et al., 2005), which adds a multimodal talking-head interface to a CAD-style system for redesigning bathrooms. The studies focus on the task of selecting appropriate ECA head and eyebrow motions to accompany the turns in which the system describes and compares the options for tiling the room, as those are the parts of the output with the most interesting and varied content. The implementations were based on a corpus of conversational facial displays derived from the behaviour of a single speaker reading approximately 450 scripted sentences generated by the COMIC output-generation system. The OpenCCG syntac</context>
</contexts>
<marker>Foster, White, Setzer, Catizone, 2005</marker>
<rawString>M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005. Multimodal generation in the COMIC dialogue system. In Proceedings of the ACL 2005 Demo Session. acl:P05-3012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL 2004 Workshop on Text Summarization.</booktitle>
<pages>04--1013</pages>
<contexts>
<context position="2511" citStr="Lin, 2004" startWordPosition="398" endWordPosition="399">ncing evaluation of generated output. However, such a study is not always practical, as recruiting sufficient subjects can be time-consuming and expensive. So automated metrics are also used in addition to—or instead of—human studies. When automatically evaluating generated output, the goal is to find metrics that can easily be computed and that can also be shown to correlate with human judgements of quality. Such metrics have been introduced in other fields, including PARADISE (Walker et al., 1997) for spoken dialogue systems, BLEU (Papineni et al., 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. Many automated generation evaluations measure the similarity between the generated output and a corpus of “gold-standard” target outputs, often using measures such as precision and recall. Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems. One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al. (2007), “[e]valuation studies that ignore the potenti</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Y. Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proceedings of the ACL 2004 Workshop on Text Summarization. acl:W04-1013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>R Dale</author>
</authors>
<title>Evaluation in the context of natural language generation.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<issue>4</issue>
<pages>10--1006</pages>
<contexts>
<context position="1583" citStr="Mellish and Dale, 1998" startWordPosition="241" endWordPosition="244">ree methods for selecting the facial displays of an embodied conversational agent, and the predictions of the metrics are compared with human judgements of the same generated output. The corpus-reproduction metrics show no relationship with the human judgements, while the intrinsic metrics that capture the number and variety of facial displays show a significant correlation with the preferences of the human users. 1 Introduction Evaluating the output of a generation system is known to be difficult: since generation is an openended task, the criteria for success can be difficult to define (cf. Mellish and Dale, 1998). In the current state of the art, there are two main strategies for evaluating the output of a generation system: the behaviour or preferences of humans in response to the output may be measured, or automated measures may be computed on the output itself. A study involving human judges is the most complete and convincing evaluation of generated output. However, such a study is not always practical, as recruiting sufficient subjects can be time-consuming and expensive. So automated metrics are also used in addition to—or instead of—human studies. When automatically evaluating generated output,</context>
</contexts>
<marker>Mellish, Dale, 1998</marker>
<rawString>C. Mellish and R. Dale. 1998. Evaluation in the context of natural language generation. Computer Speech and Language, 12(4):349–373. doi:10.1006/csla. 1998.0106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nielsen</author>
<author>J Levy</author>
</authors>
<title>Measuring usability: preference vs. performance.</title>
<date>1994</date>
<journal>Communications of the ACM,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>10--1145</pages>
<marker>Nielsen, Levy, 1994</marker>
<rawString>J. Nielsen and J. Levy. 1994. Measuring usability: preference vs. performance. Communications of the ACM, 37(4):66–75. doi:10.1145/175276.175282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>02--1040</pages>
<contexts>
<context position="2463" citStr="Papineni et al., 2002" startWordPosition="388" endWordPosition="391"> study involving human judges is the most complete and convincing evaluation of generated output. However, such a study is not always practical, as recruiting sufficient subjects can be time-consuming and expensive. So automated metrics are also used in addition to—or instead of—human studies. When automatically evaluating generated output, the goal is to find metrics that can easily be computed and that can also be shown to correlate with human judgements of quality. Such metrics have been introduced in other fields, including PARADISE (Walker et al., 1997) for spoken dialogue systems, BLEU (Papineni et al., 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. Many automated generation evaluations measure the similarity between the generated output and a corpus of “gold-standard” target outputs, often using measures such as precision and recall. Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems. One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al. (2007</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL 2002. acl: P02-1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paris</author>
<author>D Scott</author>
<author>N Green</author>
<author>K McCoy</author>
</authors>
<title>Desiderata for evaluation of natural language generation.</title>
<date>2007</date>
<booktitle>In Dale and White</booktitle>
<pages>2</pages>
<contexts>
<context position="3064" citStr="Paris et al. (2007)" startWordPosition="482" endWordPosition="485">neni et al., 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. Many automated generation evaluations measure the similarity between the generated output and a corpus of “gold-standard” target outputs, often using measures such as precision and recall. Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems. One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al. (2007), “[e]valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited.” Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety. 1Although Callison-Burch et al. (2006) have recently called into question the utility of BLEU. 95 Automated metrics that take into account other pro</context>
</contexts>
<marker>Paris, Scott, Green, McCoy, 2007</marker>
<rawString>C. Paris, D. Scott, N. Green, K. McCoy, , and D. McDonald. 2007. Desiderata for evaluation of natural language generation. In Dale and White (2007), chapter 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>M Marge</author>
<author>M Singhai</author>
</authors>
<title>Evaluating evaluation methods for generation in the presence of variation.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>341--351</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3251" citStr="Stent et al., 2005" startWordPosition="510" endWordPosition="513"> of “gold-standard” target outputs, often using measures such as precision and recall. Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems. One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al. (2007), “[e]valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited.” Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety. 1Although Callison-Burch et al. (2006) have recently called into question the utility of BLEU. 95 Automated metrics that take into account other properties than strict corpus similarity have also been used to evaluate the output of generation systems. Walker (2005) describes several evaluations that used corpus data in a different wa</context>
</contexts>
<marker>Stent, Marge, Singhai, 2005</marker>
<rawString>A. Stent, M. Marge, and M. Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In Computational Linguistics and Intelligent Text Processing, pages 341–351. Springer. doi:10. 1007/b105772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
</authors>
<title>Can we talk? Methods for evaluation and training of spoken dialogue systems.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>1</issue>
<pages>10--1007</pages>
<contexts>
<context position="3781" citStr="Walker (2005)" startWordPosition="594" endWordPosition="595">ts will be necessarily limited.” Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety. 1Although Callison-Burch et al. (2006) have recently called into question the utility of BLEU. 95 Automated metrics that take into account other properties than strict corpus similarity have also been used to evaluate the output of generation systems. Walker (2005) describes several evaluations that used corpus data in a different way: each of the corpus examples was associated with some reward function (e.g., subjective user evaluation or task success), and machine-learning techniques such as reinforcement learning or boosting were then used to train the output planner. Foster and White (2007) found that automated metrics based on factors other than corpus similarity (e.g., the amount of variation in the output) agreed better with user preferences than did the corpus-similarity scores. Belz and Gatt (2008) compare the predictions of a range of measures</context>
</contexts>
<marker>Walker, 2005</marker>
<rawString>M. A. Walker. 2005. Can we talk? Methods for evaluation and training of spoken dialogue systems. Language Resources and Evaluation, 39(1):65–75. doi: 10.1007/s10579-005-2696-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>D J Litman</author>
<author>C A Kamm</author>
<author>A Abella</author>
</authors>
<title>PARADISE: A general framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL</booktitle>
<pages>97--1035</pages>
<contexts>
<context position="2405" citStr="Walker et al., 1997" startWordPosition="379" endWordPosition="382">tomated measures may be computed on the output itself. A study involving human judges is the most complete and convincing evaluation of generated output. However, such a study is not always practical, as recruiting sufficient subjects can be time-consuming and expensive. So automated metrics are also used in addition to—or instead of—human studies. When automatically evaluating generated output, the goal is to find metrics that can easily be computed and that can also be shown to correlate with human judgements of quality. Such metrics have been introduced in other fields, including PARADISE (Walker et al., 1997) for spoken dialogue systems, BLEU (Papineni et al., 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. Many automated generation evaluations measure the similarity between the generated output and a corpus of “gold-standard” target outputs, often using measures such as precision and recall. Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems. One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range </context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>M. A. Walker, D. J. Litman, C. A. Kamm, and A. Abella. 1997. PARADISE: A general framework for evaluating spoken dialogue agents. In Proceedings of ACL/EACL 1997. acl:P97-1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
</authors>
<title>Efficient realization of coordinate structures in Combinatory Categorial Grammar.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>10--1007</pages>
<contexts>
<context position="6834" citStr="White, 2006" startWordPosition="1075" endWordPosition="1076">dal talking-head interface to a CAD-style system for redesigning bathrooms. The studies focus on the task of selecting appropriate ECA head and eyebrow motions to accompany the turns in which the system describes and compares the options for tiling the room, as those are the parts of the output with the most interesting and varied content. The implementations were based on a corpus of conversational facial displays derived from the behaviour of a single speaker reading approximately 450 scripted sentences generated by the COMIC output-generation system. The OpenCCG syntactic derivation trees (White, 2006) for the sentences form the basis of the corpus. The leaf nodes in these trees correspond to the individual words, while the internal nodes correspond to multi-word constituents. Every node in each tree was initially labelled with all of the applicable contextual features produced by the output planner: the userpreference evaluation of the tile design being described (positive/negative/neutral), the information status (given/new) of each piece of information, and the predicted speech-synthesiser prosody. The annotators then linked each facial display produced by the speaker to the node or span</context>
</contexts>
<marker>White, 2006</marker>
<rawString>M. White. 2006. Efficient realization of coordinate structures in Combinatory Categorial Grammar. Research on Language and Computation, 4(1):39–75. doi: 10.1007/s11168-006-9010-2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>