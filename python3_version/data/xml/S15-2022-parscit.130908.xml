<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.047749">
<title confidence="0.975577">
UQeResearch: Semantic Textual Similarity Quantification
</title>
<author confidence="0.99978">
Hamed Hassanzadeh1, Tudor Groza1,2, Anthony Nguyen3, Jane Hunter1
</author>
<affiliation confidence="0.994859">
1School of ITEE, The University of Queensland, St Lucia, QLD, Australia
2Garvan Institute of Medical Research, Darlinghurst, NSW, Australia
3Australian e-Health Research Centre, CSIRO, Brisbane, QLD, Australia
</affiliation>
<email confidence="0.9502065">
h.hassanzadeh@uq.edu.au, t.groza@garvan.org.au,
anthony.nguyen@csiro.au, jane@itee.uq.edu.au
</email>
<sectionHeader confidence="0.993528" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960631578948">
This paper presents an approach for estimat-
ing the Semantic Textual Similarity of full
English sentences as specified in Shared Task
2 of SemEval-2015. The semantic similarity
of sentence pairs is quantified from three per-
spectives - structural, syntactical, and seman-
tic. The numerical representations of the
derived similarity measures are then applied
to train a regression ensemble. Although none
of these three sets of measures is able to rep-
resent the semantic similarity of two sentences
individually, our experimental results show
that the combination of these features can pre-
cisely assess the semantic similarity of the
sentences. In the English subtask our system’s
best result ranked 35 among 73 system runs
with 0.7189 average Pearson correlation over
five test sets. This was 0.08 correlation points
less than the best submitted run.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907">
Semantic textual similarity (STS) aims to automat-
ically estimate the relatedness of the meaning of
sentences (Agirre et al., 2015). The literature con-
sists of a series of well-established frameworks to
explore a deeper understanding of the semantic
relationship between entities, ranging from onto-
logical reasoning to compositional as well as dis-
tributional semantics (Cohen et al., 2009).
However, automatically estimating the semantic
similarity of full sentences is still a challenging
task.
Our system aims to quantify the similarity of
pairs of sentences by encoding a variety of related-
ness features in a vector of attributes and then pre-
dicting their similarity scores by employing
machine-learning algorithms. Different syntactic,
semantic, and structural similarity measures have
been applied to quantify the similarity of texts. We
have chosen to approach the estimation of similari-
ty as a regression problem. Hence, we use the
quantified similarity of sentence pairs to train a
regressor that can then be applied to predict simi-
larity scores for the unseen pairs. The paper is
structured as follows: Section 2 presents the pro-
posed similarity measures. In Section 3, the regres-
sion models are introduced and the experimental
results are discussed in detail. The conclusions are
summarized in Section 4.
</bodyText>
<sectionHeader confidence="0.97055" genericHeader="method">
2 Similarity Measures
</sectionHeader>
<bodyText confidence="0.999586666666667">
In this section we describe the similarity measures
we have employed to calculate semantic related-
ness of pairs of sentences.
</bodyText>
<subsectionHeader confidence="0.997462">
2.1 Syntactic Similarity Measures
</subsectionHeader>
<bodyText confidence="0.9999195">
Bags of words overlap: A simple measure for
computing the similarity of a sentence pair is the
number of words they have in common. Although
a pair of sentences with the same bag of words (i.e.
unordered list of all words of a sentence) can con-
vey completely different meanings, this measure
along with some structural measures can form an
effective criterion for semantic comparison.
</bodyText>
<subsectionHeader confidence="0.70721">
Bags of lemmatised/stemmed words overlap:
</subsectionHeader>
<bodyText confidence="0.9806371">
The value of this feature is computed using the
same method as above, however, instead of using
bags of words, it uses bags of lemmas / stems.
Set similarity of lemmatised effective words:
There are a number of words in a sentence that do
not play effective roles in modelling the meaning
of that sentence, such as determiners (the, a, an)
and preposition or subordinating conjunctions (in,
on, at). We remove these terms from the bag of
words of a sentence and we call the remaining
</bodyText>
<page confidence="0.985055">
123
</page>
<bodyText confidence="0.968338208333334">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 123–127,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
words the set of effective words. In this measure
we lemmatise the effective words and compare the
resulting sets of lemmas for a pair of sentences.
Jaccard similarity of sets of words/lemmas: A
sentence can be considered as a set of words. To
incorporate this perspective, we calculate the Jac-
card similarity coefficient of a pair of sentences.
Windows of words overlap: We perform a slid-
ing window of different sizes (from window of two
words up to the size of the smaller sentence in a
pair) over a pair of sentences. Afterwards we com-
pute the total number of equal windows of words
of two sentences. Also, we keep the size of the
longest equal window of words that two sentences
share together. Due to varying sizes of sentences
and therefore varying sizes and number of win-
dows, we normalise each of these measures to
reach a comparable value between zero and one.
The same window-based measures can be alterna-
tively be calculated by only considering effective
words in sentences and also, from a grammatical
perspective, by only considering Part of Speech
(POS) tags of the constituent words of sentences.
Ratio of shared skipped bigrams: Skipped bi-
grams are the pairs of words which are created by
combining two words of a sentence that are located
in arbitrary positions. The set of these bigrams can
then be used as a basis for similarity comparison.
We create the skipped bigrams of participating
verbs, nouns, adjectives, and adverbs of a sentence
(we ignore other unimportant terms) and then cal-
culate the intersection of each set of these bigrams
with the corresponding set from the other sentence
in a pair.
Pairwise Sentence Polarity: We investigate the
presence of some lexical elements that act as nega-
tion agent, e.g., not, neither, no, etc. We apply the
NegEx algorithm (Chapmana et al., 2001) to find
the negation in sentences and then we perform
pairwise comparison of the polarity of sentences.
Ratio of Sentence Lengths: The relative length
of two sentences (length of smaller sentence over
the longer one) provides a simple measure of simi-
larity. However, this naïve attribute of a pair can
be useful when combined with other more concep-
tual measures.
</bodyText>
<subsectionHeader confidence="0.999635">
2.2 Structural Similarity Measures
</subsectionHeader>
<bodyText confidence="0.99994385">
Ratio of number of clauses: The meaning of a
sentence can be inferred from the meaning of its
clause(s). Consequently, the equality of the clauses
of a pair of sentences provides another measure for
assessing the relatedness of those sentences. In this
case, the level of equality is calculated by analys-
ing the parse tree of each sentence and finding the
number of clauses that each sentence is composed
of. The ratio of this clause-level equality is then
obtained by dividing the smaller number of clauses
by the larger number of clauses for each pair. Parse
trees were produced with the Stanford Parser
(Klein et al., 2003).
Reduced parse tree overlap: While the previ-
ous measure only considered the shallow size-
based comparison, this measure provides a more
in-depth analysis of the structural similarity. More
concretely, it quantifies the overlap of the parsed
trees for each sentence, composed of only the POS
tags of the effective words.
</bodyText>
<subsectionHeader confidence="0.999367">
2.3 Semantic Similarity Measures
</subsectionHeader>
<bodyText confidence="0.987581">
Role-based word-by-word similarity: In order to
compute this measure, we first split the sentences
into clauses and determine the subject, predicate
and object within each clause. Each of these roles
is then transformed into a bag of lemmatised
words, which is then compared to corresponding
bags of lemmatised words denoting the same role
in the other sentence. The similarity between the
two bags of words is calculated using a mixture of
two well-known semantic similarity measures –
i.e., Lin (1998) and Wu &amp; Palmer (1994), both
having WordNet (Miller, 1995) as background
knowledge. Due to WordNet’s lower coverage of
verbs, for the words in the predicate bags we com-
pute the similarity between words using FrameNet
(Fillmore et al., 2003) and by comparing sets of
corresponding frames of words in each bag.
Semantic similarity of effective words: Given
the sets of effective words of a pair of sentences,
we compute their similarity using the same method
as above, however, without taking into account the
underlying roles – i.e., it is computed in a
sentence-wide manner.
Cosine similarity of Information Content (IC)
vectors: We map the sequence of words in a sen-
tence to a vector of corresponding numeric values.
In order to create this vector we use the notion of
Information Content (IC) (Resnik, 1995). The re-
latedness of a given pair can then be estimated by
</bodyText>
<page confidence="0.993333">
124
</page>
<bodyText confidence="0.989708838709678">
employing a distance measure between the two
vectors, such as the cosine similarity.
Role-based POS tags alignment: For this simi-
larity measure we get the POS tags of each word in
the subject and object phrases of a sentence and
form a sequence of these tags. We then employ
Needleman-Wunsch algorithm (Needleman et al.,
1970) for aligning these sequences of POS tags to
find their similarity ratio.
WordNet/FrameNet based synonym
similarity: Other sets of vocabulary-based similar-
ity measures can be devised by getting all the syn-
onyms of each word of sentences and considering
them in the comparison process. One of these
measures can be calculated by applying WordNet
for obtaining synonyms of words. For this Word-
Net synonymy measure, the corresponding synsets
of all the lemmas of the effective words in sen-
tences are retrieved from WordNet. The sets of
synsets of a pair of sentences are then compared to
each other and the ratio of their similarity is calcu-
lated. Another similar measure can be calculated
using FrameNet as the background knowledge in-
stead of WordNet.
Cosine similarity of the best senses: This
measure uses a WordNet-based word sense disam-
biguation approach to find the best senses of effec-
tive words of a pair. These senses are then used to
form vectors of best senses, which can then be
compared using cosine similarity.
Normalised set similarity for best senses
synsets: Similar to the previous measure, we apply
word sense disambiguation to retrieve the best
senses for all words of the sentence, and subse-
quently create a set of synsets which can be com-
pared to the corresponding set of synsets extracted
from the other sentence.
Normalised set similarity of the best senses
skipped bigrams: We create a set of skipped bi-
grams of best senses of words instead of the
skipped bigrams of words of a sentence and then
calculate each pair’s sets similarity.
Similarity of sets of associated terms: Our last
two sets of features make use of vector space mod-
els, using Wikipedia English articles as the back-
ground corpus and Hyperspace Analogue to
Language (HAL) model to produce term vectors
(Lund et al., 1996) by employing the Seman-
ticVector library (Widdows et al., 2008). The asso-
ciated terms for words of a sentence form a set that
can be compared with a corresponding set of an-
other sentence – for example, by calculating their
intersection. The resulting value is normalised by
size of the smallest set.
Cosine similarity of matrices of associated
terms vectors: For this last feature, we use the
numerical representation (vector) of each term,
retrieved from the distributional model, to form a
matrix of associated terms vectors for a sentence.
To enhance the effectiveness of this similarity
measure, only vectors of effective words of a sen-
tence are used to build the matrix.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.99997925">
In this section, the results from applying our sys-
tem to STS 2015 (Task 2) are presented. Before
discussing the results, we firstly describe the ex-
perimental setup and training process.
</bodyText>
<subsectionHeader confidence="0.98427">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999991882352941">
All the data released in STS 2012, 2013, and 2014
was permitted to be used to develop and train the
systems. All the data sets consist of pairs of sen-
tences along with their human annotated similarity
scores. The similarity scores ranged from 0 to 5,
with 0 representing completely dissimilar pairs and
5 representing perfect similarity (or equality). In
order to evaluate the English STS systems, five test
sets were provided. Although the test data in total
consists of 8500 pairs, a subset of the instances of
each test set was sampled and used for the final
official evaluations by the organizers. The official
measurement criterion for evaluation is the Pearson
correlation. It should be mentioned that prior to
computing the measures the punctuations were
removed from sentences to avoid naïve token-level
matching of them in some similarity measures.
</bodyText>
<subsectionHeader confidence="0.99599">
3.2 Experiments Over Training Data
</subsectionHeader>
<bodyText confidence="0.9999494">
We first performed a number of experiments over
the training data in order to prepare the final re-
gression system. The training set consists of 10592
annotated pairs, achieved by merging previous
SemEval STS data sets. We approached the seman-
tic similarity estimation as a regression problem.
Hence, we investigated different regression algo-
rithms and Table 1 lists their evaluation results.
The WEKA implementations of these algorithms
have been used in our system (Hall et al., 2009).
</bodyText>
<page confidence="0.996042">
125
</page>
<table confidence="0.999429416666667">
Algorithm Pearson Corre- Root mean
lation squared error
Regression Algorithms
RepTree 0.6747 1.1207
K* 0.6968 1.1497
Linear Regression 0.6809 1.1088
Regression By Classification
Regression by Ran- 0.7745 0.964
dom Forest
Regression by KNN 0.7139 1.0651
Regression Ensemble
Ensemble 0.7813 0.9484
</table>
<tableCaption confidence="0.991454">
Table 1: Experiments on training data (5-fold cross
validation).
</tableCaption>
<bodyText confidence="0.99979519047619">
The first part of Table 1 shows the results
achieved by selected regression approaches.
Among these algorithms, K* achieved the best
Pearson correlation. In regression by classification,
the continuous similarity scores are discretised to
nominal values. Then, a classifier was used to cat-
egorize instances into the resultant nominal clas-
ses. In our experiments, the continuous range of 0
to 5 scores is discretised into 10 bins. The best re-
sults have been achieved by applying Random
Forest as the base classifier. Finally, the ensemble
of regressors is composed of three meta-regressors:
bagging, random SubSpace, and regression by dis-
cretisation. Regression by discretisation follows
precisely the same methodology as above. The
bagging strategy uses RepTree as its first level re-
gressor, while the random SubSpace employs the
K* algorithm. The final outputs of the ensemble
are the average of the prediction values from all of
the regressors. This ensemble gained the best cor-
relation amongst all of the models.
</bodyText>
<subsectionHeader confidence="0.991086">
3.3 Results Over Test Data and Discussions
</subsectionHeader>
<bodyText confidence="0.999210823529412">
We submitted three different runs to the English
STS 2015 Task 2. The same regression ensemble
has been applied to all three runs. The main differ-
ence between them is related to the data that was
used for training. The data used to train the run]
system were STS 2012 train and test sets, STS
2013 test set, and STS 2014 test set. In the second
system (run2), we used all the run1 data as well as
one additional data set which was the training set
of the SICK corpus (Marelli et al., 2014). It was
introduced in SemEval-2014 Task 1. Contrary to
STS corpora, the similarity scores from the SICK
corpus ranged from 1 to 5 (instead of 0 to 5). We
gave a unique numerical ID to each pair in the data
sets, which were then kept in the feature vectors as
well. In run3, exactly the same data was used as
run] but without the IDs in the feature vectors.
</bodyText>
<table confidence="0.999626625">
run1 run2 run3
answers-forums 0.5923 0.6132 0.6188
answers-students 0.6876 0.6882 0.6757
belief 0.5904 0.6229 0.7178
headlines 0.7521 0.7602 0.7549
images 0.7817 0.7855 0.7769
Means 0.7032 0.7130 0.7189
Rank 40 37 35
</table>
<tableCaption confidence="0.999726">
Table 2: Our systems’ results over test sets.
</tableCaption>
<bodyText confidence="0.994111533333333">
Table 2 lists the results of our system runs. It can
be observed that the third run achieved better over-
all correlations compared with the other two. By
applying the additional data set (i.e. training set of
the SICK corpus) the average correlation slightly
improved (i.e. in run2). However, as previously
mentioned, the difference in scoring the semantic
similarities (0-5 vs. 1-5) caused the regressor mod-
el to fail to encode the scores properly (especially
for lower similarity scores). In addition, as a side
experiment, but contrary to the positive experience
gained from SemEval-2014 semantic relatedness
Task, the unique numerical ID had a negative im-
pact over the outcome of the system (comparing
run1’s results – with IDs, to run3’s – without IDs).
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999996166666667">
This paper describes the system we submitted to
SemEval-2015 Task 2: STS in order to estimate
semantic similarity of full English sentences. We
approached the task as a regression problem. An
ensemble of regressors as well as a variety of simi-
larity measures was proposed. These measures
(that compared syntactic, semantic, and structural
aspects) were extracted from pairs of sentences.
Our system’s best result ranked 35 among 73 sub-
mitted runs with 0.7189 average Pearson correla-
tions over five test sets. This was 0.08 correlation
points less than the best submitted run.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982642">
This research is funded by the Australian Research
Council (ARC) Discovery Early Career Researcher
Award (DECRA) -- DE120100508. It is also par-
tially supported by CSIRO Postgraduate Student-
ship.
</bodyText>
<page confidence="0.997913">
126
</page>
<sectionHeader confidence="0.989765" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999466782608696">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Iftigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. 2015. SemEval-2015 Task 2: Semantic
Textual Similarity, English, Spanish and Pilot on
Interpretability. In Proceedings of the 9th
International Workshop on Semantic Evaluation
(SemEval 2015), Denver, CO.
Wendy W. Chapmana, Will Bridewellb, Paul Hanburya,
Gregory F. Coopera, and Bruce G. Buchanan. 2001.
A simple algorithm for identifying negated findings
and diseases in discharge summaries. Journal of
Biomedical Informatics, 34(5), 301-310.
Trevor Cohen and Dominic Widdows. 2009. Empirical
distributional semantics: Methods and biomedical
applications. Journal of Biomedical Informatics,
42(2), 390-405.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16(3), 235-250.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An
Update. SIGKDD Explorations, 11(1).
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the
Conference 41st Annual Meeting of the Association
for Computational Linguistics, pages 423-430.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In Proceedings of the 15th
International Conference on Machine Learning,
pages 296-304.
Kevin Lund and Curt Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods Instruments
&amp; Computers, 28(2), 203-208.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto
Zamparelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of the Ninth International Conference
on Language Re-sources and Evaluation (LREC-
2014), Reykjavik, Iceland.
George A. Miller. 1995. Wordnet - a Lexical Database
for English. Communications of the ACM, 38(11),
39-41.
Saul B. Needleman and Christian D. Wunsch. 1970. A
general method applicable to the search for
similarities in the amino acid sequence of two
proteins Journal of Molecular Biology, 48(3), 443 -
453.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence, pages 448-
453.
Dominic Widdows and Kathleen Ferraro. 2008.
Semantic Vectors: A Scalable Open Source Package
and Online Technology Management Application. In
Sixth International Conference on Language
Resources and Evaluation, Lrec 2008, pages 1183-
1190.
Zhibiao Wu and Martha Palmer. 1994. Verbs Semantics
and Lexical Selection. In Proceedings of the 32nd
Annual Meeting on Association for Computational
Linguistics, pages 133-138, Las Cruces, New
Mexico.
</reference>
<page confidence="0.997298">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.352750">
<title confidence="0.999894">UQeResearch: Semantic Textual Similarity Quantification</title>
<author confidence="0.999775">Tudor Anthony Jane</author>
<affiliation confidence="0.775144333333333">of ITEE, The University of Queensland, St Lucia, QLD, Institute of Medical Research, Darlinghurst, NSW, e-Health Research Centre, CSIRO, Brisbane, QLD,</affiliation>
<email confidence="0.910711">h.hassanzadeh@uq.edu.au,anthony.nguyen@csiro.au,jane@itee.uq.edu.au</email>
<abstract confidence="0.9996315">This paper presents an approach for estimating the Semantic Textual Similarity of full English sentences as specified in Shared Task 2 of SemEval-2015. The semantic similarity of sentence pairs is quantified from three perspectives structural, syntactical, and semantic. The numerical representations of the derived similarity measures are then applied to train a regression ensemble. Although none of these three sets of measures is able to represent the semantic similarity of two sentences individually, our experimental results show that the combination of these features can precisely assess the semantic similarity of the sentences. In the English subtask our system’s best result ranked 35 among 73 system runs with 0.7189 average Pearson correlation over five test sets. This was 0.08 correlation points less than the best submitted run.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Iftigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="1427" citStr="Agirre et al., 2015" startWordPosition="195" endWordPosition="198">ion ensemble. Although none of these three sets of measures is able to represent the semantic similarity of two sentences individually, our experimental results show that the combination of these features can precisely assess the semantic similarity of the sentences. In the English subtask our system’s best result ranked 35 among 73 system runs with 0.7189 average Pearson correlation over five test sets. This was 0.08 correlation points less than the best submitted run. 1 Introduction Semantic textual similarity (STS) aims to automatically estimate the relatedness of the meaning of sentences (Agirre et al., 2015). The literature consists of a series of well-established frameworks to explore a deeper understanding of the semantic relationship between entities, ranging from ontological reasoning to compositional as well as distributional semantics (Cohen et al., 2009). However, automatically estimating the semantic similarity of full sentences is still a challenging task. Our system aims to quantify the similarity of pairs of sentences by encoding a variety of relatedness features in a vector of attributes and then predicting their similarity scores by employing machine-learning algorithms. Different sy</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iftigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy W Chapmana</author>
<author>Will Bridewellb</author>
<author>Paul Hanburya</author>
<author>Gregory F Coopera</author>
<author>Bruce G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>34</volume>
<issue>5</issue>
<pages>301--310</pages>
<contexts>
<context position="5703" citStr="Chapmana et al., 2001" startWordPosition="893" endWordPosition="896">are created by combining two words of a sentence that are located in arbitrary positions. The set of these bigrams can then be used as a basis for similarity comparison. We create the skipped bigrams of participating verbs, nouns, adjectives, and adverbs of a sentence (we ignore other unimportant terms) and then calculate the intersection of each set of these bigrams with the corresponding set from the other sentence in a pair. Pairwise Sentence Polarity: We investigate the presence of some lexical elements that act as negation agent, e.g., not, neither, no, etc. We apply the NegEx algorithm (Chapmana et al., 2001) to find the negation in sentences and then we perform pairwise comparison of the polarity of sentences. Ratio of Sentence Lengths: The relative length of two sentences (length of smaller sentence over the longer one) provides a simple measure of similarity. However, this naïve attribute of a pair can be useful when combined with other more conceptual measures. 2.2 Structural Similarity Measures Ratio of number of clauses: The meaning of a sentence can be inferred from the meaning of its clause(s). Consequently, the equality of the clauses of a pair of sentences provides another measure for as</context>
</contexts>
<marker>Chapmana, Bridewellb, Hanburya, Coopera, Buchanan, 2001</marker>
<rawString>Wendy W. Chapmana, Will Bridewellb, Paul Hanburya, Gregory F. Coopera, and Bruce G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of Biomedical Informatics, 34(5), 301-310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohen</author>
<author>Dominic Widdows</author>
</authors>
<title>Empirical distributional semantics: Methods and biomedical applications.</title>
<date>2009</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>42</volume>
<issue>2</issue>
<pages>390--405</pages>
<marker>Cohen, Widdows, 2009</marker>
<rawString>Trevor Cohen and Dominic Widdows. 2009. Empirical distributional semantics: Methods and biomedical applications. Journal of Biomedical Informatics, 42(2), 390-405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<pages>235--250</pages>
<contexts>
<context position="7825" citStr="Fillmore et al., 2003" startWordPosition="1238" endWordPosition="1241">ine the subject, predicate and object within each clause. Each of these roles is then transformed into a bag of lemmatised words, which is then compared to corresponding bags of lemmatised words denoting the same role in the other sentence. The similarity between the two bags of words is calculated using a mixture of two well-known semantic similarity measures – i.e., Lin (1998) and Wu &amp; Palmer (1994), both having WordNet (Miller, 1995) as background knowledge. Due to WordNet’s lower coverage of verbs, for the words in the predicate bags we compute the similarity between words using FrameNet (Fillmore et al., 2003) and by comparing sets of corresponding frames of words in each bag. Semantic similarity of effective words: Given the sets of effective words of a pair of sentences, we compute their similarity using the same method as above, however, without taking into account the underlying roles – i.e., it is computed in a sentence-wide manner. Cosine similarity of Information Content (IC) vectors: We map the sequence of words in a sentence to a vector of corresponding numeric values. In order to create this vector we use the notion of Information Content (IC) (Resnik, 1995). The relatedness of a given pa</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles J. Fillmore, Christopher R. Johnson, and Miriam R.L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16(3), 235-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12865" citStr="Hall et al., 2009" startWordPosition="2075" endWordPosition="2078">e removed from sentences to avoid naïve token-level matching of them in some similarity measures. 3.2 Experiments Over Training Data We first performed a number of experiments over the training data in order to prepare the final regression system. The training set consists of 10592 annotated pairs, achieved by merging previous SemEval STS data sets. We approached the semantic similarity estimation as a regression problem. Hence, we investigated different regression algorithms and Table 1 lists their evaluation results. The WEKA implementations of these algorithms have been used in our system (Hall et al., 2009). 125 Algorithm Pearson Corre- Root mean lation squared error Regression Algorithms RepTree 0.6747 1.1207 K* 0.6968 1.1497 Linear Regression 0.6809 1.1088 Regression By Classification Regression by Ran- 0.7745 0.964 dom Forest Regression by KNN 0.7139 1.0651 Regression Ensemble Ensemble 0.7813 0.9484 Table 1: Experiments on training data (5-fold cross validation). The first part of Table 1 shows the results achieved by selected regression approaches. Among these algorithms, K* achieved the best Pearson correlation. In regression by classification, the continuous similarity scores are discretis</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the Conference 41st Annual Meeting of the Association for Computational Linguistics, pages 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="7584" citStr="Lin (1998)" startWordPosition="1200" endWordPosition="1201"> for each sentence, composed of only the POS tags of the effective words. 2.3 Semantic Similarity Measures Role-based word-by-word similarity: In order to compute this measure, we first split the sentences into clauses and determine the subject, predicate and object within each clause. Each of these roles is then transformed into a bag of lemmatised words, which is then compared to corresponding bags of lemmatised words denoting the same role in the other sentence. The similarity between the two bags of words is calculated using a mixture of two well-known semantic similarity measures – i.e., Lin (1998) and Wu &amp; Palmer (1994), both having WordNet (Miller, 1995) as background knowledge. Due to WordNet’s lower coverage of verbs, for the words in the predicate bags we compute the similarity between words using FrameNet (Fillmore et al., 2003) and by comparing sets of corresponding frames of words in each bag. Semantic similarity of effective words: Given the sets of effective words of a pair of sentences, we compute their similarity using the same method as above, however, without taking into account the underlying roles – i.e., it is computed in a sentence-wide manner. Cosine similarity of Inf</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In Proceedings of the 15th International Conference on Machine Learning, pages 296-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing highdimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods Instruments &amp; Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>203--208</pages>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing highdimensional semantic spaces from lexical cooccurrence. Behavior Research Methods Instruments &amp; Computers, 28(2), 203-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A SICK cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Re-sources and Evaluation (LREC2014),</booktitle>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="14780" citStr="Marelli et al., 2014" startWordPosition="2385" endWordPosition="2388"> of the regressors. This ensemble gained the best correlation amongst all of the models. 3.3 Results Over Test Data and Discussions We submitted three different runs to the English STS 2015 Task 2. The same regression ensemble has been applied to all three runs. The main difference between them is related to the data that was used for training. The data used to train the run] system were STS 2012 train and test sets, STS 2013 test set, and STS 2014 test set. In the second system (run2), we used all the run1 data as well as one additional data set which was the training set of the SICK corpus (Marelli et al., 2014). It was introduced in SemEval-2014 Task 1. Contrary to STS corpora, the similarity scores from the SICK corpus ranged from 1 to 5 (instead of 0 to 5). We gave a unique numerical ID to each pair in the data sets, which were then kept in the feature vectors as well. In run3, exactly the same data was used as run] but without the IDs in the feature vectors. run1 run2 run3 answers-forums 0.5923 0.6132 0.6188 answers-students 0.6876 0.6882 0.6757 belief 0.5904 0.6229 0.7178 headlines 0.7521 0.7602 0.7549 images 0.7817 0.7855 0.7769 Means 0.7032 0.7130 0.7189 Rank 40 37 35 Table 2: Our systems’ res</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of the Ninth International Conference on Language Re-sources and Evaluation (LREC2014), Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet - a Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="7643" citStr="Miller, 1995" startWordPosition="1210" endWordPosition="1211"> effective words. 2.3 Semantic Similarity Measures Role-based word-by-word similarity: In order to compute this measure, we first split the sentences into clauses and determine the subject, predicate and object within each clause. Each of these roles is then transformed into a bag of lemmatised words, which is then compared to corresponding bags of lemmatised words denoting the same role in the other sentence. The similarity between the two bags of words is calculated using a mixture of two well-known semantic similarity measures – i.e., Lin (1998) and Wu &amp; Palmer (1994), both having WordNet (Miller, 1995) as background knowledge. Due to WordNet’s lower coverage of verbs, for the words in the predicate bags we compute the similarity between words using FrameNet (Fillmore et al., 2003) and by comparing sets of corresponding frames of words in each bag. Semantic similarity of effective words: Given the sets of effective words of a pair of sentences, we compute their similarity using the same method as above, however, without taking into account the underlying roles – i.e., it is computed in a sentence-wide manner. Cosine similarity of Information Content (IC) vectors: We map the sequence of words</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet - a Lexical Database for English. Communications of the ACM, 38(11), 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul B Needleman</author>
<author>Christian D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins</title>
<date>1970</date>
<journal>Journal of Molecular Biology,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>443--453</pages>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>Saul B. Needleman and Christian D. Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins Journal of Molecular Biology, 48(3), 443 -453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="8394" citStr="Resnik, 1995" startWordPosition="1335" endWordPosition="1336">words using FrameNet (Fillmore et al., 2003) and by comparing sets of corresponding frames of words in each bag. Semantic similarity of effective words: Given the sets of effective words of a pair of sentences, we compute their similarity using the same method as above, however, without taking into account the underlying roles – i.e., it is computed in a sentence-wide manner. Cosine similarity of Information Content (IC) vectors: We map the sequence of words in a sentence to a vector of corresponding numeric values. In order to create this vector we use the notion of Information Content (IC) (Resnik, 1995). The relatedness of a given pair can then be estimated by 124 employing a distance measure between the two vectors, such as the cosine similarity. Role-based POS tags alignment: For this similarity measure we get the POS tags of each word in the subject and object phrases of a sentence and form a sequence of these tags. We then employ Needleman-Wunsch algorithm (Needleman et al., 1970) for aligning these sequences of POS tags to find their similarity ratio. WordNet/FrameNet based synonym similarity: Other sets of vocabulary-based similarity measures can be devised by getting all the synonyms </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pages 448-453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Kathleen Ferraro</author>
</authors>
<title>Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application.</title>
<date>2008</date>
<booktitle>In Sixth International Conference on Language Resources and Evaluation, Lrec</booktitle>
<pages>1183--1190</pages>
<marker>Widdows, Ferraro, 2008</marker>
<rawString>Dominic Widdows and Kathleen Ferraro. 2008. Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application. In Sixth International Conference on Language Resources and Evaluation, Lrec 2008, pages 1183-1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs Semantics and Lexical Selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="7607" citStr="Wu &amp; Palmer (1994)" startWordPosition="1203" endWordPosition="1206">nce, composed of only the POS tags of the effective words. 2.3 Semantic Similarity Measures Role-based word-by-word similarity: In order to compute this measure, we first split the sentences into clauses and determine the subject, predicate and object within each clause. Each of these roles is then transformed into a bag of lemmatised words, which is then compared to corresponding bags of lemmatised words denoting the same role in the other sentence. The similarity between the two bags of words is calculated using a mixture of two well-known semantic similarity measures – i.e., Lin (1998) and Wu &amp; Palmer (1994), both having WordNet (Miller, 1995) as background knowledge. Due to WordNet’s lower coverage of verbs, for the words in the predicate bags we compute the similarity between words using FrameNet (Fillmore et al., 2003) and by comparing sets of corresponding frames of words in each bag. Semantic similarity of effective words: Given the sets of effective words of a pair of sentences, we compute their similarity using the same method as above, however, without taking into account the underlying roles – i.e., it is computed in a sentence-wide manner. Cosine similarity of Information Content (IC) v</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs Semantics and Lexical Selection. In Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics, pages 133-138, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>