<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.889795">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<title confidence="0.732407666666667">
Abstracts of Current Literature
Toward a Detailed Model of Processing for
Language Describing the Physical World
</title>
<author confidence="0.921352">
David L. Waltz
</author>
<affiliation confidence="0.7185895">
Coordinated Science Laboratory
University of Illinois
1101 West Springfield Avenue
Urbana, Illinois 61801
</affiliation>
<subsubsectionHeader confidence="0.768431">
Proc. 7th IJCAI, vol. 1, August 1981, 1-6.
</subsubsectionHeader>
<bodyText confidence="0.998887958333333">
This paper explores the problem of judging whether
or not an English sentence could correspond to a real
world situation or event which is literally, physically
plausible, and the related problem of representing the
different possible physical situations. The judgment of
plausibility can be made at a high level by checking
semantic marker restrictions on verb case frame con-
stituents. Often, however, plausibility judgement can
only be based on the results of an attempt to construct
(imagine) a scene that corresponds to the sentence,
and which does not violate &amp;quot;common sense&amp;quot; (i.e. rele-
vant physical laws and expected, stereotyped behav-
ior). Methods are presented for constructing repre-
sentations for different scenes which could correspond
to a sentence. These methods incorporate (1)
&amp;quot;subscripts&amp;quot; (sequences of scenes which comprise an
event, with attached preconditions and postconditions)
to express different verb senses; (2) object representa-
tions which express properties such as shape, size,
weight, strength, and behavior under common condi-
tions; (3) physical laws, encoded as constraints on
behavior; (4) representation of context; and (5) robot
problem solving-like methods to fit all this material
together.
</bodyText>
<subsectionHeader confidence="0.7433628">
Language Comprehension in a Problem Solver
Douglas Wong
Department of Computer Science
Brown University
Providence, Rhode Island 02912
</subsectionHeader>
<subsubsectionHeader confidence="0.783528">
Proc. 7th IJCAI, vol. 1, August 1981, 7-12.
</subsubsectionHeader>
<bodyText confidence="0.99981175">
This paper describes BRUIN, a unified Al system
that can perform both problem-solving and language
comprehension tasks. Included in the system is a
frame-based knowledge-representation language called
FRAIL, a problem solving component called NASL
(which is based on McDermott&apos;s problem-solving lan-
guage of the same name), and a context-recognition
component currently known as PRAGMATICS. The
intent of this paper is to give the flavor of how the
context recognizer PRAGMATICS works and what it
can do. Examples are drawn from the inventory-
control, restaurant and blocks-world domains.
</bodyText>
<note confidence="0.7442505">
Cancelled Due to Lack of Interest
Michael Lebowitz
</note>
<affiliation confidence="0.7002805">
Department of Computer Science
Columbia University
</affiliation>
<note confidence="0.6298305">
406 Mudd Building
New York, New York 10027
</note>
<subsubsectionHeader confidence="0.648627">
Proc. 7th IJCAI, vol. 1, August 1981, 13-15.
</subsubsectionHeader>
<bodyText confidence="0.9999338">
The parts of a typical piece of text vary greatly in
interest. Presented in this paper are three ways a con-
cept can fail to be interesting — it can be irrelevant,
reconstructible, or overshadowed. The uses of interest
in understanding are also discussed.
</bodyText>
<subsectionHeader confidence="0.6138715">
Story Generation after TALE-SPIN
Natalie Dehn
</subsectionHeader>
<affiliation confidence="0.7859215">
Department of Computer Science
Yale University
</affiliation>
<address confidence="0.691791">
New Haven, Connecticut 06520
</address>
<subsubsectionHeader confidence="0.800905">
Proc. 7th IJCAI, vol. 1, August 1981, 16-18.
</subsubsectionHeader>
<bodyText confidence="0.999969833333333">
TALE-SPIN, the last major AI attempt at story
generation, approached the problem of making up
stories primarily from the perspective of an impartial
world simulator. AUTHOR is a program (under devel-
opment) which generates stories as a creative reasoner
in pursuit of her own narrative goals. It is thus in-
tended to simulate an author&apos;s mind as she makes up a
story, rather than the world as things happen in it.
The four major forces driving the story generation
process, according to the AUTHOR model, are author
intentionality, conceptual reformation, reminding, and
the opportunity enhancement metagoal.
</bodyText>
<sectionHeader confidence="0.71401" genericHeader="abstract">
Modeling Informal Debates
</sectionHeader>
<subsectionHeader confidence="0.72065">
Rachel Reichman
</subsectionHeader>
<note confidence="0.362200333333333">
Elect. Engrg. and Comp. Sci., C-014
University of California
La Jolla, California 92093
</note>
<subsubsectionHeader confidence="0.718564">
Proc. 7th IJCAI, vol. 1, August 1981, 19-24.
</subsubsectionHeader>
<bodyText confidence="0.999531857142857">
Many rules of formal debate are well documented,
are of common knowledge, and are &amp;quot;looked-up&amp;quot; in
preparation for planned debating. Informal debates,
on the other hand, are highly dynamic, are complex,
and are spontaneously generated with no prior rule-
book preparation. They too, however, are rule-
governed. In this paper I present an abstract process
model capable of modeling &amp;quot;well-formed&amp;quot; argument
structures that occur in ordinary conversations. The
formalization rests on a general theoretical framework
for discourse engagement encapsulated in a discourse
ATN grammar. A major feature of the system is its
segmentation of discourse utterances into functionally
related context spaces.
</bodyText>
<page confidence="0.542312">
268 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
</page>
<note confidence="0.980509">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<title confidence="0.6795965">
A Knowledge-Based Approach to Language
Processing: A Progress Report
</title>
<author confidence="0.912785">
Robert Wilensky
</author>
<affiliation confidence="0.884480666666667">
Department of EECS
University of California
Berkeley, California 94720
</affiliation>
<subsubsectionHeader confidence="0.797222">
Proc. 7th IJCAI, vol. 1, August 1981, 25-30.
</subsubsectionHeader>
<bodyText confidence="0.993552894736842">
We present a model of natural language use meant
to encompass the language-specific aspects of under-
standing and production. The model is motivated by
the pervasiveness of non-generative language, by the
desirability of a language analyzer and a language
production mechanism to share their knowledge, and
by the advantages of knowledge engineering features
such as ease of extension and modification.
This model has been used as the basis for PHRAN,
a language analyzer, and PHRED, a language prod-
uction mechanism. We have implemented both these
systems using a common knowledge base; we have
produced versions of PHRAN that understand Spanish
and Chinese by only changing the knowledge base and
not modifying the program; and we have implemented
PHRAN using the query language of a conventional
relational data base system, and compared the per-
formance of this system to a conventional LISP imple-
mentation.
</bodyText>
<title confidence="0.5213475">
The Need for Referent Identification
as a Planned Action
</title>
<author confidence="0.890244">
Philip R. Cohen
</author>
<affiliation confidence="0.950674333333333">
Department of Computer Science
Oregon State University
Corvallis, Oregon 97331
</affiliation>
<subsubsectionHeader confidence="0.839219">
Proc. 7th IJCAI, vol. 1, August 1981, 31-36.
</subsubsectionHeader>
<bodyText confidence="0.999916285714286">
The paper presents evidence that speakers often
attempt to get hearers to identify referents as a sepa-
rate step in the speaker&apos;s plan. Many of the communi-
cative acts performed in service of such referent iden-
tification steps can be analyzed by extending a plan-
based theory of communication for task-oriented dia-
logues to include an action representing a hearer&apos;s
identifying the referent of a description — an action
that is reasoned about in speakers&apos; and hearers&apos; plans.
The phenomenon of addressing referent identification
as a separate goal is shown to distinguish telephone
from teletype task-oriented dialogues and thus has
implications for the design of speech-understanding
systems.
</bodyText>
<note confidence="0.965968">
Integration, Unification, Reconstruction,
Modification: An Eternal Parsing Braid
Michael G. Dyer
</note>
<affiliation confidence="0.6684235">
Department of Computer Science
Yale University
</affiliation>
<address confidence="0.534365">
New Haven, Connecticut 06520
</address>
<subsubsectionHeader confidence="0.680495">
Proc. 7th IJCAI, vol. 1, August 1981, 37-42.
</subsubsectionHeader>
<bodyText confidence="0.999828266666667">
BORIS is an integrated natural language under-
standing system for narratives. In an integrated sys-
tem, processes of event assimilation, inference, and
episodic memory search occur on a word-by-word
basis as parsing proceeds. &amp;quot;Parsing&amp;quot; here refers to the
task of building a conceptual representation for each
natural language expression. In addition to being inte-
grated, the BORIS parser is also a unified parser. The
same parser is used both at story understanding time
and question answering time. This paper explores
some of the consequences which arise when the same
parser serves both tasks. For instance, one such con-
sequence is that BORIS often knows the answer to a
question before it has completely understood the ques-
tion.
</bodyText>
<subsectionHeader confidence="0.900892">
Design Characteristics of a
Machine Translation System
</subsectionHeader>
<figure confidence="0.5928998">
M. King
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve, SWITZERLAND
</figure>
<figureCaption confidence="0.30316">
Proc. 7th IJCAI, vol. 1, August 1981, 43-46.
</figureCaption>
<bodyText confidence="0.999936">
This paper distinguishes a set of criteria to be met
by a machine translation system (EUROTRA) current-
ly being planned under the sponsorship of the Com-
mission of the European Communities and attempts to
show the effect of meeting those criteria on the overall
system design.
</bodyText>
<subsectionHeader confidence="0.635903333333333">
High Level Memory Structures and
Text Coherence in Translation
C.J. Yang
Department of Computer Science
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.757511">
Proc. 7th IJCAI, vol. 1, August 1981, 47-49.
</subsubsectionHeader>
<bodyText confidence="0.9988711">
Various memory organization schemes have been
proposed in the last five years. Lots of intelligent
computer systems have been experimenting with mem-
ory schemes like scripts, plans, goals, and MOPs in the
domain of text understanding and information retriev-
al. In this paper, the focus is on the problem of trans-
lating sentences that involve lexical items which do not
have equivalent counterparts in the target language.
Examples are drawn from translations between English
and Mandarin Chinese.
</bodyText>
<note confidence="0.929541333333333">
American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 269
The FINITE STRING Newsletter Abstracts of Current Literature
Natural Lang. Dialogue about Moving Objects in
an Automatically Analyzed Traffic Scene
H. Marburger, B. Neumann, and H-J. Novak
Fachbereich Informatik
Universitaet Hamburg
Schlueterstrasse 70
D-2000 Hamburg 13
</note>
<subsubsectionHeader confidence="0.587866">
Proc. 7th IJCAI, vol. 1, August 1981, 49-51.
</subsubsectionHeader>
<bodyText confidence="0.999929857142857">
This contribution is concerned with natural lan-
guage dialogues about scenes with moving objects.
Two systems are connected: a natural language dia-
logue system originally conceived for static scenes and
an emerging scene analysis system for real-world TV-
frame sequences. The latter produces time dependent
object descriptions which serve as a referential data-
base for inquiries. The time intervals relevant for
answering the questions are determined from domain
specific parameters, the context of the dialogue, the
tense of the verbs and time adverbials. For checking
the correspondence between a verbally specified mot-
ion and a trajectory, predicates are evaluated which
can be deduced from the verb&apos;s case-frame.
</bodyText>
<subsectionHeader confidence="0.974741666666667">
Using Language and Context
in the Analysis of Text
Yigal Arens
Department of EECS
University of California, Berkeley
Berkeley, California 94720
</subsectionHeader>
<subsubsectionHeader confidence="0.808485">
Proc. 7th IJCAI, vol. 1, August 1981, 52-57.
</subsubsectionHeader>
<bodyText confidence="0.999983833333333">
We describe a theory of natural language under-
standing within which we identify two separate com-
ponents, a language centered one and a context centered
one. The former component uses a knowledge base
consisting of pairings of phrases with the concepts
associated with them to determine the meaning of
utterances. The latter component clarifies the mean-
ing found by the first one and makes it more specific
by attempting to reconcile it with the context of the
utterance. We have constructed a program called
PHRAN (PHRasal ANalyzer) which performs the task
of the language centered component.
</bodyText>
<subsectionHeader confidence="0.9276512">
Opportunistic Processing in Arguments
Rod McGuire, Lawrence Birnbaum, Margot Flowers
Department of Computer Science
Yale University
New Haven Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.805865">
Proc. 7th IJCAI, vol. 1, August 1981, 58-60.
</subsubsectionHeader>
<bodyText confidence="0.9999274">
In two previous papers we have proposed a part of
a computational theory of argumentation, including
representations for argument structure and rules for
using those representations in understanding and in
rebutting. One property of the model which we em-
phasized is the way in which argument mechanisms
and inferential memory can each help to direct the
processing of the other. In particular, we presented
examples in which inferential memory can uncover
good rebuttals to an input as a side-effect of the proc-
essing that naturally goes on in trying to understand
that input. When such opportunities for rebuttal are
noticed during understanding, they render unnecessary
the use of argument rules to find a response, since one
has already been discovered.
</bodyText>
<subsectionHeader confidence="0.927487285714286">
Natural Language Interaction with Dynamic
Knowledge Bases: Monitoring as Response
Eric Mays, Sitaram Lanka, Aravind K. Joshi, and
Bonnie L. Webber
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</subsectionHeader>
<subsubsectionHeader confidence="0.85414">
Proc. 7th IJCAI, vol. 1, August 1981, 61-63.
</subsubsectionHeader>
<bodyText confidence="0.999931428571429">
In this communication, we discuss an interesting
aspect of natural language interaction with dynamically
changing knowledge bases — the ability to monitor for
relevant future changes in that knowledge. We also
indicate the status of our current work in this area and
the overall goals of our research on question-
answering and monitoring dynamic knowledge bases.
</bodyText>
<subsectionHeader confidence="0.907172">
Variable-depth Natural Language Understanding
Daniel Kayser
</subsectionHeader>
<table confidence="0.414695777777778">
E.R.A. 452 du CNRS
Laboratoire de Recherche en Informatique
Bat.490 - Campus d&apos;Orsay
91405 Orsay, FRANCE
Daniel Coulon
L.A. 262 du CNRS
Centre de Recherche en lnformatique de Nancy
ENSMIM - Parc de Saurupt
54042 Nancy, FRANCE
</table>
<subsubsectionHeader confidence="0.659471">
Proc. 7th IJCAI, vol. 1, August 1981, 64-66.
</subsubsectionHeader>
<bodyText confidence="0.933008125">
Standard Al representations of knowledge operate
at fixed depth (i.e., the objects manipulated are de-
scribed by an amount of information which remains
constant for every task). Contrary to this approach,
Variable Depth Processing (VDP) uses a progressive
description of objects, tries different strategies accord-
ing to the quality of the result it needs, and continual-
ly controls this quality by means of an evaluation of
the approximations it makes. Contextual Production
Rules are shown to be an effective way to implement
some features of VDP. We are currently developing a
VDP question-answering system which works on texts
concerning a non-technical subject, namely an excerpt
of a general public-oriented encyclopaedia.
270 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
The FINITE STRING Newsletter Abstracts of Current Literature
</bodyText>
<subsectionHeader confidence="0.971825833333333">
Generalizations Based on Explanations
Gerald DeJong
Coordinated Science Laboratory
University of Illinois
1101 West Springfield Avenue
Urbana, Illinois 61801
</subsectionHeader>
<subsubsectionHeader confidence="0.713043">
Proc. 7th IJCAI, vol. 1, August 1981, 67-70.
</subsubsectionHeader>
<bodyText confidence="0.999985611111111">
This paper describes a new project in computer
learning. The phenomenon under study is a kind of
&amp;quot;insight learning&amp;quot; of procedural schemata. The system
described here is designed to grasp some principle
underlying a natural language input. The underlying
principle results in a new schema for the system.
Once acquired, the schema serves the same purpose as
the other schemata in the system: it aids in processing
future natural language inputs.
The process that the system uses is called explana-
tory schema acquisition. The basic idea behind it is
that the causal connections in an understood repre-
sentation of a new input can be used to propose and
propagate constraints on slot fillers. That is, from one
particular instance or situation the system can &amp;quot;reason
out&amp;quot; the general structure underlying that instance.
The system is therefore capable of learning from just
one example.
</bodyText>
<subsectionHeader confidence="0.995737666666667">
Viewing Word Expert Parsing
as Linguistic Theory
Steven Small
Department of Computer Science
Mathematical Sciences Building
University of Rochester
</subsectionHeader>
<bodyText confidence="0.361024">
Rochester, New York 14627
</bodyText>
<subsubsectionHeader confidence="0.713803">
Proc. 7th IJCAI, vol. 1, August 1981, 70-76.
</subsubsectionHeader>
<bodyText confidence="0.98222825">
The Word Expert Parser is a computer program that
analyzes fragments of natural language text in order to
extract their meaning in context. The construction of
the program has led to the development of a linguistic
theory based on notions orthogonal to those tradition-
ally found at the heart of such theories. Word Expert
Parsing explains the understanding of textual frag-
ments containing highly idiosyncratic elements, such as
idioms, collocations, cliches, and colligations, as well
as lexical sequences that contain interesting structural
phenomena. The theory perceives the individual word
of language as the organizing unit for linguistic knowl-
edge, and views understanding as consisting of lexical
interactions among procedural word experts. This pa-
per describes four classes of lexical interaction re-
quired to explain the understanding of sentences in
context, idiosyncratic interaction, linguistic interaction,
discourse interaction, and logical interaction The paper
purposely avoids programming details in order to focus
on Word Expert Parsing as linguistic theory.
</bodyText>
<subsectionHeader confidence="0.801116333333333">
A Plot Understanding System on Reference to
Both Image and Language
Norihiro Abe, Itsuya Soga, and Saburo Tsuji
Department of Control Engineering
Osaka University
Toyonaka, Osaka, JAPAN
</subsectionHeader>
<subsubsectionHeader confidence="0.700396">
Proc. 7th IJCAI, vol. 1, August 1981, 77-84.
</subsubsectionHeader>
<bodyText confidence="0.999887692307692">
A system is described that can understand a plot of
a story on reference to both image and linguistic infor-
mation. As input, a series of line drawings with colors
and narrations in English concerning these drawings
are given to the system. It searches the objects sug-
gested to be in the scene by the narrations, finding
relations among them, making the world model by
using its world knowledge. Its reference to those
drawings makes it easy for the system to analyze com-
plicated structures in the narration sentences, such as
those of prepositions, and guides the process reasoning
about the CD representation using rules and demons.
At the end of this paper, a result on QA is shown.
</bodyText>
<subsectionHeader confidence="0.955813571428571">
Metaphor Interpretation as
Selective Inferencing
Jerry R. Hobbs
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</subsectionHeader>
<subsubsectionHeader confidence="0.738148">
Proc. 7th IJCAI, vol. 1, August 1981, 85-91.
</subsubsectionHeader>
<bodyText confidence="0.999977578947369">
Metaphor pervades natural language discourse.
This paper describes a computational approach to the
interpretation of metaphors. It is based on a natural
language processing system that uses the discourse
problems posed by a text to select the relevant infer-
ences. The problem of interpreting metaphors can
then be translated into the problem of selecting the
relevant inferences to draw from the metaphorical
expression. Thus, a metaphor is frequently given a
correct interpretation as a by-product of the other
things a natural language system has to do. Two ex-
amples of metaphors are given — a spatial metaphor
schema from computer science, and a novel metaphor
— and it is shown how the interpretation problem for
each can be translated into a selective inferencing
problem and solved by the ordinary operations of the
system. This framework sheds light on the analogical
processes that underlie metaphors and begins to ex-
plain the power of metaphor.
</bodyText>
<note confidence="0.3542095">
American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 271
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<title confidence="0.358638">
A Computer Model of Child
</title>
<subsectionHeader confidence="0.960267">
Language Acquisition
Mallory Selfridge
Department of EE and CS
University of Connecticut
Storrs, Connecticut 06268
</subsectionHeader>
<subsubsectionHeader confidence="0.832694">
Proc. 7th IJCAI, vol. 1, August 1981, 92-96.
</subsubsectionHeader>
<bodyText confidence="0.9999424">
Children learn different aspects of language in a
characteristic order, and make characteristic errors
during acquisition. This paper focuses on five specific
data, and explains these data in terms of two hy-
potheses regarding the relationship of comprehension
to generation and the relationship between meaning
and syntax. A computer model, CHILD, is described
which embodies these hypotheses and which manifests
the same five data. CHILD&apos;S performance suggests
that the explanations are plausible.
</bodyText>
<subsectionHeader confidence="0.793599">
A Theory of Language Acquisition Based
on General Learning Principles
John R. Anderson
Department of Psychology
Carnegie-Mellon University
Pittsburgh, Pennsylvania 15213
</subsectionHeader>
<subsubsectionHeader confidence="0.777122">
Proc. 7th IJCAI, vol. 1, August 1981, 97-103.
</subsubsectionHeader>
<bodyText confidence="0.999996611111111">
A simulation model is described for the acquisition
of the control of syntax in language generation. This
model makes use of general learning principles and
general principles of cognition. Language generation
is modeled as a problem-solving process involving
principally the decomposition of a to-be-communicated
semantic structure into a hierarchy of subunits for
generation. The syntax of the language controls this
decomposition. It is shown how a sentence and se-
mantic structure can be compared to infer the decom-
position that led to the sentence. The learning proc-
esses involve generalizing rules to classes of words,
learning by discrimination the various contextual
constraints on a rule application, and a strength proc-
ess which monitors a rule&apos;s history of success and fail-
ure. This system is shown to apply to the learning of
noun declensions in Latin, relative clause constructions
in French, and verb auxiliary structures in English.
</bodyText>
<subsectionHeader confidence="0.972604833333333">
Inductive Learning of Pronunciation Rules
by Hypothesis Testing and Correction
S. Oakey and R.C. Cawthorn
Department of Computer Science
Teesside Polytechnic
Middlesbrough, Cleveland, ENGLAND
</subsectionHeader>
<subsubsectionHeader confidence="0.901614">
Proc. 7th IJCAI, vol. 1, August 1981, 109-114.
</subsubsectionHeader>
<bodyText confidence="0.999964533333333">
This paper describes a system that learns the rules
of pronunciation inductively. It begins with a set of
26 rules for single-letter pronunciation. Individual
words are presented to it, and the system uses its rule
set to hypothesize a pronunciation. This is compared
with a dictionary pronunciation, and if any part of the
pronunciation is incorrect new rules are created to
handle the word as an exception condition.
These rules are checked for similarity with others
already produced, and where suitable a &amp;quot;general&amp;quot; rule
is produced to deal with two or more created rules.
The effect is to produce rules that are more and more
general, and these approach the general pronunciation
rule sets that have been produced manually by other
workers.
</bodyText>
<subsectionHeader confidence="0.8393384">
Summarizing Narratives
Wendy G. Lehnert, John B. Black, and Brian J. Reiser
Department of Computer Science
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.898882">
Proc. 7th IJCAI, vol. 1, August 1981, 184-189.
</subsubsectionHeader>
<bodyText confidence="0.9999601875">
Most research on narrative text summarization has
been conducted within the paradigm of experimental
psychology. But recent language processing research
in artificial intelligence suggests that the predominant
theory of text summarization requires further examina-
tion. Seemingly minor structural modifications of a
story can result in significant alterations of summary
behavior. In this paper, highlights of summary data
from 72 subject are presented and analyzed in terms
of two competing summarization models: (1) the sto-
ry grammar model of psychology, and (2) the plot unit
model developed in artificial intelligence. We show
how selected story grammar predictions compare to
the plot unit predictions for short term summarization
and then identify two complicating factors that have a
major impact on summarization behavior.
</bodyText>
<subsectionHeader confidence="0.950602818181818">
Text Plans and World Plans
in Natural Discourse
Jerry R. Hobbs
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
Michael Agar
Department of Anthropology
University of Maryland
College Park, Maryland 20742
</subsectionHeader>
<subsubsectionHeader confidence="0.851231">
Proc. 7th IJCAI, vol. 1, August 1981, 190-196.
</subsubsectionHeader>
<bodyText confidence="0.900240818181818">
Discourse is both about the world and an accom-
plishment in the world. This fact has led to two ap-
proaches to the study of discourse in artificial intelli-
gence: one investigating &amp;quot;text plans,&amp;quot; the other,
&amp;quot;world plans.&amp;quot; By analyzing a fragment of a narrative
in which both kinds of plans figure importantly, we
explore the relationship between the two kinds of
plans, looking toward a synthesis of the two ap-
proaches.
272 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
The FINITE STRING Newsletter Abstracts of Current Literature
</bodyText>
<subsectionHeader confidence="0.6853815">
Recognizing Intended Meaning
and Speakers&apos; Plans
Candace L. Sidner and David J. Israel
Bolt Beranek and Newman Inc.
10 Moulton Street
Cambridge, Massachusetts 02238
</subsectionHeader>
<subsubsectionHeader confidence="0.821815">
Proc. 7th IJCAI, vol. 1, August 1981, 203-208.
</subsubsectionHeader>
<bodyText confidence="0.999916">
Human conversational participants depend upon the
ability of their partners to recognize their intentions,
so that those partners may respond appropriately. In
such interactions, the speaker can encode his inten-
tions that the hearer act in a variety of sentence types.
Instead of telling the hearer what to do, the speaker
may just state his goals, and expect a response that
meets these goals. This paper presents a new model
for recognizing the speaker&apos;s intended meaning in
determining a response. We show that this recognition
makes use of the speaker&apos;s plan, his beliefs about the
domain and about the hearer&apos;s relevant capacities.
</bodyText>
<subsectionHeader confidence="0.798517833333333">
Character Tracking and the Understanding
of Narratives
Brian J. Reiser
Cognitive Science Program
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.763248">
Proc. 7th IJCAI, vol. 1, August 1981, 209-212.
</subsubsectionHeader>
<bodyText confidence="0.999978357142857">
Recent work on the understanding of natural lan-
guage narratives has emphasized representations com-
posed of goals, plans, and their outcomes. A problem
that has received little attention however is the influ-
ence of perspective in understanding a narrative. Per-
spective may operate on many levels. Understanding
proceeds as a focused tracking of the fate of a
character&apos;s goals. When attention is focused on a
character in a narrative, each new input is then evalu-
ated from that character&apos;s perspective. Processing the
story from one character&apos;s perspective is one way that
the inferencing process might be constrained. Less
relevant inferences need never be made or integrated
into the representation.
</bodyText>
<subsectionHeader confidence="0.969294571428571">
Representing Implicit and Explicit Time
Relations in Narrative
Lynette Hirschman and Guy Story
Linguistic String Project
New York University
251 Mercer Street
New York, New York 10012.
</subsectionHeader>
<subsubsectionHeader confidence="0.861281">
Proc. 7th IJCAI, vol. 1, August 1981, 289-295.
</subsubsectionHeader>
<bodyText confidence="0.999993411764706">
This paper describes a representation for time rela-
tions in narrative. The time relations are based on
both explicit sources of time information (e.g., adver-
bial expressions 4:n tense) and implicit sources, such as
multiple reference to a single event, narrative time
progression and earlier events implied by change of
state words. Natural language processing is used to
analyze the input text into a set of subject-verb-object
units, connected by binary connectives; these units
correspond to the events of the narrative. With each
event is associated a time in relation to another event,
adjusted by an optional time quantity. These time
relations have a natural representation as a directed
graph whose nodes are time points and whose edges
are time intervals. The algorithm for extracting the
time relations from a text is illustrated for an excerpt
from a hospital discharge summary.
</bodyText>
<subsectionHeader confidence="0.973264333333333">
The Nature of Generalization in Understanding
Michael Lebowitz
Department of Computer Science
Columbia University
406 Mudd Building
New York, New York 10027
</subsectionHeader>
<subsubsectionHeader confidence="0.910021">
Proc. 7th IJCAI, vol. 1, August 1981, 348-353.
</subsubsectionHeader>
<bodyText confidence="0.9999092">
True understanding of natural language text re-
quires the inclusion of generalization and long-term
memory. This paper describes the generalization proc-
ess and memory used in the Integrated Partial Parser
(IPP), a computer program that reads and remembers
news stories. The need for generalization and
generalization-based memory as an integral part of
understanding natural language text is illustrated with
examples from IPP. In addition, the nature of general-
ization is discussed.
</bodyText>
<subsectionHeader confidence="0.874693666666667">
Directing and Re-directing Inference Pursuit:
Extra-textual Influences on Text Interpretation
Richard H. Granger Jr.
Department of Information and Computer Science
University of California
Irvine, California 92717
</subsectionHeader>
<subsubsectionHeader confidence="0.867269">
Proc. 7th IJCAI, vol. 1, August 1981, 354-361.
</subsubsectionHeader>
<bodyText confidence="0.984463411764706">
Understanding a text depends on a reader&apos;s ability
to construct a coherent interpretation that accounts for
the statements in the text. However, a given text does
not always imply a unique coherent interpretation. In
particular, readers can be steered away from an other-
wise plausible explanation for a story by such extra-
textual factors as the source of the text, the reading
purpose, interruptions during reading, or repeated
re-questioning of the reader. Some of these effects
have been observed in experiments in cognitive psy-
chology. This paper presents a computer program
called MACARTHUR that can vary both the depth
and direction of its inference pursuit in response to
re-questioning, resulting in a series of markedly differ-
ent interpretations of the same text.
American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 273
The FINITE STRING Newsletter Abstracts of Current Literature
</bodyText>
<subsectionHeader confidence="0.967329">
Towards Automating Explanations
</subsectionHeader>
<bodyText confidence="0.774584">
R.E. Cullingford, M.W. Krueger, M. Selfridge, and
M.A. Bienkowski
</bodyText>
<subsectionHeader confidence="0.987226666666667">
Department of EE and CS
University of Connecticut
Storrs, Connecticut 06268
</subsectionHeader>
<subsubsectionHeader confidence="0.882897">
Proc. 7th IJCAI, vol. 1, August 1981, 362-367.
</subsubsectionHeader>
<bodyText confidence="0.99998325">
This paper discusses an approach to the modelling
of the explanation process within the framework of a
graphics-based CAD system currently under develop-
ment, which can describe its own use, including the
common ways to make and recover from errors. With
a coordinated textual and pictorial display, the system,
CADHELP, simulates an expert demonstrating the
operation of the graphical features of the CAD tool.
It consults a knowledge base of feature scripts, built up
using situational script and commonsense algorithmic
methods, to explain a feature, generate prompts as the
feature is being operated, and to give certain types of
&amp;quot;help&amp;quot; when a feature is misused. CADHELP pro-
vides these services by summarizing the feature script
in different ways depending upon what it has told the
user previously. The summarization process is based
upon a series of &amp;quot;sketchification&amp;quot; strategies, which
prescribe which parts of a knowledge structure, a cau-
sal chain, or a single concept can be thrown away,
since the listener should be able to infer them.
</bodyText>
<subsectionHeader confidence="0.4523465">
Using Active Connection Graphs for Reasoning
with Recursive Rules
Donald P. McKay and Stuart C. Shapiro
Department of Computer Science
State University of New York at Buffalo
Amherst, New York 14226
</subsectionHeader>
<subsubsectionHeader confidence="0.835639">
Proc. 7th IJCAI, vol. 1, August 1981, 368-374.
</subsubsectionHeader>
<bodyText confidence="0.999564642857143">
Recursive rules, such as &amp;quot;Your parents&apos; ancestors
are your ancestors,&amp;quot; although very useful for theorem
proving, natural language understanding, question-
answering and information retrieval systems, present
problems for many such systems, either causing infi-
nite loops or requiring that arbitrarily many copies of
them be made. SNIP, the SNePS Inference Package,
can use recursive rules without either of these prob-
lems. A recursive rule causes a cycle to be built in an
active connection graph. Each pass of data through the
cycle results in another answer. Cycling stops as soon
as either the desired answer is produced, no more an-
swers can be produced, or resource bounds are ex-
ceeded.
</bodyText>
<table confidence="0.9461796">
Control of Inference: Role of Some Aspects of
Discourse Structure - Centering
Aravind K. Joshi
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, Pennsylvania 19104
Scott Weinstein
Department of Philosophy
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</table>
<subsubsectionHeader confidence="0.552897">
Proc. 7th IJCAI, vol. 1, August 1981, 385-387.
</subsubsectionHeader>
<bodyText confidence="0.979007727272727">
The purpose of this communication is to examine
one particular aspect of discourse structure, namely, a
discourse construct called center of a sentence
(utterance) in discourse and its relation to the larger
issue of control of inference. We describe very briefly
the notion of center(s) of a sentence in discourse and
discuss how the centering phenomenon might be incor-
porated in a formal model of inference and its relation
to the intrinsic complexity of certain inferences.
The Design and an Example Use of Hearsay-III
Lee Erman, Philip London, and Stephen Fickas
</bodyText>
<subsectionHeader confidence="0.604309">
USC/Information Sciences Institute
4676 Admiralty Way
Marina del Rey, California 90291
</subsectionHeader>
<subsubsectionHeader confidence="0.76185">
Proc. 7th IJCAI, vol. 1, August 1981, 409-415.
</subsubsectionHeader>
<bodyText confidence="0.999697">
Hearsay-III provides a framework for constructing
knowledge-based expert systems. While Hearsay-III
makes no commitment to any particular application
domain, it does supply a variety of generally applicable
facilities. These include representation primitives and
an interpreter for large-grained, flexibly schedulable
production rules called knowledge sources. A detailed
overview of the motivations behind Hearsay-III and
the facilities it provides are presented. Finally, an
application of Hearsay-III is described.
</bodyText>
<subsectionHeader confidence="0.948802">
Anaphora for Limited Domain Systems
Philip J. Hayes
Department of Computer Science
Carnegie-Mellon University
Pittsburgh, Pennsylvania 15213
</subsectionHeader>
<subsubsectionHeader confidence="0.885239">
Proc. 7th IJCAI, vol. 1, August 1981, 416-422.
</subsubsectionHeader>
<bodyText confidence="0.957166678571428">
This paper presents a simple mechanism for the
resolution of anaphora in limited domain natural lan-
guage systems. For such domains, this mechanism
provides functionality equivalent to the natural com-
munication mechanism of anaphora as used and under-
stood by people, but without the deep inferencing or
cognitive modeling required for full simulation of hu-
man performance. The mechanism covers simple pro-
noun anaphora, and set selection anaphora (e.g. &amp;quot;last
one,&amp;quot; &amp;quot;one before,&amp;quot; &amp;quot;others&amp;quot;). It was developed to
provide the most efficient and effective communica-
tion between system and user, even if this meant di-
274 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
The FINITE STRING Newsletter Abstracts of Current Literature
verging significantly from human performance when
this performance was impractical to reproduce. In
cases of radical divergence, we were careful to make
the behaviour of the mechanism very simple and easy
to predict. In this way, the user can rely either on his
experience of human performance or his knowledge of
the artificial, but simple, substitute to predict the be-
haviour of the system in response to his inputs. An
algorithmic description of an implemented version of
the mechanism is presented. A similar approach to
other aspects of man-machine interfaces is recom-
mended as a promising way to address the problem of
habitability that still plagues all natural language com-
puter interfaces.
</bodyText>
<table confidence="0.926204833333333">
Figuring out What the User Wants - Steps
toward an Automatic Yellow Pages Assistant
Anatole Gershman
Schlumberger-Doll Research
P.O. Box 307
Ridgefield, Connecticut 06877
</table>
<subsubsectionHeader confidence="0.435352">
Proc. 7th IJCAI, vol. 1, August 1981, 423-425.
</subsubsectionHeader>
<bodyText confidence="0.999416416666667">
An experimental system, AYPA, for automatic Yel-
low Pages assistance, is described. The system, which
operates in the domain of automobiles, automobile
parts, and related objects, reads the user&apos;s request in
simple English, analyzes it and represents it in terms
of the system&apos;s conceptual primitives. From this, the
system tries to figure out the intent of the request and
formulate a Yellow Pages query. It paraphrases the
request back to the user in English and searches its
data base for the relevant Yellow Pages categories.
The system serves as a research vehicle for experi-
ments with its various components and user interfaces.
</bodyText>
<subsectionHeader confidence="0.974736">
Computing a Map from Michi-Annai-Bun
or Written Directions
Teiji Furugori
University of Electro-communications
Chofu, Tokyo, JAPAN
</subsectionHeader>
<subsubsectionHeader confidence="0.892631">
Proc. 7th IJCAI, vol. 1, August 1981, 426-428.
</subsubsectionHeader>
<bodyText confidence="0.999830333333333">
This paper describes processes of transforming
michi-annai-bun, literally street-guide-sentence, into a
map on a display device: we first show how such
transformation is performed, and then we discuss what
it means in terms of understanding natural language
expressions.
</bodyText>
<table confidence="0.1109788">
GLP: A General Linguistic Processor
G. Goerz
University of Erlangen-Nuernberg
RRZE Martensstr.1
0-8520 Erlangen, WEST GERMANY
</table>
<subsubsectionHeader confidence="0.734522">
Proc. 7th IJCAI, vol. 1, August 1981, 429-431.
</subsubsectionHeader>
<bodyText confidence="0.998905166666667">
GLP is a general linguistic processor for the analysis
and generation of natural language, based on a second
generation version of the General Syntactic Processor
of Kaplan and Kay. It is part of a speech understand-
ing system currently under development at the Com-
puter Science Department of our university.
</bodyText>
<subsectionHeader confidence="0.852934">
Multi-Strategy Construction-Specific Parsing
for Flexible Data Base Query and Update
Philip J. Hayes and Jaime G. Carbonell
Department of Computer Science
Carnegie-Mellon University
Schenley Park
Pittsburgh, Pennsylvania 15213
</subsectionHeader>
<subsubsectionHeader confidence="0.88677">
Proc. 7th IJCAI, vol. 1, August 1981, 432-439.
</subsubsectionHeader>
<bodyText confidence="0.99994685">
The advantages of a multi-strategy, construction-
specific approach to parsing in applied natural lan-
guage processing are explained through an examination
of two pilot parsers we have constructed. Our ap-
proach exploits domain semantics and prior knowledge
of expected constructions, using multiple parsing stra-
tegies each optimized to recognize different construc-
tion types. It is shown that a multi-strategy approach
leads to robust, flexible, and efficient parsing of both
grammatical and ungrammatical input in limited-
domain, task-oriented, natural language interfaces.
We also describe plans to construct a single, practical,
multi-strategy parsing system that combines the best
aspects of the two simpler parsers already implement-
ed into a more complex, embedded-constituent control
structure. Finally, we discuss some issues in data base
access and update, and show that a construction-
specific approach, coupled with a case-structured data
base description, offers a promising approach to a
unified, interactive data base query and update system.
</bodyText>
<table confidence="0.993902571428571">
A Deterministic Analyzer for the Interpretation
of Natural Language Commands
Leonardo Lesmo, Daniela Magnani, and Piero Torasso
Instituto di Scienze dell&apos;Informazione
Universita di Torino
C.so Massimo D&apos;Azeglio
42 - 10125 Torino, ITALY
</table>
<subsubsectionHeader confidence="0.630345">
Proc. 7th IJCAI, vol. 1, August 1981, 440-442.
</subsubsectionHeader>
<bodyText confidence="0.9140985625">
This paper describes a system which translates a
query in the Italian language into a representation
which can be immediately interpreted as a sequence of
algebraic operations on a relational data base. The
use of a lookahead buffer allows the system to operate
deterministically. Different knowledge sources are
used to cope with semantics (associated with the lexi-
con) and syntax (represented as pattern-action rules).
These knowledge sources cooperate during the query
translation so that independent translations of the
command are avoided. Therefore, the term
&amp;quot;determinism&amp;quot; is used to mean that all the structures
American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 275
The FINITE STRING Newsletter Abstracts of Current Literature
built during the process concur to build the final com-
mand representation.
</bodyText>
<table confidence="0.983441714285714">
A General Semantic Analyzer
for Data Base Access
B.K. Boguraev and K. Sparck Jones
Computer Laboratory
University of Cambridge
Corn Exchange Street
Cambridge CB2 3QG, ENGLAND
</table>
<subsubsectionHeader confidence="0.445042">
Proc. 7th IJCAI, vol. 1, August 1981, 443-445.
</subsubsectionHeader>
<bodyText confidence="0.999947947368421">
The paper discusses the design principles and cur-
rent status of a natural language front end for access
to data bases. This is based on the use, first, of a
semantically-oriented question analyzer exploiting
general, language-wide semantic categories and pat-
terns, rather than data base-specific ones: and, second,
of a data base-oriented translation component for
obtaining search specifications from the meaning rep-
resentations for questions derived by the analyzer.
This approach is motivated by the desire to reduce the
effort of providing data base-specific material for the
front end, by the belief that a general analyzer is well
suited to the &amp;quot;casual&amp;quot; data base user, and by the as-
sumption that the rich semantic apparatus used will be
both adequate as a means of analysis and appropriate
as a tool for linking the characterizations of input and
data language items. The paper describes this ap-
proach in more detail, with emphasis on the existing,
tested, analyzer.
</bodyText>
<subsectionHeader confidence="0.941940142857143">
A Metalanguage Representation of Relational
Databases for Deductive Q-A Systems
Kurt Konolige
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</subsectionHeader>
<subsubsectionHeader confidence="0.800425">
Proc. 7th IJCAI, vol. 1, August 1981, 496-503.
</subsubsectionHeader>
<bodyText confidence="0.999973806451613">
This paper presents a method of formally represent-
ing the information that exists in a relational database.
The primary utility of such a representation is for de-
ductive question-answering systems that must access
an existing relational database. To respond intelligent-
ly to user inquiries, such systems must have a more
complete representation of the domain of discourse
than is generally available in the database. The prob-
lem that then arises is how to reconcile the informa-
tion present in the database with the domain repre-
sentation so that database queries can be derived to
answer the user&apos;s inquiries. Here we take the formal
approach of describing a relational database as the
model of a first-order language. Another first-order
language, the metalanguage, is used both to represent
the domain of discourse, and to describe the relation-
ship of the database to the domain. This view proves
particularly useful in two respects. First, by axioma-
tizing the database language and its associated model
in a metatheory, we are able to describe in a powerful
and flexible manner how the database corresponds to
the domain of discourse. Secondly, viewing the data-
base as a mechanizable model of the database lan-
guage enables us to take advantage of the computa-
tional properties of database query language proc-
essors. Once a database query that is equivalent to an
original query is derived, it can be evaluated against
the database to determine the truth of the original
query. Thus the algebraic operations of the database
processor can be incorporated in an elegant way into
the deductive process of question-answering.
</bodyText>
<subsectionHeader confidence="0.927817833333333">
Explaining and Justifying Expert
Consulting Programs
William R. Swartout
Laboratory for Computer Science
Massachusetts Institute of Technology
Cambridge, Massachusetts 02139
</subsectionHeader>
<subsubsectionHeader confidence="0.720468">
Proc. 7th IJCAI, vol. 2, August 1981, 815-823.
</subsubsectionHeader>
<bodyText confidence="0.999901608695652">
Traditional methods for explaining programs pro-
vide explanations by converting to English the code of
the program or traces of the execution of that code.
While such methods can provide adequate explanations
of what the program does or did, they typically cannot
provide justifications of the code without resorting to
canned-text explanations. That is, such systems can-
not tell why what the system is doing is a reasonable
thing to be doing. The problem is that the knowledge
required to provide these justifications is needed only
when the program is being written and does not ap-
pear in the code itself.
The XPLAIN system uses an automatic program-
mer to generate the consulting program by refinement
from abstract goals. The automatic programmer uses a
domain model, consisting of facts about the applica-
tion domain, and a set of domain principles which
drive the refinement process forward. By examining
the refinement structure created by the automatic pro-
grammer it is possible to provide justifications of the
code. This paper discusses the system described above
and outlines additional advantages this approach has
for explanation.
</bodyText>
<subsectionHeader confidence="0.7237">
Last Steps towards an Ultimate PROLOG
</subsectionHeader>
<note confidence="0.242169">
A. Colmerauer, H. Kanoui, and M. Van Caneghem
</note>
<footnote confidence="0.6248948">
Groupe d&apos;Intelligence Artificielle
Faculte des Sciences de Luminy
Universite d&apos;Aix-Marseille II
13288 Marseille Cedex 9, FRANCE
Proc. 7th IJCAI, vol. 2, August 1981, 947-948.
</footnote>
<page confidence="0.609011">
276 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
</page>
<note confidence="0.639209">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999850428571429">
A portable version of PROLOG, an Artificial Intel-
ligence language, is presented. A complete system has
been implemented on a micro-computer using a
floppy-disk virtual memory. The general methodology
of the implementation is discussed in terms of an ab-
stract machine (Micromegas) supporting a language
(Candide) in which the PROLOG system is written.
</bodyText>
<subsectionHeader confidence="0.9646056">
Six Topics in Search of a Parser:
An Overview of Al Language Research
Eugene Charniak
Department of Computer Science
Brown University
</subsectionHeader>
<bodyText confidence="0.338499">
Providence, Rhode Island 02912
</bodyText>
<subsubsectionHeader confidence="0.823904">
Proc. 7th IJCAI, vol. 2, August 1981, 1079-1087.
</subsubsectionHeader>
<bodyText confidence="0.99980352631579">
My purpose in this paper is to give an overview of
natural language understanding work within artificial
intelligence (AI). I will concentrate on the problem of
parsing — going from natural language input to a se-
mantic representation. Naturally, the form of seman-
tic representation is a factor in such discussions, so it
will receive some attention as well. Furthermore, I
doubt that parsing can be completely isolated from
text processing issues, and hence I will touch upon
such seemingly non-parsing issues as script application.
Nevertheless, the topic is parsing.
Unfortunately, to present Al parsing work with any
sort of historical accuracy would be to produce a be-
wildering forest of names (both of people and pro-
grams). Rather, I will try to extract from the histori-
cal record a group of ideas which I believe can be
molded into a coherent framework. Naturally, even
within these limitations my remarks will be sketchy —
this is an article, not a book.
</bodyText>
<subsectionHeader confidence="0.912664">
Center-Embedding Revisited
Michael B. Kac
Department of Linguistics
University of Minnesota
Minneapolis, Minnesota 55455
</subsectionHeader>
<subsubsectionHeader confidence="0.809568">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 123-125.
</subsubsectionHeader>
<bodyText confidence="0.954720363636364">
The severe comprehension difficulty associated with
certain center-embedding constructions is perhaps the
best known of psychosyntactic phenomena. Most
attempts at explanation have been variations on a
single theme — that the c.e. configuration leads to an
overload of short-term memory during processing.
That this is not the whole story can be seen from con-
sidering the fact, rarely noted, that c.e. constructions
exist which are understood quite easily. Several exam-
ples are discussed.
The Natural Natural Language Understander
</bodyText>
<table confidence="0.8671202">
Henry Hamburger
Computer Science Section
National Science Foundation
1800 G Street, N.W.
Washington, D.C. 20550
</table>
<subsectionHeader confidence="0.87441075">
Stephen Crain
Department of Computer Sciences
The University of Texas
Austin, Texas 78712
</subsectionHeader>
<subsubsectionHeader confidence="0.813103">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 128-130.
</subsubsectionHeader>
<bodyText confidence="0.99997335">
This study of natural language comprehension by
natural understanding systems (children) is based on a
procedural analysis represented in the form of a pro-
gramming language. To clarify what is cognitively
required for a child to respond appropriately to certain
expressions in English, we show how these forms can
be translated into procedures in a high-level program-
ming language. It is then possible to discuss two kinds
of difficulties a natural language form can present to
the listener: (1) incompatibility of the form with its
associated procedure, and (2) complexity of that pro-
cedure. An example of procedure complexity is the
nesting of loops, whereas a contributor to incompati-
bility is a word or contiguous phrase that corresponds
to separated pieces of the procedure. We present
evidence of both types of difficulty from experiments
with children and compare the predictions of our pro-
cedural view with those of a less detailed syntactic
explanation that has been advanced for a subset of the
phenomena.
</bodyText>
<subsectionHeader confidence="0.898135833333333">
Why Do Children Say &amp;quot;Goed&amp;quot;? A Computer
Model of Child Language Generation
Mallory Selfridge
Department of EE and CS
University of Connecticut
Storrs, Connecticut 06268
</subsectionHeader>
<subsubsectionHeader confidence="0.797544">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 131-133.
</subsubsectionHeader>
<bodyText confidence="0.982039392857143">
An important question in modelling child language
generation is why children say regular forms of irregu-
lar words, such as &amp;quot;goed,&amp;quot; during development, al-
though they never hear them. Three other general
characteristics of children&apos;s generation also require
explanation. First, Benedict&apos;s work suggests clearly
that comprehension of various aspects of language
precede the generation of those aspects. Second, the
length of the utterances children say become generally
longer as as development proceeds. Third, Wetstone
and Friedlander suggest that first children say things in
the wrong order, and then say things in the correct
order.
In order to address these issues, this paper explores
the hypothesis that learning to talk is driven by learn-
ing to understand. This hypothesis begins by assuming
that the principal effect of learning to understand is
American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 277
The FINITE STRING Newsletter Abstracts of Current Literature
the development of the lexicon as additional words are
learned and their &amp;quot;definitions&amp;quot; are refined and modi-
fied. It further assumes that the language generation
process is not learned, but is an innate part of a child&apos;s
cognitive repertoire. Finally, it states that the ability
to generate grows as the lexicon develops during the
development of comprehension. The hypothesis pre-
dicts that a computer model which incorporated it
would display the characteristics described above.
</bodyText>
<subsectionHeader confidence="0.7034166">
Writing with a Computer
Ira Goldstein
Xerox Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, California 94304
</subsectionHeader>
<subsubsectionHeader confidence="0.795915">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 145-148.
</subsubsectionHeader>
<bodyText confidence="0.9998999">
This essay conjectures that an author&apos;s planning
process will be facilitated by a tool that represents his
plan at various levels of abstraction as a network of
subgoals, with the subgoal not necessarily restricted to
a linear order. Machine reasoning on such structures
has been explored in artificial intelligence research.
Our proposal is to make these structures available to
the writer as a calculus for representing his essays and
to use the computer as an interactive editing tool to
manipulate them.
</bodyText>
<subsectionHeader confidence="0.9070686">
MOPs and Learning
Roger C. Schank
Department of Computer Science
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.832815">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 166-170.
</subsubsectionHeader>
<bodyText confidence="0.997036727272727">
This paper is an attempt to sketch out some of what
MOPs are about. A MOP is an orderer of scenes. A
scene is a memory structure that groups together ac-
tions with a common goal, a common time, and some
other common thread. It provides a sequence of very
general actions. Specific memories are stored in
scenes, indexed with respect to how they differ from
the general action in the scene. Scenes actually point
to specific memories. MOPs do not. MOPs merely
point to scenes. Scripts are particularly common in-
stantiations of scenes.
</bodyText>
<subsectionHeader confidence="0.670948833333333">
Shaping Explanations: Effects of Questioning
on Text Interpretation
Richard H. Granger Jr.
Computer Science Department
University of California
Irvine, California 92717
</subsectionHeader>
<subsubsectionHeader confidence="0.829305">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 193-197.
</subsubsectionHeader>
<bodyText confidence="0.9997727">
Results in cognitive psychology have shown that
readers can be steered away from an otherwise plausi-
ble interpretation of a story by extra-textual factors
such as the source of the text, the stated reading pur-
pose, interruptions and repetition of questions about
the text. For instance, successive repetitions of the
same question about a given text will often elicit a
series of alternative interpretations of the text. This
effect cannot be accounted for by established princi-
ples of text processing behavior, such as people&apos;s pref-
erence for cohesive and parsimonious representations
of text. This paper presents a computer program
called MACARTHUR, which models this behavior by
varying the depth and direction of its inference pursuit
in response to re-questioning, resulting in a series of
markedly different interpretations of the same text. In
light of the results, some new experiments are suggest-
ed in hopes of arriving at a new principle, beyond
cohesion and parsimony, to account for the observed
text processing behavior.
</bodyText>
<subsectionHeader confidence="0.9546702">
Memory in Story Invention
Natalie Dehn
Department of Computer Science
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.757092">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 213-216.
</subsubsectionHeader>
<bodyText confidence="0.999833636363636">
AUTHOR is a story generating program (under
development) being built as a model of how human
authors make up stories. Like TALE-SPIN, AUTHOR
requires human-like knowledge of the world, but un-
like TALE-SPIN, AUTHOR also requires human-like
memory organization of this knowledge. The two
features of human memory most essential to the AU-
THOR model of story generation are: (1) reconstruc-
tion, and (2) reminding. The former is responsible for
the directed nature of making up stories, the latter for
the author&apos;s more &amp;quot;fortuitous&amp;quot; ideas and insights.
</bodyText>
<subsectionHeader confidence="0.9009828">
Controlling Parsing by Passing Messages
Brian Phillips and James Hendler
Texas Instruments
P.O. Box 225936, MS 371
Dallas, Texas 75265
</subsectionHeader>
<subsubsectionHeader confidence="0.808923">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 228-231.
</subsubsectionHeader>
<bodyText confidence="0.999961642857143">
The functional segmentation of linguistic knowledge
into rules about form and rules about meaning has
been vital in unravelling the complexities of language.
However, it does not follow that the process of analy-
sis will respect the same boundaries, and so the very
segmentation that provided the insights can be trou-
blesome when one seeks to create a dynamic model of
language. We believe that a language understanding
system should have the ability to bring syntactic and
semantic knowledge to bear on the analysis at many
points in the computation. This enables it to resolve
the alternatives as soon as possible and prevent the
flow of extraneous analyses to later phases. Our ap-
proach to creating such a model is to use the notion of
</bodyText>
<page confidence="0.559591">
278 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
</page>
<note confidence="0.619091">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.996021">
a society of communicating, knowledge-based
problem-solving experts, called &amp;quot;actors&amp;quot;. These ac-
tors can communicate by passing messages to any
other actor in the system. This flexible control struc-
ture allows actors at any level of the analysis to inter-
act with actors at other levels.
</bodyText>
<subsectionHeader confidence="0.8629548">
A Parser with Something for Everyone
Eugene Charniak
Department of Computer Science
Brown University
Providence, Rhode Island 02912
</subsectionHeader>
<subsubsectionHeader confidence="0.876636">
Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 231-234.
</subsubsectionHeader>
<bodyText confidence="0.999845888888889">
We present a syntactic parser, Paragram, which tries
to accommodate three goals. First, it will parse, in a
natural way, ungrammatical sentences. Secondly, it
aspires to &amp;quot;capture the relevant generalizations,&amp;quot; as in
transformational grammar, and thus its rules are in
virtual one-to-one correspondence with typical trans-
formational rules. Finally, it promises to be reason-
ably efficient, especially given certain limited parallel
processing capabilities.
</bodyText>
<subsectionHeader confidence="0.946190666666667">
GLISP: An Efficient, English-Like
Programming Language
Gordon S. Novak Jr.
Department of Computer Science
University of Texas
Austin, Texas 78712
</subsectionHeader>
<subsubsectionHeader confidence="0.83076">
Proc. 3rd Ann. Conf. Cog. Sc!. Soc., Aug. 1981, 249-252.
</subsubsectionHeader>
<bodyText confidence="0.999957076923077">
My earlier research on computer understanding of
physics problems, stated in English, has convinced me
that English is best viewed as a programming lan-
guage. That is, an English sentence does not contain
the message to be transmitted to the reader, but rather
is a program which provides the minimum information
necessary for the reader to construct the message from
what the reader already knows. Study of the ways in
which English permits compact expression of complex
ideas reveals several features which would be useful if
incorporated into programming languages. GLISP is a
LISP-based programming language which permits
English-like programs containing definite references.
</bodyText>
<subsectionHeader confidence="0.89716225">
Computerized Language Processing for Multiple
Use of Narrative Discharge Summaries
Naomi Sager, Lynette Hirschman,
and Margaret Lyman
Linguistic String Project
New York University
251 Mercer Street
New York, New York 10012
</subsectionHeader>
<subsubsectionHeader confidence="0.528154">
Proc. 2nd Ann. Symp. on Cornp. App!. in Med. Care,
IEEE, 1978, 330-343.
</subsubsectionHeader>
<bodyText confidence="0.999295625">
At New York University, computer programs have
been developed that convert natural language medical
records into a structured data base, i.e., into a table
containing the same information as the stored docu-
ments. In this form specific information can be quick-
ly retrieved, and summaries of the different kinds of
information in the documents can be automatically
generated. The automatic conversion of the informa-
tion from its free-text form to a tabular form is called
information formatting. This paper describes the ap-
plication of the information formatting programs to a
small set of pediatric discharge summaries for hospital-
izations due to sickle cell disease. The programs cre-
ated a table of approximately 50 columns in which
each different type of information in the documents
appeared under a separate heading. From this, a re-
trieval program extracted instances where symptoms of
possible infection preceded symptoms of painful crisis,
as suggested by the literature on sickle cell disease. In
answer to more detailed queries the program checked
the time-order of findings within one document. The
potential use of such tables in continuing medical edu-
cation and other applications in the hospital setting are
discussed.
</bodyText>
<subsectionHeader confidence="0.97351675">
Automatic Application of Health Care Criteria
to Narrative Patient Records
Lynette Hirschman, Naomi Sager,
and Margaret Lyman
Linguistic String Project
New York University
251 Mercer Street
New York, New York 10012
</subsectionHeader>
<subsubsectionHeader confidence="0.467039">
Proc. 3rd Ann. Symp. on Cornp. Appl. in Med. Care,
IEEE, 1979, 105-113.
</subsubsectionHeader>
<bodyText confidence="0.9641024">
This paper describes an experimental computer
program for the application of health care review
criteria to hospital discharge summaries. The use of
the computer in this process would make it possible to
speed up the routine screening of patient records; it
could also facilitate experimental evaluation of alter-
nate proposed audit criteria. The computer program
has two components. The first component creates a
structured form of the information contained in natu-
ral language medical records. It maps the words of
each sentence into labelled columns of a table (or
information format) according to the type of medical
information contained in each word. This structured
information is suitable for use as a data base in many
areas of clinical research. The second component
consists of a set of retrieval routines, each of which
corresponds to a criterion of the health care evaluation
form, e.g., &amp;quot;was the patient afebrile at discharge?&amp;quot;
The retrieval component is built up in modular fash-
ion, so that basic routines can be used in other appli-
cations. The application of this program to a sample
American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 279
The FINITE STRING Newsletter Abstracts of Current Literature
hospital discharge summary is presented and compared
to the results obtained by a physician reviewer.
</bodyText>
<subsectionHeader confidence="0.779145">
A CODASYL-Type Schema for Natural Language
Medical Records
</subsectionHeader>
<author confidence="0.571792">
N. Sager, L. Tick, G. Story, and L. Hirschman
</author>
<subsectionHeader confidence="0.90898675">
Linguistic String Project
New York University
251 Mercer Street
New York, New York 10012
</subsectionHeader>
<subsubsectionHeader confidence="0.548575">
Proc. 4th Ann. Symp. on Comp. Appl. in Med. Care,
IEEE, 1980, 1027-1033.
</subsubsectionHeader>
<bodyText confidence="0.99999336">
This paper describes a CODASYL (network) data-
base schema for information derived from narrative
clinical reports. The goal of this work is to create an
automated process that accepts natural language docu-
ments as input and maps this information into a data-
base of a type managed by existing database manage-
ment systems. The schema described here represents
the medical events and facts identified through the
natural language processing. This processing decom-
poses each narrative into a set of elementary asser-
tions, represented as MEDFACT records in the data-
base. Each assertion in turn consists of a subject and
a predicate classed according to a limited number of
medical event types, e.g., signs/symptoms, laboratory
tests, etc. The subject and predicate are represented
by EVENT records which are owned by the MED-
FACT record associated with the assertion. The
CODASYL-type network structure was found to be
suitable for expressing most of the relations needed to
represent the natural language information. However,
special mechanisms were developed for storing the
time relations between EVENT records and for re-
cording connections (such as causality) between cer-
tain MEDFACT records. This schema has been im-
plemented using the UNIVAC DMS-1100 DBMS.
</bodyText>
<sectionHeader confidence="0.8131255" genericHeader="keywords">
Research into Methods for Automatic Classifi-
cation and Fact Retrieval in Science Subfields
</sectionHeader>
<reference confidence="0.8488025">
N. Sager, L. Hirschman, C. White, C. Foster, S. Wolff,
R. Grad and E. Fitzpatrick
</reference>
<subsectionHeader confidence="0.8685415">
Linguistic String Project
New York University
251 Mercer Street
New York, New York 10012
</subsectionHeader>
<bodyText confidence="0.986476684210526">
Report No. 13, October 1980, 93 pages.
The broad goal of this research was to extend the
applicability of techniques for automated language
analysis being developed for the processing of infor-
mation in natural language data stores. The work
proceeded on two levels: (1) procedures applicable to
English material independent of the subject matter;
(2) procedures directed to the special use of language
in particular disciplines (the sublanguage of the disci-
pline). Previous work had shown that a computerized
grammar of English could be used in a parsing pro-
gram to obtain the grammatical structure of input sen-
tences; and subsequent processing, using the word
classes special to a given subject area, could arrange
the information in the sentences in table-like forms
(called information formats). With these methods,
textual information which heretofore had only been
accessible via key words and other word-scanning
methods could be organized in tabular form, similar to
numerical and scientific data bases.
To make these methods more widely applicable the
problems inherent in word classification have to be
faced. Automatic parsing of English text sentences
requires a computerized dictionary that gives parts of
speech and other grammatical properties of words;
mapping parsed sentences into information formats
requires a sublanguage dictionary that gives the
subject-matter (= semantic) word class membership of
the words. The research investigated several
computer-based methods for preparing and utilizing
such dictionaries.
On the sublanguage level, experiments in automatic
word class generation and sublanguage word-class
&amp;quot;boot strapping&amp;quot; were undertaken. In the latter, we
use patterns of sublanguage word class co-occurrence,
known from an initial set of texts, to determine the
sublanguage word class of new words in subsequent
texts.
</bodyText>
<subsectionHeader confidence="0.987233142857143">
Retrieving Time Information from
Natural Language Texts
Lynette Hirschman
Linguistic String Project
New York University
251 Mercer Street
New York, New York 10012
</subsectionHeader>
<bodyText confidence="0.958105096774194">
In Information Retrieval Research, Oddy et al. (eds.),
London: Butterworths, 1981, 154-171.
An understanding of time relations is central to
processing information contained in a narrative. A
typical narrative is concerned with the relative order-
ing or progression of events over time, and the infor-
mation that one would like to retrieve is often of the
type &amp;quot;What happened to event x?&amp;quot; or &amp;quot;Did event x
precede event y?&amp;quot; Determination of causality also
requires a knowledge of time relations, since an event
x can cause an event y only if it precedes event y in
time.
There has been considerable interest in the process-
ing of time information among researchers in artificial
intelligence. Several systems have been designed
which compute time relations from a structured input
of time specifications. The work described here dif-
fers from this other work in that it focuses specifically
on the problem of extracting time information from
coherent natural language text.
280 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
The FINITE STRING Newsletter Abstracts of Current Literature
The program described here was developed in the
course of ongoing research in natural language proc-
essing at the Linguistic String Project of New York
University. This research has been concerned with the
creation of a database from natural language input and
the retrieval of information from such a database. The
work on narrative time was an outgrowth of an experi-
ment on retrieval of information from narrative medi-
cal records.
</bodyText>
<reference confidence="0.378380888888889">
Model-theoretic Pragmatics: Dynamic Models
and an Application to Presuppositions and
lmplicature
Douglas B. Moran
Department of Computer Science
Oregon State University
Corvallis, Oregon 97331
Univ. of Michigan Ph.D. Dissertation, Cornp. Studies in
Formal Ling. N-22, October 1980, 328 pages.
</reference>
<bodyText confidence="0.999310386363637">
Model-theoretic semantics is a computationally
attractive formalism for the semantics of natural lan-
guages. The logical model can be viewed as a data-
base of information about the world with the evalua-
tion of logical formulas (representing the semantics of
sentences) retrieving information from this database.
However, this formalism has the limitation that the
information in a model is complete and static. To
overcome this problem, a formalism called dynamic
model-theoretic semantics is developed and applied to
the system given in Montague&apos;s PTQ. Dynamic mod-
els contain incomplete information and can have infor-
mation added to them as they are used. The evalua-
tion of logical formulas in a dynamic environment
produces effects — termed model-theoretic pragmatics
— that do not occur when a conventional model is
used.
The primary pragmatic effect studied here accounts
for presupposition and implicature with meaning postu-
lates — logical formulas that must be true for the
model to be reasonably used in the given application.
With conventional models, meaning postulates are
used to select reasonable models; with dynamic mod-
els, they are consistency checks on the information
being added to the model. Since only new information
is checked, this mechanism produces different behav-
iors for different models (contexts).
Dynamic model-theoretic semantics is non-
monotonic: it is impossible to detect when critical
elements are missing from the domains of quantifiers
or from the domains of functions being compared, and
thus the truth-value of a quantifier expression of an
equality test may change when the model expands.
A constructed element (a function or set) can have
its domain expanded between the time that it is speci-
fied (selected) and the time that it is used. A mecha-
nism for maintaining the consistency of an expanded
element with its original specification is presented and
shown to have interesting consequences: it permits
procedural — as well as declarative — representations
of the information in the model, and it permits incon-
sistent information to be entered. This mechanism
also has the capability of providing a motivated re-
striction on the kinds of inconsistencies that can occur.
</bodyText>
<subsectionHeader confidence="0.951785666666667">
Using Semantics in Non-Context-Free Parsing
of Montague Grammar
David Scott Warren
Department of Computer Science
State University of New York
Stony Brook, New York 11794
Joyce Friedman
Department of Computer and Communication Sciences
The University of Michigan
</subsectionHeader>
<reference confidence="0.537469333333333">
Ann Arbor, Michigan 48109
Comp. Studies in Formal Ling. N-27, August 1981,
41 pages.
</reference>
<bodyText confidence="0.999902294117647">
In natural language processing, questions concern-
ing the appropriate interaction of syntax and semantics
have long been of interest. Montague grammar and its
fully formalized syntax and semantics provide a com-
plete, well-defined context in which these questions
can be considered. This paper describes how seman-
tics can be used during parsing to reduce the combina-
toric explosion of syntactic ambiguity in Montague
grammar. A parsing algorithm, called semantic equi-
valence parsing, is described and examples of its oper-
ation are given. The algorithm is applicable to general
non-context-free grammars that include a formal se-
mantic component. The second portion of the paper
places semantic equivalence parsing in the context of
the very general definition of an interpreted language
as a homomorphism between syntactic and semantic
algebras.
</bodyText>
<subsectionHeader confidence="0.954272333333333">
Adaptation of Montague Grammar
to the Requirements of Parsing
Jan Landsbergen
</subsectionHeader>
<sectionHeader confidence="0.5665975" genericHeader="method">
Philips Research Laboratories
Eindhoven, THE NETHERLANDS
</sectionHeader>
<construct confidence="0.567617">
MC Tract 136, Formal Methods in the Study of
Language, Mathematics Centre, Amsterdam, 1981,
399-419.
</construct>
<bodyText confidence="0.999819166666667">
The paper describes a variant of Montague gram-
mar, of which the composition rules have analytical
counterparts on which a parsing algorithm can be
based. Separate attention is given to the consequences
of including rule schemes and syntactic variables in the
grammar.
</bodyText>
<note confidence="0.845039">
American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 281
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<title confidence="0.597879">
A Framework for Processing III-Formed Input
</title>
<reference confidence="0.920526333333333">
Ralph M. Weischedel
Department of Computer and Information Sciences
University of Delaware
Newmark, Delaware 19711
Norman K. Sondheimer
Software Research MS 2G3
Sperry Univac
Blue Bell, Pennsylvania 19424
Technical Report, 1981, 38 pages.
</reference>
<bodyText confidence="0.999876714285714">
If natural language processing systems are ever to
pass the Turing test, or if they are ever to achieve
natural, cooperative behavior, they must be able to
process input that is ill-formed lexically, syntactically,
semantically, or pragmatically. Systems must be able
to partially understand or at least give specific, appro-
priate error messages when input does not correspond
to their model of language and of context.
Out of our own work and the work of others, we
propose meta-rules and a control structure under
which they are invoked as a framework for processing
ill-formed input. The left-hand-side of a meta-rule
diagnoses (hypothesizes) a problem with the input as a
violated rule of normal processing. The right-hand-
side rewrites a violated rule as a relaxed one and states
how processing may be resumed, if at all.
Several specific meta-rules are given as examples.
In addition, a sketch is included of how several signifi-
cant heuristics developed by others can be formulated
as meta-rules. An analysis of the limitations of this
framework is also provided.
</bodyText>
<subsectionHeader confidence="0.64538">
Inference and Control in Multiprocessing
Environments
Harold Shubin
Department of Computer Science
</subsectionHeader>
<bodyText confidence="0.418359">
State University of New York at Buffalo
Amherst, New York 14226
</bodyText>
<subsubsectionHeader confidence="0.642699">
Technical Report No. 186, September 1981, 56 pages.
</subsubsectionHeader>
<bodyText confidence="0.999898681818182">
The ideas behind inference, as used in Artificial
Intelligence (AI) systems, are similar to those of cer-
tain control structures used in other areas of computa-
tion. This paper discusses those ideas and specifically
studies forward, backward, and bi-directional infer-
ence; and the data flow concept, lazy evaluation and
bi-directional search. A model of computation called
the Supplier-Producer-Consumer (SPC) Model, is
introduced as a vehicle for making contrasts between
pairs of inference and control strategies. Contrasts
along another dimension are made in the model by
discussing static and eager evaluation schemes.
Multiprocessing is an idea which has been imple-
mented differently in different areas of computer sci-
ence. This paper discusses the benefits of software
simulations of multiprocessing on uni-processing sys-
tems for these control and inference methods. Finally,
bi-directional methods (computation, inference, and
search) are discussed. The combination of forward
and backward computation methods allows each to
assist the other, and suggests new ways for a program
to interact with a user.
</bodyText>
<sectionHeader confidence="0.34871" genericHeader="method">
Analyzing Intention in Dialogues
</sectionHeader>
<reference confidence="0.8040046">
J.F. Allen and C.R. Perrault
Department of Computer Science
The University of Rochester
Rochester, New York 14627
Report No. TR50, April 1979, 75 pages.
</reference>
<bodyText confidence="0.999738909090909">
This paper describes a model of cooperative behav-
ior and shows how such a model can explain some
interesting linguistic behavior. We assume that agents
attempt to recognize the plans of other agents and
then use this plan when deciding what response to
make. In particular, we show that, given a setting in
which purposeful dialogues occur, this model can ac-
count for responses that provide more information
than explicitly requested and for appropriate responses
to both short sentence fragments and indirect speech
acts.
</bodyText>
<subsectionHeader confidence="0.530953">
Beyond Question-Answering
</subsectionHeader>
<reference confidence="0.98585975">
P.R. Cohen, C.R. Perrault, and J.F Allen
Bolt Beranek and Newman Inc.
10 Moulton Street
Cambridge, Massachusetts 02238
Technical Report 4644, May 1981.
(To appear in Strategies for Natural Language
Processing, Lehnert and Ringle (eds.), Erlbaum Assoc.,
in press.)
</reference>
<bodyText confidence="0.998289333333333">
We show that users of question-answering systems
expect those systems to be responsive to their unstated
plans and goals. Techniques needed to accomplish this
should be special cases of more general abilities, in
particular, the ability to recognize the user&apos;s plan and
to plan a helpful response. We propose and justify a
new system architecture embodying this framework,
and we illustrate how that architecture is applied in
two implemented systems. The first is a question-
answering system, and the second is a simple decision-
support system, for which both graphic and linguistic
means of communication are available.
</bodyText>
<subsectionHeader confidence="0.6238485">
Conversational Coherency in
Technical Conversations
</subsectionHeader>
<reference confidence="0.8852305">
Rachel Reichman
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 43, 1979.
</reference>
<page confidence="0.684184">
282 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
</page>
<bodyText confidence="0.966365071428572">
The FINITE STRING Newsletter Abstracts of Current Literature
An analysis of technical conversations is presented
which uses the framework of context spaces earlier
developed for the analysis of social exchanges. Both
forms of discourse are shown to display similarities
which are brought out by this form of analysis. A
number of instances of surface linguistic phenomena,
such as deictic &amp;quot;that,&amp;quot; present progressive tense, pron-
ominalization, and clue words such as &amp;quot;it&apos;s like&amp;quot; and
&amp;quot;now&amp;quot;, are presented and discussed. These hitherto
unexplained phenomena are accounted for in terms of
the underlying discourse structure and the resulting
state and focus level assignments to discourse constitu-
ents.
</bodyText>
<reference confidence="0.5060785">
Upward Branching Phrase Markers:
The State of the Debate
G. Sampson
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 45, 1980.
</reference>
<bodyText confidence="0.992692111111111">
A 1975 paper by the author (&amp;quot;the single mother
condition&amp;quot;) argued that the definition of phrase marker
in linguistics should, for reasons both of empirical
adequacy and theoretical elegance, be modified to
permit both upward as well as downward branching.
This paper examines various reactions to this proposal
that have been published. Certain criticisms are ac-
cepted as valid but not fatal. The rest turn out to be
based on a misunderstanding of the original claim.
</bodyText>
<reference confidence="0.43348175">
Three Strategic Goals Employed
in Conversational Openings
M. Rosner
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 46, 1981.
</reference>
<bodyText confidence="0.9982244375">
This paper tries to explain a short transcript of a
conversational opening as completely as possible with-
in the framework which takes conversational behavi-
our as defined by the operation of a sophisticated
planning mechanism. To account for conversational
openings in general and this transcript in particular, it
is argued that a crucial role is played by the satisfac-
tion, for each participant, of three strategic goals relat-
ing to attention, identification, and greeting. Addi-
tional tactics for gaining information are also de-
scribed as necessary to account for this transcript.
The final analysis employs the definitions of the goals
and tactics as defined. It is concluded that many of
the deficiencies in the final analysis could be avoided
by further investigations aimed at formalizing the ex-
planatory framework.
</bodyText>
<reference confidence="0.797041222222222">
A Computer-Based Teaching Scheme
for Creative Writing
Mike Sharpies
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
Proc. 3rd World Conf. on Computers in Education,
Lewis and Tagg (eds.), North-Holland, 1981, 483-488.
</reference>
<bodyText confidence="0.9996529">
The paper describes a computer-based teaching
scheme for creative writing. The scheme, based on a
cognitive theory of the writing process, develops
children&apos;s meta-linguistic knowledge and applies it to
the exploration and improvement of written style. The
children use computer programs to generate and trans-
form text and to explore the process of story prod-
uction. We present an outline description of the
scheme and the results of a pilot project with 6 eleven-
year-old pupils.
</bodyText>
<sectionHeader confidence="0.488486" genericHeader="method">
Microcomputers and Creative Writing
Mike Sharpies
</sectionHeader>
<reference confidence="0.529236666666667">
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
In Microcomputers in Secondary Education, Howe and
Ross (Eds.), London: Kogan Page, 1981, 138-157.
</reference>
<bodyText confidence="0.999977772727273">
There are few examples of computer aids, for educa-
tion in language arts and, of those few, most provide
pupils with drill and practice exercises in grammar,
spelling, or writing style. The computer programs
described in this paper are different. They offer pow-
erful and general learning aids — a sentence genera-
tor, a story planner, a word processor/text transform-
er, an automated thesaurus and dictionary, a spelling
corrector — which may be used both by the teacher to
demonstrate language construction and by the pupil to
compose and alter text. Being tools rather than teach-
ing systems they are not aligned to a particular sylla-
bus and, as the linguistic information is held in simple
data files, the programs may be easily modified to
manipulate other languages — from Latin to mathe-
matical expressions. The paper begins with a brief
history and critique of computer assisted instruction
(CAI) followed by an impression of a computer-based
&amp;quot;Workshop&amp;quot; for exploring language and creative writ-
ing and, lastly, an account of a project with eleven-
year-old children who used a prototype part of the
workshop in a creative writing course.
</bodyText>
<note confidence="0.366873">
American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 283
</note>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99870475">The FINITE STRING Newsletter Abstracts of Current Literature Abstracts of Current Literature Toward a Detailed Model of Processing for Language Describing the Physical World</title>
<author confidence="0.999978">David L Waltz</author>
<affiliation confidence="0.999978">Coordinated Science Laboratory University of Illinois</affiliation>
<address confidence="0.9992815">1101 West Springfield Avenue Urbana, Illinois 61801</address>
<abstract confidence="0.99267348">Proc. 7th IJCAI, vol. 1, August 1981, 1-6. This paper explores the problem of judging whether or not an English sentence could correspond to a real world situation or event which is literally, physically plausible, and the related problem of representing the different possible physical situations. The judgment of plausibility can be made at a high level by checking semantic marker restrictions on verb case frame constituents. Often, however, plausibility judgement can only be based on the results of an attempt to construct (imagine) a scene that corresponds to the sentence, and which does not violate &amp;quot;common sense&amp;quot; (i.e. relevant physical laws and expected, stereotyped behavior). Methods are presented for constructing representations for different scenes which could correspond to a sentence. These methods incorporate (1) &amp;quot;subscripts&amp;quot; (sequences of scenes which comprise an event, with attached preconditions and postconditions) to express different verb senses; (2) object representations which express properties such as shape, size, weight, strength, and behavior under common conditions; (3) physical laws, encoded as constraints on behavior; (4) representation of context; and (5) robot problem solving-like methods to fit all this material together.</abstract>
<title confidence="0.991038">Language Comprehension in a Problem Solver</title>
<author confidence="0.999974">Douglas Wong</author>
<affiliation confidence="0.9999525">Department of Computer Science Brown University</affiliation>
<address confidence="0.999606">Providence, Rhode Island 02912</address>
<abstract confidence="0.951313846153846">Proc. 7th IJCAI, vol. 1, August 1981, 7-12. This paper describes BRUIN, a unified Al system that can perform both problem-solving and language comprehension tasks. Included in the system is a frame-based knowledge-representation language called FRAIL, a problem solving component called NASL (which is based on McDermott&apos;s problem-solving language of the same name), and a context-recognition component currently known as PRAGMATICS. The intent of this paper is to give the flavor of how the context recognizer PRAGMATICS works and what it can do. Examples are drawn from the inventorycontrol, restaurant and blocks-world domains.</abstract>
<affiliation confidence="0.896324">Cancelled Due to Lack of Interest Department of Computer Science Columbia University</affiliation>
<address confidence="0.9509565">406 Mudd Building New York, New York 10027</address>
<note confidence="0.5537745">Proc. 7th IJCAI, vol. 1, August 1981, 13-15. The parts of a typical piece of text vary greatly in</note>
<abstract confidence="0.98505525">interest. Presented in this paper are three ways a concept can fail to be interesting — it can be irrelevant, reconstructible, or overshadowed. The uses of interest in understanding are also discussed.</abstract>
<author confidence="0.43131">Story Generation after TALE-SPIN</author>
<affiliation confidence="0.981423">Department of Computer Science Yale University</affiliation>
<address confidence="0.994306">New Haven, Connecticut 06520</address>
<abstract confidence="0.955917076923077">Proc. 7th IJCAI, vol. 1, August 1981, 16-18. TALE-SPIN, the last major AI attempt at story generation, approached the problem of making up stories primarily from the perspective of an impartial simulator. a program (under development) which generates stories as a creative reasoner in pursuit of her own narrative goals. It is thus intended to simulate an author&apos;s mind as she makes up a story, rather than the world as things happen in it. The four major forces driving the story generation process, according to the AUTHOR model, are author intentionality, conceptual reformation, reminding, and the opportunity enhancement metagoal.</abstract>
<title confidence="0.99895">Modeling Informal Debates</title>
<author confidence="0.999209">Rachel Reichman</author>
<address confidence="0.438724">Elect. Engrg. and Comp. Sci., C-014</address>
<affiliation confidence="0.999711">University of California</affiliation>
<address confidence="0.996398">La Jolla, California 92093</address>
<note confidence="0.679181">Proc. 7th IJCAI, vol. 1, August 1981, 19-24.</note>
<abstract confidence="0.9722015">Many rules of formal debate are well documented, are of common knowledge, and are &amp;quot;looked-up&amp;quot; in preparation for planned debating. Informal debates, on the other hand, are highly dynamic, are complex, and are spontaneously generated with no prior rulebook preparation. They too, however, are rulegoverned. In this paper I present an abstract process model capable of modeling &amp;quot;well-formed&amp;quot; argument structures that occur in ordinary conversations. The formalization rests on a general theoretical framework for discourse engagement encapsulated in a discourse ATN grammar. A major feature of the system is its segmentation of discourse utterances into functionally related context spaces.</abstract>
<note confidence="0.924535">Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981</note>
<title confidence="0.998262666666667">The FINITE STRING Newsletter Abstracts of Current Literature A Knowledge-Based Approach to Language Processing: A Progress Report</title>
<author confidence="0.99999">Robert Wilensky</author>
<affiliation confidence="0.9999475">Department of EECS University of California</affiliation>
<address confidence="0.999904">Berkeley, California 94720</address>
<abstract confidence="0.98858885">Proc. 7th IJCAI, vol. 1, August 1981, 25-30. We present a model of natural language use meant to encompass the language-specific aspects of understanding and production. The model is motivated by the pervasiveness of non-generative language, by the desirability of a language analyzer and a language production mechanism to share their knowledge, and by the advantages of knowledge engineering features such as ease of extension and modification. model has been used as the basis for language analyzer, and language production mechanism. We have implemented both these systems using a common knowledge base; we have produced versions of PHRAN that understand Spanish and Chinese by only changing the knowledge base and not modifying the program; and we have implemented the query language of a conventional relational data base system, and compared the perof this system to a conventional implementation.</abstract>
<title confidence="0.992205">The Need for Referent Identification as a Planned Action</title>
<author confidence="0.999996">Philip R Cohen</author>
<affiliation confidence="0.9999555">Department of Computer Science Oregon State University</affiliation>
<address confidence="0.999443">Corvallis, Oregon 97331</address>
<abstract confidence="0.972692133333333">Proc. 7th IJCAI, vol. 1, August 1981, 31-36. The paper presents evidence that speakers often attempt to get hearers to identify referents as a separate step in the speaker&apos;s plan. Many of the communicative acts performed in service of such referent identification steps can be analyzed by extending a planbased theory of communication for task-oriented diato include an a hearer&apos;s identifying the referent of a description — an action that is reasoned about in speakers&apos; and hearers&apos; plans. The phenomenon of addressing referent identification as a separate goal is shown to distinguish telephone from teletype task-oriented dialogues and thus has implications for the design of speech-understanding systems.</abstract>
<keyword confidence="0.708042">Integration, Unification, Reconstruction,</keyword>
<title confidence="0.946069">Modification: An Eternal Parsing Braid</title>
<author confidence="0.999953">Michael G Dyer</author>
<affiliation confidence="0.982302">Department of Computer Science Yale University</affiliation>
<address confidence="0.99692">New Haven, Connecticut 06520</address>
<abstract confidence="0.98454475">Proc. 7th IJCAI, vol. 1, August 1981, 37-42. is integrated natural language understanding system for narratives. In an integrated system, processes of event assimilation, inference, and episodic memory search occur on a word-by-word basis as parsing proceeds. &amp;quot;Parsing&amp;quot; here refers to the task of building a conceptual representation for each natural language expression. In addition to being intethe is also a unified parser. The same parser is used both at story understanding time and question answering time. This paper explores some of the consequences which arise when the same parser serves both tasks. For instance, one such conis that knows the answer to a question before it has completely understood the question.</abstract>
<title confidence="0.81457">Design Characteristics of a Machine Translation System</title>
<author confidence="0.999934">M King</author>
<affiliation confidence="0.995481">ISSCO Universite de Geneve</affiliation>
<address confidence="0.740988">17 Rue de CandoIle CH-1205 Geneve, SWITZERLAND</address>
<abstract confidence="0.898367">Proc. 7th IJCAI, vol. 1, August 1981, 43-46. This paper distinguishes a set of criteria to be met by a machine translation system (EUROTRA) currently being planned under the sponsorship of the Commission of the European Communities and attempts to show the effect of meeting those criteria on the overall system design.</abstract>
<title confidence="0.9976675">High Level Memory Structures and Text Coherence in Translation</title>
<author confidence="0.999926">C J Yang</author>
<affiliation confidence="0.9821845">Department of Computer Science Yale University</affiliation>
<address confidence="0.993536">New Haven, Connecticut 06520</address>
<abstract confidence="0.924661545454545">Proc. 7th IJCAI, vol. 1, August 1981, 47-49. Various memory organization schemes have been proposed in the last five years. Lots of intelligent computer systems have been experimenting with memory schemes like scripts, plans, goals, and MOPs in the domain of text understanding and information retrieval. In this paper, the focus is on the problem of translating sentences that involve lexical items which do not have equivalent counterparts in the target language. Examples are drawn from translations between English and Mandarin Chinese.</abstract>
<note confidence="0.926867">Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981</note>
<title confidence="0.863162666666667">The FINITE STRING Newsletter Abstracts of Current Literature Natural Lang. Dialogue about Moving Objects in an Automatically Analyzed Traffic Scene</title>
<author confidence="0.988538">H Marburger</author>
<author confidence="0.988538">B Neumann</author>
<author confidence="0.988538">H-J Novak</author>
<affiliation confidence="0.9205925">Fachbereich Informatik Universitaet Hamburg</affiliation>
<address confidence="0.880788">Schlueterstrasse 70</address>
<note confidence="0.617906333333333">D-2000 Hamburg 13 Proc. 7th IJCAI, vol. 1, August 1981, 49-51. This contribution is concerned with natural lan-</note>
<abstract confidence="0.999815769230769">guage dialogues about scenes with moving objects. Two systems are connected: a natural language dialogue system originally conceived for static scenes and an emerging scene analysis system for real-world TVframe sequences. The latter produces time dependent object descriptions which serve as a referential database for inquiries. The time intervals relevant for answering the questions are determined from domain specific parameters, the context of the dialogue, the tense of the verbs and time adverbials. For checking the correspondence between a verbally specified motion and a trajectory, predicates are evaluated which can be deduced from the verb&apos;s case-frame.</abstract>
<title confidence="0.984152">Using Language and Context in the Analysis of Text</title>
<author confidence="0.969365">Yigal Arens</author>
<affiliation confidence="0.996954">Department of EECS University of California, Berkeley</affiliation>
<address confidence="0.999854">Berkeley, California 94720</address>
<abstract confidence="0.983611461538462">Proc. 7th IJCAI, vol. 1, August 1981, 52-57. We describe a theory of natural language understanding within which we identify two separate coma centered and a centered one. The former component uses a knowledge base consisting of pairings of phrases with the concepts associated with them to determine the meaning of utterances. The latter component clarifies the meaning found by the first one and makes it more specific by attempting to reconcile it with the context of the utterance. We have constructed a program called PHRAN (PHRasal ANalyzer) which performs the task of the language centered component.</abstract>
<title confidence="0.981785">Opportunistic Processing in Arguments</title>
<author confidence="0.999612">Rod McGuire</author>
<author confidence="0.999612">Lawrence Birnbaum</author>
<author confidence="0.999612">Margot Flowers</author>
<affiliation confidence="0.982256">Department of Computer Science Yale University</affiliation>
<address confidence="0.995408">New Haven Connecticut 06520</address>
<abstract confidence="0.958457125">Proc. 7th IJCAI, vol. 1, August 1981, 58-60. In two previous papers we have proposed a part of a computational theory of argumentation, including representations for argument structure and rules for using those representations in understanding and in rebutting. One property of the model which we emphasized is the way in which argument mechanisms and inferential memory can each help to direct the processing of the other. In particular, we presented examples in which inferential memory can uncover good rebuttals to an input as a side-effect of the processing that naturally goes on in trying to understand that input. When such opportunities for rebuttal are noticed during understanding, they render unnecessary the use of argument rules to find a response, since one has already been discovered.</abstract>
<title confidence="0.981512">Natural Language Interaction with Dynamic Knowledge Bases: Monitoring as Response</title>
<author confidence="0.999644">Eric Mays</author>
<author confidence="0.999644">Sitaram Lanka</author>
<author confidence="0.999644">Aravind K Joshi</author>
<author confidence="0.999644">Bonnie L Webber</author>
<affiliation confidence="0.9999045">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999501">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.979591375">Proc. 7th IJCAI, vol. 1, August 1981, 61-63. In this communication, we discuss an interesting aspect of natural language interaction with dynamically changing knowledge bases — the ability to monitor for relevant future changes in that knowledge. We also indicate the status of our current work in this area and the overall goals of our research on questionanswering and monitoring dynamic knowledge bases.</abstract>
<title confidence="0.987864">Variable-depth Natural Language Understanding</title>
<author confidence="0.999782">Daniel Kayser</author>
<affiliation confidence="0.9232345">E.R.A. 452 du CNRS Laboratoire de Recherche en Informatique</affiliation>
<address confidence="0.9972855">Bat.490 - Campus d&apos;Orsay 91405 Orsay, FRANCE</address>
<author confidence="0.99236">Daniel Coulon</author>
<affiliation confidence="0.782370666666667">L.A. 262 du CNRS Centre de Recherche en lnformatique de Nancy ENSMIM - Parc de Saurupt</affiliation>
<address confidence="0.995336">54042 Nancy, FRANCE</address>
<abstract confidence="0.967281066666667">Proc. 7th IJCAI, vol. 1, August 1981, 64-66. Standard Al representations of knowledge operate at fixed depth (i.e., the objects manipulated are described by an amount of information which remains constant for every task). Contrary to this approach, Depth Processing (VDP) uses a objects, tries strategies according to the quality of the result it needs, and continualcontrols this quality by means of an approximations makes. Contextual Production Rules are shown to be an effective way to implement some features of VDP. We are currently developing a VDP question-answering system which works on texts concerning a non-technical subject, namely an excerpt of a general public-oriented encyclopaedia.</abstract>
<note confidence="0.924166">Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981</note>
<title confidence="0.9979825">The FINITE STRING Newsletter Abstracts of Current Literature Generalizations Based on Explanations</title>
<author confidence="0.999914">Gerald DeJong</author>
<affiliation confidence="0.9999805">Coordinated Science Laboratory University of Illinois</affiliation>
<address confidence="0.9992">1101 West Springfield Avenue Urbana, Illinois 61801</address>
<abstract confidence="0.958764263157895">Proc. 7th IJCAI, vol. 1, August 1981, 67-70. This paper describes a new project in computer learning. The phenomenon under study is a kind of &amp;quot;insight learning&amp;quot; of procedural schemata. The system described here is designed to grasp some principle underlying a natural language input. The underlying principle results in a new schema for the system. Once acquired, the schema serves the same purpose as the other schemata in the system: it aids in processing future natural language inputs. process that the system uses is called explanaschema acquisition. basic idea behind it is that the causal connections in an understood representation of a new input can be used to propose and propagate constraints on slot fillers. That is, from one particular instance or situation the system can &amp;quot;reason out&amp;quot; the general structure underlying that instance. The system is therefore capable of learning from just one example.</abstract>
<title confidence="0.981524">Viewing Word Expert Parsing as Linguistic Theory</title>
<author confidence="0.999981">Steven Small</author>
<affiliation confidence="0.997431666666667">Department of Computer Science Mathematical Sciences Building University of Rochester</affiliation>
<address confidence="0.999392">Rochester, New York 14627</address>
<abstract confidence="0.975923571428572">Proc. 7th IJCAI, vol. 1, August 1981, 70-76. Expert Parser a computer program that analyzes fragments of natural language text in order to extract their meaning in context. The construction of the program has led to the development of a linguistic theory based on notions orthogonal to those traditionally found at the heart of such theories. Word Expert Parsing explains the understanding of textual fragments containing highly idiosyncratic elements, such as idioms, collocations, cliches, and colligations, as well as lexical sequences that contain interesting structural phenomena. The theory perceives the individual word of language as the organizing unit for linguistic knowland views understanding as consisting of procedural word experts. This paper describes four classes of lexical interaction required to explain the understanding of sentences in interaction, linguistic interaction, interaction, interaction paper purposely avoids programming details in order to focus on Word Expert Parsing as linguistic theory.</abstract>
<title confidence="0.9384455">A Plot Understanding System on Reference to Both Image and Language</title>
<author confidence="0.99947">Norihiro Abe</author>
<author confidence="0.99947">Itsuya Soga</author>
<author confidence="0.99947">Saburo Tsuji</author>
<affiliation confidence="0.999894">Department of Control Engineering Osaka University</affiliation>
<address confidence="0.996515">Toyonaka, Osaka, JAPAN</address>
<abstract confidence="0.926807142857143">Proc. 7th IJCAI, vol. 1, August 1981, 77-84. A system is described that can understand a plot of a story on reference to both image and linguistic information. As input, a series of line drawings with colors and narrations in English concerning these drawings are given to the system. It searches the objects suggested to be in the scene by the narrations, finding relations among them, making the world model by using its world knowledge. Its reference to those drawings makes it easy for the system to analyze complicated structures in the narration sentences, such as those of prepositions, and guides the process reasoning about the CD representation using rules and demons. At the end of this paper, a result on QA is shown.</abstract>
<title confidence="0.955975">Metaphor Interpretation as Selective Inferencing</title>
<author confidence="0.999968">Jerry R Hobbs</author>
<affiliation confidence="0.999738">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.9976875">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.9548994">Proc. 7th IJCAI, vol. 1, August 1981, 85-91. Metaphor pervades natural language discourse. This paper describes a computational approach to the interpretation of metaphors. It is based on a natural language processing system that uses the discourse problems posed by a text to select the relevant inferences. The problem of interpreting metaphors can then be translated into the problem of selecting the relevant inferences to draw from the metaphorical expression. Thus, a metaphor is frequently given a correct interpretation as a by-product of the other things a natural language system has to do. Two examples of metaphors are given — a spatial metaphor schema from computer science, and a novel metaphor — and it is shown how the interpretation problem for each can be translated into a selective inferencing problem and solved by the ordinary operations of the system. This framework sheds light on the analogical processes that underlie metaphors and begins to explain the power of metaphor.</abstract>
<note confidence="0.934161">Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981</note>
<title confidence="0.997980333333333">The FINITE STRING Newsletter Abstracts of Current Literature A Computer Model of Child Language Acquisition</title>
<author confidence="0.999983">Mallory Selfridge</author>
<affiliation confidence="0.9999365">Department of EE and CS University of Connecticut</affiliation>
<address confidence="0.997757">Storrs, Connecticut 06268</address>
<abstract confidence="0.970501545454545">Proc. 7th IJCAI, vol. 1, August 1981, 92-96. Children learn different aspects of language in a characteristic order, and make characteristic errors during acquisition. This paper focuses on five specific data, and explains these data in terms of two hypotheses regarding the relationship of comprehension to generation and the relationship between meaning syntax. A computer model, described which embodies these hypotheses and which manifests the same five data. CHILD&apos;S performance suggests that the explanations are plausible.</abstract>
<title confidence="0.8999645">A Theory of Language Acquisition Based on General Learning Principles</title>
<author confidence="0.999714">John R Anderson</author>
<affiliation confidence="0.9999325">Department of Psychology Carnegie-Mellon University</affiliation>
<address confidence="0.999615">Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.505863">Proc. 7th IJCAI, vol. 1, August 1981, 97-103. A simulation model is described for the acquisition</note>
<abstract confidence="0.996756411764706">of the control of syntax in language generation. This model makes use of general learning principles and general principles of cognition. Language generation is modeled as a problem-solving process involving principally the decomposition of a to-be-communicated semantic structure into a hierarchy of subunits for generation. The syntax of the language controls this decomposition. It is shown how a sentence and semantic structure can be compared to infer the decomposition that led to the sentence. The learning processes involve generalizing rules to classes of words, learning by discrimination the various contextual constraints on a rule application, and a strength process which monitors a rule&apos;s history of success and failure. This system is shown to apply to the learning of noun declensions in Latin, relative clause constructions in French, and verb auxiliary structures in English.</abstract>
<title confidence="0.926686">Inductive Learning of Pronunciation Rules by Hypothesis Testing and Correction</title>
<author confidence="0.999648">S Oakey</author>
<author confidence="0.999648">R C Cawthorn</author>
<affiliation confidence="0.978104">Department of Computer Science Teesside Polytechnic</affiliation>
<address confidence="0.998028">Middlesbrough, Cleveland, ENGLAND</address>
<abstract confidence="0.97610525">Proc. 7th IJCAI, vol. 1, August 1981, 109-114. This paper describes a system that learns the rules of pronunciation inductively. It begins with a set of 26 rules for single-letter pronunciation. Individual words are presented to it, and the system uses its rule set to hypothesize a pronunciation. This is compared with a dictionary pronunciation, and if any part of the pronunciation is incorrect new rules are created to handle the word as an exception condition. These rules are checked for similarity with others already produced, and where suitable a &amp;quot;general&amp;quot; rule is produced to deal with two or more created rules. The effect is to produce rules that are more and more general, and these approach the general pronunciation rule sets that have been produced manually by other workers.</abstract>
<title confidence="0.986999">Summarizing Narratives</title>
<author confidence="0.999995">Wendy G Lehnert</author>
<author confidence="0.999995">John B Black</author>
<author confidence="0.999995">Brian J Reiser</author>
<affiliation confidence="0.9820425">Department of Computer Science Yale University</affiliation>
<address confidence="0.99003">New Haven, Connecticut 06520</address>
<note confidence="0.655851">Proc. 7th IJCAI, vol. 1, August 1981, 184-189.</note>
<abstract confidence="0.98095125">Most research on narrative text summarization has been conducted within the paradigm of experimental psychology. But recent language processing research in artificial intelligence suggests that the predominant theory of text summarization requires further examination. Seemingly minor structural modifications of a story can result in significant alterations of summary behavior. In this paper, highlights of summary data from 72 subject are presented and analyzed in terms of two competing summarization models: (1) the story grammar model of psychology, and (2) the plot unit model developed in artificial intelligence. We show how selected story grammar predictions compare to the plot unit predictions for short term summarization and then identify two complicating factors that have a major impact on summarization behavior.</abstract>
<title confidence="0.984292">Text Plans and World Plans in Natural Discourse</title>
<author confidence="0.999948">Jerry R Hobbs</author>
<affiliation confidence="0.9997375">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.997955">333 Ravenswood Avenue Menlo Park, California 94025</address>
<author confidence="0.995554">Michael Agar</author>
<affiliation confidence="0.9999625">Department of Anthropology University of Maryland</affiliation>
<address confidence="0.997082">College Park, Maryland 20742</address>
<abstract confidence="0.967">Proc. 7th IJCAI, vol. 1, August 1981, 190-196. is both world and an accomworld. This fact has led to two approaches to the study of discourse in artificial intelligence: one investigating &amp;quot;text plans,&amp;quot; the other, &amp;quot;world plans.&amp;quot; By analyzing a fragment of a narrative in which both kinds of plans figure importantly, we explore the relationship between the two kinds of plans, looking toward a synthesis of the two approaches.</abstract>
<note confidence="0.9674">Journal of Computational Linguistics, Volume 7, Number 4, October-December</note>
<title confidence="0.929626333333333">The FINITE STRING Newsletter Abstracts of Current Literature Recognizing Intended Meaning and Speakers&apos; Plans</title>
<author confidence="0.999801">Candace L Sidner</author>
<author confidence="0.999801">David J Israel</author>
<affiliation confidence="0.996488">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.9996175">10 Moulton Street Cambridge, Massachusetts 02238</address>
<abstract confidence="0.969763153846154">Proc. 7th IJCAI, vol. 1, August 1981, 203-208. Human conversational participants depend upon the ability of their partners to recognize their intentions, so that those partners may respond appropriately. In such interactions, the speaker can encode his intentions that the hearer act in a variety of sentence types. Instead of telling the hearer what to do, the speaker may just state his goals, and expect a response that meets these goals. This paper presents a new model for recognizing the speaker&apos;s intended meaning in determining a response. We show that this recognition makes use of the speaker&apos;s plan, his beliefs about the domain and about the hearer&apos;s relevant capacities.</abstract>
<title confidence="0.7848455">Character Tracking and the Understanding of Narratives</title>
<author confidence="0.999992">Brian J Reiser</author>
<affiliation confidence="0.8273295">Cognitive Science Program Yale University</affiliation>
<address confidence="0.977565">New Haven, Connecticut 06520</address>
<note confidence="0.8753995">Proc. 7th IJCAI, vol. 1, August 1981, 209-212. Recent work on the understanding of natural lan-</note>
<abstract confidence="0.997492615384615">guage narratives has emphasized representations composed of goals, plans, and their outcomes. A problem that has received little attention however is the influof understanding a narrative. Perspective may operate on many levels. Understanding proceeds as a focused tracking of the fate of a character&apos;s goals. When attention is focused on a character in a narrative, each new input is then evaluated from that character&apos;s perspective. Processing the story from one character&apos;s perspective is one way that the inferencing process might be constrained. Less relevant inferences need never be made or integrated into the representation.</abstract>
<title confidence="0.981121">Representing Implicit and Explicit Time Relations in Narrative</title>
<author confidence="0.824713">Lynette Hirschman</author>
<author confidence="0.824713">Guy Story Linguistic String Project</author>
<affiliation confidence="0.998352">New York University</affiliation>
<address confidence="0.999687">251 Mercer Street</address>
<note confidence="0.533418333333333">New York, New York 10012. Proc. 7th IJCAI, vol. 1, August 1981, 289-295. This paper describes a representation for time rela-</note>
<abstract confidence="0.9959336875">tions in narrative. The time relations are based on both explicit sources of time information (e.g., adverbial expressions 4:n tense) and implicit sources, such as multiple reference to a single event, narrative time progression and earlier events implied by change of state words. Natural language processing is used to analyze the input text into a set of subject-verb-object units, connected by binary connectives; these units correspond to the events of the narrative. With each event is associated a time in relation to another event, adjusted by an optional time quantity. These time relations have a natural representation as a directed graph whose nodes are time points and whose edges are time intervals. The algorithm for extracting the time relations from a text is illustrated for an excerpt from a hospital discharge summary.</abstract>
<title confidence="0.991339">The Nature of Generalization in Understanding</title>
<author confidence="0.999996">Michael Lebowitz</author>
<affiliation confidence="0.9999725">Department of Computer Science Columbia University</affiliation>
<address confidence="0.992874">406 Mudd Building New York, New York 10027</address>
<abstract confidence="0.988913272727273">Proc. 7th IJCAI, vol. 1, August 1981, 348-353. True understanding of natural language text requires the inclusion of generalization and long-term memory. This paper describes the generalization process and memory used in the Integrated Partial Parser (IPP), a computer program that reads and remembers news stories. The need for generalization generalization-based memory as an integral part of understanding natural language text is illustrated with from addition, the nature of generalization is discussed.</abstract>
<title confidence="0.9910225">Directing and Re-directing Inference Pursuit: Extra-textual Influences on Text Interpretation</title>
<author confidence="0.999989">Richard H Granger Jr</author>
<affiliation confidence="0.99995">Department of Information and Computer Science University of California</affiliation>
<address confidence="0.999477">Irvine, California 92717</address>
<abstract confidence="0.9202220625">Proc. 7th IJCAI, vol. 1, August 1981, 354-361. Understanding a text depends on a reader&apos;s ability to construct a coherent interpretation that accounts for the statements in the text. However, a given text does not always imply a unique coherent interpretation. In particular, readers can be steered away from an otherwise plausible explanation for a story by such extratextual factors as the source of the text, the reading purpose, interruptions during reading, or repeated re-questioning of the reader. Some of these effects have been observed in experiments in cognitive psychology. This paper presents a computer program called MACARTHUR that can vary both the depth and direction of its inference pursuit in response to re-questioning, resulting in a series of markedly different interpretations of the same text.</abstract>
<note confidence="0.89892">Journal of Computational Linguistics, Volume 7, Number 4, October-December 273</note>
<title confidence="0.9975595">The FINITE STRING Newsletter Abstracts of Current Literature Towards Automating Explanations</title>
<author confidence="0.989309">M W Krueger</author>
<author confidence="0.989309">M Selfridge</author>
<author confidence="0.989309">M A Bienkowski</author>
<affiliation confidence="0.9987395">of CS University of Connecticut</affiliation>
<address confidence="0.998363">Storrs, Connecticut 06268</address>
<abstract confidence="0.992755523809524">Proc. 7th IJCAI, vol. 1, August 1981, 362-367. This paper discusses an approach to the modelling of the explanation process within the framework of a graphics-based CAD system currently under development, which can describe its own use, including the common ways to make and recover from errors. With a coordinated textual and pictorial display, the system, simulates an expert operation of the graphical features of the CAD tool. consults a knowledge base of scripts, up using situational script and commonsense algorithmic methods, to explain a feature, generate prompts as the feature is being operated, and to give certain types of &amp;quot;help&amp;quot; when a feature is misused. CADHELP provides these services by summarizing the feature script in different ways depending upon what it has told the user previously. The summarization process is based upon a series of &amp;quot;sketchification&amp;quot; strategies, which prescribe which parts of a knowledge structure, a causal chain, or a single concept can be thrown away, since the listener should be able to infer them.</abstract>
<title confidence="0.9985415">Using Active Connection Graphs for Reasoning with Recursive Rules</title>
<author confidence="0.999984">Donald P McKay</author>
<author confidence="0.999984">Stuart C Shapiro</author>
<affiliation confidence="0.999799">Department of Computer Science State University of New York at Buffalo</affiliation>
<address confidence="0.999241">Amherst, New York 14226</address>
<abstract confidence="0.955819866666667">Proc. 7th IJCAI, vol. 1, August 1981, 368-374. Recursive rules, such as &amp;quot;Your parents&apos; ancestors are your ancestors,&amp;quot; although very useful for theorem proving, natural language understanding, questionanswering and information retrieval systems, present problems for many such systems, either causing infinite loops or requiring that arbitrarily many copies of them be made. SNIP, the SNePS Inference Package, can use recursive rules without either of these problems. A recursive rule causes a cycle to be built in an connection graph. pass of data through the cycle results in another answer. Cycling stops as soon as either the desired answer is produced, no more answers can be produced, or resource bounds are exceeded.</abstract>
<title confidence="0.790774">Control of Inference: Role of Some Aspects of Discourse Structure - Centering</title>
<author confidence="0.999881">Aravind K Joshi</author>
<affiliation confidence="0.9998825">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.991287">Pennsylvania</address>
<author confidence="0.999857">Scott Weinstein</author>
<affiliation confidence="0.9993305">Department of Philosophy University of Pennsylvania</affiliation>
<address confidence="0.999365">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.960726">Proc. 7th IJCAI, vol. 1, August 1981, 385-387. The purpose of this communication is to examine one particular aspect of discourse structure, namely, a construct called a sentence discourse its relation to the larger issue of control of inference. We describe very briefly the notion of center(s) of a sentence in discourse and discuss how the centering phenomenon might be incorporated in a formal model of inference and its relation to the intrinsic complexity of certain inferences.</abstract>
<title confidence="0.776138">The Design and an Example Use of Hearsay-III</title>
<author confidence="0.828555">London Erman</author>
<author confidence="0.828555">Stephen Fickas</author>
<affiliation confidence="0.999535">USC/Information Sciences Institute</affiliation>
<address confidence="0.996254">4676 Admiralty Way Marina del Rey, California 90291</address>
<abstract confidence="0.913286909090909">Proc. 7th IJCAI, vol. 1, August 1981, 409-415. Hearsay-III provides a framework for constructing knowledge-based expert systems. While Hearsay-III makes no commitment to any particular application domain, it does supply a variety of generally applicable facilities. These include representation primitives and an interpreter for large-grained, flexibly schedulable production rules called knowledge sources. A detailed overview of the motivations behind Hearsay-III and the facilities it provides are presented. Finally, an application of Hearsay-III is described.</abstract>
<title confidence="0.995276">Anaphora for Limited Domain Systems</title>
<author confidence="0.999992">Philip J Hayes</author>
<affiliation confidence="0.999824">Department of Computer Science Carnegie-Mellon University</affiliation>
<address confidence="0.999568">Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.799946">Proc. 7th IJCAI, vol. 1, August 1981, 416-422. This paper presents a simple mechanism for the</note>
<abstract confidence="0.973007555555556">of anaphora in limited domain natural lan- For such domains, this mechanism provides functionality equivalent to the natural communication mechanism of anaphora as used and understood by people, but without the deep inferencing or cognitive modeling required for full simulation of human performance. The mechanism covers simple pronoun anaphora, and set selection anaphora (e.g. &amp;quot;last one,&amp;quot; &amp;quot;one before,&amp;quot; &amp;quot;others&amp;quot;). It was developed to provide the most efficient and effective communicabetween system and user, even if this meant di- Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 The FINITE STRING Newsletter Abstracts of Current Literature verging significantly from human performance when this performance was impractical to reproduce. In cases of radical divergence, we were careful to make the behaviour of the mechanism very simple and easy to predict. In this way, the user can rely either on his experience of human performance or his knowledge of the artificial, but simple, substitute to predict the behaviour of the system in response to his inputs. An algorithmic description of an implemented version of the mechanism is presented. A similar approach to other aspects of man-machine interfaces is recommended as a promising way to address the problem of habitability that still plagues all natural language computer interfaces.</abstract>
<title confidence="0.8734525">Figuring out What the User Wants - Steps toward an Automatic Yellow Pages Assistant</title>
<author confidence="0.999002">Anatole Gershman</author>
<affiliation confidence="0.997527">Schlumberger-Doll Research</affiliation>
<address confidence="0.9885385">P.O. Box 307 Ridgefield, Connecticut 06877</address>
<note confidence="0.518491">Proc. 7th IJCAI, vol. 1, August 1981, 423-425. An experimental system, AYPA, for automatic Yel-</note>
<abstract confidence="0.993761727272727">low Pages assistance, is described. The system, which operates in the domain of automobiles, automobile parts, and related objects, reads the user&apos;s request in simple English, analyzes it and represents it in terms of the system&apos;s conceptual primitives. From this, the system tries to figure out the intent of the request and formulate a Yellow Pages query. It paraphrases the request back to the user in English and searches its data base for the relevant Yellow Pages categories. The system serves as a research vehicle for experiments with its various components and user interfaces.</abstract>
<title confidence="0.981932">Computing a Map from Michi-Annai-Bun or Written Directions</title>
<author confidence="0.998707">Teiji Furugori</author>
<affiliation confidence="0.999694">University of Electro-communications</affiliation>
<address confidence="0.993625">Chofu, Tokyo, JAPAN</address>
<abstract confidence="0.947280571428571">Proc. 7th IJCAI, vol. 1, August 1981, 426-428. This paper describes processes of transforming michi-annai-bun, literally street-guide-sentence, into a map on a display device: we first show how such transformation is performed, and then we discuss what it means in terms of understanding natural language expressions.</abstract>
<title confidence="0.992622">GLP: A General Linguistic Processor</title>
<author confidence="0.999608">G Goerz</author>
<affiliation confidence="0.999621">University of Erlangen-Nuernberg</affiliation>
<address confidence="0.9859765">RRZE Martensstr.1 0-8520 Erlangen, WEST GERMANY</address>
<abstract confidence="0.914262428571429">Proc. 7th IJCAI, vol. 1, August 1981, 429-431. GLP is a general linguistic processor for the analysis and generation of natural language, based on a second generation version of the General Syntactic Processor of Kaplan and Kay. It is part of a speech understanding system currently under development at the Computer Science Department of our university.</abstract>
<title confidence="0.998939">Multi-Strategy Construction-Specific Parsing for Flexible Data Base Query and Update</title>
<author confidence="0.999998">Philip J Hayes</author>
<author confidence="0.999998">Jaime G Carbonell</author>
<affiliation confidence="0.9998615">Department of Computer Science Carnegie-Mellon University</affiliation>
<address confidence="0.889184">Schenley Park Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.763408">Proc. 7th IJCAI, vol. 1, August 1981, 432-439. The advantages of a multi-strategy, construction-</note>
<abstract confidence="0.996223631578948">specific approach to parsing in applied natural language processing are explained through an examination of two pilot parsers we have constructed. Our approach exploits domain semantics and prior knowledge of expected constructions, using multiple parsing strategies each optimized to recognize different construction types. It is shown that a multi-strategy approach leads to robust, flexible, and efficient parsing of both grammatical and ungrammatical input in limiteddomain, task-oriented, natural language interfaces. We also describe plans to construct a single, practical, multi-strategy parsing system that combines the best aspects of the two simpler parsers already implemented into a more complex, embedded-constituent control structure. Finally, we discuss some issues in data base access and update, and show that a constructionspecific approach, coupled with a case-structured data base description, offers a promising approach to a unified, interactive data base query and update system.</abstract>
<title confidence="0.973109">A Deterministic Analyzer for the Interpretation of Natural Language Commands</title>
<author confidence="0.997511">Leonardo Lesmo</author>
<author confidence="0.997511">Daniela Magnani</author>
<author confidence="0.997511">Piero Torasso</author>
<affiliation confidence="0.9975045">Instituto di Scienze dell&apos;Informazione Universita di Torino</affiliation>
<address confidence="0.7934145">C.so Massimo D&apos;Azeglio 42 - 10125 Torino, ITALY</address>
<abstract confidence="0.957313588235294">Proc. 7th IJCAI, vol. 1, August 1981, 440-442. This paper describes a system which translates a query in the Italian language into a representation which can be immediately interpreted as a sequence of algebraic operations on a relational data base. The use of a lookahead buffer allows the system to operate deterministically. Different knowledge sources are used to cope with semantics (associated with the lexicon) and syntax (represented as pattern-action rules). These knowledge sources cooperate during the query translation so that independent translations of the command are avoided. Therefore, the term &amp;quot;determinism&amp;quot; is used to mean that all the structures Journal of Computational Linguistics, Volume 7, Number 4, October-December 275 The FINITE STRING Newsletter Abstracts of Current Literature built during the process concur to build the final command representation.</abstract>
<title confidence="0.9608665">A General Semantic Analyzer for Data Base Access</title>
<author confidence="0.964401">B K Boguraev</author>
<author confidence="0.964401">K Sparck Jones</author>
<affiliation confidence="0.84108">Computer Laboratory University of Cambridge Corn Exchange Street</affiliation>
<address confidence="0.996962">Cambridge CB2 3QG, ENGLAND</address>
<abstract confidence="0.952076">Proc. 7th IJCAI, vol. 1, August 1981, 443-445. The paper discusses the design principles and current status of a natural language front end for access to data bases. This is based on the use, first, of a semantically-oriented question analyzer exploiting general, language-wide semantic categories and patterns, rather than data base-specific ones: and, second, of a data base-oriented translation component for obtaining search specifications from the meaning representations for questions derived by the analyzer. This approach is motivated by the desire to reduce the effort of providing data base-specific material for the front end, by the belief that a general analyzer is well suited to the &amp;quot;casual&amp;quot; data base user, and by the assumption that the rich semantic apparatus used will be both adequate as a means of analysis and appropriate as a tool for linking the characterizations of input and data language items. The paper describes this approach in more detail, with emphasis on the existing, tested, analyzer.</abstract>
<title confidence="0.9847215">A Metalanguage Representation of Relational Databases for Deductive Q-A Systems</title>
<author confidence="0.999485">Kurt Konolige</author>
<affiliation confidence="0.9997875">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.997986">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.98518059375">Proc. 7th IJCAI, vol. 1, August 1981, 496-503. This paper presents a method of formally representing the information that exists in a relational database. The primary utility of such a representation is for deductive question-answering systems that must access an existing relational database. To respond intelligently to user inquiries, such systems must have a more complete representation of the domain of discourse than is generally available in the database. The problem that then arises is how to reconcile the information present in the database with the domain representation so that database queries can be derived to answer the user&apos;s inquiries. Here we take the formal approach of describing a relational database as the model of a first-order language. Another first-order language, the metalanguage, is used both to represent the domain of discourse, and to describe the relationship of the database to the domain. This view proves particularly useful in two respects. First, by axiomatizing the database language and its associated model in a metatheory, we are able to describe in a powerful and flexible manner how the database corresponds to the domain of discourse. Secondly, viewing the database as a mechanizable model of the database language enables us to take advantage of the computational properties of database query language processors. Once a database query that is equivalent to an original query is derived, it can be evaluated against the database to determine the truth of the original query. Thus the algebraic operations of the database processor can be incorporated in an elegant way into the deductive process of question-answering.</abstract>
<title confidence="0.991835">Explaining and Justifying Expert Consulting Programs</title>
<author confidence="0.991599">Swartout</author>
<affiliation confidence="0.9999015">Laboratory for Computer Science Massachusetts Institute of Technology</affiliation>
<address confidence="0.99995">Cambridge, Massachusetts 02139</address>
<abstract confidence="0.9834255">Proc. 7th IJCAI, vol. 2, August 1981, 815-823. Traditional methods for explaining programs provide explanations by converting to English the code of the program or traces of the execution of that code. While such methods can provide adequate explanations of what the program does or did, they typically cannot provide justifications of the code without resorting to explanations. That is, such systems cannot tell why what the system is doing is a reasonable thing to be doing. The problem is that the knowledge required to provide these justifications is needed only when the program is being written and does not appear in the code itself. The XPLAIN system uses an automatic programmer to generate the consulting program by refinement from abstract goals. The automatic programmer uses a domain model, consisting of facts about the application domain, and a set of domain principles which drive the refinement process forward. By examining the refinement structure created by the automatic programmer it is possible to provide justifications of the code. This paper discusses the system described above and outlines additional advantages this approach has for explanation.</abstract>
<title confidence="0.63545">Last Steps towards an Ultimate PROLOG Colmerauer, and M. Van Caneghem Groupe d&apos;Intelligence Artificielle</title>
<author confidence="0.401782">Faculte des Sciences de_Luminy</author>
<affiliation confidence="0.994234">Universite d&apos;Aix-Marseille II</affiliation>
<address confidence="0.99542">13288 Marseille Cedex 9, FRANCE</address>
<note confidence="0.992813">Proc. 7th IJCAI, vol. 2, August 1981, 947-948. Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981</note>
<title confidence="0.970575">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.926209571428571">A portable version of PROLOG, an Artificial Intelligence language, is presented. A complete system has been implemented on a micro-computer using a floppy-disk virtual memory. The general methodology of the implementation is discussed in terms of an abstract machine (Micromegas) supporting a language (Candide) in which the PROLOG system is written.</abstract>
<title confidence="0.961417">Six Topics in Search of a Parser: An Overview of Al Language Research</title>
<author confidence="0.999605">Eugene Charniak</author>
<affiliation confidence="0.9999445">Department of Computer Science Brown University</affiliation>
<address confidence="0.999507">Providence, Rhode Island 02912</address>
<abstract confidence="0.96129165">Proc. 7th IJCAI, vol. 2, August 1981, 1079-1087. My purpose in this paper is to give an overview of natural language understanding work within artificial I concentrate on the problem of parsing — going from natural language input to a semantic representation. Naturally, the form of semantic representation is a factor in such discussions, so it will receive some attention as well. Furthermore, I doubt that parsing can be completely isolated from text processing issues, and hence I will touch upon such seemingly non-parsing issues as script application. Nevertheless, the topic is parsing. Unfortunately, to present Al parsing work with any sort of historical accuracy would be to produce a bewildering forest of names (both of people and programs). Rather, I will try to extract from the historical record a group of ideas which I believe can be molded into a coherent framework. Naturally, even within these limitations my remarks will be sketchy — this is an article, not a book.</abstract>
<title confidence="0.99581">Center-Embedding Revisited</title>
<author confidence="0.99991">Michael B Kac</author>
<affiliation confidence="0.9999695">Department of Linguistics University of Minnesota</affiliation>
<address confidence="0.999829">Minneapolis, Minnesota 55455</address>
<abstract confidence="0.979500818181818">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 123-125. The severe comprehension difficulty associated with certain center-embedding constructions is perhaps the best known of psychosyntactic phenomena. Most attempts at explanation have been variations on a single theme — that the c.e. configuration leads to an overload of short-term memory during processing. That this is not the whole story can be seen from considering the fact, rarely noted, that c.e. constructions exist which are understood quite easily. Several examples are discussed.</abstract>
<title confidence="0.9888">The Natural Natural Language Understander</title>
<author confidence="0.999977">Henry Hamburger</author>
<affiliation confidence="0.9948865">Science National Science Foundation</affiliation>
<address confidence="0.947084">1800 G Street, N.W.</address>
<email confidence="0.7139">D.C.</email>
<author confidence="0.999464">Stephen Crain</author>
<affiliation confidence="0.9997215">Department of Computer Sciences The University of Texas</affiliation>
<address confidence="0.998136">Austin, Texas 78712</address>
<abstract confidence="0.955013047619048">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 128-130. This study of natural language comprehension by natural understanding systems (children) is based on a procedural analysis represented in the form of a programming language. To clarify what is cognitively required for a child to respond appropriately to certain expressions in English, we show how these forms can be translated into procedures in a high-level programming language. It is then possible to discuss two kinds of difficulties a natural language form can present to the listener: (1) incompatibility of the form with its associated procedure, and (2) complexity of that procedure. An example of procedure complexity is the nesting of loops, whereas a contributor to incompatibility is a word or contiguous phrase that corresponds to separated pieces of the procedure. We present evidence of both types of difficulty from experiments with children and compare the predictions of our procedural view with those of a less detailed syntactic explanation that has been advanced for a subset of the phenomena.</abstract>
<title confidence="0.9292905">Why Do Children Say &amp;quot;Goed&amp;quot;? A Computer Model of Child Language Generation</title>
<author confidence="0.999972">Mallory Selfridge</author>
<affiliation confidence="0.9999325">Department of EE and CS University of Connecticut</affiliation>
<address confidence="0.997279">Storrs, Connecticut 06268</address>
<abstract confidence="0.979371068965517">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 131-133. An important question in modelling child language generation is why children say regular forms of irregular words, such as &amp;quot;goed,&amp;quot; during development, although they never hear them. Three other general characteristics of children&apos;s generation also require explanation. First, Benedict&apos;s work suggests clearly that comprehension of various aspects of language precede the generation of those aspects. Second, the length of the utterances children say become generally longer as as development proceeds. Third, Wetstone and Friedlander suggest that first children say things in the wrong order, and then say things in the correct order. In order to address these issues, this paper explores the hypothesis that learning to talk is driven by learning to understand. This hypothesis begins by assuming that the principal effect of learning to understand is Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 The FINITE STRING Newsletter Abstracts of Current Literature the development of the lexicon as additional words are learned and their &amp;quot;definitions&amp;quot; are refined and modified. It further assumes that the language generation not learned, but is an innate part of a child&apos;s cognitive repertoire. Finally, it states that the ability to generate grows as the lexicon develops during the development of comprehension. The hypothesis predicts that a computer model which incorporated it would display the characteristics described above.</abstract>
<title confidence="0.974345">Writing with a Computer</title>
<author confidence="0.999933">Ira Goldstein</author>
<affiliation confidence="0.997493">Xerox Palo Alto Research Center</affiliation>
<address confidence="0.997463">3333 Coyote Hill Road Palo Alto, California 94304</address>
<abstract confidence="0.904196230769231">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 145-148. This essay conjectures that an author&apos;s planning process will be facilitated by a tool that represents his plan at various levels of abstraction as a network of subgoals, with the subgoal not necessarily restricted to a linear order. Machine reasoning on such structures has been explored in artificial intelligence research. Our proposal is to make these structures available to the writer as a calculus for representing his essays and to use the computer as an interactive editing tool to manipulate them. MOPs and Learning C.</abstract>
<affiliation confidence="0.981757">Department of Computer Science Yale University</affiliation>
<address confidence="0.996525">New Haven, Connecticut 06520</address>
<abstract confidence="0.965626666666667">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 166-170. This paper is an attempt to sketch out some of what MOPs are about. A MOP is an orderer of scenes. A scene is a memory structure that groups together actions with a common goal, a common time, and some other common thread. It provides a sequence of very general actions. Specific memories are stored scenes, indexed with respect to how they differ from the general action in the scene. Scenes actually point to specific memories. MOPs do not. MOPs merely point to scenes. Scripts are particularly common instantiations of scenes.</abstract>
<title confidence="0.8317935">Shaping Explanations: Effects of Questioning on Text Interpretation</title>
<author confidence="0.806512">Jr</author>
<affiliation confidence="0.9999255">Computer Science Department University of California</affiliation>
<address confidence="0.999629">Irvine, California 92717</address>
<abstract confidence="0.971169047619048">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 193-197. Results in cognitive psychology have shown that readers can be steered away from an otherwise plausible interpretation of a story by extra-textual factors such as the source of the text, the stated reading purpose, interruptions and repetition of questions about the text. For instance, successive repetitions of the same question about a given text will often elicit a series of alternative interpretations of the text. This effect cannot be accounted for by established principles of text processing behavior, such as people&apos;s preference for cohesive and parsimonious representations of text. This paper presents a computer program called MACARTHUR, which models this behavior by varying the depth and direction of its inference pursuit in response to re-questioning, resulting in a series of markedly different interpretations of the same text. In light of the results, some new experiments are suggested in hopes of arriving at a new principle, beyond cohesion and parsimony, to account for the observed text processing behavior.</abstract>
<title confidence="0.990507">Memory in Story Invention</title>
<author confidence="0.999529">Natalie Dehn</author>
<affiliation confidence="0.9823715">Department of Computer Science Yale University</affiliation>
<address confidence="0.99748">New Haven, Connecticut 06520</address>
<abstract confidence="0.98022">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 213-216. a story generating program (under development) being built as a model of how human make up stories. Like TALE-SPIN, requires human-like knowledge of the world, but unlike TALE-SPIN, AUTHOR also requires human-like memory organization of this knowledge. The two of human memory most essential to the AUmodel story generation are: (1) reconstruction, and (2) reminding. The former is responsible for the directed nature of making up stories, the latter for the author&apos;s more &amp;quot;fortuitous&amp;quot; ideas and insights.</abstract>
<title confidence="0.99912">Controlling Parsing by Passing Messages</title>
<author confidence="0.999808">Brian Phillips</author>
<author confidence="0.999808">James Hendler</author>
<affiliation confidence="0.922604">Texas Instruments</affiliation>
<address confidence="0.9868715">P.O. Box 225936, MS 371 Dallas, Texas 75265</address>
<abstract confidence="0.943782565217391">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 228-231. The functional segmentation of linguistic knowledge into rules about form and rules about meaning has been vital in unravelling the complexities of language. However, it does not follow that the process of analysis will respect the same boundaries, and so the very segmentation that provided the insights can be troublesome when one seeks to create a dynamic model of language. We believe that a language understanding system should have the ability to bring syntactic and semantic knowledge to bear on the analysis at many points in the computation. This enables it to resolve the alternatives as soon as possible and prevent the flow of extraneous analyses to later phases. Our approach to creating such a model is to use the notion of Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 The FINITE STRING Newsletter Abstracts of Current Literature a society of communicating, knowledge-based problem-solving experts, called &amp;quot;actors&amp;quot;. These actors can communicate by passing messages to any other actor in the system. This flexible control structure allows actors at any level of the analysis to interact with actors at other levels.</abstract>
<title confidence="0.980162">A Parser with Something for Everyone</title>
<author confidence="0.998763">Eugene Charniak</author>
<affiliation confidence="0.999944">Department of Computer Science Brown University</affiliation>
<address confidence="0.999737">Providence, Rhode Island 02912</address>
<abstract confidence="0.9866619">Proc. 3rd Ann. Conf. Cog. Sci. Soc., Aug. 1981, 231-234. We present a syntactic parser, Paragram, which tries to accommodate three goals. First, it will parse, in a natural way, ungrammatical sentences. Secondly, it aspires to &amp;quot;capture the relevant generalizations,&amp;quot; as in transformational grammar, and thus its rules are in virtual one-to-one correspondence with typical transformational rules. Finally, it promises to be reasonably efficient, especially given certain limited parallel processing capabilities.</abstract>
<title confidence="0.9070235">GLISP: An Efficient, English-Like Programming Language</title>
<author confidence="0.999885">Gordon S Novak Jr</author>
<affiliation confidence="0.999968">Department of Computer Science University of Texas</affiliation>
<address confidence="0.9981">Austin, Texas 78712</address>
<note confidence="0.4909315">Proc. 3rd Ann. Conf. Cog. Sc!. Soc., Aug. 1981, 249-252. My earlier research on computer understanding of</note>
<abstract confidence="0.997720916666667">physics problems, stated in English, has convinced me that English is best viewed as a programming language. That is, an English sentence does not contain the message to be transmitted to the reader, but rather is a program which provides the minimum information necessary for the reader to construct the message from what the reader already knows. Study of the ways in which English permits compact expression of complex ideas reveals several features which would be useful if incorporated into programming languages. GLISP is a LISP-based programming language which permits English-like programs containing definite references.</abstract>
<title confidence="0.9871145">Computerized Language Processing for Multiple Use of Narrative Discharge Summaries</title>
<author confidence="0.786389333333333">Naomi Sager</author>
<author confidence="0.786389333333333">Lynette Hirschman</author>
<author confidence="0.786389333333333">Margaret Lyman Linguistic String Project</author>
<affiliation confidence="0.99792">New York University</affiliation>
<address confidence="0.9560745">251 Mercer Street New York, New York 10012</address>
<note confidence="0.976987">Proc. 2nd Ann. Symp. on Cornp. App!. in Med. Care, IEEE, 1978, 330-343.</note>
<abstract confidence="0.9881245">At New York University, computer programs have been developed that convert natural language medical records into a structured data base, i.e., into a table containing the same information as the stored documents. In this form specific information can be quickly retrieved, and summaries of the different kinds of information in the documents can be automatically generated. The automatic conversion of the information from its free-text form to a tabular form is called information formatting. This paper describes the application of the information formatting programs to a small set of pediatric discharge summaries for hospitalizations due to sickle cell disease. The programs created a table of approximately 50 columns in which each different type of information in the documents appeared under a separate heading. From this, a retrieval program extracted instances where symptoms of possible infection preceded symptoms of painful crisis, as suggested by the literature on sickle cell disease. In answer to more detailed queries the program checked the time-order of findings within one document. The potential use of such tables in continuing medical education and other applications in the hospital setting are discussed.</abstract>
<title confidence="0.721675">Automatic Application of Health Care Criteria to Narrative Patient Records</title>
<author confidence="0.751779333333333">Lynette Hirschman</author>
<author confidence="0.751779333333333">Naomi Sager</author>
<author confidence="0.751779333333333">Margaret Lyman Linguistic String Project</author>
<affiliation confidence="0.997743">New York University</affiliation>
<address confidence="0.971048">251 Mercer Street New York, New York 10012</address>
<note confidence="0.581995333333333">Proc. 3rd Ann. Symp. on Cornp. Appl. in Med. Care, 105-113. This paper describes an experimental computer</note>
<abstract confidence="0.980931166666667">program for the application of health care review criteria to hospital discharge summaries. The use of the computer in this process would make it possible to speed up the routine screening of patient records; it could also facilitate experimental evaluation of alternate proposed audit criteria. The computer program has two components. The first component creates a structured form of the information contained in natural language medical records. It maps the words of each sentence into labelled columns of a table (or format) to the type of medical information contained in each word. This structured information is suitable for use as a data base in many areas of clinical research. The second component consists of a set of retrieval routines, each of which corresponds to a criterion of the health care evaluation form, e.g., &amp;quot;was the patient afebrile at discharge?&amp;quot; The retrieval component is built up in modular fashion, so that basic routines can be used in other applications. The application of this program to a sample Journal of Computational Linguistics, Volume 7, Number 1981 The FINITE STRING Newsletter Abstracts of Current Literature hospital discharge summary is presented and compared to the results obtained by a physician reviewer.</abstract>
<title confidence="0.98738">A CODASYL-Type Schema for Natural Language Medical Records</title>
<author confidence="0.883882">N Sager</author>
<author confidence="0.883882">L Tick</author>
<author confidence="0.883882">G Story</author>
<author confidence="0.883882">L Hirschman Linguistic String Project</author>
<affiliation confidence="0.998557">New York University</affiliation>
<address confidence="0.9562355">251 Mercer Street New York, New York 10012</address>
<note confidence="0.911619333333333">Proc. 4th Ann. Symp. on Comp. Appl. in Med. Care, IEEE, 1980, 1027-1033. This paper describes a CODASYL (network) data-</note>
<abstract confidence="0.99462925">base schema for information derived from narrative clinical reports. The goal of this work is to create an automated process that accepts natural language documents as input and maps this information into a database of a type managed by existing database management systems. The schema described here represents the medical events and facts identified through the natural language processing. This processing decomposes each narrative into a set of elementary assertions, represented as MEDFACT records in the database. Each assertion in turn consists of a subject and a predicate classed according to a limited number of medical event types, e.g., signs/symptoms, laboratory tests, etc. The subject and predicate are represented by EVENT records which are owned by the MED- FACT record associated with the assertion. The CODASYL-type network structure was found to be suitable for expressing most of the relations needed to represent the natural language information. However, special mechanisms were developed for storing the time relations between EVENT records and for recording connections (such as causality) between certain MEDFACT records. This schema has been implemented using the UNIVAC DMS-1100 DBMS.</abstract>
<title confidence="0.874151">Research into Methods for Automatic Classification and Fact Retrieval in Science Subfields</title>
<author confidence="0.928882">N Sager</author>
<author confidence="0.928882">L Hirschman</author>
<author confidence="0.928882">C White</author>
<author confidence="0.928882">C Foster</author>
<author confidence="0.928882">S Wolff</author>
<author confidence="0.928882">R Grad</author>
<author confidence="0.928882">E Fitzpatrick Linguistic String Project</author>
<affiliation confidence="0.998601">New York University</affiliation>
<address confidence="0.9964895">251 Mercer Street New York, New York 10012</address>
<abstract confidence="0.998821421052632">Report No. 13, October 1980, 93 pages. The broad goal of this research was to extend the applicability of techniques for automated language analysis being developed for the processing of information in natural language data stores. The work proceeded on two levels: (1) procedures applicable to English material independent of the subject matter; (2) procedures directed to the special use of language in particular disciplines (the sublanguage of the discipline). Previous work had shown that a computerized grammar of English could be used in a parsing program to obtain the grammatical structure of input sentences; and subsequent processing, using the word classes special to a given subject area, could arrange the information in the sentences in table-like forms (called information formats). With these methods, textual information which heretofore had only been accessible via key words and other word-scanning methods could be organized in tabular form, similar to numerical and scientific data bases. To make these methods more widely applicable the problems inherent in word classification have to be faced. Automatic parsing of English text sentences requires a computerized dictionary that gives parts of speech and other grammatical properties of words; mapping parsed sentences into information formats requires a sublanguage dictionary that gives the subject-matter (= semantic) word class membership of the words. The research investigated computer-based methods for preparing and utilizing such dictionaries. On the sublanguage level, experiments in automatic word class generation and sublanguage word-class &amp;quot;boot strapping&amp;quot; were undertaken. In the latter, we use patterns of sublanguage word class co-occurrence, known from an initial set of texts, to determine the sublanguage word class of new words in subsequent texts.</abstract>
<title confidence="0.97576">Retrieving Time Information from Natural Language Texts</title>
<author confidence="0.8780575">Lynette Hirschman Linguistic String Project</author>
<affiliation confidence="0.998545">New York University</affiliation>
<address confidence="0.9610775">251 Mercer Street New York, New York 10012</address>
<note confidence="0.760846333333333">In Information Retrieval Research, Oddy et al. (eds.), London: Butterworths, 1981, 154-171. An understanding of time relations is central to</note>
<abstract confidence="0.922737677419355">processing information contained in a narrative. A typical narrative is concerned with the relative ordering or progression of events over time, and the information that one would like to retrieve is often of the type &amp;quot;What happened to event x?&amp;quot; or &amp;quot;Did event x precede event y?&amp;quot; Determination of causality also requires a knowledge of time relations, since an event x can cause an event y only if it precedes event y in time. There has been considerable interest in the processing of time information among researchers in artificial intelligence. Several systems have been designed which compute time relations from a structured input of time specifications. The work described here differs from this other work in that it focuses specifically on the problem of extracting time information from coherent natural language text. Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 The FINITE STRING Newsletter Abstracts of Current Literature The program described here was developed in the course of ongoing research in natural language processing at the Linguistic String Project of New York University. This research has been concerned with the creation of a database from natural language input and the retrieval of information from such a database. The work on narrative time was an outgrowth of an experiment on retrieval of information from narrative medical records. Model-theoretic Pragmatics: Dynamic Models and an Application to Presuppositions and lmplicature</abstract>
<author confidence="0.999921">Douglas B Moran</author>
<affiliation confidence="0.999953">Department of Computer Science Oregon State University</affiliation>
<address confidence="0.999348">Corvallis, Oregon 97331</address>
<abstract confidence="0.988144456521739">Univ. of Michigan Ph.D. Dissertation, Cornp. Studies in Formal Ling. N-22, October 1980, 328 pages. Model-theoretic semantics is a computationally attractive formalism for the semantics of natural languages. The logical model can be viewed as a database of information about the world with the evaluation of logical formulas (representing the semantics of sentences) retrieving information from this database. However, this formalism has the limitation that the information in a model is complete and static. To this problem, a formalism called model-theoretic semantics is developed and applied to system given in Montague&apos;s models contain incomplete information and can have information added to them as they are used. The evaluation of logical formulas in a dynamic environment produces effects — termed model-theoretic pragmatics — that do not occur when a conventional model is used. The primary pragmatic effect studied here accounts presupposition and implicature with postulogical formulas that must be true for the model to be reasonably used in the given application. With conventional models, meaning postulates are used to select reasonable models; with dynamic models, they are consistency checks on the information being added to the model. Since only new information is checked, this mechanism produces different behaviors for different models (contexts). Dynamic model-theoretic semantics is nonmonotonic: it is impossible to detect when critical elements are missing from the domains of quantifiers or from the domains of functions being compared, and thus the truth-value of a quantifier expression of an equality test may change when the model expands. A constructed element (a function or set) can have its domain expanded between the time that it is specified (selected) and the time that it is used. A mechanism for maintaining the consistency of an expanded with its original specification is shown to have interesting consequences: it permits procedural — as well as declarative — representations of the information in the model, and it permits inconsistent information to be entered. This mechanism also has the capability of providing a motivated restriction on the kinds of inconsistencies that can occur.</abstract>
<title confidence="0.9663345">Using Semantics in Non-Context-Free Parsing of Montague Grammar</title>
<author confidence="0.999842">David Scott Warren</author>
<affiliation confidence="0.955575">Department of Computer Science State University of New York</affiliation>
<address confidence="0.687793">Brook, New York</address>
<author confidence="0.99985">Joyce Friedman</author>
<affiliation confidence="0.9998455">Department of Computer and Communication Sciences The University of Michigan</affiliation>
<address confidence="0.997368">Ann Arbor, Michigan 48109</address>
<abstract confidence="0.961204684210526">Comp. Studies in Formal Ling. N-27, August 1981, 41 pages. language processing, questions concerning the appropriate interaction of syntax and semantics have long been of interest. Montague grammar and its fully formalized syntax and semantics provide a complete, well-defined context in which these questions can be considered. This paper describes how semantics can be used during parsing to reduce the combinatoric explosion of syntactic ambiguity in Montague grammar. A parsing algorithm, called semantic equivalence parsing, is described and examples of its operation are given. The algorithm is applicable to general non-context-free grammars that include a formal semantic component. The second portion of the paper places semantic equivalence parsing in the context of the very general definition of an interpreted language as a homomorphism between syntactic and semantic algebras.</abstract>
<title confidence="0.7217075">Adaptation of Montague Grammar to the Requirements of Parsing</title>
<author confidence="0.999369">Jan Landsbergen</author>
<affiliation confidence="0.997304">Philips Research Laboratories</affiliation>
<address confidence="0.472447">Eindhoven, THE NETHERLANDS</address>
<note confidence="0.6330065">MC Tract 136, Formal Methods in the Study of Language, Mathematics Centre, Amsterdam, 1981, 399-419. The paper describes a variant of Montague gram-</note>
<abstract confidence="0.9787826">mar, of which the composition rules have analytical counterparts on which a parsing algorithm can be based. Separate attention is given to the consequences of including rule schemes and syntactic variables in the grammar.</abstract>
<note confidence="0.934037">Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981</note>
<title confidence="0.9926645">The FINITE STRING Newsletter Abstracts of Current Literature Framework for Processing Input</title>
<author confidence="0.910217">Weischedel</author>
<affiliation confidence="0.999897">Department of Computer and Information Sciences University of Delaware</affiliation>
<address confidence="0.99356">Newmark, Delaware 19711</address>
<author confidence="0.999816">Norman K Sondheimer</author>
<affiliation confidence="0.9523425">Software Research MS 2G3 Sperry Univac</affiliation>
<address confidence="0.958079">Blue Bell, Pennsylvania 19424</address>
<abstract confidence="0.999202181818182">Technical Report, 1981, 38 pages. If natural language processing systems are ever to pass the Turing test, or if they are ever to achieve natural, cooperative behavior, they must be able to process input that is ill-formed lexically, syntactically, semantically, or pragmatically. Systems must be able to partially understand or at least give specific, appropriate error messages when input does not correspond to their model of language and of context. Out of our own work and the work of others, we propose meta-rules and a control structure under which they are invoked as a framework for processing ill-formed input. The left-hand-side of a meta-rule diagnoses (hypothesizes) a problem with the input as a violated rule of normal processing. The right-handside rewrites a violated rule as a relaxed one and states how processing may be resumed, if at all. Several specific meta-rules are given as examples. a sketch is included of how several significant heuristics developed by others can be formulated as meta-rules. An analysis of the limitations of this framework is also provided.</abstract>
<title confidence="0.863418">Inference and Control in Multiprocessing</title>
<author confidence="0.314706">Environments</author>
<affiliation confidence="0.998963">Department of Computer Science State University of New York at Buffalo</affiliation>
<address confidence="0.999162">Amherst, New York 14226</address>
<abstract confidence="0.997696086956522">Technical Report No. 186, September 1981, 56 pages. The ideas behind inference, as used in Artificial Intelligence (AI) systems, are similar to those of certain control structures used in other areas of computation. This paper discusses those ideas and specifically studies forward, backward, and bi-directional inference; and the data flow concept, lazy evaluation and bi-directional search. A model of computation called the Supplier-Producer-Consumer (SPC) Model, is introduced as a vehicle for making contrasts between pairs of inference and control strategies. Contrasts along another dimension are made in the model by discussing static and eager evaluation schemes. Multiprocessing is an idea which has been implemented differently in different areas of computer science. This paper discusses the benefits of software simulations of multiprocessing on uni-processing systems for these control and inference methods. Finally, bi-directional methods (computation, inference, and search) are discussed. The combination of forward and backward computation methods allows each to assist the other, and suggests new ways for a program to interact with a user.</abstract>
<title confidence="0.998439">Analyzing Intention in Dialogues</title>
<author confidence="0.999398">J F Allen</author>
<author confidence="0.999398">C R Perrault</author>
<affiliation confidence="0.9998405">Department of Computer Science The University of Rochester</affiliation>
<address confidence="0.999092">Rochester, New York 14627</address>
<abstract confidence="0.998574916666667">Report No. TR50, April 1979, 75 pages. This paper describes a model of cooperative behavior and shows how such a model can explain some interesting linguistic behavior. We assume that agents attempt to recognize the plans of other agents and then use this plan when deciding what response to make. In particular, we show that, given a setting in which purposeful dialogues occur, this model can account for responses that provide more information than explicitly requested and for appropriate responses to both short sentence fragments and indirect speech acts.</abstract>
<title confidence="0.936167">Beyond Question-Answering</title>
<author confidence="0.945704">C R Perrault</author>
<author confidence="0.945704">J F Allen</author>
<affiliation confidence="0.992349">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.999506">10 Moulton Street Cambridge, Massachusetts 02238</address>
<note confidence="0.9814005">Technical Report 4644, May 1981. (To appear in Strategies for Natural Language</note>
<author confidence="0.26261">Lehnert Processing</author>
<author confidence="0.26261">Ringle</author>
<abstract confidence="0.995899">in press.) We show that users of question-answering systems expect those systems to be responsive to their unstated plans and goals. Techniques needed to accomplish this should be special cases of more general abilities, in particular, the ability to recognize the user&apos;s plan and to plan a helpful response. We propose and justify a new system architecture embodying this framework, and we illustrate how that architecture is applied in two implemented systems. The first is a questionanswering system, and the second is a simple decisionsupport system, for which both graphic and linguistic means of communication are available.</abstract>
<title confidence="0.9695175">Conversational Coherency in Technical Conversations</title>
<author confidence="0.999974">Rachel Reichman</author>
<affiliation confidence="0.9923155">ISSCO Universite de Geneve</affiliation>
<address confidence="0.6168375">17 Rue de CandoIle CH-1205 Geneve SWITZERLAND</address>
<note confidence="0.9905035">Working Paper 43, 1979. Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981</note>
<title confidence="0.96772">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.992750230769231">An analysis of technical conversations is presented uses the framework of spaces developed for the analysis of social exchanges. Both forms of discourse are shown to display similarities which are brought out by this form of analysis. A number of instances of surface linguistic phenomena, such as deictic &amp;quot;that,&amp;quot; present progressive tense, pronominalization, and clue words such as &amp;quot;it&apos;s like&amp;quot; and &amp;quot;now&amp;quot;, are presented and discussed. These hitherto unexplained phenomena are accounted for in terms of the underlying discourse structure and the resulting state and focus level assignments to discourse constituents.</abstract>
<title confidence="0.972234">Upward Branching Phrase Markers: The State of the Debate</title>
<author confidence="0.999879">G Sampson</author>
<affiliation confidence="0.985481">ISSCO Universite de Geneve</affiliation>
<abstract confidence="0.861871333333333">17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 45, 1980. A 1975 paper by the author (&amp;quot;the single mother argued that the definition of in linguistics should, for reasons both of empirical adequacy and theoretical elegance, be modified to permit both upward as well as downward branching. This paper examines various reactions to this proposal that have been published. Certain criticisms are accepted as valid but not fatal. The rest turn out to be based on a misunderstanding of the original claim.</abstract>
<title confidence="0.9721365">Three Strategic Goals Employed in Conversational Openings</title>
<author confidence="0.999979">M Rosner</author>
<affiliation confidence="0.984967">ISSCO Universite de Geneve</affiliation>
<abstract confidence="0.924543578947368">17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 46, 1981. This paper tries to explain a short transcript of a conversational opening as completely as possible within the framework which takes conversational behaviour as defined by the operation of a sophisticated planning mechanism. To account for conversational openings in general and this transcript in particular, it is argued that a crucial role is played by the satisfaction, for each participant, of three strategic goals relating to attention, identification, and greeting. Additional tactics for gaining information are also described as necessary to account for this transcript. The final analysis employs the definitions of the goals and tactics as defined. It is concluded that many of the deficiencies in the final analysis could be avoided by further investigations aimed at formalizing the explanatory framework.</abstract>
<title confidence="0.9963785">A Computer-Based Teaching Scheme for Creative Writing</title>
<author confidence="0.997937">Mike Sharpies</author>
<affiliation confidence="0.9996215">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.634227">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<note confidence="0.7223065">Proc. 3rd World Conf. on Computers in Education, Lewis and Tagg (eds.), North-Holland, 1981, 483-488.</note>
<abstract confidence="0.9900948">The paper describes a computer-based teaching scheme for creative writing. The scheme, based on a cognitive theory of the writing process, develops children&apos;s meta-linguistic knowledge and applies it to the exploration and improvement of written style. The children use computer programs to generate and transform text and to explore the process of story production. We present an outline description of the scheme and the results of a pilot project with 6 elevenyear-old pupils.</abstract>
<title confidence="0.983297">Microcomputers and Creative Writing</title>
<author confidence="0.99782">Mike Sharpies</author>
<affiliation confidence="0.999605">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.560784">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.94137225">In Microcomputers in Secondary Education, Howe and Ross (Eds.), London: Kogan Page, 1981, 138-157. There are few examples of computer aids, for education in language arts and, of those few, most provide pupils with drill and practice exercises in grammar, spelling, or writing style. The computer programs described in this paper are different. They offer powerful and general learning aids — a sentence generator, a story planner, a word processor/text transformer, an automated thesaurus and dictionary, a spelling corrector — which may be used both by the teacher to demonstrate language construction and by the pupil to compose and alter text. Being tools rather than teaching systems they are not aligned to a particular syllabus and, as the linguistic information is held in simple data files, the programs may be easily modified to manipulate other languages — from Latin to mathematical expressions. The paper begins with a brief history and critique of computer assisted instruction (CAI) followed by an impression of a computer-based &amp;quot;Workshop&amp;quot; for exploring language and creative writing and, lastly, an account of a project with elevenyear-old children who used a prototype part of the workshop in a creative writing course.</abstract>
<note confidence="0.720813">Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>N Sager</author>
<author>L Hirschman</author>
<author>C White</author>
<author>C Foster</author>
<author>S Wolff</author>
<author>R Grad</author>
<author>E Fitzpatrick</author>
</authors>
<title>Model-theoretic Pragmatics: Dynamic Models and an Application to Presuppositions and lmplicature</title>
<marker>Sager, Hirschman, White, Foster, Wolff, Grad, Fitzpatrick, </marker>
<rawString>N. Sager, L. Hirschman, C. White, C. Foster, S. Wolff, R. Grad and E. Fitzpatrick Model-theoretic Pragmatics: Dynamic Models and an Application to Presuppositions and lmplicature</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Douglas</author>
</authors>
<date>1980</date>
<booktitle>Studies in Formal Ling. N-22,</booktitle>
<pages>328</pages>
<institution>Moran Department of Computer Science Oregon State University Corvallis, Oregon 97331 Univ. of Michigan Ph.D. Dissertation, Cornp.</institution>
<marker>Douglas, 1980</marker>
<rawString>Douglas B. Moran Department of Computer Science Oregon State University Corvallis, Oregon 97331 Univ. of Michigan Ph.D. Dissertation, Cornp. Studies in Formal Ling. N-22, October 1980, 328 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Arbor</author>
</authors>
<date></date>
<booktitle>Comp. Studies in Formal Ling. N-27,</booktitle>
<pages>41</pages>
<location>Michigan</location>
<marker>Arbor, </marker>
<rawString>Ann Arbor, Michigan 48109 Comp. Studies in Formal Ling. N-27, August 1981, 41 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ralph</author>
</authors>
<date>1971</date>
<institution>Weischedel Department of Computer and Information Sciences University of Delaware Newmark,</institution>
<location>Delaware</location>
<marker>Ralph, 1971</marker>
<rawString>Ralph M. Weischedel Department of Computer and Information Sciences University of Delaware Newmark, Delaware 19711</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Norman</author>
</authors>
<booktitle>Sondheimer Software Research MS 2G3 Sperry Univac</booktitle>
<marker>Norman, </marker>
<rawString>Norman K. Sondheimer Software Research MS 2G3 Sperry Univac</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blue Bell</author>
</authors>
<date>1942</date>
<tech>Technical Report,</tech>
<volume>38</volume>
<pages>14627</pages>
<institution>Perrault Department of Computer Science The University of Rochester Rochester,</institution>
<location>Pennsylvania</location>
<marker>Bell, 1942</marker>
<rawString>Blue Bell, Pennsylvania 19424 Technical Report, 1981, 38 pages. J.F. Allen and C.R. Perrault Department of Computer Science The University of Rochester Rochester, New York 14627</rawString>
</citation>
<citation valid="true">
<authors>
<author>Report No</author>
</authors>
<date>1979</date>
<tech>TR50,</tech>
<volume>10</volume>
<location>Moulton Street Cambridge, Massachusetts</location>
<marker>No, 1979</marker>
<rawString>Report No. TR50, April 1979, 75 pages. P.R. Cohen, C.R. Perrault, and J.F Allen Bolt Beranek and Newman Inc. 10 Moulton Street Cambridge, Massachusetts 02238 Technical Report 4644, May 1981.</rawString>
</citation>
<citation valid="false">
<booktitle>To appear in Strategies for Natural Language Processing, Lehnert and Ringle (eds.), Erlbaum Assoc., in press.</booktitle>
<marker></marker>
<rawString>(To appear in Strategies for Natural Language Processing, Lehnert and Ringle (eds.), Erlbaum Assoc., in press.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Reichman</author>
</authors>
<title>Upward Branching Phrase Markers: The State of the Debate G. Sampson</title>
<date>1979</date>
<booktitle>ISSCO Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 43,</booktitle>
<marker>Reichman, 1979</marker>
<rawString>Rachel Reichman ISSCO Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 43, 1979. Upward Branching Phrase Markers: The State of the Debate G. Sampson</rawString>
</citation>
<citation valid="true">
<date>1980</date>
<booktitle>ISSCO Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 45,</booktitle>
<marker>1980</marker>
<rawString>ISSCO Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 45, 1980.</rawString>
</citation>
<citation valid="false">
<title>Three Strategic Goals Employed</title>
<booktitle>in Conversational Openings M. Rosner ISSCO</booktitle>
<marker></marker>
<rawString>Three Strategic Goals Employed in Conversational Openings M. Rosner ISSCO</rawString>
</citation>
<citation valid="true">
<date>1981</date>
<booktitle>Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 46,</booktitle>
<marker>1981</marker>
<rawString>Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 46, 1981.</rawString>
</citation>
<citation valid="false">
<title>A Computer-Based Teaching Scheme for Creative Writing Mike Sharpies</title>
<institution>Department of Artificial Intelligence University of Edinburgh</institution>
<marker></marker>
<rawString>A Computer-Based Teaching Scheme for Creative Writing Mike Sharpies Department of Artificial Intelligence University of Edinburgh</rawString>
</citation>
<citation valid="true">
<date>1981</date>
<booktitle>Hill Edinburgh EH1 2QL SCOTLAND Proc. 3rd World Conf. on Computers in Education, Lewis and Tagg (eds.), North-Holland,</booktitle>
<pages>483--488</pages>
<editor>Forrest</editor>
<institution>Department of Artificial Intelligence University of Edinburgh</institution>
<marker>1981</marker>
<rawString>Forrest Hill Edinburgh EH1 2QL SCOTLAND Proc. 3rd World Conf. on Computers in Education, Lewis and Tagg (eds.), North-Holland, 1981, 483-488. Department of Artificial Intelligence University of Edinburgh</rawString>
</citation>
<citation valid="true">
<authors>
<author>Forrest Hill</author>
</authors>
<title>Edinburgh EH1 2QL SCOTLAND</title>
<date>1981</date>
<booktitle>In Microcomputers in Secondary Education, Howe and Ross (Eds.),</booktitle>
<pages>138--157</pages>
<location>London: Kogan Page,</location>
<marker>Hill, 1981</marker>
<rawString>Forrest Hill Edinburgh EH1 2QL SCOTLAND In Microcomputers in Secondary Education, Howe and Ross (Eds.), London: Kogan Page, 1981, 138-157.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>