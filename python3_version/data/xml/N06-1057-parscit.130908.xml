<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000726">
<title confidence="0.997275">
ParaEval: Using Paraphrases to Evaluate Summaries Automatically
</title>
<author confidence="0.999645">
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy
</author>
<affiliation confidence="0.993529">
University of Southern California
Information Sciences Institute
</affiliation>
<address confidence="0.7875595">
4676 Admiralty Way
Marina del Rey, CA 90292-6695
</address>
<email confidence="0.959868">
{liangz, cyl, dragos, Novy} @isi.edu
</email>
<sectionHeader confidence="0.995339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999679357142857">
ParaEval is an automated evaluation
method for comparing reference and peer
summaries. It facilitates a tiered-
comparison strategy where recall-oriented
global optimal and local greedy searches
for paraphrase matching are enabled in
the top tiers. We utilize a domain-
independent paraphrase table extracted
from a large bilingual parallel corpus us-
ing methods from Machine Translation
(MT). We show that the quality of ParaE-
val’s evaluations, measured by correlating
with human judgments, closely resembles
that of ROUGE’s.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968462962963">
Content coverage is commonly measured in sum-
mary comparison to assess how much information
from the reference summary is included in a peer
summary. Both manual and automatic methodolo-
gies have been used. Naturally, there is a great
amount of confidence in manual evaluation since
humans can infer, paraphrase, and use world
knowledge to relate text units with similar mean-
ings, but which are worded differently. Human
efforts are preferred if the evaluation task is easily
conducted and managed, and does not need to be
performed repeatedly. However, when resources
are limited, automated evaluation methods become
more desirable.
For years, the summarization community has
been actively seeking an automatic evaluation
methodology that can be readily applied to various
summarization tasks. ROUGE (Lin and Hovy,
2003) has gained popularity due to its simplicity
and high correlation with human judgments. Even
though validated by high correlations with human
judgments gathered from previous Document Un-
derstanding Conference (DUC) experiments, cur-
rent automatic procedures (Lin and Hovy, 2003;
Hovy et al., 2005) only employ lexical n-gram
matching. The lack of support for word or phrase
matching that stretches beyond strict lexical
matches has limited the expressiveness and utility
of these methods. We need a mechanism that sup-
plements literal matching—i.e. paraphrase and
synonym—and approximates semantic closeness.
In this paper we present ParaEval, an automatic
summarization evaluation method, which facili-
tates paraphrase matching in an overall three-level
comparison strategy. At the top level, favoring
higher coverage in reference, we perform an opti-
mal search via dynamic programming to find
multi-word to multi-word paraphrase matches be-
tween phrases in the reference summary (usually
human-written) and those in the peer summary
(system-generated). The non-matching fragments
from the previous level are then searched by a
greedy algorithm to find single-word para-
phrase/synonym matches. At the third and the low-
est level, we perform literal lexical unigram
matching on the remaining texts. This tiered design
for summary comparison guarantees at least a
ROUGE-1 level of summary content matching if
no paraphrases are found.
The first two levels employ a paraphrase table.
Since manually created multi-word paraphrases-
—phrases determined by humans to be paraphrases
of one another—are not available in sufficient
quantities, we automatically build a paraphrase
</bodyText>
<page confidence="0.979256">
447
</page>
<note confidence="0.995307">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 447–454,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99802525">
table using methods from the Machine Translation
(MT) field. The assumption made in creating this
table is that if two English phrases are translated
into the same foreign phrase with high probability
(shown in the alignment results from a statistically
trained alignment algorithm), then the two English
phrases are paraphrases of each other.
This paper is organized in the following way:
Section 2 introduces previous work in summariza-
tion evaluation; Section 3 describes the motivation
behind this work; paraphrase acquisition is dis-
cussed in Section 4; Section 5 explains in detail
our summary comparison mechanism; Section 6
validates ParaEval with human summary judg-
ments; and we conclude and discuss future work in
Section 7.
</bodyText>
<sectionHeader confidence="0.995778" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99888503125">
There has been considerable work in both manual
and automatic summarization evaluations. Three
most noticeable efforts in manual evaluation are
SEE (Lin and Hovy, 2001), Factoid (Van Halteren
and Teufel, 2003), and the Pyramid method
(Nenkova and Passonneau, 2004).
SEE provides a user-friendly environment in
which human assessors evaluate the quality of
system-produced peer summary by comparing it to
a reference summary. Summaries are represented
by a list of summary units (sentences, clauses,
etc.). Assessors can assign full or partial content
coverage score to peer summary units in compari-
son to the corresponding reference summary units.
Grammaticality can also be graded unit-wise.
The goal of the Factoid work is to compare the
information content of different summaries of the
same text and determine the minimum number of
summaries, which was shown through experimen-
tation to be 20-30, needed to achieve stable con-
sensus among 50 human-written summaries.
The Pyramid method uses identified consen-
sus—a pyramid of phrases created by annota-
tors—from multiple reference summaries as the
gold-standard reference summary. Summary com-
parisons are performed on Summarization Content
Units (SCUs) that are approximately of clause
length.
To facilitate fast summarization system design-
evaluation cycles, ROUGE was created (Lin and
Hovy, 2003). It is an automatic evaluation package
that measures a number of n-gram co-occurrence
</bodyText>
<figure confidence="0.877623636363636">
SCU1: the crime in question was the Lockerbie {Scotland} bombing
1 [for the Lockerbie bombing]
2 [for blowing up] [over Lockerbie, Scotland]
3 [of bombing] [over Lockerbie, Scotland]
4 [was blown up over Lockerbie, Scotland, ]
5 [the bombing of Pan Am Flight 103]
6 [bombing over Lockerbie, Scotland, ]
7 [for Lockerbie bombing]
8 [bombing of Pan Am flight 103 over Lockerbie. ]
9 [linked to the Lockerbie bombing]
10 [in the Lockerbie bombing case. ]
</figure>
<figureCaption confidence="0.999949">
Figure 1. Paraphrases created by Pyramid annotation.
</figureCaption>
<bodyText confidence="0.999497125">
statistics between peer and reference summary
pairs. ROUGE was inspired by BLEU (Papineni et
al., 2001) which was adopted by the machine
translation (MT) community for automatic MT
evaluation. A problem with ROUGE is that the
summary units used in automatic comparison are
of fixed length. A more desirable design is to have
summary units of variable size. This idea was im-
plemented in the Basic Elements (BE) framework
(Hovy et al., 2005) which has not been completed
due to its lack of support for paraphrase matching.
Both ROUGE and BE have been shown to corre-
late well with past DUC human summary judg-
ments, despite incorporating only lexical matching
on summary units (Lin and Hovy, 2003; Hovy et
al., 2005).
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="method">
3 Motivation
</sectionHeader>
<subsectionHeader confidence="0.999925">
3.1 Paraphrase Matching
</subsectionHeader>
<bodyText confidence="0.999782473684211">
An important difference that separates current
manual evaluation methods from their automatic
counterparts is that semantic matching of content
units is performed by human summary assessors.
An essential part of the semantic matching in-
volves paraphrase matching—determining whether
phrases worded differently carry the same semantic
information. This paraphrase matching process is
observed in the Pyramid annotation procedure
shown in (Nenkova and Passonneau, 2004) over
three summary sets (10 summaries each). In the
example shown in Figure 1 (reproduced from
Pyramid results), each of the 10 phrases (numbered
1 to 10) extracted from summary sentences carries
the same semantic content as the overall summary
content unit labeled SCU1 does. Each extracted
phrase is identified as a summary content unit
(SCU). In our work in building an automatic
evaluation procedure that enables paraphrase
</bodyText>
<page confidence="0.997703">
448
</page>
<bodyText confidence="0.991046">
matching, we aim to automatically identify these
10 phrases as paraphrases of one another.
</bodyText>
<subsectionHeader confidence="0.999697">
3.2 Synonymy Relations
</subsectionHeader>
<bodyText confidence="0.999950435897436">
Synonym matching and paraphrase matching are
often mentioned in the same context in discussions
of extending current automated summarization
evaluation methods to incorporate the matching of
semantic units. While evaluating automatically
extracted paraphrases via WordNet (Miller et al.,
1990), Barzilay and McKeown (2001) quantita-
tively validated that synonymy is not the only
source of paraphrasing. We envisage that this
claim is also valid for summary comparisons.
From an in-depth analysis on the manually cre-
ated SCUs of the DUC2003 summary set D30042
(Nenkova and Passonneau, 2004), we find that
54.48% of 1746 cases where a non-stop word from
one SCU did not match with its supposedly hu-
man-aligned pairing SCUs are in need of some
level of paraphrase matching support. For example,
in the first two extracted SCUs (labeled as 1 and 2)
in Figure 1—“for the Lockerbie bombing” and “for
blowing up ... over Lockerbie, Scotland”—no
non-stop word other than the word “Lockerbie”
occurs in both phrases. But these two phrases were
judged to carry the same semantic meaning be-
cause human annotators think the word “bombing”
and the phrase “blowing up” refer to the same ac-
tion, namely the one associated with “explosion.”
However, “bombing” and “blowing up” cannot be
matched through synonymy relations by using
WordNet, since one is a noun and the other is a
verb phrase (if tagged within context). Even when
the search is extended to finding synonyms and
hypernyms for their categorical variants and/or
using other parts of speech (verb for “bombing”
and noun phrase for “blowing up”), a match still
cannot be found.
To include paraphrase matching in summary
evaluation, a collection of less-strict paraphrases
must be created and a matching strategy needs to
be investigated.
</bodyText>
<sectionHeader confidence="0.987882" genericHeader="method">
4 Paraphrase Acquisition
</sectionHeader>
<bodyText confidence="0.999905636363636">
Paraphrases are alternative verbalizations for con-
veying the same information and are required by
many Natural Language Processing (NLP) appli-
cations. In particular, summary creation and
evaluation methods need to recognize paraphrases
and their semantic equivalence. Unfortunately, we
have yet to incorporate into the evaluation frame-
work previous findings in paraphrase identification
and extraction (Barzilay and McKeown, 2001;
Pang et al., 2003; Bannard and Callison-Burch,
2005).
</bodyText>
<subsectionHeader confidence="0.593206">
4.1 Related Work on Paraphrasing
</subsectionHeader>
<bodyText confidence="0.999837285714286">
Three major approaches in paraphrase collection
are manual collection (domain-specific), collection
utilizing existing lexical resources (i.e. WordNet),
and derivation from corpora. Hermjakob et al.
(2002) view paraphrase recognition as
reformulation by pattern recognition. Pang et al.
(2003) use word lattices as paraphrase representa-
tions from semantically equivalent translations
sets. Using parallel corpora, Barzilay and McKe-
own (2001) identify paraphrases from multiple
translations of classical novels, where as Bannard
and Callison-Burch (2005) develop a probabilistic
representation for paraphrases extracted from large
Machine Translation (MT) data sets.
</bodyText>
<subsectionHeader confidence="0.997837">
4.2 Extracting Paraphrases
</subsectionHeader>
<bodyText confidence="0.99981012">
Our method to automatically construct a large do-
main-independent paraphrase collection is based
on the assumption that two different English
phrases of the same meaning may have the same
translation in a foreign language.
Phrase-based Statistical Machine Translation
(SMT) systems analyze large quantities of bilin-
gual parallel texts in order to learn translational
alignments between pairs of words and phrases in
two languages (Och and Ney, 2004). The sentence-
based translation model makes word/phrase align-
ment decisions probabilistically by computing the
optimal model parameters with application of the
statistical estimation theory. This alignment proc-
ess results in a corpus of word/phrase-aligned par-
allel sentences from which we can extract phrase
pairs that are translations of each other. We ran the
alignment algorithm from (Och and Ney, 2003) on
a Chinese-English parallel corpus of 218 million
English words. Phrase pairs are extracted by fol-
lowing the method described in (Och and Ney,
2004) where all contiguous phrase pairs having
consistent alignments are extraction candidates.
The resulting phrase table is of high quality; both
the alignment models and phrase extraction meth-
</bodyText>
<page confidence="0.998405">
449
</page>
<figureCaption confidence="0.999666">
Figure 2. An example of paraphrase extraction.
</figureCaption>
<bodyText confidence="0.999925272727273">
ods have been shown to produce very good results
for SMT. Using these pairs we build paraphrase
sets by joining together all English phrases with
the same Chinese translation. Figure 2 shows an
example word/phrase alignment for two parallel
sentence pairs from our corpus where the phrases
“blowing up” and “bombing” have the same Chi-
nese translation. On the right side of the figure we
show the paraphrase set which contains these two
phrases, which is typical in our collection of ex-
tracted paraphrases.
</bodyText>
<sectionHeader confidence="0.968509" genericHeader="method">
5 Summary Comparison in ParaEval
</sectionHeader>
<bodyText confidence="0.999505">
This section describes the process of comparing a
peer summary against a reference summary and the
summary grading mechanism.
</bodyText>
<subsectionHeader confidence="0.930785">
5.1 Description
</subsectionHeader>
<bodyText confidence="0.999840965517241">
We adopt a three-tier matching strategy for sum-
mary comparison. The score received by a peer
summary is the ratio of the number of reference
words matched to the total number of words in the
reference summary. The total number of matched
reference words is the sum of matched words in
reference throughout all three tiers. At the top
level, favoring high recall coverage, we perform an
optimal search to find multi-word paraphrase
matches between phrases in the reference summary
and those in the peer. Then a greedy search is per-
formed to find single-word paraphrase/synonym
matches among the remaining text. Operations
conducted in these two top levels are marked as
linked rounded rectangles in Figure 3. At the bot-
tom level, we find lexical identity matches, as
marked in rectangles in the example. If no para-
phrases are found, this last level provides a guar-
antee of lexical comparison that is equivalent to
what other automated systems give. In our system,
the bottom level currently performs unigram
matching. Thus, we are ensured with at least a
ROUGE-1 type of summary comparison. Alterna-
tively, equivalence of other ROUGE configura-
tions can replace the ROUGE-1 implementation.
There is no theoretical reason why the first two
levels should not merge. But due to high computa-
tional cost in modeling an optimal search, the sepa-
ration is needed. We explain this in detail below.
</bodyText>
<subsectionHeader confidence="0.997523">
5.2 Multi-Word Paraphrase Matching
</subsectionHeader>
<bodyText confidence="0.999875">
In this section we describe the algorithm that per-
forms the multi-word paraphrase matching be-
tween phrases from reference and peer summaries.
Using the example in Figure 3, this algorithm cre-
ates the phrases shown in the rounded rectangles
and establishes the appropriate links indicating
corresponding paraphrase matches.
</bodyText>
<subsectionHeader confidence="0.879882">
Problem Description
</subsectionHeader>
<bodyText confidence="0.999818833333333">
Measuring content coverage of a peer summary
using a single reference summary requires com-
puting the recall score of how much information
from the reference summary is included in the
peer. A summary unit, either from reference or
peer, cannot be matched for more than once. For
</bodyText>
<figureCaption confidence="0.997843">
Figure 3. Comparison of summaries.
</figureCaption>
<page confidence="0.87217">
450
</page>
<bodyText confidence="0.999723117647059">
example, the phrase “imposed sanctions on Libya”
(r1) in Figure 3’s reference summary was matched
with the peer summary’s “voted sanctions against
Libya” (p1). If later in the peer summary there is
another phrase p2 that is also a paraphrase of r1, the
match of r1 cannot be counted twice. Conversely,
double counting is not permissible for
phrase/words in the peer summary, either.
We conceptualize the comparison of peer
against reference as a task that is to complete over
several time intervals. If the reference summary
contains n sentences, there will be n time intervals,
where at time ti, phrases from a particular sentence
i of the reference summary are being considered
with all possible phrases from the peer summary
for paraphrase matches. A decision needs to be
made at each time interval:
</bodyText>
<listItem confidence="0.996860142857143">
• Do we employ a local greedy match algo-
rithm that is recall generous (preferring more
matched words from reference) towards only the
reference sentence currently being analyzed,
• Or do we need to explore globally, in-
specting all reference sentences and find the best
overall matching combinations?
</listItem>
<figure confidence="0.330733666666667">
Consider the scenario in Figure 4:
1) at t0: L(p1 = r2) &gt; L(p2 = r1) and r2 contains r1.
A local search algorithm leads to match(p1, r2). L() indi-
</figure>
<figureCaption confidence="0.852855875">
cates the number of words in reference matched by the
peer phrase through paraphrase matching and match()
indicates a paraphrase match has occurred (more in the
figure).
2) at t1: L(p1 = r3) &gt; L(p1 = r2). A global algo-
rithm reverses the decision match(p1, r2) made at t0 and
concludes match(p1, r3) and match(p2, r1). A local
search algorithm would have returned no match.
</figureCaption>
<bodyText confidence="0.999590333333333">
Clearly, the global search algorithm achieves
higher overall recall (in words). The matching of
paraphrases between a reference and its peer be-
comes a global optimization problem, maximizing
the content coverage of the peer compared in refer-
ence.
</bodyText>
<subsectionHeader confidence="0.948095">
Solution Model
</subsectionHeader>
<bodyText confidence="0.973581304347826">
We use dynamic programming to derive the solu-
tion of finding the best paraphrase-matching com-
binations. The optimization problem is as follows:
Sentences from a reference summary and a peer
summary can be broken into phrases of various
lengths. A paraphrase lookup table is used to find
whether a reference phrase and a peer phrase are
paraphrases of each other. What is the optimal
paraphrase matching combination of phrases from
reference and peer that gives the highest recall
score (in number of matched reference words) for
this given peer? The solution should be recall ori-
ented (favoring a peer phrase that matches more
reference words than those match less).
Following (Trick, 1997), the solution can be
characterized as:
1) This problem can be divided into n stages
corresponding to the n sentences of the reference
summary. At each stage, a decision is required to
determine the best combination of matched para-
phrases between the reference sentence and the
entire peer summary that results in no double
counting of phrases on the peer side. There is no
double counting of reference phrases across stages
since we are processing one reference sentence at a
time and are finding the best paraphrase matches
using the entire peer summary. As long as there is
no double counting in peers, we are guaranteed to
have none in reference, either.
2) At each stage, we define a number of pos-
sible states as follows. If, out of all possible
phrases of any length extracted from the reference
sentence, m phrases were found to have matching
paraphrases in the peer summary, then a state is
any subset of the m phrases.
3) Since no double counting in matched
phrases/words is allowed in either the reference
summary or the peer summary, the decision of
which phrases (leftover text segments in reference
Pj and ri represent phrases chosen for paraphrase
matching from peer and reference respectively.
Pj = ri indicates that the phrase Pj from peer is
found to be a paraphrase to the phrase ri from
reference.
L(Pj = ri) indicates the number of words matched
by Pj in ri when they are found to be paraphrases of
</bodyText>
<figureCaption confidence="0.7287276">
each other.
Figure 4. Local vs. global paraphrase matching. L(Pj = ri) and L(Pj = ri+1) may not be equal if the
451 number of words in ri, indicated by L(ri), does not
equal to the number of words in ri+1, indicated by
L(ri+1).
</figureCaption>
<bodyText confidence="0.762565037037037">
and in peer) are allowed to match for the next stage
is made in the current stage.
4) Principle of optimality: at a given state, it
is not necessary to know what matches occurred at
previous stages, only on the accumulated recall
score (matched reference words) from previous
stages and what text segments (phrases) in peer
have not been taken/matched in previous stages.
5) There exists a recursive relationship that
identifies the optimal decision for stage s (out of n
total stages), given that stage s+1 has already been
solved.
6) The final stage, n (last sentence in refer-
ence), is solved by choosing the state that has the
highest accumulated recall score and yet resulted
no double counting in any phrase/word in peer the
summary.
Figure 5 demonstrates the optimal solution (12
reference words matched) for the example shown
in Figure 4. We can express the calculations in the
following formulas:
where fy(xb) denotes the optimal recall coverage
(number of words in the reference summary
matched by the phrases from the peer summary) at
state xb in stage y. r(xb) is the recall coverage given
state xb. And c(xb) records the phrases matched in
peer with no double counting, given state xb.
</bodyText>
<figureCaption confidence="0.971768">
Figure 5. Solution for the example in Figure 4.
</figureCaption>
<subsectionHeader confidence="0.998539">
5.3 Synonym Matching
</subsectionHeader>
<bodyText confidence="0.999892466666667">
All paraphrases whose pairings do not involve
multi-word to multi-word matching are called
synonyms in our experiment. Since these phrases
have either a n-to-1 or 1-to-n matching ratio (such
as the phrases “blowing up” and “bombing”), a
greedy algorithm favoring higher recall coverage
reduces the state creation and stage comparison
costs associated with the optimal procedure
(O(m6): O(m3) for state creation, and for 2 stages at
any time)). The paraphrase table described in Sec-
tion 4 is used.
Synonym matching is performed only on parts
of the reference and peer summaries that were not
matched from the multi-word paraphrase-matching
phase.
</bodyText>
<subsectionHeader confidence="0.998378">
5.4 Lexical Matching
</subsectionHeader>
<bodyText confidence="0.999955555555555">
This matching phase performs straightforward
lexical matching, as exemplified by the text frag-
ments marked in rectangles in Figure 3. Unigrams
are used as the units for counting matches in ac-
cordance with the previous two matching phases.
During all three matching phases, we employed
a ROUGE-1 style of counting. Other alternatives,
such as ROUGE-2, ROUGE-SU4, etc., can easily
be adapted to each phase.
</bodyText>
<sectionHeader confidence="0.875415" genericHeader="evaluation">
6 Evaluation of ParaEval
</sectionHeader>
<bodyText confidence="0.999983333333333">
To evaluate and validate the effectiveness of an
automatic evaluation metric, it is necessary to
show that automatic evaluations correlate with
human assessments highly, positively, and consis-
tently (Lin and Hovy, 2003). In other words, an
automatic evaluation procedure should be able to
distinguish good and bad summarization systems
by assigning scores with close resemblance to hu-
mans’ assessments.
</bodyText>
<subsectionHeader confidence="0.999171">
6.1 Document Understanding Conference
</subsectionHeader>
<bodyText confidence="0.999867375">
The Document Understanding Conference has
provided large-scale evaluations on both human-
created and system-generated summaries annually.
Research teams are invited to participate in solving
summarization problems with their systems. Sys-
tem-generated summaries are then assessed by
humans and/or automatic evaluation procedures.
The collection of human judgments on systems and
their summaries has provided a test-bed for devel-
oping and validating automated summary grading
methods (Lin and Hovy, 2003; Hovy et al., 2005).
The correlations reported by ROUGE and BE
show that the evaluation correlations between these
two systems and DUC human evaluations are
much higher on single-document summarization
tasks. One possible explanation is that when sum-
</bodyText>
<page confidence="0.996955">
452
</page>
<bodyText confidence="0.99989">
marizing from only one source (text), both human-
and system-generated summaries are mostly ex-
tractive. The reason for humans to take phrases (or
maybe even sentences) verbatim is that there is less
motivation to abstract when the input is not highly
redundant, in contrast to input for multi-document
summarization tasks, which we speculate allows
more abstracting. ROUGE and BE both facilitate
lexical n-gram matching, hence, achieving amaz-
ing correlations. Since our baseline matching strat-
egy is lexically based when paraphrase matching is
not activated, validation on single-doc summariza-
tion results is not repeated in our experiment.
</bodyText>
<subsectionHeader confidence="0.999597">
6.2 Validation and Discussion
</subsectionHeader>
<bodyText confidence="0.9998582">
We use summary judgments from DUC2003’s
multi-document summarization (MDS) task to
evaluate ParaEval. During DUC2003, participating
systems created short summaries (~100 words) for
30 document sets. For each set, one assessor-
written summary was used as the reference to
compare peer summaries created by 18 automatic
systems (including baselines) and 3 other human-
written summaries. A system ranking was pro-
duced by taking the averaged performance on all
summaries created by systems. This evaluation
process is replicated in our validation setup for
ParaEval. In all, 630 summary pairs were com-
pared. Pearson’s correlation coefficient is com-
puted for the validation tests, using DUC2003
assessors’ results as the gold standard.
Table 1 illustrates the correlation figures from
the DUC2003 test set. ParaEval-para_only shows
the correlation result when using only paraphrase
and synonym matching, without the baseline uni-
gram matching. ParaEval-2 uses multi-word para-
phrase matching and unigram matching, omitting
the greedy synonym-matching phrase. ParaEval-3
incorporates matching at all three granularity lev-
els.
We see that the current implementation of
ParaEval closely resembles the way ROUGE-1
differentiates system-generated summaries. We
believe this is due to the identical calculations of
recall scores. The score that a peer summary re-
ceives from ParaEval depends on the number of
words matched in the reference summary from its
paraphrase, synonym, and unigram matches. The
counting of individual words in reference indicates
a ROUGE-1 design in grading. However, a de-
</bodyText>
<table confidence="0.982901">
DUC-2003 Pearson
ROUGE-1 0.622
ParaEval-para_only 0.41
ParaEval-2 0.651
ParaEval-3 0.657
</table>
<tableCaption confidence="0.999988">
Table 1. Correlation with DUC 2003 MDS results.
</tableCaption>
<bodyText confidence="0.99968076">
tailed examination on individual reference-peer
comparisons shows that paraphrase and synonym
comparisons and matches, in addition to lexical n-
gram matching, do measure a higher level of con-
tent coverage. This is demonstrated in Figure 6a
and b. Strict unigram matching reflects the content
retained by a peer summary mostly in the 0.2-0.4
ranges in recall, shown as dark-colored dots in the
graphs. Allowing paraphrase and synonym match-
ing increases the detection of peer coverage to the
range of 0.3-0.5, shown as light-colored dots.
We conducted a manual evaluation to further
examine the paraphrases being matched. Using 10
summaries from the Pyramid data, we asked three
human subjects to judge the validity of 128 (ran-
domly selected) paraphrase pairs extracted and
identified by ParaEval. Each pair of paraphrases
was coupled with its respective sentences as con-
texts. All paraphrases judged were multi-word.
ParaEval received an average precision of 68.0%.
The complete agreement between judges is 0.582
according to the Kappa coefficient (Cohen, 1960).
In Figure 7, we show two examples that the human
judges consider to be good paraphrases produced
and matched by ParaEval. Judges voiced difficul-
</bodyText>
<figure confidence="0.9941529">
System Summaries: ParaEval vs. ROUGE-1
b).
System-
generated
summaries.
0.3
0.2
0.1
0
DUC2003 Systems
</figure>
<figureCaption confidence="0.9960235">
Figure 6. A detailed look at the scores assigned by
lexical and paraphrase/synonym comparisons.
</figureCaption>
<figure confidence="0.999399333333333">
0.7
Recall (% word match)
0.6
0.5
0.4
ROUGE-1 Scores
ParaEval-3 Scores
a).
Human-
written
summaries.
Recall (% word match)
0.7
0.6
0.5
0.4
0.9
0.2
0.1
0
Human Summaries: ParaEval vs. ROUGE-1
DUC2009 Summary Writers
ROUGE-1 Scores
ParaEval-9 Scores
</figure>
<page confidence="0.961518">
453
</page>
<figureCaption confidence="0.99547">
Figure 7. Paraphrases matched by ParaEval.
</figureCaption>
<bodyText confidence="0.999701375">
ties in determining “semantic equivalence.” There
were cases where paraphrases would be generally
interchangeable but could not be matched because
of non-semantic equivalence in their contexts. And
there were paraphrases that were determined as
matches, but if taken out of context, would not be
direct replacements of each other. These two situa-
tions are where the judges mostly disagreed.
</bodyText>
<sectionHeader confidence="0.993076" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999965029411765">
In this paper, we have described an automatic
summarization evaluation method, ParaEval, that
facilitates paraphrase matching using a large do-
main-independent paraphrase table extracted from
a bilingual parallel corpus. The three-layer match-
ing strategy guarantees a ROUGE-like baseline
comparison if paraphrase matching fails.
The paraphrase extraction module from the cur-
rent implementation of ParaEval does not dis-
criminate among the phrases that are found to be
paraphrases of one another. We wish to incorporate
the probabilistic paraphrase extraction model from
(Bannard and Callison-Burch, 2005) to better ap-
proximate the relations between paraphrases. This
adaptation will also lead to a stochastic model for
the low-level lexical matching and scoring.
We chose English-Chinese MT parallel data be-
cause they are news-oriented which coincides with
the task genre from DUC. However, it is unknown
how large a parallel corpus is sufficient in provid-
ing a paraphrase collection good enough to help
the evaluation process. The quality of the para-
phrase table is also affected by changes in the do-
main and language pair of the MT parallel data.
We plan to use ParaEval to investigate the impact
of these changes on paraphrase quality under the
assumption that better paraphrase collections lead
to better summary evaluation results.
The immediate impact and continuation of the
described work would be to incorporate paraphrase
matching and extraction into the summary creation
process. And with ParaEval, it is possible for us to
evaluate systems that do incorporate some level of
abstraction, especially paraphrasing.
</bodyText>
<sectionHeader confidence="0.996417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984397826087">
Bannard, C. and C. Callison-Burch. 2005. Paraphrasing with bilingual
parallel corpora. Proceedings of ACL-2005.
Barzilay, R. and K. McKeown. 2001. Extracting paraphrases from a
parallel corpus. Proceedings of ACL/EACL-2001.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, R. L. Mercer.
1993. The mathematics of machine translation: Parameter estima-
tion. Computational Linguistics, 19(2): 263—311, 1993.
Cohen, J. 1960. A coefficient of agreement for nominal scales. Edu-
cation and Psychological Measurement, 43(6):37—46.
Diab, M. and P. Resnik. 2002. An unsupervised method for word
sense tagging using parallel corpora. Proceedings of ACL-2002.
DUC. 2001—2005. Document Understanding Conferences.
Hermjakob, U., A. Echihabi, and D. Marcu. 2002. Natural language
based reformulation resource and web exploitation for question
answering. Proceedings of TREC-2002.
Hovy, E, C.Y. Lin, and L. Zhou. 2005. Evaluating DUC 2005 using
basic elements. Proceedings of DUC-2005.
Hovy, E., C.Y. Lin, L. Zhou, and J. Fukumoto. 2005a. Basic Ele-
ments. http://www.isi.edu/~cyl/BE.
Lin, C.Y. 2001. http://www.isi.edu/~cyl/SEE.
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of summaries
using n-gram co-occurrence statistics. Proceedings of the HLT-
2003.
Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller.
1990. Introduction to WordNet: An on-line lexical database. Inter-
national Journal of Lexicography, 3(4): 235—245.
Nenkova, A. and R. Passonneau. 2004. Evaluating content selection in
summarization: the pyramid method. Proceedings of the HLT-
NAACL 2004.
Och, F. J. and H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29(1): 19-
—51, 2003.
Och, F. J. and H. Ney. 2004. The alignment template approach to
statistical machine translation. Computational Linguistics, 30(4),
2004.
Pang, B. , K. Knight and D. Marcu. 2003. Syntax-based alignment of
multiple translations: extracting paraphrases and generating new
sentences. Proceedings of HLT/NAACL-2003.
Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. IBM research report
Bleu: a method for automatic evaluation of machine translation
IBM Research Division Technical Report, RC22176, 2001.
Trick, M. A. 1997. A tutorial on dynamic program-
ming.http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html.
Van Halteren, H. and S. Teufel. 2003. Examining the consensus be-
tween human summaries: initial experiments with factoid analysis.
Proceedings of HLT-2003.
</reference>
<page confidence="0.999176">
454
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.660636">
<title confidence="0.997523">ParaEval: Using Paraphrases to Evaluate Summaries Automatically</title>
<author confidence="0.991786">Liang Zhou</author>
<author confidence="0.991786">Chin-Yew Lin</author>
<author confidence="0.991786">Dragos Stefan Munteanu</author>
<author confidence="0.991786">Eduard</author>
<affiliation confidence="0.998807">University of Southern Information Sciences</affiliation>
<address confidence="0.989922">4676 Admiralty</address>
<author confidence="0.691412">Marina del Rey</author>
<author confidence="0.691412">CA</author>
<email confidence="0.993385">liangz@isi.edu</email>
<email confidence="0.993385">cyl@isi.edu</email>
<email confidence="0.993385">dragos@isi.edu</email>
<email confidence="0.993385">Novy@isi.edu</email>
<abstract confidence="0.998848666666667">ParaEval is an automated evaluation method for comparing reference and peer summaries. It facilitates a tieredcomparison strategy where recall-oriented global optimal and local greedy searches for paraphrase matching are enabled in the top tiers. We utilize a domainindependent paraphrase table extracted from a large bilingual parallel corpus using methods from Machine Translation (MT). We show that the quality of ParaEval’s evaluations, measured by correlating with human judgments, closely resembles that of ROUGE’s.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>Proceedings of ACL-2005.</booktitle>
<contexts>
<context position="10238" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1548" endWordPosition="1551">ummary evaluation, a collection of less-strict paraphrases must be created and a matching strategy needs to be investigated. 4 Paraphrase Acquisition Paraphrases are alternative verbalizations for conveying the same information and are required by many Natural Language Processing (NLP) applications. In particular, summary creation and evaluation methods need to recognize paraphrases and their semantic equivalence. Unfortunately, we have yet to incorporate into the evaluation framework previous findings in paraphrase identification and extraction (Barzilay and McKeown, 2001; Pang et al., 2003; Bannard and Callison-Burch, 2005). 4.1 Related Work on Paraphrasing Three major approaches in paraphrase collection are manual collection (domain-specific), collection utilizing existing lexical resources (i.e. WordNet), and derivation from corpora. Hermjakob et al. (2002) view paraphrase recognition as reformulation by pattern recognition. Pang et al. (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. Using parallel corpora, Barzilay and McKeown (2001) identify paraphrases from multiple translations of classical novels, where as Bannard and Callison-Burch (2005) develop a p</context>
<context position="27860" citStr="Bannard and Callison-Burch, 2005" startWordPosition="4314" endWordPosition="4317">. 7 Conclusion and Future Work In this paper, we have described an automatic summarization evaluation method, ParaEval, that facilitates paraphrase matching using a large domain-independent paraphrase table extracted from a bilingual parallel corpus. The three-layer matching strategy guarantees a ROUGE-like baseline comparison if paraphrase matching fails. The paraphrase extraction module from the current implementation of ParaEval does not discriminate among the phrases that are found to be paraphrases of one another. We wish to incorporate the probabilistic paraphrase extraction model from (Bannard and Callison-Burch, 2005) to better approximate the relations between paraphrases. This adaptation will also lead to a stochastic model for the low-level lexical matching and scoring. We chose English-Chinese MT parallel data because they are news-oriented which coincides with the task genre from DUC. However, it is unknown how large a parallel corpus is sufficient in providing a paraphrase collection good enough to help the evaluation process. The quality of the paraphrase table is also affected by changes in the domain and language pair of the MT parallel data. We plan to use ParaEval to investigate the impact of th</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Bannard, C. and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. Proceedings of ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>Proceedings of ACL/EACL-2001.</booktitle>
<contexts>
<context position="8271" citStr="Barzilay and McKeown (2001)" startWordPosition="1238" endWordPosition="1241">ary content unit labeled SCU1 does. Each extracted phrase is identified as a summary content unit (SCU). In our work in building an automatic evaluation procedure that enables paraphrase 448 matching, we aim to automatically identify these 10 phrases as paraphrases of one another. 3.2 Synonymy Relations Synonym matching and paraphrase matching are often mentioned in the same context in discussions of extending current automated summarization evaluation methods to incorporate the matching of semantic units. While evaluating automatically extracted paraphrases via WordNet (Miller et al., 1990), Barzilay and McKeown (2001) quantitatively validated that synonymy is not the only source of paraphrasing. We envisage that this claim is also valid for summary comparisons. From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. For example, in the first two extracted SCUs (labeled as 1 and 2) in Figure 1—“for the Lockerbie bombing” and “for blowing up ... over Lockerbie</context>
<context position="10184" citStr="Barzilay and McKeown, 2001" startWordPosition="1540" endWordPosition="1543">t be found. To include paraphrase matching in summary evaluation, a collection of less-strict paraphrases must be created and a matching strategy needs to be investigated. 4 Paraphrase Acquisition Paraphrases are alternative verbalizations for conveying the same information and are required by many Natural Language Processing (NLP) applications. In particular, summary creation and evaluation methods need to recognize paraphrases and their semantic equivalence. Unfortunately, we have yet to incorporate into the evaluation framework previous findings in paraphrase identification and extraction (Barzilay and McKeown, 2001; Pang et al., 2003; Bannard and Callison-Burch, 2005). 4.1 Related Work on Paraphrasing Three major approaches in paraphrase collection are manual collection (domain-specific), collection utilizing existing lexical resources (i.e. WordNet), and derivation from corpora. Hermjakob et al. (2002) view paraphrase recognition as reformulation by pattern recognition. Pang et al. (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. Using parallel corpora, Barzilay and McKeown (2001) identify paraphrases from multiple translations of classical novels, </context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Barzilay, R. and K. McKeown. 2001. Extracting paraphrases from a parallel corpus. Proceedings of ACL/EACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, R. L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2): 263—311, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Education and Psychological Measurement,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="26203" citStr="Cohen, 1960" startWordPosition="4068" endWordPosition="4069">ses the detection of peer coverage to the range of 0.3-0.5, shown as light-colored dots. We conducted a manual evaluation to further examine the paraphrases being matched. Using 10 summaries from the Pyramid data, we asked three human subjects to judge the validity of 128 (randomly selected) paraphrase pairs extracted and identified by ParaEval. Each pair of paraphrases was coupled with its respective sentences as contexts. All paraphrases judged were multi-word. ParaEval received an average precision of 68.0%. The complete agreement between judges is 0.582 according to the Kappa coefficient (Cohen, 1960). In Figure 7, we show two examples that the human judges consider to be good paraphrases produced and matched by ParaEval. Judges voiced difficulSystem Summaries: ParaEval vs. ROUGE-1 b). Systemgenerated summaries. 0.3 0.2 0.1 0 DUC2003 Systems Figure 6. A detailed look at the scores assigned by lexical and paraphrase/synonym comparisons. 0.7 Recall (% word match) 0.6 0.5 0.4 ROUGE-1 Scores ParaEval-3 Scores a). Humanwritten summaries. Recall (% word match) 0.7 0.6 0.5 0.4 0.9 0.2 0.1 0 Human Summaries: ParaEval vs. ROUGE-1 DUC2009 Summary Writers ROUGE-1 Scores ParaEval-9 Scores 453 Figure 7</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. 1960. A coefficient of agreement for nominal scales. Education and Psychological Measurement, 43(6):37—46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>P Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>Proceedings of ACL-2002. DUC. 2001—2005. Document Understanding Conferences.</booktitle>
<marker>Diab, Resnik, 2002</marker>
<rawString>Diab, M. and P. Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. Proceedings of ACL-2002. DUC. 2001—2005. Document Understanding Conferences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hermjakob</author>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>Natural language based reformulation resource and web exploitation for question answering.</title>
<date>2002</date>
<booktitle>Proceedings of TREC-2002.</booktitle>
<contexts>
<context position="10478" citStr="Hermjakob et al. (2002)" startWordPosition="1578" endWordPosition="1581">ny Natural Language Processing (NLP) applications. In particular, summary creation and evaluation methods need to recognize paraphrases and their semantic equivalence. Unfortunately, we have yet to incorporate into the evaluation framework previous findings in paraphrase identification and extraction (Barzilay and McKeown, 2001; Pang et al., 2003; Bannard and Callison-Burch, 2005). 4.1 Related Work on Paraphrasing Three major approaches in paraphrase collection are manual collection (domain-specific), collection utilizing existing lexical resources (i.e. WordNet), and derivation from corpora. Hermjakob et al. (2002) view paraphrase recognition as reformulation by pattern recognition. Pang et al. (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. Using parallel corpora, Barzilay and McKeown (2001) identify paraphrases from multiple translations of classical novels, where as Bannard and Callison-Burch (2005) develop a probabilistic representation for paraphrases extracted from large Machine Translation (MT) data sets. 4.2 Extracting Paraphrases Our method to automatically construct a large domain-independent paraphrase collection is based on the assumptio</context>
</contexts>
<marker>Hermjakob, Echihabi, Marcu, 2002</marker>
<rawString>Hermjakob, U., A. Echihabi, and D. Marcu. 2002. Natural language based reformulation resource and web exploitation for question answering. Proceedings of TREC-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
<author>L Zhou</author>
</authors>
<title>Evaluating DUC</title>
<date>2005</date>
<booktitle>Proceedings of DUC-2005.</booktitle>
<contexts>
<context position="1942" citStr="Hovy et al., 2005" startWordPosition="280" endWordPosition="283">and does not need to be performed repeatedly. However, when resources are limited, automated evaluation methods become more desirable. For years, the summarization community has been actively seeking an automatic evaluation methodology that can be readily applied to various summarization tasks. ROUGE (Lin and Hovy, 2003) has gained popularity due to its simplicity and high correlation with human judgments. Even though validated by high correlations with human judgments gathered from previous Document Understanding Conference (DUC) experiments, current automatic procedures (Lin and Hovy, 2003; Hovy et al., 2005) only employ lexical n-gram matching. The lack of support for word or phrase matching that stretches beyond strict lexical matches has limited the expressiveness and utility of these methods. We need a mechanism that supplements literal matching—i.e. paraphrase and synonym—and approximates semantic closeness. In this paper we present ParaEval, an automatic summarization evaluation method, which facilitates paraphrase matching in an overall three-level comparison strategy. At the top level, favoring higher coverage in reference, we perform an optimal search via dynamic programming to find multi</context>
<context position="6623" citStr="Hovy et al., 2005" startWordPosition="993" endWordPosition="996">g] 8 [bombing of Pan Am flight 103 over Lockerbie. ] 9 [linked to the Lockerbie bombing] 10 [in the Lockerbie bombing case. ] Figure 1. Paraphrases created by Pyramid annotation. statistics between peer and reference summary pairs. ROUGE was inspired by BLEU (Papineni et al., 2001) which was adopted by the machine translation (MT) community for automatic MT evaluation. A problem with ROUGE is that the summary units used in automatic comparison are of fixed length. A more desirable design is to have summary units of variable size. This idea was implemented in the Basic Elements (BE) framework (Hovy et al., 2005) which has not been completed due to its lack of support for paraphrase matching. Both ROUGE and BE have been shown to correlate well with past DUC human summary judgments, despite incorporating only lexical matching on summary units (Lin and Hovy, 2003; Hovy et al., 2005). 3 Motivation 3.1 Paraphrase Matching An important difference that separates current manual evaluation methods from their automatic counterparts is that semantic matching of content units is performed by human summary assessors. An essential part of the semantic matching involves paraphrase matching—determining whether phras</context>
<context position="22522" citStr="Hovy et al., 2005" startWordPosition="3517" endWordPosition="3520">gning scores with close resemblance to humans’ assessments. 6.1 Document Understanding Conference The Document Understanding Conference has provided large-scale evaluations on both humancreated and system-generated summaries annually. Research teams are invited to participate in solving summarization problems with their systems. System-generated summaries are then assessed by humans and/or automatic evaluation procedures. The collection of human judgments on systems and their summaries has provided a test-bed for developing and validating automated summary grading methods (Lin and Hovy, 2003; Hovy et al., 2005). The correlations reported by ROUGE and BE show that the evaluation correlations between these two systems and DUC human evaluations are much higher on single-document summarization tasks. One possible explanation is that when sum452 marizing from only one source (text), both humanand system-generated summaries are mostly extractive. The reason for humans to take phrases (or maybe even sentences) verbatim is that there is less motivation to abstract when the input is not highly redundant, in contrast to input for multi-document summarization tasks, which we speculate allows more abstracting. </context>
</contexts>
<marker>Hovy, Lin, Zhou, 2005</marker>
<rawString>Hovy, E, C.Y. Lin, and L. Zhou. 2005. Evaluating DUC 2005 using basic elements. Proceedings of DUC-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
<author>L Zhou</author>
<author>J Fukumoto</author>
</authors>
<date>2005</date>
<note>Basic Elements. http://www.isi.edu/~cyl/BE.</note>
<contexts>
<context position="1942" citStr="Hovy et al., 2005" startWordPosition="280" endWordPosition="283">and does not need to be performed repeatedly. However, when resources are limited, automated evaluation methods become more desirable. For years, the summarization community has been actively seeking an automatic evaluation methodology that can be readily applied to various summarization tasks. ROUGE (Lin and Hovy, 2003) has gained popularity due to its simplicity and high correlation with human judgments. Even though validated by high correlations with human judgments gathered from previous Document Understanding Conference (DUC) experiments, current automatic procedures (Lin and Hovy, 2003; Hovy et al., 2005) only employ lexical n-gram matching. The lack of support for word or phrase matching that stretches beyond strict lexical matches has limited the expressiveness and utility of these methods. We need a mechanism that supplements literal matching—i.e. paraphrase and synonym—and approximates semantic closeness. In this paper we present ParaEval, an automatic summarization evaluation method, which facilitates paraphrase matching in an overall three-level comparison strategy. At the top level, favoring higher coverage in reference, we perform an optimal search via dynamic programming to find multi</context>
<context position="6623" citStr="Hovy et al., 2005" startWordPosition="993" endWordPosition="996">g] 8 [bombing of Pan Am flight 103 over Lockerbie. ] 9 [linked to the Lockerbie bombing] 10 [in the Lockerbie bombing case. ] Figure 1. Paraphrases created by Pyramid annotation. statistics between peer and reference summary pairs. ROUGE was inspired by BLEU (Papineni et al., 2001) which was adopted by the machine translation (MT) community for automatic MT evaluation. A problem with ROUGE is that the summary units used in automatic comparison are of fixed length. A more desirable design is to have summary units of variable size. This idea was implemented in the Basic Elements (BE) framework (Hovy et al., 2005) which has not been completed due to its lack of support for paraphrase matching. Both ROUGE and BE have been shown to correlate well with past DUC human summary judgments, despite incorporating only lexical matching on summary units (Lin and Hovy, 2003; Hovy et al., 2005). 3 Motivation 3.1 Paraphrase Matching An important difference that separates current manual evaluation methods from their automatic counterparts is that semantic matching of content units is performed by human summary assessors. An essential part of the semantic matching involves paraphrase matching—determining whether phras</context>
<context position="22522" citStr="Hovy et al., 2005" startWordPosition="3517" endWordPosition="3520">gning scores with close resemblance to humans’ assessments. 6.1 Document Understanding Conference The Document Understanding Conference has provided large-scale evaluations on both humancreated and system-generated summaries annually. Research teams are invited to participate in solving summarization problems with their systems. System-generated summaries are then assessed by humans and/or automatic evaluation procedures. The collection of human judgments on systems and their summaries has provided a test-bed for developing and validating automated summary grading methods (Lin and Hovy, 2003; Hovy et al., 2005). The correlations reported by ROUGE and BE show that the evaluation correlations between these two systems and DUC human evaluations are much higher on single-document summarization tasks. One possible explanation is that when sum452 marizing from only one source (text), both humanand system-generated summaries are mostly extractive. The reason for humans to take phrases (or maybe even sentences) verbatim is that there is less motivation to abstract when the input is not highly redundant, in contrast to input for multi-document summarization tasks, which we speculate allows more abstracting. </context>
</contexts>
<marker>Hovy, Lin, Zhou, Fukumoto, 2005</marker>
<rawString>Hovy, E., C.Y. Lin, L. Zhou, and J. Fukumoto. 2005a. Basic Elements. http://www.isi.edu/~cyl/BE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<date>2001</date>
<note>http://www.isi.edu/~cyl/SEE.</note>
<marker>Lin, 2001</marker>
<rawString>Lin, C.Y. 2001. http://www.isi.edu/~cyl/SEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>Proceedings of the HLT2003.</booktitle>
<contexts>
<context position="1646" citStr="Lin and Hovy, 2003" startWordPosition="237" endWordPosition="240">sed. Naturally, there is a great amount of confidence in manual evaluation since humans can infer, paraphrase, and use world knowledge to relate text units with similar meanings, but which are worded differently. Human efforts are preferred if the evaluation task is easily conducted and managed, and does not need to be performed repeatedly. However, when resources are limited, automated evaluation methods become more desirable. For years, the summarization community has been actively seeking an automatic evaluation methodology that can be readily applied to various summarization tasks. ROUGE (Lin and Hovy, 2003) has gained popularity due to its simplicity and high correlation with human judgments. Even though validated by high correlations with human judgments gathered from previous Document Understanding Conference (DUC) experiments, current automatic procedures (Lin and Hovy, 2003; Hovy et al., 2005) only employ lexical n-gram matching. The lack of support for word or phrase matching that stretches beyond strict lexical matches has limited the expressiveness and utility of these methods. We need a mechanism that supplements literal matching—i.e. paraphrase and synonym—and approximates semantic clos</context>
<context position="5592" citStr="Lin and Hovy, 2003" startWordPosition="823" endWordPosition="826">compare the information content of different summaries of the same text and determine the minimum number of summaries, which was shown through experimentation to be 20-30, needed to achieve stable consensus among 50 human-written summaries. The Pyramid method uses identified consensus—a pyramid of phrases created by annotators—from multiple reference summaries as the gold-standard reference summary. Summary comparisons are performed on Summarization Content Units (SCUs) that are approximately of clause length. To facilitate fast summarization system designevaluation cycles, ROUGE was created (Lin and Hovy, 2003). It is an automatic evaluation package that measures a number of n-gram co-occurrence SCU1: the crime in question was the Lockerbie {Scotland} bombing 1 [for the Lockerbie bombing] 2 [for blowing up] [over Lockerbie, Scotland] 3 [of bombing] [over Lockerbie, Scotland] 4 [was blown up over Lockerbie, Scotland, ] 5 [the bombing of Pan Am Flight 103] 6 [bombing over Lockerbie, Scotland, ] 7 [for Lockerbie bombing] 8 [bombing of Pan Am flight 103 over Lockerbie. ] 9 [linked to the Lockerbie bombing] 10 [in the Lockerbie bombing case. ] Figure 1. Paraphrases created by Pyramid annotation. statisti</context>
<context position="6876" citStr="Lin and Hovy, 2003" startWordPosition="1037" endWordPosition="1040"> BLEU (Papineni et al., 2001) which was adopted by the machine translation (MT) community for automatic MT evaluation. A problem with ROUGE is that the summary units used in automatic comparison are of fixed length. A more desirable design is to have summary units of variable size. This idea was implemented in the Basic Elements (BE) framework (Hovy et al., 2005) which has not been completed due to its lack of support for paraphrase matching. Both ROUGE and BE have been shown to correlate well with past DUC human summary judgments, despite incorporating only lexical matching on summary units (Lin and Hovy, 2003; Hovy et al., 2005). 3 Motivation 3.1 Paraphrase Matching An important difference that separates current manual evaluation methods from their automatic counterparts is that semantic matching of content units is performed by human summary assessors. An essential part of the semantic matching involves paraphrase matching—determining whether phrases worded differently carry the same semantic information. This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). In the example shown in Figu</context>
<context position="21780" citStr="Lin and Hovy, 2003" startWordPosition="3414" endWordPosition="3417">traightforward lexical matching, as exemplified by the text fragments marked in rectangles in Figure 3. Unigrams are used as the units for counting matches in accordance with the previous two matching phases. During all three matching phases, we employed a ROUGE-1 style of counting. Other alternatives, such as ROUGE-2, ROUGE-SU4, etc., can easily be adapted to each phase. 6 Evaluation of ParaEval To evaluate and validate the effectiveness of an automatic evaluation metric, it is necessary to show that automatic evaluations correlate with human assessments highly, positively, and consistently (Lin and Hovy, 2003). In other words, an automatic evaluation procedure should be able to distinguish good and bad summarization systems by assigning scores with close resemblance to humans’ assessments. 6.1 Document Understanding Conference The Document Understanding Conference has provided large-scale evaluations on both humancreated and system-generated summaries annually. Research teams are invited to participate in solving summarization problems with their systems. System-generated summaries are then assessed by humans and/or automatic evaluation procedures. The collection of human judgments on systems and t</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. Proceedings of the HLT2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>235--245</pages>
<contexts>
<context position="8242" citStr="Miller et al., 1990" startWordPosition="1234" endWordPosition="1237">nt as the overall summary content unit labeled SCU1 does. Each extracted phrase is identified as a summary content unit (SCU). In our work in building an automatic evaluation procedure that enables paraphrase 448 matching, we aim to automatically identify these 10 phrases as paraphrases of one another. 3.2 Synonymy Relations Synonym matching and paraphrase matching are often mentioned in the same context in discussions of extending current automated summarization evaluation methods to incorporate the matching of semantic units. While evaluating automatically extracted paraphrases via WordNet (Miller et al., 1990), Barzilay and McKeown (2001) quantitatively validated that synonymy is not the only source of paraphrasing. We envisage that this claim is also valid for summary comparisons. From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. For example, in the first two extracted SCUs (labeled as 1 and 2) in Figure 1—“for the Lockerbie bombing” and “for </context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4): 235—245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: the pyramid method.</title>
<date>2004</date>
<booktitle>Proceedings of the HLTNAACL</booktitle>
<contexts>
<context position="4509" citStr="Nenkova and Passonneau, 2004" startWordPosition="662" endWordPosition="665"> way: Section 2 introduces previous work in summarization evaluation; Section 3 describes the motivation behind this work; paraphrase acquisition is discussed in Section 4; Section 5 explains in detail our summary comparison mechanism; Section 6 validates ParaEval with human summary judgments; and we conclude and discuss future work in Section 7. 2 Previous Work There has been considerable work in both manual and automatic summarization evaluations. Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). SEE provides a user-friendly environment in which human assessors evaluate the quality of system-produced peer summary by comparing it to a reference summary. Summaries are represented by a list of summary units (sentences, clauses, etc.). Assessors can assign full or partial content coverage score to peer summary units in comparison to the corresponding reference summary units. Grammaticality can also be graded unit-wise. The goal of the Factoid work is to compare the information content of different summaries of the same text and determine the minimum number of summaries, which was shown t</context>
<context position="7402" citStr="Nenkova and Passonneau, 2004" startWordPosition="1109" endWordPosition="1112">man summary judgments, despite incorporating only lexical matching on summary units (Lin and Hovy, 2003; Hovy et al., 2005). 3 Motivation 3.1 Paraphrase Matching An important difference that separates current manual evaluation methods from their automatic counterparts is that semantic matching of content units is performed by human summary assessors. An essential part of the semantic matching involves paraphrase matching—determining whether phrases worded differently carry the same semantic information. This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). In the example shown in Figure 1 (reproduced from Pyramid results), each of the 10 phrases (numbered 1 to 10) extracted from summary sentences carries the same semantic content as the overall summary content unit labeled SCU1 does. Each extracted phrase is identified as a summary content unit (SCU). In our work in building an automatic evaluation procedure that enables paraphrase 448 matching, we aim to automatically identify these 10 phrases as paraphrases of one another. 3.2 Synonymy Relations Synonym matching and paraphrase matching are often me</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Nenkova, A. and R. Passonneau. 2004. Evaluating content selection in summarization: the pyramid method. Proceedings of the HLTNAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11821" citStr="Och and Ney, 2003" startWordPosition="1768" endWordPosition="1771">atistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentencebased translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words. Phrase pairs are extracted by following the method described in (Och and Ney, 2004) where all contiguous phrase pairs having consistent alignments are extraction candidates. The resulting phrase table is of high quality; both the alignment models and phrase extraction meth449 Figure 2. An example of paraphrase extraction. ods have been shown to produce very good results for SMT. Using these pairs we build paraphrase sets by joining together all English phrases with the same Chinese translation. Figure 2 shows an example word/p</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J. and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19-—51, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="11414" citStr="Och and Ney, 2004" startWordPosition="1708" endWordPosition="1711">nard and Callison-Burch (2005) develop a probabilistic representation for paraphrases extracted from large Machine Translation (MT) data sets. 4.2 Extracting Paraphrases Our method to automatically construct a large domain-independent paraphrase collection is based on the assumption that two different English phrases of the same meaning may have the same translation in a foreign language. Phrase-based Statistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentencebased translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words. Phrase pairs are extracted by following the method described in (Och and Ney, 2004) where all contiguous phrase pairs having </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Och, F. J. and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4), 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>Proceedings of HLT/NAACL-2003.</booktitle>
<marker>Knight, Marcu, 2003</marker>
<rawString>Pang, B. , K. Knight and D. Marcu. 2003. Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences. Proceedings of HLT/NAACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>IBM research report Bleu: a method for automatic evaluation of machine translation IBM Research Division</title>
<date>2001</date>
<tech>Technical Report, RC22176,</tech>
<contexts>
<context position="6287" citStr="Papineni et al., 2001" startWordPosition="936" endWordPosition="939">co-occurrence SCU1: the crime in question was the Lockerbie {Scotland} bombing 1 [for the Lockerbie bombing] 2 [for blowing up] [over Lockerbie, Scotland] 3 [of bombing] [over Lockerbie, Scotland] 4 [was blown up over Lockerbie, Scotland, ] 5 [the bombing of Pan Am Flight 103] 6 [bombing over Lockerbie, Scotland, ] 7 [for Lockerbie bombing] 8 [bombing of Pan Am flight 103 over Lockerbie. ] 9 [linked to the Lockerbie bombing] 10 [in the Lockerbie bombing case. ] Figure 1. Paraphrases created by Pyramid annotation. statistics between peer and reference summary pairs. ROUGE was inspired by BLEU (Papineni et al., 2001) which was adopted by the machine translation (MT) community for automatic MT evaluation. A problem with ROUGE is that the summary units used in automatic comparison are of fixed length. A more desirable design is to have summary units of variable size. This idea was implemented in the Basic Elements (BE) framework (Hovy et al., 2005) which has not been completed due to its lack of support for paraphrase matching. Both ROUGE and BE have been shown to correlate well with past DUC human summary judgments, despite incorporating only lexical matching on summary units (Lin and Hovy, 2003; Hovy et a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. IBM research report Bleu: a method for automatic evaluation of machine translation IBM Research Division Technical Report, RC22176, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Trick</author>
</authors>
<title>A tutorial on dynamic programming.http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html.</title>
<date>1997</date>
<contexts>
<context position="17550" citStr="Trick, 1997" startWordPosition="2706" endWordPosition="2707">se-matching combinations. The optimization problem is as follows: Sentences from a reference summary and a peer summary can be broken into phrases of various lengths. A paraphrase lookup table is used to find whether a reference phrase and a peer phrase are paraphrases of each other. What is the optimal paraphrase matching combination of phrases from reference and peer that gives the highest recall score (in number of matched reference words) for this given peer? The solution should be recall oriented (favoring a peer phrase that matches more reference words than those match less). Following (Trick, 1997), the solution can be characterized as: 1) This problem can be divided into n stages corresponding to the n sentences of the reference summary. At each stage, a decision is required to determine the best combination of matched paraphrases between the reference sentence and the entire peer summary that results in no double counting of phrases on the peer side. There is no double counting of reference phrases across stages since we are processing one reference sentence at a time and are finding the best paraphrase matches using the entire peer summary. As long as there is no double counting in p</context>
</contexts>
<marker>Trick, 1997</marker>
<rawString>Trick, M. A. 1997. A tutorial on dynamic programming.http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Van Halteren</author>
<author>S Teufel</author>
</authors>
<title>Examining the consensus between human summaries: initial experiments with factoid analysis.</title>
<date>2003</date>
<booktitle>Proceedings of HLT-2003.</booktitle>
<marker>Van Halteren, Teufel, 2003</marker>
<rawString>Van Halteren, H. and S. Teufel. 2003. Examining the consensus between human summaries: initial experiments with factoid analysis. Proceedings of HLT-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>