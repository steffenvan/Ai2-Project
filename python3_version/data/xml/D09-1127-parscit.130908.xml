<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.983871">
Bilingually-Constrained (Monolingual) Shift-Reduce Parsing
</title>
<author confidence="0.993888">
Liang Huang
</author>
<affiliation confidence="0.933964">
Google Research
</affiliation>
<address confidence="0.9148415">
1350 Charleston Rd.
Mountain View, CA 94043, USA
</address>
<email confidence="0.991971">
lianghuang@google.com
liang.huang.sh@gmail.com
</email>
<author confidence="0.711471">
Wenbin Jiang and Qun Liu
</author>
<affiliation confidence="0.846629666666667">
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.745604">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.997848">
jiangwenbin@ict.ac.cn
</email>
<sectionHeader confidence="0.995621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969272727273">
Jointly parsing two languages has been
shown to improve accuracies on either or
both sides. However, its search space is
much bigger than the monolingual case,
forcing existing approaches to employ
complicated modeling and crude approxi-
mations. Here we propose a much simpler
alternative, bilingually-constrained mono-
lingual parsing, where a source-language
parser learns to exploit reorderings as ad-
ditional observation, but not bothering to
build the target-side tree as well. We show
specifically how to enhance a shift-reduce
dependency parser with alignment fea-
tures to resolve shift-reduce conflicts. Ex-
periments on the bilingual portion of Chi-
nese Treebank show that, with just 3 bilin-
gual features, we can improve parsing ac-
curacies by 0.6% (absolute) for both En-
glish and Chinese over a state-of-the-art
baseline, with negligible (∼6%) efficiency
overhead, thus much faster than biparsing.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999484428571429">
Ambiguity resolution is a central task in Natu-
ral Language Processing. Interestingly, not all lan-
guages are ambiguous in the same way. For exam-
ple, prepositional phrase (PP) attachment is (no-
toriously) ambiguous in English (and related Eu-
ropean languages), but is strictly unambiguous in
Chinese and largely unambiguous Japanese; see
</bodyText>
<listItem confidence="0.881885166666667">
(1a) I [ saw Bill ] [ with a telescope].
wo [ yong wangyuanjin] [kandao le Bi’er].
“I used a telescope to see Bill.”
(1b) I saw [ Bill [ with a telescope]].
wo kandao le [ [ na wangyuanjin ] de Bi’er].
“I saw Bill who had a telescope at hand.”
</listItem>
<figureCaption confidence="0.813984666666667">
Figure 1: PP-attachment is unambiguous in Chi-
nese, which can help English parsing.
Figure 1 for an example.1 It is thus intuitive to use
</figureCaption>
<bodyText confidence="0.996362666666666">
two languages for better disambiguation, which
has been applied not only to this PP-attachment
problem (Fossum and Knight, 2008; Schwartz et
al., 2003), but also to the more fundamental prob-
lem of syntactic parsing which subsumes the for-
mer as a subproblem. For example, Smith and
Smith (2004) and Burkett and Klein (2008) show
that joint parsing (or reranking) on a bitext im-
proves accuracies on either or both sides by lever-
aging bilingual constraints, which is very promis-
ing for syntax-based machine translation which re-
quires (good-quality) parse trees for rule extrac-
tion (Galley et al., 2004; Mi and Huang, 2008).
However, the search space of joint parsing is in-
evitably much bigger than the monolingual case,
</bodyText>
<footnote confidence="0.9111044">
1Chinese uses word-order to disambiguate the attachment
(see below). By contrast, Japanese resorts to case-markers
and the unambiguity is limited: it works for the “V or N”
attachment ambiguities like in Figure 1 (see (Schwartz et al.,
2003)) but not for the “N1 or N2” case (Mitch Marcus, p.c.).
</footnote>
<page confidence="0.81808">
1222
</page>
<note confidence="0.996625">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999966571428571">
forcing existing approaches to employ compli-
cated modeling and crude approximations. Joint
parsing with a simplest synchronous context-free
grammar (Wu, 1997) is O(n6) as opposed to the
monolingual O(n3) time. To make things worse,
languages are non-isomorphic, i.e., there is no 1-
to-1 mapping between tree nodes, thus in practice
one has to use more expressive formalisms such
as synchronous tree-substitution grammars (Eis-
ner, 2003; Galley et al., 2004). In fact, rather than
joint parsing per se, Burkett and Klein (2008) re-
sort to separate monolingual parsing and bilingual
reranking over k2 tree pairs, which covers a tiny
fraction of the whole space (Huang, 2008).
We instead propose a much simpler alterna-
tive, bilingually-constrained monolingual parsing,
where a source-language parser is extended to ex-
ploit the reorderings between languages as addi-
tional observation, but not bothering to build a tree
for the target side simultaneously. To illustrate the
idea, suppose we are parsing the sentence
</bodyText>
<listItem confidence="0.828828">
(1) I saw Bill [PP with a telescope ].
which has 2 parses based on the attachment of PP:
(1a) I [ saw Bill ] [PP with a telescope ].
(1b) I saw [ Bill [PP with a telescope ]].
</listItem>
<bodyText confidence="0.999935722222222">
Both are possible, but with a Chinese translation
the choice becomes clear (see Figure 1), because
a Chinese PP always immediately precedes the
phrase it is modifying, thus making PP-attachment
strictly unambiguous.2 We can thus use Chinese to
help parse English, i.e., whenever we have a PP-
attachment ambiguity, we will consult the Chinese
translation (from a bitext), and based on the align-
ment information, decide where to attach the En-
glish PP. On the other hand, English can help Chi-
nese parsing as well, for example in deciding the
scope of relative clauses which is unambiguous in
English but ambiguous in Chinese.
This method is much simpler than joint pars-
ing because it remains monolingual in the back-
bone, with alignment information merely as soft
evidence, rather than hard constraints since auto-
matic word alignment is far from perfect. It is thus
</bodyText>
<footnote confidence="0.665470333333333">
2to be precise, in Fig. 1(b), the English PP is translated
into a Chinese relative clause, but nevertheless all phrasal
modifiers attach to the immediate right in Mandarin Chinese.
</footnote>
<bodyText confidence="0.9844564">
straightforward to implement within a monolin-
gual parsing algorithm. In this work we choose
shift-reduce dependency parsing for its simplicity
and efficiency. Specifically, we make the following
contributions:
</bodyText>
<listItem confidence="0.987671095238095">
• we develop a baseline shift-reduce depen-
dency parser using the less popular, but clas-
sical, “arc-standard” style (Section 2), and
achieve similar state-of-the-art performance
with the the dominant but complicated “arc-
eager” style of Nivre and Scholz (2004);
• we propose bilingual features based on word-
alignment information to prefer “target-side
contiguity” in resolving shift-reduce conflicts
(Section 3);
• we verify empirically that shift-reduce con-
flicts are the major source of errors, and cor-
rect shift-reduce decisions strongly correlate
with the above bilingual contiguity condi-
tions even with automatic alignments (Sec-
tion 5.3);
• finally, with just three bilingual features,
we improve dependency parsing accuracy
by 0.6% for both English and Chinese over
the state-of-the-art baseline with negligible
(∼6%) efficiency overhead (Section 5.4).
</listItem>
<sectionHeader confidence="0.547103" genericHeader="method">
2 Simpler Shift-Reduce Dependency
</sectionHeader>
<subsectionHeader confidence="0.90378">
Parsing with Three Actions
</subsectionHeader>
<bodyText confidence="0.99970775">
The basic idea of classical shift-reduce parsing
from compiler theory (Aho and Ullman, 1972) is
to perform a left-to-right scan of the input sen-
tence, and at each step, choose one of the two ac-
tions: either shift the current word onto the stack,
or reduce the top two (or more) items on the stack,
replacing them with their combination. This idea
has been applied to constituency parsing, for ex-
ample in Sagae and Lavie (2006), and we describe
below a simple variant for dependency parsing
similar to Yamada and Matsumoto (2003) and the
“arc-standard” version of Nivre (2004).
</bodyText>
<subsectionHeader confidence="0.993143">
2.1 The Three Actions
</subsectionHeader>
<bodyText confidence="0.998171">
Basically, we just need to split the reduce ac-
tion into two symmetric (sub-)actions, reduceL
and reduceR, depending on which one of the two
</bodyText>
<page confidence="0.923577">
1223
</page>
<table confidence="0.998558666666666">
stack queue arcs
previous S wi|Q A
shift S|wi Q A
previous S|st−1|st Q A
reduceL S|st Q A U {(st,st−1)}
reduceR S|st−1 Q A U {(st−1, st)}
</table>
<tableCaption confidence="0.999248">
Table 1: Formal description of the three actions.
</tableCaption>
<bodyText confidence="0.946609625">
Note that shift requires non-empty queue while
reduce requires at least two elements on the stack.
items becomes the head after reduction. More for-
mally, we describe a parser configuration by a tu-
ple (S, Q, A) where S is the stack, Q is the queue
of remaining words of the input, and A is the set
of dependency arcs accumulated so far.3 At each
step, we can choose one of the three actions:
</bodyText>
<listItem confidence="0.8387568">
1. shift: move the head of (a non-empty) queue
Q onto stack S;
2. reduceL: combine the top two items on the
stack, st and st−1 (t &gt; 2), and replace
them with st (as the head), and add a left arc
(st, st−1) to A;
3. reduceR: combine the top two items on the
stack, st and st−1 (t &gt; 2), and replace them
with st−1 (as the head), and add a right arc
(st−1, st) to A.
</listItem>
<bodyText confidence="0.999004166666667">
These actions are summarized in Table 1. The
initial configuration is always (0, w1 ... wn, 0)
with empty stack and no arcs, and the final con-
figuration is (wj, 0, A) where wj is recognized as
the root of the whole sentence, and A encodes a
spanning tree rooted at wj. For a sentence of n
words, there are exactly 2n − 1 actions: n shifts
and n − 1 reductions, since every word must be
pushed onto stack once, and every word except the
root will eventually be popped in a reduction. The
time complexity, as other shift-reduce instances, is
clearly O(n).
</bodyText>
<subsectionHeader confidence="0.999779">
2.2 Example of Shift-Reduce Conflict
</subsectionHeader>
<bodyText confidence="0.9851675">
Figure 2 shows the trace of this paradigm on the
example sentence. For the first two configurations
</bodyText>
<footnote confidence="0.994509666666667">
3a “configuration” is sometimes called a “state” (Zhang
and Clark, 2008), but that term is confusing with the states in
shift-reduce LR/LL parsing, which are quite different.
</footnote>
<figure confidence="0.858934714285714">
0 - I saw Bill with a ...
1 shift I saw Bill with a ...
2 shift I saw Bill with a ...
3 reduceL I saw Bill with a ...
4 shift I saw Bill with a ...
5a reduceR I saw Bill with a ...
5b shift I saw Bill with a ...
</figure>
<figureCaption confidence="0.9468285">
Figure 2: A trace of 3-action shift-reduce on the
example sentence. Shaded words are on stack,
</figureCaption>
<bodyText confidence="0.972315545454545">
while gray words have been popped from stack.
After step (4), the process can take either (5a)
or (5b), which correspond to the two attachments
(1a) and (1b) in Figure 1, respectively.
(0) and (1), only shift is possible since there are
not enough items on the stack for reduction. At
step (3), we perform a reduceL, making word “I”
a modifier of “saw”; after that the stack contains
a single word and we have to shift the next word
“Bill” (step 4). Now we face a shift-reduce con-
flict: we can either combine “saw” and “Bill” in
a reduceR action (5a), or shift “Bill” (5b). We will
use features extracted from the configuration to re-
solve the conflict. For example, one such feature
could be a bigram st o st−1, capturing how likely
these two words are combined; see Table 2 for the
complete list of feature templates we use in this
baseline parser.
We argue that this kind of shift-reduce conflicts
are the major source of parsing errors, since the
other type of conflict, reduce-reduce conflict (i.e.,
whether left or right) is relatively easier to resolve
given the part-of-speech information. For exam-
ple, between a noun and an adjective, the former
is much more likely to be the head (and so is a
verb vs. a preposition or an adverb). Shift-reduce
resolution, however, is more non-local, and often
involves a triple, for example, (saw, Bill, with) for
a typical PP-attachment. On the other hand, if we
indeed make a wrong decision, a reduce-reduce
mistake just flips the head and the modifier, and
often has a more local effect on the shape of the
tree, whereas a shift-reduce mistake always leads
</bodyText>
<page confidence="0.969531">
1224
</page>
<table confidence="0.999316916666667">
Type Features
Unigram st T(st) st oT(st)
st−1 T(st−1) st−1 o T(st−1)
wi T(wi) wi oT(wi)
Bigram st o st−1 T(st) o T(st−1) T(st) o T(wi)
T(st) o st−1 o T(st−1) st o st−1 o T(st−1) st oT(st) o T(st−1)
st oT(st) o st−1 st oT(st) o st−1 o T(st−1)
Trigram T(st) oT(wi) oT(wi+1) T(st−1) oT(st) oT(wi) T(st−2) o T(st−1) oT(st)
st o T(wi) o T(wi+1) T(st−1) o st o T(wi)
Modifier T(st−1) o T(lc(st−1)) oT(st) T(st−1) o T(rc(st−1)) oT(st) T(st−1) oT(st) oT(lc(st))
T(st−1) oT(st) oT(rc(st)) T(st−1) o T(lc(st−1)) o st T(st−1) o T(rc(st−1)) o st
T(st−1) o st o T(lc(st))
</table>
<tableCaption confidence="0.98944">
Table 2: Feature templates of the baseline parser. st, st−1 denote the top and next to top words on the
</tableCaption>
<bodyText confidence="0.9897261">
stack; wi and wi+1 denote the current and next words on the queue. T(·) denotes the POS tag of a
given word, and lc(·) and rc(·) represent the leftmost and rightmost child. Symbol o denotes feature
conjunction. Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR.
to vastly incompatible tree shapes with crossing
brackets (for example, [saw Bill] vs. [Bill with a
telescope]). We will see in Section 5.3 that this
is indeed the case in practice, thus suggesting us
to focus on shift-reduce resolution, which we will
return to with the help of bilingual constraints in
Section 3.
</bodyText>
<subsectionHeader confidence="0.999911">
2.3 Comparison with Arc-Eager
</subsectionHeader>
<bodyText confidence="0.999433363636364">
The three action system was originally described
by Yamada and Matsumoto (2003) (although their
methods require multiple passes over the input),
and then appeared as “arc-standard” in Nivre
(2004), but was argued against in comparison to
the four-action “arc-eager” variant. Most subse-
quent works on shift-reduce or “transition-based”
dependency parsing followed “arc-eager” (Nivre
and Scholz, 2004; Zhang and Clark, 2008), which
now becomes the dominant style. But we argue
that “arc-standard” is preferable because:
</bodyText>
<listItem confidence="0.99717725">
1. in the three action “arc-standard” system, the
stack always contains a list of unrelated sub-
trees recognized so far, with no arcs between
any of them, e.g. (I← saw) and (Bill) in step
4 of Figure 2), whereas the four action “arc-
eager” style can have left or right arrows be-
tween items on the stack;
2. the semantics of the three actions are atomic
</listItem>
<bodyText confidence="0.83987212">
and disjoint, whereas the semantics of 4 ac-
tions are not completely disjoint. For exam-
ple, their Left action assumes an implicit Re-
duce of the left item, and their Right ac-
tion assumes an implicit Shift. Furthermore,
these two actions have non-trivial precondi-
tions which also causes the next problem (see
below). We argue that this is rather compli-
cated to implement.
3. the “arc-standard” scan always succeeds,
since at the end we can always reduce with
empty queue, whereas the “arc-eager” style
sometimes goes into deadends where no ac-
tion can perform (prevented by precondi-
tions, otherwise the result will not be a well-
formed tree). This becomes parsing failures
in practice (Nivre and Scholz, 2004), leaving
more than one fragments on stack.
As we will see in Section 5.1, this simpler
arc-standard system performs equally well with
a state-of-the-art arc-eager system (Zhang and
Clark, 2008) on standard English Treebank pars-
ing (which is never shown before). We argue
that all things being equal, this simpler paradigm
should be preferred in practice. 4
</bodyText>
<subsectionHeader confidence="0.996501">
2.4 Beam Search Extension
</subsectionHeader>
<bodyText confidence="0.999973125">
We also enhance deterministic shift-reduce pars-
ing with beam search, similar to Zhang and Clark
(2008), where k configurations develop in paral-
lel. Pseudocode 1 illustrates the algorithm, where
we keep an agenda V of the current active con-
figurations, and at each step try to extend them by
applying one of the three actions. We then dump
the best k new configurations from the buffer back
</bodyText>
<footnote confidence="0.9107555">
4On the other hand, there are also arguments for “arc-
eager”, e.g., “incrementality”; see (Nivre, 2004; Nivre, 2008).
</footnote>
<page confidence="0.97017">
1225
</page>
<bodyText confidence="0.742337">
Pseudocode 1 beam-search shift-reduce parsing.
</bodyText>
<listItem confidence="0.997717461538462">
1: Input: POS-tagged word sequence wl ... wn
2: start ← (0, wl ... wn, 0) &gt; initial config: empty stack,
no arcs
3: V ← {start} &gt; initial agenda
4: for step ← 1 ... 2n − 1 do
5: BUF ← 0 &gt; buffer for new configs
6: for each config in agenda V do
7: for act E {shift, reduceL, reduceR} do
8: if act is applicable to config then
9: next ← apply act to config
10: insert next into buffer BUF
11: V ← top k configurations of BUF
12: Output: the tree of the best config in V
</listItem>
<bodyText confidence="0.999827666666667">
into the agenda for the next step. The complexity
of this algorithm is O(nk), which subsumes the
determinstic mode as a special case (k = 1).
</bodyText>
<subsectionHeader confidence="0.89062">
2.5 Online Training
</subsectionHeader>
<bodyText confidence="0.999950172413793">
To train the parser we need an “oracle” or gold-
standard action sequence for gold-standard depen-
dency trees. This oracle turns out to be non-unique
for the three-action system (also non-unique for
the four-action system), because left dependents
of a head can be reduced either before or after all
right dependents are reduced. For example, in Fig-
ure 2, “I” is a left dependent of “saw”, and can in
principle wait until “Bill” and “with” are reduced,
and then finally combine with “saw”. We choose
to use the heuristic of “shortest stack” that always
prefers reduceL over shift, which has the effect that
all left dependents are first recognized inside-out,
followed by all right dependents, also inside-out,
which coincides with the head-driven constituency
parsing model of Collins (1999).
We use the popular online learning algorithm
of structured perceptron with parameter averag-
ing (Collins, 2002). Following Collins and Roark
(2004) we also use the “early-update” strategy,
where an update happens whenever the gold-
standard action-sequence falls off the beam, with
the rest of the sequence neglected. As a special
case, for the deterministic mode, updates always
co-occur with the first mistake made. The intuition
behind this strategy is that future mistakes are of-
ten caused by previous ones, so with the parser on
the wrong track, future actions become irrelevant
for learning. See Section 5.3 for more discussions.
</bodyText>
<figure confidence="0.994634">
(a) I saw ✿✿✿✿✿✿✿✿✿ Bill with a telescope .
wo yong wangyuanjin kandao le Bi’er.
c(st−1, st) =+; reduce is correct
(b) I saw ✿✿✿✿✿✿✿✿✿ Bill with a telescope .
wo kandao le na wangyuanjin de Bi’er.
c(st−1, st) =−; reduce is wrong
(c) I saw Bill ✿✿✿✿✿✿✿✿✿✿✿ with✿✿✿a✿✿✿✿✿✿✿✿✿✿ telescope✿ .
wo kandao le na wangyuanjin de Bi’er.
cR(st, wi) =+; shift is correct
(d) I saw Bill ✿✿✿✿✿✿✿✿✿ with✿✿✿a✿✿✿✿✿✿✿✿✿✿ telescope✿ .
wo yong wangyuanjin kandao le Bi’er.
cR(st, wi) =−; shift is wrong
</figure>
<figureCaption confidence="0.997168">
Figure 3: Bilingual contiguity features c(st−1, st)
</figureCaption>
<bodyText confidence="0.857865">
and cR(st, wi) at step (4) in Fig. 2 (facing a shift-
reduce decision). Bold words are currently on
stack while gray ones have been popped. Here the
stack tops are st = Bill, st−1 = saw, and the queue
head is wi = with; underlined texts mark the source
and target spans being considered, and wavy un-
derlines mark the allowed spans (Tab. 3). Red bold
alignment links violate contiguity constraints.
</bodyText>
<sectionHeader confidence="0.974011" genericHeader="method">
3 Soft Bilingual Constraints as Features
</sectionHeader>
<bodyText confidence="0.9521432">
As suggested in Section 2.2, shift-reduce con-
flicts are the central problem we need to address
here. Our intuition is, whenever we face a deci-
sion whether to combine the stack tops st−1 and
st or to shift the current word wi, we will consult
the other language, where the word-alignment in-
formation would hopefully provide a preference,
as in the running example of PP-attachment (see
Figure 1). We now develop this idea into bilingual
contiguity features.
</bodyText>
<page confidence="0.952205">
1226
</page>
<subsectionHeader confidence="0.920993">
3.1 A Pro-Reduce Feature c(st−1, st)
</subsectionHeader>
<bodyText confidence="0.999976277777778">
Informally, if the correct decision is a reduction,
then it is likely that the corresponding words of
st−1 and st on the target-side should also form a
contiguous span. For example, in Figure 3(a), the
source span of a reduction is [saw .. Bill], which
maps onto [kandao ... Bi’er] on the Chinese side.
This target span is contiguous, because no word
within this span is aligned to a source word out-
side of the source span. In this case we say feature
c(st−1, st) =+, which encourages “reduce”.
However, in Figure 3(b), the source span is still
[saw .. Bill], but this time maps onto a much
longer span on the Chinese side. This target span
is discontiguous, since the Chinese words na and
wangyuanjin are alinged to English “with” and
“telescope”, both of which fall outside of the
source span. In this case we say feature c(st−1, st)
=−, which discourages “reduce” .
</bodyText>
<subsectionHeader confidence="0.976349">
3.2 A Pro-Shift Feature cR(st, wi)
</subsectionHeader>
<bodyText confidence="0.999956333333333">
Similarly, we can develop another feature
cR(st, wi) for the shift action. In Figure 3(c),
when considering shifting “with”, the source
span becomes [Bill .. with] which maps to [na
.. Bi’er] on the Chinese side. This target span
looks like discontiguous in the above definition
with wangyuanjin aligned to “telescope”, but we
tolerate this case for the following reasons. There
is a crucial difference between shift and reduce:
in a shift, we do not know yet the subtree spans
(unlike in a reduce we are always combining two
well-formed subtrees). The only thing we are
sure of in a shift action is that st and wi will be
combined before st−1 and st are combined (Aho
and Ullman, 1972), so we can tolerate any target
word aligned to source word still in the queue,
but do not allow any target word aligned to an
already recognized source word. This explains
the notational difference between cR(st, wi) and
c(st−1, st), where subscript “R” means “right
contiguity”.
As a final example, in Figure 3(d), Chinese
word kandao aligns to “saw”, which is already
recognized, and this violates the right contiguity.
So cR(st, wi) =−, suggesting that shift is probably
wrong. To be more precise, Table 3 shows the for-
mal definitions of the two features. We basically
</bodyText>
<table confidence="0.8942174">
feature f source target allowed
span sp span tp span ap
c(st−1, st) [st−1..st] M(sp) [st−1..st]
cR(st, wi) [st..wi] M(sp) [st..wn]
f = + iff. M−1(M(sp)) C ap
</table>
<tableCaption confidence="0.959708">
Table 3: Formal definition of bilingual features.
</tableCaption>
<bodyText confidence="0.99136475">
M(·) is maps a source span to the target language,
and M−1(·) is the reverse operation mapping back
to the source language.
map a source span sp to its target span M(sp),
and check whether its reverse image back onto the
source language M−1(M(sp)) falls inside the al-
lowed span ap. For cR(st, wi), the allowed span
extends to the right end of the sentence.5
</bodyText>
<subsectionHeader confidence="0.998453">
3.3 Variations and Implementation
</subsectionHeader>
<bodyText confidence="0.99995675">
To conclude so far, we have got two alignment-
based features, c(st−1, st) correlating with reduce,
and cR(st, wi) correlating with shift. In fact, the
conjunction of these two features,
</bodyText>
<equation confidence="0.752333">
c(st−1, st) O cR(st, wi)
</equation>
<bodyText confidence="0.80335">
is another feature with even stronger discrimina-
tion power. If
</bodyText>
<equation confidence="0.831841">
c(st−1, st) O cR(st, wi) = + O −
it is strongly recommending reduce, while
c(st−1, st) O cR(st, wi) = − O +
</equation>
<bodyText confidence="0.9370716875">
is a very strong signal for shift. So in total we got
three bilingual feature (templates), which in prac-
tice amounts to 24 instances (after cross-product
with {−, +} and the three actions). We show in
Section 5.3 that these features do correlate with
the correct shift/reduce actions in practice.
The naive implemention of bilingual feature
computation would be of O(kn2) complexity
in the worse case because when combining the
largest spans one has to scan over the whole sen-
tence. We envision the use of a clever datastructure
would reduce the complexity, but leave this to fu-
ture work, as the experiments (Table 8) show that
5Our definition implies that we only consider faithful
spans to be contiguous (Galley et al., 2004). Also note that
source spans include all dependents of st and st−1.
</bodyText>
<page confidence="0.988521">
1227
</page>
<bodyText confidence="0.999506">
the parser is only marginally (∼6%) slower with
the new bilingual features. This is because the ex-
tra work, with just 3 bilingual features, is not the
bottleneck in practice, since the extraction of the
vast amount of other features in Table 2 dominates
the computation.
</bodyText>
<sectionHeader confidence="0.999305" genericHeader="method">
4 Related Work in Grammar Induction
</sectionHeader>
<bodyText confidence="0.9998394">
Besides those cited in Section 1, there are some
other related work on using bilingual constraints
for grammar induction (rather than parsing). For
example, Hwa et al. (2005) use simple heuris-
tics to project English trees to Spanish and Chi-
nese, but get discouraging accuracy results learned
from those projected trees. Following this idea,
Ganchev et al. (2009) and Smith and Eisner (2009)
use constrained EM and parser adaptation tech-
niques, respectively, to perform more principled
projection, and both achieve encouraging results.
Our work, by constrast, never uses bilingual
tree pairs not tree projections, and only uses word
alignment alone to enhance a monolingual gram-
mar, which learns to prefer target-side contiguity.
</bodyText>
<sectionHeader confidence="0.99924" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.975554">
5.1 Baseline Parser
</subsectionHeader>
<bodyText confidence="0.99972665">
We implement our baseline monolingual parser (in
C++) based on the shift-reduce algorithm in Sec-
tion 2, with feature templates from Table 2. We
evaluate its performance on the standard Penn En-
glish Treebank (PTB) dependency parsing task,
i.e., train on sections 02-21 and test on section 23
with automatically assigned POS tags (at 97.2%
accuracy) using a tagger similar to Collins (2002),
and using the headrules of Yamada and Mat-
sumoto (2003) for conversion into dependency
trees. We use section 22 as dev set to deter-
mine the optimal number of iterations in per-
ceptron training. Table 4 compares our baseline
against the state-of-the-art graph-based (McDon-
ald et al., 2005) and transition-based (Zhang and
Clark, 2008) approaches, and confirms that our
system performs at the same level with those state-
of-the-art, and runs extremely fast in the determin-
istic mode (k=1), and still quite fast in the beam-
search mode (k=16).
</bodyText>
<table confidence="0.382161">
parser accuracy secs/sent
McDonald et al. (2005) 90.7 0.150
Zhang and Clark (2008) 91.4 0.195
our baseline at k=1 90.2 0.009
our baseline at k=16 91.3 0.125
</table>
<tableCaption confidence="0.644331">
Table 4: Baseline parser performance on standard
Penn English Treebank dependency parsing task.
The speed numbers are not exactly comparable
since they are reported on different machines.
</tableCaption>
<table confidence="0.999601333333333">
Training Dev Test
CTB Articles 1-270 301-325 271-300
Bilingual Paris 2745 273 290
</table>
<tableCaption confidence="0.9981025">
Table 5: Training, dev, and test sets from bilingual
Chinese Treebank a` la Burkett and Klein (2008).
</tableCaption>
<subsectionHeader confidence="0.999342">
5.2 Bilingual Data
</subsectionHeader>
<bodyText confidence="0.999994409090909">
The bilingual data we use is the translated por-
tion of the Penn Chinese Treebank (CTB) (Xue
et al., 2002), corresponding to articles 1-325 of
PTB, which have English translations with gold-
standard parse trees (Bies et al., 2007). Table 5
shows the split of this data into training, devel-
opment, and test subsets according to Burkett and
Klein (2008). Note that not all sentence pairs could
be included, since many of them are not one-
to-one aligned at the sentence level. Our word-
alignments are generated from the HMM aligner
of Liang et al. (2006) trained on approximately
1.7M sentence pairs (provided to us by David Bur-
kett, p.c.). This aligner outputs “soft alignments”,
i.e., posterior probabilities for each source-target
word pair. We use a pruning threshold of 0.535 to
remove low-confidence alignment links,6 and use
the remaining links as hard alignments; we leave
the use of alignment probabilities to future work.
For simplicity reasons, in the following exper-
iments we always supply gold-standard POS tags
as part of the input to the parser.
</bodyText>
<subsectionHeader confidence="0.999672">
5.3 Testing our Hypotheses
</subsectionHeader>
<bodyText confidence="0.999069666666667">
Before evaluating our bilingual approach, we need
to verify empirically the two assumptions we
made about the parser in Sections 2 and 3:
</bodyText>
<footnote confidence="0.992134">
6and also removing notoriously bad links in {the, a, an}x
{de, le} following Fossum and Knight (2008).
</footnote>
<page confidence="0.930005">
1228
</page>
<table confidence="0.994932">
sh ⊲ re re ⊲ sh sh-re re-re
# 92 98 190 7
% 46.7% 49.7% 96.4% 3.6%
</table>
<tableCaption confidence="0.914176">
Table 6: [Hypothesis 1] Error distribution in the
</tableCaption>
<bodyText confidence="0.97395">
baseline model (k = 1) on English dev set.
“sh ⊲ re” means “should shift, but reduced”. Shift-
reduce conflicts overwhelmingly dominate.
</bodyText>
<listItem confidence="0.936349666666667">
1. (monolingual) shift-reduce conflict is the ma-
jor source of errors while reduce-reduce con-
flict is a minor issue;
2. (bilingual) the gold-standard decisions of
shift or reduce should correlate with contigu-
ities of c(st−1, st), and of cR(st, wi).
</listItem>
<bodyText confidence="0.999549576923077">
Hypothesis 1 is verified in Table 6, where we
count all the first mistakes the baseline parser
makes (in the deterministic mode) on the En-
glish dev set (273 sentences). In shift-reduce pars-
ing, further mistakes are often caused by previ-
ous ones, so only the first mistake in each sen-
tence (if there is one) is easily identifiable;7 this
is also the argument for “early update” in apply-
ing perceptron learning to these incremental pars-
ing algorithms (Collins and Roark, 2004) (see also
Section 2). Among the 197 first mistakes (other
76 sentences have perfect output), the vast ma-
jority, 190 of them (96.4%), are shift-reduce er-
rors (equally distributed between shift-becomes-
reduce and reduce-becomes-shift), and only 7
(3.6%) are due to reduce-reduce conflicts.8 These
statistics confirm our intuition that shift-reduce de-
cisions are much harder to make during parsing,
and contribute to the overwhelming majority of er-
rors, which is studied in the next hypothesis.
Hypothesis 2 is verified in Table 7. We take
the gold-standard shift-reduce sequence on the En-
glish dev set, and classify them into the four cat-
egories based on bilingual contiguity features: (a)
c(st−1, st), i.e. whether the top 2 spans on stack
is contiguous, and (b) cR(st, wi), i.e. whether the
</bodyText>
<footnote confidence="0.9137935">
7to be really precise one can define “independent mis-
takes” as those not affected by previous ones, i.e., errors
made after the parser recovers from previous mistakes; but
this is much more involved and we leave it to future work.
8Note that shift-reduce errors include those due to the
non-uniqueness of oracle, i.e., between some reduceL and
shift. Currently we are unable to identify “genuine” errors
that would result in an incorrect parse. See also Section 2.5.
</footnote>
<table confidence="0.999427666666667">
c(st−1, st) cR(st, wi) shift reduce
+ — 172 « 1,209
— + 1,432 &gt; 805
+ + 4,430 — 3,696
— — 525 — 576
total 6,559 = 6,286
</table>
<tableCaption confidence="0.93091325">
Table 7: [Hyp. 2] Correlation of gold-standard
shift/reduce decisions with bilingual contiguity
conditions (on English dev set). Note there is al-
ways one more shift than reduce in each sentence.
</tableCaption>
<bodyText confidence="0.999221958333333">
stack top is contiguous with the current word wi.
According to discussions in Section 3, when (a) is
contiguous and (b) is not, it is a clear signal for
reduce (to combine the top two elements on the
stack) rather than shift, and is strongly supported
by the data (first line: 1209 reduces vs. 172 shifts);
and while when (b) is contiguous and (a) is not,
it should suggest shift (combining st and wi be-
fore st−1 and st are combined) rather than reduce,
and is mildly supported by the data (second line:
1432 shifts vs. 805 reduces). When (a) and (b) are
both contiguous or both discontiguous, it should
be considered a neutral signal, and is also consis-
tent with the data (next two lines). So to conclude,
this bilingual hypothesis is empirically justified.
On the other hand, we would like to note that
these correlations are done with automatic word
alignments (in our case, from the Berkeley aligner)
which can be quite noisy. We suspect (and will fin-
ish in the future work) that using manual align-
ments would result in a better correlation, though
for the main parsing results (see below) we can
only afford automatic alignments in order for our
approach to be widely applicable to any bitext.
</bodyText>
<sectionHeader confidence="0.781792" genericHeader="evaluation">
5.4 Results
</sectionHeader>
<bodyText confidence="0.9999869">
We incorporate the three bilingual features (again,
with automatic alignments) into the baseline
parser, retrain it, and test its performance on the
English dev set, with varying beam size. Table 8
shows that bilingual constraints help more with
larger beams, from almost no improvement with
the deterministic mode (k=1) to +0.5% better with
the largest beam (k=16). This could be explained
by the fact that beam-search is more robust than
the deterministic mode, where in the latter, if our
</bodyText>
<page confidence="0.971465">
1229
</page>
<table confidence="0.998898142857143">
k baseline +bilingual
accuracy time (s) accuracy time (s)
1 84.58 0.011 84.67 0.012
2 85.30 0.025 85.62 0.028
4 85.42 0.040 85.81 0.044
8 85.50 0.081 85.95 0.085
16 85.57 0.158 86.07 0.168
</table>
<tableCaption confidence="0.7947656">
Table 8: Effects of beam size k on efficiency and
accuracy (on English dev set). Time is average
per sentence (in secs). Bilingual constraints show
more improvement with larger beams, with a frac-
tional efficiency overhead over the baseline.
</tableCaption>
<table confidence="0.997358666666667">
English Chinese
monolingual baseline 86.9 85.7
+bilingual features 87.5 86.3
improvement +0.6 +0.6
signficance level p &lt; 0.05 p &lt; 0.08
Berkeley parser 86.1 87.9
</table>
<tableCaption confidence="0.913467">
Table 9: Final results of dependency accuracy (%)
on the test set (290 sentences, beam size k=16).
</tableCaption>
<bodyText confidence="0.999850625">
bilingual features misled the parser into a mistake,
there is no chance of getting back, while in the
former multiple configurations are being pursued
in parallel. In terms of speed, both parsers run pro-
portionally slower with larger beams, as the time
complexity is linear to the beam-size. Computing
the bilingual features further slows it down, but
only fractionally so (just 1.06 times as slow as the
baseline at k=16), which is appealing in practice.
By contrast, Burkett and Klein (2008) reported
their approach of “monolingual k-best parsing fol-
lowed by bilingual k2-best reranking” to be “3.8
times slower” than monolingual parsing.
Our final results on the test set (290 sentences)
are summarized in Table 9. On both English
and Chinese, the addition of bilingual features
improves dependency arc accuracies by +0.6%,
which is mildly significant using the Z-test of
Collins et al. (2005). We also compare our results
against the Berkeley parser (Petrov and Klein,
2007) as a reference system, with the exact same
setting (i.e., trained on the bilingual data, and test-
ing using gold-standard POS tags), and the result-
ing trees are converted into dependency via the
same headrules. We use 5 iterations of split-merge
grammar induction as the 6th iteration overfits the
small training set. The result is worse than our
baseline on English, but better than our bilingual
parser on Chinese. The discrepancy between En-
glish and Chinese is probably due to the fact that
our baseline feature templates (Table 2) are engi-
neered on English not Chinese.
</bodyText>
<sectionHeader confidence="0.996626" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999901481481482">
We have presented a novel parsing paradigm,
bilingually-constrained monolingual parsing,
which is much simpler than joint (bi-)parsing, yet
still yields mild improvements in parsing accuracy
in our preliminary experiments. Specifically,
we showed a simple method of incorporating
alignment features as soft evidence on top of a
state-of-the-art shift-reduce dependency parser,
which helped better resolve shift-reduce conflicts
with fractional efficiency overhead.
The fact that we managed to do this with only
three alignment features is on one hand encour-
aging, but on the other hand leaving the bilingual
feature space largely unexplored. So we will en-
gineer more such features, especially with lexical-
ization and soft alignments (Liang et al., 2006),
and study the impact of alignment quality on pars-
ing improvement. From a linguistics point of view,
we would like to see how linguistics distance
affects this approach, e.g., we suspect English-
French would not help each other as much as
English-Chinese do; and it would be very interest-
ing to see what types of syntactic ambiguities can
be resolved across different language pairs. Fur-
thermore, we believe this bilingual-monolingual
approach can easily transfer to shift-reduce con-
stituency parsing (Sagae and Lavie, 2006).
</bodyText>
<sectionHeader confidence="0.998366" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9118049">
We thank the anonymous reviewers for pointing to
us references about “arc-standard”. We also thank
Aravind Joshi and Mitch Marcus for insights on
PP attachment, Joakim Nivre for discussions on
arc-eager, Yang Liu for suggestion to look at man-
ual alignments, and David A. Smith for sending
us his paper. The second and third authors were
supported by National Natural Science Foundation
of China, Contracts 60603095 and 60736014, and
863 State Key Project No. 2006AA010108.
</bodyText>
<page confidence="0.986016">
1230
</page>
<sectionHeader confidence="0.99552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999832436170213">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory ofParsing, Translation, and Compiling, vol-
ume I: Parsing of Series in Automatic Computation.
Prentice Hall, Englewood Cliffs, New Jersey.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v1.0. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings ofEMNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings ofACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings ofACL, pages 531–540,
Ann Arbor, Michigan, June.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
ofACL (poster), pages 205–208.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual chinese-english word alignments to resolve pp-
attachment ambiguity in english. In Proceedings of
AMTA Student Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings ofHLT-NAACL, pages 273–280.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of COLING-ACL, Sydney, Australia, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP,
Honolulu, Haiwaii.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of COLING, Geneva.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop
at ACL-2004, Barcelona.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings ofACL
(poster).
Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003.
Disambiguation of english pp attachment using mul-
tilingual aligned data. In Proceedings ofMT Summit
IX.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous features.
In Proceedings ofEMNLP.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of COLING.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings ofIWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings ofEMNLP.
</reference>
<page confidence="0.991405">
1231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291677">
<title confidence="0.995071">Bilingually-Constrained (Monolingual) Shift-Reduce Parsing</title>
<author confidence="0.587859">Liang</author>
<affiliation confidence="0.577791">Google</affiliation>
<address confidence="0.979974">1350 Charleston Mountain View, CA 94043,</address>
<email confidence="0.999607">liang.huang.sh@gmail.com</email>
<author confidence="0.907006">Jiang</author>
<affiliation confidence="0.8448645">Key Lab. of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.9622885">Chinese Academy of P.O. Box 2704, Beijing 100190,</address>
<email confidence="0.962304">jiangwenbin@ict.ac.cn</email>
<abstract confidence="0.999276217391304">Jointly parsing two languages has been shown to improve accuracies on either or both sides. However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations. Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well. We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts. Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art with negligible efficiency overhead, thus much faster than biparsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory ofParsing, Translation, and Compiling, volume I: Parsing of Series in Automatic Computation.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="6635" citStr="Aho and Ullman, 1972" startWordPosition="1036" endWordPosition="1039">onflicts (Section 3); • we verify empirically that shift-reduce conflicts are the major source of errors, and correct shift-reduce decisions strongly correlate with the above bilingual contiguity conditions even with automatic alignments (Section 5.3); • finally, with just three bilingual features, we improve dependency parsing accuracy by 0.6% for both English and Chinese over the state-of-the-art baseline with negligible (∼6%) efficiency overhead (Section 5.4). 2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL </context>
<context position="19954" citStr="Aho and Ullman, 1972" startWordPosition="3381" endWordPosition="3384">t action. In Figure 3(c), when considering shifting “with”, the source span becomes [Bill .. with] which maps to [na .. Bi’er] on the Chinese side. This target span looks like discontiguous in the above definition with wangyuanjin aligned to “telescope”, but we tolerate this case for the following reasons. There is a crucial difference between shift and reduce: in a shift, we do not know yet the subtree spans (unlike in a reduce we are always combining two well-formed subtrees). The only thing we are sure of in a shift action is that st and wi will be combined before st−1 and st are combined (Aho and Ullman, 1972), so we can tolerate any target word aligned to source word still in the queue, but do not allow any target word aligned to an already recognized source word. This explains the notational difference between cR(st, wi) and c(st−1, st), where subscript “R” means “right contiguity”. As a final example, in Figure 3(d), Chinese word kandao aligns to “saw”, which is already recognized, and this violates the right contiguity. So cR(st, wi) =−, suggesting that shift is probably wrong. To be more precise, Table 3 shows the formal definitions of the two features. We basically feature f source target all</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory ofParsing, Translation, and Compiling, volume I: Parsing of Series in Automatic Computation. Prentice Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Martha Palmer</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
</authors>
<title>English chinese translation treebank v1.0.</title>
<date>2007</date>
<pages>2007--02</pages>
<contexts>
<context position="25082" citStr="Bies et al., 2007" startWordPosition="4233" endWordPosition="4236">able 4: Baseline parser performance on standard Penn English Treebank dependency parsing task. The speed numbers are not exactly comparable since they are reported on different machines. Training Dev Test CTB Articles 1-270 301-325 271-300 Bilingual Paris 2745 273 290 Table 5: Training, dev, and test sets from bilingual Chinese Treebank a` la Burkett and Klein (2008). 5.2 Bilingual Data The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with goldstandard parse trees (Bies et al., 2007). Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008). Note that not all sentence pairs could be included, since many of them are not oneto-one aligned at the sentence level. Our wordalignments are generated from the HMM aligner of Liang et al. (2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.). This aligner outputs “soft alignments”, i.e., posterior probabilities for each source-target word pair. We use a pruning threshold of 0.535 to remove low-confidence alignment links,6 and use the</context>
</contexts>
<marker>Bies, Palmer, Mott, Warner, 2007</marker>
<rawString>Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007. English chinese translation treebank v1.0. LDC2007T02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="2336" citStr="Burkett and Klein (2008)" startWordPosition="358" endWordPosition="361">r]. “I used a telescope to see Bill.” (1b) I saw [ Bill [ with a telescope]]. wo kandao le [ [ na wangyuanjin ] de Bi’er]. “I saw Bill who had a telescope at hand.” Figure 1: PP-attachment is unambiguous in Chinese, which can help English parsing. Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in F</context>
<context position="3713" citStr="Burkett and Klein (2008)" startWordPosition="576" endWordPosition="579">in Natural Language Processing, pages 1222–1231, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP forcing existing approaches to employ complicated modeling and crude approximations. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6) as opposed to the monolingual O(n3) time. To make things worse, languages are non-isomorphic, i.e., there is no 1- to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004). In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008). We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously. To illustrate the idea, suppose we are parsing the sentence (1) I saw Bill [PP with a telescope ]. which has 2 parses based on the attachment of PP: (1a) I [ saw Bill ] [PP wit</context>
<context position="24833" citStr="Burkett and Klein (2008)" startWordPosition="4191" endWordPosition="4194">remely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16). parser accuracy secs/sent McDonald et al. (2005) 90.7 0.150 Zhang and Clark (2008) 91.4 0.195 our baseline at k=1 90.2 0.009 our baseline at k=16 91.3 0.125 Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task. The speed numbers are not exactly comparable since they are reported on different machines. Training Dev Test CTB Articles 1-270 301-325 271-300 Bilingual Paris 2745 273 290 Table 5: Training, dev, and test sets from bilingual Chinese Treebank a` la Burkett and Klein (2008). 5.2 Bilingual Data The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with goldstandard parse trees (Bies et al., 2007). Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008). Note that not all sentence pairs could be included, since many of them are not oneto-one aligned at the sentence level. Our wordalignments are generated from the HMM aligner of Liang et al. (2006) trained on approximately 1.7M s</context>
<context position="31619" citStr="Burkett and Klein (2008)" startWordPosition="5338" endWordPosition="5341"> 0.08 Berkeley parser 86.1 87.9 Table 9: Final results of dependency accuracy (%) on the test set (290 sentences, beam size k=16). bilingual features misled the parser into a mistake, there is no chance of getting back, while in the former multiple configurations are being pursued in parallel. In terms of speed, both parsers run proportionally slower with larger beams, as the time complexity is linear to the beam-size. Computing the bilingual features further slows it down, but only fractionally so (just 1.06 times as slow as the baseline at k=16), which is appealing in practice. By contrast, Burkett and Klein (2008) reported their approach of “monolingual k-best parsing followed by bilingual k2-best reranking” to be “3.8 times slower” than monolingual parsing. Our final results on the test set (290 sentences) are summarized in Table 9. On both English and Chinese, the addition of bilingual features improves dependency arc accuracies by +0.6%, which is mildly significant using the Z-test of Collins et al. (2005). We also compare our results against the Berkeley parser (Petrov and Klein, 2007) as a reference system, with the exact same setting (i.e., trained on the bilingual data, and testing using gold-st</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="16407" citStr="Collins and Roark (2004)" startWordPosition="2769" endWordPosition="2772"> are reduced. For example, in Figure 2, “I” is a left dependent of “saw”, and can in principle wait until “Bill” and “with” are reduced, and then finally combine with “saw”. We choose to use the heuristic of “shortest stack” that always prefers reduceL over shift, which has the effect that all left dependents are first recognized inside-out, followed by all right dependents, also inside-out, which coincides with the head-driven constituency parsing model of Collins (1999). We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected. As a special case, for the deterministic mode, updates always co-occur with the first mistake made. The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning. See Section 5.3 for more discussions. (a) I saw ✿✿✿✿✿✿✿✿✿ Bill with a telescope . wo yong wangyuanjin kandao le Bi’er. c(st−1, st) =+; reduce is correc</context>
<context position="27155" citStr="Collins and Roark, 2004" startWordPosition="4584" endWordPosition="4587">is a minor issue; 2. (bilingual) the gold-standard decisions of shift or reduce should correlate with contiguities of c(st−1, st), and of cR(st, wi). Hypothesis 1 is verified in Table 6, where we count all the first mistakes the baseline parser makes (in the deterministic mode) on the English dev set (273 sentences). In shift-reduce parsing, further mistakes are often caused by previous ones, so only the first mistake in each sentence (if there is one) is easily identifiable;7 this is also the argument for “early update” in applying perceptron learning to these incremental parsing algorithms (Collins and Roark, 2004) (see also Section 2). Among the 197 first mistakes (other 76 sentences have perfect output), the vast majority, 190 of them (96.4%), are shift-reduce errors (equally distributed between shift-becomesreduce and reduce-becomes-shift), and only 7 (3.6%) are due to reduce-reduce conflicts.8 These statistics confirm our intuition that shift-reduce decisions are much harder to make during parsing, and contribute to the overwhelming majority of errors, which is studied in the next hypothesis. Hypothesis 2 is verified in Table 7. We take the gold-standard shift-reduce sequence on the English dev set,</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="32022" citStr="Collins et al. (2005)" startWordPosition="5401" endWordPosition="5404">o the beam-size. Computing the bilingual features further slows it down, but only fractionally so (just 1.06 times as slow as the baseline at k=16), which is appealing in practice. By contrast, Burkett and Klein (2008) reported their approach of “monolingual k-best parsing followed by bilingual k2-best reranking” to be “3.8 times slower” than monolingual parsing. Our final results on the test set (290 sentences) are summarized in Table 9. On both English and Chinese, the addition of bilingual features improves dependency arc accuracies by +0.6%, which is mildly significant using the Z-test of Collins et al. (2005). We also compare our results against the Berkeley parser (Petrov and Klein, 2007) as a reference system, with the exact same setting (i.e., trained on the bilingual data, and testing using gold-standard POS tags), and the resulting trees are converted into dependency via the same headrules. We use 5 iterations of split-merge grammar induction as the 6th iteration overfits the small training set. The result is worse than our baseline on English, but better than our bilingual parser on Chinese. The discrepancy between English and Chinese is probably due to the fact that our baseline feature tem</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings ofACL, pages 531–540, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="16259" citStr="Collins (1999)" startWordPosition="2750" endWordPosition="2751">(also non-unique for the four-action system), because left dependents of a head can be reduced either before or after all right dependents are reduced. For example, in Figure 2, “I” is a left dependent of “saw”, and can in principle wait until “Bill” and “with” are reduced, and then finally combine with “saw”. We choose to use the heuristic of “shortest stack” that always prefers reduceL over shift, which has the effect that all left dependents are first recognized inside-out, followed by all right dependents, also inside-out, which coincides with the head-driven constituency parsing model of Collins (1999). We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected. As a special case, for the deterministic mode, updates always co-occur with the first mistake made. The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning. See</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="16371" citStr="Collins, 2002" startWordPosition="2766" endWordPosition="2767">after all right dependents are reduced. For example, in Figure 2, “I” is a left dependent of “saw”, and can in principle wait until “Bill” and “with” are reduced, and then finally combine with “saw”. We choose to use the heuristic of “shortest stack” that always prefers reduceL over shift, which has the effect that all left dependents are first recognized inside-out, followed by all right dependents, also inside-out, which coincides with the head-driven constituency parsing model of Collins (1999). We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected. As a special case, for the deterministic mode, updates always co-occur with the first mistake made. The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning. See Section 5.3 for more discussions. (a) I saw ✿✿✿✿✿✿✿✿✿ Bill with a telescope . wo yong wangyuanjin kandao le Bi’</context>
<context position="23767" citStr="Collins (2002)" startWordPosition="4020" endWordPosition="4021">r work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees. We use section 22 as dev set to determine the optimal number of iterations in perceptron training. Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those stateof-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16). parser accuracy secs/sent McDonald et al. (2005) 90.7 0.150 Z</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL (poster),</booktitle>
<pages>205--208</pages>
<contexts>
<context position="3622" citStr="Eisner, 2003" startWordPosition="561" endWordPosition="563">ch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP forcing existing approaches to employ complicated modeling and crude approximations. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6) as opposed to the monolingual O(n3) time. To make things worse, languages are non-isomorphic, i.e., there is no 1- to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004). In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008). We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously. To illustrate the idea, suppose we are parsing the sentence (1) I saw Bill [PP with a</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings ofACL (poster), pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
</authors>
<title>Using bilingual chinese-english word alignments to resolve ppattachment ambiguity in english.</title>
<date>2008</date>
<booktitle>In Proceedings of AMTA Student Workshop.</booktitle>
<contexts>
<context position="2141" citStr="Fossum and Knight, 2008" startWordPosition="324" endWordPosition="327">ish (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see (1a) I [ saw Bill ] [ with a telescope]. wo [ yong wangyuanjin] [kandao le Bi’er]. “I used a telescope to see Bill.” (1b) I saw [ Bill [ with a telescope]]. wo kandao le [ [ na wangyuanjin ] de Bi’er]. “I saw Bill who had a telescope at hand.” Figure 1: PP-attachment is unambiguous in Chinese, which can help English parsing. Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese u</context>
<context position="26174" citStr="Fossum and Knight (2008)" startWordPosition="4413" endWordPosition="4416">ilities for each source-target word pair. We use a pruning threshold of 0.535 to remove low-confidence alignment links,6 and use the remaining links as hard alignments; we leave the use of alignment probabilities to future work. For simplicity reasons, in the following experiments we always supply gold-standard POS tags as part of the input to the parser. 5.3 Testing our Hypotheses Before evaluating our bilingual approach, we need to verify empirically the two assumptions we made about the parser in Sections 2 and 3: 6and also removing notoriously bad links in {the, a, an}x {de, le} following Fossum and Knight (2008). 1228 sh ⊲ re re ⊲ sh sh-re re-re # 92 98 190 7 % 46.7% 49.7% 96.4% 3.6% Table 6: [Hypothesis 1] Error distribution in the baseline model (k = 1) on English dev set. “sh ⊲ re” means “should shift, but reduced”. Shiftreduce conflicts overwhelmingly dominate. 1. (monolingual) shift-reduce conflict is the major source of errors while reduce-reduce conflict is a minor issue; 2. (bilingual) the gold-standard decisions of shift or reduce should correlate with contiguities of c(st−1, st), and of cR(st, wi). Hypothesis 1 is verified in Table 6, where we count all the first mistakes the baseline parse</context>
</contexts>
<marker>Fossum, Knight, 2008</marker>
<rawString>Victoria Fossum and Kevin Knight. 2008. Using bilingual chinese-english word alignments to resolve ppattachment ambiguity in english. In Proceedings of AMTA Student Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="2612" citStr="Galley et al., 2004" startWordPosition="403" endWordPosition="406">hus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP forcing existing approach</context>
<context position="22235" citStr="Galley et al., 2004" startWordPosition="3773" endWordPosition="3776">e amounts to 24 instances (after cross-product with {−, +} and the three actions). We show in Section 5.3 that these features do correlate with the correct shift/reduce actions in practice. The naive implemention of bilingual feature computation would be of O(kn2) complexity in the worse case because when combining the largest spans one has to scan over the whole sentence. We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments (Table 8) show that 5Our definition implies that we only consider faithful spans to be contiguous (Galley et al., 2004). Also note that source spans include all dependents of st and st−1. 1227 the parser is only marginally (∼6%) slower with the new bilingual features. This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation. 4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing). For example, Hwa et al. (2005) use simple heuristics to project English tre</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings ofHLT-NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="22978" citStr="Ganchev et al. (2009)" startWordPosition="3896" endWordPosition="3899">the new bilingual features. This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation. 4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing). For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees. Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English T</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL: HLT,</booktitle>
<location>Columbus, OH,</location>
<contexts>
<context position="2633" citStr="Huang, 2008" startWordPosition="409" endWordPosition="410">guages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP forcing existing approaches to employ complica</context>
<context position="3858" citStr="Huang, 2008" startWordPosition="602" endWordPosition="603">and crude approximations. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6) as opposed to the monolingual O(n3) time. To make things worse, languages are non-isomorphic, i.e., there is no 1- to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004). In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008). We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously. To illustrate the idea, suppose we are parsing the sentence (1) I saw Bill [PP with a telescope ]. which has 2 parses based on the attachment of PP: (1a) I [ saw Bill ] [PP with a telescope ]. (1b) I saw [ Bill [PP with a telescope ]]. Both are possible, but with a Chinese translation the choice becomes clear (see Figur</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the ACL: HLT, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering.</title>
<date>2005</date>
<contexts>
<context position="22790" citStr="Hwa et al. (2005)" startWordPosition="3866" endWordPosition="3869">consider faithful spans to be contiguous (Galley et al., 2004). Also note that source spans include all dependents of st and st−1. 1227 the parser is only marginally (∼6%) slower with the new bilingual features. This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation. 4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing). For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees. Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implemen</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Sydney, Australia,</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of COLING-ACL, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="24055" citStr="McDonald et al., 2005" startWordPosition="4065" endWordPosition="4069">based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees. We use section 22 as dev set to determine the optimal number of iterations in perceptron training. Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those stateof-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16). parser accuracy secs/sent McDonald et al. (2005) 90.7 0.150 Zhang and Clark (2008) 91.4 0.195 our baseline at k=1 90.2 0.009 our baseline at k=16 91.3 0.125 Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task. The speed numbers are not exactly comparable since they are reported on different machines. Trai</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Honolulu, Haiwaii.</location>
<contexts>
<context position="2633" citStr="Mi and Huang, 2008" startWordPosition="407" endWordPosition="410">two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP forcing existing approaches to employ complica</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP, Honolulu, Haiwaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva.</location>
<contexts>
<context position="5883" citStr="Nivre and Scholz (2004)" startWordPosition="929" endWordPosition="932">b), the English PP is translated into a Chinese relative clause, but nevertheless all phrasal modifiers attach to the immediate right in Mandarin Chinese. straightforward to implement within a monolingual parsing algorithm. In this work we choose shift-reduce dependency parsing for its simplicity and efficiency. Specifically, we make the following contributions: • we develop a baseline shift-reduce dependency parser using the less popular, but classical, “arc-standard” style (Section 2), and achieve similar state-of-the-art performance with the the dominant but complicated “arceager” style of Nivre and Scholz (2004); • we propose bilingual features based on wordalignment information to prefer “target-side contiguity” in resolving shift-reduce conflicts (Section 3); • we verify empirically that shift-reduce conflicts are the major source of errors, and correct shift-reduce decisions strongly correlate with the above bilingual contiguity conditions even with automatic alignments (Section 5.3); • finally, with just three bilingual features, we improve dependency parsing accuracy by 0.6% for both English and Chinese over the state-of-the-art baseline with negligible (∼6%) efficiency overhead (Section 5.4). 2</context>
<context position="12721" citStr="Nivre and Scholz, 2004" startWordPosition="2133" endWordPosition="2136">see in Section 5.3 that this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3. 2.3 Comparison with Arc-Eager The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as “arc-standard” in Nivre (2004), but was argued against in comparison to the four-action “arc-eager” variant. Most subsequent works on shift-reduce or “transition-based” dependency parsing followed “arc-eager” (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style. But we argue that “arc-standard” is preferable because: 1. in the three action “arc-standard” system, the stack always contains a list of unrelated subtrees recognized so far, with no arcs between any of them, e.g. (I← saw) and (Bill) in step 4 of Figure 2), whereas the four action “arceager” style can have left or right arrows between items on the stack; 2. the semantics of the three actions are atomic and disjoint, whereas the semantics of 4 actions are not completely disjoint. For example, their Left action assumes an implicit </context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of english text. In Proceedings of COLING, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL-2004,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="7118" citStr="Nivre (2004)" startWordPosition="1123" endWordPosition="1124">ependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR, depending on which one of the two 1223 stack queue arcs previous S wi|Q A shift S|wi Q A previous S|st−1|st Q A reduceL S|st Q A U {(st,st−1)} reduceR S|st−1 Q A U {(st−1, st)} Table 1: Formal description of the three actions. Note that shift requires non-empty queue while reduce requires at least two elements on the stack. items becomes the head after reduction. More formally, we describe a parser configuration by a tuple (S, Q, A) where S is the stack, Q is the qu</context>
<context position="12519" citStr="Nivre (2004)" startWordPosition="2108" endWordPosition="2109">s is further conjoined with the 3 actions shift, reduceL, and reduceR. to vastly incompatible tree shapes with crossing brackets (for example, [saw Bill] vs. [Bill with a telescope]). We will see in Section 5.3 that this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3. 2.3 Comparison with Arc-Eager The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as “arc-standard” in Nivre (2004), but was argued against in comparison to the four-action “arc-eager” variant. Most subsequent works on shift-reduce or “transition-based” dependency parsing followed “arc-eager” (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style. But we argue that “arc-standard” is preferable because: 1. in the three action “arc-standard” system, the stack always contains a list of unrelated subtrees recognized so far, with no arcs between any of them, e.g. (I← saw) and (Bill) in step 4 of Figure 2), whereas the four action “arceager” style can have left or right arrows betw</context>
<context position="14771" citStr="Nivre, 2004" startWordPosition="2481" endWordPosition="2482">e argue that all things being equal, this simpler paradigm should be preferred in practice. 4 2.4 Beam Search Extension We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. Pseudocode 1 illustrates the algorithm, where we keep an agenda V of the current active configurations, and at each step try to extend them by applying one of the three actions. We then dump the best k new configurations from the buffer back 4On the other hand, there are also arguments for “arceager”, e.g., “incrementality”; see (Nivre, 2004; Nivre, 2008). 1225 Pseudocode 1 beam-search shift-reduce parsing. 1: Input: POS-tagged word sequence wl ... wn 2: start ← (0, wl ... wn, 0) &gt; initial config: empty stack, no arcs 3: V ← {start} &gt; initial agenda 4: for step ← 1 ... 2n − 1 do 5: BUF ← 0 &gt; buffer for new configs 6: for each config in agenda V do 7: for act E {shift, reduceL, reduceR} do 8: if act is applicable to config then 9: next ← apply act to config 10: insert next into buffer BUF 11: V ← top k configurations of BUF 12: Output: the tree of the best config in V into the agenda for the next step. The complexity of this algor</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL-2004, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="14785" citStr="Nivre, 2008" startWordPosition="2483" endWordPosition="2484">all things being equal, this simpler paradigm should be preferred in practice. 4 2.4 Beam Search Extension We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. Pseudocode 1 illustrates the algorithm, where we keep an agenda V of the current active configurations, and at each step try to extend them by applying one of the three actions. We then dump the best k new configurations from the buffer back 4On the other hand, there are also arguments for “arceager”, e.g., “incrementality”; see (Nivre, 2004; Nivre, 2008). 1225 Pseudocode 1 beam-search shift-reduce parsing. 1: Input: POS-tagged word sequence wl ... wn 2: start ← (0, wl ... wn, 0) &gt; initial config: empty stack, no arcs 3: V ← {start} &gt; initial agenda 4: for step ← 1 ... 2n − 1 do 5: BUF ← 0 &gt; buffer for new configs 6: for each config in agenda V do 7: for act E {shift, reduceL, reduceR} do 8: if act is applicable to config then 9: next ← apply act to config 10: insert next into buffer BUF 11: V ← top k configurations of BUF 12: Output: the tree of the best config in V into the agenda for the next step. The complexity of this algorithm is O(nk),</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="32104" citStr="Petrov and Klein, 2007" startWordPosition="5414" endWordPosition="5417">y fractionally so (just 1.06 times as slow as the baseline at k=16), which is appealing in practice. By contrast, Burkett and Klein (2008) reported their approach of “monolingual k-best parsing followed by bilingual k2-best reranking” to be “3.8 times slower” than monolingual parsing. Our final results on the test set (290 sentences) are summarized in Table 9. On both English and Chinese, the addition of bilingual features improves dependency arc accuracies by +0.6%, which is mildly significant using the Z-test of Collins et al. (2005). We also compare our results against the Berkeley parser (Petrov and Klein, 2007) as a reference system, with the exact same setting (i.e., trained on the bilingual data, and testing using gold-standard POS tags), and the resulting trees are converted into dependency via the same headrules. We use 5 iterations of split-merge grammar induction as the 6th iteration overfits the small training set. The result is worse than our baseline on English, but better than our bilingual parser on Chinese. The discrepancy between English and Chinese is probably due to the fact that our baseline feature templates (Table 2) are engineered on English not Chinese. 6 Conclusion and Future Wo</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL (poster).</booktitle>
<contexts>
<context position="6969" citStr="Sagae and Lavie (2006)" startWordPosition="1098" endWordPosition="1101">uracy by 0.6% for both English and Chinese over the state-of-the-art baseline with negligible (∼6%) efficiency overhead (Section 5.4). 2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR, depending on which one of the two 1223 stack queue arcs previous S wi|Q A shift S|wi Q A previous S|st−1|st Q A reduceL S|st Q A U {(st,st−1)} reduceR S|st−1 Q A U {(st−1, st)} Table 1: Formal description of the three actions. Note that shift requires non-empty queue while reduce requires at least two elements on the st</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proceedings ofACL (poster).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Schwartz</author>
<author>Takako Aikawa</author>
<author>Chris Quirk</author>
</authors>
<title>Disambiguation of english pp attachment using multilingual aligned data.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT Summit IX.</booktitle>
<contexts>
<context position="2165" citStr="Schwartz et al., 2003" startWordPosition="328" endWordPosition="331"> languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see (1a) I [ saw Bill ] [ with a telescope]. wo [ yong wangyuanjin] [kandao le Bi’er]. “I used a telescope to see Bill.” (1b) I saw [ Bill [ with a telescope]]. wo kandao le [ [ na wangyuanjin ] de Bi’er]. “I saw Bill who had a telescope at hand.” Figure 1: PP-attachment is unambiguous in Chinese, which can help English parsing. Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese uses word-order to disamb</context>
</contexts>
<marker>Schwartz, Aikawa, Quirk, 2003</marker>
<rawString>Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003. Disambiguation of english pp attachment using multilingual aligned data. In Proceedings ofMT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous features.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="23006" citStr="Smith and Eisner (2009)" startWordPosition="3901" endWordPosition="3904">. This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation. 4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing). For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees. Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English Treebank (PTB) dependency par</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David A. Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous features. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: Using english to parse korean.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2307" citStr="Smith and Smith (2004)" startWordPosition="353" endWordPosition="356">angyuanjin] [kandao le Bi’er]. “I used a telescope to see Bill.” (1b) I saw [ Bill [ with a telescope]]. wo kandao le [ [ na wangyuanjin ] de Bi’er]. “I saw Bill who had a telescope at hand.” Figure 1: PP-attachment is unambiguous in Chinese, which can help English parsing. Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem. For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008). However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” att</context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: Using english to parse korean. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3345" citStr="Wu, 1997" startWordPosition="518" endWordPosition="519">ese uses word-order to disambiguate the attachment (see below). By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the “V or N” attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the “N1 or N2” case (Mitch Marcus, p.c.). 1222 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP forcing existing approaches to employ complicated modeling and crude approximations. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6) as opposed to the monolingual O(n3) time. To make things worse, languages are non-isomorphic, i.e., there is no 1- to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004). In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008). We instead propose a much simpler alternative, bilingually-constrained monolingual pa</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>Building a large-scale annotated chinese corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="24959" citStr="Xue et al., 2002" startWordPosition="4214" endWordPosition="4217">t al. (2005) 90.7 0.150 Zhang and Clark (2008) 91.4 0.195 our baseline at k=1 90.2 0.009 our baseline at k=16 91.3 0.125 Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task. The speed numbers are not exactly comparable since they are reported on different machines. Training Dev Test CTB Articles 1-270 301-325 271-300 Bilingual Paris 2745 273 290 Table 5: Training, dev, and test sets from bilingual Chinese Treebank a` la Burkett and Klein (2008). 5.2 Bilingual Data The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with goldstandard parse trees (Bies et al., 2007). Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008). Note that not all sentence pairs could be included, since many of them are not oneto-one aligned at the sentence level. Our wordalignments are generated from the HMM aligner of Liang et al. (2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.). This aligner outputs “soft alignments”, i.e., posterior probabilities f</context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>Nianwen Xue, Fu-Dong Chiou, and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT.</booktitle>
<contexts>
<context position="7071" citStr="Yamada and Matsumoto (2003)" startWordPosition="1114" endWordPosition="1117">%) efficiency overhead (Section 5.4). 2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination. This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the “arc-standard” version of Nivre (2004). 2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR, depending on which one of the two 1223 stack queue arcs previous S wi|Q A shift S|wi Q A previous S|st−1|st Q A reduceL S|st Q A U {(st,st−1)} reduceR S|st−1 Q A U {(st−1, st)} Table 1: Formal description of the three actions. Note that shift requires non-empty queue while reduce requires at least two elements on the stack. items becomes the head after reduction. More formally, we describe a parser configuration by a tu</context>
<context position="12402" citStr="Yamada and Matsumoto (2003)" startWordPosition="2089" endWordPosition="2092">n word, and lc(·) and rc(·) represent the leftmost and rightmost child. Symbol o denotes feature conjunction. Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR. to vastly incompatible tree shapes with crossing brackets (for example, [saw Bill] vs. [Bill with a telescope]). We will see in Section 5.3 that this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3. 2.3 Comparison with Arc-Eager The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as “arc-standard” in Nivre (2004), but was argued against in comparison to the four-action “arc-eager” variant. Most subsequent works on shift-reduce or “transition-based” dependency parsing followed “arc-eager” (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style. But we argue that “arc-standard” is preferable because: 1. in the three action “arc-standard” system, the stack always contains a list of unrelated subtrees recognized so far, with no arcs between any of them, e.g. (I</context>
<context position="23823" citStr="Yamada and Matsumoto (2003)" startWordPosition="4027" endWordPosition="4031">tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity. 5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees. We use section 22 as dev set to determine the optimal number of iterations in perceptron training. Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those stateof-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16). parser accuracy secs/sent McDonald et al. (2005) 90.7 0.150 Zhang and Clark (2008) 91.4 0.195 our baseline at k=1 90.</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="8993" citStr="Zhang and Clark, 2008" startWordPosition="1476" endWordPosition="1479">nfiguration is (wj, 0, A) where wj is recognized as the root of the whole sentence, and A encodes a spanning tree rooted at wj. For a sentence of n words, there are exactly 2n − 1 actions: n shifts and n − 1 reductions, since every word must be pushed onto stack once, and every word except the root will eventually be popped in a reduction. The time complexity, as other shift-reduce instances, is clearly O(n). 2.2 Example of Shift-Reduce Conflict Figure 2 shows the trace of this paradigm on the example sentence. For the first two configurations 3a “configuration” is sometimes called a “state” (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. 0 - I saw Bill with a ... 1 shift I saw Bill with a ... 2 shift I saw Bill with a ... 3 reduceL I saw Bill with a ... 4 shift I saw Bill with a ... 5a reduceR I saw Bill with a ... 5b shift I saw Bill with a ... Figure 2: A trace of 3-action shift-reduce on the example sentence. Shaded words are on stack, while gray words have been popped from stack. After step (4), the process can take either (5a) or (5b), which correspond to the two attachments (1a) and (1b) in Figure 1, respectively. (0) a</context>
<context position="12745" citStr="Zhang and Clark, 2008" startWordPosition="2137" endWordPosition="2140">this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3. 2.3 Comparison with Arc-Eager The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as “arc-standard” in Nivre (2004), but was argued against in comparison to the four-action “arc-eager” variant. Most subsequent works on shift-reduce or “transition-based” dependency parsing followed “arc-eager” (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style. But we argue that “arc-standard” is preferable because: 1. in the three action “arc-standard” system, the stack always contains a list of unrelated subtrees recognized so far, with no arcs between any of them, e.g. (I← saw) and (Bill) in step 4 of Figure 2), whereas the four action “arceager” style can have left or right arrows between items on the stack; 2. the semantics of the three actions are atomic and disjoint, whereas the semantics of 4 actions are not completely disjoint. For example, their Left action assumes an implicit Reduce of the left item,</context>
<context position="14090" citStr="Zhang and Clark, 2008" startWordPosition="2365" endWordPosition="2368"> causes the next problem (see below). We argue that this is rather complicated to implement. 3. the “arc-standard” scan always succeeds, since at the end we can always reduce with empty queue, whereas the “arc-eager” style sometimes goes into deadends where no action can perform (prevented by preconditions, otherwise the result will not be a wellformed tree). This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). We argue that all things being equal, this simpler paradigm should be preferred in practice. 4 2.4 Beam Search Extension We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. Pseudocode 1 illustrates the algorithm, where we keep an agenda V of the current active configurations, and at each step try to extend them by applying one of the three actions. We then dump the best k new configurations from the buffer back 4On the other hand, </context>
<context position="24100" citStr="Zhang and Clark, 2008" startWordPosition="4072" endWordPosition="4075">n 2, with feature templates from Table 2. We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees. We use section 22 as dev set to determine the optimal number of iterations in perceptron training. Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those stateof-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16). parser accuracy secs/sent McDonald et al. (2005) 90.7 0.150 Zhang and Clark (2008) 91.4 0.195 our baseline at k=1 90.2 0.009 our baseline at k=16 91.3 0.125 Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task. The speed numbers are not exactly comparable since they are reported on different machines. Training Dev Test CTB Articles 1-270 301-325 271-</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings ofEMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>