<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056545">
<title confidence="0.848926">
Making Hidden Markov Models More Transparent
</title>
<author confidence="0.626393">
Nashira Richard Lincoln* and Marc Light†
*†Linguistics Department
</author>
<affiliation confidence="0.91959525">
†School of Library and Information Science
†Computer Science Department
University of Iowa
Iowa, USA 52242
</affiliation>
<email confidence="0.998977">
{nashira-lincoln, marc-light}@uiowa.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953642857143">
Understanding the decoding algorithm for
hidden Markov models is a difficult task
for many students. A comprehensive un-
derstanding is difficult to gain from static
state transition diagrams and tables of ob-
servation production probabilities. We
have built a number of visualizations de-
picting a hidden Markov model for part-
of-speech tagging and the operation of the
Viterbi algorithm. The visualizations are
designed to help students grasp the oper-
ation of the HMM. In addition, we have
found that the displays are useful as de-
bugging tools for experienced researchers.
</bodyText>
<sectionHeader confidence="0.998035" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999830307692308">
Hidden Markov Models (HMMs) are an important
part of the natural language processing toolkit and
are often one of the first stochastic generation mod-
els that students1 encounter. The corresponding
Viterbi algorithm is also often the first example
of dynamic programming that students encounter.
Thus, HMMs provide an opportunity to start stu-
dents on the correct path of understanding stochas-
tic models, not simply treating them as black boxes.
Unfortunately, static state transition diagrams, ta-
bles of probability values, and lattice diagrams are
not enough for many students. They have a general
idea of how a HMM works but often have common
</bodyText>
<footnote confidence="0.991971">
1The Introduction to Computational Linguistics course at
the University of Iowa has no prerequisites, and over half the
students are not CS majors.
</footnote>
<page confidence="0.994855">
32
</page>
<bodyText confidence="0.999987363636364">
misconceptions. For example, we have found that
students often believe that as the Viterbi algorithm
calculates joint state sequence observation sequence
probabilities, the best state sequence so far is always
a prefix of global best path. This is of course false.
Working a long example to show this is very tedious
and thus text books seldom provide such examples.
Even for practitioners, HMMs are often opaque
in that the cause of a mis-tagging error is often left
uncharacterized. A display would be helpful to pin-
point why an HMM chose an incorrect state se-
quence instead of the correct one.
Below we describe two displays that attempt to
remedy the above mentioned problems and we dis-
cuss a Java implementation of these displays in the
context of a part-of-speech tagging HMM (Kupiec,
1992). The system is freely available and has an
XML model specification that allows models calcu-
lated by other methods to be viewed. (A standard
maximum likelihood estimation was implemented
and can be used to create models from tagged data.
A model is also provided.)
</bodyText>
<sectionHeader confidence="0.994478" genericHeader="introduction">
2 Displays
</sectionHeader>
<bodyText confidence="0.9996446">
Figure 1 shows a snapshot of our first display. It
contains three kinds of information: most likely
path for input, transition probabilities, and history of
most likely prefixes for each observation index in the
Viterbi lattice. The user can input text at the bottom
of the display, e.g., Pelham pointed out that Geor-
gia voters rejected the bill. The system then runs
Viterbi and animates the search through all possible
state sequences and displays the best state sequence
prefix as it works its way through the observation
</bodyText>
<note confidence="0.981343">
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 32–36,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<figureCaption confidence="0.99922">
Figure 1: The system’s main display. Top pane: shows the state space and animates the derivation of the
</figureCaption>
<bodyText confidence="0.602622">
most likely path for “Pelman pointed out that Georgia voters ...”; Middle pane: a mouse-over-triggered bar
graph of out transition probabilities for a state; Bottom pane: a history of most likely prefixes for each
observation index in the Viterbi lattice. Below the panes is the input text field.
</bodyText>
<page confidence="0.996883">
33
</page>
<figureCaption confidence="0.972817">
Figure 2: Contrast display: The user enters a sequence on the top text field and presses enter, the sequence
is tagged and displayed in both the top and bottom text fields. Finally, the user changes any incorrect tags in
the top text field and presses enter and the probability ratio bars are then displayed.
</figureCaption>
<page confidence="0.986447">
34
</page>
<figureCaption confidence="0.8987337">
sequence from left to right (these are lines connect- and Martin, 2000). A model was trained using
ing the states in Figure 1). At any point, the stu- Maximum Likelihood from the UPenn Treebank
dent can mouse-over a state to see probabilities for (Marcus et al., 1993). The input model file is
transitions out of that state (this is the bar graph in encoded using XML and thus models built by other
Figure 1). Finally, the history of most likely pre- systems can be read in and displayed.
fixes is displayed (this history appears below the bar The system is implemented in Java and requires
graph in Figure 1). We mentioned that students often 1.4 or higher to run. It has been tested on Linux and
falsely believe that the most likely prefix is extended Apple operating systems. We will release it under a
monotonically. By seeing the path through the states standard open source license.
reconfigure itself in the middle of the observation se- 4 Summary and future work
</figureCaption>
<bodyText confidence="0.993172486486486">
quence and by looking at the prefix history, a student Students (and researchers) need to understand
has a good chance of dispelling the false belief of HMMs. We have built a display that allow users
monotonicity. to probe different aspects of an HMM and watch
The second display allows the user to contrast two Viterbi in action. In addition, our system provides
state sequences for the same observation sequence. a display that allows users to contrast state sequence
See Figure 2. For each contrasting state pairs, it probabilities. To drive these displays, we have built
shows the ratio of the corresponding transition to a standard HMM system including parameter esti-
each state and it shows the ratio of the generation of mating and decoding and provide a part-of-speech
the observation conditioned on each state. For exam- model trained on UPenn Treebank data. The system
ple, in Figure 2 the transition DT→JJ is less likely can also read in models constructed by other sys-
than DT→NNP. The real culprit is generation proba- tems.
bility P(Equal|JJ) which is almost 7 times larger than This system was built during this year’s offering
P(Equal|NNP). Later in the sequence we see a simi- of Introduction to Computational Linguistics at the
lar problem with generating opportunity from a NNP University of Iowa. In the Spring of 2006 it will be
state. These generation probabilities seem to drown deployed in the classroom for the first time. We plan
out any gains made by the likelihood of NNP runs. on giving a demonstration of the system during a
To use this display, the user types in a sentence lecture on HMMs and part-of-speech tagging. A re-
in the box above the graph and presses enter. The lated problem set using the system will be assigned.
HMM is used to tag the input. The user then modi- The students will be given several mis-tagged sen-
fies (e.g., corrects) the tag sequence and presses en- tences and asked to analyze the errors and report
ter and the ratio bars then appear. on precisely why they occurred. A survey will be
Let us consider another example: in Figure 2, the administered at the end and improvements will be
mis-tagging of raises as a verb instead of a noun at made to the system based on the feedback provided.
the end of the sentence. The display shows us that In the future we plan to implement Good-Turing
although NN→NNS is more likely than NN→VBZ, smoothing and a method for dealing with unknown
the generation probability for raises as a verb is words. We also plan to provide an additional display
over twice as high as a noun. (If this pattern of that shows the traditional Viterbi lattice figure, i.e.,
mis-taggings caused by high generation probabil- observations listed left-to-right, possible states listed
ity ratios was found repeatedly, we might consider from top-to-bottom, and lines from left-to-right con-
smoothing these distributions more aggressively.) necting states at observation index i with the previ-
3 Implementation ous states, i-1, that are part of the most likely state
The HMM part-of-speech tagging model and sequence to i. Finally, we would like to incorpo-
corresponding Viterbi algorithm were implemented rate an additional display that will provide a visual-
based on their description in the updated version, ization of EM HMM training. We will use (Eisner,
http://www.cs.colorado.edu/˜martin/ 2002) as a starting point.
SLP/updated.html , of chapter 8 of (Jurafsky
35
</bodyText>
<sectionHeader confidence="0.996159" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999101875">
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proc. of the
ACL 2002 Workshop on effective tools and method-
ologies for teaching natural language processing and
computational linguistics.
Daniel Jurafsky and James H. Martin. 2000. Speech and
Language Processing: an introduction to natural lan-
guage processing, and computational linguistics, and
speech recognition. Prentice-Hall.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden markov model. Computer Speech and Lan-
guage, 6:225–242.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330, June.
</reference>
<page confidence="0.998941">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.761348">
<title confidence="0.998779">Making Hidden Markov Models More Transparent</title>
<author confidence="0.998126">Richard</author>
<affiliation confidence="0.975673666666667">of Library and Information Science University of</affiliation>
<address confidence="0.829192">Iowa, USA</address>
<abstract confidence="0.998105">Understanding the decoding algorithm for hidden Markov models is a difficult task for many students. A comprehensive understanding is difficult to gain from static state transition diagrams and tables of observation production probabilities. We have built a number of visualizations depicting a hidden Markov model for partof-speech tagging and the operation of the Viterbi algorithm. The visualizations are designed to help students grasp the operation of the HMM. In addition, we have found that the displays are useful as debugging tools for experienced researchers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>An interactive spreadsheet for teaching the forward-backward algorithm.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL</booktitle>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. An interactive spreadsheet for teaching the forward-backward algorithm. In Proc. of the ACL 2002 Workshop on effective tools and methodologies for teaching natural language processing and computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing: an introduction to natural language processing, and computational linguistics, and speech recognition.</title>
<date>2000</date>
<publisher>Prentice-Hall.</publisher>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2000. Speech and Language Processing: an introduction to natural language processing, and computational linguistics, and speech recognition. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--225</pages>
<contexts>
<context position="2443" citStr="Kupiec, 1992" startWordPosition="381" endWordPosition="382"> sequence so far is always a prefix of global best path. This is of course false. Working a long example to show this is very tedious and thus text books seldom provide such examples. Even for practitioners, HMMs are often opaque in that the cause of a mis-tagging error is often left uncharacterized. A display would be helpful to pinpoint why an HMM chose an incorrect state sequence instead of the correct one. Below we describe two displays that attempt to remedy the above mentioned problems and we discuss a Java implementation of these displays in the context of a part-of-speech tagging HMM (Kupiec, 1992). The system is freely available and has an XML model specification that allows models calculated by other methods to be viewed. (A standard maximum likelihood estimation was implemented and can be used to create models from tagged data. A model is also provided.) 2 Displays Figure 1 shows a snapshot of our first display. It contains three kinds of information: most likely path for input, transition probabilities, and history of most likely prefixes for each observation index in the Viterbi lattice. The user can input text at the bottom of the display, e.g., Pelham pointed out that Georgia vot</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>J. Kupiec. 1992. Robust part-of-speech tagging using a hidden markov model. Computer Speech and Language, 6:225–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4413" citStr="Marcus et al., 1993" startWordPosition="708" endWordPosition="711">w the panes is the input text field. 33 Figure 2: Contrast display: The user enters a sequence on the top text field and presses enter, the sequence is tagged and displayed in both the top and bottom text fields. Finally, the user changes any incorrect tags in the top text field and presses enter and the probability ratio bars are then displayed. 34 sequence from left to right (these are lines connect- and Martin, 2000). A model was trained using ing the states in Figure 1). At any point, the stu- Maximum Likelihood from the UPenn Treebank dent can mouse-over a state to see probabilities for (Marcus et al., 1993). The input model file is transitions out of that state (this is the bar graph in encoded using XML and thus models built by other Figure 1). Finally, the history of most likely pre- systems can be read in and displayed. fixes is displayed (this history appears below the bar The system is implemented in Java and requires graph in Figure 1). We mentioned that students often 1.4 or higher to run. It has been tested on Linux and falsely believe that the most likely prefix is extended Apple operating systems. We will release it under a monotonically. By seeing the path through the states standard </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>