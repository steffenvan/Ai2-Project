<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.63028">
Discriminative Learning over Constrained Latent Representations
</title>
<author confidence="0.94899">
Ming-Wei Chang and Dan Goldwasser and Dan Roth and Vivek Srikumar
</author>
<affiliation confidence="0.982941">
University of Illinois at Urbana Champaign
</affiliation>
<address confidence="0.821992">
Urbana, IL 61801
</address>
<email confidence="0.999568">
{mchang,goldwas1,danr,vsrikum2}@uiuc.edu
</email>
<sectionHeader confidence="0.996671" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865619047619">
This paper proposes a general learning frame-
work for a class of problems that require learn-
ing over latent intermediate representations.
Many natural language processing (NLP) de-
cision problems are defined over an expressive
intermediate representation that is not explicit
in the input, leaving the algorithm with both
the task of recovering a good intermediate rep-
resentation and learning to classify correctly.
Most current systems separate the learning
problem into two stages by solving the first
step of recovering the intermediate representa-
tion heuristically and using it to learn the final
classifier. This paper develops a novel joint
learning algorithm for both tasks, that uses the
final prediction to guide the selection of the
best intermediate representation. We evalu-
ate our algorithm on three different NLP tasks
– transliteration, paraphrase identification and
textual entailment – and show that our joint
method significantly improves performance.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999202136363636">
Many NLP tasks can be phrased as decision prob-
lems over complex linguistic structures. Successful
learning depends on correctly encoding these (of-
ten latent) structures as features for the learning sys-
tem. Tasks such as transliteration discovery (Kle-
mentiev and Roth, 2008), recognizing textual en-
tailment (RTE) (Dagan et al., 2006) and paraphrase
identification (Dolan et al., 2004) are a few proto-
typical examples. However, the input to such prob-
lems does not specify the latent structures and the
problem is defined in terms of surface forms only.
Most current solutions transform the raw input into
a meaningful intermediate representation1, and then
encode its structural properties as features for the
learning algorithm.
Consider the RTE task of identifying whether the
meaning of a short text snippet (called the hypoth-
esis) can be inferred from that of another snippet
(called the text). A common solution (MacCartney
et al., 2008; Roth et al., 2009) is to begin by defining
an alignment over the corresponding entities, pred-
icates and their arguments as an intermediate rep-
resentation. A classifier is then trained using fea-
tures extracted from the intermediate representation.
The idea of using a intermediate representation also
occurs frequently in other NLP tasks (Bergsma and
Kondrak, 2007; Qiu et al., 2006).
While the importance of finding a good inter-
mediate representation is clear, emphasis is typi-
cally placed on the later stage of extracting features
over this intermediate representation, thus separat-
ing learning into two stages – specifying the la-
tent representation, and then extracting features for
learning. The latent representation is obtained by an
inference process using predefined models or well-
designed heuristics. While these approaches often
perform well, they ignore a useful resource when
generating the latent structure – the labeled data for
the final learning task. As we will show in this pa-
per, this results in degraded performance for the ac-
tual classification task at hand. Several works have
considered this issue (McCallum et al., 2005; Gold-
wasser and Roth, 2008b; Chang et al., 2009; Das
and Smith, 2009); however, they provide solutions
</bodyText>
<footnote confidence="0.9934015">
1In this paper, the phrases “intermediate representation” and
“latent representation” are used interchangeably.
</footnote>
<page confidence="0.96505">
429
</page>
<note confidence="0.759408">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998101090909091">
that do not easily generalize to new tasks.
In this paper, we propose a unified solution to the
problem of learning to make the classification deci-
sion jointly with determining the intermediate rep-
resentation. Our Learning Constrained Latent Rep-
resentations (LCLR) framework is guided by the in-
tuition that there is no intrinsically good intermedi-
ate representation, but rather that a representation is
good only to the extent to which it improves perfor-
mance on the final classification task. In the rest of
this section we discuss the properties of our frame-
work and highlight its contributions.
Learning over Latent Representations This pa-
per formulates the problem of learning over latent
representations and presents a novel and general so-
lution applicable to a wide range of NLP applica-
tions. We analyze the properties of our learning
solution, thus allowing new research to take advan-
tage of a well understood learning and optimization
framework rather than an ad-hoc solution. We show
the generality of our framework by successfully ap-
plying it to three domains: transliteration, RTE and
paraphrase identification.
Joint Learning Algorithm In contrast to most
existing approaches that employ domain specific
heuristics to construct intermediate representations
to learn the final classifier, our algorithm learns to
construct the optimal intermediate representation to
support the learning problem. Learning to represent
is a difficult structured learning problem however,
unlike other works that use labeled data at the in-
termediate level, our algorithm only uses the binary
supervision supplied for the final learning problem.
Flexible Inference Successful learning depends
on constraining the intermediate representation with
task-specific knowledge. Our framework uses the
declarative Integer Linear Programming (ILP) infer-
ence formulation, which makes it easy to define the
intermediate representation and to inject knowledge
in the form of constraints. While ILP has been ap-
plied to structured output learning, to the best of our
knowledge, this is the first work that makes use of
ILP in formalizing the general problem of learning
intermediate representations.
</bodyText>
<sectionHeader confidence="0.987794" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.99894">
We introduce notation using the Paraphrase Iden-
tification task as a running example. This is the bi-
nary classification task of identifying whether one
sentence is a paraphrase of another. A paraphrase
pair from the MSR Paraphrase corpus (Quirk et al.,
2004) is shown in Figure 1. In order to identify
that the sentences paraphrase each other , we need
to align constituents of these sentences. One possi-
ble alignment is shown in the figure, in which the
dotted edges correspond to the aligned constituents.
An alignment can be specified using binary variables
corresponding to every edge between constituents,
indicating whether the edge is included in the align-
ment. Different activations of these variables induce
the space of intermediate representations.
</bodyText>
<figureCaption confidence="0.968351">
Figure 1: The dotted lines represent a possible intermediate
</figureCaption>
<bodyText confidence="0.993377032258065">
representation for the paraphrase identification task. Since dif-
ferent representation choices will impact the binary identifica-
tion decision directly, our approach chooses the representation
that facilitates the binary learning task.
To formalize this setting, let x denote the input
to a decision function, which maps x to {−1,1}.
We consider problems where this decision depends
on an intermediate representation (for example, the
collection of all dotted edges in Figure 1), which can
be represented by a binary vector h.
In the literature, a common approach is to sepa-
rate the problem into two stages. First, a genera-
tion stage predicts h for each x using a pre-defined
model or a heuristic. This is followed by a learn-
ing stage, in which the classifier is trained using h.
In our example, if the generation stage predicts the
alignment shown, then the learning stage would use
the features computed based on the alignments. For-
mally, the two-stage approach uses a pre-defined in-
ference procedure that finds an intermediate repre-
sentation h&apos;. Using features 4b(x, h&apos;) and a learned
weight vector B, the example is classified as positive
if BT 4b(x, h&apos;) &gt; 0.
However, in the two stage approach, the latent
representation, which is provided to the learning al-
gorithm, is determined before learning starts, and
without any feedback from the final task. It is dic-
tated by the intuition of the developer. This approach
makes two implicit assumptions: first, it assumes
The notification was first reported Friday by MSNBC.
MSNBC.com first reported the CIA request on Friday.
</bodyText>
<page confidence="0.97">
430
</page>
<bodyText confidence="0.999941333333333">
the existence of a “correct” latent representation and,
second, that the model or heuristic used to generate
it is the correct one for the learning problem at hand.
</bodyText>
<sectionHeader confidence="0.9864225" genericHeader="method">
3 Joint Learning with an Intermediate
Representation
</sectionHeader>
<bodyText confidence="0.999936307692308">
In contrast to two-stage approaches, we use the
annotated data for the final classification task to
learn a suitable intermediate representation which,
in turn, helps the final classification.
Choosing a good representation is an optimization
problem that selects which of the elements (features)
of the representation best contribute to success-
ful classification given some legitimacy constraints;
therefore, we (1) set up the optimization framework
that finds legitimate representations (Section 3.1),
and (2) learn an objective function for this optimiza-
tion problem, such that it makes the best final deci-
sion (Section 3.2.)
</bodyText>
<subsectionHeader confidence="0.971746">
3.1 Inference
</subsectionHeader>
<bodyText confidence="0.999983458333333">
Our goal is to correctly predict the final label
rather than matching a “gold” intermediate repre-
sentation. In our framework, attempting to learn the
final decision drives both the selection of the inter-
mediate representation and the final predictions.
For each x, let F(x) be the set of all substructures
of all possible intermediate representations. In Fig-
ure 1, this could be the set of all alignment edges
connecting the constituents of the sentences. Given
a vocabulary of such structures of size N, we denote
intermediate representations by h E {0,1}N, which
“select” the components of the vocabulary that con-
stitute the intermediate representation. We define
φs(x) to be a feature vector over the substructure
s, which is used to describe the characteristics of s,
and define a weight vector u over these features.
Let C denote the set of feasible intermediate repre-
sentations h, specified by means of linear constraints
over h. While F(x) might be large, the set of those
elements in h that are active can be constrained by
controlling C. After we have learned a weight vec-
tor u that scores intermediate representations for the
final classification task, we define our decision func-
tion as
</bodyText>
<equation confidence="0.948182">
fu(x) = max uT X hsφs(x), (1)
h∈C s∈Γ(x)
</equation>
<bodyText confidence="0.984091904761905">
and classify the input as positive if fu(x) &gt; 0.
In Eq. (1), uTφs(x) is the score associated with
the substructure s, and fu(x) is the score for the en-
tire intermediate representation. Therefore, our de-
cision function fu(x) &gt; 0 makes use of the interme-
diate representation and its score to classify the in-
put. An input is labeled as positive if its underlying
intermediate structure allows it to cross the decision
threshold. The intermediate representation is cho-
sen to maximize the overall score of the input. This
design is especially beneficial for many phenomena
in NLP, where only positive examples have a mean-
ingful underlying structure. In our paraphrase iden-
tification example, good alignments generally exist
only for positive examples.
One unique feature of our framework is that we
treat Eq. (1) as an Integer Linear Programming
(ILP) instance. A concrete instantiation of this set-
ting to the paraphrase identification problem, along
with the actual ILP formulation is shown in Section
4.
</bodyText>
<subsectionHeader confidence="0.999791">
3.2 Learning
</subsectionHeader>
<bodyText confidence="0.9990525">
We now present an algorithm that learns the
weight vector u. For a loss function ` : R —* R,
the goal of learning is to solve the following opti-
mization problem:
</bodyText>
<equation confidence="0.987309444444444">
λ X
2 IluIl2 +
i
Here, λ is the regularization parameter. Substituting
Eq. (1) into Eq. (2), we get
⎛ ⎞
`⎝−yi max ⎠ (3)
h∈C uT X hsφs(xi)
s∈Γ(x)
</equation>
<bodyText confidence="0.945868333333333">
Note that there is a maximization term inside the
global minimization problem, making Eq. (3) a non-
convex problem. The minimization drives u towards
smaller empirical loss while the maximization uses
u to find the best representation for each example.
The algorithm for Learning over Constrained La-
tent Representations (LCLR) is listed in Algorithm
1. In each iteration, first, we find the best feature
representations for all positive examples (lines 3-5).
This step can be solved with an off-the-shelf ILP
solver. Having fixed the representations for the pos-
itive examples, we update the u by solving Eq. (4)
at line 6 in the algorithm. It is important to observe
min
u
</bodyText>
<equation confidence="0.982587666666667">
` (−yifu(xi)) (2)
λ X
2 IluIl2+
i
min
u
</equation>
<page confidence="0.971903">
431
</page>
<construct confidence="0.58309">
Algorithm 1 LCLR :The algorithm that optimizes (3)
</construct>
<listItem confidence="0.996602857142857">
1: initialize: u +— up
2: repeat
3: for all positive examples (xi, yi = 1) do
4: Find h*i +— arg maxhEC P hsuT Os(xi)
s
5: end for
6: Update u by solving
</listItem>
<equation confidence="0.9788215">
hsOs(xi)) (4)
i:yz=−1
</equation>
<listItem confidence="0.732522">
7: until convergence
8: return u
</listItem>
<bodyText confidence="0.9421505625">
that for positive examples in Eq. (4), we use the in-
termediate representations h* from line 4.
Algorithm 1 satisfies the following property:
Theorem 1 If the loss function E is a non-
decreasing function, then the objective function
value of Eq. (3) is guaranteed to decrease in every
iteration of Algorithm 1. Moreover, if the loss func-
tion is also convex, then Eq. (4) in Algorithm 1 is
convex.
Due to the space limitation, we omit the proof.
Theoretically, we can use any loss function that
satisfies the conditions of the theorem. In the exper-
iments in this paper, we use the squared-hinge loss
function: E(−yfu(x)) = max(0,1 − yfu(x))2.
Recall that Eq. (4) is not the traditional SVM or
logistic regression formulation. This is because in-
side the inner loop, the best representation for each
negative example must be found. Therefore, we
need to perform inference for every negative exam-
ple when updating the weight vector solution. In-
stead of solving a difficult non-convex optimization
problem (Eq. (3)), LCLR iteratively solves a series
of easier problems (Eq. (4)). This is especially true
for our loss function because Eq. (4) is convex and
can be solved efficiently.
We use a cutting plane algorithm to solve Eq. (4).
A similar idea has been proposed in (Joachims et al.,
2009). The algorithm for solving Eq. (4) is presented
as Algorithm 2. This algorithm uses a “cache” Hj
to store all intermediate representations for negative
examples that have been seen in previous iterations
Algorithm 2 Cutting plane algorithm to optimize Eq. (4)
</bodyText>
<listItem confidence="0.98696875">
1: for each negative example xj, Hj +— 0
2: repeat
3: for each negative example xj do
P
4: Find h* j � arg maxhEC s hsuT Os(xj)
5: Hj — Hj U {h*j}
6: end for
7: Solve
</listItem>
<equation confidence="0.99561">
� X
2 Ilul12 +
s
hsOs(xi)) (5)
i:yz=−1
</equation>
<listItem confidence="0.6308725">
8: until no new element is added to any Hj
9: return u
</listItem>
<bodyText confidence="0.999226555555556">
(lines 3-6) 2. The difference between Eq. (5) in line
7 of Algorithm 2 and Eq. (4) is that in Eq. (5), we do
not search over the entire space of intermediate rep-
resentations. The search space for the minimization
problem Eq. (5) is restricted to the cache Hj. There-
fore, instead of solving the minimization problem
Eq. (4), we can now solve several simpler problems
shown in Eq. (5). The algorithm is guaranteed to
stop (line 8) because the space of intermediate rep-
resentations is finite. Furthermore, in practice, the
algorithm needs to consider only a small subset of
“hard” examples before it converges.
Inspired by (Hsieh et al., 2008), we apply an effi-
cient coordinate descent algorithm for the dual for-
mulation of (5) which is guaranteed to find its global
minimum. Due to space considerations, we do not
present the derivation of dual formulation and the
details of the optimization algorithm.
</bodyText>
<sectionHeader confidence="0.975855" genericHeader="method">
4 Encoding with ILP: A Paraphrase
Identification Example
</sectionHeader>
<bodyText confidence="0.999892142857143">
In this section, we define the latent representation
for the paraphrase identification task. Unlike the ear-
lier example, where we considered the alignment of
lexical items, we describe a more complex interme-
diate representation by aligning graphs created using
semantic resources.
An input example is represented as two acyclic
</bodyText>
<footnote confidence="0.838769333333333">
2In our implementation, we keep a global cache Hj for each
negative example xj. Therefore, in Algorithm 2, we start with
a non-empty cache improving the speed significantly.
</footnote>
<figure confidence="0.890212958333333">
� X
2 Ilul12 +
E(−uT X
s
h*i,sOs(xi))
min
u
i:yz=1
X
+
E(max
hEC
uT X
s
min
u
E(−uT X h*i,sOs(xi))
i:yz=1
X
+
E(max
hEHj
uT X
s
</figure>
<page confidence="0.994315">
432
</page>
<bodyText confidence="0.94208625">
graphs, G1 and G2, corresponding to the first
and second input sentences. Each vertex in the
graph contains word information (lemma and part-
of-speech) and the edges denote dependency rela-
tions, generated by the Stanford parser (Klein and
Manning, 2003). The intermediate representation
for this task can now be defined as an alignment be-
tween the graphs, which captures lexical and syntac-
tic correlations between the sentences.
We use V (G) and E(G) to denote the set of ver-
tices and edges in G respectively, and define four
hidden variable types to encode vertex and edge
mappings between G1 and G2.
• The word-mapping variables, denoted by
hv1,v2, define possible pairings of vertices,
where v1 E V (G1) and v2 E V (G2).
</bodyText>
<listItem confidence="0.903277444444444">
• The edge-mapping variables, denoted by
he1,e2, define possible pairings of the graphs
edges, where e1 E E(G1) and e2 E E(G2).
• The word-deletion variables hv1,* (or h*,v2) al-
low for vertices v1 E V (G1) (or v2 E V (G2))
to be deleted. This accounts for omission of
words (like function words).
• The edge-deletion variables, he1,* (or h*,e2) al-
low for deletion of edges from G1 (or G2).
</listItem>
<bodyText confidence="0.927058666666667">
Our inference problem is to find the optimal set of
hidden variable activations, restricted according to
the following set of linear constraints
</bodyText>
<listItem confidence="0.9976405">
• Each vertex in G1 (or G2) can either be mapped
to a single vertex in G2 (or G1) or marked as
deleted. In terms of the word-mapping and
word-deletion variables, we have
</listItem>
<equation confidence="0.99972275">
bv1 E V (G1); hv1,* + � hv1,v2 = 1 (6)
v2EV(G2)
bv2 E V (G2); h*,v2 + � hv1,v2 = 1 (7)
v1EV (G1)
</equation>
<listItem confidence="0.99934825">
• Each edge in G1 (or G2) can either be mapped
to a single edge in G2 (or G1) or marked as
deleted. In terms of the edge-mapping and
edge-deletion variables, we have
</listItem>
<equation confidence="0.9932875">
be1 E E(G1); he1,* + � he1,e2 = 1 (8)
e2EE(G2)
be2 E E(G2); h*,e2 + � he1,e2 = 1 (9)
e1EE(G1)
</equation>
<listItem confidence="0.5877106">
• The edge mappings can be active if, and only
if, the corresponding node mappings are ac-
tive. Suppose e1 = (v1, vi) E E(G1) and
e2 = (v2, v2) E E(G2), where v1, vi E V (G1)
and v2, v2 E V (G2). Then, we have
</listItem>
<equation confidence="0.999891">
hv1,v2 + hvl,v2 − he1,e2 : 51 (10)
hv1,v2 &gt; he1,e2; hvl,v2 &gt; he1,e2 (11)
</equation>
<bodyText confidence="0.99995975">
These constraints define the feasible set for the in-
ference problem specified in Equation (1). This in-
ference problem can be formulated as an ILP prob-
lem with the objective function from Equation (1):
</bodyText>
<equation confidence="0.954956">
hsuT os(x)
subject to (6)-(11); bs; hs E 10, 11 (12)
</equation>
<bodyText confidence="0.999841">
This example demonstrates the use of integer linear
programming to define intermediate representations
incorporating domain intuition.
</bodyText>
<sectionHeader confidence="0.999284" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999928892857143">
We applied our framework to three different NLP
tasks: transliteration discovery (Klementiev and
Roth, 2008), RTE (Dagan et al., 2006), and para-
phrase identification (Dolan et al., 2004).
Our experiments are designed to answer the fol-
lowing research question: “Given a binary classifi-
cation problem defined over latent representations,
will the joint LCLR algorithm perform better than a
two-stage approach?” To ensure a fair comparison,
both systems use the same feature functions and def-
inition of intermediate representation. We use the
same ILP formulation in both configurations, with a
single exception – the objective function parameters:
the two stage approach uses a task-specific heuristic,
while LCLR learns it iteratively.
The ILP formulation results in very strong two
stage systems. For example, in the paraphrase iden-
tification task, even our two stage system is the cur-
rent state-of-the-art performance. In these settings,
the improvement obtained by our joint approach is
non-trivial and can be clearly attributed to the su-
periority of the joint learning algorithm. Interest-
ingly, we find that our more general approach is bet-
ter than specially designed joint approaches (Gold-
wasser and Roth, 2008b; Das and Smith, 2009).
Since the objective function (3) of the joint ap-
proach is not convex, a good initialization is re-
quired. We use the weight vector learned by the two
</bodyText>
<figure confidence="0.91426125">
max
h
�
s
</figure>
<page confidence="0.999074">
433
</page>
<bodyText confidence="0.999989">
stage approach as the starting point for the joint ap-
proach. The algorithm terminates when the relative
improvement of the objective is smaller than 10−5.
</bodyText>
<subsectionHeader confidence="0.997218">
5.1 Transliteration Discovery
</subsectionHeader>
<bodyText confidence="0.999580162790698">
Transliteration discovery is the problem of iden-
tifying if a word pair, possibly written using two
different character sets, refers to the same underly-
ing entity. The intermediate representation consists
of all possible character mappings between the two
character sets. Identifying this mapping is not easy,
as most writing systems do not perfectly align pho-
netically and orthographically; rather, this mapping
can be context-dependent and ambiguous.
For an input pair of words (w1, w2), the interme-
diate structure h is a mapping between their charac-
ters, with the latent variable hid indicating if the ith
character in w1 is aligned to the jth character in w2.
The feature vector associated with the variable hid
contains unigram character mapping, bigram char-
acter mapping (by considering surrounding charac-
ters). We adopt the one-to-one mapping and non-
crossing constraint used in (Chang et al., 2009).
We evaluated our system using the English-
Hebrew corpus (Goldwasser and Roth, 2008a),
which consists of 250 positive transliteration pairs
for training, and 300 pairs for testing. As negative
examples for training, we sample 10% from random
pairings of words from the positive data. We report
two evaluation measurements – (1) the Mean Recip-
rocal Rank (MRR), which is the average of the mul-
tiplicative inverse of the rank of the correct answer,
and (2) the accuracy (Acc), which is the percentage
of the top rank candidates being correct.
We initialized the two stage inference process as
detailed in (Chang et al., 2009) using a Romaniza-
tion table to assign uniform weights to prominent
character mappings. This initialization procedure
resembles the approach used in (Bergsma and Kon-
drak, 2007). An alignment is first built by solving
the constrained optimization problem. Then, a sup-
port vector machine with squared-hinge loss func-
tion is used to train a classifier using features ex-
tracted from the alignment. We refer to this two
stage approach as Alignment+Learning.
The results summarized in Table 1 show the sig-
nificant improvement obtained by the joint approach
(95.4% MRR) compared to the two stage approach
</bodyText>
<table confidence="0.997662">
Transliteration System Acc MRR
(Goldwasser and Roth, N/A 89.4
2008b)
Alignment + Learning 80.0 85.7
LCLR 92.3 95.4
</table>
<tableCaption confidence="0.7324382">
Table 1: Experimental results for transliteration. We compare
a two-stage system: `Alignment+Learning” with LCLR, our
joint algorithm. Both `Alignment+Learning” and LCLR use
the same features and the same intermediate representation def-
inition.
</tableCaption>
<bodyText confidence="0.695062">
(85.7%). Moreover, LCLR outperforms the joint
system introduced in (Goldwasser and Roth, 2008b).
</bodyText>
<subsectionHeader confidence="0.996865">
5.2 Textual Entailment
</subsectionHeader>
<bodyText confidence="0.99989659375">
Recognizing Textual Entailment (RTE) is an im-
portant textual inference task of predicting if a given
text snippet, entails the meaning of another (the hy-
pothesis). In many current RTE systems, the entail-
ment decision depends on successfully aligning the
constituents of the text and hypothesis, accounting
for the internal linguistic structure of the input.
The raw input – the text and hypothesis – are
represented as directed acyclic graphs, where ver-
tices correspond to words. Directed edges link verbs
to the head words of semantic role labeling argu-
ments produced by (Punyakanok et al., 2008). All
other words are connected by dependency edges.
The intermediate representation is an alignment be-
tween the nodes and edges of the graphs. We used
three hidden variable types from Section 4 – word-
mapping, word-deletion and edge-mapping, along
with the associated constraints as defined earlier.
Since the text is typically much longer than the hy-
pothesis, we create word-deletion latent variables
(and features) only for the hypothesis.
The second column of Table 2 lists the resources
used to generate features corresponding to each hid-
den variable type. For word-mapping variables, the
features include a WordNet based metric (WNSim),
indicators for the POS tags and negation identifiers.
We used the state-of-the-art coreference resolution
system of (Bengtson and Roth, 2008) to identify the
canonical entities for pronouns and extract features
accordingly. For word deletion, we use only the POS
tags of the corresponding tokens (generated by the
LBJ POS tagger3) to generate features. For edge
</bodyText>
<footnote confidence="0.961457">
3http://L2R.cs.uiuc.edu/˜cogcomp/software.php
</footnote>
<page confidence="0.994016">
434
</page>
<table confidence="0.9994865">
Hidden RTE Paraphrase
Variable features features
word-mapping WordNet, POS, WordNet, POS,
Coref, Neg NE, ED
word-deletion POS POS, NE
edge-mapping NODE-INFO NODE-INFO,
DEP
edge-deletion N/A DEP
</table>
<tableCaption confidence="0.966254375">
Table 2: Summary of latent variables and feature resources for
the entailment and paraphrase identification tasks. See Section
4 for an explanation of the hidden variable types. The linguistic
resources used to generate features are abbreviated as follows –
POS: Part of speech, Coref: Canonical coreferent entities; NE:
Named Entity, ED: Edit distance, Neg: Negation markers, DEP:
Dependency labels, NODE-INFO: corresponding node align-
ment resources, N/A: Hidden variable not used.
</tableCaption>
<table confidence="0.999006">
Entailment System Acc
Median of TAC 2009 systems 61.5
Alignment + Learning 65.0
LCLR 66.8
</table>
<tableCaption confidence="0.9774875">
Table 3: Experimental results for recognizing textual entail-
ment. The first row is the median of best performing systems of
all teams that participated in the RTE5 challenge (Bentivogli et
al., 2009). Alignment + Learning is our two-stage system im-
plementation, and LCLR is our joint learning algorithm. Details
about these systems are provided in the text.
</tableCaption>
<bodyText confidence="0.999817">
mapping variables, we include the features of the
corresponding word mapping variables, scaled by
the word similarity of the words forming the edge.
We evaluated our system using the RTE-5
data (Bentivogli et al., 2009), consisting of 600 sen-
tence pairs for training and testing respectively, in
which positive and negative examples are equally
distributed. In these experiments the joint LCLR al-
gorithm converged after 5 iterations.
For the two stage system, we used WN-
Sim to score alignments during inference. The
word-based scores influence the edge variables
via the constraints. This two-stage system (the
Alignment+Learning system) is significantly better
than the median performance of the RTE-5 submis-
sions. Using LCLR further improves the result by al-
most 2%, a substantial improvement in this domain.
</bodyText>
<subsectionHeader confidence="0.998246">
5.3 Paraphrase Identification
</subsectionHeader>
<bodyText confidence="0.998791">
Our final task is Paraphrase Identification, dis-
cussed in detail at Section 4. We use all the four
hidden variable types described in that section. The
features used are similar to those described earlier
</bodyText>
<note confidence="0.6067448">
Paraphrase System Acc
Experiments using (Dolan et al., 2004)
(Qiu et al., 2006) 72.00
(Das and Smith, 2009) 73.86
(Wan et al., 2006) 75.60
</note>
<table confidence="0.9775376">
Alignment + Learning 76.23
LCLR 76.41
Experiments using Extended data set
Alignment + Learning 72.00
LCLR 72.75
</table>
<tableCaption confidence="0.999347">
Table 4: Experimental Result For Paraphrasing Identification.
</tableCaption>
<bodyText confidence="0.943926441176471">
Our joint LCLR approach achieves the best results compared
to several previously published systems, and our own two stage
system implementation (Alignment + Learning). We evaluated
the systems performance across two datasets: (Dolan et al.,
2004) dataset and the Extended dataset, see the text for details.
Note that LCLR outperforms (Das and Smith, 2009), which is a
specifically designed joint approach for this task.
for the RTE system and are summarized in Table 2.
We used the MSR paraphrase dataset of (Dolan
et al., 2004) for empirical evaluation. Additionally,
we generated a second corpus (called the Extended
dataset) by sampling 500 sentence pairs from the
MSR dataset for training and using the entire test
collection of the original dataset. In the Extended
dataset, for every sentence pair, we extended the
longer sentence by concatenating it with itself. This
results in a more difficult inference problem because
it allows more mappings between words. Note that
the performance on the original dataset sets the ceil-
ing on the second one.
The results are summarized in Table 4. The first
part of the table compares the LCLR system with
a two stage system (Alignment + Learning) and
three published results that use the MSR dataset.
(We only list single systems in the table4) Inter-
estingly, although still outperformed by our joint
LCLR algorithm, the two stage system is able per-
form significantly better than existing systems for
that dataset (Qiu et al., 2006; Das and Smith, 2009;
Wan et al., 2006). We attribute this improvement,
consistent across both the ILP based systems, to the
intermediate representation we defined.
We hypothesize that the similarity in performance
between the joint LCLR algorithm and the two stage
</bodyText>
<footnote confidence="0.8748715">
4Previous work (Das and Smith, 2009) has shown that com-
bining the results of several systems improves performance.
</footnote>
<page confidence="0.998418">
435
</page>
<bodyText confidence="0.998669833333333">
(Alignment + Learning) systems is due to the limited
intermediate representation space for input pairs in
this dataset. We evaluated these systems on the more
difficult Extended dataset. Results indeed show that
the margin between the two systems increases as the
inference problem becomes harder.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999968530612245">
Recent NLP research has largely focused on two-
stage approaches. Examples include RTE (Zanzotto
and Moschitti, 2006; MacCartney et al., 2008; Roth
et al., 2009); string matching (Bergsma and Kon-
drak, 2007); transliteration (Klementiev and Roth,
2008); and paraphrase identification (Qiu et al.,
2006; Wan et al., 2006).
(MacCartney et al., 2008) considered construct-
ing a latent representation to be an independent task
and used manually labeled alignment data (Brockett,
2007) to tune the inference procedure parameters.
While this method identifies alignments well, it does
not improve entailment decisions. This strengthens
our intuition that the latent representation should be
guided by the final task.
There are several exceptions to the two-stage ap-
proach in the NLP community (Haghighi et al.,
2005; McCallum et al., 2005; Goldwasser and Roth,
2008b; Das and Smith, 2009); however, the interme-
diate representation and the inference for construct-
ing it are closely coupled with the application task.
In contrast, LCLR provides a general formulation
that allows the use of expressive constraints, mak-
ing it applicable to many NLP tasks.
Unlike other latent variable SVM frameworks
(Felzenszwalb et al., 2009; Yu and Joachims, 2009)
which often use task-specific inference procedure,
LCLR utilizes the declarative inference framework
that allows using constraints over intermediate rep-
resentation and provides a general platform for a
wide range of NLP tasks.
The optimization procedure in this work and
(Felzenszwalb et al., 2009) are quite different.
We use the coordinate descent and cutting-plane
methods ensuring we have fewer parameters and
the inference procedure can be easily parallelized.
Our procedure also allows different loss functions.
(Cherry and Quirk, 2008) adopts the Latent SVM al-
gorithm to define a language model. Unfortunately,
their implementation is not guaranteed to converge.
In CRF-like models with latent variables (McCal-
lum et al., 2005), the decision function marginal-
izes over the all hidden states when presented with
an input example. Unfortunately, the computational
cost of applying their framework is prohibitive with
constrained latent representations. In contrast, our
framework requires only the best hidden representa-
tion instead of marginalizing over all possible repre-
sentations, thus reducing the computational effort.
</bodyText>
<sectionHeader confidence="0.998084" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.957155583333333">
We consider the problem of learning over an inter-
mediate representation. We assume the existence of
a latent structure in the input, relevant to the learn-
ing problem, but not accessible to the learning algo-
rithm. Many NLP tasks fall into these settings and
each can consider a different hidden input structure.
We propose a unifying thread for the different prob-
lems and present a novel framework for Learning
over Constrained Latent Representations (LCLR).
Our framework can be applied to many different la-
tent representations such as parse trees, orthographic
mapping and tree alignments. Our approach con-
trasts with existing work in which learning is done
over a fixed representation, as we advocate jointly
learning it with the final task.
We successfully apply the proposed framework to
three learning tasks – Transliteration, Textual En-
tailment and Paraphrase Identification. Our joint
LCLR algorithm achieves superior performance in
all three tasks. We attribute the performance im-
provement to our novel training algorithm and flex-
ible inference procedure, allowing us to encode do-
main knowledge. This presents an interesting line of
future work in which more linguistic intuitions can
be encoded into the learning problem. For these rea-
sons, we believe that our framework provides an im-
portant step forward in understanding the problem
of learning over hidden structured inputs.
Acknowledgment We thank James Clarke and Mark Sam-
mons for their insightful comments. This research was partly sponsored
by the Army Research Laboratory (ARL) (accomplished under Cooper-
ative Agreement Number W911NF-09-2-0053) and by Air Force Re-
search Laboratory (AFRL) under prime contract no. FA8750-09-C-
0181. Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not necessarily
reflect the view of the ARL or of AFRL.
</bodyText>
<page confidence="0.999139">
436
</page>
<sectionHeader confidence="0.993897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997845">
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In Proc. of TAC Work-
shop.
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In ACL.
C. Brockett. 2007. Aligning the RTE 2006 corpus.
In Technical Report MSR-TR-2007-77, Microsoft Re-
search.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
C. Cherry and C. Quirk. 2008. Discriminative, syntactic
language modeling through latent svms. In Proc. of
the Eighth Conference of AMTA.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
ACL.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrimina-
tively trained part based models. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In ACL. Short
Paper.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In EMNLP.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In HLT-EMNLP.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural svms. Machine
Learning.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
NIPS.
A. Klementiev and D. Roth. 2008. Named entity translit-
eration and discovery in multilingual corpora. In
Cyril Goutte, Nicola Cancedda, Marc Dymetman, and
George Foster, editors, Learning Machine Translation.
B. MacCartney, M. Galley, and C. D. Manning. 2008.
A phrase-based alignment model for natural language
inference. In EMNLP.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In UAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
EMNLP.
D. Roth, M. Sammons, and V.G. Vydiswaran. 2009. A
framework for entailed relation recognition. In ACL.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the ¨para-farce¨out
of paraphrase. In Proc. of the Australasian Language
Technology Workshop (ALTW).
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In ICML.
F. M. Zanzotto and A. Moschitti. 2006. Automatic learn-
ing of textual entailments with cross-pair similarities.
In ACL.
</reference>
<page confidence="0.998625">
437
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984905">
<title confidence="0.999498">Discriminative Learning over Constrained Latent Representations</title>
<author confidence="0.995057">Chang Goldwasser Roth</author>
<affiliation confidence="0.999914">University of Illinois at Urbana</affiliation>
<address confidence="0.998102">Urbana, IL 61801</address>
<abstract confidence="0.999636409090909">This paper proposes a general learning framework for a class of problems that require learning over latent intermediate representations. Many natural language processing (NLP) decision problems are defined over an expressive intermediate representation that is not explicit in the input, leaving the algorithm with both the task of recovering a good intermediate representation and learning to classify correctly. Most current systems separate the learning problem into two stages by solving the first step of recovering the intermediate representation heuristically and using it to learn the final classifier. This paper develops a novel joint learning algorithm for both tasks, that uses the final prediction to guide the selection of the best intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identification and textual entailment – and show that our joint method significantly improves performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bengtson</author>
<author>D Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="24151" citStr="Bengtson and Roth, 2008" startWordPosition="3930" endWordPosition="3933">idden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the features include a WordNet based metric (WNSim), indicators for the POS tags and negation identifiers. We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3) to generate features. For edge 3http://L2R.cs.uiuc.edu/˜cogcomp/software.php 434 Hidden RTE Paraphrase Variable features features word-mapping WordNet, POS, WordNet, POS, Coref, Neg NE, ED word-deletion POS POS, NE edge-mapping NODE-INFO NODE-INFO, DEP edge-deletion N/A DEP Table 2: Summary of latent variables and feature resources for the entailment and paraphrase identification tasks. See Section 4 for an</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>E. Bengtson and D. Roth. 2008. Understanding the value of features for coreference resolution. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bentivogli</author>
<author>I Dagan</author>
<author>H T Dang</author>
<author>D Giampiccolo</author>
<author>B Magnini</author>
</authors>
<title>The fifth PASCAL recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Proc. of TAC Workshop.</booktitle>
<contexts>
<context position="25388" citStr="Bentivogli et al., 2009" startWordPosition="4110" endWordPosition="4113">of the hidden variable types. The linguistic resources used to generate features are abbreviated as follows – POS: Part of speech, Coref: Canonical coreferent entities; NE: Named Entity, ED: Edit distance, Neg: Negation markers, DEP: Dependency labels, NODE-INFO: corresponding node alignment resources, N/A: Hidden variable not used. Entailment System Acc Median of TAC 2009 systems 61.5 Alignment + Learning 65.0 LCLR 66.8 Table 3: Experimental results for recognizing textual entailment. The first row is the median of best performing systems of all teams that participated in the RTE5 challenge (Bentivogli et al., 2009). Alignment + Learning is our two-stage system implementation, and LCLR is our joint learning algorithm. Details about these systems are provided in the text. mapping variables, we include the features of the corresponding word mapping variables, scaled by the word similarity of the words forming the edge. We evaluated our system using the RTE-5 data (Bentivogli et al., 2009), consisting of 600 sentence pairs for training and testing respectively, in which positive and negative examples are equally distributed. In these experiments the joint LCLR algorithm converged after 5 iterations. For the</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and B. Magnini. 2009. The fifth PASCAL recognizing textual entailment challenge. In Proc. of TAC Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>G Kondrak</author>
</authors>
<title>Alignment-based discriminative string similarity.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2526" citStr="Bergsma and Kondrak, 2007" startWordPosition="374" endWordPosition="377">operties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning ta</context>
<context position="21869" citStr="Bergsma and Kondrak, 2007" startWordPosition="3580" endWordPosition="3584"> testing. As negative examples for training, we sample 10% from random pairings of words from the positive data. We report two evaluation measurements – (1) the Mean Reciprocal Rank (MRR), which is the average of the multiplicative inverse of the rank of the correct answer, and (2) the accuracy (Acc), which is the percentage of the top rank candidates being correct. We initialized the two stage inference process as detailed in (Chang et al., 2009) using a Romanization table to assign uniform weights to prominent character mappings. This initialization procedure resembles the approach used in (Bergsma and Kondrak, 2007). An alignment is first built by solving the constrained optimization problem. Then, a support vector machine with squared-hinge loss function is used to train a classifier using features extracted from the alignment. We refer to this two stage approach as Alignment+Learning. The results summarized in Table 1 show the significant improvement obtained by the joint approach (95.4% MRR) compared to the two stage approach Transliteration System Acc MRR (Goldwasser and Roth, N/A 89.4 2008b) Alignment + Learning 80.0 85.7 LCLR 92.3 95.4 Table 1: Experimental results for transliteration. We compare a</context>
<context position="29286" citStr="Bergsma and Kondrak, 2007" startWordPosition="4724" endWordPosition="4728">work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. 435 (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 200</context>
</contexts>
<marker>Bergsma, Kondrak, 2007</marker>
<rawString>S. Bergsma and G. Kondrak. 2007. Alignment-based discriminative string similarity. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<tech>Technical Report MSR-TR-2007-77, Microsoft Research.</tech>
<contexts>
<context position="29558" citStr="Brockett, 2007" startWordPosition="4766" endWordPosition="4767">ded dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use o</context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>C. Brockett. 2007. Aligning the RTE 2006 corpus. In Technical Report MSR-TR-2007-77, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>D Goldwasser</author>
<author>D Roth</author>
<author>Y Tu</author>
</authors>
<title>Unsupervised constraint driven learning for transliteration discovery.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3353" citStr="Chang et al., 2009" startWordPosition="506" endWordPosition="509">hus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constra</context>
<context position="21072" citStr="Chang et al., 2009" startWordPosition="3452" endWordPosition="3455">is mapping is not easy, as most writing systems do not perfectly align phonetically and orthographically; rather, this mapping can be context-dependent and ambiguous. For an input pair of words (w1, w2), the intermediate structure h is a mapping between their characters, with the latent variable hid indicating if the ith character in w1 is aligned to the jth character in w2. The feature vector associated with the variable hid contains unigram character mapping, bigram character mapping (by considering surrounding characters). We adopt the one-to-one mapping and noncrossing constraint used in (Chang et al., 2009). We evaluated our system using the EnglishHebrew corpus (Goldwasser and Roth, 2008a), which consists of 250 positive transliteration pairs for training, and 300 pairs for testing. As negative examples for training, we sample 10% from random pairings of words from the positive data. We report two evaluation measurements – (1) the Mean Reciprocal Rank (MRR), which is the average of the multiplicative inverse of the rank of the correct answer, and (2) the accuracy (Acc), which is the percentage of the top rank candidates being correct. We initialized the two stage inference process as detailed i</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Tu, 2009</marker>
<rawString>M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009. Unsupervised constraint driven learning for transliteration discovery. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>C Quirk</author>
</authors>
<title>Discriminative, syntactic language modeling through latent svms.</title>
<date>2008</date>
<booktitle>In Proc. of the Eighth Conference of AMTA.</booktitle>
<contexts>
<context position="30861" citStr="Cherry and Quirk, 2008" startWordPosition="4959" endWordPosition="4962">atent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NLP tasks. The optimization procedure in this work and (Felzenszwalb et al., 2009) are quite different. We use the coordinate descent and cutting-plane methods ensuring we have fewer parameters and the inference procedure can be easily parallelized. Our procedure also allows different loss functions. (Cherry and Quirk, 2008) adopts the Latent SVM algorithm to define a language model. Unfortunately, their implementation is not guaranteed to converge. In CRF-like models with latent variables (McCallum et al., 2005), the decision function marginalizes over the all hidden states when presented with an input example. Unfortunately, the computational cost of applying their framework is prohibitive with constrained latent representations. In contrast, our framework requires only the best hidden representation instead of marginalizing over all possible representations, thus reducing the computational effort. 7 Conclusion</context>
</contexts>
<marker>Cherry, Quirk, 2008</marker>
<rawString>C. Cherry and C. Quirk. 2008. Discriminative, syntactic language modeling through latent svms. In Proc. of the Eighth Conference of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
<author>editors</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<contexts>
<context position="1554" citStr="Dagan et al., 2006" startWordPosition="221" endWordPosition="224">inal prediction to guide the selection of the best intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identification and textual entailment – and show that our joint method significantly improves performance. 1 Introduction Many NLP tasks can be phrased as decision problems over complex linguistic structures. Successful learning depends on correctly encoding these (often latent) structures as features for the learning system. Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1, and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al</context>
<context position="18715" citStr="Dagan et al., 2006" startWordPosition="3079" endWordPosition="3082"> we have hv1,v2 + hvl,v2 − he1,e2 : 51 (10) hv1,v2 &gt; he1,e2; hvl,v2 &gt; he1,e2 (11) These constraints define the feasible set for the inference problem specified in Equation (1). This inference problem can be formulated as an ILP problem with the objective function from Equation (1): hsuT os(x) subject to (6)-(11); bs; hs E 10, 11 (12) This example demonstrates the use of integer linear programming to define intermediate representations incorporating domain intuition. 5 Experiments We applied our framework to three different NLP tasks: transliteration discovery (Klementiev and Roth, 2008), RTE (Dagan et al., 2006), and paraphrase identification (Dolan et al., 2004). Our experiments are designed to answer the following research question: “Given a binary classification problem defined over latent representations, will the joint LCLR algorithm perform better than a two-stage approach?” To ensure a fair comparison, both systems use the same feature functions and definition of intermediate representation. We use the same ILP formulation in both configurations, with a single exception – the objective function parameters: the two stage approach uses a task-specific heuristic, while LCLR learns it iteratively.</context>
</contexts>
<marker>Dagan, Glickman, Magnini, editors, 2006</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini, editors. 2006. The PASCAL Recognising Textual Entailment Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3375" citStr="Das and Smith, 2009" startWordPosition="510" endWordPosition="513">ing into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constrained Latent Representa</context>
<context position="19818" citStr="Das and Smith, 2009" startWordPosition="3249" endWordPosition="3252">jective function parameters: the two stage approach uses a task-specific heuristic, while LCLR learns it iteratively. The ILP formulation results in very strong two stage systems. For example, in the paraphrase identification task, even our two stage system is the current state-of-the-art performance. In these settings, the improvement obtained by our joint approach is non-trivial and can be clearly attributed to the superiority of the joint learning algorithm. Interestingly, we find that our more general approach is better than specially designed joint approaches (Goldwasser and Roth, 2008b; Das and Smith, 2009). Since the objective function (3) of the joint approach is not convex, a good initialization is required. We use the weight vector learned by the two max h � s 433 stage approach as the starting point for the joint approach. The algorithm terminates when the relative improvement of the objective is smaller than 10−5. 5.1 Transliteration Discovery Transliteration discovery is the problem of identifying if a word pair, possibly written using two different character sets, refers to the same underlying entity. The intermediate representation consists of all possible character mappings between the</context>
<context position="26700" citStr="Das and Smith, 2009" startWordPosition="4315" endWordPosition="4318"> influence the edge variables via the constraints. This two-stage system (the Alignment+Learning system) is significantly better than the median performance of the RTE-5 submissions. Using LCLR further improves the result by almost 2%, a substantial improvement in this domain. 5.3 Paraphrase Identification Our final task is Paraphrase Identification, discussed in detail at Section 4. We use all the four hidden variable types described in that section. The features used are similar to those described earlier Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.60 Alignment + Learning 76.23 LCLR 76.41 Experiments using Extended data set Alignment + Learning 72.00 LCLR 72.75 Table 4: Experimental Result For Paraphrasing Identification. Our joint LCLR approach achieves the best results compared to several previously published systems, and our own two stage system implementation (Alignment + Learning). We evaluated the systems performance across two datasets: (Dolan et al., 2004) dataset and the Extended dataset, see the text for details. Note that LCLR outperforms (Das and Smith, 2009), which is a specifically designed join</context>
<context position="28404" citStr="Das and Smith, 2009" startWordPosition="4589" endWordPosition="4592">lts in a more difficult inference problem because it allows more mappings between words. Note that the performance on the original dataset sets the ceiling on the second one. The results are summarized in Table 4. The first part of the table compares the LCLR system with a two stage system (Alignment + Learning) and three published results that use the MSR dataset. (We only list single systems in the table4) Interestingly, although still outperformed by our joint LCLR algorithm, the two stage system is able perform significantly better than existing systems for that dataset (Qiu et al., 2006; Das and Smith, 2009; Wan et al., 2006). We attribute this improvement, consistent across both the ILP based systems, to the intermediate representation we defined. We hypothesize that the similarity in performance between the joint LCLR algorithm and the two stage 4Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. 435 (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the </context>
<context position="29960" citStr="Das and Smith, 2009" startWordPosition="4826" endWordPosition="4829">aphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NLP tasks. The optimizati</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>D. Das and N. A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In</title>
<date>2004</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence.</journal>
<contexts>
<context position="1605" citStr="Dolan et al., 2004" startWordPosition="228" endWordPosition="231">intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identification and textual entailment – and show that our joint method significantly improves performance. 1 Introduction Many NLP tasks can be phrased as decision problems over complex linguistic structures. Successful learning depends on correctly encoding these (often latent) structures as features for the learning system. Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1, and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining</context>
<context position="18767" citStr="Dolan et al., 2004" startWordPosition="3087" endWordPosition="3090"> he1,e2; hvl,v2 &gt; he1,e2 (11) These constraints define the feasible set for the inference problem specified in Equation (1). This inference problem can be formulated as an ILP problem with the objective function from Equation (1): hsuT os(x) subject to (6)-(11); bs; hs E 10, 11 (12) This example demonstrates the use of integer linear programming to define intermediate representations incorporating domain intuition. 5 Experiments We applied our framework to three different NLP tasks: transliteration discovery (Klementiev and Roth, 2008), RTE (Dagan et al., 2006), and paraphrase identification (Dolan et al., 2004). Our experiments are designed to answer the following research question: “Given a binary classification problem defined over latent representations, will the joint LCLR algorithm perform better than a two-stage approach?” To ensure a fair comparison, both systems use the same feature functions and definition of intermediate representation. We use the same ILP formulation in both configurations, with a single exception – the objective function parameters: the two stage approach uses a task-specific heuristic, while LCLR learns it iteratively. The ILP formulation results in very strong two stag</context>
<context position="26653" citStr="Dolan et al., 2004" startWordPosition="4306" endWordPosition="4309">nments during inference. The word-based scores influence the edge variables via the constraints. This two-stage system (the Alignment+Learning system) is significantly better than the median performance of the RTE-5 submissions. Using LCLR further improves the result by almost 2%, a substantial improvement in this domain. 5.3 Paraphrase Identification Our final task is Paraphrase Identification, discussed in detail at Section 4. We use all the four hidden variable types described in that section. The features used are similar to those described earlier Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.60 Alignment + Learning 76.23 LCLR 76.41 Experiments using Extended data set Alignment + Learning 72.00 LCLR 72.75 Table 4: Experimental Result For Paraphrasing Identification. Our joint LCLR approach achieves the best results compared to several previously published systems, and our own two stage system implementation (Alignment + Learning). We evaluated the systems performance across two datasets: (Dolan et al., 2004) dataset and the Extended dataset, see the text for details. Note that LCLR outperforms (Das and Smit</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>W. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In COLING. P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. 2009. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Active sample selection for named entity transliteration.</title>
<date>2008</date>
<booktitle>In ACL. Short Paper.</booktitle>
<contexts>
<context position="3332" citStr="Goldwasser and Roth, 2008" startWordPosition="501" endWordPosition="505">termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation.</context>
<context position="19795" citStr="Goldwasser and Roth, 2008" startWordPosition="3244" endWordPosition="3248"> a single exception – the objective function parameters: the two stage approach uses a task-specific heuristic, while LCLR learns it iteratively. The ILP formulation results in very strong two stage systems. For example, in the paraphrase identification task, even our two stage system is the current state-of-the-art performance. In these settings, the improvement obtained by our joint approach is non-trivial and can be clearly attributed to the superiority of the joint learning algorithm. Interestingly, we find that our more general approach is better than specially designed joint approaches (Goldwasser and Roth, 2008b; Das and Smith, 2009). Since the objective function (3) of the joint approach is not convex, a good initialization is required. We use the weight vector learned by the two max h � s 433 stage approach as the starting point for the joint approach. The algorithm terminates when the relative improvement of the objective is smaller than 10−5. 5.1 Transliteration Discovery Transliteration discovery is the problem of identifying if a word pair, possibly written using two different character sets, refers to the same underlying entity. The intermediate representation consists of all possible charact</context>
<context position="21155" citStr="Goldwasser and Roth, 2008" startWordPosition="3465" endWordPosition="3468">ically and orthographically; rather, this mapping can be context-dependent and ambiguous. For an input pair of words (w1, w2), the intermediate structure h is a mapping between their characters, with the latent variable hid indicating if the ith character in w1 is aligned to the jth character in w2. The feature vector associated with the variable hid contains unigram character mapping, bigram character mapping (by considering surrounding characters). We adopt the one-to-one mapping and noncrossing constraint used in (Chang et al., 2009). We evaluated our system using the EnglishHebrew corpus (Goldwasser and Roth, 2008a), which consists of 250 positive transliteration pairs for training, and 300 pairs for testing. As negative examples for training, we sample 10% from random pairings of words from the positive data. We report two evaluation measurements – (1) the Mean Reciprocal Rank (MRR), which is the average of the multiplicative inverse of the rank of the correct answer, and (2) the accuracy (Acc), which is the percentage of the top rank candidates being correct. We initialized the two stage inference process as detailed in (Chang et al., 2009) using a Romanization table to assign uniform weights to prom</context>
<context position="22744" citStr="Goldwasser and Roth, 2008" startWordPosition="3712" endWordPosition="3715">ach as Alignment+Learning. The results summarized in Table 1 show the significant improvement obtained by the joint approach (95.4% MRR) compared to the two stage approach Transliteration System Acc MRR (Goldwasser and Roth, N/A 89.4 2008b) Alignment + Learning 80.0 85.7 LCLR 92.3 95.4 Table 1: Experimental results for transliteration. We compare a two-stage system: `Alignment+Learning” with LCLR, our joint algorithm. Both `Alignment+Learning” and LCLR use the same features and the same intermediate representation definition. (85.7%). Moreover, LCLR outperforms the joint system introduced in (Goldwasser and Roth, 2008b). 5.2 Textual Entailment Recognizing Textual Entailment (RTE) is an important textual inference task of predicting if a given text snippet, entails the meaning of another (the hypothesis). In many current RTE systems, the entailment decision depends on successfully aligning the constituents of the text and hypothesis, accounting for the internal linguistic structure of the input. The raw input – the text and hypothesis – are represented as directed acyclic graphs, where vertices correspond to words. Directed edges link verbs to the head words of semantic role labeling arguments produced by (</context>
<context position="29937" citStr="Goldwasser and Roth, 2008" startWordPosition="4822" endWordPosition="4825">iev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NL</context>
</contexts>
<marker>Goldwasser, Roth, 2008</marker>
<rawString>D. Goldwasser and D. Roth. 2008a. Active sample selection for named entity transliteration. In ACL. Short Paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Transliteration as constrained optimization.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3332" citStr="Goldwasser and Roth, 2008" startWordPosition="501" endWordPosition="505">termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation.</context>
<context position="19795" citStr="Goldwasser and Roth, 2008" startWordPosition="3244" endWordPosition="3248"> a single exception – the objective function parameters: the two stage approach uses a task-specific heuristic, while LCLR learns it iteratively. The ILP formulation results in very strong two stage systems. For example, in the paraphrase identification task, even our two stage system is the current state-of-the-art performance. In these settings, the improvement obtained by our joint approach is non-trivial and can be clearly attributed to the superiority of the joint learning algorithm. Interestingly, we find that our more general approach is better than specially designed joint approaches (Goldwasser and Roth, 2008b; Das and Smith, 2009). Since the objective function (3) of the joint approach is not convex, a good initialization is required. We use the weight vector learned by the two max h � s 433 stage approach as the starting point for the joint approach. The algorithm terminates when the relative improvement of the objective is smaller than 10−5. 5.1 Transliteration Discovery Transliteration discovery is the problem of identifying if a word pair, possibly written using two different character sets, refers to the same underlying entity. The intermediate representation consists of all possible charact</context>
<context position="21155" citStr="Goldwasser and Roth, 2008" startWordPosition="3465" endWordPosition="3468">ically and orthographically; rather, this mapping can be context-dependent and ambiguous. For an input pair of words (w1, w2), the intermediate structure h is a mapping between their characters, with the latent variable hid indicating if the ith character in w1 is aligned to the jth character in w2. The feature vector associated with the variable hid contains unigram character mapping, bigram character mapping (by considering surrounding characters). We adopt the one-to-one mapping and noncrossing constraint used in (Chang et al., 2009). We evaluated our system using the EnglishHebrew corpus (Goldwasser and Roth, 2008a), which consists of 250 positive transliteration pairs for training, and 300 pairs for testing. As negative examples for training, we sample 10% from random pairings of words from the positive data. We report two evaluation measurements – (1) the Mean Reciprocal Rank (MRR), which is the average of the multiplicative inverse of the rank of the correct answer, and (2) the accuracy (Acc), which is the percentage of the top rank candidates being correct. We initialized the two stage inference process as detailed in (Chang et al., 2009) using a Romanization table to assign uniform weights to prom</context>
<context position="22744" citStr="Goldwasser and Roth, 2008" startWordPosition="3712" endWordPosition="3715">ach as Alignment+Learning. The results summarized in Table 1 show the significant improvement obtained by the joint approach (95.4% MRR) compared to the two stage approach Transliteration System Acc MRR (Goldwasser and Roth, N/A 89.4 2008b) Alignment + Learning 80.0 85.7 LCLR 92.3 95.4 Table 1: Experimental results for transliteration. We compare a two-stage system: `Alignment+Learning” with LCLR, our joint algorithm. Both `Alignment+Learning” and LCLR use the same features and the same intermediate representation definition. (85.7%). Moreover, LCLR outperforms the joint system introduced in (Goldwasser and Roth, 2008b). 5.2 Textual Entailment Recognizing Textual Entailment (RTE) is an important textual inference task of predicting if a given text snippet, entails the meaning of another (the hypothesis). In many current RTE systems, the entailment decision depends on successfully aligning the constituents of the text and hypothesis, accounting for the internal linguistic structure of the input. The raw input – the text and hypothesis – are represented as directed acyclic graphs, where vertices correspond to words. Directed edges link verbs to the head words of semantic role labeling arguments produced by (</context>
<context position="29937" citStr="Goldwasser and Roth, 2008" startWordPosition="4822" endWordPosition="4825">iev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NL</context>
</contexts>
<marker>Goldwasser, Roth, 2008</marker>
<rawString>D. Goldwasser and D. Roth. 2008b. Transliteration as constrained optimization. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>A Ng</author>
<author>C Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP.</booktitle>
<contexts>
<context position="29887" citStr="Haghighi et al., 2005" startWordPosition="4814" endWordPosition="4817">a and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and </context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>A. Haghighi, A. Ng, and C. Manning. 2005. Robust textual inference via graph matching. In HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-J Hsieh</author>
<author>K-W Chang</author>
<author>C-J Lin</author>
<author>S S Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A dual coordinate descent method for large-scale linear svm.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="15158" citStr="Hsieh et al., 2008" startWordPosition="2444" endWordPosition="2447">fference between Eq. (5) in line 7 of Algorithm 2 and Eq. (4) is that in Eq. (5), we do not search over the entire space of intermediate representations. The search space for the minimization problem Eq. (5) is restricted to the cache Hj. Therefore, instead of solving the minimization problem Eq. (4), we can now solve several simpler problems shown in Eq. (5). The algorithm is guaranteed to stop (line 8) because the space of intermediate representations is finite. Furthermore, in practice, the algorithm needs to consider only a small subset of “hard” examples before it converges. Inspired by (Hsieh et al., 2008), we apply an efficient coordinate descent algorithm for the dual formulation of (5) which is guaranteed to find its global minimum. Due to space considerations, we do not present the derivation of dual formulation and the details of the optimization algorithm. 4 Encoding with ILP: A Paraphrase Identification Example In this section, we define the latent representation for the paraphrase identification task. Unlike the earlier example, where we considered the alignment of lexical items, we describe a more complex intermediate representation by aligning graphs created using semantic resources. </context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. 2008. A dual coordinate descent method for large-scale linear svm. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
<author>T Finley</author>
<author>Chun-Nam Yu</author>
</authors>
<title>Cutting-plane training of structural svms.</title>
<date>2009</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="13996" citStr="Joachims et al., 2009" startWordPosition="2230" endWordPosition="2233">ot the traditional SVM or logistic regression formulation. This is because inside the inner loop, the best representation for each negative example must be found. Therefore, we need to perform inference for every negative example when updating the weight vector solution. Instead of solving a difficult non-convex optimization problem (Eq. (3)), LCLR iteratively solves a series of easier problems (Eq. (4)). This is especially true for our loss function because Eq. (4) is convex and can be solved efficiently. We use a cutting plane algorithm to solve Eq. (4). A similar idea has been proposed in (Joachims et al., 2009). The algorithm for solving Eq. (4) is presented as Algorithm 2. This algorithm uses a “cache” Hj to store all intermediate representations for negative examples that have been seen in previous iterations Algorithm 2 Cutting plane algorithm to optimize Eq. (4) 1: for each negative example xj, Hj +— 0 2: repeat 3: for each negative example xj do P 4: Find h* j � arg maxhEC s hsuT Os(xj) 5: Hj — Hj U {h*j} 6: end for 7: Solve � X 2 Ilul12 + s hsOs(xi)) (5) i:yz=−1 8: until no new element is added to any Hj 9: return u (lines 3-6) 2. The difference between Eq. (5) in line 7 of Algorithm 2 and Eq.</context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>T. Joachims, T. Finley, and Chun-Nam Yu. 2009. Cutting-plane training of structural svms. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="16363" citStr="Klein and Manning, 2003" startWordPosition="2645" endWordPosition="2648">emantic resources. An input example is represented as two acyclic 2In our implementation, we keep a global cache Hj for each negative example xj. Therefore, in Algorithm 2, we start with a non-empty cache improving the speed significantly. � X 2 Ilul12 + E(−uT X s h*i,sOs(xi)) min u i:yz=1 X + E(max hEC uT X s min u E(−uT X h*i,sOs(xi)) i:yz=1 X + E(max hEHj uT X s 432 graphs, G1 and G2, corresponding to the first and second input sentences. Each vertex in the graph contains word information (lemma and partof-speech) and the edges denote dependency relations, generated by the Stanford parser (Klein and Manning, 2003). The intermediate representation for this task can now be defined as an alignment between the graphs, which captures lexical and syntactic correlations between the sentences. We use V (G) and E(G) to denote the set of vertices and edges in G respectively, and define four hidden variable types to encode vertex and edge mappings between G1 and G2. • The word-mapping variables, denoted by hv1,v2, define possible pairings of vertices, where v1 E V (G1) and v2 E V (G2). • The edge-mapping variables, denoted by he1,e2, define possible pairings of the graphs edges, where e1 E E(G1) and e2 E E(G2). •</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Klementiev</author>
<author>D Roth</author>
</authors>
<title>Named entity transliteration and discovery in multilingual corpora.</title>
<date>2008</date>
<booktitle>Learning Machine Translation.</booktitle>
<editor>In Cyril Goutte, Nicola Cancedda, Marc Dymetman, and George Foster, editors,</editor>
<contexts>
<context position="1495" citStr="Klementiev and Roth, 2008" startWordPosition="211" endWordPosition="215">s a novel joint learning algorithm for both tasks, that uses the final prediction to guide the selection of the best intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identification and textual entailment – and show that our joint method significantly improves performance. 1 Introduction Many NLP tasks can be phrased as decision problems over complex linguistic structures. Successful learning depends on correctly encoding these (often latent) structures as features for the learning system. Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1, and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another sni</context>
<context position="18689" citStr="Klementiev and Roth, 2008" startWordPosition="3074" endWordPosition="3077">V (G1) and v2, v2 E V (G2). Then, we have hv1,v2 + hvl,v2 − he1,e2 : 51 (10) hv1,v2 &gt; he1,e2; hvl,v2 &gt; he1,e2 (11) These constraints define the feasible set for the inference problem specified in Equation (1). This inference problem can be formulated as an ILP problem with the objective function from Equation (1): hsuT os(x) subject to (6)-(11); bs; hs E 10, 11 (12) This example demonstrates the use of integer linear programming to define intermediate representations incorporating domain intuition. 5 Experiments We applied our framework to three different NLP tasks: transliteration discovery (Klementiev and Roth, 2008), RTE (Dagan et al., 2006), and paraphrase identification (Dolan et al., 2004). Our experiments are designed to answer the following research question: “Given a binary classification problem defined over latent representations, will the joint LCLR algorithm perform better than a two-stage approach?” To ensure a fair comparison, both systems use the same feature functions and definition of intermediate representation. We use the same ILP formulation in both configurations, with a single exception – the objective function parameters: the two stage approach uses a task-specific heuristic, while L</context>
<context position="29331" citStr="Klementiev and Roth, 2008" startWordPosition="4730" endWordPosition="4733">bining the results of several systems improves performance. 435 (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth</context>
</contexts>
<marker>Klementiev, Roth, 2008</marker>
<rawString>A. Klementiev and D. Roth. 2008. Named entity transliteration and discovery in multilingual corpora. In Cyril Goutte, Nicola Cancedda, Marc Dymetman, and George Foster, editors, Learning Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2161" citStr="MacCartney et al., 2008" startWordPosition="318" endWordPosition="321">an et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1, and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning in</context>
<context position="29221" citStr="MacCartney et al., 2008" startWordPosition="4714" endWordPosition="4717"> between the joint LCLR algorithm and the two stage 4Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. 435 (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to </context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>B. MacCartney, M. Galley, and C. D. Manning. 2008. A phrase-based alignment model for natural language inference. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Bellare</author>
<author>F Pereira</author>
</authors>
<title>A conditional random field for discriminatively-trained finitestate string edit distance.</title>
<date>2005</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="3305" citStr="McCallum et al., 2005" startWordPosition="497" endWordPosition="500">g features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the i</context>
<context position="29910" citStr="McCallum et al., 2005" startWordPosition="4818" endWordPosition="4821">ransliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general plat</context>
</contexts>
<marker>McCallum, Bellare, Pereira, 2005</marker>
<rawString>A. McCallum, K. Bellare, and F. Pereira. 2005. A conditional random field for discriminatively-trained finitestate string edit distance. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics.</title>
<date>2008</date>
<contexts>
<context position="23368" citStr="Punyakanok et al., 2008" startWordPosition="3811" endWordPosition="3814">b). 5.2 Textual Entailment Recognizing Textual Entailment (RTE) is an important textual inference task of predicting if a given text snippet, entails the meaning of another (the hypothesis). In many current RTE systems, the entailment decision depends on successfully aligning the constituents of the text and hypothesis, accounting for the internal linguistic structure of the input. The raw input – the text and hypothesis – are represented as directed acyclic graphs, where vertices correspond to words. Directed edges link verbs to the head words of semantic role labeling arguments produced by (Punyakanok et al., 2008). All other words are connected by dependency edges. The intermediate representation is an alignment between the nodes and edges of the graphs. We used three hidden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the feature</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M-Y Kan</author>
<author>T-S Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2545" citStr="Qiu et al., 2006" startWordPosition="378" endWordPosition="381">e learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show</context>
<context position="26672" citStr="Qiu et al., 2006" startWordPosition="4310" endWordPosition="4313">ce. The word-based scores influence the edge variables via the constraints. This two-stage system (the Alignment+Learning system) is significantly better than the median performance of the RTE-5 submissions. Using LCLR further improves the result by almost 2%, a substantial improvement in this domain. 5.3 Paraphrase Identification Our final task is Paraphrase Identification, discussed in detail at Section 4. We use all the four hidden variable types described in that section. The features used are similar to those described earlier Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.60 Alignment + Learning 76.23 LCLR 76.41 Experiments using Extended data set Alignment + Learning 72.00 LCLR 72.75 Table 4: Experimental Result For Paraphrasing Identification. Our joint LCLR approach achieves the best results compared to several previously published systems, and our own two stage system implementation (Alignment + Learning). We evaluated the systems performance across two datasets: (Dolan et al., 2004) dataset and the Extended dataset, see the text for details. Note that LCLR outperforms (Das and Smith, 2009), which is </context>
<context position="28383" citStr="Qiu et al., 2006" startWordPosition="4585" endWordPosition="4588"> itself. This results in a more difficult inference problem because it allows more mappings between words. Note that the performance on the original dataset sets the ceiling on the second one. The results are summarized in Table 4. The first part of the table compares the LCLR system with a two stage system (Alignment + Learning) and three published results that use the MSR dataset. (We only list single systems in the table4) Interestingly, although still outperformed by our joint LCLR algorithm, the two stage system is able perform significantly better than existing systems for that dataset (Qiu et al., 2006; Das and Smith, 2009; Wan et al., 2006). We attribute this improvement, consistent across both the ILP based systems, to the intermediate representation we defined. We hypothesize that the similarity in performance between the joint LCLR algorithm and the two stage 4Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. 435 (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that th</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>L. Qiu, M.-Y. Kan, and T.-S. Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>C Brockett</author>
<author>W Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6173" citStr="Quirk et al., 2004" startWordPosition="927" endWordPosition="930">nference formulation, which makes it easy to define the intermediate representation and to inject knowledge in the form of constraints. While ILP has been applied to structured output learning, to the best of our knowledge, this is the first work that makes use of ILP in formalizing the general problem of learning intermediate representations. 2 Preliminaries We introduce notation using the Paraphrase Identification task as a running example. This is the binary classification task of identifying whether one sentence is a paraphrase of another. A paraphrase pair from the MSR Paraphrase corpus (Quirk et al., 2004) is shown in Figure 1. In order to identify that the sentences paraphrase each other , we need to align constituents of these sentences. One possible alignment is shown in the figure, in which the dotted edges correspond to the aligned constituents. An alignment can be specified using binary variables corresponding to every edge between constituents, indicating whether the edge is included in the alignment. Different activations of these variables induce the space of intermediate representations. Figure 1: The dotted lines represent a possible intermediate representation for the paraphrase ide</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>C. Quirk, C. Brockett, and W. Dolan. 2004. Monolingual machine translation for paraphrase generation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>M Sammons</author>
<author>V G Vydiswaran</author>
</authors>
<title>A framework for entailed relation recognition.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2181" citStr="Roth et al., 2009" startWordPosition="322" endWordPosition="325">phrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1, and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – spec</context>
<context position="29241" citStr="Roth et al., 2009" startWordPosition="4718" endWordPosition="4721">lgorithm and the two stage 4Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. 435 (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approa</context>
</contexts>
<marker>Roth, Sammons, Vydiswaran, 2009</marker>
<rawString>D. Roth, M. Sammons, and V.G. Vydiswaran. 2009. A framework for entailed relation recognition. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>M Dras</author>
<author>R Dale</author>
<author>C Paris</author>
</authors>
<title>Using dependency-based features to take the ¨para-farce¨out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proc. of the Australasian Language Technology Workshop (ALTW).</booktitle>
<contexts>
<context position="26725" citStr="Wan et al., 2006" startWordPosition="4320" endWordPosition="4323">s via the constraints. This two-stage system (the Alignment+Learning system) is significantly better than the median performance of the RTE-5 submissions. Using LCLR further improves the result by almost 2%, a substantial improvement in this domain. 5.3 Paraphrase Identification Our final task is Paraphrase Identification, discussed in detail at Section 4. We use all the four hidden variable types described in that section. The features used are similar to those described earlier Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.60 Alignment + Learning 76.23 LCLR 76.41 Experiments using Extended data set Alignment + Learning 72.00 LCLR 72.75 Table 4: Experimental Result For Paraphrasing Identification. Our joint LCLR approach achieves the best results compared to several previously published systems, and our own two stage system implementation (Alignment + Learning). We evaluated the systems performance across two datasets: (Dolan et al., 2004) dataset and the Extended dataset, see the text for details. Note that LCLR outperforms (Das and Smith, 2009), which is a specifically designed joint approach for this task.</context>
<context position="28423" citStr="Wan et al., 2006" startWordPosition="4593" endWordPosition="4596">lt inference problem because it allows more mappings between words. Note that the performance on the original dataset sets the ceiling on the second one. The results are summarized in Table 4. The first part of the table compares the LCLR system with a two stage system (Alignment + Learning) and three published results that use the MSR dataset. (We only list single systems in the table4) Interestingly, although still outperformed by our joint LCLR algorithm, the two stage system is able perform significantly better than existing systems for that dataset (Qiu et al., 2006; Das and Smith, 2009; Wan et al., 2006). We attribute this improvement, consistent across both the ILP based systems, to the intermediate representation we defined. We hypothesize that the similarity in performance between the joint LCLR algorithm and the two stage 4Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. 435 (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increas</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using dependency-based features to take the ¨para-farce¨out of paraphrase. In Proc. of the Australasian Language Technology Workshop (ALTW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>T Joachims</author>
</authors>
<title>Learning structural svms with latent variables.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="30318" citStr="Yu and Joachims, 2009" startWordPosition="4881" endWordPosition="4884">strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NLP tasks. The optimization procedure in this work and (Felzenszwalb et al., 2009) are quite different. We use the coordinate descent and cutting-plane methods ensuring we have fewer parameters and the inference procedure can be easily parallelized. Our procedure also allows different loss functions. (Cherry and Quirk, 2008) adopts the Latent SVM algorithm to define a language mod</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>C. Yu and T. Joachims. 2009. Learning structural svms with latent variables. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>A Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="29196" citStr="Zanzotto and Moschitti, 2006" startWordPosition="4710" endWordPosition="4713"> the similarity in performance between the joint LCLR algorithm and the two stage 4Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. 435 (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There a</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>F. M. Zanzotto and A. Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>