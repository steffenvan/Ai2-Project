<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019804">
<title confidence="0.985987">
Word Alignment with Synonym Regularization
</title>
<author confidence="0.867696">
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
</author>
<affiliation confidence="0.632185">
NTT Communication Science Laboratories, NTT Corp.
</affiliation>
<address confidence="0.630201">
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
</address>
<email confidence="0.9581805">
{shindo,a.fujino}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.99731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99974">
We present a novel framework for word
alignment that incorporates synonym
knowledge collected from monolingual
linguistic resources in a bilingual proba-
bilistic model. Synonym information is
helpful for word alignment because we
can expect a synonym to correspond to
the same word in a different language.
We design a generative model for word
alignment that uses synonym information
as a regularization term. The experimental
results show that our proposed method
significantly improves word alignment
quality.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926671875">
Word alignment is an essential step in most phrase
and syntax based statistical machine translation
(SMT). It is an inference problem of word cor-
respondences between different languages given
parallel sentence pairs. Accurate word alignment
can induce high quality phrase detection and trans-
lation probability, which leads to a significant im-
provement in SMT performance. Many word
alignment approaches based on generative mod-
els have been proposed and they learn from bilin-
gual sentences in an unsupervised manner (Vo-
gel et al., 1996; Och and Ney, 2003; Fraser and
Marcu, 2007).
One way to improve word alignment quality
is to add linguistic knowledge derived from a
monolingual corpus. This monolingual knowl-
edge makes it easier to determine corresponding
words correctly. For instance, functional words
in one language tend to correspond to functional
words in another language (Deng and Gao, 2007),
and the syntactic dependency of words in each lan-
guage can help the alignment process (Ma et al.,
2008). It has been shown that such grammatical
information works as a constraint in word align-
ment models and improves word alignment qual-
ity.
A large number of monolingual lexical seman-
tic resources such as WordNet (Miller, 1995) have
been constructed in more than fifty languages
(Sagot and Fiser, 2008). They include word-
level relations such as synonyms, hypernyms and
hyponyms. Synonym information is particularly
helpful for word alignment because we can ex-
pect a synonym to correspond to the same word
in a different language. In this paper, we explore a
method for using synonym information effectively
to improve word alignment quality.
In general, synonym relations are defined in
terms of word sense, not in terms of word form. In
other words, synonym relations are usually con-
text or domain dependent. For instance, ‘head’
and ‘chief’ are synonyms in contexts referring to
working environment, while ‘head’ and ‘forefront’
are synonyms in contexts referring to physical po-
sitions. It is difficult, however, to imagine a con-
text where ‘chief’ and ‘forefront’ are synonyms.
Therefore, it is easy to imagine that simply replac-
ing all occurrences of ‘chief’ and ‘forefront’ with
‘head’ do sometimes harm with word alignment
accuracy, and we have to model either the context
or senses of words.
We propose a novel method that incorporates
synonyms from monolingual resources in a bilin-
gual word alignment model. We formulate a syn-
onym pair generative model with a topic variable
and use this model as a regularization term with a
bilingual word alignment model. The topic vari-
able in our synonym model is helpful for disam-
biguating the meanings of synonyms. We extend
HM-BiTAM, which is a HMM-based word align-
ment model with a latent topic, with a novel syn-
onym pair generative model. We applied the pro-
posed method to an English-French word align-
ment task and successfully improved the word
</bodyText>
<page confidence="0.971411">
137
</page>
<note confidence="0.8431505">
Proceedings of the ACL 2010 Conference Short Papers, pages 137–141,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.997549">
Figure 1: Graphical model of HM-BiTAM
</figureCaption>
<bodyText confidence="0.782556">
alignment quality.
</bodyText>
<sectionHeader confidence="0.992136" genericHeader="method">
2 Bilingual Word Alignment Model
</sectionHeader>
<bodyText confidence="0.999746625">
In this section, we review a conventional gener-
ative word alignment model, HM-BiTAM (Zhao
and Xing, 2008).
HM-BiTAM is a bilingual generative model
with topic z, alignment a and topic weight vec-
tor 0 as latent variables. Topic variables such
as ‘science’ or ‘economy’ assigned to individual
sentences help to disambiguate the meanings of
words. HM-BiTAM assumes that the nth bilin-
gual sentence pair, (En, Fn), is generated under a
given latent topic zn E {1, ... , k, ... , K}, where
K is the number of latent topics. Let N be the
number of sentence pairs, and In and Jn be the
lengths of En and Fn, respectively. In this frame-
work, all of the bilingual sentence pairs {E, F} =
{(En, Fn)}Nn�1 are generated as follows.
</bodyText>
<listItem confidence="0.9754971">
1. θ — Dirichlet (α): sample topic-weight vector
2. For each sentence pair (En, Fn)
(a) zn — Multinomial (θ): sample the topic
(b) en,z;zn |zn — p (En |zn; β ): sample English
words from a monolingual unigram model given
topic zn
(c) For each position jn = 1, ... ,Jn
i. ain — p (ain |ain−1; T ): sample an align-
ment link ain from a first order Markov pro-
cess
</listItem>
<bodyText confidence="0.934521095238095">
ii. fin — p (fin |En, ain, zn; B ): sample a
target word fin given an aligned source
word and topic
where alignment ajn = i denotes source word ei
and target word fjn are aligned. a is a parame-
ter over the topic weight vector 0, Q = {Qk,e} is
the source word probability given the kth topic:
p (e Iz = k). B = {Bf,e,k} represents the word
translation probability from e to f under the kth
topic: p (f Ie, z = k). T = {Ti,i′} is a state tran-
sition probability of a first order Markov process.
Fig. 1 shows a graphical model of HM-BiTAM.
The total likelihood of bilingual sentence pairs
{E, F} can be obtained by marginalizing out la-
tent variables z, a and 0,
� E f p (F, E, z, a, θ; Ψ) dθ, (1)
p (F, E; Ψ) = a
z
where IF = {a, Q, T, B} is a parameter set. In
this model, we can infer word alignment a by max-
imizing the likelihood above.
</bodyText>
<sectionHeader confidence="0.999168" genericHeader="method">
3 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.999735">
3.1 Synonym Pair Generative Model
</subsectionHeader>
<bodyText confidence="0.999626714285714">
We design a generative model for synonym pairs
{f, f′} in language F, which assumes that the
synonyms are collected from monolingual linguis-
tic resources. We assume that each synonym pair
(f, f′) is generated independently given the same
‘sense’ s. Under this assumption, the probability
of synonym pair (f, f′) can be formulated as,
</bodyText>
<equation confidence="0.941143">
p (f, f′) a Ep (f |s) p (f′ |s) p (s) . (2)
3
</equation>
<bodyText confidence="0.999374166666667">
We define a pair (e, k) as a representation of
the sense s, where e and k are a word in a dif-
ferent language E and a latent topic, respectively.
It has been shown that a word e in a different
language is an appropriate representation of s in
synonym modeling (Bannard and Callison-Burch,
2005). We assume that adding a latent topic k for
the sense is very useful for disambiguating word
meaning, and thus that (e, k) gives us a good ap-
proximation of s. Under this assumption, the syn-
onym pair generative model can be defined as fol-
lows.
</bodyText>
<equation confidence="0.989253">
)p ({f, f′� ; Ψ�
�a E p(f|e, k;�Ψ)p(f′|e, k;�Ψ)p(e, k; �Ψ),(3)
U,�′) e,k
</equation>
<bodyText confidence="0.994549">
where IF is the parameter set of our model.
</bodyText>
<subsectionHeader confidence="0.8283595">
3.2 Word Alignment with Synonym
Regularization
</subsectionHeader>
<bodyText confidence="0.999944">
In this section, we extend the bilingual genera-
tive model (HM-BiTAM) with our synonym pair
model. Our expectation is that synonym pairs
</bodyText>
<page confidence="0.994702">
138
</page>
<figureCaption confidence="0.6749715">
Figure 2: Graphical model of synonym pair gen-
erative process
</figureCaption>
<bodyText confidence="0.998389428571429">
correspond to the same word in a different lan-
guage, thus they make it easy to infer accurate
word alignment. HM-BiTAM and the synonym
model share parameters in order to incorporate
monolingual synonym information into the bilin-
gual word alignment model. This can be achieved
via reparameterizing Ψ� in eq. 3 as,
</bodyText>
<equation confidence="0.93533875">
( )
p f ���e, k; �� = p (f |e, k; B ) , (4)
( )
p e, k; �� = p (e |k; β ) p (k; α) . (5)
</equation>
<bodyText confidence="0.9428595">
Overall, we re-define the synonym pair model
with the HM-BiTAM parameter set Ψ,
</bodyText>
<equation confidence="0.99755">
p({f, f′} ; �)
1 /�
OC ri Eαk,3k,eBf,e,kBf′,e,k. (6)
Ek′ αk′ U,f′) k,e
</equation>
<bodyText confidence="0.9994515">
Fig. 2 shows a graphical model of the synonym
pair generative process. We estimate the param-
eter values to maximize the likelihood of HM-
BiTAM with respect to bilingual sentences and
that of the synonym model with respect to syn-
onym pairs collected from monolingual resources.
Namely, the parameter estimate, ˆΨ, is computed
as
</bodyText>
<equation confidence="0.991370666666667">
�� = arg max {log p(F, E;&apos;Y) + ζ log p({f, f′} ; &apos;F)} ,
41
(7)
</equation>
<bodyText confidence="0.999918375">
where C is a regularization weight that should
be set for training. We can expect that the second
term of eq. 7 to constrain parameter set Ψ and
avoid overfitting for the bilingual word alignment
model. We resort to the variational EM approach
(Bernardo et al., 2003) to infer Ψˆ following HM-
BiTAM. We omit the parameter update equation
due to lack of space.
</bodyText>
<sectionHeader confidence="0.999884" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997691">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999896042553192">
For an empirical evaluation of the proposed
method, we used a bilingual parallel corpus of
English-French Hansards (Mihalcea and Pedersen,
2003). The corpus consists of over 1 million sen-
tence pairs, which include 447 manually word-
aligned sentences. We selected 100 sentence pairs
randomly from the manually word-aligned sen-
tences as development data for tuning the regu-
larization weight C, and used the 347 remaining
sentence pairs as evaluation data. We also ran-
domly selected 10k, 50k, and 100k sized sentence
pairs from the corpus as additional training data.
We ran the unsupervised training of our proposed
word alignment model on the additional training
data and the 347 sentence pairs of the evaluation
data. Note that manual word alignment of the
347 sentence pairs was not used for the unsuper-
vised training. After the unsupervised training, we
evaluated the word alignment performance of our
proposed method by comparing the manual word
alignment of the 347 sentence pairs with the pre-
diction provided by the trained model.
We collected English and French synonym pairs
from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4
(Sagot and Fiser, 2008), respectively. WOLF is a
semantic resource constructed from the Princeton
WordNet and various multilingual resources. We
selected synonym pairs where both words were in-
cluded in the bilingual training set.
We compared the word alignment performance
of our model with that of GIZA++ 1.03 1 (Vo-
gel et al., 1996; Och and Ney, 2003), and HM-
BiTAM (Zhao and Xing, 2008) implemented by
us. GIZA++ is an implementation of IBM-model
4 and HMM, and HM-BiTAM corresponds to C =
0 in eq. 7. We adopted K = 3 topics, following
the setting in (Zhao and Xing, 2006).
We trained the word alignment in two direc-
tions: English to French, and French to English.
The alignment results for both directions were re-
fined with ‘GROW’ heuristics to yield high preci-
sion and high recall in accordance with previous
work (Och and Ney, 2003; Zhao and Xing, 2006).
We evaluated these results for precision, recall, F-
measure and alignment error rate (AER), which
are standard metrics for word alignment accuracy
(Och and Ney, 2000).
</bodyText>
<footnote confidence="0.983723">
1http://fjoch.com/GIZA++.html
</footnote>
<page confidence="0.994962">
139
</page>
<table confidence="0.9999685">
10k Precision Recall F-measure AER
GIZA++ standard 0.856 0.718 0.781 0.207
with SRH 0.874 0.720 0.789 0.198
HM-BiTAM standard 0.869 0.788 0.826 0.169
with SRH 0.884 0.790 0.834 0.160
Proposed 0.941 0.808 0.870 0.123
50k Precision Recall F-measure AER
GIZA++ standard 0.905 0.770 0.832 0.156
with SRH 0.903 0.759 0.825 0.164
HM-BiTAM standard 0.901 0.814 0.855 0.140
with SRH 0.899 0.808 0.853 0.145
Proposed 0.947 0.824 0.881 0.112
100k Precision Recall F-measure AER
GIZA++ standard 0.925 0.791 0.853 0.136
with SRH 0.934 0.803 0.864 0.126
HM-BiTAM standard 0.898 0.851 0.874 0.124
with SRH 0.909 0.860 0.879 0.114
Proposed 0.927 0.862 0.893 0.103
</table>
<tableCaption confidence="0.93278275">
Table 1: Comparison of word alignment accuracy.
The best results are indicated in bold type. The
additional data set sizes are (a) 10k, (b) 50k, (c)
100k.
</tableCaption>
<subsectionHeader confidence="0.691915">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999816913043479">
Table 1 shows the word alignment accuracy of the
three methods trained with 10k, 50k, and 100k ad-
ditional sentence pairs. For all settings, our pro-
posed method outperformed other conventional
methods. This result shows that synonym infor-
mation is effective for improving word alignment
quality as we expected.
As mentioned in Sections 1 and 3.1, the main
idea of our proposed method is to introduce la-
tent topics for modeling synonym pairs, and then
to utilize the synonym pair model for the regu-
larization of word alignment models. We expect
the latent topics to be useful for modeling poly-
semous words included in synonym pairs and to
enable us to incorporate synonym information ef-
fectively into word alignment models. To con-
firm the effect of the synonym pair model with
latent topics, we also tested GIZA++ and HM-
BiTAM with what we call Synonym Replacement
Heuristics (SRH), where all of the synonym pairs
in the bilingual training sentences were simply re-
placed with a representative word. For instance,
the words ‘sick’ and ‘ill’ in the bilingual sentences
</bodyText>
<table confidence="0.9935944">
# vocabularies 10k 50k 100k
English standard 8578 16924 22817
with SRH 5435 7235 13978
French standard 10791 21872 30294
with SRH 9737 20077 27970
</table>
<tableCaption confidence="0.9518525">
Table 2: The number of vocabularies in the 10k,
50k and 100k data sets.
</tableCaption>
<bodyText confidence="0.987405186046512">
were replaced with the word ‘sick’. As shown in
Table 2, the number of vocabularies in the English
and French data sets decreased as a result of em-
ploying the SRH.
We show the performance of GIZA++ and HM-
BiTAM with the SRH in the lines entitled “with
SRH” in Table 1. The GIZA++ and HM-BiTAM
with the SRH slightly outperformed the standard
GIZA++ and HM-BiTAM for the 10k and 100k
data sets, but underperformed with the 50k data
set. We assume that the SRH mitigated the over-
fitting of these models into low-frequency word
pairs in bilingual sentences, and then improved the
word alignment performance. The SRH regards
all of the different words coupled with the same
word in the synonym pairs as synonyms. For in-
stance, the words ‘head’, ‘chief’ and ‘forefront’ in
the bilingual sentences are replaced with ‘chief’,
since (‘head’, ‘chief’) and (‘head’, ‘forefront’) are
synonyms. Obviously, (‘chief’, ‘forefront’) are
not synonyms, which is detrimented to word align-
ment.
The proposed method consistently outper-
formed GIZA++ and HM-BiTAM with the SRH
in 10k, 50k and 100k data sets in F-measure.
The synonym pair model in our proposed method
can automatically learn that (‘head’, ‘chief’) and
(‘head’, ‘forefront’) are individual synonyms with
different meanings by assigning these pairs to dif-
ferent topics. By sharing latent topics between
the synonym pair model and the word alignment
model, the synonym information incorporated in
the synonym pair model is used directly for train-
ing word alignment model. The experimental re-
sults show that our proposed method was effec-
tive in improving the performance of the word
alignment model by using synonym pairs includ-
ing such ambiguous synonym words.
Finally, we discuss the data set size used for un-
supervised training. As shown in Table 1, using
a large number of additional sentence pairs im-
proved the performance of all the models. In all
our experimental settings, all the additional sen-
</bodyText>
<page confidence="0.989116">
140
</page>
<bodyText confidence="0.999915928571429">
tence pairs and the evaluation data were selected
from the Hansards data set. These experimental
results show that a larger number of sentence pairs
was more effective in improving word alignment
performance when the sentence pairs were col-
lected from a homogeneous data source. However,
in practice, it might be difficult to collect a large
number of such homogeneous sentence pairs for
a specific target domain and language pair. One
direction for future work is to confirm the effect
of the proposed method when training the word
alignment model by using a large number of sen-
tence pairs collected from various data sources in-
cluding many topics for a specific language pair.
</bodyText>
<sectionHeader confidence="0.999325" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999752">
We proposed a novel framework that incorpo-
rates synonyms from monolingual linguistic re-
sources in a word alignment generative model.
This approach utilizes both bilingual and mono-
lingual synonym resources effectively for word
alignment. Our proposed method uses a latent
topic for bilingual sentences and monolingual syn-
onym pairs, which is helpful in terms of word
sense disambiguation. Our proposed method im-
proved word alignment quality with both small
and large data sets. Future work will involve ex-
amining the proposed method for different lan-
guage pairs such as English-Chinese and English-
Japanese and evaluating the impact of our pro-
posed method on SMT performance. We will also
apply our proposed method to a larger data sets
of multiple domains since we can expect a fur-
ther improvement in word alignment accuracy if
we use more bilingual sentences and more mono-
lingual knowledge.
</bodyText>
<sectionHeader confidence="0.998696" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999066323529411">
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 597–604. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P.
Dawid, D. Heckerman, A. F. M. Smith, and M. West.
2003. The variational bayesian EM algorithm for in-
complete data: with application to scoring graphical
model structures. In Bayesian Statistics 7: Proceed-
ings of the 7th Valencia International Meeting, June
2-6, 2002, page 453. Oxford University Press, USA.
Y. Deng and Y. Gao. 2007. Guiding statistical word
alignment models with prior knowledge. In Pro-
ceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 1–8,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
A. Fraser and D. Marcu. 2007. Getting the struc-
ture right for word alignment: LEAF. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 51–60, Prague, Czech Republic,
June. Association for Computational Linguistics.
Y. Ma, S. Ozdowska, Y. Sun, and A. Way. 2008.
Improving word alignment using syntactic depen-
dencies. In Proceedings of the ACL-08: HLT Sec-
ond Workshop on Syntax and Structure in Statisti-
cal Translation (SSST-2), pages 69–77, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
R. Mihalcea and T. Pedersen. 2003. An evaluation
exercise for word alignment. In Proceedings of the
HLT-NAACL 2003 Workshop on building and using
parallel texts: data driven machine translation and
beyond-Volume 3, page 10. Association for Compu-
tational Linguistics.
G. A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):41.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 440–447. Association for Computational
Linguistics Morristown, NJ, USA.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
B. Sagot and D. Fiser. 2008. Building a free French
wordnet from multilingual resources. In Proceed-
ings of Ontolex.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th Conference on Computa-
tional Linguistics-Volume 2, pages 836–841. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main Conference
Poster Sessions, page 976. Association for Compu-
tational Linguistics.
B. Zhao and E. P. Xing. 2008. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation.
In Advances in Neural Information Processing Sys-
tems 20, pages 1689–1696, Cambridge, MA. MIT
Press.
</reference>
<page confidence="0.998257">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.560435">
<title confidence="0.998598">Word Alignment with Synonym Regularization</title>
<author confidence="0.993563">Akinori Fujino Shindo</author>
<author confidence="0.993563">Nagata</author>
<affiliation confidence="0.997291">NTT Communication Science Laboratories, NTT Corp.</affiliation>
<address confidence="0.961161">2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan</address>
<email confidence="0.776632">shindo@cslab.kecl.ntt.co.jpnagata.masaaki@lab.ntt.co.jp</email>
<email confidence="0.776632">a.fujino@cslab.kecl.ntt.co.jpnagata.masaaki@lab.ntt.co.jp</email>
<abstract confidence="0.9946606">We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>597--604</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6589" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1118" endWordPosition="1121"> pairs {f, f′} in language F, which assumes that the synonyms are collected from monolingual linguistic resources. We assume that each synonym pair (f, f′) is generated independently given the same ‘sense’ s. Under this assumption, the probability of synonym pair (f, f′) can be formulated as, p (f, f′) a Ep (f |s) p (f′ |s) p (s) . (2) 3 We define a pair (e, k) as a representation of the sense s, where e and k are a word in a different language E and a latent topic, respectively. It has been shown that a word e in a different language is an appropriate representation of s in synonym modeling (Bannard and Callison-Burch, 2005). We assume that adding a latent topic k for the sense is very useful for disambiguating word meaning, and thus that (e, k) gives us a good approximation of s. Under this assumption, the synonym pair generative model can be defined as follows. )p ({f, f′� ; Ψ� �a E p(f|e, k;�Ψ)p(f′|e, k;�Ψ)p(e, k; �Ψ),(3) U,�′) e,k where IF is the parameter set of our model. 3.2 Word Alignment with Synonym Regularization In this section, we extend the bilingual generative model (HM-BiTAM) with our synonym pair model. Our expectation is that synonym pairs 138 Figure 2: Graphical model of synonym pair generative</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>C. Bannard and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 597–604. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Bernardo</author>
<author>M J Bayarri</author>
<author>J O Berger</author>
<author>A P Dawid</author>
<author>D Heckerman</author>
<author>A F M Smith</author>
<author>M West</author>
</authors>
<title>The variational bayesian EM algorithm for incomplete data: with application to scoring graphical model structures.</title>
<date>2003</date>
<booktitle>In Bayesian Statistics 7: Proceedings of the 7th Valencia International Meeting,</booktitle>
<pages>453</pages>
<publisher>Oxford University Press, USA.</publisher>
<contexts>
<context position="8408" citStr="Bernardo et al., 2003" startWordPosition="1456" endWordPosition="1459">he synonym pair generative process. We estimate the parameter values to maximize the likelihood of HMBiTAM with respect to bilingual sentences and that of the synonym model with respect to synonym pairs collected from monolingual resources. Namely, the parameter estimate, ˆΨ, is computed as �� = arg max {log p(F, E;&apos;Y) + ζ log p({f, f′} ; &apos;F)} , 41 (7) where C is a regularization weight that should be set for training. We can expect that the second term of eq. 7 to constrain parameter set Ψ and avoid overfitting for the bilingual word alignment model. We resort to the variational EM approach (Bernardo et al., 2003) to infer Ψˆ following HMBiTAM. We omit the parameter update equation due to lack of space. 4 Experiments 4.1 Experimental Setting For an empirical evaluation of the proposed method, we used a bilingual parallel corpus of English-French Hansards (Mihalcea and Pedersen, 2003). The corpus consists of over 1 million sentence pairs, which include 447 manually wordaligned sentences. We selected 100 sentence pairs randomly from the manually word-aligned sentences as development data for tuning the regularization weight C, and used the 347 remaining sentence pairs as evaluation data. We also randomly</context>
</contexts>
<marker>Bernardo, Bayarri, Berger, Dawid, Heckerman, Smith, West, 2003</marker>
<rawString>J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith, and M. West. 2003. The variational bayesian EM algorithm for incomplete data: with application to scoring graphical model structures. In Bayesian Statistics 7: Proceedings of the 7th Valencia International Meeting, June 2-6, 2002, page 453. Oxford University Press, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Deng</author>
<author>Y Gao</author>
</authors>
<title>Guiding statistical word alignment models with prior knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1706" citStr="Deng and Gao, 2007" startWordPosition="241" endWordPosition="244">and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality. A large number of monolingual lexical semantic resources such as WordNet (Miller, 1995) have been constructed in more than fifty languages (Sagot and Fiser, 2008). They include wordlevel relations such as synonyms, hypernyms and hyponyms. Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the sa</context>
</contexts>
<marker>Deng, Gao, 2007</marker>
<rawString>Y. Deng and Y. Gao. 2007. Guiding statistical word alignment models with prior knowledge. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1–8, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Getting the structure right for word alignment: LEAF.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>51--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1383" citStr="Fraser and Marcu, 2007" startWordPosition="192" endWordPosition="195">roves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality. A large number of monolingual lexi</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>A. Fraser and D. Marcu. 2007. Getting the structure right for word alignment: LEAF. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 51–60, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ma</author>
<author>S Ozdowska</author>
<author>Y Sun</author>
<author>A Way</author>
</authors>
<title>Improving word alignment using syntactic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>69--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1811" citStr="Ma et al., 2008" startWordPosition="260" endWordPosition="263">t approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality. A large number of monolingual lexical semantic resources such as WordNet (Miller, 1995) have been constructed in more than fifty languages (Sagot and Fiser, 2008). They include wordlevel relations such as synonyms, hypernyms and hyponyms. Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. In this paper, we explore a method for using synonym information effecti</context>
</contexts>
<marker>Ma, Ozdowska, Sun, Way, 2008</marker>
<rawString>Y. Ma, S. Ozdowska, Y. Sun, and A. Way. 2008. Improving word alignment using syntactic dependencies. In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<contexts>
<context position="8683" citStr="Mihalcea and Pedersen, 2003" startWordPosition="1499" endWordPosition="1502">te, ˆΨ, is computed as �� = arg max {log p(F, E;&apos;Y) + ζ log p({f, f′} ; &apos;F)} , 41 (7) where C is a regularization weight that should be set for training. We can expect that the second term of eq. 7 to constrain parameter set Ψ and avoid overfitting for the bilingual word alignment model. We resort to the variational EM approach (Bernardo et al., 2003) to infer Ψˆ following HMBiTAM. We omit the parameter update equation due to lack of space. 4 Experiments 4.1 Experimental Setting For an empirical evaluation of the proposed method, we used a bilingual parallel corpus of English-French Hansards (Mihalcea and Pedersen, 2003). The corpus consists of over 1 million sentence pairs, which include 447 manually wordaligned sentences. We selected 100 sentence pairs randomly from the manually word-aligned sentences as development data for tuning the regularization weight C, and used the 347 remaining sentence pairs as evaluation data. We also randomly selected 10k, 50k, and 100k sized sentence pairs from the corpus as additional training data. We ran the unsupervised training of our proposed word alignment model on the additional training data and the 347 sentence pairs of the evaluation data. Note that manual word align</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>R. Mihalcea and T. Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of the HLT-NAACL 2003 Workshop on building and using parallel texts: data driven machine translation and beyond-Volume 3, page 10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2036" citStr="Miller, 1995" startWordPosition="299" endWordPosition="300">ity is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality. A large number of monolingual lexical semantic resources such as WordNet (Miller, 1995) have been constructed in more than fifty languages (Sagot and Fiser, 2008). They include wordlevel relations such as synonyms, hypernyms and hyponyms. Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. In this paper, we explore a method for using synonym information effectively to improve word alignment quality. In general, synonym relations are defined in terms of word sense, not in terms of word form. In other words, synonym relations are usually context or domain dependent. For instance, ‘he</context>
<context position="9651" citStr="Miller, 1995" startWordPosition="1658" endWordPosition="1659">ized sentence pairs from the corpus as additional training data. We ran the unsupervised training of our proposed word alignment model on the additional training data and the 347 sentence pairs of the evaluation data. Note that manual word alignment of the 347 sentence pairs was not used for the unsupervised training. After the unsupervised training, we evaluated the word alignment performance of our proposed method by comparing the manual word alignment of the 347 sentence pairs with the prediction provided by the trained model. We collected English and French synonym pairs from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively. WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources. We selected synonym pairs where both words were included in the bilingual training set. We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to C = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We train</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10692" citStr="Och and Ney, 2000" startWordPosition="1835" endWordPosition="1838">GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to C = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000). 1http://fjoch.com/GIZA++.html 139 10k Precision Recall F-measure AER GIZA++ standard 0.856 0.718 0.781 0.207 with SRH 0.874 0.720 0.789 0.198 HM-BiTAM standard 0.869 0.788 0.826 0.169 with SRH 0.884 0.790 0.834 0.160 Proposed 0.941 0.808 0.870 0.123 50k Precision Recall F-measure AER GIZA++ standard 0.905 0.770 0.832 0.156 with SRH 0.903 0.759 0.825 0.164 HM-BiTAM standard 0.901 0.814 0.855 0.140 with SRH 0.899 0.808 0.853 0.145 Proposed 0.947 0.824 0.881 0.112 100k Precision Recall F-measure AER GIZA++ standard 0.925 0.791 0.853 0.136 with SRH 0.934 0.803 0.864 0.126 HM-BiTAM standard 0.898</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 440–447. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1358" citStr="Och and Ney, 2003" startWordPosition="188" endWordPosition="191">d significantly improves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality. A large n</context>
<context position="10019" citStr="Och and Ney, 2003" startWordPosition="1717" endWordPosition="1720">ed the word alignment performance of our proposed method by comparing the manual word alignment of the 347 sentence pairs with the prediction provided by the trained model. We collected English and French synonym pairs from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively. WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources. We selected synonym pairs where both words were included in the bilingual training set. We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to C = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), w</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sagot</author>
<author>D Fiser</author>
</authors>
<title>Building a free French wordnet from multilingual resources.</title>
<date>2008</date>
<booktitle>In Proceedings of Ontolex.</booktitle>
<contexts>
<context position="2111" citStr="Sagot and Fiser, 2008" startWordPosition="309" endWordPosition="312">us. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality. A large number of monolingual lexical semantic resources such as WordNet (Miller, 1995) have been constructed in more than fifty languages (Sagot and Fiser, 2008). They include wordlevel relations such as synonyms, hypernyms and hyponyms. Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. In this paper, we explore a method for using synonym information effectively to improve word alignment quality. In general, synonym relations are defined in terms of word sense, not in terms of word form. In other words, synonym relations are usually context or domain dependent. For instance, ‘head’ and ‘chief’ are synonyms in contexts referring to working environment, </context>
<context position="9690" citStr="Sagot and Fiser, 2008" startWordPosition="1663" endWordPosition="1666">orpus as additional training data. We ran the unsupervised training of our proposed word alignment model on the additional training data and the 347 sentence pairs of the evaluation data. Note that manual word alignment of the 347 sentence pairs was not used for the unsupervised training. After the unsupervised training, we evaluated the word alignment performance of our proposed method by comparing the manual word alignment of the 347 sentence pairs with the prediction provided by the trained model. We collected English and French synonym pairs from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively. WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources. We selected synonym pairs where both words were included in the bilingual training set. We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to C = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions</context>
</contexts>
<marker>Sagot, Fiser, 2008</marker>
<rawString>B. Sagot and D. Fiser. 2008. Building a free French wordnet from multilingual resources. In Proceedings of Ontolex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMMbased word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics-Volume 2,</booktitle>
<pages>836--841</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1339" citStr="Vogel et al., 1996" startWordPosition="183" endWordPosition="187">t our proposed method significantly improves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment</context>
<context position="9999" citStr="Vogel et al., 1996" startWordPosition="1712" endWordPosition="1716">training, we evaluated the word alignment performance of our proposed method by comparing the manual word alignment of the 347 sentence pairs with the prediction provided by the trained model. We collected English and French synonym pairs from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively. WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources. We selected synonym pairs where both words were included in the bilingual training set. We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to C = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMMbased word alignment in statistical translation. In Proceedings of the 16th Conference on Computational Linguistics-Volume 2, pages 836–841. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>E P Xing</author>
</authors>
<title>BiTAM: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main Conference Poster Sessions,</booktitle>
<pages>976</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10241" citStr="Zhao and Xing, 2006" startWordPosition="1760" endWordPosition="1763">rom WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively. WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources. We selected synonym pairs where both words were included in the bilingual training set. We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to C = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000). 1http://fjoch.com/GIZA++.html 139 10k Precision Recall F-measure AER GIZA++ standard 0.856 0.718 0.781 0.207 with SRH 0.874 0.720 0.789 0.198 HM-Bi</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual topic admixture models for word alignment. In Proceedings of the COLING/ACL on Main Conference Poster Sessions, page 976. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>E P Xing</author>
</authors>
<title>HM-BiTAM: Bilingual topic exploration, word alignment, and translation.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20,</booktitle>
<pages>1689--1696</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4054" citStr="Zhao and Xing, 2008" startWordPosition="622" endWordPosition="625">biguating the meanings of synonyms. We extend HM-BiTAM, which is a HMM-based word alignment model with a latent topic, with a novel synonym pair generative model. We applied the proposed method to an English-French word alignment task and successfully improved the word 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Figure 1: Graphical model of HM-BiTAM alignment quality. 2 Bilingual Word Alignment Model In this section, we review a conventional generative word alignment model, HM-BiTAM (Zhao and Xing, 2008). HM-BiTAM is a bilingual generative model with topic z, alignment a and topic weight vector 0 as latent variables. Topic variables such as ‘science’ or ‘economy’ assigned to individual sentences help to disambiguate the meanings of words. HM-BiTAM assumes that the nth bilingual sentence pair, (En, Fn), is generated under a given latent topic zn E {1, ... , k, ... , K}, where K is the number of latent topics. Let N be the number of sentence pairs, and In and Jn be the lengths of En and Fn, respectively. In this framework, all of the bilingual sentence pairs {E, F} = {(En, Fn)}Nn�1 are generate</context>
<context position="10054" citStr="Zhao and Xing, 2008" startWordPosition="1724" endWordPosition="1727"> of our proposed method by comparing the manual word alignment of the 347 sentence pairs with the prediction provided by the trained model. We collected English and French synonym pairs from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively. WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources. We selected synonym pairs where both words were included in the bilingual training set. We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to C = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word </context>
</contexts>
<marker>Zhao, Xing, 2008</marker>
<rawString>B. Zhao and E. P. Xing. 2008. HM-BiTAM: Bilingual topic exploration, word alignment, and translation. In Advances in Neural Information Processing Systems 20, pages 1689–1696, Cambridge, MA. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>