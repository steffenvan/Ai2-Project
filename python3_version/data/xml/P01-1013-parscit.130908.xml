<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000174">
<title confidence="0.841353">
Chinese text segmentation with MBDP-1: Making the most of
training corpora
</title>
<author confidence="0.844369">
Michael R. Brent and Xiaopeng Tao
</author>
<affiliation confidence="0.724599333333333">
Department of Computer Science
Campus Box 1045
Washington University
</affiliation>
<address confidence="0.869206">
St. Louis, MO 63130-4899
</address>
<email confidence="0.993212">
{xptao, brent}@cs.wusthedu
</email>
<sectionHeader confidence="0.995691" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99985335">
This paper describes a system for seg-
menting Chinese text into words us-
ing the MBDP-1 algorithm. MBDP-1
is a knowledge-free segmentation algo-
rithm that bootstraps its own lexicon,
which starts out empty. Experiments
on Chinese and English corpora show
that MBDP-1 reliably outperforms the
best previous algorithm when the avail-
able hand-segmented training corpus is
small. As the size of the hand-segmented
training corpus grows, the performance
of MBDP-1 converges toward that of the
best previous algorithm. The fact that
MBDP-1 can be used with a small cor-
pus is expected to be useful not only for
the rare event of adapting to a new lan-
guage, but also for the common event of
adapting to a new genre within the same
language.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999775738636364">
Many languages, including Chinese and Japanese,
are written without spaces or other delimiters be-
tween the words. A word segmentation algorithm
is therefore required as a front end for any lan-
guage procesing system that relies on a word-
based representation. Most systems for parsing,
indexing, document retrieval, spell checking, and
grammar checking fall into this category. As a re-
sult, the text segmentation problem has received
considerable attention, particularly for Chinese. A
variety of methods have been investigated. One
approach is based on looking up strings in a pre-
existing dictionary and using the longest match
(Cheng et al., 1999). A second approach is based
on bigram frequencies or more general substring
frequencies (Dai et al., 1999; Ponte and Croft,
1996; Teahan et al., 2000). A third approach uses
transformation-based learning (Hockenmaier and
Brew, 1998; Palmer, 1997). A review of earlier
work can be found in (Wu and Tseng, 1993). All
of these methods require either a pre-existing dic-
tionary or else a supervised training regimen using
a manually segmented corpus.
Dictionary-based methods have the well-known
advantages and disadvantages of all knowledge-
intensive approaches. Manually curated linguistic
knowledge tends to be more accurate than what
can be gleaned by an adaptive learning system,
especially when handling relatively rare cases. On
the other hand, it tends to give insufficient weight
to the common cases and suffers from a lack of
adaptability to new languages, genres, and appli-
cations. Since this paper is concerned primarily
with adaptability, we will focus on the best avail-
able methods that do not require a pre-existing
dictionary.
PPM is an adaptive text compression algo-
rithm that has been applied to the segmenta-
tion problem (Teahan et al., 1998; Teahan et al.,
2000). Many text compression algorithms, includ-
ing PPM, work by estimating a probability distri-
bution on the next symbol in a text given the pre-
vious context. Distributions estimated from a text
that includes word-boundary delimiters assign a
probability to word-boundary delimiters in each
context. If an unsegmented text is viewed as hav-
ing hidden word-boundary delimiters, a Viterbi-
style algorithm can be used to find the most prob-
able locations of the hidden delimiters, according
to the estimated probability model. Teahan and
colleagues have done this with the PPM proba-
bility model. The result appears to be the best
available algorithm for segmentation of large cor-
pora of written text, both in English and in Chi-
nese. PPM requires supervised training using a
manually segmented corpus.
The MBDP-1 algorithm (Brent, 1999a) was de-
veloped as a model of how children segment speech
in the course of learning their native languages.
Because speech contains no known acoustic mark-
ing of word boundaries, children must segment
the utterances they hear in order to learn the
words of their language. Further, children start
out without knowing any words and without ac-
cess to a presegmented speech sample of the sort
that would be required for supervised training.
Thus, MBDP-1 — the algorithm underlying an
abstract cognitive model known as INCDROP, for
INCremental Distributional Regularity OPtimiza-
tion (Brent, 1999b; Dahan and Brent, 1999) — re-
quires neither a dictionary nor a segmented train-
ing corpus. It bootsraps its own dictionary, which
is initially empty, using a probability model and
Viterbi-style optimization algorithm. In this pa-
per, we show that MBDP-1 is useful for text seg-
mentation in both English and Chinese.
The remainder of the paper is organized as fol-
lows. The next section describes the probability
model underlying INCDROP and the objective
function that results from the model. Section 3
describes the optimization algorithm that MBDP-
1 uses to find the most probable segmentation,
according to the model. Section 4 reports exper-
iments in which MBDP-1 is compared to PPM
(Teahan et al., 2000) using the PH corpus of Chi-
nese newspaper text and the English portion of
the Hansard corpus. Finally, Section 5 considers
the broader implications of this work.
</bodyText>
<sectionHeader confidence="0.993053" genericHeader="method">
2 Generative Probability Model
</sectionHeader>
<bodyText confidence="0.991677260869565">
This section introduces a language-independent
prior probability distribution on all possible seg-
mented texts. Given an unsegmented text T, this
prior distribution defines a conditional distribu-
tion on all segmented texts that yield T after
word-boundary deletion. The conditional distri-
bution determines the most probable segmenta-
tion of T, according to the model. The prior
distribution is derived from a five-step model for
the generation of texts. The steps are presented
below, along with their probability distributions.
This section describes a mathematical model, not
an algorithm that is intended to be run.
In the following generative model, let E be the
alphabet or character set of the text to be seg-
mented, and let # and $ be reserved symbols that
are not in E. These symbols will be used to repre-
sent word boundaries and sentence boundaries, re-
spectively.&apos; After describing each step of the gen-
erative procedure, we provide a sample result that
could be produced from that step. Combining all
five sample results yields a sample segmented text
that can be generated by this model.
</bodyText>
<footnote confidence="0.875686">
1. Choose the number of distinct word types in
1-While word boundaries are not marked in the text
to be segmented, sentence boundaries are.
</footnote>
<bodyText confidence="0.529777">
the text, n, according to the distribution:
</bodyText>
<equation confidence="0.97729275">
6
= ( 1 ) 2
Pr(n) — (1)
7r n
</equation>
<bodyText confidence="0.98011724">
The inverse-squared distribution on the pos-
itive integers was chosen because it is a sim-
ple, smooth distribution that is relatively fiat,
representing a relatively unbiased prior, yet
its sum converges (unlike the sum of 1/x).
The 6/72 term normalizes the sum to one.
Sample result: n = 6.
2. Let E&apos; = E U {#} be the character set to-
gether with the word-boundary marker, and
let {pi_ ...plE,1} be a probability distribution
on E&apos;. For each i from 1 to n, generate word
type i by choosing characters at random, ac-
cording to the probabilities {pi_ ... /31E, }, un-
til the word boundary character # is cho-
sen. If the word boundary character is cho-
sen first, discard it and choose again until a
non-boundary character is chosen, ensuring
that the word has at least one such charac-
ter. Call the resulting word type Wi, and
let L = {T471 be the resulting lexicon.
For notational convenience, let Wo = $, the
reserved sentence-boundary marker. To com-
pute the probability of a given lexicon, we
estimate probabilities of the characters using
add-one smoothing:2
</bodyText>
<equation confidence="0.9309135">
Cl
Pt =
</equation>
<bodyText confidence="0.9992806">
where c1 is one greater than the frequency
count of letter 1 in the currently hypothesized
segmentation, and S = Ei c1. If L is a set of
word types that are all chosen independently,
then
</bodyText>
<equation confidence="0.98515">
1n ci ci
Pr(L n) = n! (1 c# (§) (2)
s
</equation>
<bodyText confidence="0.951458692307692">
The product at the right of (2) represents
the probability of selecting the sequence of
word types W1 in that particular or-
der, when multiple word boundaries (#&apos;s) are
permitted to occur in sequence. The center
term on the right hand side of (2) results from
imposing the constraint a word cannot begin
with #. The n! term reflects the fact that L
is an unorderd set, so any permutation of the
n words is permissible.
2As described below, we treated each byte of each
Chinese character as a separate character, so the al-
phabet for MBDP-1 has just 28 characters.
</bodyText>
<equation confidence="0.878324">
Sample result:
=do# W2 =the# W3 =kb#
W4 =like# W5 =see# W6 =mbo#
Wo =$
</equation>
<bodyText confidence="0.9893795">
3. For each i from 1 to n, choose f (i), the fre-
quency of word W in the text, according to
the inverse-squared distribution on the posi-
tive integers:
</bodyText>
<equation confidence="0.704078666666667">
6 ( 1 )2 (3)
= k) =
Sample result:
f (1) = 2 f (2) = 4 f (3) = 2 f (4) = 1
f (5) = 2 f (6) = 2 f (0) = 2
4. Let m = f(i) be the total number of
</equation>
<bodyText confidence="0.9926347">
word tokens. Choose an ordering function
s : {1,. . . , m} —&gt; {1, .,n} that maps each
position in the text to be generated to the
index of the word that will appear in that
position. Thus, word type Ws() appears as
the jth word in the text. Note that s is con-
strained to map exactly f (i) positions to word
type W. Choose s according to the uniform
distribution on the distinct orderings (there
are only finitely many, given n, L, and f):
</bodyText>
<equation confidence="0.996432571428571">
f (i)! (4)
Pr(sln,L, .f) = (E .f(i))!
Sample Result:
s(1) = 1 s(2) = 3 s(3) = 5 s(4) = 2
s(5) = 6 s(6) = 0 s(7) = 5 s(8) = 2
s(9) = 2 s(10) = 0 s(11) = 1 s(12) = 3
s(13) = 4 s(14) = 2 s(15) = 6
</equation>
<bodyText confidence="0.9998092">
Define wi to be Ws(), the jth word in the
text. Define wk = wi ...wk, the concatena-
tion of the first k words of the text. Note
that wk is a text in which word boundaries
are marked by #.
</bodyText>
<equation confidence="0.9882455">
Example:
wi = Ws(1) = W1 = (10#
W2 — Ws(2) = W3 = kb#
w3 = Ws(3) = W5 = see#
</equation>
<bodyText confidence="0.997405333333333">
5. Delete the #&apos;s from wm, and output the re-
sult. The output is a text in which the word
boundaries are not marked, like the texts to
be segmented. This is a deterministic process,
so the unique possible outcome has probabil-
ity 1.0.
</bodyText>
<equation confidence="0.6664185">
Sample output:
dokbseethemboSseethetheSdokblikethembo
</equation>
<bodyText confidence="0.99914">
The probability with which steps 1-4 generate a
particular segmented text wm, is simply the prod-
uct of equations (1)-(4). The conditional probabil-
ity that steps 1-4 generated a segmented text wm,
given the unsegmented text T resulting from step
5, is proportional to the marginal probability if T
can be obtained by deleting the word boundaries
from wm; otherwise it is zero.
The probability of wm, resulting from steps 1-4
can be factored by defining R (for relative proba-
bility) as follows:3
</bodyText>
<equation confidence="0.8948226">
R(wk) =
Pr(wk-i)&apos;
where Pr(wo) is defined to be 1. Now
Pr(Wk) = R(Wk)Pr(Wk-1)
(5)
</equation>
<bodyText confidence="0.992539833333333">
Hereafter, we focus on R(wi).
If Wk has occurred previously (wk E
{w1 , • • • , w,_1}) we say wk is a familiar word;
otherwise, we say it is a novel word. Brent (1999a)
showed that if wk is a familiar word that occurs
f(wk) times in wk then
</bodyText>
<equation confidence="0.9997595">
f(wk) (f(wk) - 1)2
R(Wk) = k f (wk) (6)
</equation>
<bodyText confidence="0.999995090909091">
Note that the final occurrence in position k is
included in f(wk), so f(wk) &gt; 2 for a familiar
word and thus (6) is never zero. The first term
on the right hand side of (6) is the relative fre-
quency of the word so far, with one added to both
the numerator and the denominator (since the oc-
currence in position k is included). This term is
similar to the familiar maximum likelihood esti-
mate for a parameter of a multinomial distribu-
tion on words. The second term can be thought
of as an adjustment for the fact that the observed
</bodyText>
<equation confidence="0.785843">
Pr(wk)
</equation>
<bodyText confidence="0.9948195">
3Algebraically, the relative probability can be
treated as the conditional probability of the kth word
given the first k — 1 words, but the semantics is differ-
ent.
</bodyText>
<equation confidence="0.725855666666667">
Example:
= do#kb#
o n l y
</equation>
<bodyText confidence="0.999256888888889">
or familiar, its frequency so far, the total number
of words so far, and the estimated probabilities
of all the letters, are based on the best segmenta-
tions found for all previous sentences in the cor-
pus. Formally, if wi is the sequence of words re-
sulting from the segmentation of all previous sen-
tences in the corpus and cri,k is the string that lies
between nodes j and k in the current sentence,
then
</bodyText>
<equation confidence="0.795569">
RelProb[j, k] = R(w ocri,k),
</equation>
<bodyText confidence="0.999984181818182">
where 0 is the concatenation operator and R is the
function defined by (6) and (7).
Normally, MBDP-1 is initialized to a naive state
in which all words are novel and all letters have
observed frequency 0. However, using MBDP-1
with a manually segmented training corpus could
not be simpler — the training corpus is simply
appended to the test corpus as a prefix, with its
segmentation frozen in advance. Thus, when the
first test sentence is processed, the training corpus
plays the role of wi in the last equation.
</bodyText>
<sectionHeader confidence="0.99749" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.99751076">
Based on published results, MBDP-1 appears to
be the most effective known algorithm for seg-
mentation of phonemically transcribed sponta-
neous speech by mothers to young children (Brent,
1999a; Brent, 1999b). However, it was designed as
a computational model of how children segment
speech in the course of acquiring their native lan-
guage, an application that does not permit the use
of manually segmented training texts. Further,
written text has very different characteristics from
spontaneous child-directed speech, even in an al-
phabetic writing system. Text written in Chinese
characters is even more remote from the corpora
on which MBDP-1 had been tested. Thus, we had
no idea how MBDP-1 would perform on Chinese
text segmentation after being trained on a manu-
ally segmented text. In the following experiment,
we applied MBDP-1 to the segmentation of (1)
the PH Corpus of Chinese newspaper text, and
(2) Part A of the English portion of the Hansard
corpus, which is a sample of the official proceed-
ings of the Canadian parliament. We also tested
PPM, which is apparently the best known algo-
rithm for this problem (Teahan et al., 2000), on
the same corpora.
</bodyText>
<subsectionHeader confidence="0.508811">
4.1 Input
</subsectionHeader>
<bodyText confidence="0.9994003125">
The first corpus we used is Guo Jin&apos;s Mandarin
Chinese PH corpus, containing more than one
million words of newspaper stories from the Xin-
hua news agency of PR China written between
January, 1990 and March, 1991. This manually
segmented corpus is represented in the standard
GB coding scheme, which uses two bytes for each
Chinese character. Following the procedure used
by Teahan et al. (2000), we treated each byte as
a separate input symbol for both MBDP-1 and
PPM. Thus, it is possible for either algorithm to
insert a word boundary between the two bytes of
a Chinese character.
We chose the first one million words of PH as
a training corpus and the following 16,000 words
as a test corpus. The two algorithms were trained
on subsets of the overall training corpus whose
sizes varied from 22 up to 220 words, not counting
punctuation. The test corpus was divided into 16
samples of 1,000 words each so we could assess
the variance in performance across samples of a
given genre. The test corpus was preprocessed to
remove all spaces and create separate &amp;quot;sentences&amp;quot;
at punctuation marks, which are used in Chinese
approximately as in English.
The second corpus we used was Part A of the
English portion of the Hansard corpus, which con-
tains a sample of the proceedings of the Canadian
parliament. We extracted training and testing
samples exactly as for the PH corpus, but since
the Hansard is larger we were able to investigate
training samples of up to 222 words.
</bodyText>
<sectionHeader confidence="0.838284" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.995105652173913">
We evaluated the results by applying the stan-
dard measures precision and recall to pairs of con-
secutive word boundaries (not to individual word
boundaries). To compute these measures, each
character of the automatic segmentation is aligned
with the corresponding character of the standard
segmentation. Each word in the automatic seg-
mentation is labeled a true positive if it lines up
exactly with a word in the standard segmentation–
that is, both boundaries match. Each word in
the automatic segmentation that does not align
exactly with a word in the standard segmenta-
tion is labeled a false positive. Each word in the
standard segmentation that does not align exactly
with a word in the automatic segmentation is la-
beled a false negative. For example, if the test sen-
tence is &amp;quot;ABCAEDF&amp;quot;, the standard segmentation
is &amp;quot;A BC AED F&amp;quot;, and the automatic segmenta-
tion is &amp;quot;A BC A ED F&amp;quot;, then the true positives are
&apos;A&apos;, &apos;BC&apos;, and &apos;F&apos;. The false positives are &apos;A&apos; (the
second one) and &apos;ED&apos;. &apos;AED&apos; is a false negative.
Using these terms, we define precision and recall
as follows:
</bodyText>
<equation confidence="0.696539333333333">
(8)
true positives + false positives
precision =
</equation>
<bodyText confidence="0.979216682926829">
true positives
recall = true positives (9)
true positives + false negatives
Precision is the proportion of the machine-
segmented words that are right. Recall is the
proportion of words in the standard segmentation
words that are identified by the algorithm. These
two measures can diverge, and good performance
is achieved only when both are high.
Figure 2 shows the results on the Chinese PH
corpus, as a function of the logarithm of the train-
ing corpus size. The left panel shows precision and
the right shows recall. Data points for MBDP-
1 are disks, data points for PPM are triangles,
and error bars span two standard errors of the
mean. The results show that MBDP-1 has bet-
ter recall than PPM on the PH corpus until the
training corpus reaches 220 words; at 220 words
the two algorithms are statistically indistinguish-
able.5 MBDP-1&apos;s precision is significantly better
than PPM when the training size is 210 words or
less. After that the two become statistically indis-
tinguishable.
Interestingly, PPM occasionally inserts a word
boundary between the two bytes of a Chinese char-
acter, whereas MBDP-1, although it could seg-
ment between bytes, never does.
Figure 3 shows the results on the English
Hansard corpus, as a function of the logarithm of
the training corpus size. The format is the same
as in Figure 2. The results show that MBDP-
1 has reliably better recall than PPM on the
Hansard corpus, except when the training corpus
size is between 212 words and 216 words, where
the two algorithms are statisticaly indistinguish-
able. MBDP-1 has better precision, too, for both
large and small training corpora. The two algo-
rithms are tied at training sizes of 210 and 216, and
PPM has greater precision between those points.
At very large training corpus sizes both algorithms
perform extremely well.°
</bodyText>
<footnote confidence="0.972335375">
5For the smallest training corpus sizes MBDP-1
will learn as it tries to segment the test corpus, so per-
formance should be better than shown here on larger
test corpora. For the training corpora above 216 words
performance is already so good that this effect is neg-
ligeable.
6PPM has one free parameter, the order of the
model. All experiments in this paper use order 3,
which Teahan et al. (2000) describe as giving the best
results overall. Although Teahan et al. do not discuss
adaptation of the order, we have implmented an adap-
tation scheme and found that it generally chooses or-
der 5 for large English corpora. This improves the re-
sults for large corpora by about two percentage points,
making them statistically indistinguishable from the
results of MBDP-1.
</footnote>
<subsectionHeader confidence="0.985409">
4.3 Discussion of the Experiment
</subsectionHeader>
<bodyText confidence="0.9999778125">
In this discussion we focus on the results in Chi-
nese, since text segmentation has no real appli-
cation in English. Considering precision and re-
call together, it appears that MBDP-1 performs
much better when the available training corpus
is smaller than 212 words, somewhat better when
the training corpus is between 212 words and 218
words, and indistinguishably when the training
corpus is 220 words.
There is no simple, complete explanation for the
fact that MBDP-1 outperforms PPM with small
training corpora. However, it is worth keeping in
mind that MBDP-1 is able to hypothesize the exis-
tence of words even when it has no training corpus,
recognize them when they occur in later utter-
ances, and segment them out. It does this, in part,
by making very good use of sentence boundaries
and other punctuation. Initially, when it has little
or no experience, MBDP-1 tends to treat entire
sentences (or other punctuation-bound phrases)
as single words, storing them in a list of famil-
iar words. Occasionally, phrases do in fact consist
of only one or a few words. Such short phrases are
likely to occur again embedded in longer phrases.
When they do, they tend to be segmented out,
leaving the remaining contiguous segments of the
phrase to be treated as though they were separate
phrases. This leads to the isolation and storage
of more words. For example, the one-word sen-
tence Look! would generally be interpreted as a
single word and stored in the list of familiar words.
If it occurred again in the phrase Lookhere!, it
would tend to be segmented out, leaving here to
be treated as though a separate phrase.
PPM, on the other hand, is not based on hy-
pothesizing words but rather on estimating the
probability of word boundaries in various contexts.
Since word boundaries appear only in segmented
training text, PPM does not learn from unseg-
mented text. The fact that it can learn only from
the training corpus and not from exposure to un-
segmented text may be one reason that it requires
a larger training sample.
The training and test materials for this experi-
ment, while distinct, came from exactly the same
source. The performance of both algorithms can
be expected to deteriorate as the training and test
corpora diverge in genre.
</bodyText>
<sectionHeader confidence="0.998178" genericHeader="conclusions">
5 General Discussion
</sectionHeader>
<bodyText confidence="0.99967775">
The process of adapting a natural language pro-
cessing algorithm to a new language holds great
theoretical interest. In general, algorithms that
can be adapted automatically and cheaply are to
</bodyText>
<figure confidence="0.989049263888889">
100
90
80
70
60
precision
50
40
30
20
10
0
100
90
80
70
60
recall
50
40
30
20
10
0
100
90
80
70
precision
60
50
40
30
20
10
0
100
90
80
70
60
recall
50
40
30
20
10
0
2 4 6 8 10 12 14 16 18 20
22
2 4 6 8 10
12 14 16 18 20 22
PH Precision
MBDP Trained
PPM
2 4 6 8 10 12 14 16 18 20
PH Recall
2 4 6 8 10 12 14 16 18 20
MBDP Trained
PPM
Hansard Recall
Hansard Precision
log2( text size )
log2( text size)
MBDP Trained
PPM
MBDP Trained
PPM
log2( text size )
log2( teXt siZe )
segmented training text.
5.1 Web site for more information
</figure>
<bodyText confidence="0.9996484">
For more information, please see the web site
for the Language Science Research Group at
lsrg.cs.wustl.edu. An online demo of MBDP-1
(using no training text whatsoever) can be found
on the demos page.
</bodyText>
<sectionHeader confidence="0.997281" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999918333333333">
We are very grateful to Bill Teahan, Yingying
Wen, and Ian Witten for sharing their source code
with us and helping us to reproduce their experi-
ments. This work was supported, in part, by grant
number DC03082 from the National Institutes of
Health to MRB.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997889956521739">
Michael R. Brent. 1999a. An efficient, probabilis-
tically sound algorithm for segmentation and
word discovery. Machine Learning, 34:71-106.
Michael R. Brent. 1999b. Speech segmentation
and word discovery: A computational perspec-
tive. Trends in Cognitive Science, 3:294-301.
Kwok-Shing Cheng, Gilbert H. Young, and Kam-
Fai Wong. 1999. A sudy on word-based and
integral-bit chinese text compression algorithm.
Journal of the American Society for Informa-
tion Science, 50:218-228.
Delphine Dahan and Michael R. Brent. 1999.
On the discovery of novel word-like units from
utterances: An artificial-language study with
implications for native-language acquisition.
Journal of Experimental Psychology: General,
128:165-188.
Yubin Dai, Christopher S. G. Khoo, and Teck Ee
Loh. 1999. A new statistical formula for chi-
nese text segmentation incorporating contex-
tual information. In Proceedings of ACM SI-
GIR, pages 82-89.
Julia Hockenmaier and Chris Brew. 1998. Error
driven segmentation of chinese. In Communica-
tions of COMPS, volume 8, pages 69-84.
David Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics.
Jay M. Ponte and W. Bruce Croft. 1996. USeg:
a retargetable word segmentation procedure for
information retrieval. Technical Report TR96-
2, University of Massachusetts, Amherst, MA.
W. J. Teahan, S. Inglis, John G. Cleary, and
G. Holmes. 1998. Correcting english text using
ppm models. In J. A. Storer and J. H. Reif, ed-
itors, Proceedings of Data Compression Confer-
ence, pages 289-298, Los Alamitos, CA. IEEE
Computer Society Press.
W. J. Teahan, Yingying Wen, Rodger McNab, and
Ian H. Witten. 2000. A compression-based al-
gorithm for chinese word segmentation. Com-
putational Linguistics, 26:375-393.
Zimin Wu and Gwyneth Tseng. 1993. Chinese
text segmentation for text retrieval: Achieve-
ments and problems. JASIS, 44:532-542.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.484128">
<note confidence="0.584673">Chinese text segmentation with MBDP-1: Making the most of</note>
<title confidence="0.943797">training corpora</title>
<author confidence="0.999965">Michael R Brent</author>
<author confidence="0.999965">Xiaopeng Tao</author>
<affiliation confidence="0.999914">Department of Computer Science</affiliation>
<address confidence="0.999214">Campus Box 1045</address>
<affiliation confidence="0.998161">Washington University</affiliation>
<address confidence="0.999834">St. Louis, MO 63130-4899</address>
<email confidence="0.999612">xptao@cs.wusthedu</email>
<email confidence="0.999612">brent@cs.wusthedu</email>
<abstract confidence="0.995731571428571">This paper describes a system for segmenting Chinese text into words using the MBDP-1 algorithm. MBDP-1 is a knowledge-free segmentation algorithm that bootstraps its own lexicon, which starts out empty. Experiments on Chinese and English corpora show that MBDP-1 reliably outperforms the best previous algorithm when the available hand-segmented training corpus is small. As the size of the hand-segmented training corpus grows, the performance of MBDP-1 converges toward that of the best previous algorithm. The fact that MBDP-1 can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language, but also for the common event of adapting to a new genre within the same language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="3630" citStr="Brent, 1999" startWordPosition="575" endWordPosition="576">ndary delimiters assign a probability to word-boundary delimiters in each context. If an unsegmented text is viewed as having hidden word-boundary delimiters, a Viterbistyle algorithm can be used to find the most probable locations of the hidden delimiters, according to the estimated probability model. Teahan and colleagues have done this with the PPM probability model. The result appears to be the best available algorithm for segmentation of large corpora of written text, both in English and in Chinese. PPM requires supervised training using a manually segmented corpus. The MBDP-1 algorithm (Brent, 1999a) was developed as a model of how children segment speech in the course of learning their native languages. Because speech contains no known acoustic marking of word boundaries, children must segment the utterances they hear in order to learn the words of their language. Further, children start out without knowing any words and without access to a presegmented speech sample of the sort that would be required for supervised training. Thus, MBDP-1 — the algorithm underlying an abstract cognitive model known as INCDROP, for INCremental Distributional Regularity OPtimization (Brent, 1999b; Dahan </context>
<context position="10565" citStr="Brent (1999" startWordPosition="1853" endWordPosition="1854">tional probability that steps 1-4 generated a segmented text wm, given the unsegmented text T resulting from step 5, is proportional to the marginal probability if T can be obtained by deleting the word boundaries from wm; otherwise it is zero. The probability of wm, resulting from steps 1-4 can be factored by defining R (for relative probability) as follows:3 R(wk) = Pr(wk-i)&apos; where Pr(wo) is defined to be 1. Now Pr(Wk) = R(Wk)Pr(Wk-1) (5) Hereafter, we focus on R(wi). If Wk has occurred previously (wk E {w1 , • • • , w,_1}) we say wk is a familiar word; otherwise, we say it is a novel word. Brent (1999a) showed that if wk is a familiar word that occurs f(wk) times in wk then f(wk) (f(wk) - 1)2 R(Wk) = k f (wk) (6) Note that the final occurrence in position k is included in f(wk), so f(wk) &gt; 2 for a familiar word and thus (6) is never zero. The first term on the right hand side of (6) is the relative frequency of the word so far, with one added to both the numerator and the denominator (since the occurrence in position k is included). This term is similar to the familiar maximum likelihood estimate for a parameter of a multinomial distribution on words. The second term can be thought of as a</context>
<context position="12557" citStr="Brent, 1999" startWordPosition="2217" endWordPosition="2218">tialized to a naive state in which all words are novel and all letters have observed frequency 0. However, using MBDP-1 with a manually segmented training corpus could not be simpler — the training corpus is simply appended to the test corpus as a prefix, with its segmentation frozen in advance. Thus, when the first test sentence is processed, the training corpus plays the role of wi in the last equation. 4 Experiment Based on published results, MBDP-1 appears to be the most effective known algorithm for segmentation of phonemically transcribed spontaneous speech by mothers to young children (Brent, 1999a; Brent, 1999b). However, it was designed as a computational model of how children segment speech in the course of acquiring their native language, an application that does not permit the use of manually segmented training texts. Further, written text has very different characteristics from spontaneous child-directed speech, even in an alphabetic writing system. Text written in Chinese characters is even more remote from the corpora on which MBDP-1 had been tested. Thus, we had no idea how MBDP-1 would perform on Chinese text segmentation after being trained on a manually segmented text. In t</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael R. Brent. 1999a. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>Speech segmentation and word discovery: A computational perspective.</title>
<date>1999</date>
<booktitle>Trends in Cognitive Science,</booktitle>
<pages>3--294</pages>
<contexts>
<context position="3630" citStr="Brent, 1999" startWordPosition="575" endWordPosition="576">ndary delimiters assign a probability to word-boundary delimiters in each context. If an unsegmented text is viewed as having hidden word-boundary delimiters, a Viterbistyle algorithm can be used to find the most probable locations of the hidden delimiters, according to the estimated probability model. Teahan and colleagues have done this with the PPM probability model. The result appears to be the best available algorithm for segmentation of large corpora of written text, both in English and in Chinese. PPM requires supervised training using a manually segmented corpus. The MBDP-1 algorithm (Brent, 1999a) was developed as a model of how children segment speech in the course of learning their native languages. Because speech contains no known acoustic marking of word boundaries, children must segment the utterances they hear in order to learn the words of their language. Further, children start out without knowing any words and without access to a presegmented speech sample of the sort that would be required for supervised training. Thus, MBDP-1 — the algorithm underlying an abstract cognitive model known as INCDROP, for INCremental Distributional Regularity OPtimization (Brent, 1999b; Dahan </context>
<context position="10565" citStr="Brent (1999" startWordPosition="1853" endWordPosition="1854">tional probability that steps 1-4 generated a segmented text wm, given the unsegmented text T resulting from step 5, is proportional to the marginal probability if T can be obtained by deleting the word boundaries from wm; otherwise it is zero. The probability of wm, resulting from steps 1-4 can be factored by defining R (for relative probability) as follows:3 R(wk) = Pr(wk-i)&apos; where Pr(wo) is defined to be 1. Now Pr(Wk) = R(Wk)Pr(Wk-1) (5) Hereafter, we focus on R(wi). If Wk has occurred previously (wk E {w1 , • • • , w,_1}) we say wk is a familiar word; otherwise, we say it is a novel word. Brent (1999a) showed that if wk is a familiar word that occurs f(wk) times in wk then f(wk) (f(wk) - 1)2 R(Wk) = k f (wk) (6) Note that the final occurrence in position k is included in f(wk), so f(wk) &gt; 2 for a familiar word and thus (6) is never zero. The first term on the right hand side of (6) is the relative frequency of the word so far, with one added to both the numerator and the denominator (since the occurrence in position k is included). This term is similar to the familiar maximum likelihood estimate for a parameter of a multinomial distribution on words. The second term can be thought of as a</context>
<context position="12557" citStr="Brent, 1999" startWordPosition="2217" endWordPosition="2218">tialized to a naive state in which all words are novel and all letters have observed frequency 0. However, using MBDP-1 with a manually segmented training corpus could not be simpler — the training corpus is simply appended to the test corpus as a prefix, with its segmentation frozen in advance. Thus, when the first test sentence is processed, the training corpus plays the role of wi in the last equation. 4 Experiment Based on published results, MBDP-1 appears to be the most effective known algorithm for segmentation of phonemically transcribed spontaneous speech by mothers to young children (Brent, 1999a; Brent, 1999b). However, it was designed as a computational model of how children segment speech in the course of acquiring their native language, an application that does not permit the use of manually segmented training texts. Further, written text has very different characteristics from spontaneous child-directed speech, even in an alphabetic writing system. Text written in Chinese characters is even more remote from the corpora on which MBDP-1 had been tested. Thus, we had no idea how MBDP-1 would perform on Chinese text segmentation after being trained on a manually segmented text. In t</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael R. Brent. 1999b. Speech segmentation and word discovery: A computational perspective. Trends in Cognitive Science, 3:294-301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kwok-Shing Cheng</author>
<author>Gilbert H Young</author>
<author>KamFai Wong</author>
</authors>
<title>A sudy on word-based and integral-bit chinese text compression algorithm.</title>
<date>1999</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>50--218</pages>
<contexts>
<context position="1631" citStr="Cheng et al., 1999" startWordPosition="256" endWordPosition="259">inese and Japanese, are written without spaces or other delimiters between the words. A word segmentation algorithm is therefore required as a front end for any language procesing system that relies on a wordbased representation. Most systems for parsing, indexing, document retrieval, spell checking, and grammar checking fall into this category. As a result, the text segmentation problem has received considerable attention, particularly for Chinese. A variety of methods have been investigated. One approach is based on looking up strings in a preexisting dictionary and using the longest match (Cheng et al., 1999). A second approach is based on bigram frequencies or more general substring frequencies (Dai et al., 1999; Ponte and Croft, 1996; Teahan et al., 2000). A third approach uses transformation-based learning (Hockenmaier and Brew, 1998; Palmer, 1997). A review of earlier work can be found in (Wu and Tseng, 1993). All of these methods require either a pre-existing dictionary or else a supervised training regimen using a manually segmented corpus. Dictionary-based methods have the well-known advantages and disadvantages of all knowledgeintensive approaches. Manually curated linguistic knowledge ten</context>
</contexts>
<marker>Cheng, Young, Wong, 1999</marker>
<rawString>Kwok-Shing Cheng, Gilbert H. Young, and KamFai Wong. 1999. A sudy on word-based and integral-bit chinese text compression algorithm. Journal of the American Society for Information Science, 50:218-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delphine Dahan</author>
<author>Michael R Brent</author>
</authors>
<title>On the discovery of novel word-like units from utterances: An artificial-language study with implications for native-language acquisition.</title>
<date>1999</date>
<journal>Journal of Experimental Psychology: General,</journal>
<pages>128--165</pages>
<contexts>
<context position="4246" citStr="Dahan and Brent, 1999" startWordPosition="671" endWordPosition="674">, 1999a) was developed as a model of how children segment speech in the course of learning their native languages. Because speech contains no known acoustic marking of word boundaries, children must segment the utterances they hear in order to learn the words of their language. Further, children start out without knowing any words and without access to a presegmented speech sample of the sort that would be required for supervised training. Thus, MBDP-1 — the algorithm underlying an abstract cognitive model known as INCDROP, for INCremental Distributional Regularity OPtimization (Brent, 1999b; Dahan and Brent, 1999) — requires neither a dictionary nor a segmented training corpus. It bootsraps its own dictionary, which is initially empty, using a probability model and Viterbi-style optimization algorithm. In this paper, we show that MBDP-1 is useful for text segmentation in both English and Chinese. The remainder of the paper is organized as follows. The next section describes the probability model underlying INCDROP and the objective function that results from the model. Section 3 describes the optimization algorithm that MBDP1 uses to find the most probable segmentation, according to the model. Section </context>
</contexts>
<marker>Dahan, Brent, 1999</marker>
<rawString>Delphine Dahan and Michael R. Brent. 1999. On the discovery of novel word-like units from utterances: An artificial-language study with implications for native-language acquisition. Journal of Experimental Psychology: General, 128:165-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yubin Dai</author>
<author>Christopher S G Khoo</author>
<author>Teck Ee Loh</author>
</authors>
<title>A new statistical formula for chinese text segmentation incorporating contextual information.</title>
<date>1999</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>82--89</pages>
<contexts>
<context position="1737" citStr="Dai et al., 1999" startWordPosition="273" endWordPosition="276">orithm is therefore required as a front end for any language procesing system that relies on a wordbased representation. Most systems for parsing, indexing, document retrieval, spell checking, and grammar checking fall into this category. As a result, the text segmentation problem has received considerable attention, particularly for Chinese. A variety of methods have been investigated. One approach is based on looking up strings in a preexisting dictionary and using the longest match (Cheng et al., 1999). A second approach is based on bigram frequencies or more general substring frequencies (Dai et al., 1999; Ponte and Croft, 1996; Teahan et al., 2000). A third approach uses transformation-based learning (Hockenmaier and Brew, 1998; Palmer, 1997). A review of earlier work can be found in (Wu and Tseng, 1993). All of these methods require either a pre-existing dictionary or else a supervised training regimen using a manually segmented corpus. Dictionary-based methods have the well-known advantages and disadvantages of all knowledgeintensive approaches. Manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system, especially when handling r</context>
</contexts>
<marker>Dai, Khoo, Loh, 1999</marker>
<rawString>Yubin Dai, Christopher S. G. Khoo, and Teck Ee Loh. 1999. A new statistical formula for chinese text segmentation incorporating contextual information. In Proceedings of ACM SIGIR, pages 82-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Chris Brew</author>
</authors>
<title>Error driven segmentation of chinese.</title>
<date>1998</date>
<journal>In Communications of COMPS,</journal>
<volume>8</volume>
<pages>69--84</pages>
<contexts>
<context position="1863" citStr="Hockenmaier and Brew, 1998" startWordPosition="291" endWordPosition="294">ion. Most systems for parsing, indexing, document retrieval, spell checking, and grammar checking fall into this category. As a result, the text segmentation problem has received considerable attention, particularly for Chinese. A variety of methods have been investigated. One approach is based on looking up strings in a preexisting dictionary and using the longest match (Cheng et al., 1999). A second approach is based on bigram frequencies or more general substring frequencies (Dai et al., 1999; Ponte and Croft, 1996; Teahan et al., 2000). A third approach uses transformation-based learning (Hockenmaier and Brew, 1998; Palmer, 1997). A review of earlier work can be found in (Wu and Tseng, 1993). All of these methods require either a pre-existing dictionary or else a supervised training regimen using a manually segmented corpus. Dictionary-based methods have the well-known advantages and disadvantages of all knowledgeintensive approaches. Manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system, especially when handling relatively rare cases. On the other hand, it tends to give insufficient weight to the common cases and suffers from a lack of a</context>
</contexts>
<marker>Hockenmaier, Brew, 1998</marker>
<rawString>Julia Hockenmaier and Chris Brew. 1998. Error driven segmentation of chinese. In Communications of COMPS, volume 8, pages 69-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1878" citStr="Palmer, 1997" startWordPosition="295" endWordPosition="296">g, indexing, document retrieval, spell checking, and grammar checking fall into this category. As a result, the text segmentation problem has received considerable attention, particularly for Chinese. A variety of methods have been investigated. One approach is based on looking up strings in a preexisting dictionary and using the longest match (Cheng et al., 1999). A second approach is based on bigram frequencies or more general substring frequencies (Dai et al., 1999; Ponte and Croft, 1996; Teahan et al., 2000). A third approach uses transformation-based learning (Hockenmaier and Brew, 1998; Palmer, 1997). A review of earlier work can be found in (Wu and Tseng, 1993). All of these methods require either a pre-existing dictionary or else a supervised training regimen using a manually segmented corpus. Dictionary-based methods have the well-known advantages and disadvantages of all knowledgeintensive approaches. Manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system, especially when handling relatively rare cases. On the other hand, it tends to give insufficient weight to the common cases and suffers from a lack of adaptability to </context>
</contexts>
<marker>Palmer, 1997</marker>
<rawString>David Palmer. 1997. A trainable rule-based algorithm for word segmentation. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>USeg: a retargetable word segmentation procedure for information retrieval.</title>
<date>1996</date>
<tech>Technical Report TR96-2,</tech>
<institution>University of Massachusetts,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="1760" citStr="Ponte and Croft, 1996" startWordPosition="277" endWordPosition="280">e required as a front end for any language procesing system that relies on a wordbased representation. Most systems for parsing, indexing, document retrieval, spell checking, and grammar checking fall into this category. As a result, the text segmentation problem has received considerable attention, particularly for Chinese. A variety of methods have been investigated. One approach is based on looking up strings in a preexisting dictionary and using the longest match (Cheng et al., 1999). A second approach is based on bigram frequencies or more general substring frequencies (Dai et al., 1999; Ponte and Croft, 1996; Teahan et al., 2000). A third approach uses transformation-based learning (Hockenmaier and Brew, 1998; Palmer, 1997). A review of earlier work can be found in (Wu and Tseng, 1993). All of these methods require either a pre-existing dictionary or else a supervised training regimen using a manually segmented corpus. Dictionary-based methods have the well-known advantages and disadvantages of all knowledgeintensive approaches. Manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system, especially when handling relatively rare cases. O</context>
</contexts>
<marker>Ponte, Croft, 1996</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1996. USeg: a retargetable word segmentation procedure for information retrieval. Technical Report TR96-2, University of Massachusetts, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>S Inglis</author>
<author>John G Cleary</author>
<author>G Holmes</author>
</authors>
<title>Correcting english text using ppm models.</title>
<date>1998</date>
<booktitle>Proceedings of Data Compression Conference,</booktitle>
<pages>289--298</pages>
<editor>In J. A. Storer and J. H. Reif, editors,</editor>
<publisher>IEEE Computer Society Press.</publisher>
<location>Los Alamitos, CA.</location>
<contexts>
<context position="2785" citStr="Teahan et al., 1998" startWordPosition="438" endWordPosition="441">geintensive approaches. Manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system, especially when handling relatively rare cases. On the other hand, it tends to give insufficient weight to the common cases and suffers from a lack of adaptability to new languages, genres, and applications. Since this paper is concerned primarily with adaptability, we will focus on the best available methods that do not require a pre-existing dictionary. PPM is an adaptive text compression algorithm that has been applied to the segmentation problem (Teahan et al., 1998; Teahan et al., 2000). Many text compression algorithms, including PPM, work by estimating a probability distribution on the next symbol in a text given the previous context. Distributions estimated from a text that includes word-boundary delimiters assign a probability to word-boundary delimiters in each context. If an unsegmented text is viewed as having hidden word-boundary delimiters, a Viterbistyle algorithm can be used to find the most probable locations of the hidden delimiters, according to the estimated probability model. Teahan and colleagues have done this with the PPM probability </context>
</contexts>
<marker>Teahan, Inglis, Cleary, Holmes, 1998</marker>
<rawString>W. J. Teahan, S. Inglis, John G. Cleary, and G. Holmes. 1998. Correcting english text using ppm models. In J. A. Storer and J. H. Reif, editors, Proceedings of Data Compression Conference, pages 289-298, Los Alamitos, CA. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>Yingying Wen</author>
<author>Rodger McNab</author>
<author>Ian H Witten</author>
</authors>
<title>A compression-based algorithm for chinese word segmentation.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--375</pages>
<contexts>
<context position="1782" citStr="Teahan et al., 2000" startWordPosition="281" endWordPosition="284">nd for any language procesing system that relies on a wordbased representation. Most systems for parsing, indexing, document retrieval, spell checking, and grammar checking fall into this category. As a result, the text segmentation problem has received considerable attention, particularly for Chinese. A variety of methods have been investigated. One approach is based on looking up strings in a preexisting dictionary and using the longest match (Cheng et al., 1999). A second approach is based on bigram frequencies or more general substring frequencies (Dai et al., 1999; Ponte and Croft, 1996; Teahan et al., 2000). A third approach uses transformation-based learning (Hockenmaier and Brew, 1998; Palmer, 1997). A review of earlier work can be found in (Wu and Tseng, 1993). All of these methods require either a pre-existing dictionary or else a supervised training regimen using a manually segmented corpus. Dictionary-based methods have the well-known advantages and disadvantages of all knowledgeintensive approaches. Manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system, especially when handling relatively rare cases. On the other hand, it t</context>
<context position="4924" citStr="Teahan et al., 2000" startWordPosition="783" endWordPosition="786">corpus. It bootsraps its own dictionary, which is initially empty, using a probability model and Viterbi-style optimization algorithm. In this paper, we show that MBDP-1 is useful for text segmentation in both English and Chinese. The remainder of the paper is organized as follows. The next section describes the probability model underlying INCDROP and the objective function that results from the model. Section 3 describes the optimization algorithm that MBDP1 uses to find the most probable segmentation, according to the model. Section 4 reports experiments in which MBDP-1 is compared to PPM (Teahan et al., 2000) using the PH corpus of Chinese newspaper text and the English portion of the Hansard corpus. Finally, Section 5 considers the broader implications of this work. 2 Generative Probability Model This section introduces a language-independent prior probability distribution on all possible segmented texts. Given an unsegmented text T, this prior distribution defines a conditional distribution on all segmented texts that yield T after word-boundary deletion. The conditional distribution determines the most probable segmentation of T, according to the model. The prior distribution is derived from a </context>
<context position="13506" citStr="Teahan et al., 2000" startWordPosition="2373" endWordPosition="2376">ven in an alphabetic writing system. Text written in Chinese characters is even more remote from the corpora on which MBDP-1 had been tested. Thus, we had no idea how MBDP-1 would perform on Chinese text segmentation after being trained on a manually segmented text. In the following experiment, we applied MBDP-1 to the segmentation of (1) the PH Corpus of Chinese newspaper text, and (2) Part A of the English portion of the Hansard corpus, which is a sample of the official proceedings of the Canadian parliament. We also tested PPM, which is apparently the best known algorithm for this problem (Teahan et al., 2000), on the same corpora. 4.1 Input The first corpus we used is Guo Jin&apos;s Mandarin Chinese PH corpus, containing more than one million words of newspaper stories from the Xinhua news agency of PR China written between January, 1990 and March, 1991. This manually segmented corpus is represented in the standard GB coding scheme, which uses two bytes for each Chinese character. Following the procedure used by Teahan et al. (2000), we treated each byte as a separate input symbol for both MBDP-1 and PPM. Thus, it is possible for either algorithm to insert a word boundary between the two bytes of a Chi</context>
<context position="18343" citStr="Teahan et al. (2000)" startWordPosition="3204" endWordPosition="3207">rge and small training corpora. The two algorithms are tied at training sizes of 210 and 216, and PPM has greater precision between those points. At very large training corpus sizes both algorithms perform extremely well.° 5For the smallest training corpus sizes MBDP-1 will learn as it tries to segment the test corpus, so performance should be better than shown here on larger test corpora. For the training corpora above 216 words performance is already so good that this effect is negligeable. 6PPM has one free parameter, the order of the model. All experiments in this paper use order 3, which Teahan et al. (2000) describe as giving the best results overall. Although Teahan et al. do not discuss adaptation of the order, we have implmented an adaptation scheme and found that it generally chooses order 5 for large English corpora. This improves the results for large corpora by about two percentage points, making them statistically indistinguishable from the results of MBDP-1. 4.3 Discussion of the Experiment In this discussion we focus on the results in Chinese, since text segmentation has no real application in English. Considering precision and recall together, it appears that MBDP-1 performs much bett</context>
</contexts>
<marker>Teahan, Wen, McNab, Witten, 2000</marker>
<rawString>W. J. Teahan, Yingying Wen, Rodger McNab, and Ian H. Witten. 2000. A compression-based algorithm for chinese word segmentation. Computational Linguistics, 26:375-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>JASIS,</journal>
<pages>44--532</pages>
<contexts>
<context position="1941" citStr="Wu and Tseng, 1993" startWordPosition="306" endWordPosition="309">mar checking fall into this category. As a result, the text segmentation problem has received considerable attention, particularly for Chinese. A variety of methods have been investigated. One approach is based on looking up strings in a preexisting dictionary and using the longest match (Cheng et al., 1999). A second approach is based on bigram frequencies or more general substring frequencies (Dai et al., 1999; Ponte and Croft, 1996; Teahan et al., 2000). A third approach uses transformation-based learning (Hockenmaier and Brew, 1998; Palmer, 1997). A review of earlier work can be found in (Wu and Tseng, 1993). All of these methods require either a pre-existing dictionary or else a supervised training regimen using a manually segmented corpus. Dictionary-based methods have the well-known advantages and disadvantages of all knowledgeintensive approaches. Manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system, especially when handling relatively rare cases. On the other hand, it tends to give insufficient weight to the common cases and suffers from a lack of adaptability to new languages, genres, and applications. Since this paper is co</context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. JASIS, 44:532-542.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>