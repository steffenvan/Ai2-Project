<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000211">
<title confidence="0.931475">
A Simple Measure to Assess Non-response
</title>
<note confidence="0.503269">
Anselmo Pe˜nas and Alvaro Rodrigo
UNED NLP &amp; IR Group
Juan del Rosal, 16
</note>
<address confidence="0.893468">
28040 Madrid, Spain
</address>
<email confidence="0.999186">
{anselmo,alvarory@lsi.uned.es}
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999491333333333">
There are several tasks where is preferable not
responding than responding incorrectly. This
idea is not new, but despite several previous at-
tempts there isn’t a commonly accepted mea-
sure to assess non-response. We study here an
extension of accuracy measure with this fea-
ture and a very easy to understand interpreta-
tion. The measure proposed (c@1) has a good
balance of discrimination power, stability and
sensitivity properties. We show also how this
measure is able to reward systems that main-
tain the same number of correct answers and
at the same time decrease the number of in-
correct ones, by leaving some questions unan-
swered. This measure is well suited for tasks
such as Reading Comprehension tests, where
multiple choices per question are given, but
only one is correct.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931685714285">
There is some tendency to consider that an incorrect
result is simply the absence of a correct one. This is
particularly true in the evaluation of Information Re-
trieval systems where, in fact, the absence of results
sometimes is the worse output.
However, there are scenarios where we should
consider the possibility of not responding, because
this behavior has more value than responding incor-
rectly. For example, during the process of introduc-
ing new features in a search engine it is important
to preserve users’ confidence in the system. Thus,
a system must decide whether it should give or not
a result in the new fashion or keep on with the old
kind of output. A similar example is the decision
about showing or not ads related to the query. Show-
ing wrong ads harms the business model more than
showing nothing. A third example more related to
Natural Language Processing is the Machine Read-
ing evaluation through reading comprehension tests.
In this case, where multiple choices for a question
are offered, choosing a wrong option should be pun-
ished against leaving the question unanswered.
In the latter case, the use of utility functions is
a very common option. However, utility functions
give arbitrary value to not responding and ignore
the system’s behavior showed when it responds (see
Section 2). To avoid this, we present c@1 measure
(Section 2.2), as an extension of accuracy (the pro-
portion of correctly answered questions). In Sec-
tion 3 we show that no other extension produces a
sensible measure. In Section 4 we evaluate c@1 in
terms of stability, discrimination power and sensibil-
ity, and some real examples of its behavior are given
in the context of Question Answering. Related work
is discussed in Section 5.
</bodyText>
<sectionHeader confidence="0.958364" genericHeader="method">
2 Looking for the Value of Not Responding
</sectionHeader>
<bodyText confidence="0.999883666666667">
Lets take the scenario of Reading Comprehension
tests to argue about the development of the measure.
Our scenario assumes the following:
</bodyText>
<listItem confidence="0.999979">
• There are several questions.
• Each question has several options.
• One option is correct (and only one).
</listItem>
<bodyText confidence="0.998603">
The first step is to consider the possibility of not
responding. If the system responds, then the assess-
ment will be one of two: correct or wrong. But if
</bodyText>
<page confidence="0.93198">
1415
</page>
<note confidence="0.97907">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1415–1424,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999521">
the system doesn’t respond there is no assessment.
Since every question has a correct answer, non re-
sponse is not correct but it is not incorrect either.
This is represented in contingency Table 1, where:
monotonic transformation of (1) permit us to pre-
serve the ranking produced by the measure. Let
f(x)=0.5x+0.5 be the monotonic function to be used
for the transformation. Applying this function to
Formula (1) results in Formula (2):
</bodyText>
<listItem confidence="0.999866333333333">
• nac: number of questions for which the answer
is correct
• naw: number of questions for which the answer
is incorrect
• nu: number of questions not answered
• n: number of questions (n = nac + naw + nu)
</listItem>
<figure confidence="0.986464769230769">
0.5nac − naw 0.5 [nac − naw + n] =
n + 0.5 =
n
0.5 [nac − naw + nac + naw + nu]
=
n
0.5 nac + 0.5nu
= [2nac + nu] = n
n n
(2)
Correct (C) Incorrect (-,C)
Answered (A) nac naw
Unanswered (-,A) nu
</figure>
<tableCaption confidence="0.996639">
Table 1: Contingency table for our scenario
</tableCaption>
<bodyText confidence="0.997928">
Let’s start studying a simple utility function able
to establish the preference order we want:
</bodyText>
<listItem confidence="0.999702333333333">
• -1 if question receives an incorrect response
• 0 if question is left unanswered
• 1 if question receives a correct response
</listItem>
<bodyText confidence="0.999815">
Let U(i) be the utility function that returns one of
the above values for a given question i. Thus, if we
want to consider n questions in the evaluation, the
measure would be:
</bodyText>
<equation confidence="0.945238">
nac − naw (1)
U(i) =
n
</equation>
<bodyText confidence="0.99994784">
The rationale of this utility function is intuitive:
not answering adds no value and wrong answers add
negative values. Positive values of UF indicate more
correct answers than incorrect ones, while negative
values indicate the opposite. However, the utility
function is giving an arbitrary value to the prefer-
ences (-1, 0, 1).
Now we want to interpret in some way the value
that Formula (1) assigns to unanswered questions.
For this purpose, we need to transform Formula (1)
into a more meaningful measure with a parameter
for the number of unanswered questions (nu). A
Measure (2) provides the same ranking of sys-
tems than measure (1). The first summand of For-
mula (2) corresponds to accuracy, while the second
is adding an arbitrary constant weight of 0.5 to the
proportion of unanswered questions. In other words,
unanswered questions are receiving the same value
as if half of them had been answered correctly.
This does not seem correct given that not answer-
ing is being rewarded in the same proportion to all
the systems, without taking into account the per-
formance they have shown with the answered ques-
tions. We need to propose a more sensible estima-
tion for the weight of unanswered questions.
</bodyText>
<subsectionHeader confidence="0.995998">
2.1 A rationale for the Value of Unanswered
Questions
</subsectionHeader>
<bodyText confidence="0.993548714285714">
According to the utility function suggested, unan-
swered questions would have value as if half of them
had been answered correctly. Why half and not other
value? Even more, Why a constant value? Let’s gen-
eralize this idea and estate more clearly our hypoth-
esis:
Unanswered questions have the same value as if a
proportion of them would have been answered cor-
rectly.
We can express this idea according to contingency
Table 1 in the following way:
P(C n A) can be estimated by nac/n, P(-,A)
can be estimated by nu/n, and we have to estimate
P(C/-,A). Our hypothesis is saying that P(C/-,A)
</bodyText>
<equation confidence="0.99979875">
UF = 1 n
n Z=1
= P(C n A) + P(C/-,A) * P(-,A) (3)
P(C) = P(C n A) + P(C n -,A) =
</equation>
<page confidence="0.907375">
1416
</page>
<bodyText confidence="0.997171714285714">
is different from 0. The utility measure (2) corre-
sponds to P(C) in Formula (3) where P(C/-,A) re-
ceives a constant value of 0.5. It is assuming arbi-
trarily that P(C/-,A) = P(C/A).
Following this, our measure must consist of two
parts: The overall accuracy and a better estimation
of correctness over the unanswered questions.
</bodyText>
<subsectionHeader confidence="0.999456">
2.2 The Measure Proposed: c@1
</subsectionHeader>
<bodyText confidence="0.999658571428571">
From the answered questions we have already ob-
served the proportion of questions that received a
correct answer (P(C n A) = nac/n). We can use this
observation as our estimation for P(C/-,A) instead
of the arbitrary value of 0.5.
Thus, the measure we propose is c@1 (correct-
ness at one) and is formally represented as follows:
</bodyText>
<equation confidence="0.951717333333333">
nac + nac nu = 1 (nac + nac nu) (4)
c@1 = n n n n
n
</equation>
<bodyText confidence="0.997586">
The most important features of c@1 are:
</bodyText>
<listItem confidence="0.993399">
1. A system that answers all the questions will re-
ceive a score equal to the traditional accuracy
measure: nu=0 and therefore c@1=nac/n.
2. Unanswered questions will add value to c@1
as if they were answered with the accuracy al-
ready shown.
3. A system that does not return any answer would
receive a score equal to 0 due to nac=0 in both
summands.
</listItem>
<bodyText confidence="0.854894833333333">
According to the reasoning above, we can inter-
pret c@1 in terms of probability as P(C) where
P(C/-,A) has been estimated with P(C n A). In
the following section we will show that there is no
other estimation for P(C/-,A) able to provide a rea-
sonable evaluation measure.
</bodyText>
<sectionHeader confidence="0.978413" genericHeader="method">
3 Other Estimations for P(C/-,A)
</sectionHeader>
<bodyText confidence="0.98515525">
In this section we study whether other estimations
of P(C/-,A) can provide a sensible measure for QA
when unanswered questions are taken into account.
They are:
</bodyText>
<listItem confidence="0.9640265">
1. P(C/-,A) - 0
2. P(C/-,A) - 1
3. P(C/-,A) - P(-,C/-,A) - 0.5
4. P(C/-,A) - P(C/A)
5. P(C/-,A) - P(-,C/A)
3.1 P(C/-,A) - 0
</listItem>
<bodyText confidence="0.971096666666667">
This estimation considers the absence of response as
incorrect response and we have the traditional accu-
racy (nac/n).
</bodyText>
<equation confidence="0.7779495">
Obviously, this is against our purposes.
3.2 P(C/-,A) - 1
</equation>
<bodyText confidence="0.99989725">
This estimation considers all unanswered questions
as correctly answered. This option is not reasonable
and is given for completeness: systems giving no
answer would get maximum score.
</bodyText>
<subsectionHeader confidence="0.589995">
3.3 P(C/-,A) - P(-,C/-,A) - 0.5
</subsectionHeader>
<bodyText confidence="0.9996">
It could be argued that since we cannot have obser-
vations of correctness for unanswered questions, we
should assume equiprobability between P(C/-,A)
and P(-,C/-,A). In this case, P(C) corresponds
to the expression (2) already discussed. As previ-
ously explained, in this case we are giving an arbi-
trary constant value to unanswered questions inde-
pendently of the system’s performance shown with
answered ones. This seems unfair. We should be
aiming at rewarding those systems not responding
instead of giving wrong answers, not reward the sole
fact that the system is not responding.
</bodyText>
<subsectionHeader confidence="0.686753">
3.4 P(C/-,A) - P(C/A)
</subsectionHeader>
<bodyText confidence="0.9999832">
An alternative is to estimate the probability of cor-
rectness for the unanswered questions as the pre-
cision observed over the answered ones: P(C/A)=
nac/(nac+ naw). In this case, our measure would be
like the one shown in Formula (5):
</bodyText>
<equation confidence="0.9990852">
P(C) = P(C n A) + P(C/-,A) * P(-,A) =
= P(C/A) * P(A) + P(C/A) * P(-,A) =
nac
=
P(C/A) = nac + naw
</equation>
<bodyText confidence="0.9999734">
The resulting measure is again the observed pre-
cision over the answered ones. This is not a sensible
measure, as it would reward a cheating system that
decides to leave all questions unanswered except one
for which it is sure to have a correct answer.
</bodyText>
<figure confidence="0.365627">
(5)
</figure>
<page confidence="0.958341">
1417
</page>
<bodyText confidence="0.9996105">
Furthermore, from the idea that P(C/,A) is
equal to P(C/A) the underlying assumption is that
systems choose to answer or not to answer ran-
domly, whereas we want to reward the systems that
choose not responding because they are able to de-
cide that their candidate options are wrong or be-
cause they are unable to decide which candidate is
correct.
</bodyText>
<subsectionHeader confidence="0.556526">
3.5 P(C/,A) - P(,C/A)
</subsectionHeader>
<bodyText confidence="0.999638333333333">
The last option to be considered explores the idea
that systems fail not responding in the same propor-
tion that they fail when they give an answer (i.e. pro-
portion of incorrect answers).
Estimating P(C/,A) as naw / (nac+ naw), the
measure would be:
</bodyText>
<equation confidence="0.999275">
P(C) = P(C n A) + P(C/,A) * P(,A) =
= P(C n A) * P(,C/A) * P(,A) = (6)
</equation>
<bodyText confidence="0.9988745">
This measure is very easy to cheat. It is possible
to obtain almost a perfect score just by answering in-
correctly only one question and leaving unanswered
the rest of the questions.
</bodyText>
<sectionHeader confidence="0.831127" genericHeader="method">
4 Evaluation of c@1
</sectionHeader>
<bodyText confidence="0.999988590909091">
When a new measure is proposed, it is important
to study the reliability of the results obtained us-
ing that measure. For this purpose, we have cho-
sen the method described by Buckley and Voorhees
(2000) for assessing the stability and discrimination
power, as well as the method described by Voorhees
and Buckley (2002) for examining the sensitivity of
our measure. These methods have been used for
studying IR metrics (showing similar results with
the methods based on statistics (Sakai, 2006)), as
well as for evaluating the reliability of other QA
measures different to the ones studied here (Sakai,
2007a; Voorhees, 2002; Voorhees, 2003).
We have compared the results over c@1 with the
ones obtained using both accuracy and the utility
function (UF) defined in Formula (1). This compari-
son is useful to show how confident can a researcher
be with the results obtained using each evaluation
measure.
In the following subsections we will first show the
data used for our study. Then, the experiments about
stability and sensitivity will be described.
</bodyText>
<subsectionHeader confidence="0.998999">
4.1 Data sets
</subsectionHeader>
<bodyText confidence="0.999989615384615">
We used the test collections and runs from the Ques-
tion Answering track at the Cross Language Evalu-
ation Forum 2009 (CLEF) (Pe˜nas et al., 2010). The
collection has a set of 500 questions with their an-
swers. The 44 runs in different languages contain
the human assessments for the answers given by ac-
tual participants. Systems could chose not to answer
a question. In this case, they had the chance to sub-
mit their best candidate in order to assess the perfor-
mance of their validation module (the one that de-
cides whether to give or not the answer).
This data collection allows us to compare c@1
and accuracy over the same runs.
</bodyText>
<subsectionHeader confidence="0.992014">
4.2 Stability vs. Discrimination Power
</subsectionHeader>
<bodyText confidence="0.999991964285714">
The more stable a measure is, the lower the probabil-
ity of errors associated with the conclusion “system
A is better than system B” is. Measures with a high
error must be used more carefully performing more
experiments than in the case of using a measure with
lower error.
In order to study the stability of c@1 and to com-
pare it with accuracy we used the method described
by Buckley and Voorhees (2000). This method al-
lows also to study the number of times systems are
deemed to be equivalent with respect to a certain
measure, which reflects the discrimination power of
that measure. The less discriminative the measure
is, the more ties between systems there will be. This
means that longer difference in scores will be needed
for concluding which system is better (Buckley and
Voorhees, 2000).
The method works as follows: let S denote a set
of runs. Let x and y denote a pair of runs from S.
Let Q denote the entire evaluation collection. Let f
represents the fuzziness value, which is the percent
difference between scores such that if the difference
is smaller than f then the two scores are deemed to
be equivalent. We apply the algorithm of Figure 1
to obtain the information needed for computing the
error rate (Formula (7)). Stability is inverse to this
value, the lower the error rate is, the more stable
the measure is. The same algorithm gives us the
</bodyText>
<equation confidence="0.8741544">
nac
=
n
+ naw * nu
nac + naw n
</equation>
<page confidence="0.933896">
1418
</page>
<bodyText confidence="0.99985375">
proportion of ties (Formula (8)), which we use for
measuring discrimination power, that is the lower
the proportion of ties is, the more discriminative the
measure is.
</bodyText>
<equation confidence="0.9529007">
for each pair of runs x,y c S
for each trial from 1 to 100
Qi = select at random subcol of size c from Q;
margin = f * max (M(x,Qi),M(y,Qi));
if(|M(x,Qi) - M(y,Qi) |&lt; |margin|)
EQM(x,y)++;
else if(|M(x,Qi) &gt; M(y,Qi)|)
GTM(x,y)++;
else
GTM(y,x)++;
</equation>
<figureCaption confidence="0.9956135">
Figure 1: Algorithm for computing EQM(x,y),
GTM(x,y) and GTM(y,x) in the stability method
</figureCaption>
<bodyText confidence="0.99998475862069">
We assume that for each measure the correct de-
cision about whether run x is better than run y hap-
pens when there are more cases where the value of
x is better than the value of y. Then, the number of
times y is better than x is considered as the number
of times the test is misleading, while the number of
times the values of x and y are equivalent is consid-
ered the number of ties.
On the other hand, it is clear that larger fuzziness
values decrease the error rate but also decrease the
discrimination power of a measure. Since a fixed
fuzziness value might imply different trade-offs for
different metrics, we decided to vary the fuzziness
value from 0.01 to 0.10 (following the work by Sakai
(2007b)) and to draw for each measure a proportion-
of-ties / error-rate curve. Figure 2 shows these
curves for the c@1, accuracy and OF measures. In
the Figure we can see how there is a consistent de-
crease of the error rate of all measures when the
proportion of ties increases (this corresponds to the
increase in the fuzziness value). Figure 2 shows
that the curves of accuracy and c@1 are quite simi-
lar (slightly better behavior of c@1) , which means
that they have a similar stability and discrimination
power.
The results suggest that the three measures are
quite stable, having c@1 and accuracy a lower er-
ror rate than OF when the proportion of ties grows.
These curves are similar to the ones obtained for
</bodyText>
<figureCaption confidence="0.997417">
Figure 2: Error-rate / Proportion of ties curves for accu-
racy, c@1 and OF with c = 250
</figureCaption>
<bodyText confidence="0.666889">
other QA evaluation measures (Sakai, 2007a).
</bodyText>
<subsectionHeader confidence="0.989602">
4.3 Sensitivity
</subsectionHeader>
<bodyText confidence="0.99998753125">
The swap-rate (Voorhees and Buckley, 2002) repre-
sents the chance of obtaining a discrepancy between
two question sets (of the same size) as to whether
a system is better than another given a certain dif-
ference bin. Looking at the swap-rates of all the
difference performance bins, the performance dif-
ference required in order to conclude that a run is
better than another for a given confidence value can
be estimated. For example, if we want to know the
required difference for concluding that system A is
better than system B with a confidence of 95%, then
we select the difference that represents the first bin
where the swap-rate is lower or equal than 0.05.
The sensitivity of the measure is the number of
times among all the comparisons in the experi-
ment where this performance difference is obtained
(Sakai, 2007b). That is, the more comparisons ac-
complish the estimated performance difference, the
more sensitive is the measure. The more sensitive
the measure, the more useful it is for system dis-
crimination.
The swap method works as follows: let S denote
a set of runs, let x and y denote a pair of runs from S.
Let Q denote the entire evaluation collection. And
let d denote a performance difference between two
runs. Then, we first define 21 performance differ-
ence bins: the first bin represents performance dif-
ferences between systems such that 0 ≤ d &lt; 0.01;
the second bin represents differences such that 0.01
≤ d &lt; 0.02; and the limits for the remaining bins in-
crease by increments of 0.01, with the last bin con-
taining all the differences equal or higher than 0.2.
</bodyText>
<page confidence="0.847834">
1419
</page>
<equation confidence="0.9990376">
Ex,yES min(GTM(x, y), GTM(y,x))
Error rateM = (7)
Ex,yϵS(GTM(x,y) + GTM(y,x) + EQM(x,y))
Prop TiesM = r Ex,yϵS E QM(x, y)(8)
Ex,yϵS (GTm (x, y) + GTM(y, x) + EQM(x, y))
</equation>
<bodyText confidence="0.99140525">
Let BIN(d) denote a mapping from a difference d to
one of the 21 bins where it belongs. Thus, algorithm
in Figure 3 is applied for calculating the swap-rate
of each bin.
</bodyText>
<equation confidence="0.990906818181818">
for each pair of runs x,y ϵ S
for each trial from 1 to 100
select Qi , Q′i C Q, where
Qi n Q′i == ϕ and Qi == Q′i == c;
dM(Qi) = M(x, Qi) − M(y, Qi);
dM(Q′i) = M(x, Q′i) − M(y, Q′i);
counter(BIN( dM(Qi) ))++;
if(dM(Qi) * dM(Q′i) &lt; 0)
swap counter(BIN( dM(Qi) ))++;
for each bin b
swap rate(b) = swap counter(b)/counter(b);
</equation>
<figureCaption confidence="0.954189">
Figure 3: Algorithm for computing swap-rates
</figureCaption>
<table confidence="0.97543175">
(i) (ii) (iii) (iv)
OF 0.17 0.48 35.12% 59.30%
c@1 0.09 0.77 11.69% 58.40%
accuracy 0.09 0.68 13.24% 55.00%
</table>
<tableCaption confidence="0.963697833333333">
Table 2: Results obtained applying the swap method to
accuracy, c@1 and UF at 95% of confidence, with c =
250: (i) Absolute difference required; (ii) Highest value
obtained; (iii) Relative difference required ((i)/(ii)); (iv)
percentage of comparisons that accomplish the required
difference (sensitivity)
</tableCaption>
<bodyText confidence="0.999940857142857">
Given that Qi and Q′i must be disjoint, their size
can only be up to half of the size of the original col-
lection. Thus, we use the value c=250 for our exper-
iment1. Table 2 shows the results obtained by apply-
ing the swap method to accuracy, c@1 and UF, with
c = 250, swap-rate &lt; 5, and sensitivity given a con-
fidence of 95% (Column (iv)). The range of values
</bodyText>
<footnote confidence="0.6444465">
1We use the same size for experiments in Section 4.2 for
homogeneity reasons.
</footnote>
<bodyText confidence="0.9977569">
are similar to the ones obtained for other measures
according to (Sakai, 2007a).
According to Column (i), a higher absolute dif-
ference is required for concluding that a system is
better than another using UF. However, the relative
difference is similar to the one required by c@1.
Thus, similar percentage of comparisons using c@1
and UF accomplish the required difference (Column
(iv)). These results show that their sensitivity values
are similar, and higher than the value for accuracy.
</bodyText>
<subsectionHeader confidence="0.995612">
4.4 Qualitative evaluation
</subsectionHeader>
<bodyText confidence="0.999983">
In addition to the theoretical study, we undertook a
study to interpret the results obtained by real sys-
tems in a real scenario. The aim is to compare the
results of the proposed c@1 measure with accuracy
in order to compare their behavior. For this purpose
we inspected the real systems runs in the data set.
</bodyText>
<table confidence="0.9007154">
System c@1 accuracy (i) (ii) (iii)
icia091ro 0.58 0.47 237 156 107
uaic092ro 0.47 0.47 236 264 0
loga092de 0.44 0.37 187 230 83
base092de 0.38 0.38 189 311 0
</table>
<tableCaption confidence="0.98318875">
Table 3: Example of system results in QA@CLEF 2009.
(i) number of questions correctly answered; (ii) number
of questions incorrectly answered; (iii) number of unan-
swered questions.
</tableCaption>
<bodyText confidence="0.9973875">
Table 3 shows a couple of examples where two
systems have answered correctly a similar num-
ber of questions. For example, this is the case of
icia091ro and uaic092ro that, therefore, obtain al-
most the same accuracy value. However, icia091ro
has returned less incorrect answers by not respond-
ing some questions. This is the kind of behavior we
want to measure and reward. Table 3 shows how
accuracy is sensitive only to the number of correct
answers whereas c@1 is able to distinguish when
</bodyText>
<page confidence="0.977463">
1420
</page>
<bodyText confidence="0.9997225">
systems keep the number of correct answers but re-
duce the number of incorrect ones by not respond-
ing to some. The same reasoning is applicable to
loga092de compared to base092de for German.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999988757575757">
The decision of leaving a query without response is
related to the system ability to measure accurately its
self-confidence about the correctness of their candi-
date answers. Although there have been one attempt
to make the self-confidence score explicit and use
it (Herrera et al., 2005), rankings are, usually, the
implicit way to evaluate this self-confidence. Mean
Reciprocal Rank (MRR) has traditionally been used
to evaluate Question Answering systems when sev-
eral answers per question were allowed and given
in order (Fukumoto et al., 2002; Voorhees and Tice,
1999). However, as it occurs with Accuracy (propor-
tion of questions correctly answered), the risk of giv-
ing a wrong answer is always preferred better than
not responding.
The QA track at TREC 2001 was the first eval-
uation campaign in which systems were allowed
to leave a question unanswered (Voorhees, 2001).
The main evaluation measure was MRR, but perfor-
mance was also measured by means of the percent-
age of answered questions and the portion of them
that were correctly answered. However, no combi-
nation of these two values into a unique measure was
proposed.
TREC 2002 discarded the idea of including unan-
swered questions in the evaluation. Only one answer
by question was allowed and all answers had to be
ranked according to the system’s self-confidence in
the correctness of the answer. Systems were evalu-
ated by means of Confidence Weighted Score (CWS),
rewarding those systems able to provide more cor-
rect answers at the top of the ranking (Voorhees,
2002). The formulation of CWS is the following:
</bodyText>
<equation confidence="0.983732">
C(i) (9)
i
</equation>
<bodyText confidence="0.989798333333333">
Where n is the number of questions, and C(i) is
the number of correct answers up to the position i in
the ranking. Formally:
</bodyText>
<equation confidence="0.995486333333333">
Z
C(i) = I(j) (10)
j=1
</equation>
<bodyText confidence="0.99993575">
where I(j) is a function that returns 1 if answer j
is correct and 0 if it is not. The formulation of CWS
is inspired by the Average Precision (AP) over the
ranking for one question:
</bodyText>
<equation confidence="0.994468">
1 �
AP = R
r
</equation>
<bodyText confidence="0.99977172972973">
where R is the number of known relevant results
for a topic, and r is a position in the ranking. Since
only one answer per question is requested, R equals
to n (the number of questions) in CWS. However,
in AP formula the summands belong to the posi-
tions of the ranking where there is a relevant result
(product of I(r)), whereas in CWS every position of
the ranking add value to the measure regardless of
whether there is a relevant result or not in that po-
sition. Therefore, CWS gives much more value to
some questions over others: questions whose an-
swers are at the top of the ranking are giving almost
the complete value to CWS, whereas those questions
whose answers are at the bottom of the ranking are
almost not counting in the evaluation.
Although CWS was aimed at promoting the de-
velopment of better self-confidence scores, it was
discussed as a measure for evaluating QA systems
performance. CWS was discarded in the following
campaigns of TREC in favor of accuracy (Voorhees,
2003). Subsequently, accuracy was adopted by the
QA track at the Cross-Language Evaluation Forum
from the beginning (Magnini et al., 2005).
There was an attempt to consider explicitly sys-
tems confidence self-score (Herrera et al., 2005): the
use of the Pearson’s correlation coefficient and the
proposal of measures K and K1 (see Formula 12).
These measures are based in a utility function that
returns -1 if the answer is incorrect and 1 if it is
correct. This positive or negative value is weighted
with the normalized confidence self-score given by
the system to each answer. K is a variation of K1
for being used in evaluations where more than an
answer per question is allowed.
If the self-score is 0, then the answer is ignored
and thus, this measure is permitting to leave a ques-
tion unanswered. A system that always returns a
</bodyText>
<equation confidence="0.989490555555556">
CW S = 1 n
n Z=1
I(r)C(r)
r
(11)
1421
Eself score(i) − � self score(i)
K1 = iϵlcorrect.nswers} iϵlincorrect.nswers} E [−1, 1] (12)
n
</equation>
<bodyText confidence="0.99996152631579">
self-score equals to 0 (no answer) obtains a K1 value
of 0. However, the final value of K1 is difficult to
interpret: a positive value does not indicate neces-
sarily more correct answers than incorrect ones, but
that the sum of scores of correct answers is higher
than the sum resulting from the scores of incorrect
answers. This could explain the little success of this
measure for evaluating QA systems in favor, again,
of accuracy measure.
Accuracy is the simplest and most intuitive evalu-
ation measure. At the same time is able to reward
those systems showing good performance. How-
ever, together with MRR belongs to the set of mea-
sures that pushes in favor of giving always a re-
sponse, even wrong, since there is no punishment for
it. Thus, the development of better validation tech-
nologies (systems able to decide whether the can-
didate answers are correct or not) is not promoted,
despite new QA architectures require them.
In effect, most QA systems during TREC and
CLEF campaigns had an upper bound of accuracy
around 60%. An explanation for this was the effect
of error propagation in the most extended pipeline
architecture: Passage Retrieval, Answer Extraction,
Answer Ranking. Even with performances higher
than 80% in each step, the overall performance
drops dramatically just because of the product of
partial performances. Thus, a way to break the
pipeline architecture is the development of a mod-
ule able to decide whether the QA system must con-
tinue or not its searching for new candidate answers:
the Answer Validation module. This idea is behind
the architecture of IBM’s Watson (DeepQA project)
that successfully participated at Jeopardy (Ferrucci
et al., 2010).
In 2006, the first Answer Validation Exercise
(AVE) proposed an evaluation task to advance the
state of the art in Answer Validation technologies
(Pe˜nas et al., 2007). The starting point was the re-
formulation of Answer Validation as a Recognizing
Textual Entailment problem, under the assumption
that hypotheses can be automatically generated by
combining the question with the candidate answer
(Pe˜nas et al., 2008a). Thus, validation was seen as a
binary classification problem whose evaluation must
deal with unbalanced collections (different propor-
tion of positive and negative examples, correct and
incorrect answers). For this reason, AVE 2006 used
F-measure based on precision and recall for correct
answers selection (Pe˜nas et al., 2007). Other op-
tion is an evaluation based on the analysis of Re-
ceiver Operating Characteristic (ROC) space, some-
times preferred for classification tasks with unbal-
anced collections. A comparison of both approaches
for Answer Validation evaluation is provided in (Ro-
drigo et al., 2011).
AVE 2007 changed its evaluation methodology
with two objectives: the first one was to bring sys-
tems based on Textual Entailment to the Automatic
Hypothesis Generation problem which is not part it-
self of the Recognising Textual Entailment (RTE)
task but an Answer Validation need. The second
one was an attempt to quantify the gain in QA per-
formance when more sophisticated validation mod-
ules are introduced (Pe˜nas et al., 2008b). With this
aim, several measures were proposed to assess: the
correct selection of candidate answers, the correct
rejection of wrong answer and finally estimate the
potential gain (in terms of accuracy) that Answer
Validation modules can provide to QA (Rodrigo et
al., 2008). The idea was to give value to the cor-
rectly rejected answers as if they could be correctly
answered with the accuracy shown selecting the cor-
rect answers. This extension of accuracy in the An-
swer Validation scenario inspired the initial develop-
ment of c@1 considering non-response.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99968525">
The central idea of this work is that not respond-
ing has more value than responding incorrectly. This
idea is not new, but despite several attempts in TREC
and CLEF there wasn’t a commonly accepted mea-
</bodyText>
<page confidence="0.982399">
1422
</page>
<bodyText confidence="0.999994304347826">
sure to assess non-response. We have studied here
an extension of accuracy measure with this feature,
and with a very easy to understand rationale: Unan-
swered questions have the same value as if a pro-
portion of them had been answered correctly, and
the value they add is related to the performance (ac-
curacy) observed over the answered questions. We
have shown that no other estimation of this value
produce a sensible measure.
We have shown also that the proposed measure
c@1 has a good balance of discrimination power,
stability and sensitivity properties. Finally, we have
shown how this measure rewards systems able to
maintain the same number of correct answers and at
the same time reduce the number of incorrect ones,
by leaving some questions unanswered.
Among other tasks, measure c@1 is well suited
for evaluating Reading Comprehension tests, where
multiple choices per question are given, but only one
is correct. Non-response must be assessed if we
want to measure effective reading and not just the
ability to rank options. This is clearly not enough
for the development of reading technologies.
</bodyText>
<sectionHeader confidence="0.998392" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99389">
This work has been partially supported by the
Research Network MA2VICMR (S2009/TIC-1542)
and Holopedia project (TIN2010-21128-C02).
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99525207">
Chris Buckley and Ellen M. Voorhees. 2000. Evalu-
ating evaluation measure stability. In Proceedings of
the 23rd annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 33–40. ACM.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John Prager,
Nico Schlaefer, and Chris Welty. 2010. Building Wat-
son: An Overview of the DeepQA Project. AI Maga-
zine, 31(3).
Junichi Fukumoto, Tsuneaki Kato, and Fumito Masui.
2002. Question and Answering Challenge (QAC-
1): Question Answering Evaluation at NTCIR Work-
shop 3. In Working Notes of the Third NTCIR Work-
shop Meeting Part IV: Question Answering Challenge
(QAC-1), pages 1-10.
Jes´us Herrera, Anselmo Pe˜nas, and Felisa Verdejo. 2005.
Question Answering Pilot Task at CLEF 2004. In Mul-
tilingual Information Access for Text, Speech and Im-
ages, CLEF 2004, Revised Selected Papers., volume
3491 of Lecture Notes in Computer Science, Springer,
pages 581–590.
Bernardo Magnini, Alessandro Vallin, Christelle Ayache,
Gregor Erbach, Anselmo Pe˜nas, Maarten de Rijke,
Paulo Rocha, Kiril Ivanov Simov, and Richard F. E.
Sutcliffe. 2005. Overview of the CLEF 2004 Multi-
lingual Question Answering Track. In Multilingual In-
formation Access for Text, Speech and Images, CLEF
2004, Revised Selected Papers., volume 3491 of Lec-
ture Notes in Computer Science, Springer, pages 371–
391.
Anselmo Pe˜nas, ´Alvaro Rodrigo, Valentin Sama, and Fe-
lisa Verdejo. 2007. Overview of the Answer Valida-
tion Exercise 2006. In Evaluation of Multilingual and
Multi-modal Information Retrieval, CLEF 2006, Re-
vised Selected Papers, volume 4730 of Lecture Notes
in Computer Science, Springer, pages 257–264.
Anselmo Pe˜nas, ´Alvaro Rodrigo, Valentin Sama, and Fe-
lisa Verdejo. 2008a. Testing the Reasoning for Ques-
tion Answering Validation. In Journal of Logic and
Computation. 18(3), pages 459–474.
Anselmo Pe˜nas, ´Alvaro Rodrigo, and Felisa Verdejo.
2008b. Overview of the Answer Validation Exercise
2007. In Advances in Multilingual and Multimodal
Information Retrieval, CLEF 2007, Revised Selected
Papers, volume 5152 of Lecture Notes in Computer
Science, Springer, pages 237–248.
Anselmo Pe˜nas, Pamela Forner, Richard Sutcliffe, ´Alvaro
Rodrigo, Corina Forascu, I˜naki Alegria, Danilo Gi-
ampiccolo, Nicolas Moreau, and Petya Osenova.
2010. Overview of ResPubliQA 2009: Question An-
swering Evaluation over European Legislation. In
Multilingual Information Access Evaluation I. Text Re-
trieval Experiments, CLEF 2009, Revised Selected Pa-
pers, volume 6241 of Lecture Notes in Computer Sci-
ence, Springer.
Alvaro Rodrigo, Anselmo Pe˜nas, and Felisa Verdejo.
2008. Evaluating Answer Validation in Multi-stream
Question Answering. In Proceedings of the Second In-
ternational Workshop on Evaluating Information Ac-
cess (EVIA 2008).
Alvaro Rodrigo, Anselmo Pe˜nas, and Felisa Verdejo.
2011. Evaluating Question Answering Validation as a
classification problem. Language Resources and Eval-
uation, Springer Netherlands (In Press).
Tetsuya Sakai. 2006. Evaluating Evaluation Metrics
based on the Bootstrap. In SIGIR 2006: Proceedings
of the 29th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, Seattle, Washington, USA, August 6-11, 2006,
pages 525–532.
1423
Tetsuya Sakai. 2007a. On the Reliability of Factoid
Question Answering Evaluation. ACM Trans. Asian
Lang. Inf. Process., 6(1).
Tetsuya Sakai. 2007b. On the reliability of information
retrieval metrics based on graded relevance. Inf. Pro-
cess. Manage., 43(2):531–548.
Ellen M. Voorhees and Chris Buckley. 2002. The effect
of Topic Set Size on Retrieval Experiment Error. In SI-
GIR ’02: Proceedings of the 25th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 316–323.
Ellen M. Voorhees and Dawn M. Tice. 1999. The TREC-
8 Question Answering Track Evaluation. In Text Re-
trieval Conference TREC-8, pages 83–105.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. In E. M. voorhees, D. K.
Harman, editors: Proceedings of the Tenth Text RE-
trieval Conference (TREC 2001). NIST Special Publi-
cation 500-250.
Ellen M. Voorhees. 2002. Overview of TREC 2002
Question Answering Track. In E.M. Voorhees, L. P.
Buckland, editors: Proceedings of the Eleventh Text
REtrieval Conference (TREC 2002). NIST Publication
500-251.
Ellen M. Voorhees. 2003. Overview of the TREC 2003
Question Answering Track. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
</reference>
<page confidence="0.99461">
1424
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.810177">
<title confidence="0.908881333333333">A Simple Measure to Assess Non-response and Alvaro UNED NLP &amp; IR</title>
<author confidence="0.980721">Juan del Rosal</author>
<address confidence="0.976064">28040 Madrid,</address>
<abstract confidence="0.999017052631579">There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an of with this feature and a very easy to understand interpreta- The measure proposed has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Ellen M Voorhees</author>
</authors>
<title>Evaluating evaluation measure stability.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>33--40</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10969" citStr="Buckley and Voorhees (2000)" startWordPosition="1919" endWordPosition="1922">proportion that they fail when they give an answer (i.e. proportion of incorrect answers). Estimating P(C/,A) as naw / (nac+ naw), the measure would be: P(C) = P(C n A) + P(C/,A) * P(,A) = = P(C n A) * P(,C/A) * P(,A) = (6) This measure is very easy to cheat. It is possible to obtain almost a perfect score just by answering incorrectly only one question and leaving unanswered the rest of the questions. 4 Evaluation of c@1 When a new measure is proposed, it is important to study the reliability of the results obtained using that measure. For this purpose, we have chosen the method described by Buckley and Voorhees (2000) for assessing the stability and discrimination power, as well as the method described by Voorhees and Buckley (2002) for examining the sensitivity of our measure. These methods have been used for studying IR metrics (showing similar results with the methods based on statistics (Sakai, 2006)), as well as for evaluating the reliability of other QA measures different to the ones studied here (Sakai, 2007a; Voorhees, 2002; Voorhees, 2003). We have compared the results over c@1 with the ones obtained using both accuracy and the utility function (UF) defined in Formula (1). This comparison is usefu</context>
<context position="12905" citStr="Buckley and Voorhees (2000)" startWordPosition="2251" endWordPosition="2254">sess the performance of their validation module (the one that decides whether to give or not the answer). This data collection allows us to compare c@1 and accuracy over the same runs. 4.2 Stability vs. Discrimination Power The more stable a measure is, the lower the probability of errors associated with the conclusion “system A is better than system B” is. Measures with a high error must be used more carefully performing more experiments than in the case of using a measure with lower error. In order to study the stability of c@1 and to compare it with accuracy we used the method described by Buckley and Voorhees (2000). This method allows also to study the number of times systems are deemed to be equivalent with respect to a certain measure, which reflects the discrimination power of that measure. The less discriminative the measure is, the more ties between systems there will be. This means that longer difference in scores will be needed for concluding which system is better (Buckley and Voorhees, 2000). The method works as follows: let S denote a set of runs. Let x and y denote a pair of runs from S. Let Q denote the entire evaluation collection. Let f represents the fuzziness value, which is the percent </context>
</contexts>
<marker>Buckley, Voorhees, 2000</marker>
<rawString>Chris Buckley and Ellen M. Voorhees. 2000. Evaluating evaluation measure stability. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 33–40. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya A Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
</authors>
<title>Eric Nyberg,</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<location>John Prager, Nico</location>
<contexts>
<context position="26534" citStr="Ferrucci et al., 2010" startWordPosition="4635" endWordPosition="4638">the effect of error propagation in the most extended pipeline architecture: Passage Retrieval, Answer Extraction, Answer Ranking. Even with performances higher than 80% in each step, the overall performance drops dramatically just because of the product of partial performances. Thus, a way to break the pipeline architecture is the development of a module able to decide whether the QA system must continue or not its searching for new candidate answers: the Answer Validation module. This idea is behind the architecture of IBM’s Watson (DeepQA project) that successfully participated at Jeopardy (Ferrucci et al., 2010). In 2006, the first Answer Validation Exercise (AVE) proposed an evaluation task to advance the state of the art in Answer Validation technologies (Pe˜nas et al., 2007). The starting point was the reformulation of Answer Validation as a Recognizing Textual Entailment problem, under the assumption that hypotheses can be automatically generated by combining the question with the candidate answer (Pe˜nas et al., 2008a). Thus, validation was seen as a binary classification problem whose evaluation must deal with unbalanced collections (different proportion of positive and negative examples, corre</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building Watson: An Overview of the DeepQA Project. AI Magazine, 31(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junichi Fukumoto</author>
<author>Tsuneaki Kato</author>
<author>Fumito Masui</author>
</authors>
<title>Question and Answering Challenge (QAC1): Question Answering Evaluation at NTCIR Workshop 3.</title>
<date>2002</date>
<booktitle>In Working Notes of the Third NTCIR Workshop Meeting Part IV: Question Answering Challenge (QAC-1),</booktitle>
<pages>1--10</pages>
<contexts>
<context position="21517" citStr="Fukumoto et al., 2002" startWordPosition="3764" endWordPosition="3767">is applicable to loga092de compared to base092de for German. 5 Related Work The decision of leaving a query without response is related to the system ability to measure accurately its self-confidence about the correctness of their candidate answers. Although there have been one attempt to make the self-confidence score explicit and use it (Herrera et al., 2005), rankings are, usually, the implicit way to evaluate this self-confidence. Mean Reciprocal Rank (MRR) has traditionally been used to evaluate Question Answering systems when several answers per question were allowed and given in order (Fukumoto et al., 2002; Voorhees and Tice, 1999). However, as it occurs with Accuracy (proportion of questions correctly answered), the risk of giving a wrong answer is always preferred better than not responding. The QA track at TREC 2001 was the first evaluation campaign in which systems were allowed to leave a question unanswered (Voorhees, 2001). The main evaluation measure was MRR, but performance was also measured by means of the percentage of answered questions and the portion of them that were correctly answered. However, no combination of these two values into a unique measure was proposed. TREC 2002 disca</context>
</contexts>
<marker>Fukumoto, Kato, Masui, 2002</marker>
<rawString>Junichi Fukumoto, Tsuneaki Kato, and Fumito Masui. 2002. Question and Answering Challenge (QAC1): Question Answering Evaluation at NTCIR Workshop 3. In Working Notes of the Third NTCIR Workshop Meeting Part IV: Question Answering Challenge (QAC-1), pages 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Herrera</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>Question Answering Pilot Task at CLEF</title>
<date>2005</date>
<booktitle>In Multilingual Information Access for Text, Speech and Images, CLEF 2004, Revised Selected Papers.,</booktitle>
<volume>3491</volume>
<pages>581--590</pages>
<publisher>Springer,</publisher>
<marker>Herrera, Pe˜nas, Verdejo, 2005</marker>
<rawString>Jes´us Herrera, Anselmo Pe˜nas, and Felisa Verdejo. 2005. Question Answering Pilot Task at CLEF 2004. In Multilingual Information Access for Text, Speech and Images, CLEF 2004, Revised Selected Papers., volume 3491 of Lecture Notes in Computer Science, Springer, pages 581–590.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bernardo Magnini</author>
<author>Alessandro Vallin</author>
<author>Christelle Ayache</author>
<author>Gregor Erbach</author>
<author>Anselmo Pe˜nas</author>
<author>Maarten de Rijke</author>
<author>Paulo Rocha</author>
<author>Kiril Ivanov Simov</author>
<author>Richard F E Sutcliffe</author>
</authors>
<title>Multilingual Question Answering Track.</title>
<date>2005</date>
<journal>Overview of the CLEF</journal>
<booktitle>In Multilingual Information Access for Text, Speech and Images, CLEF 2004, Revised Selected Papers.,</booktitle>
<volume>3491</volume>
<pages>371--391</pages>
<publisher>Springer,</publisher>
<marker>Magnini, Vallin, Ayache, Erbach, Pe˜nas, de Rijke, Rocha, Simov, Sutcliffe, 2005</marker>
<rawString>Bernardo Magnini, Alessandro Vallin, Christelle Ayache, Gregor Erbach, Anselmo Pe˜nas, Maarten de Rijke, Paulo Rocha, Kiril Ivanov Simov, and Richard F. E. Sutcliffe. 2005. Overview of the CLEF 2004 Multilingual Question Answering Track. In Multilingual Information Access for Text, Speech and Images, CLEF 2004, Revised Selected Papers., volume 3491 of Lecture Notes in Computer Science, Springer, pages 371– 391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Pe˜nas</author>
<author>´Alvaro Rodrigo</author>
<author>Valentin Sama</author>
<author>Felisa Verdejo</author>
</authors>
<title>Overview of the Answer Validation Exercise</title>
<date>2007</date>
<booktitle>In Evaluation of Multilingual and Multi-modal Information Retrieval, CLEF</booktitle>
<volume>4730</volume>
<pages>257--264</pages>
<publisher>Springer,</publisher>
<marker>Pe˜nas, Rodrigo, Sama, Verdejo, 2007</marker>
<rawString>Anselmo Pe˜nas, ´Alvaro Rodrigo, Valentin Sama, and Felisa Verdejo. 2007. Overview of the Answer Validation Exercise 2006. In Evaluation of Multilingual and Multi-modal Information Retrieval, CLEF 2006, Revised Selected Papers, volume 4730 of Lecture Notes in Computer Science, Springer, pages 257–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Pe˜nas</author>
<author>´Alvaro Rodrigo</author>
<author>Valentin Sama</author>
<author>Felisa Verdejo</author>
</authors>
<title>Testing the Reasoning for Question Answering Validation.</title>
<date>2008</date>
<booktitle>In Journal of Logic and Computation.</booktitle>
<volume>18</volume>
<issue>3</issue>
<pages>459--474</pages>
<marker>Pe˜nas, Rodrigo, Sama, Verdejo, 2008</marker>
<rawString>Anselmo Pe˜nas, ´Alvaro Rodrigo, Valentin Sama, and Felisa Verdejo. 2008a. Testing the Reasoning for Question Answering Validation. In Journal of Logic and Computation. 18(3), pages 459–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Pe˜nas</author>
<author>´Alvaro Rodrigo</author>
<author>Felisa Verdejo</author>
</authors>
<title>Overview of the Answer Validation Exercise</title>
<date>2008</date>
<booktitle>In Advances in Multilingual and Multimodal Information Retrieval, CLEF</booktitle>
<volume>5152</volume>
<pages>237--248</pages>
<publisher>Springer,</publisher>
<marker>Pe˜nas, Rodrigo, Verdejo, 2008</marker>
<rawString>Anselmo Pe˜nas, ´Alvaro Rodrigo, and Felisa Verdejo. 2008b. Overview of the Answer Validation Exercise 2007. In Advances in Multilingual and Multimodal Information Retrieval, CLEF 2007, Revised Selected Papers, volume 5152 of Lecture Notes in Computer Science, Springer, pages 237–248.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anselmo Pe˜nas</author>
<author>Pamela Forner</author>
<author>Richard Sutcliffe</author>
<author>´Alvaro Rodrigo</author>
<author>Corina Forascu</author>
<author>I˜naki Alegria</author>
<author>Danilo Giampiccolo</author>
<author>Nicolas Moreau</author>
<author>Petya Osenova</author>
</authors>
<title>Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation. In Multilingual Information Access Evaluation I. Text Retrieval Experiments, CLEF 2009, Revised Selected Papers,</title>
<date>2010</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>6241</volume>
<publisher>Springer.</publisher>
<marker>Pe˜nas, Forner, Sutcliffe, Rodrigo, Forascu, Alegria, Giampiccolo, Moreau, Osenova, 2010</marker>
<rawString>Anselmo Pe˜nas, Pamela Forner, Richard Sutcliffe, ´Alvaro Rodrigo, Corina Forascu, I˜naki Alegria, Danilo Giampiccolo, Nicolas Moreau, and Petya Osenova. 2010. Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation. In Multilingual Information Access Evaluation I. Text Retrieval Experiments, CLEF 2009, Revised Selected Papers, volume 6241 of Lecture Notes in Computer Science, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro Rodrigo</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>Evaluating Answer Validation in Multi-stream Question Answering.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Information Access (EVIA</booktitle>
<marker>Rodrigo, Pe˜nas, Verdejo, 2008</marker>
<rawString>Alvaro Rodrigo, Anselmo Pe˜nas, and Felisa Verdejo. 2008. Evaluating Answer Validation in Multi-stream Question Answering. In Proceedings of the Second International Workshop on Evaluating Information Access (EVIA 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro Rodrigo</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>Evaluating Question Answering Validation as a classification problem. Language Resources and Evaluation,</title>
<date>2011</date>
<publisher>Springer Netherlands (In Press).</publisher>
<marker>Rodrigo, Pe˜nas, Verdejo, 2011</marker>
<rawString>Alvaro Rodrigo, Anselmo Pe˜nas, and Felisa Verdejo. 2011. Evaluating Question Answering Validation as a classification problem. Language Resources and Evaluation, Springer Netherlands (In Press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
</authors>
<title>Evaluating Evaluation Metrics based on the Bootstrap. In</title>
<date>2006</date>
<booktitle>SIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>525--532</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="11261" citStr="Sakai, 2006" startWordPosition="1966" endWordPosition="1967"> just by answering incorrectly only one question and leaving unanswered the rest of the questions. 4 Evaluation of c@1 When a new measure is proposed, it is important to study the reliability of the results obtained using that measure. For this purpose, we have chosen the method described by Buckley and Voorhees (2000) for assessing the stability and discrimination power, as well as the method described by Voorhees and Buckley (2002) for examining the sensitivity of our measure. These methods have been used for studying IR metrics (showing similar results with the methods based on statistics (Sakai, 2006)), as well as for evaluating the reliability of other QA measures different to the ones studied here (Sakai, 2007a; Voorhees, 2002; Voorhees, 2003). We have compared the results over c@1 with the ones obtained using both accuracy and the utility function (UF) defined in Formula (1). This comparison is useful to show how confident can a researcher be with the results obtained using each evaluation measure. In the following subsections we will first show the data used for our study. Then, the experiments about stability and sensitivity will be described. 4.1 Data sets We used the test collection</context>
</contexts>
<marker>Sakai, 2006</marker>
<rawString>Tetsuya Sakai. 2006. Evaluating Evaluation Metrics based on the Bootstrap. In SIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, Washington, USA, August 6-11, 2006, pages 525–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
</authors>
<title>On the Reliability of Factoid Question Answering Evaluation.</title>
<date>2007</date>
<journal>ACM Trans. Asian Lang. Inf. Process.,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="11374" citStr="Sakai, 2007" startWordPosition="1985" endWordPosition="1986"> c@1 When a new measure is proposed, it is important to study the reliability of the results obtained using that measure. For this purpose, we have chosen the method described by Buckley and Voorhees (2000) for assessing the stability and discrimination power, as well as the method described by Voorhees and Buckley (2002) for examining the sensitivity of our measure. These methods have been used for studying IR metrics (showing similar results with the methods based on statistics (Sakai, 2006)), as well as for evaluating the reliability of other QA measures different to the ones studied here (Sakai, 2007a; Voorhees, 2002; Voorhees, 2003). We have compared the results over c@1 with the ones obtained using both accuracy and the utility function (UF) defined in Formula (1). This comparison is useful to show how confident can a researcher be with the results obtained using each evaluation measure. In the following subsections we will first show the data used for our study. Then, the experiments about stability and sensitivity will be described. 4.1 Data sets We used the test collections and runs from the Question Answering track at the Cross Language Evaluation Forum 2009 (CLEF) (Pe˜nas et al., 2</context>
<context position="15109" citStr="Sakai (2007" startWordPosition="2651" endWordPosition="2652">n y happens when there are more cases where the value of x is better than the value of y. Then, the number of times y is better than x is considered as the number of times the test is misleading, while the number of times the values of x and y are equivalent is considered the number of ties. On the other hand, it is clear that larger fuzziness values decrease the error rate but also decrease the discrimination power of a measure. Since a fixed fuzziness value might imply different trade-offs for different metrics, we decided to vary the fuzziness value from 0.01 to 0.10 (following the work by Sakai (2007b)) and to draw for each measure a proportionof-ties / error-rate curve. Figure 2 shows these curves for the c@1, accuracy and OF measures. In the Figure we can see how there is a consistent decrease of the error rate of all measures when the proportion of ties increases (this corresponds to the increase in the fuzziness value). Figure 2 shows that the curves of accuracy and c@1 are quite similar (slightly better behavior of c@1) , which means that they have a similar stability and discrimination power. The results suggest that the three measures are quite stable, having c@1 and accuracy a low</context>
<context position="16783" citStr="Sakai, 2007" startWordPosition="2944" endWordPosition="2945">e bin. Looking at the swap-rates of all the difference performance bins, the performance difference required in order to conclude that a run is better than another for a given confidence value can be estimated. For example, if we want to know the required difference for concluding that system A is better than system B with a confidence of 95%, then we select the difference that represents the first bin where the swap-rate is lower or equal than 0.05. The sensitivity of the measure is the number of times among all the comparisons in the experiment where this performance difference is obtained (Sakai, 2007b). That is, the more comparisons accomplish the estimated performance difference, the more sensitive is the measure. The more sensitive the measure, the more useful it is for system discrimination. The swap method works as follows: let S denote a set of runs, let x and y denote a pair of runs from S. Let Q denote the entire evaluation collection. And let d denote a performance difference between two runs. Then, we first define 21 performance difference bins: the first bin represents performance differences between systems such that 0 ≤ d &lt; 0.01; the second bin represents differences such that</context>
<context position="19185" citStr="Sakai, 2007" startWordPosition="3379" endWordPosition="3380"> ((i)/(ii)); (iv) percentage of comparisons that accomplish the required difference (sensitivity) Given that Qi and Q′i must be disjoint, their size can only be up to half of the size of the original collection. Thus, we use the value c=250 for our experiment1. Table 2 shows the results obtained by applying the swap method to accuracy, c@1 and UF, with c = 250, swap-rate &lt; 5, and sensitivity given a confidence of 95% (Column (iv)). The range of values 1We use the same size for experiments in Section 4.2 for homogeneity reasons. are similar to the ones obtained for other measures according to (Sakai, 2007a). According to Column (i), a higher absolute difference is required for concluding that a system is better than another using UF. However, the relative difference is similar to the one required by c@1. Thus, similar percentage of comparisons using c@1 and UF accomplish the required difference (Column (iv)). These results show that their sensitivity values are similar, and higher than the value for accuracy. 4.4 Qualitative evaluation In addition to the theoretical study, we undertook a study to interpret the results obtained by real systems in a real scenario. The aim is to compare the resul</context>
</contexts>
<marker>Sakai, 2007</marker>
<rawString>Tetsuya Sakai. 2007a. On the Reliability of Factoid Question Answering Evaluation. ACM Trans. Asian Lang. Inf. Process., 6(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
</authors>
<title>On the reliability of information retrieval metrics based on graded relevance.</title>
<date>2007</date>
<journal>Inf. Process. Manage.,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="11374" citStr="Sakai, 2007" startWordPosition="1985" endWordPosition="1986"> c@1 When a new measure is proposed, it is important to study the reliability of the results obtained using that measure. For this purpose, we have chosen the method described by Buckley and Voorhees (2000) for assessing the stability and discrimination power, as well as the method described by Voorhees and Buckley (2002) for examining the sensitivity of our measure. These methods have been used for studying IR metrics (showing similar results with the methods based on statistics (Sakai, 2006)), as well as for evaluating the reliability of other QA measures different to the ones studied here (Sakai, 2007a; Voorhees, 2002; Voorhees, 2003). We have compared the results over c@1 with the ones obtained using both accuracy and the utility function (UF) defined in Formula (1). This comparison is useful to show how confident can a researcher be with the results obtained using each evaluation measure. In the following subsections we will first show the data used for our study. Then, the experiments about stability and sensitivity will be described. 4.1 Data sets We used the test collections and runs from the Question Answering track at the Cross Language Evaluation Forum 2009 (CLEF) (Pe˜nas et al., 2</context>
<context position="15109" citStr="Sakai (2007" startWordPosition="2651" endWordPosition="2652">n y happens when there are more cases where the value of x is better than the value of y. Then, the number of times y is better than x is considered as the number of times the test is misleading, while the number of times the values of x and y are equivalent is considered the number of ties. On the other hand, it is clear that larger fuzziness values decrease the error rate but also decrease the discrimination power of a measure. Since a fixed fuzziness value might imply different trade-offs for different metrics, we decided to vary the fuzziness value from 0.01 to 0.10 (following the work by Sakai (2007b)) and to draw for each measure a proportionof-ties / error-rate curve. Figure 2 shows these curves for the c@1, accuracy and OF measures. In the Figure we can see how there is a consistent decrease of the error rate of all measures when the proportion of ties increases (this corresponds to the increase in the fuzziness value). Figure 2 shows that the curves of accuracy and c@1 are quite similar (slightly better behavior of c@1) , which means that they have a similar stability and discrimination power. The results suggest that the three measures are quite stable, having c@1 and accuracy a low</context>
<context position="16783" citStr="Sakai, 2007" startWordPosition="2944" endWordPosition="2945">e bin. Looking at the swap-rates of all the difference performance bins, the performance difference required in order to conclude that a run is better than another for a given confidence value can be estimated. For example, if we want to know the required difference for concluding that system A is better than system B with a confidence of 95%, then we select the difference that represents the first bin where the swap-rate is lower or equal than 0.05. The sensitivity of the measure is the number of times among all the comparisons in the experiment where this performance difference is obtained (Sakai, 2007b). That is, the more comparisons accomplish the estimated performance difference, the more sensitive is the measure. The more sensitive the measure, the more useful it is for system discrimination. The swap method works as follows: let S denote a set of runs, let x and y denote a pair of runs from S. Let Q denote the entire evaluation collection. And let d denote a performance difference between two runs. Then, we first define 21 performance difference bins: the first bin represents performance differences between systems such that 0 ≤ d &lt; 0.01; the second bin represents differences such that</context>
<context position="19185" citStr="Sakai, 2007" startWordPosition="3379" endWordPosition="3380"> ((i)/(ii)); (iv) percentage of comparisons that accomplish the required difference (sensitivity) Given that Qi and Q′i must be disjoint, their size can only be up to half of the size of the original collection. Thus, we use the value c=250 for our experiment1. Table 2 shows the results obtained by applying the swap method to accuracy, c@1 and UF, with c = 250, swap-rate &lt; 5, and sensitivity given a confidence of 95% (Column (iv)). The range of values 1We use the same size for experiments in Section 4.2 for homogeneity reasons. are similar to the ones obtained for other measures according to (Sakai, 2007a). According to Column (i), a higher absolute difference is required for concluding that a system is better than another using UF. However, the relative difference is similar to the one required by c@1. Thus, similar percentage of comparisons using c@1 and UF accomplish the required difference (Column (iv)). These results show that their sensitivity values are similar, and higher than the value for accuracy. 4.4 Qualitative evaluation In addition to the theoretical study, we undertook a study to interpret the results obtained by real systems in a real scenario. The aim is to compare the resul</context>
</contexts>
<marker>Sakai, 2007</marker>
<rawString>Tetsuya Sakai. 2007b. On the reliability of information retrieval metrics based on graded relevance. Inf. Process. Manage., 43(2):531–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Chris Buckley</author>
</authors>
<title>The effect of Topic Set Size on Retrieval Experiment Error.</title>
<date>2002</date>
<booktitle>In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>316--323</pages>
<contexts>
<context position="11086" citStr="Voorhees and Buckley (2002)" startWordPosition="1937" endWordPosition="1940">/ (nac+ naw), the measure would be: P(C) = P(C n A) + P(C/,A) * P(,A) = = P(C n A) * P(,C/A) * P(,A) = (6) This measure is very easy to cheat. It is possible to obtain almost a perfect score just by answering incorrectly only one question and leaving unanswered the rest of the questions. 4 Evaluation of c@1 When a new measure is proposed, it is important to study the reliability of the results obtained using that measure. For this purpose, we have chosen the method described by Buckley and Voorhees (2000) for assessing the stability and discrimination power, as well as the method described by Voorhees and Buckley (2002) for examining the sensitivity of our measure. These methods have been used for studying IR metrics (showing similar results with the methods based on statistics (Sakai, 2006)), as well as for evaluating the reliability of other QA measures different to the ones studied here (Sakai, 2007a; Voorhees, 2002; Voorhees, 2003). We have compared the results over c@1 with the ones obtained using both accuracy and the utility function (UF) defined in Formula (1). This comparison is useful to show how confident can a researcher be with the results obtained using each evaluation measure. In the following</context>
<context position="16006" citStr="Voorhees and Buckley, 2002" startWordPosition="2807" endWordPosition="2810">s (this corresponds to the increase in the fuzziness value). Figure 2 shows that the curves of accuracy and c@1 are quite similar (slightly better behavior of c@1) , which means that they have a similar stability and discrimination power. The results suggest that the three measures are quite stable, having c@1 and accuracy a lower error rate than OF when the proportion of ties grows. These curves are similar to the ones obtained for Figure 2: Error-rate / Proportion of ties curves for accuracy, c@1 and OF with c = 250 other QA evaluation measures (Sakai, 2007a). 4.3 Sensitivity The swap-rate (Voorhees and Buckley, 2002) represents the chance of obtaining a discrepancy between two question sets (of the same size) as to whether a system is better than another given a certain difference bin. Looking at the swap-rates of all the difference performance bins, the performance difference required in order to conclude that a run is better than another for a given confidence value can be estimated. For example, if we want to know the required difference for concluding that system A is better than system B with a confidence of 95%, then we select the difference that represents the first bin where the swap-rate is lower</context>
</contexts>
<marker>Voorhees, Buckley, 2002</marker>
<rawString>Ellen M. Voorhees and Chris Buckley. 2002. The effect of Topic Set Size on Retrieval Experiment Error. In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 316–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Dawn M Tice</author>
</authors>
<title>The TREC8 Question Answering Track Evaluation.</title>
<date>1999</date>
<booktitle>In Text Retrieval Conference TREC-8,</booktitle>
<pages>83--105</pages>
<contexts>
<context position="21543" citStr="Voorhees and Tice, 1999" startWordPosition="3768" endWordPosition="3771">2de compared to base092de for German. 5 Related Work The decision of leaving a query without response is related to the system ability to measure accurately its self-confidence about the correctness of their candidate answers. Although there have been one attempt to make the self-confidence score explicit and use it (Herrera et al., 2005), rankings are, usually, the implicit way to evaluate this self-confidence. Mean Reciprocal Rank (MRR) has traditionally been used to evaluate Question Answering systems when several answers per question were allowed and given in order (Fukumoto et al., 2002; Voorhees and Tice, 1999). However, as it occurs with Accuracy (proportion of questions correctly answered), the risk of giving a wrong answer is always preferred better than not responding. The QA track at TREC 2001 was the first evaluation campaign in which systems were allowed to leave a question unanswered (Voorhees, 2001). The main evaluation measure was MRR, but performance was also measured by means of the percentage of answered questions and the portion of them that were correctly answered. However, no combination of these two values into a unique measure was proposed. TREC 2002 discarded the idea of including</context>
</contexts>
<marker>Voorhees, Tice, 1999</marker>
<rawString>Ellen M. Voorhees and Dawn M. Tice. 1999. The TREC8 Question Answering Track Evaluation. In Text Retrieval Conference TREC-8, pages 83–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2001</date>
<booktitle>Proceedings of the Tenth Text REtrieval Conference (TREC 2001). NIST Special Publication</booktitle>
<pages>500--250</pages>
<editor>In E. M. voorhees, D. K. Harman, editors:</editor>
<contexts>
<context position="21846" citStr="Voorhees, 2001" startWordPosition="3821" endWordPosition="3822">se it (Herrera et al., 2005), rankings are, usually, the implicit way to evaluate this self-confidence. Mean Reciprocal Rank (MRR) has traditionally been used to evaluate Question Answering systems when several answers per question were allowed and given in order (Fukumoto et al., 2002; Voorhees and Tice, 1999). However, as it occurs with Accuracy (proportion of questions correctly answered), the risk of giving a wrong answer is always preferred better than not responding. The QA track at TREC 2001 was the first evaluation campaign in which systems were allowed to leave a question unanswered (Voorhees, 2001). The main evaluation measure was MRR, but performance was also measured by means of the percentage of answered questions and the portion of them that were correctly answered. However, no combination of these two values into a unique measure was proposed. TREC 2002 discarded the idea of including unanswered questions in the evaluation. Only one answer by question was allowed and all answers had to be ranked according to the system’s self-confidence in the correctness of the answer. Systems were evaluated by means of Confidence Weighted Score (CWS), rewarding those systems able to provide more </context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Ellen M. Voorhees. 2001. Overview of the TREC 2001 Question Answering Track. In E. M. voorhees, D. K. Harman, editors: Proceedings of the Tenth Text REtrieval Conference (TREC 2001). NIST Special Publication 500-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of TREC</title>
<date>2002</date>
<booktitle>Proceedings of the Eleventh Text REtrieval Conference (TREC 2002). NIST Publication</booktitle>
<pages>500--251</pages>
<editor>In E.M. Voorhees, L. P. Buckland, editors:</editor>
<contexts>
<context position="11391" citStr="Voorhees, 2002" startWordPosition="1987" endWordPosition="1988">w measure is proposed, it is important to study the reliability of the results obtained using that measure. For this purpose, we have chosen the method described by Buckley and Voorhees (2000) for assessing the stability and discrimination power, as well as the method described by Voorhees and Buckley (2002) for examining the sensitivity of our measure. These methods have been used for studying IR metrics (showing similar results with the methods based on statistics (Sakai, 2006)), as well as for evaluating the reliability of other QA measures different to the ones studied here (Sakai, 2007a; Voorhees, 2002; Voorhees, 2003). We have compared the results over c@1 with the ones obtained using both accuracy and the utility function (UF) defined in Formula (1). This comparison is useful to show how confident can a researcher be with the results obtained using each evaluation measure. In the following subsections we will first show the data used for our study. Then, the experiments about stability and sensitivity will be described. 4.1 Data sets We used the test collections and runs from the Question Answering track at the Cross Language Evaluation Forum 2009 (CLEF) (Pe˜nas et al., 2010). The collect</context>
<context position="22504" citStr="Voorhees, 2002" startWordPosition="3932" endWordPosition="3933">performance was also measured by means of the percentage of answered questions and the portion of them that were correctly answered. However, no combination of these two values into a unique measure was proposed. TREC 2002 discarded the idea of including unanswered questions in the evaluation. Only one answer by question was allowed and all answers had to be ranked according to the system’s self-confidence in the correctness of the answer. Systems were evaluated by means of Confidence Weighted Score (CWS), rewarding those systems able to provide more correct answers at the top of the ranking (Voorhees, 2002). The formulation of CWS is the following: C(i) (9) i Where n is the number of questions, and C(i) is the number of correct answers up to the position i in the ranking. Formally: Z C(i) = I(j) (10) j=1 where I(j) is a function that returns 1 if answer j is correct and 0 if it is not. The formulation of CWS is inspired by the Average Precision (AP) over the ranking for one question: 1 � AP = R r where R is the number of known relevant results for a topic, and r is a position in the ranking. Since only one answer per question is requested, R equals to n (the number of questions) in CWS. However,</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>Ellen M. Voorhees. 2002. Overview of TREC 2002 Question Answering Track. In E.M. Voorhees, L. P. Buckland, editors: Proceedings of the Eleventh Text REtrieval Conference (TREC 2002). NIST Publication 500-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="11408" citStr="Voorhees, 2003" startWordPosition="1989" endWordPosition="1990">posed, it is important to study the reliability of the results obtained using that measure. For this purpose, we have chosen the method described by Buckley and Voorhees (2000) for assessing the stability and discrimination power, as well as the method described by Voorhees and Buckley (2002) for examining the sensitivity of our measure. These methods have been used for studying IR metrics (showing similar results with the methods based on statistics (Sakai, 2006)), as well as for evaluating the reliability of other QA measures different to the ones studied here (Sakai, 2007a; Voorhees, 2002; Voorhees, 2003). We have compared the results over c@1 with the ones obtained using both accuracy and the utility function (UF) defined in Formula (1). This comparison is useful to show how confident can a researcher be with the results obtained using each evaluation measure. In the following subsections we will first show the data used for our study. Then, the experiments about stability and sensitivity will be described. 4.1 Data sets We used the test collections and runs from the Question Answering track at the Cross Language Evaluation Forum 2009 (CLEF) (Pe˜nas et al., 2010). The collection has a set of </context>
<context position="23893" citStr="Voorhees, 2003" startWordPosition="4190" endWordPosition="4191">to the measure regardless of whether there is a relevant result or not in that position. Therefore, CWS gives much more value to some questions over others: questions whose answers are at the top of the ranking are giving almost the complete value to CWS, whereas those questions whose answers are at the bottom of the ranking are almost not counting in the evaluation. Although CWS was aimed at promoting the development of better self-confidence scores, it was discussed as a measure for evaluating QA systems performance. CWS was discarded in the following campaigns of TREC in favor of accuracy (Voorhees, 2003). Subsequently, accuracy was adopted by the QA track at the Cross-Language Evaluation Forum from the beginning (Magnini et al., 2005). There was an attempt to consider explicitly systems confidence self-score (Herrera et al., 2005): the use of the Pearson’s correlation coefficient and the proposal of measures K and K1 (see Formula 12). These measures are based in a utility function that returns -1 if the answer is incorrect and 1 if it is correct. This positive or negative value is weighted with the normalized confidence self-score given by the system to each answer. K is a variation of K1 for</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Ellen M. Voorhees. 2003. Overview of the TREC 2003 Question Answering Track. In Proceedings of the Twelfth Text REtrieval Conference (TREC 2003).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>