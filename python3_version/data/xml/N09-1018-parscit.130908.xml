<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.966592">
Jointly Identifying Predicates, Arguments and Senses using Markov Logic
</title>
<author confidence="0.999289">
Ivan Meza-Ruiz* Sebastian Riedeltt
</author>
<affiliation confidence="0.979515">
*School of Informatics, University of Edinburgh, UK
tDepartment of Computer Science, University of Tokyo, Japan
tDatabase Center for Life Science, Research Organization of Information and System, Japan
</affiliation>
<email confidence="0.997028">
*I.V.Meza-Ruiz@sms.ed.ac.uk tsebastian.riedel@gmail.com
</email>
<sectionHeader confidence="0.995832" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995831578947369">
In this paper we present a Markov Logic Net-
work for Semantic Role Labelling that jointly
performs predicate identification, frame dis-
ambiguation, argument identification and ar-
gument classification for all predicates in a
sentence. Empirically we find that our ap-
proach is competitive: our best model would
appear on par with the best entry in the
CoNLL 2008 shared task open track, and at
the 4th place of the closed track—right be-
hind the systems that use significantly better
parsers to generate their input features. More-
over, we observe that by fully capturing the
complete SRL pipeline in a single probabilis-
tic model we can achieve significant improve-
ments over more isolated systems, in particu-
lar for out-of-domain data. Finally, we show
that despite the joint approach, our system is
still efficient.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999783431818182">
Semantic Role Labelling (SRL, M´arquez et al.,
2008) is generally understood as the task of iden-
tifying and classifying the semantic arguments and
modifiers of the predicates mentioned in a sentence.
For example, in the case of the following sentence:
we are to find out that for the predicate token “plays”
with sense “play a role” (play.02) the phrase headed
by the token “Haag” is referring to the player (A0)
of the play event, and the phrase headed by the token
“Elianti” is referring to the role (A1) being played.
SRL is considered as a key task for applications that
require to answer “Who”, “What”, “Where”, etc.
questions, such as Information Extraction, Question
Answering and Summarization.
Any real-world SRL system needs to make sev-
eral decisions, either explicitly or implicitly: which
are the predicate tokens of a sentence (predicate
identification), which are the tokens that have se-
mantic roles with respect to these predicates (argu-
ment identification), which are the roles these to-
kens play (argument classification), and which is the
sense of the predicate (sense disambiguation).
In this paper we use Markov Logic (ML), a Statis-
tical Relational Learning framework that combines
First Order Logic and Markov Networks, to develop
a joint probabilistic model over all decisions men-
tioned above. The following paragraphs will moti-
vate this choice.
First, it allows us to readily capture global cor-
relations between decisions, such as the constraint
that a predicate can only have one agent. This type
of correlations has been successfully exploited in
several previous SRL approaches (Toutanova et al.,
2005; Punyakanok et al., 2005).
Second, we can use the joint model to evaluate
the benefit of incorporating decisions into the joint
model that either have not received much attention
within the SRL community (predicate identification
and sense disambiguation), or been largely made in
isolation (argument identification and classification
for all predicates of a sentence).
Third, our ML model is essentially a template that
describes a class of Markov Networks. Algorithms
can perform inference in terms of this template with-
</bodyText>
<page confidence="0.986525">
155
</page>
<note confidence="0.8907505">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 155–163,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99949441025641">
out ever having to fully instantiate the complete
Markov Network (Riedel, 2008; Singla and Domin-
gos, 2008). This can dramatically improve the effi-
ciency of an SRL system when compared to a propo-
sitional approach such as Integer Linear Program-
ming (ILP).
Finally, when it comes to actually building an
SRL system with ML there are “only” four things
to do: preparing input data files, converting out-
put data files, and triggering learning and inference.
The remaining work can be done by an off-the-
shelf Markov Logic interpreter. This is to be con-
trasted with pipeline systems where several compo-
nents need to be trained and connected, or Integer
Linear Programming approaches for which we need
to write additional wrapper code to generate ILPs.
Empirically we find that our system is
competitive—our best model would appear on
par with the best entry in the CoNLL 2008 shared
task open track, and at the 4th place of the closed
track—right behind systems that use significantly
better parsers1 to generate their input features.
We also observe that by integrating frame disam-
biguation into the joint SRL model, and by extract-
ing all arguments for all predicates in a sentence
simultaneously, significant improvements compared
to more isolated systems can be achieved. These
improvements are particularly large in the case of
out-of-domain data, suggesting that a joint approach
helps to increase the robustness of SRL. Finally, we
show that despite the joint approach, our system is
still efficient.
Our paper is organised as follows: we first intro-
duce ML (section 2), then we present our model in
terms of ML (section 3) and illustrate how to per-
form learning and inference with it (section 4). How
this model will be evaluated is explained in section 5
with the corresponding evaluation presented in sec-
tion 6. We conclude in section 7.
</bodyText>
<sectionHeader confidence="0.92796" genericHeader="method">
2 Markov Logic
</sectionHeader>
<footnote confidence="0.679270142857143">
Markov Logic (ML, Richardson and Domingos,
2005) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First Or-
der Logic to allow formulae that can be violated with
1Our unlabelled accuracy for syntactic dependencies is at
least 3% points under theirs.
</footnote>
<bodyText confidence="0.996502611111111">
some penalty. From an alternative point of view, it is
an expressive template language that uses First Or-
der Logic formulae to instantiate Markov Networks
of repetitive structure.
Let us describe ML by considering the predicate
identification task. In ML we can model this task by
first introducing a set of logical predicates2 such as
isPredicate(Token) or word(Token,Word). Then we
specify a set of weighted first order formulae that
define a distribution over sets of ground atoms of
these predicates (or so-called possible worlds).
Ideally, the distribution we define with these
weighted formulae assigns high probability to possi-
ble worlds where SRL predicates are correctly iden-
tified and a low probability to worlds where this is
not the case. For example, a suitable set of weighted
formulae would assign a high probability to the
world3
</bodyText>
<construct confidence="0.5361806">
{word (1, Haag) , word(2, plays),
word(3, Elianti), isPredicate(2)}
and a low one to
{word (1, Haag) , word(2, plays),
word(3, Elianti), isPredicate(3)}
</construct>
<bodyText confidence="0.9993142">
In Markov Logic a set of weighted formulae is called
a Markov Logic Network (MLN). Formally speak-
ing, an MLN M is a set of pairs (0, w) where 0 is a
first order formula and w a real weight. M assigns
the probability
</bodyText>
<equation confidence="0.997787">
p(y) = Z exp w X fOc (y) (1)
((O,w)∈M c∈Cφ
</equation>
<bodyText confidence="0.997564818181818">
to the possible world y. Here CO is the set of all
possible bindings of the free variables in 0 with the
constants of our domain. fOc is a feature function
that returns 1 if in the possible world y the ground
formula we get by replacing the free variables in 0
by the constants in c is true and 0 otherwise. Z
is a normalisation constant. Note that this distri-
bution corresponds to a Markov Network (the so-
called Ground Markov Network) where nodes repre-
sent ground atoms and factors represent ground for-
mulae.
</bodyText>
<footnote confidence="0.99867225">
2In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
3“Haag plays Elianti” is a segment of a sentence in the train-
ing corpus.
</footnote>
<page confidence="0.994416">
156
</page>
<bodyText confidence="0.729648">
For example, if M contains the formula 0
</bodyText>
<equation confidence="0.871873">
word (x, take) ⇒ isPredicate (x)
</equation>
<bodyText confidence="0.956504333333333">
then its corresponding log-linear model has, among
others, a feature fφt1 for which x in 0 has been re-
placed by the constant t1 and that returns 1 if
</bodyText>
<equation confidence="0.9555425">
word (1, take) ⇒ isPredicate (1)
is true in y and 0 otherwise.
</equation>
<bodyText confidence="0.99982875">
We will refer predicates such as word as observed
because they are known in advance. In contrast, is-
Predicate is hidden because we need to infer it at test
time.
</bodyText>
<sectionHeader confidence="0.996424" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.99945184375">
Conceptually we divide our SRL system into three
stages: one stage that identifies the predicates of
a sentence, one stage that identifies and classifies
the arguments of these predicates, and a final stage
that predicts the sense of each predicate. We should
stress that this architecture is intended to illustrate
a typical SRL system, and to describe the pipeline-
based approach we will compare our models to.
However, it does not correspond to the way in-
ference is performed in our proposed model—we
jointly infer all decisions described above.
Note that while the proposed division into con-
ceptual stages seems somewhat intuitive, it is by no
means uncontroversial. In fact, for the CoNLL 2008
shared task slightly more than one half of the par-
ticipants performed sense disambiguation before ar-
gument identification and classification; most other
participants framed the problem in the reverse or-
der.4
We define five hidden predicates for the three
stages of the task. Figure 1 illustrates these pred-
icates and the stage they belong to. For predicate
identification, we use the predicate isPredicate. is-
Predicate(p) indicates that the word in the position
p is an SRL predicate. For argument identifica-
tion and classification, we use the predicates isAr-
gument, hasRole and role. The atom isArgument(a)
signals that the word in the position a is a SRL ar-
gument of some (unspecified) SRL predicate while
hasRole(p,a) indicates that the token at position a is
4However, for almost all pipeline based systems, predicate
identification was the first stage of the role labelling process.
</bodyText>
<figureCaption confidence="0.999069">
Figure 1: MLN hidden predicates divided in stages
</figureCaption>
<bodyText confidence="0.995613230769231">
an argument of the predicate in position p. The pred-
icate role(p,a,r) corresponds to the decision that the
argument at position a has the role r with respect to
the predicate in position p. Finally, for sense disam-
biguation we define the predicate sense(p,e) which
signals that the predicate in position p has the sense
e.
Before we continue to describe the formulae of
our Markov Logic Network we would like to high-
light the introduction of the isArgument predicate
mentioned above. This predicate corresponds to a
decision that is usually made implicitly: a token is
an argument if there exists a predicate for which it
plays a semantic role. Here we model this decision
explicitly, assuming that there exist cases where a
token clearly has to be an argument of some pred-
icate, regardless of which predicate in the sentence
this might be. It is this assumption that requires us to
infer the arguments for all predicates of a sentence
at once—otherwise we cannot make sure that for a
marked argument there exists at least one predicate
for which the argument plays a semantic role.
In addition to the hidden predicates, we define
observable predicates to represent the information
available in the corpus. Table 1 presents these pred-
icates.
</bodyText>
<subsectionHeader confidence="0.99909">
3.1 Local formulae
</subsectionHeader>
<bodyText confidence="0.99765725">
A formula is local if its groundings relate any num-
ber of observed ground atoms to exactly one hidden
ground atom. For example, two groundings of the
local formula
</bodyText>
<equation confidence="0.726042">
lemma(p, +l1)nlemma(a, +l2) ⇒ hasRole(p, a)
</equation>
<bodyText confidence="0.9306125">
can be seen in the Factor Graph of Figure 2. Both
connect a single hidden hasRole ground atom with
</bodyText>
<figure confidence="0.99809">
Sense
Disambiguation
Argument
Identification &amp;
clasification
Predicate
Identification
Pipeline direction
isArgument
isPredicate
hasRole
sense
role
Bottom-up Top-Down
</figure>
<page confidence="0.955648">
157
</page>
<bodyText confidence="0.9990415">
word(i,w) Token i has word w
lemma(i,l) Token i has lemma l
ppos(i,p) Token i has POS tag p
cpos(i,p) Token i has coarse POS tag p
voice(i,v) Token i is verb and has voice v
(Active/Passive).
subcat(i,f) Token i has subcategorization
frame f
dep(i,j,d) Token h is head of token m and
has dependency label d
palmer(i,j) Token j can be semantic argu-
ment for token i according to
high recall heuristic*
depPath(i,j,p) Dependency path between to-
kens i and j is p*
depFrame(i,j,f) f is a syntactic (dependency)
frame in which tokens i and j
are designated as “pivots”*
</bodyText>
<tableCaption confidence="0.988244666666667">
Table 1: Observable predicates; predicates marked with
* are dependency parsing-based versions for features of
Xue and Palmer (2004).
</tableCaption>
<bodyText confidence="0.99994875">
two observed lemma ground atoms. The + notation
indicates that the MLN contains one instance of the
rule, with a separate weight, for each assignment of
the variables with a plus sign (?).
The local formulae for isPredicate, isArgument
and sense aim to capture the relation of the tokens
with their lexical and syntactic surroundings. This
includes formulae such as
</bodyText>
<equation confidence="0.872222">
subcat(p, +f) ⇒ isPredicate(p)
</equation>
<bodyText confidence="0.9998916">
which implies that a certain token is a predicate
with a weight that depends on the subcategorization
frame of the token. Further local formulae are con-
structed using those observed predicates in table 1
that relate single tokens and their properties.
The local formulae for role and hasRole focus on
properties of the predicate and argument token—the
formula illustrated in figure 2 is an example of this—
and on the relation between the two tokens. An ex-
ample of the latter type is the formula
</bodyText>
<equation confidence="0.427597">
depPath(p, a, +d) ⇒ role(p, a, +r)
</equation>
<bodyText confidence="0.9989042">
which implies that token a plays the semantic role r
with respect to token p, and for which the weight de-
pends on the syntactic (dependency) path d between
p and a and on the actual role to assign. Again,
further formulae are constructed using the observed
</bodyText>
<figureCaption confidence="0.9719376">
Figure 2: Factor graph for the first local formula in sec-
tion 3.1. Here round nodes represent variables (corre-
sponding to the states of ground atoms) and the rectan-
gular nodes represent the factor and their parameters at-
tached to the ground formulae.
</figureCaption>
<bodyText confidence="0.998820714285714">
predicates in table 1; however, this time we consider
both predicates that relate tokens to their individual
properties and predicates that describe the relation
between tokens.
Unfortunately, the complete set of local formulae
is too large to be exhaustively described in this pa-
per. Its size results from the fact that we also con-
sider conjunctions of several atoms as conditions,
and lexical windows around tokens. Hence, instead
of describing all local formulae we refer the reader
to our MLN model files.5 They can be used both as
a reference and as input to our Markov Logic En-
gine,6 and thus allow the reader to easily reproduce
our results.
</bodyText>
<subsectionHeader confidence="0.997174">
3.2 Global formulae
</subsectionHeader>
<bodyText confidence="0.94678125">
Global formulae relate several hidden ground atoms.
We use this type of formula for two purposes: to
ensure consistency between the predicates of all
SRL stages, and to capture some of our background
knowledge about SRL. We will refer to formulae
that serve the first purpose as structural constraints.
For example, a structural constraint is given by the
(deterministic) formula
</bodyText>
<equation confidence="0.715544">
role(p, a, r) ⇒ hasRole(p, a)
</equation>
<bodyText confidence="0.9996505">
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a). Note that this formula by itself models
the traditional “bottom-up” argument identification
and classification pipeline (Xue and Palmer, 2004):
</bodyText>
<footnote confidence="0.999962">
5http://code.google.com/p/thebeast/
source/browse/#svn/mlns/naacl-hlt
6http://code.google.com/p/thebeast
</footnote>
<page confidence="0.99226">
158
</page>
<bodyText confidence="0.999856">
it is possible to not assign a role r to an predicate-
argument pair (p, a) proposed by the identification
stage; however, it is impossible to assign a role r
to token pairs (p, a) that have not been proposed as
potential arguments.
An example of another class of structural con-
straints is
</bodyText>
<equation confidence="0.465261">
hasRole(p, a) ⇒ ∃r.role(p, a, r)
</equation>
<bodyText confidence="0.998895">
which, by itself, models an inverted or “top-down”
pipeline. In this architecture the argument classifi-
cation stage can assign roles to tokens that have not
been proposed by the argument identification stage.
However, it must assign a label to any token pair the
previous stage proposes.
For the SRL predicates that perform a labelling
task (role and sense) we also need a structural con-
straint which ensures that not more than one label is
assigned. For instance,
</bodyText>
<equation confidence="0.828523">
(role(p, a, r1) ∧ r1 =6 r2 ⇒ ¬role(p, a, r2))
</equation>
<bodyText confidence="0.999203875">
forbids two different semantic roles for a pair of
words.
There are three global formulae that capture our
linguistic background knowledge. The first one is
a deterministic constraint that had been frequently
applied in the SRL literature. It forbids cases where
distinct arguments of a predicate have the same role
unless the role describes a modifier:
</bodyText>
<equation confidence="0.80518725">
role (p, a1, r) ∧ ¬mod (r) ∧ a1 =6 a2 ⇒
¬role (p, a2, r)
The second “linguistic” global formula is
role(p, a, +r) ∧ lemma(p, +l) ⇒ sense(p, +s)
</equation>
<bodyText confidence="0.998546">
which implies that when a predicate p with lemma l
has an argument a with role r it has to have the sense
s. Here the weight depends on the combination of
role r, lemma l and sense s.
The third and final “linguistic” global formula is
</bodyText>
<equation confidence="0.7255305">
lemma(p, +l) ∧ ppos(a, +p)
∧hasRole(p, a) ⇒ sense(p, +f)
</equation>
<bodyText confidence="0.997368181818182">
It implies that if a predicate p has the lemma l and an
argument a with POS tag p it has to have the sense
s. This time the weight depends on the combination
of POS tag p, lemma l and sense s.
Note that the final two formulae evaluate the se-
mantic frame of a predicate and become local for-
mulae in a pipeline system that performs sense dis-
ambiguation after argument identification and clas-
sification.
Table 2 summarises the global formulae we use in
this work.
</bodyText>
<sectionHeader confidence="0.995935" genericHeader="method">
4 Inference and Learning
</sectionHeader>
<bodyText confidence="0.999978378378378">
Assuming that we have an MLN, a set of weights
and a given sentence then we need to predict the
choice of predicates, frame types, arguments and
role labels with maximal a posteriori probabil-
ity (MAP). To this end we apply a method that
is both exact and efficient: Cutting Plane Infer-
ence (CPI, Riedel, 2008) with Integer Linear Pro-
gramming (ILP) as base solver.
Instead of fully instantiating the Markov Network
that a Markov Logic Network describes, CPI begins
with a subset of factors/edges—in our case we use
the factors that correspond to the local formulae of
our model—and solves the MAP problem for this
subset using the base solver. It then inspects the
solution for ground formulae/features that are not
yet included but could, if added, lead to a different
solution—this process is usually referred to as sep-
aration. The ground formulae that we have found
are added and the network is solved again. This pro-
cess is repeated until the network does not change
anymore.
This type of algorithm could also be realised for
an ILP formulation of SRL. However, it would re-
quire us to write a dedicated separation routine for
each type of constraint we want to add. In Markov
Logic, on the other hand, separation can be gener-
ically implemented as the search for variable bind-
ings that render a weighted first order formulae true
(if its weight is negative) or false (if its weight is
positive). In practise this means that we can try new
global formulae/constraints without any additional
implementation overhead.
We learn the weights associated with each MLN
using 1-best MIRA (Crammer and Singer, 2003)
Online Learning method. As MAP inference
method that is applied in the inner loop of the on-
line learner we apply CPI, again with ILP as base
</bodyText>
<page confidence="0.996824">
159
</page>
<table confidence="0.997087923076923">
Bottom-up sense(p, s) ⇒ isPredicate(p)
hasRole(p, a) ⇒ isPredicate(p)
hasRole(p, a) ⇒ isArgument(a)
role(p, a, r) ⇒ hasLabel(p, a)
Top-Down isPredicate(p) ⇒ ∃s.sense(p, s)
isPredicate(p) ⇒ ∃a.hasRole(p, a)
isArgument(a) ⇒ ∃p.hasRole(p, a)
hasLabel(p, a) ⇒ ∃r.role(p, a, r)
Unique Labels role(p, a, r1) ∧ r1 =6 r2 ⇒ ¬role(p, a, r2)
sense(p, s1) ∧ s1 =6 s2 ⇒ ¬sense(p, r2)
Linguistic role (p, a1, r) ∧ ¬mod (r) ∧ a1 =6 a2 ⇒ ¬role (p, a2, r)
lemma(p, +l) ∧ ppos(a, +p) ∧ hasRole(p, a) ⇒ sense(p, +f)
lemma(p, +l) ∧ role(p, a, +r) ⇒ sense(p, +f)
</table>
<tableCaption confidence="0.997181">
Table 2: Global formulae for ML model
</tableCaption>
<bodyText confidence="0.647563">
solver.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.966208896551724">
For training and testing our SRL systems we used a
version of the CoNLL 2008 shared task (Surdeanu
et al., 2008) dataset that only mentions verbal predi-
cates, disregarding the nominal predicates available
in the original corpus.7 While the original (open
track) corpus came with MALT (Nivre et al., 2007)
dependencies, we observed slightly better results
when using the dependency parses generated with
a Charniak parser (Charniak, 2000). Hence we used
the latter for all our experiments.
To assess the performance of our model, and it to
evaluate the possible gains to be made from consid-
ering a joint model of the complete SRL pipeline,
we set up several systems. The full system uses a
Markov Logic Network with all local and global for-
mulae described in section 3. For the bottom-up sys-
tem we removed the structural top-down constraints
from the complete model—previous work Riedel
and Meza-Ruiz (2008) has shown that this can lead
to improved performance. The bottom-up (-arg) sys-
tem is equivalent to the bottom-up system, but it
does not include any formulae that mention the hid-
den isArgument predicate.
For the systems presented so far we perform joint
inference and learning. The pipeline system dif-
fers in this regard. For this system we train a sep-
arate model for each stage in the pipeline of figure
1. The predicate identification stage identifies the
predicates (using all local isPredicate formulae) of
</bodyText>
<footnote confidence="0.960772">
7The reason for this choice where license problems.
</footnote>
<bodyText confidence="0.998743555555556">
a sentence. The next stage predicts arguments and
their roles for the identified predicates. Here we in-
clude all local and global formulae that involve only
the predicates of this stage. In the last stage we pre-
dict the sense of each identified predicate using all
formulae that involve the sense, without the struc-
tural constraints that connect the sense predicate to
the previous stages of the pipeline (these constraints
are enforced by architecture).
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.989061761904762">
Table 3 shows the results of our systems for the
CoNLL 2008 development set and the WSJ and
brown test sets. The scores are calculated using the
semantic evaluation metric of the CoNLL-08 shared
task (Surdeanu et al., 2008). This metric measures
the precision, recall and F1 score of the recovered
semantic dependencies. A semantic dependency is
created for each predicate and its arguments, the
label of such dependency is the role of the argu-
ment. Additionally, there is a semantic dependency
for each predicate and a ROOT argument which has
the sense of the predicate as label.
To put these results into context, let us compare
them to those of the participants of the CoNLL 2008
shared task (see the last three rows of table 3).8 Our
best model, Bottom-up, would reach the highest F1
WSJ score, and second highest Brown score, for
the open track. Here the best-performing participant
was Vickrey and Koller (2008).
Table 3 also shows the results of the best (Jo-
hansson and Nugues, 2008) and fourth best sys-
</bodyText>
<footnote confidence="0.966126">
8Results of other systems were extracted from Table 16 of
the shared task overview paper (Surdeanu et al., 2008).
</footnote>
<page confidence="0.995862">
160
</page>
<bodyText confidence="0.99972">
tem (Zhao and Kit, 2008) of the closed track. We
note that we do significantly worse than Johansson
and Nugues (2008), and roughly equivalent to Zhao
and Kit (2008); this places us on the fourth rank of
19 participants. However, note that all three sys-
tems above us, as well as Zhao and Kit (2008), use
parsers with at least about 90% (unlabelled) accu-
racy on the WSJ test set (Johansson’s parser has
about 92% unlabelled accuracy).9 By contrast, with
about 87% unlabelled accuracy our parses are sig-
nificantly worse.
Finally, akin to Riedel and Meza-Ruiz (2008) we
observe that the bottom-up joint model performs
better than the full joint model.
</bodyText>
<table confidence="0.99979275">
System Devel WSJ Brown
Full 76.93 79.09 67.64
Bottom-up 77.96 80.16 68.02
Bottom-up (-arg) 77.57 79.37 66.70
Pipeline 75.69 78.19 64.66
Vickrey N/A 79.75 69.57
Johansson N/A 86.37 71.87
Zhao N/A 79.40 66.38
</table>
<tableCaption confidence="0.99160175">
Table 3: Semantic Fl scores for our systems and three
CoNLL 2008 shared task participants. The Bottom-up
results are statistically significantly different to all others
(i.e., p ≤ 0.05 according to the sign test).
</tableCaption>
<subsectionHeader confidence="0.944651">
6.1 Joint Model vs. Pipeline
</subsectionHeader>
<bodyText confidence="0.998567125">
Table 3 suggests that by including sense disam-
biguation into the joint model (as is the case for all
systems but the pipeline) significant improvements
can be gained. Where do these improvements come
from? We tried to answer this question by taking a
closer look at how accurately the pipeline predicts
the isPredicate, isArgument, hasRole, role and
sense relations, and how this compares to the result
of the joint full model.
Table 4 shows that the joint model mainly does
better when it comes to predicting the right predi-
cate senses. This is particularly true for the case of
the Brown corpus—here we gain about 10% points.
These results suggest that a more joint approach may
be particularly useful in order to increase the robust-
ness of an SRL system in out-of-domain scenarios.10
</bodyText>
<footnote confidence="0.939933">
9Since our parses use a different label set we could not com-
</footnote>
<table confidence="0.976223428571429">
WSJ Fu. Brown Fu.
Pipe. Pipe.
isPredicate 96.6 96.5 92.2 92.5
isArgument 90.3 90.6 85.9 86.9
hasRole 88.0 87.9 83.6 83.8
role 75.4 75.5 64.2 64.6
sense 85.5 88.5 67.3 77.1
</table>
<tableCaption confidence="0.9714905">
Table 4: Fl scores for M predicates; Pipe. refers to the
Pipeline system, Fu. to the full system.
</tableCaption>
<subsectionHeader confidence="0.999685">
6.2 Modelling if a Token is an Argument
</subsectionHeader>
<bodyText confidence="0.996715945945946">
In table 3 we also observe that improvements can be
made if we explicitly model the decision whether a
token is a semantic argument of some predicate or
not. As we mentioned in section 3, this aspect of our
model requires us to jointly perform inference for
all predicates of a sentence, and hence our results
justify the per-sentence SRL approach proposed in
this paper.
In order to analyse where these improvements
come from, we again list our results on a per-SRL-
predicate basis. Table 5 shows that by including the
isArgument predicate and the corresponding for-
mulae we gain around 0.6% and 1.0% points across
the board for WSJ and Brown, respectively.11 As
shown in table 3, these improvements result in about
1.0% improvements for both WSJ and Brown in
terms of the CoNLL 2008 metric. Hence, an ex-
plicit model of the “is an argument” decision helps
the SRL at all levels.
How the isArgument helps to improve the over-
all role labelling score can be illustrated with the
example in figure 3. Here the model without a
hidden isArgument predicate fails to attach the
preposition “on” to the predicate “start.01” (here 01
refers to the sense of the predicate). Apparently
the model has not enough confidence to assign the
preposition to either “start.01” or “get.03”, so it just
drops the argument altogether. However, because
the isArgument model knows that most preposi-
tions have to be modifying some predicate, pres-
pare labelled accuracy.
10The differences between results of the full and joint model
are statistically significant with the exception of the results for
the isPredicate predicate for the WSJ test set.
11The differences between results of the w/ and w/o model
are statistically significant with the exception of the results for
the sense predicate for the Brown test set.
</bodyText>
<page confidence="0.99634">
161
</page>
<figureCaption confidence="0.9531904">
Figure 3: Segment of the CoNLL 2008 development set
for which the bottom-up model w/o isArgument predi-
cate fails to attach the preposition “on” as an “AM-LOC”
for “started”. The joint bottom-up model attaches the
preposition correctly.
</figureCaption>
<bodyText confidence="0.9596375">
sure is created that forces a decision between the
two predicates. And because for the role model
“start.01” looks like a better fit than “get.03”, the
correct attachment is found.
</bodyText>
<table confidence="0.998887333333333">
WSJ w/ Brown w/
w/o w/o
isPredicate 96.3 96.5 91.4 92.5
hasRole 87.1 87.7 82.5 83.6
role 76.9 77.5 65.2 66.2
sense 88.3 89.0 76.1 77.5
</table>
<tableCaption confidence="0.989278">
Table 5: Fl scores for ML predicates; w/o refers to
a Bottom-up system without isArgument predicate, w/
refers to a Bottom-up system with isArgument predicate.
</tableCaption>
<subsectionHeader confidence="0.958295">
6.3 Efficiency
</subsectionHeader>
<bodyText confidence="0.999664266666667">
In the previous sections we have shown that our joint
model indeed does better than an equivalent pipeline
system. However, usually most joint approaches
come at a price: efficiency. Interestingly, in our case
we observe the opposite: our joint model is actually
faster than the pipeline. This can be seen in table 6,
where we list the time it took for several different
system to process the WSJ and Brown test corpus,
respectively. When we compare the times for the
bottom-up model to those of the pipeline, we note
that the joint model is twice as fast. While the indi-
vidual stages within the pipeline may be faster than
the joint system (even when we sum up inference
times), extracting results from one system and feed-
ing them into another creates overhead which offsets
this potential reduction.
Table 6 also lists the run-time of a bottom-up
system that solves the inference problem by fully
grounding the Markov Network that the Markov
Logic (ML) model describes, mapping this network
to an Integer Linear Program, and finding the most
likely assignment using an ILP solver. This sys-
tem (Bottom-up (-CPI)) is four times slower than the
equivalent system that uses Cutting Plane Inference
(Bottom-up). This suggests that if we were to imple-
ment the same joint model using ILP instead of ML,
our system would either be significantly slower, or
we would need to implement a Cutting Plane algo-
rithm for the corresponding ILP formulation—when
we use ML this algorithm comes “for free”.
</bodyText>
<table confidence="0.998130333333333">
System WSJ Brown
Full 9.2m 1.5m
Full (-CPI) 38.4m 7.47m
Bottom-up 9.5m 1.6m
Bottom-up (-CPI) 38.8m 6.9m
Pipeline 18.9m 2.9m
</table>
<tableCaption confidence="0.9582402">
Table 6: Testing times for full model and bottom-up when
CPI algorithm is not used. The WSJ test set contains 2414
sentences, the Brown test set 426. Our best systems thus
takes on average 230ms per WSJ sentence (on a 2.4Ghz
system).
</tableCaption>
<sectionHeader confidence="0.997163" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999779777777778">
In this paper we have presented a Markov Logic Net-
work that jointly models all predicate identification,
argument identification and classification and sense
disambiguation decisions for a sentence. We have
shown that this approach is competitive, in particular
if we consider that our input parses are significantly
worse than those of the top CoNLL 2008 systems.
We demonstrated the benefit of jointly predicting
senses and semantic arguments when compared to a
pipeline system that first picks arguments and then
senses. We also showed that by modelling whether
a token is an argument of some predicate and jointly
picking arguments for all predicates of a sentence,
further improvements can be achieved.
Finally, we demonstrated that our system is effi-
cient, despite following a global approach. This ef-
ficiency was also shown to stem from the first order
inference method our Markov Logic engine applies.
</bodyText>
<sectionHeader confidence="0.997622" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9992735">
The authors are grateful to Mihai Surdeanu for pro-
viding the version of the corpus used in this work.
</bodyText>
<page confidence="0.996981">
162
</page>
<sectionHeader confidence="0.995868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999311539682539">
Eugene Charniak. A maximum-entropy-inspired
parser. In Proceedings of NAACL-2000, 2000.
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–
991, 2003.
Richard Johansson and Pierre Nugues. Dependency-
based semantic role labeling of propbank. In Pro-
ceedings of EMNLP-2008., 2008.
Lluis M´arquez, Xavier Carreras, Ken Litkowski, and
Suzanne Stevenson. Semantic role labeling. Com-
putational Linguistics, 34(2), 2008. Introduction
to the Special Issue on Semantic Role Labeling.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kuebler, S. Marinov, and E. Marsi. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language
Engineering, 13(02):95–135, 2007.
V. Punyakanok, D. Roth, and W. Yih. General-
ized inference with multiple semantic role label-
ing systems. In Ido Dagan and Dan Gildea, ed-
itors, CoNLL ’05: Proceedings of the Annual
Conference on Computational Natural Language
Learning, pages 181–184, 2005.
Matthew Richardson and Pedro Domingos. Markov
logic networks. Technical report, University of
Washington, 2005.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
UAI ’08: Proceedings of the Annual Conference
on Uncertainty in AI, 2008.
Sebastian Riedel and Ivan Meza-Ruiz. Collective
semantic role labelling with markov logic. In
Conference on Computational Natural Language
Learning, 2008.
P. Singla and P. Domingos. Lifted First-Order Belief
Propagation. Association for the Advancement of
Artificial Intelligence (AAAI), 2008.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the
12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), 2008.
Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. Joint learning improves seman-
tic role labeling. In ACL ’05: Proceedings of the
43rd Annual Meeting on Association for Compu-
tational Linguistics, Morristown, NJ, USA, 2005.
David Vickrey and Daphne Koller. Applying sen-
tence simplification to the conll-2008 shared task.
In Proceedings of CoNLL-2008., 2008.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In EMNLP ’04:
Proceedings of the Annual Conference on Em-
pirical Methods in Natural Language Processing,
2004.
Hai Zhao and Chunyu Kit. Parsing syntactic and se-
mantic dependencies with two single-stage max-
imum entropy models. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, Manchester,
England, 2008.
</reference>
<page confidence="0.999118">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.909343">
<title confidence="0.996279">Jointly Identifying Predicates, Arguments and Senses using Markov Logic</title>
<author confidence="0.995195">Sebastian</author>
<affiliation confidence="0.981882333333333">of Informatics, University of Edinburgh, of Computer Science, University of Tokyo, Center for Life Science, Research Organization of Information and System,</affiliation>
<abstract confidence="0.9983821">In this paper we present a Markov Logic Network for Semantic Role Labelling that jointly performs predicate identification, frame disambiguation, argument identification and argument classification for all predicates in a sentence. Empirically we find that our approach is competitive: our best model would appear on par with the best entry in the CoNLL 2008 shared task open track, and at the 4th place of the closed track—right behind the systems that use significantly better parsers to generate their input features. Moreover, we observe that by fully capturing the complete SRL pipeline in a single probabilistic model we can achieve significant improvements over more isolated systems, in particular for out-of-domain data. Finally, we show that despite the joint approach, our system is still efficient.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-2000,</booktitle>
<contexts>
<context position="20066" citStr="Charniak, 2000" startWordPosition="3345" endWordPosition="3346"> a2, r) lemma(p, +l) ∧ ppos(a, +p) ∧ hasRole(p, a) ⇒ sense(p, +f) lemma(p, +l) ∧ role(p, a, +r) ⇒ sense(p, +f) Table 2: Global formulae for ML model solver. 5 Experimental Setup For training and testing our SRL systems we used a version of the CoNLL 2008 shared task (Surdeanu et al., 2008) dataset that only mentions verbal predicates, disregarding the nominal predicates available in the original corpus.7 While the original (open track) corpus came with MALT (Nivre et al., 2007) dependencies, we observed slightly better results when using the dependency parses generated with a Charniak parser (Charniak, 2000). Hence we used the latter for all our experiments. To assess the performance of our model, and it to evaluate the possible gains to be made from considering a joint model of the complete SRL pipeline, we set up several systems. The full system uses a Markov Logic Network with all local and global formulae described in section 3. For the bottom-up system we removed the structural top-down constraints from the complete model—previous work Riedel and Meza-Ruiz (2008) has shown that this can lead to improved performance. The bottom-up (-arg) system is equivalent to the bottom-up system, but it do</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. A maximum-entropy-inspired parser. In Proceedings of NAACL-2000, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>991</pages>
<contexts>
<context position="18874" citStr="Crammer and Singer, 2003" startWordPosition="3139" endWordPosition="3142">s type of algorithm could also be realised for an ILP formulation of SRL. However, it would require us to write a dedicated separation routine for each type of constraint we want to add. In Markov Logic, on the other hand, separation can be generically implemented as the search for variable bindings that render a weighted first order formulae true (if its weight is negative) or false (if its weight is positive). In practise this means that we can try new global formulae/constraints without any additional implementation overhead. We learn the weights associated with each MLN using 1-best MIRA (Crammer and Singer, 2003) Online Learning method. As MAP inference method that is applied in the inner loop of the online learner we apply CPI, again with ILP as base 159 Bottom-up sense(p, s) ⇒ isPredicate(p) hasRole(p, a) ⇒ isPredicate(p) hasRole(p, a) ⇒ isArgument(a) role(p, a, r) ⇒ hasLabel(p, a) Top-Down isPredicate(p) ⇒ ∃s.sense(p, s) isPredicate(p) ⇒ ∃a.hasRole(p, a) isArgument(a) ⇒ ∃p.hasRole(p, a) hasLabel(p, a) ⇒ ∃r.role(p, a, r) Unique Labels role(p, a, r1) ∧ r1 =6 r2 ⇒ ¬role(p, a, r2) sense(p, s1) ∧ s1 =6 s2 ⇒ ¬sense(p, r2) Linguistic role (p, a1, r) ∧ ¬mod (r) ∧ a1 =6 a2 ⇒ ¬role (p, a2, r) lemma(p, +l) ∧ </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951– 991, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependencybased semantic role labeling of propbank.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-2008.,</booktitle>
<contexts>
<context position="22554" citStr="Johansson and Nugues, 2008" startWordPosition="3762" endWordPosition="3766">predicate and its arguments, the label of such dependency is the role of the argument. Additionally, there is a semantic dependency for each predicate and a ROOT argument which has the sense of the predicate as label. To put these results into context, let us compare them to those of the participants of the CoNLL 2008 shared task (see the last three rows of table 3).8 Our best model, Bottom-up, would reach the highest F1 WSJ score, and second highest Brown score, for the open track. Here the best-performing participant was Vickrey and Koller (2008). Table 3 also shows the results of the best (Johansson and Nugues, 2008) and fourth best sys8Results of other systems were extracted from Table 16 of the shared task overview paper (Surdeanu et al., 2008). 160 tem (Zhao and Kit, 2008) of the closed track. We note that we do significantly worse than Johansson and Nugues (2008), and roughly equivalent to Zhao and Kit (2008); this places us on the fourth rank of 19 participants. However, note that all three systems above us, as well as Zhao and Kit (2008), use parsers with at least about 90% (unlabelled) accuracy on the WSJ test set (Johansson’s parser has about 92% unlabelled accuracy).9 By contrast, with about 87% </context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. Dependencybased semantic role labeling of propbank. In Proceedings of EMNLP-2008., 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis M´arquez</author>
<author>Xavier Carreras</author>
<author>Ken Litkowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M´arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>Lluis M´arquez, Xavier Carreras, Ken Litkowski, and Suzanne Stevenson. Semantic role labeling. Computational Linguistics, 34(2), 2008. Introduction to the Special Issue on Semantic Role Labeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S Kuebler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="19933" citStr="Nivre et al., 2007" startWordPosition="3325" endWordPosition="3328"> a, r1) ∧ r1 =6 r2 ⇒ ¬role(p, a, r2) sense(p, s1) ∧ s1 =6 s2 ⇒ ¬sense(p, r2) Linguistic role (p, a1, r) ∧ ¬mod (r) ∧ a1 =6 a2 ⇒ ¬role (p, a2, r) lemma(p, +l) ∧ ppos(a, +p) ∧ hasRole(p, a) ⇒ sense(p, +f) lemma(p, +l) ∧ role(p, a, +r) ⇒ sense(p, +f) Table 2: Global formulae for ML model solver. 5 Experimental Setup For training and testing our SRL systems we used a version of the CoNLL 2008 shared task (Surdeanu et al., 2008) dataset that only mentions verbal predicates, disregarding the nominal predicates available in the original corpus.7 While the original (open track) corpus came with MALT (Nivre et al., 2007) dependencies, we observed slightly better results when using the dependency parses generated with a Charniak parser (Charniak, 2000). Hence we used the latter for all our experiments. To assess the performance of our model, and it to evaluate the possible gains to be made from considering a joint model of the complete SRL pipeline, we set up several systems. The full system uses a Markov Logic Network with all local and global formulae described in section 3. For the bottom-up system we removed the structural top-down constraints from the complete model—previous work Riedel and Meza-Ruiz (200</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, Kuebler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kuebler, S. Marinov, and E. Marsi. MaltParser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13(02):95–135, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Generalized inference with multiple semantic role labeling systems.</title>
<date>2005</date>
<booktitle>In Ido Dagan and Dan Gildea, editors, CoNLL ’05: Proceedings of the Annual Conference on Computational Natural Language Learning,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="2852" citStr="Punyakanok et al., 2005" startWordPosition="437" endWordPosition="440">fication), and which is the sense of the predicate (sense disambiguation). In this paper we use Markov Logic (ML), a Statistical Relational Learning framework that combines First Order Logic and Markov Networks, to develop a joint probabilistic model over all decisions mentioned above. The following paragraphs will motivate this choice. First, it allows us to readily capture global correlations between decisions, such as the constraint that a predicate can only have one agent. This type of correlations has been successfully exploited in several previous SRL approaches (Toutanova et al., 2005; Punyakanok et al., 2005). Second, we can use the joint model to evaluate the benefit of incorporating decisions into the joint model that either have not received much attention within the SRL community (predicate identification and sense disambiguation), or been largely made in isolation (argument identification and classification for all predicates of a sentence). Third, our ML model is essentially a template that describes a class of Markov Networks. Algorithms can perform inference in terms of this template with155 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, p</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. Generalized inference with multiple semantic role labeling systems. In Ido Dagan and Dan Gildea, editors, CoNLL ’05: Proceedings of the Annual Conference on Computational Natural Language Learning, pages 181–184, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>University of Washington,</institution>
<contexts>
<context position="5448" citStr="Richardson and Domingos, 2005" startWordPosition="855" endWordPosition="858"> improvements are particularly large in the case of out-of-domain data, suggesting that a joint approach helps to increase the robustness of SRL. Finally, we show that despite the joint approach, our system is still efficient. Our paper is organised as follows: we first introduce ML (section 2), then we present our model in terms of ML (section 3) and illustrate how to perform learning and inference with it (section 4). How this model will be evaluated is explained in section 5 with the corresponding evaluation presented in section 6. We conclude in section 7. 2 Markov Logic Markov Logic (ML, Richardson and Domingos, 2005) is a Statistical Relational Learning language based on First Order Logic and Markov Networks. It can be seen as a formalism that extends First Order Logic to allow formulae that can be violated with 1Our unlabelled accuracy for syntactic dependencies is at least 3% points under theirs. some penalty. From an alternative point of view, it is an expressive template language that uses First Order Logic formulae to instantiate Markov Networks of repetitive structure. Let us describe ML by considering the predicate identification task. In ML we can model this task by first introducing a set of logi</context>
</contexts>
<marker>Richardson, Domingos, 2005</marker>
<rawString>Matthew Richardson and Pedro Domingos. Markov logic networks. Technical report, University of Washington, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
</authors>
<title>Improving the accuracy and efficiency of map inference for markov logic.</title>
<date>2008</date>
<booktitle>In UAI ’08: Proceedings of the Annual Conference on Uncertainty in AI,</booktitle>
<contexts>
<context position="3623" citStr="Riedel, 2008" startWordPosition="553" endWordPosition="554">n the SRL community (predicate identification and sense disambiguation), or been largely made in isolation (argument identification and classification for all predicates of a sentence). Third, our ML model is essentially a template that describes a class of Markov Networks. Algorithms can perform inference in terms of this template with155 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 155–163, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics out ever having to fully instantiate the complete Markov Network (Riedel, 2008; Singla and Domingos, 2008). This can dramatically improve the efficiency of an SRL system when compared to a propositional approach such as Integer Linear Programming (ILP). Finally, when it comes to actually building an SRL system with ML there are “only” four things to do: preparing input data files, converting output data files, and triggering learning and inference. The remaining work can be done by an off-theshelf Markov Logic interpreter. This is to be contrasted with pipeline systems where several components need to be trained and connected, or Integer Linear Programming approaches fo</context>
<context position="17576" citStr="Riedel, 2008" startWordPosition="2923" endWordPosition="2924">d sense s. Note that the final two formulae evaluate the semantic frame of a predicate and become local formulae in a pipeline system that performs sense disambiguation after argument identification and classification. Table 2 summarises the global formulae we use in this work. 4 Inference and Learning Assuming that we have an MLN, a set of weights and a given sentence then we need to predict the choice of predicates, frame types, arguments and role labels with maximal a posteriori probability (MAP). To this end we apply a method that is both exact and efficient: Cutting Plane Inference (CPI, Riedel, 2008) with Integer Linear Programming (ILP) as base solver. Instead of fully instantiating the Markov Network that a Markov Logic Network describes, CPI begins with a subset of factors/edges—in our case we use the factors that correspond to the local formulae of our model—and solves the MAP problem for this subset using the base solver. It then inspects the solution for ground formulae/features that are not yet included but could, if added, lead to a different solution—this process is usually referred to as separation. The ground formulae that we have found are added and the network is solved again</context>
</contexts>
<marker>Riedel, 2008</marker>
<rawString>Sebastian Riedel. Improving the accuracy and efficiency of map inference for markov logic. In UAI ’08: Proceedings of the Annual Conference on Uncertainty in AI, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Ivan Meza-Ruiz</author>
</authors>
<title>Collective semantic role labelling with markov logic.</title>
<date>2008</date>
<booktitle>In Conference on Computational Natural Language Learning,</booktitle>
<contexts>
<context position="20535" citStr="Riedel and Meza-Ruiz (2008)" startWordPosition="3424" endWordPosition="3427">MALT (Nivre et al., 2007) dependencies, we observed slightly better results when using the dependency parses generated with a Charniak parser (Charniak, 2000). Hence we used the latter for all our experiments. To assess the performance of our model, and it to evaluate the possible gains to be made from considering a joint model of the complete SRL pipeline, we set up several systems. The full system uses a Markov Logic Network with all local and global formulae described in section 3. For the bottom-up system we removed the structural top-down constraints from the complete model—previous work Riedel and Meza-Ruiz (2008) has shown that this can lead to improved performance. The bottom-up (-arg) system is equivalent to the bottom-up system, but it does not include any formulae that mention the hidden isArgument predicate. For the systems presented so far we perform joint inference and learning. The pipeline system differs in this regard. For this system we train a separate model for each stage in the pipeline of figure 1. The predicate identification stage identifies the predicates (using all local isPredicate formulae) of 7The reason for this choice where license problems. a sentence. The next stage predicts </context>
<context position="23254" citStr="Riedel and Meza-Ruiz (2008)" startWordPosition="3884" endWordPosition="3887">6 of the shared task overview paper (Surdeanu et al., 2008). 160 tem (Zhao and Kit, 2008) of the closed track. We note that we do significantly worse than Johansson and Nugues (2008), and roughly equivalent to Zhao and Kit (2008); this places us on the fourth rank of 19 participants. However, note that all three systems above us, as well as Zhao and Kit (2008), use parsers with at least about 90% (unlabelled) accuracy on the WSJ test set (Johansson’s parser has about 92% unlabelled accuracy).9 By contrast, with about 87% unlabelled accuracy our parses are significantly worse. Finally, akin to Riedel and Meza-Ruiz (2008) we observe that the bottom-up joint model performs better than the full joint model. System Devel WSJ Brown Full 76.93 79.09 67.64 Bottom-up 77.96 80.16 68.02 Bottom-up (-arg) 77.57 79.37 66.70 Pipeline 75.69 78.19 64.66 Vickrey N/A 79.75 69.57 Johansson N/A 86.37 71.87 Zhao N/A 79.40 66.38 Table 3: Semantic Fl scores for our systems and three CoNLL 2008 shared task participants. The Bottom-up results are statistically significantly different to all others (i.e., p ≤ 0.05 according to the sign test). 6.1 Joint Model vs. Pipeline Table 3 suggests that by including sense disambiguation into the</context>
</contexts>
<marker>Riedel, Meza-Ruiz, 2008</marker>
<rawString>Sebastian Riedel and Ivan Meza-Ruiz. Collective semantic role labelling with markov logic. In Conference on Computational Natural Language Learning, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Singla</author>
<author>P Domingos</author>
</authors>
<title>Lifted First-Order Belief Propagation. Association for the Advancement of Artificial Intelligence (AAAI),</title>
<date>2008</date>
<contexts>
<context position="3651" citStr="Singla and Domingos, 2008" startWordPosition="555" endWordPosition="559">unity (predicate identification and sense disambiguation), or been largely made in isolation (argument identification and classification for all predicates of a sentence). Third, our ML model is essentially a template that describes a class of Markov Networks. Algorithms can perform inference in terms of this template with155 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 155–163, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics out ever having to fully instantiate the complete Markov Network (Riedel, 2008; Singla and Domingos, 2008). This can dramatically improve the efficiency of an SRL system when compared to a propositional approach such as Integer Linear Programming (ILP). Finally, when it comes to actually building an SRL system with ML there are “only” four things to do: preparing input data files, converting output data files, and triggering learning and inference. The remaining work can be done by an off-theshelf Markov Logic interpreter. This is to be contrasted with pipeline systems where several components need to be trained and connected, or Integer Linear Programming approaches for which we need to write add</context>
</contexts>
<marker>Singla, Domingos, 2008</marker>
<rawString>P. Singla and P. Domingos. Lifted First-Order Belief Propagation. Association for the Advancement of Artificial Intelligence (AAAI), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008),</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="2826" citStr="Toutanova et al., 2005" startWordPosition="433" endWordPosition="436">ns play (argument classification), and which is the sense of the predicate (sense disambiguation). In this paper we use Markov Logic (ML), a Statistical Relational Learning framework that combines First Order Logic and Markov Networks, to develop a joint probabilistic model over all decisions mentioned above. The following paragraphs will motivate this choice. First, it allows us to readily capture global correlations between decisions, such as the constraint that a predicate can only have one agent. This type of correlations has been successfully exploited in several previous SRL approaches (Toutanova et al., 2005; Punyakanok et al., 2005). Second, we can use the joint model to evaluate the benefit of incorporating decisions into the joint model that either have not received much attention within the SRL community (predicate identification and sense disambiguation), or been largely made in isolation (argument identification and classification for all predicates of a sentence). Third, our ML model is essentially a template that describes a class of Markov Networks. Algorithms can perform inference in terms of this template with155 Human Language Technologies: The 2009 Annual Conference of the North Amer</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. Joint learning improves semantic role labeling. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, Morristown, NJ, USA, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Daphne Koller</author>
</authors>
<title>Applying sentence simplification to the conll-2008 shared task.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008.,</booktitle>
<contexts>
<context position="22481" citStr="Vickrey and Koller (2008)" startWordPosition="3749" endWordPosition="3752">vered semantic dependencies. A semantic dependency is created for each predicate and its arguments, the label of such dependency is the role of the argument. Additionally, there is a semantic dependency for each predicate and a ROOT argument which has the sense of the predicate as label. To put these results into context, let us compare them to those of the participants of the CoNLL 2008 shared task (see the last three rows of table 3).8 Our best model, Bottom-up, would reach the highest F1 WSJ score, and second highest Brown score, for the open track. Here the best-performing participant was Vickrey and Koller (2008). Table 3 also shows the results of the best (Johansson and Nugues, 2008) and fourth best sys8Results of other systems were extracted from Table 16 of the shared task overview paper (Surdeanu et al., 2008). 160 tem (Zhao and Kit, 2008) of the closed track. We note that we do significantly worse than Johansson and Nugues (2008), and roughly equivalent to Zhao and Kit (2008); this places us on the fourth rank of 19 participants. However, note that all three systems above us, as well as Zhao and Kit (2008), use parsers with at least about 90% (unlabelled) accuracy on the WSJ test set (Johansson’s</context>
</contexts>
<marker>Vickrey, Koller, 2008</marker>
<rawString>David Vickrey and Daphne Koller. Applying sentence simplification to the conll-2008 shared task. In Proceedings of CoNLL-2008., 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In EMNLP ’04: Proceedings of the Annual Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="12226" citStr="Xue and Palmer (2004)" startWordPosition="2005" endWordPosition="2008"> i has POS tag p cpos(i,p) Token i has coarse POS tag p voice(i,v) Token i is verb and has voice v (Active/Passive). subcat(i,f) Token i has subcategorization frame f dep(i,j,d) Token h is head of token m and has dependency label d palmer(i,j) Token j can be semantic argument for token i according to high recall heuristic* depPath(i,j,p) Dependency path between tokens i and j is p* depFrame(i,j,f) f is a syntactic (dependency) frame in which tokens i and j are designated as “pivots”* Table 1: Observable predicates; predicates marked with * are dependency parsing-based versions for features of Xue and Palmer (2004). two observed lemma ground atoms. The + notation indicates that the MLN contains one instance of the rule, with a separate weight, for each assignment of the variables with a plus sign (?). The local formulae for isPredicate, isArgument and sense aim to capture the relation of the tokens with their lexical and syntactic surroundings. This includes formulae such as subcat(p, +f) ⇒ isPredicate(p) which implies that a certain token is a predicate with a weight that depends on the subcategorization frame of the token. Further local formulae are constructed using those observed predicates in table</context>
<context position="15048" citStr="Xue and Palmer, 2004" startWordPosition="2478" endWordPosition="2481">es: to ensure consistency between the predicates of all SRL stages, and to capture some of our background knowledge about SRL. We will refer to formulae that serve the first purpose as structural constraints. For example, a structural constraint is given by the (deterministic) formula role(p, a, r) ⇒ hasRole(p, a) which ensures that, whenever the argument a is given a label r with respect to the predicate p, this argument must be an argument of a as denoted by hasRole(p,a). Note that this formula by itself models the traditional “bottom-up” argument identification and classification pipeline (Xue and Palmer, 2004): 5http://code.google.com/p/thebeast/ source/browse/#svn/mlns/naacl-hlt 6http://code.google.com/p/thebeast 158 it is possible to not assign a role r to an predicateargument pair (p, a) proposed by the identification stage; however, it is impossible to assign a role r to token pairs (p, a) that have not been proposed as potential arguments. An example of another class of structural constraints is hasRole(p, a) ⇒ ∃r.role(p, a, r) which, by itself, models an inverted or “top-down” pipeline. In this architecture the argument classification stage can assign roles to tokens that have not been propos</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. Calibrating features for semantic role labeling. In EMNLP ’04: Proceedings of the Annual Conference on Empirical Methods in Natural Language Processing, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Parsing syntactic and semantic dependencies with two single-stage maximum entropy models.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<location>Manchester, England,</location>
<contexts>
<context position="22716" citStr="Zhao and Kit, 2008" startWordPosition="3792" endWordPosition="3795"> which has the sense of the predicate as label. To put these results into context, let us compare them to those of the participants of the CoNLL 2008 shared task (see the last three rows of table 3).8 Our best model, Bottom-up, would reach the highest F1 WSJ score, and second highest Brown score, for the open track. Here the best-performing participant was Vickrey and Koller (2008). Table 3 also shows the results of the best (Johansson and Nugues, 2008) and fourth best sys8Results of other systems were extracted from Table 16 of the shared task overview paper (Surdeanu et al., 2008). 160 tem (Zhao and Kit, 2008) of the closed track. We note that we do significantly worse than Johansson and Nugues (2008), and roughly equivalent to Zhao and Kit (2008); this places us on the fourth rank of 19 participants. However, note that all three systems above us, as well as Zhao and Kit (2008), use parsers with at least about 90% (unlabelled) accuracy on the WSJ test set (Johansson’s parser has about 92% unlabelled accuracy).9 By contrast, with about 87% unlabelled accuracy our parses are significantly worse. Finally, akin to Riedel and Meza-Ruiz (2008) we observe that the bottom-up joint model performs better tha</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. Parsing syntactic and semantic dependencies with two single-stage maximum entropy models. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, Manchester, England, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>