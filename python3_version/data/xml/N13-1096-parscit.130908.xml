<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005236">
<title confidence="0.984461">
Measuring the Structural Importance through Rhetorical Structure Index
</title>
<author confidence="0.987246">
Narine Kokhlikyant, Alex Waibelt t, Yuqi Zhangt, Joy Ying Zhangt
</author>
<affiliation confidence="0.6226534">
tKarlsruhe Institute of Technology
Adenauerring 2
76131 Karlsruhe, Germany
t Carnegie Mellon University
NASA Research Park, Bldg. 23
</affiliation>
<address confidence="0.98799">
Moffett Field, CA 94035
</address>
<email confidence="0.9504855">
narine.kokhlikyan@student.kit.edu, waibel@cs.cmu.edu,
yuqi.zhang@kit.edu, joy.zhang@sv.cmu.edu
</email>
<sectionHeader confidence="0.996607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985733066666667">
In this paper, we propose a novel Rhetorical
Structure Index (RSI) to measure the struc-
tural importance of a word or a phrase. Un-
like TF-IDF and other content-driven mea-
surements, RSI identifies words or phrases
that are structural cues in an unstructured doc-
ument. We show structurally motivated fea-
tures with high RSI values are more useful
than content-driven features for applications
such as segmenting unstructured lecture tran-
scripts into meaningful segments. Experi-
ments show that using RSI significantly im-
proves the segmentation accuracy compared
to TF-IDF, a traditional content-based feature
weighting scheme.
</bodyText>
<sectionHeader confidence="0.998735" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947357142857">
Online learning, a new trend in distance learning,
provides numerous lectures to students all over the
world. More than 19,000 colleges offer thousands
of free online lectures1. Starting from video record-
ings of lectures which sometimes also come with the
presentation material, a set of processes can be ap-
plied to extract information from the unstructured
data to assist students in browsing, searching and
understanding the content of the lecture. These pro-
cesses include automatic speech recognition (ASR)
which converts the audio to text, lecture segmen-
tation which inserts paragraph boundaries and adds
section titles to the lecture transcriptions, automatic
summarization that generates a short summary from
</bodyText>
<footnote confidence="0.9282525">
1http://www.thebestcolleges.org/
free-online-classes-and-course-lectures/
</footnote>
<bodyText confidence="0.999902852941177">
the full lecture, and lecture translation that translates
the lecture from the original language to the native
language of the student.
The transcription of a lecture generated by the
ASR system is a sequence of words which does
not contain any structural information such as para-
graph, section boundaries and section titles. Zhang
et al. (2007; 2008; 2010) used acoustic and lin-
guistic features for rhetorical structure detection and
summarization. They showed that linguistic features
such as TF-IDF are the most influential in segmenta-
tion and summarization and that knowing the struc-
ture of a lecture can significantly improve the perfor-
mance of lecture summarization. Our experiments
with a real-time lecture translation system also show
that displaying the rolling translation results of a live
lecture with proper paragraphing and inserted sec-
tion titles makes it easier for students to grasp the
key points during a lecture.
In this paper, we apply existing algorithms,
namely the Hidden Markov Model (HMM) (Gales
and Young, 2007) to unstructured lecture transcrip-
tion to infer the underlying structure for better lec-
ture segmentation and summarization. HMM has
been successfully applied in early works (van Mul-
bregt et al., 1998; Sherman and Liu, 2008) for text
segmentation, event tracking and boundary detec-
tion. The focus of this work is to identify cue
words and phrases that are good indicators of lec-
ture structure. Intuitively, words and phrases such
as “last week we talked about”, “this is an out-
line of my talk”, “now I am going to talk about”,
“in conclusion”, and “any questions” should be
important features to recognize lecture structure.
</bodyText>
<page confidence="0.981462">
783
</page>
<subsectionHeader confidence="0.295078">
Proceedings of NAACL-HLT 2013, pages 783–788,
</subsectionHeader>
<bodyText confidence="0.95544125">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
These words/phrases, however, may not be so im-
portant content-wise. Thus, content-driven met-
rics such as the TF-IDF score usually do not as-
sign higher weights to these structurally impor-
tant words/phrases. We propose a novel metric
called Rhetorical Structural Index (RSI) to weigh
words/phrases based on their structural importance.
</bodyText>
<sectionHeader confidence="0.876958" genericHeader="method">
2 Rhetorical Structural Index
</sectionHeader>
<bodyText confidence="0.999918846153846">
RSI incorporates both frequency of occurrences and,
more importantly, the position distribution of occur-
rences of a word/phrase. The intuition is that if a
term is a structural marker, it usually occurs at a cer-
tain position in a lecture. Because the term is mainly
about the structure rather than the content of a lec-
ture, it can appear with high frequency in lectures
that are of different topics. For example, “today we”
occurs at the beginning of a lecture and “thank you”
usually appears towards the end (Figure 1) no mat-
ter whether the lecture is about history or computer
science.
We define the RSI of a word w as:
</bodyText>
<equation confidence="0.9981115">
1
RSI(w) = AVar(L,,,) + (1 − A)idf(w, D) (1)
</equation>
<bodyText confidence="0.999509304347826">
where L,,, is the random variable of “normalized po-
sitions” of a word w in a lecture. For each occur-
rence of w in a particular lecture d, we divide its
position by the length of the lecture |d |to estimate
its “normalized position”. L,,, takes a value between
[0, 1]. A value close to 0 indicates this word occurs
at the beginning and close to 1 means w is close to
the end of the lecture. Var(L,,,) is the variance of the
normalized position of a word w. A small Var(L,,,)
indicates that w always occurs at certain positions
of a typical lecture (e.g., “bye”) while a large value
means w can occur at any position (e.g., function
words “of” and “the”).
The second part of RSI is the inverse document
frequency (idf), or effectively the document fre-
quency since RSI is proportional to the 1/idf term.
Lectures, such as different research talks, can vary
in content but usually have a very similar structure
and share some common structural cues. A good
structural cue word should be common to many lec-
tures. idf has been widely used in information re-
trieval research to assign higher weights to words
that occur in just a few documents as compared to
</bodyText>
<tableCaption confidence="0.908451">
Table 1: Examples of n-grams with high RSI values
which are likely to be structural cues.
</tableCaption>
<bodyText confidence="0.999336666666667">
n-gram
now
here
class
week
goodbye
thank you
talk about
dealing with
today we
see how
ladies and gentlemen
last time we
here we have
next time we
common words that occur in all documents. Define
the idf of a word w given a collection of lectures D
as:
</bodyText>
<equation confidence="0.974429">
idf(w, D) = log |{d E D|w E d} |(2)
|D|
</equation>
<bodyText confidence="0.999870529411765">
|D |is the number of all lectures in the collection and
|{d E D|w E d} |is the number of lectures where w
appears. A low idf (w, D) value indicates that word
w occurs in many documents and thus is more likely
to be a common structural cue.
Combining the variance of normalized position
and idf by scaling factor A, we define RSI as in equa-
tion 1. We found 0.9 as an optimal value of A accord-
ing our experiments over all data sets. A word w
with high RSI value is more likely to be structurally
important. Similarly, we can calculate the RSI val-
ues for phrases (n-grams) such as “I would like to
talk about”, “I will switch gear to” and “thank you
for your attention”.
Table 1 shows examples of n-grams and the cal-
culated variance, idf-scores and RSI values from a
collection of lectures.
</bodyText>
<sectionHeader confidence="0.9825045" genericHeader="method">
3 Incorporating RSI in Lecture
Segmentation
</sectionHeader>
<bodyText confidence="0.994992">
Several algorithms have been developed for text seg-
mentation including the Naive Bayes classifier for
keyword extraction (Balagopalan et al., 2012), the
</bodyText>
<table confidence="0.999649266666667">
Var(L,,,) idf RSI
0.0004 0.60 1.04
0.0004 0.62 1.03
0.0001 2.12 0.90
0.0001 2.23 0.89
0.0001 3.62 0.80
0.0003 1.53 0.95
0.0003 1.90 0.92
0.0002 2.00 0.91
0.0003 2.51 0.87
0.0009 2.69 0.85
0.0008 1.35 0.96
0.0004 2.22 0.89
0.0005 2.35 0.88
0.0002 2.51 0.86
</table>
<page confidence="0.989775">
784
</page>
<figureCaption confidence="0.99837175">
Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams “today we” and “thank you”.
“today we” appears more frequently at the beginning of a lecture, whereas “thank you” more in the concluding part
of a lecture. The x-axis is the normalized word position in a lecture and y-axis is the probability of seeing the word at
a position.
</figureCaption>
<bodyText confidence="0.9986152">
Hidden Markov Model (Gales and Young, 2007),
the Maximum entropy Markov model (McCallum et
al., 2000), the Conditional Random Field (Lafferty
et al., 2001) and the Latent Content Analysis (Ponte
and Croft, 1997). In this paper, we evaluate the ef-
fectiveness of the proposed RSI feature on lecture
segmentation using an HMM.
We represent each segment in a lecture as a state
in the Markov model and use the EM algorithm
to learn HMM parameters from unlabeled lecture
data. We use a fully connected HMM with five
states. Typical state labels for lecture are: “Introduc-
tion”, “Background”, “Main Topic”, “Questions”
and “Conclusion” as shown in Figure 2. HMM states
emit word tokens. Instead of considering the full
vocabulary as the possible emission alphabet, which
usually leads to model over-fitting, we only consider
terms with high RSI values and high TF-IDF* scores
for comparison. For a word w, define its TF-IDF*
score as:
</bodyText>
<equation confidence="0.963893">
TF-IDF*(w) = max TF-IDF(w,d), (3)
</equation>
<bodyText confidence="0.977495866666667">
d
which is the highest TF-IDF score of a word in any
document in the collection. Our experiments try to
answer the question that “if HMM is meant to cap-
ture the underlying structure of lectures no matter
which topic the lecture is about, what kind of fea-
tures should be emitted from each state to reflect
such structural patterns among lectures?”
The learned HMM model is then applied to un-
seen lecture data to label each sentence to be “In-
troduction”, “Background”, “Main Topic”, “Ques-
tions” or “Conclusion” and, based on the label, we
segment the lecture to different sections for evalu-
ation. Segment boundaries are defined in the posi-
tions where sentence labels change.
</bodyText>
<subsectionHeader confidence="0.8388225">
3.0.1 Bootstrap HMM from K-Means
Clustering Segmentation
</subsectionHeader>
<bodyText confidence="0.999927333333333">
Initial HMM parameters are bootstrapped using
results from K-means clustering where we cluster a
sequence of sentences to form a “segment”. K cor-
responds to the number of desired segments of a lec-
ture. Similarities are computed based on the content
similarity (using n-gram matches) and the relative
</bodyText>
<page confidence="0.994632">
785
</page>
<bodyText confidence="0.9491138">
sentence position defined as:
Sim(Si, Cj) = αM(Si, Cj) + (1 − α)P(Si, Cj),
(4)
where Si is the i-th sentence, Cj is the centroid of
the j-th cluster. M(Si, Cj) is the content similarity
between sentence Si and centroid Cj and P(Si, Cj)
is the position similarity (distance). α is a scaling
factor (set to optimal value 0.2 based on all data sets
in our experiments).
Content similarity is based on the number of com-
mon words between two sentences, or between a
sentence and the centroid vector of a cluster. Denote
the binary word frequency vector (bag of words) in
sentence Si as ~Si and similarly ~Cj for cluster cen-
troid Cj, define:
</bodyText>
<equation confidence="0.999222">
M(Si, Cj) =
L
P(Si, Cj) = |Pos(Si) − Pos(Cj) |+ E, (6)
</equation>
<bodyText confidence="0.999898166666667">
where Si is the i-th sentence, Cj is the j-th cluster.
Pos(Si) is the position of sentence Si. Pos(Cj) is
the average sentence position of all members belong
to cluster Cj and L is the total number of sentences
in a lecture. c is a small constant to avoid division
by zeros.
</bodyText>
<sectionHeader confidence="0.997003" genericHeader="evaluation">
4 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999931909090909">
We evaluated segmentation on three different data
sets: college lectures recorded by Karlsruhe In-
stitute of Technology (KIT), Microsoft research
(MSR) lectures2 and scientific papers3. Both col-
lege and Microsoft research lectures are manually
transcribed. The reason why we do not include ex-
periments on ASR output is that current ASR quality
of lecture data is still quite poor. Word-Error-Rates
(WER) of ASR output range from 24.37 to 30.80 for
KIT lectures. Roughly speaking, every one out of 3
or 4 words is mis-recognized.
</bodyText>
<footnote confidence="0.999552">
2http://research.microsoft.com/apps/catalog/
3http://aclweb.org/anthology-new/
</footnote>
<bodyText confidence="0.999179">
For evaluation, human annotators annotated a few
lectures to create test/reference sets. The test data
from KIT is annotated by one human annotator and
MSR lectures are annotated by four annotators. The
segmentation gold standard is created based on the
agreed annotations. Since the number of annotated
lectures is small and human annotation is subjective,
we also used ACL papers as an additional data set.
ACL papers are in a way “lectures in written form”
and have titles for section and subsections which can
be used to identify the segments and annotate the
data set automatically. The statistics of each data set
are listed in Table 2.
</bodyText>
<tableCaption confidence="0.974904111111111">
Table 2: Statistics of three data sets used in the exper-
iments: our own lecture data (KIT), Microsoft research
talks (MSR) and conference proceedings from ACL an-
thology archive. We removed equations, short titles such
as “Abstract” and “Conclusion”, when extracting text
from PDF files from the ACL anthology, which results
in a relatively small number of words per paper. Words
are simply tokenized without case normalization or stem-
ming, which results in relatively large vocabulary sizes.
</tableCaption>
<table confidence="0.997234666666667">
Properties KIT MSR ACL
Num. 74 1,182 3,583
Avg. Num. of Sent. 484 655 212
Avg. Num. of Words 10,078 10,225 3,896
Avg. Duration (Min.) 43.57 39.15 -
Vocabulary Size 1.3K 22K 24K
</table>
<bodyText confidence="0.999840611111111">
First, we calculate the RSI and TF-IDF* scores
for each word in the dataset and choose the top N
words as the HMM emission vocabulary. To avoid
over-fitting, we choose N that is much smaller than
the full vocabulary size of the data set. In our ex-
periments, we set N=300 for KIT, N=5000 for MSR
and N=5400 for ACL. The top 5 words with the
highest TF-IDF* scores from the MSR data set are:
“RFID”, “Cherokee”, “tree-to-string”, “GPU”, and
“data-triggered”, whereas the top 5 words selected
by RSI are “today”, “work”, “question”, “now”, and
“thank”, which are more structurally informative.
To estimate the accuracy of the segmentation
module, we used Recall, Precision, F-Measure and
Pk (Beeferman et al., 1999) as evaluation metric.
We used an error window of length 6 to calculate
Precision, Recall and F-Measure and a sliding win-
dow with a length equal to half of the average seg-
</bodyText>
<figure confidence="0.901774">
. (5)
� ~Si �� ~Cj �
P(Si, Cj) measures the position similarity of two
sentences. Position similarity is based on the rela-
tive position distance between the sentence and the
cluster:Define
~Si·
~Cj
786
Questions
Introduction Background Main Topic Conclusion
Introduction Background Content Questions Conclusion
word tokens word tokens word tokens word tokens word tokens
</figure>
<figureCaption confidence="0.9932445">
Figure 2: Fully connected 5-state HMM representing Introduction, Background, Main Topic, Questions, Conclusion
in a typical lecture.
</figureCaption>
<bodyText confidence="0.999809875">
ment length to estimate the Pk score. With error
window we mean that hypothesis boundaries do not
have to be exactly the same as the reference segment
boundaries. Hypothesis boundaries are acceptable
if they are close enough to reference boundaries in
that window. The Pk score indicates the probability
of segmentation inconsistency. Therefore, the lower
the Pk score the better the segmentation is.
</bodyText>
<tableCaption confidence="0.9861894">
Table 3: Segmentation results measured by Pk (the
smaller the better), Precision, Recall and F-Measure
scores (the higher the better) for three data sets compar-
ing HMM using TF-IDF*-filtered word tokens as emis-
sion and RSI-filtered words as emissions.
</tableCaption>
<table confidence="0.999489538461538">
Evaluation Score KIT MSR ACL
Pk 0.06 0.06 0.05
HMM + TF-IDF*
HMM + RSI 0.01 0.02 0.01
Precision
HMM + TF-IDF* 32.01 30.47 32.85
HMM + RSI 41.10 41.01 42.70
Recall
HMM + TF-IDF* 39.32 36.09 38.08
HMM + RSI 47.38 46.39 48.95
F-Measure
HMM + TF-IDF* 35.29 33.04 35.27
HMM + RSI 44.01 43.53 45.61
</table>
<bodyText confidence="0.973891333333333">
The evaluation results on all data sets listed in
Table 3 show that according F-Measure and Pk
scores, considering words with high RSI values as
HMM emission significantly improve over the base-
line method of choosing word tokens with high TF-
IDF* scores.
</bodyText>
<sectionHeader confidence="0.999294" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999835">
In this work we propose the Rhetorical Structure In-
dex (RSI), a method to identify structurally impor-
tant terms in lectures. Experiments show that terms
with high RSI values are better candidates than those
with high TF-IDF values when used by an HMM-
based segmenter as state emissions. In other words,
terms with high RSI values are more likely to be
structural cues in lectures independent of the lecture
topic. In the future we will run experiments on ASR
output and incorporate other prosodic features such
as pitch, intensity, duration into the RSI to improve
this metric for structural analysis of lectures and ap-
ply the RSI to other structure discovery applications
such as dialogue segmentation.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999655">
The authors gratefully acknowledge the support by
an interACT student exchange scholarship. The re-
search leading to these results has received funding
from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement no
287658.
We would like to thank Jan Niehues and Teresa
Herrmann for their suggestions and help.
</bodyText>
<page confidence="0.995142">
787
</page>
<sectionHeader confidence="0.993869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801340909091">
A. Balagopalan, L.L. Balasubramanian, V. Balasubrama-
nian, N. Chandrasekharan, and A. Damodar. 2012.
Automatic keyphrase extraction and segmentation of
video lectures. In Technology Enhanced Education
(ICTEE), 2012 IEEE International Conference on,
pages 1–10.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Mach.
Learn., 34(1-3):177–210, February.
Mark J. F. Gales and Steve J. Young. 2007. The ap-
plication of hidden markov models in speech recog-
nition. Foundations and Trends in Signal Processing,
1(3):195–304.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282–289.
Andrew McCallum, Dayne Freitag, and Fernando C. N.
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In Proceed-
ings of the Seventeenth International Conference on
Machine Learning, ICML ’00, pages 591–598, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmenta-
tion by topic. In ECDL, pages 113–125.
Melissa Sherman and Yang Liu. 2008. Using hid-
den markov models for topic segmentation of meeting
transcripts. In SLT, pages 185–188.
Paul van Mulbregt, Ira Carp, Lawrence Gillick, Steve
Lowe, and Jon Yamron. 1998. Text segmentation and
topic tracking on broadcast news via a hidden markov
model approach. In ICSLP.
Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung.
2007. Improving lecture speech summarization using
rhetorical information. In ASRU, pages 195–200.
Justin Jian Zhang, Shilei Huang, and Pascale Fung. 2008.
Rshmm++ for extractive lecture speech summariza-
tion. In SLT, pages 161–164.
Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung.
2010. Extractive speech summarization using shallow
rhetorical structure modeling. IEEE Transactions on
Audio, Speech &amp; Language Processing, 18(6):1147–
1157.
</reference>
<page confidence="0.996927">
788
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.099073">
<title confidence="0.999696">Measuring the Structural Importance through Rhetorical Structure Index</title>
<author confidence="0.999802">Alex Yuqi Joy Ying</author>
<affiliation confidence="0.915408">Institute of Adenauerring</affiliation>
<address confidence="0.713497">76131 Karlsruhe, Mellon</address>
<affiliation confidence="0.298289">NASA Research Park, Bldg.</affiliation>
<address confidence="0.432422">Moffett Field, CA</address>
<email confidence="0.9175115">narine.kokhlikyan@student.kit.edu,yuqi.zhang@kit.edu,joy.zhang@sv.cmu.edu</email>
<abstract confidence="0.999404">In this paper, we propose a novel Rhetorical Structure Index (RSI) to measure the structural importance of a word or a phrase. Unlike TF-IDF and other content-driven measurements, RSI identifies words or phrases that are structural cues in an unstructured document. We show structurally motivated features with high RSI values are more useful than content-driven features for applications such as segmenting unstructured lecture transcripts into meaningful segments. Experiments show that using RSI significantly improves the segmentation accuracy compared to TF-IDF, a traditional content-based feature weighting scheme.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Balagopalan</author>
<author>L L Balasubramanian</author>
<author>V Balasubramanian</author>
<author>N Chandrasekharan</author>
<author>A Damodar</author>
</authors>
<title>Automatic keyphrase extraction and segmentation of video lectures.</title>
<date>2012</date>
<booktitle>In Technology Enhanced Education (ICTEE), 2012 IEEE International Conference on,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="7161" citStr="Balagopalan et al., 2012" startWordPosition="1166" endWordPosition="1169">d 0.9 as an optimal value of A according our experiments over all data sets. A word w with high RSI value is more likely to be structurally important. Similarly, we can calculate the RSI values for phrases (n-grams) such as “I would like to talk about”, “I will switch gear to” and “thank you for your attention”. Table 1 shows examples of n-grams and the calculated variance, idf-scores and RSI values from a collection of lectures. 3 Incorporating RSI in Lecture Segmentation Several algorithms have been developed for text segmentation including the Naive Bayes classifier for keyword extraction (Balagopalan et al., 2012), the Var(L,,,) idf RSI 0.0004 0.60 1.04 0.0004 0.62 1.03 0.0001 2.12 0.90 0.0001 2.23 0.89 0.0001 3.62 0.80 0.0003 1.53 0.95 0.0003 1.90 0.92 0.0002 2.00 0.91 0.0003 2.51 0.87 0.0009 2.69 0.85 0.0008 1.35 0.96 0.0004 2.22 0.89 0.0005 2.35 0.88 0.0002 2.51 0.86 784 Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams “today we” and “thank you”. “today we” appears more frequently at the beginning of a lecture, whereas “thank you” more in the concluding part of a lecture. The x-axis is the normalized word position in a lecture and y-axis is the probability of</context>
</contexts>
<marker>Balagopalan, Balasubramanian, Balasubramanian, Chandrasekharan, Damodar, 2012</marker>
<rawString>A. Balagopalan, L.L. Balasubramanian, V. Balasubramanian, N. Chandrasekharan, and A. Damodar. 2012. Automatic keyphrase extraction and segmentation of video lectures. In Technology Enhanced Education (ICTEE), 2012 IEEE International Conference on, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<pages>34--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="13416" citStr="Beeferman et al., 1999" startWordPosition="2210" endWordPosition="2213">e top N words as the HMM emission vocabulary. To avoid over-fitting, we choose N that is much smaller than the full vocabulary size of the data set. In our experiments, we set N=300 for KIT, N=5000 for MSR and N=5400 for ACL. The top 5 words with the highest TF-IDF* scores from the MSR data set are: “RFID”, “Cherokee”, “tree-to-string”, “GPU”, and “data-triggered”, whereas the top 5 words selected by RSI are “today”, “work”, “question”, “now”, and “thank”, which are more structurally informative. To estimate the accuracy of the segmentation module, we used Recall, Precision, F-Measure and Pk (Beeferman et al., 1999) as evaluation metric. We used an error window of length 6 to calculate Precision, Recall and F-Measure and a sliding window with a length equal to half of the average seg. (5) � ~Si �� ~Cj � P(Si, Cj) measures the position similarity of two sentences. Position similarity is based on the relative position distance between the sentence and the cluster:Define ~Si· ~Cj 786 Questions Introduction Background Main Topic Conclusion Introduction Background Content Questions Conclusion word tokens word tokens word tokens word tokens word tokens Figure 2: Fully connected 5-state HMM representing Introdu</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Mach. Learn., 34(1-3):177–210, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark J F Gales</author>
<author>Steve J Young</author>
</authors>
<title>The application of hidden markov models in speech recognition.</title>
<date>2007</date>
<booktitle>Foundations and Trends in Signal Processing,</booktitle>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="2861" citStr="Gales and Young, 2007" startWordPosition="409" endWordPosition="412">ture detection and summarization. They showed that linguistic features such as TF-IDF are the most influential in segmentation and summarization and that knowing the structure of a lecture can significantly improve the performance of lecture summarization. Our experiments with a real-time lecture translation system also show that displaying the rolling translation results of a live lecture with proper paragraphing and inserted section titles makes it easier for students to grasp the key points during a lecture. In this paper, we apply existing algorithms, namely the Hidden Markov Model (HMM) (Gales and Young, 2007) to unstructured lecture transcription to infer the underlying structure for better lecture segmentation and summarization. HMM has been successfully applied in early works (van Mulbregt et al., 1998; Sherman and Liu, 2008) for text segmentation, event tracking and boundary detection. The focus of this work is to identify cue words and phrases that are good indicators of lecture structure. Intuitively, words and phrases such as “last week we talked about”, “this is an outline of my talk”, “now I am going to talk about”, “in conclusion”, and “any questions” should be important features to recog</context>
<context position="7836" citStr="Gales and Young, 2007" startWordPosition="1281" endWordPosition="1284">1.03 0.0001 2.12 0.90 0.0001 2.23 0.89 0.0001 3.62 0.80 0.0003 1.53 0.95 0.0003 1.90 0.92 0.0002 2.00 0.91 0.0003 2.51 0.87 0.0009 2.69 0.85 0.0008 1.35 0.96 0.0004 2.22 0.89 0.0005 2.35 0.88 0.0002 2.51 0.86 784 Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams “today we” and “thank you”. “today we” appears more frequently at the beginning of a lecture, whereas “thank you” more in the concluding part of a lecture. The x-axis is the normalized word position in a lecture and y-axis is the probability of seeing the word at a position. Hidden Markov Model (Gales and Young, 2007), the Maximum entropy Markov model (McCallum et al., 2000), the Conditional Random Field (Lafferty et al., 2001) and the Latent Content Analysis (Ponte and Croft, 1997). In this paper, we evaluate the effectiveness of the proposed RSI feature on lecture segmentation using an HMM. We represent each segment in a lecture as a state in the Markov model and use the EM algorithm to learn HMM parameters from unlabeled lecture data. We use a fully connected HMM with five states. Typical state labels for lecture are: “Introduction”, “Background”, “Main Topic”, “Questions” and “Conclusion” as shown in F</context>
</contexts>
<marker>Gales, Young, 2007</marker>
<rawString>Mark J. F. Gales and Steve J. Young. 2007. The application of hidden markov models in speech recognition. Foundations and Trends in Signal Processing, 1(3):195–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="7948" citStr="Lafferty et al., 2001" startWordPosition="1298" endWordPosition="1301">3 2.51 0.87 0.0009 2.69 0.85 0.0008 1.35 0.96 0.0004 2.22 0.89 0.0005 2.35 0.88 0.0002 2.51 0.86 784 Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams “today we” and “thank you”. “today we” appears more frequently at the beginning of a lecture, whereas “thank you” more in the concluding part of a lecture. The x-axis is the normalized word position in a lecture and y-axis is the probability of seeing the word at a position. Hidden Markov Model (Gales and Young, 2007), the Maximum entropy Markov model (McCallum et al., 2000), the Conditional Random Field (Lafferty et al., 2001) and the Latent Content Analysis (Ponte and Croft, 1997). In this paper, we evaluate the effectiveness of the proposed RSI feature on lecture segmentation using an HMM. We represent each segment in a lecture as a state in the Markov model and use the EM algorithm to learn HMM parameters from unlabeled lecture data. We use a fully connected HMM with five states. Typical state labels for lecture are: “Introduction”, “Background”, “Main Topic”, “Questions” and “Conclusion” as shown in Figure 2. HMM states emit word tokens. Instead of considering the full vocabulary as the possible emission alphab</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00,</booktitle>
<pages>591--598</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7894" citStr="McCallum et al., 2000" startWordPosition="1290" endWordPosition="1293">0003 1.53 0.95 0.0003 1.90 0.92 0.0002 2.00 0.91 0.0003 2.51 0.87 0.0009 2.69 0.85 0.0008 1.35 0.96 0.0004 2.22 0.89 0.0005 2.35 0.88 0.0002 2.51 0.86 784 Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams “today we” and “thank you”. “today we” appears more frequently at the beginning of a lecture, whereas “thank you” more in the concluding part of a lecture. The x-axis is the normalized word position in a lecture and y-axis is the probability of seeing the word at a position. Hidden Markov Model (Gales and Young, 2007), the Maximum entropy Markov model (McCallum et al., 2000), the Conditional Random Field (Lafferty et al., 2001) and the Latent Content Analysis (Ponte and Croft, 1997). In this paper, we evaluate the effectiveness of the proposed RSI feature on lecture segmentation using an HMM. We represent each segment in a lecture as a state in the Markov model and use the EM algorithm to learn HMM parameters from unlabeled lecture data. We use a fully connected HMM with five states. Typical state labels for lecture are: “Introduction”, “Background”, “Main Topic”, “Questions” and “Conclusion” as shown in Figure 2. HMM states emit word tokens. Instead of consideri</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, pages 591–598, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>Text segmentation by topic.</title>
<date>1997</date>
<booktitle>In ECDL,</booktitle>
<pages>113--125</pages>
<contexts>
<context position="8004" citStr="Ponte and Croft, 1997" startWordPosition="1307" endWordPosition="1310">2 0.89 0.0005 2.35 0.88 0.0002 2.51 0.86 784 Figure 1: Fitted Poisson-distribution of normalized positions in lectures for the bigrams “today we” and “thank you”. “today we” appears more frequently at the beginning of a lecture, whereas “thank you” more in the concluding part of a lecture. The x-axis is the normalized word position in a lecture and y-axis is the probability of seeing the word at a position. Hidden Markov Model (Gales and Young, 2007), the Maximum entropy Markov model (McCallum et al., 2000), the Conditional Random Field (Lafferty et al., 2001) and the Latent Content Analysis (Ponte and Croft, 1997). In this paper, we evaluate the effectiveness of the proposed RSI feature on lecture segmentation using an HMM. We represent each segment in a lecture as a state in the Markov model and use the EM algorithm to learn HMM parameters from unlabeled lecture data. We use a fully connected HMM with five states. Typical state labels for lecture are: “Introduction”, “Background”, “Main Topic”, “Questions” and “Conclusion” as shown in Figure 2. HMM states emit word tokens. Instead of considering the full vocabulary as the possible emission alphabet, which usually leads to model over-fitting, we only c</context>
</contexts>
<marker>Ponte, Croft, 1997</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1997. Text segmentation by topic. In ECDL, pages 113–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melissa Sherman</author>
<author>Yang Liu</author>
</authors>
<title>Using hidden markov models for topic segmentation of meeting transcripts.</title>
<date>2008</date>
<booktitle>In SLT,</booktitle>
<pages>185--188</pages>
<contexts>
<context position="3084" citStr="Sherman and Liu, 2008" startWordPosition="444" endWordPosition="447">ormance of lecture summarization. Our experiments with a real-time lecture translation system also show that displaying the rolling translation results of a live lecture with proper paragraphing and inserted section titles makes it easier for students to grasp the key points during a lecture. In this paper, we apply existing algorithms, namely the Hidden Markov Model (HMM) (Gales and Young, 2007) to unstructured lecture transcription to infer the underlying structure for better lecture segmentation and summarization. HMM has been successfully applied in early works (van Mulbregt et al., 1998; Sherman and Liu, 2008) for text segmentation, event tracking and boundary detection. The focus of this work is to identify cue words and phrases that are good indicators of lecture structure. Intuitively, words and phrases such as “last week we talked about”, “this is an outline of my talk”, “now I am going to talk about”, “in conclusion”, and “any questions” should be important features to recognize lecture structure. 783 Proceedings of NAACL-HLT 2013, pages 783–788, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics These words/phrases, however, may not be so important content-wise</context>
</contexts>
<marker>Sherman, Liu, 2008</marker>
<rawString>Melissa Sherman and Yang Liu. 2008. Using hidden markov models for topic segmentation of meeting transcripts. In SLT, pages 185–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul van Mulbregt</author>
<author>Ira Carp</author>
<author>Lawrence Gillick</author>
<author>Steve Lowe</author>
<author>Jon Yamron</author>
</authors>
<title>Text segmentation and topic tracking on broadcast news via a hidden markov model approach.</title>
<date>1998</date>
<booktitle>In ICSLP.</booktitle>
<marker>van Mulbregt, Carp, Gillick, Lowe, Yamron, 1998</marker>
<rawString>Paul van Mulbregt, Ira Carp, Lawrence Gillick, Steve Lowe, and Jon Yamron. 1998. Text segmentation and topic tracking on broadcast news via a hidden markov model approach. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Jian Zhang</author>
<author>Ricky Ho Yin Chan</author>
<author>Pascale Fung</author>
</authors>
<title>Improving lecture speech summarization using rhetorical information.</title>
<date>2007</date>
<booktitle>In ASRU,</booktitle>
<pages>195--200</pages>
<contexts>
<context position="2167" citStr="Zhang et al. (2007" startWordPosition="302" endWordPosition="305"> converts the audio to text, lecture segmentation which inserts paragraph boundaries and adds section titles to the lecture transcriptions, automatic summarization that generates a short summary from 1http://www.thebestcolleges.org/ free-online-classes-and-course-lectures/ the full lecture, and lecture translation that translates the lecture from the original language to the native language of the student. The transcription of a lecture generated by the ASR system is a sequence of words which does not contain any structural information such as paragraph, section boundaries and section titles. Zhang et al. (2007; 2008; 2010) used acoustic and linguistic features for rhetorical structure detection and summarization. They showed that linguistic features such as TF-IDF are the most influential in segmentation and summarization and that knowing the structure of a lecture can significantly improve the performance of lecture summarization. Our experiments with a real-time lecture translation system also show that displaying the rolling translation results of a live lecture with proper paragraphing and inserted section titles makes it easier for students to grasp the key points during a lecture. In this pap</context>
</contexts>
<marker>Zhang, Chan, Fung, 2007</marker>
<rawString>Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung. 2007. Improving lecture speech summarization using rhetorical information. In ASRU, pages 195–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Jian Zhang</author>
<author>Shilei Huang</author>
<author>Pascale Fung</author>
</authors>
<title>Rshmm++ for extractive lecture speech summarization.</title>
<date>2008</date>
<booktitle>In SLT,</booktitle>
<pages>161--164</pages>
<marker>Zhang, Huang, Fung, 2008</marker>
<rawString>Justin Jian Zhang, Shilei Huang, and Pascale Fung. 2008. Rshmm++ for extractive lecture speech summarization. In SLT, pages 161–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Jian Zhang</author>
<author>Ricky Ho Yin Chan</author>
<author>Pascale Fung</author>
</authors>
<title>Extractive speech summarization using shallow rhetorical structure modeling.</title>
<date>2010</date>
<journal>IEEE Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>18</volume>
<issue>6</issue>
<pages>1157</pages>
<marker>Zhang, Chan, Fung, 2010</marker>
<rawString>Justin Jian Zhang, Ricky Ho Yin Chan, and Pascale Fung. 2010. Extractive speech summarization using shallow rhetorical structure modeling. IEEE Transactions on Audio, Speech &amp; Language Processing, 18(6):1147– 1157.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>