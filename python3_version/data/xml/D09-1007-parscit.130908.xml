<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.98652">
Cube Pruning as Heuristic Search
</title>
<author confidence="0.941102">
Mark Hopkins and Greg Langmead
</author>
<affiliation confidence="0.7764255">
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
</affiliation>
<address confidence="0.690649">
Marina del Rey, CA 90292
</address>
<email confidence="0.999207">
{mhopkins,glangmead}@languageweaver.com
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919428571429">
Cube pruning is a fast inexact method for
generating the items of a beam decoder.
In this paper, we show that cube pruning
is essentially equivalent to A* search on a
specific search space with specific heuris-
tics. We use this insight to develop faster
and exact variants of cube pruning.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997739102564102">
In recent years, an intense research focus on ma-
chine translation (MT) has raised the quality of
MT systems to the degree that they are now viable
for a variety of real-world applications. Because
of this, the research community has turned its at-
tention to a major drawback of such systems: they
are still quite slow. Recent years have seen a flurry
of innovative techniques designed to tackle this
problem. These include cube pruning (Chiang,
2007), cube growing (Huang and Chiang, 2007),
early pruning (Moore and Quirk, 2007), clos-
ing spans (Roark and Hollingshead, 2008; Roark
and Hollingshead, 2009), coarse-to-fine methods
(Petrov et al., 2008), pervasive laziness (Pust and
Knight, 2009), and many more.
This massive interest in speed is bringing rapid
progress to the field, but it comes with a certain
amount of baggage. Each technique brings its own
terminology (from the cubes of (Chiang, 2007)
to the lazy lists of (Pust and Knight, 2009)) into
the mix. Often, it is not entirely clear why they
work. Many apply only to specialized MT situ-
ations. Without a deeper understanding of these
methods, it is difficult for the practitioner to com-
bine them and adapt them to new use cases.
In this paper, we attempt to bring some clarity
to the situation by taking a closer look at one of
these existing methods. Specifically, we cast the
popular technique of cube pruning (Chiang, 2007)
in the well-understood terms of heuristic search
(Pearl, 1984). We show that cube pruning is essen-
tially equivalent to A* search on a specific search
space with specific heuristics. This simple obser-
vation affords a deeper insight into how and why
cube pruning works. We show how this insight en-
ables us to easily develop faster and exact variants
of cube pruning for tree-to-string transducer-based
MT (Galley et al., 2004; Galley et al., 2006; DeN-
ero et al., 2009).
</bodyText>
<sectionHeader confidence="0.975395" genericHeader="introduction">
2 Motivating Example
</sectionHeader>
<bodyText confidence="0.999252">
We begin by describing the problem that cube
pruning addresses. Consider a synchronous
context-free grammar (SCFG) that includes the
following rules:
</bodyText>
<equation confidence="0.99803725">
A —* (A 0 B 1 , A 0 B 1 ) (1)
B —* (A 0 B 1 , B 1 A 0 ) (2)
A —* (B 0 A 1 , c B 0 b A 1 ) (3)
B —* (B 0 A 1 , B 0 A 1 ) (4)
</equation>
<bodyText confidence="0.99910735">
Figure 1 shows CKY decoding in progress. CKY
is a bottom-up algorithm that works by building
objects known as items, over increasingly larger
spans of an input sentence (in the context of SCFG
decoding, the items represent partial translations
of the input sentence). To limit running time, it is
common practice to keep only the n “best” items
per span (this is known as beam decoding). At
this point in Figure 1, every span of size 2 or less
has already been filled, and now we want to fill
span [2, 5] with the n items of lowest cost. Cube
pruning addresses the problem of how to compute
the n-best items efficiently.
We can be more precise if we introduce some
terminology. An SCFG rule has the form X —*
(a, cp, —), where X is a nonterminal (called the
postcondition), a, cp are strings that may contain
terminals and nonterminals, and — is a 1-1 corre-
spondence between equivalent nonterminals of a
and cp.
</bodyText>
<page confidence="0.99091">
62
</page>
<note confidence="0.9980765">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 62–71,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.7574665">
Figure 1: CKY decoding in progress. We want to
fill span [2,5] with the lowest cost items.
</figureCaption>
<bodyText confidence="0.9991849">
Usually SCFG rules are represented like the ex-
ample rules (1)-(4). The subscripts indicate cor-
responding nonterminals (according to —). Define
the preconditions of a rule as the ordered sequence
of its nonterminals. For clarity of presentation, we
will henceforth restrict our focus to binary rules,
i.e. rules of the form: Z —* (X 0 Y 1 , ϕ). Observe
that all the rules of our example are binary rules.
An item is a triple that contains a span and two
strings. We refer to these strings as the postcon-
dition and the carry, respectively. The postcon-
dition tells us which rules may be applied to the
item. The carry gives us extra information re-
quired to correctly score the item (in SCFG decod-
ing, typically it consists of boundary words for an
n-gram language model). 1 To flatten the notation,
we will generally represent items as a 4-tuple, e.g.
[2, 4, X, a o b].
In CKY, new items are created by applying rules
to existing items:
</bodyText>
<equation confidence="0.993782666666667">
r : Z —* (X 0 Y 1 ,ϕ) [α, δ, X, κ1] [δ, β, Y, κ2]
[α, β, Z, carry(r, κ1, κ2)]
(5)
</equation>
<bodyText confidence="0.971477222222222">
In other words, we are allowed to apply a
rule r to a pair of items ι1, ι2 if the item
spans are complementary and preconditions(r) =
(postcondition(ι1), postcondition(ι2)). The new
item has the same postcondition as the applied
rule. We form the carry for the new item through
an application-dependent function carry that com-
bines the carries of its subitems (e.g. if the carry is
n-gram boundary words, then carry computes the
</bodyText>
<footnote confidence="0.6770295">
1Note that the carry is a generic concept that can store any
kind of non-local scoring information.
</footnote>
<bodyText confidence="0.997581166666667">
new boundary words). As a shorthand, we intro-
duce the notation ι1 &gt; r &lt; ι2 to describe an item
created by applying formula (5) to rule r and items
ι1, ι2.
When we create a new item, it is scored using
the following formula: 2
</bodyText>
<equation confidence="0.99931225">
cost(ι1 &gt; r &lt; ι2) ° cost(r)
+ cost(ι1)
+ cost(ι2)
+ interaction(r, κ1, κ2)
</equation>
<bodyText confidence="0.9998915">
We assume that each grammar rule r has an
associated cost, denoted cost(r). The interac-
tion cost, denoted interaction(r, κ1, κ2), uses the
carry information to compute cost components
that cannot be incorporated offline into the rule
costs (again, for our purposes, this is a language
model score).
Cube pruning addresses the problem of effi-
ciently computing the n items of lowest cost for
a given span.
</bodyText>
<sectionHeader confidence="0.960983" genericHeader="method">
3 Item Generation as Heuristic Search
</sectionHeader>
<bodyText confidence="0.998892769230769">
Refer again to the example in Figure 1. We want to
fill span [2,5]. There are 26 distinct ways to apply
formula (5), which result in 10 unique items. One
approach to finding the lowest-cost n items: per-
form all 26 distinct inferences, compute the cost of
the 10 unique items created, then choose the low-
est n.
The 26 different ways to form the items can be
structured as a search tree. See Figure 2. First
we choose the subspans, then the rule precondi-
tions, then the rule, and finally the subitems. No-
tice that this search space is already quite large,
even for such a simple example. In a realistic situ-
ation, we are likely to have a search tree with thou-
sands (possibly millions) of nodes, and we may
only want to find the best 100 or so goal nodes.
To explore this entire search space seems waste-
ful. Can we do better?
Why not perform heuristic search directly on
this search space to find the lowest-cost n items?
In order to do this, we just need to add heuristics
to the internal nodes of the space.
Before doing so, it will help to elaborate on
some of the details of the search tree. Let
rules(X, Y) be the subset of rules with precondi-
tions (X, Y), sorted by increasing cost. Similarly,
</bodyText>
<footnote confidence="0.709397333333333">
2Without loss of generality, we assume an additive cost
function.
(6)
</footnote>
<page confidence="0.983851">
63
</page>
<figureCaption confidence="0.97794">
Figure 2: Item creation, structured as a search
</figureCaption>
<bodyText confidence="0.942586">
space. rule(X, Y, k) denotes the kth lowest-cost
rule with preconditions (X, Y). item(α, Q, X, k)
denotes the kth lowest-cost item of span [α, Q]
with postcondition X.
let items(α, Q, X) be the subset of items with span
[α, Q] and postcondition X, also sorted by increas-
ing cost. Finally, let rule(X, Y, k) denote the kth
rule of rules(X, Y) and let item(α, Q, X, k) denote
the kth item of items(α, Q, X).
A path through the search tree consists of the
following sequence of decisions:
</bodyText>
<listItem confidence="0.981283823529412">
1. Set i, j, k to 1.
2. Choose the subspans: [α, S], [S, Q].
3. Choose the first precondition X of the rule.
4. Choose the second precondition Y of the
rule.
5. While rule not yet accepted and i &lt;
|rules(X, Y)|:
(a) Choose to accept/reject rule(X, Y, i). If
reject, then increment i.
6. While item not yet accepted for subspan
[α, S] and j &lt; |items(α, S, X)|:
(a) Choose to accept/reject item(α, S, X, j).
If reject, then increment j.
7. While item not yet accepted for subspan [S, Q]
and k &lt; |items(S,Q, Y)|:
(a) Choose to accept/reject item(S, Q, Y, k).
If reject, then increment k.
</listItem>
<figureCaption confidence="0.7709545">
Figure 3: The lookahead heuristic. We set the
heuristics for rule and item nodes by looking
ahead at the cost of the greedy solution from that
point in the search space.
</figureCaption>
<bodyText confidence="0.993981161290322">
Figure 2 shows two complete search paths for
our example, terminated by goal nodes (in black).
Notice that the internal nodes of the search space
can be classified by the type of decision they
govern. To distinguish between these nodes, we
will refer to them as subspan nodes, precondition
nodes, rule nodes, and item nodes.
We can now proceed to attach heuristics to the
nodes and run a heuristic search protocol, say A*,
on this search space. For subspan and precondition
nodes, we attach trivial uninformative heuristics,
i.e. h = −oc. For goal nodes, the heuristic is the
actual cost of the item they represent. For rule and
item nodes, we will use a simple type of heuristic,
often referred to in the literature as a lookahead
heuristic. Since the rule nodes and item nodes are
ordered, respectively, by rule and item cost, it is
possible to “look ahead” at a greedy solution from
any of those nodes. See Figure 3. This greedy so-
lution is reached by choosing to accept every de-
cision presented until we hit a goal node.
If these heuristics were admissible (i.e. lower
bounds on the cost of the best reachable goal
node), this would enable us to exactly generate the
n-best items without exhausting the search space
(assuming the heuristics are strong enough for A*
to do some pruning). Here, the lookahead heuris-
tics are clearly not admissible, however the hope
is that A* will generate n “good” items, and that
the time savings will be worth sacrificing exact-
ness for.
</bodyText>
<page confidence="0.998539">
64
</page>
<sectionHeader confidence="0.905607" genericHeader="method">
4 Cube Pruning as Heuristic Search
</sectionHeader>
<bodyText confidence="0.9998935">
In this section, we will compare cube pruning with
our A* search protocol, by tracing through their
respective behaviors on the simple example of Fig-
ure 1.
</bodyText>
<subsectionHeader confidence="0.998052">
4.1 Phase 1: Initialization
</subsectionHeader>
<bodyText confidence="0.999849">
To fill span [α, β], cube pruning (CP) begins by
constructing a cube for each tuple of the form:
</bodyText>
<equation confidence="0.514233">
([α, δ], [δ, β],X , Y)
</equation>
<bodyText confidence="0.99997585">
where X and Y are nonterminals. A cube consists
of three axes: rules(X, Y) and items(α, δ, X) and
items(δ, β, Y). Figure 4(left) shows the nontrivial
cubes for our example scenario.
Contrast this with A*, which begins by adding
the root node of our search space to an empty heap
(ordered by heuristic cost). It proceeds to repeat-
edly pop the lowest-cost node from the heap, then
add its children to the heap (we refer to this op-
eration as visiting the node). Note that before A*
ever visits a rule node, it will have visited every
subspan and precondition node (because they all
have cost h = −oc). Figure 4(right) shows the
state of A* at this point in the search. We assume
that we do not generate dead-end nodes (a simple
matter of checking that there exist applicable rules
and items for the chosen subspans and precondi-
tions). Observe the correspondence between the
cubes and the heap contents at this point in the A*
search.
</bodyText>
<subsectionHeader confidence="0.980364">
4.2 Phase 2: Seeding the Heap
</subsectionHeader>
<bodyText confidence="0.9984684375">
Cube pruning proceeds by computing the “best”
item of each cube ([α, δ], [δ, β], X , Y), i.e.
item(α, δ, X,1) &gt; rule(X, Y, 1) &lt; item(δ,β, Y, 1)
Because of the interaction cost, there is no guaran-
tee that this will really be the best item of the cube,
however it is likely to be a good item because the
costs of the individual components are low. These
items are added to a heap (to avoid confusion, we
will henceforth refer to the two heaps as the CP
heap and the A* heap), and prioritized by their
costs.
Consider again the example. CP seeds its heap
with the “best” items of the 4 cubes. There is now
a direct correspondence between the CP heap and
the A* heap. Moreover, the costs associated with
the heap elements also correspond. See Figure 5.
</bodyText>
<subsectionHeader confidence="0.966297">
4.3 Phase 3: Finding the First Item
</subsectionHeader>
<bodyText confidence="0.935284571428571">
Cube pruning now pops the lowest-cost item from
the CP heap. This means that CP has decided to
keep the item. After doing so, it forms the “one-
off” items and pushes those onto the CP heap. See
Figure 5(left). The popped item is:
item (viii) &gt; rule (1) &lt; item (xii)
CP then pushes the following one-off successors
onto the CP heap:
item (viii) &gt; rule (2) &lt; item (xii)
item (ix) &gt; rule (1) &lt; item (xii)
item (viii) &gt; rule (1) &lt; item (xiii)
Contrast this with A*, which pops the lowest-
cost search node from the A* heap. Here we need
to assume that our A* protocol differs slightly
from standard A*. Specifically, it will practice
node-tying, meaning that when it visits a rule node
or an item node, then it also (atomically) visits all
nodes on the path to its lookahead goal node. See
Figure 5(right). Observe that all of these nodes
have the same heuristic cost, thus standard A* is
likely to visit these nodes in succession without
the need to enforce node-tying, but it would not
be guaranteed (because the heuristics are not ad-
missible). A* keeps the goal node it finds and adds
the successors to the heap, scored with their looka-
head heuristics. Again, note the direct correspon-
dence between what CP and A* keep, and what
they add to their respective heaps.
</bodyText>
<subsectionHeader confidence="0.992747">
4.4 Phase 4: Finding Subsequent Items
</subsectionHeader>
<bodyText confidence="0.9999228125">
Cube pruning and A* continue to repeat Phase
3 until k unique items have been kept. While
we could continue to trace through the example,
by now it should be clear: cube pruning and our
A* protocol with node-tying are doing the same
thing at each step. In fact, they are exactly the
same algorithm. We do not present a formal proof
here; this statement should be regarded as confi-
dent conjecture.
The node-tying turns out to be an unnecessary
artifact. In our early experiments, we discovered
that node-tying has no impact on speed or qual-
ity. Hence, for the remainder of the paper, we
view cube pruning in very simple terms: as noth-
ing more than standard A* search on the search
space of Section 3.
</bodyText>
<page confidence="0.998998">
65
</page>
<figureCaption confidence="0.991817">
Figure 4: (left) Cube formation for our example. (right) The A* protocol, after all subspan and precon-
dition nodes have been visited. Notice the correspondence between the cubes and the A* heap contents.
Figure 5: (left) One step of cube pruning. (right) One step of the A* protocol. In this figure,
cost(r, G1, G2) °= cost(G1 ⋗ r ⋖ G2).
</figureCaption>
<page confidence="0.942515">
66
</page>
<sectionHeader confidence="0.992971" genericHeader="method">
5 Augmented Cube Pruning
</sectionHeader>
<bodyText confidence="0.9998017">
Viewed in this light, the idiosyncracies of cube
pruning begin to reveal themselves. On the one
hand, rule and item nodes are associated with
strong but inadmissible heuristics (the short expla-
nation for why cube pruning is an inexact algo-
rithm). On the other hand, subspan and precondi-
tion nodes are associated with weak trivial heuris-
tics. This should be regarded neither as a surprise
nor a criticism, considering cube pruning’s origins
in hierarchical phrase-based MT models (Chiang,
2007), which have only a small number of distinct
nonterminals.
But the situation is much different in tree-
to-string transducer-based MT (Galley et al.,
2004; Galley et al., 2006; DeNero et al., 2009).
Transducer-based MT relies on SCFGs with large
nonterminal sets. Binarizing the grammars (Zhang
et al., 2006) further increases the size of these sets,
due to the introduction of virtual nonterminals.
A key benefit of the heuristic search viewpoint
is that it is well positioned to take advantage of
such insights into the structure of a particular de-
coding problem. In the case of transducer-based
MT, the large set of preconditions encourages us
to introduce a nontrivial heuristic for the precon-
dition nodes. The inclusion of these heuristics into
the CP search will enable A* to eliminate cer-
tain preconditions from consideration, giving us a
speedup. For this reason we call this strategy aug-
mented cube pruning.
</bodyText>
<equation confidence="0.8902015">
h(S, X, Y) = cost(rule(X, Y,1))
+ cost(item(α, S, X,1))
+ cost(item(S, Q, Y,1))
+ ih(S, X, Y)
</equation>
<bodyText confidence="0.99994752631579">
The first three terms are admissible because
each is simply the minimum possible cost of
some choice remaining to be made. To con-
struct the interaction heuristic ih(S, X, Y), con-
sider that in a translation model with an inte-
grated n-gram language model, the interaction
cost interaction(r, K1, K2) is computed by adding
the language model costs of any new complete n-
grams that are created by combining the carries
(boundary words) with each other and with the
lexical items on the rule’s target side, taking into
account any reordering that the rule may perform.
We construct a backoff-style estimate of these
new n-grams by looking at item(α, S, X,1) =
[α, S, X, K1], item(S, Q, Y, 1) = [S, Q, Y, K2], and
rule(X, Y, 1). We set ih(S, X, Y) to be a linear
combination of the backoff n-grams of the carries
K1 and K2, as well as any n-grams introduced by
the rule. For instance, if
</bodyText>
<equation confidence="0.997892666666667">
K1 = a b o c d
K2 = e f o g h
rule(X,Y,1) = Z —* (X 0 Y 1 , X 0 g h i Y 1 )
</equation>
<bodyText confidence="0.459984">
then
</bodyText>
<subsectionHeader confidence="0.981606">
5.1 Heuristics on preconditions
</subsectionHeader>
<bodyText confidence="0.999968666666667">
Recall that the total cost of a goal node is given by
Equation (6), which has four terms. We will form
the heuristic for a precondition node by creating
a separate heuristic for each of the four terms and
using the sum as the overall heuristic.
To describe these heuristics, we will make intu-
itive use of the wildcard operator * to extend our
existing notation. For instance, items(α, Q, *) will
denote the union of items(α, Q, X) over all possi-
ble X, sorted by cost.
We associate the heuristic h(S, X, Y) with the
search node reached by choosing subspans [α, S],
[S, Q], precondition X (for span [α, S]), and precon-
dition Y (for span [S, Q]). The heuristic is the sum
of four terms, mirroring Equation (6):
</bodyText>
<equation confidence="0.9989835">
ih(S,X,Y) = &apos;y1 · LM(a) + &apos;y2 · LM(ab)
+ &apos;y1 · LM(e) + &apos;y2 · LM(e f)
+ &apos;y1 · LM(g) + &apos;y2 · LM(g h)
+ &apos;y3 · LM(g h i)
</equation>
<bodyText confidence="0.997864285714286">
The coefficients of the combination are free pa-
rameters that we can tune to trade off between
more pruning and more admissability. Setting the
coefficients to zero gives perfect admissibility but
is also weak.
The heuristic for the first precondition node is
computed similarly:
</bodyText>
<construct confidence="0.6256325">
h(S, X, *) = cost(rule(X, *,1))
+ cost(item(α, S, X, 1))
+ cost(item(S,Q, *,1))
+ ih(S,X,*)
</construct>
<page confidence="0.994762">
67
</page>
<table confidence="0.998693333333333">
Standard CP Augmented CP
nodes (k) BLEU time nodes (k) BLEU time
80 34.9 2.5 52 34.7 1.9
148 36.1 3.9 92 35.9 2.4
345 37.2 7.9 200 37.3 5.4
520 37.7 13.4 302 37.7 8.5
725 38.2 17.1 407 38.0 10.7
1092 38.3 27.1 619 38.2 16.3
1812 38.6 45.9 1064 38.5 27.7
</table>
<tableCaption confidence="0.957397">
Table 1: Results of standard and augmented cube
</tableCaption>
<figureCaption confidence="0.8882654">
pruning. The number of (thousands of) search
nodes visited is given along with BLEU and av-
erage time to decode one sentence, in seconds.
Figure 6: Nodes visited by standard and aug-
mented cube pruning.
</figureCaption>
<bodyText confidence="0.997498">
We also apply analogous heuristics to the subspan
nodes.
</bodyText>
<subsectionHeader confidence="0.99795">
5.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999592928571429">
We evaluated all of the algorithms in this paper on
a syntax-based Arabic-English translation system
based on (Galley et al., 2006), with rules extracted
from 200 million words of parallel data from NIST
2008 and GALE data collections, and with a 4-
gram language model trained on 1 billion words
of monolingual English data from the LDC Giga-
word corpus. We evaluated the system’s perfor-
mance on the NIST 2008 test corpus, which con-
sists of 1357 Arabic sentences from a mixture of
newswire and web domains, with four English ref-
erence translations. We report BLEU scores (Pa-
pineni et al., 2002) on untokenized, recapitalized
output.
</bodyText>
<sectionHeader confidence="0.582486" genericHeader="method">
5.3 Results for Augmented Cube Pruning
</sectionHeader>
<bodyText confidence="0.974305">
The results for augmented cube pruning are com-
pared against cube pruning in Table 1. The data
</bodyText>
<figureCaption confidence="0.9831595">
Figure 7: Time spent by standard and augmented
cube pruning, average seconds per sentence.
</figureCaption>
<table confidence="0.99955225">
Standard CP Augmented CP
subspan 12936 12792
precondition 851458 379954
rule 33734 33331
item 119703 118889
goal 74618 74159
TOTAL 1092449 619125
BLEU 38.33 38.22
</table>
<tableCaption confidence="0.9484955">
Table 2: Breakdown of visited search nodes by
type (for a fixed beam size).
</tableCaption>
<bodyText confidence="0.90856468">
from that table are also plotted in Figure 6 and
Figure 7. Each line gives the number of nodes
visited by the heuristic search, the average time
to decode one sentence, and the BLEU of the out-
put. The number of items kept by each span (the
beam) is increased in each subsequent line of the
table to indicate how the two algorithms differ at
various beam sizes. This also gives a more com-
plete picture of the speed/BLEU tradeoff offered
by each algorithm. Because the two algorithms
make the same sorts of lookahead computations
with the same implementation, they can be most
directly compared by examining the number of
visited nodes. Augmenting cube pruning with ad-
missible heuristics on the precondition nodes leads
to a substantial decrease in visited nodes, by 35-
44%. The reduction in nodes converges to a con-
sistent 40% as the beam increases. The BLEU
with augmented cube pruning drops by an average
of 0.1 compared to standard cube pruning. This is
due to the additional inadmissibility of the interac-
tion heuristic.
To see in more detail how the heuristics affect
the search, we give in Table 2 the number of nodes
of each type visited by both variants for one beam
</bodyText>
<page confidence="0.998578">
68
</page>
<bodyText confidence="0.998985">
size. The precondition heuristic enables A* to
prune more than half the precondition nodes.
</bodyText>
<sectionHeader confidence="0.979169" genericHeader="method">
6 Exact Cube Pruning
</sectionHeader>
<bodyText confidence="0.999905068965517">
Common wisdom is that the speed of cube prun-
ing more than compensates for its inexactness (re-
call that this inexactness is due to the fact that it
uses A* search with inadmissible heuristics). Es-
pecially when we move into transducer-based MT,
the search space becomes so large that brute-force
item generation is much too slow to be practi-
cal. Still, within the heuristic search framework
we may ask the question: is it possible to apply
strictly admissible heuristics to the cube pruning
search space, and in so doing, create a version of
cube pruning that is both fast and exact, one that
finds the n best items for each span and not just
n good items? One might not expect such a tech-
nique to outperform cube pruning in practice, but
for a given use case, it would give us a relatively
fast way of assessing the BLEU drop incurred by
the inexactness of cube pruning.
Recall again that the total cost of a goal node
is given by Equation (6), which has four terms. It
is easy enough to devise strong lower bounds for
the first three of these terms by extending the rea-
soning of Section 5. Table 3 shows these heuris-
tics. The major challenge is to devise an effective
lower bound on the fourth term of the cost func-
tion, the interaction heuristic, which in our case is
the incremental language model cost.
We take advantage of the following observa-
tions:
</bodyText>
<listItem confidence="0.515242375">
1. In a given span, many boundary word pat-
terns are repeated. In other words, for a par-
ticular span [α, Q] and carry κ, we often see
many items of the form [α, Q, X, κ], where
the only difference is the postcondition X.
2. Most rules do not introduce lexical items. In
other words, most of the grammar rules have
the form Z —* (X0 Y1, X0 Y1) (concatena-
</listItem>
<bodyText confidence="0.966480125">
tion rules) or Z —* (X0 Y1, Y1 X0) (inver-
sion rules).
The idea is simple. We split the search into three
searches: one for concatenation rules, one for in-
version rules, and one for lexical rules. Each
search finds the n–best items that can be created
using its respective set of rules. We then take these
3n items and keep the best n.
</bodyText>
<figureCaption confidence="0.9461985">
Figure 8: Time spent by standard and exact cube
pruning, average seconds per sentence.
</figureCaption>
<bodyText confidence="0.999878583333333">
Doing this split enables us to precompute a
strong and admissible heuristic on the interaction
cost. Namely, for a given span [α, Q], we pre-
compute ihadm(S, X, Y), which is the best LM
cost of combining carries from items(α, S, X)
and items(S, Q, Y). Notice that this statistic is
only straightforward to compute once we can as-
sume that the rules are concatenation rules or
inversion rules. For the lexical rules, we set
ihadm(S, X, Y) = 0, an admissible but weak
heuristic that we can fortunately get away with be-
cause of the small number of lexical rules.
</bodyText>
<sectionHeader confidence="0.857606" genericHeader="evaluation">
6.1 Results for Exact Cube Pruning
</sectionHeader>
<bodyText confidence="0.999963285714286">
Computing the ihadm(S, X, Y) heuristic is not
cheap. To be fair, we first compare exact CP to
standard CP in terms of overall running time, in-
cluding the computational cost of this overhead.
We plot this comparison in Figure 8. Surprisingly,
the time/quality tradeoff of exact CP is extremely
similar to standard CP, suggesting that exact cube
pruning is actually a practical alternative to stan-
dard CP, and not just of theoretical value. We
found that the BLEU loss of standard cube prun-
ing at moderate beam sizes was between 0.4 and
0.6.
Another surprise comes when we contrast the
number of visited search nodes of exact CP and
standard CP. See Figure 9. While we initially ex-
pected that exact CP must visit fewer nodes to
make up for the computational overhead of its ex-
pensive heuristics, this did not turn out to be the
case, suggesting that the computational cost of
standard CP’s lookahead heuristics is just as ex-
pensive as the precomputation of ihadm(S, X, Y).
</bodyText>
<page confidence="0.996905">
69
</page>
<table confidence="0.998055428571429">
heuristic components
subspan precondition1 precondition2 rule item1 item2
h(S) h(S, X) h(S, X, Y) h(S, X, Y, i) h(S, X, Y, i, j) h(S, X, Y, i, j, k)
r rule(*, *,1) rule(X, *,1) rule(X, Y,1) rule(X, Y, i) rule(X, Y, i) rule(X, Y, i)
t1 item(α, S, *,1) item(α, S, X,1) item(α, S, X,1) item(α, S, X,1) item(α, S, X, j) item(α, S, X, j)
t2 item(S, (3, *, 1) item(S, (3, *,1) item(S, (3, Y,1) item(S, (3, Y, 1) item(S, (3, Y,1) item(S, (3, Y, k)
ih ihadm(S, *, *) ihadm(S, X, *) ihadm(S, X, Y) ihadm(S, X, Y) ihadm(S, X, Y) ihadm(S, X, Y)
</table>
<tableCaption confidence="0.80591075">
Table 3: Admissible heuristics for exact CP. We attach heuristic h(S, X, Y, i, j, k) to the search node
reached by choosing subspans [α, S], [S, (3], preconditions X and Y, the ith rule of rules(X, Y), the jth
item of item(α, S, X), and the kth item of item(S, (3, Y). To form the heuristic for a particular type of
search node (column), compute the following: cost(r) + cost(t1) + cost(t2) + ih
</tableCaption>
<figureCaption confidence="0.9912735">
Figure 9: Nodes visited by standard and exact
cube pruning.
</figureCaption>
<sectionHeader confidence="0.998157" genericHeader="conclusions">
7 Implications
</sectionHeader>
<bodyText confidence="0.9998841">
This paper’s core idea is the utility of framing
CKY item generation as a heuristic search prob-
lem. Once we recognize cube pruning as noth-
ing more than A* on a particular search space
with particular heuristics, this deeper understand-
ing makes it easy to create faster and exact vari-
ants for other use cases (in this paper, we focus
on tree-to-string transducer-based MT). Depend-
ing on one’s own particular use case, a variety of
possibilities may present themselves:
</bodyText>
<listItem confidence="0.780528">
1. What if we try different heuristics? In this pa-
</listItem>
<bodyText confidence="0.929824818181818">
per, we do some preliminary inquiry into this
question, but it should be clear that our minor
changes are just the tip of the iceberg. One
can easily imagine clever and creative heuris-
tics that outperform the simple ones we have
proposed here.
2. What if we try a different search space? Why
are we using this particular search space?
Perhaps a different one, one that makes de-
cisions in a different order, would be more
effective.
</bodyText>
<listItem confidence="0.714501">
3. What if we try a different search algorithm?
</listItem>
<bodyText confidence="0.933441166666667">
A* has nice guarantees (Dechter and Pearl,
1985), but it is space-consumptive and it is
not anytime. For a use case where we would
like a finer-grained speed/quality tradeoff, it
might be useful to consider an anytime search
algorithm, like depth-first branch-and-bound
(Zhang and Korf, 1995).
By working towards a deeper and unifying under-
standing of the smorgasbord of current MT speed-
up techniques, our hope is to facilitate the task of
implementing such methods, combining them ef-
fectively, and adapting them to new use cases.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999388571428571">
We would like to thank Abdessamad Echihabi,
Kevin Knight, Daniel Marcu, Dragos Munteanu,
Ion Muslea, Radu Soricut, Wei Wang, and the
anonymous reviewers for helpful comments and
advice. Thanks also to David Chiang for the use
of his LaTeX macros. This work was supported in
part by CCS grant 2008-1245117-000.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998905666666667">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Rina Dechter and Judea Pearl. 1985. Generalized best-
first search strategies and the optimality of a*. Jour-
nal of the ACM, 32(3):505–536.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference.
</reference>
<page confidence="0.967072">
70
</page>
<reference confidence="0.999921553191489">
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings ofHLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic models. In Proceedings of
ACL-COLING.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings ofACL.
Robert C. Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In Proceedings ofMT Summit XI.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
Judea Pearl. 1984. Heuristics. Addison-Wesley.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of EMNLP.
Michael Pust and Kevin Knight. 2009. Faster mt de-
coding through pervasive laziness. In Proceedings
ofNAACL.
Brian Roark and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 745–752.
Brian Roark and Kristy Hollingshead. 2009. Lin-
ear complexity context-free parsing pipelines via
chart constraints. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 647–655,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Weixiong Zhang and Richard E. Korf. 1995. Perfor-
mance of linear-space search algorithms. Artificial
Intelligence, 79(2):241–292.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 256–263.
</reference>
<page confidence="0.999144">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622811">
<title confidence="0.998943">Cube Pruning as Heuristic Search</title>
<author confidence="0.993634">Mark Hopkins</author>
<author confidence="0.993634">Greg</author>
<affiliation confidence="0.869448">Language Weaver,</affiliation>
<address confidence="0.995666">4640 Admiralty Way, Suite</address>
<author confidence="0.723514">Marina del Rey</author>
<author confidence="0.723514">CA</author>
<abstract confidence="0.999352">Cube pruning is a fast inexact method for generating the items of a beam decoder. In this paper, we show that cube pruning is essentially equivalent to A* search on a specific search space with specific heuristics. We use this insight to develop faster and exact variants of cube pruning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="944" citStr="Chiang, 2007" startWordPosition="152" endWordPosition="153">uivalent to A* search on a specific search space with specific heuristics. We use this insight to develop faster and exact variants of cube pruning. 1 Introduction In recent years, an intense research focus on machine translation (MT) has raised the quality of MT systems to the degree that they are now viable for a variety of real-world applications. Because of this, the research community has turned its attention to a major drawback of such systems: they are still quite slow. Recent years have seen a flurry of innovative techniques designed to tackle this problem. These include cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), early pruning (Moore and Quirk, 2007), closing spans (Roark and Hollingshead, 2008; Roark and Hollingshead, 2009), coarse-to-fine methods (Petrov et al., 2008), pervasive laziness (Pust and Knight, 2009), and many more. This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they work. Many apply only to specialized MT sit</context>
<context position="15076" citStr="Chiang, 2007" startWordPosition="2721" endWordPosition="2722"> (right) One step of the A* protocol. In this figure, cost(r, G1, G2) °= cost(G1 ⋗ r ⋖ G2). 66 5 Augmented Cube Pruning Viewed in this light, the idiosyncracies of cube pruning begin to reveal themselves. On the one hand, rule and item nodes are associated with strong but inadmissible heuristics (the short explanation for why cube pruning is an inexact algorithm). On the other hand, subspan and precondition nodes are associated with weak trivial heuristics. This should be regarded neither as a surprise nor a criticism, considering cube pruning’s origins in hierarchical phrase-based MT models (Chiang, 2007), which have only a small number of distinct nonterminals. But the situation is much different in treeto-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). Transducer-based MT relies on SCFGs with large nonterminal sets. Binarizing the grammars (Zhang et al., 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. A key benefit of the heuristic search viewpoint is that it is well positioned to take advantage of such insights into the structure of a particular decoding problem. In the case of transducer-based MT, </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rina Dechter</author>
<author>Judea Pearl</author>
</authors>
<title>Generalized bestfirst search strategies and the optimality of a*.</title>
<date>1985</date>
<journal>Journal of the ACM,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="26995" citStr="Dechter and Pearl, 1985" startWordPosition="4874" endWordPosition="4877">iety of possibilities may present themselves: 1. What if we try different heuristics? In this paper, we do some preliminary inquiry into this question, but it should be clear that our minor changes are just the tip of the iceberg. One can easily imagine clever and creative heuristics that outperform the simple ones we have proposed here. 2. What if we try a different search space? Why are we using this particular search space? Perhaps a different one, one that makes decisions in a different order, would be more effective. 3. What if we try a different search algorithm? A* has nice guarantees (Dechter and Pearl, 1985), but it is space-consumptive and it is not anytime. For a use case where we would like a finer-grained speed/quality tradeoff, it might be useful to consider an anytime search algorithm, like depth-first branch-and-bound (Zhang and Korf, 1995). By working towards a deeper and unifying understanding of the smorgasbord of current MT speedup techniques, our hope is to facilitate the task of implementing such methods, combining them effectively, and adapting them to new use cases. Acknowledgments We would like to thank Abdessamad Echihabi, Kevin Knight, Daniel Marcu, Dragos Munteanu, Ion Muslea, </context>
</contexts>
<marker>Dechter, Pearl, 1985</marker>
<rawString>Rina Dechter and Judea Pearl. 1985. Generalized bestfirst search strategies and the optimality of a*. Journal of the ACM, 32(3):505–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Mohit Bansal</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Efficient parsing for transducer grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference.</booktitle>
<contexts>
<context position="2352" citStr="DeNero et al., 2009" startWordPosition="386" endWordPosition="390">ity to the situation by taking a closer look at one of these existing methods. Specifically, we cast the popular technique of cube pruning (Chiang, 2007) in the well-understood terms of heuristic search (Pearl, 1984). We show that cube pruning is essentially equivalent to A* search on a specific search space with specific heuristics. This simple observation affords a deeper insight into how and why cube pruning works. We show how this insight enables us to easily develop faster and exact variants of cube pruning for tree-to-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). 2 Motivating Example We begin by describing the problem that cube pruning addresses. Consider a synchronous context-free grammar (SCFG) that includes the following rules: A —* (A 0 B 1 , A 0 B 1 ) (1) B —* (A 0 B 1 , B 1 A 0 ) (2) A —* (B 0 A 1 , c B 0 b A 1 ) (3) B —* (B 0 A 1 , B 0 A 1 ) (4) Figure 1 shows CKY decoding in progress. CKY is a bottom-up algorithm that works by building objects known as items, over increasingly larger spans of an input sentence (in the context of SCFG decoding, the items represent partial translations of the input sentence). To limit running time, it is common</context>
<context position="15271" citStr="DeNero et al., 2009" startWordPosition="2751" endWordPosition="2754">veal themselves. On the one hand, rule and item nodes are associated with strong but inadmissible heuristics (the short explanation for why cube pruning is an inexact algorithm). On the other hand, subspan and precondition nodes are associated with weak trivial heuristics. This should be regarded neither as a surprise nor a criticism, considering cube pruning’s origins in hierarchical phrase-based MT models (Chiang, 2007), which have only a small number of distinct nonterminals. But the situation is much different in treeto-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). Transducer-based MT relies on SCFGs with large nonterminal sets. Binarizing the grammars (Zhang et al., 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. A key benefit of the heuristic search viewpoint is that it is well positioned to take advantage of such insights into the structure of a particular decoding problem. In the case of transducer-based MT, the large set of preconditions encourages us to introduce a nontrivial heuristic for the precondition nodes. The inclusion of these heuristics into the CP search will enable A* to eliminate certa</context>
</contexts>
<marker>DeNero, Bansal, Pauls, Klein, 2009</marker>
<rawString>John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. 2009. Efficient parsing for transducer grammars. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="2309" citStr="Galley et al., 2004" startWordPosition="378" endWordPosition="381"> this paper, we attempt to bring some clarity to the situation by taking a closer look at one of these existing methods. Specifically, we cast the popular technique of cube pruning (Chiang, 2007) in the well-understood terms of heuristic search (Pearl, 1984). We show that cube pruning is essentially equivalent to A* search on a specific search space with specific heuristics. This simple observation affords a deeper insight into how and why cube pruning works. We show how this insight enables us to easily develop faster and exact variants of cube pruning for tree-to-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). 2 Motivating Example We begin by describing the problem that cube pruning addresses. Consider a synchronous context-free grammar (SCFG) that includes the following rules: A —* (A 0 B 1 , A 0 B 1 ) (1) B —* (A 0 B 1 , B 1 A 0 ) (2) A —* (B 0 A 1 , c B 0 b A 1 ) (3) B —* (B 0 A 1 , B 0 A 1 ) (4) Figure 1 shows CKY decoding in progress. CKY is a bottom-up algorithm that works by building objects known as items, over increasingly larger spans of an input sentence (in the context of SCFG decoding, the items represent partial translations of the input sen</context>
<context position="15228" citStr="Galley et al., 2004" startWordPosition="2743" endWordPosition="2746">idiosyncracies of cube pruning begin to reveal themselves. On the one hand, rule and item nodes are associated with strong but inadmissible heuristics (the short explanation for why cube pruning is an inexact algorithm). On the other hand, subspan and precondition nodes are associated with weak trivial heuristics. This should be regarded neither as a surprise nor a criticism, considering cube pruning’s origins in hierarchical phrase-based MT models (Chiang, 2007), which have only a small number of distinct nonterminals. But the situation is much different in treeto-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). Transducer-based MT relies on SCFGs with large nonterminal sets. Binarizing the grammars (Zhang et al., 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. A key benefit of the heuristic search viewpoint is that it is well positioned to take advantage of such insights into the structure of a particular decoding problem. In the case of transducer-based MT, the large set of preconditions encourages us to introduce a nontrivial heuristic for the precondition nodes. The inclusion of these heuristics into the </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic models.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING.</booktitle>
<contexts>
<context position="2330" citStr="Galley et al., 2006" startWordPosition="382" endWordPosition="385">pt to bring some clarity to the situation by taking a closer look at one of these existing methods. Specifically, we cast the popular technique of cube pruning (Chiang, 2007) in the well-understood terms of heuristic search (Pearl, 1984). We show that cube pruning is essentially equivalent to A* search on a specific search space with specific heuristics. This simple observation affords a deeper insight into how and why cube pruning works. We show how this insight enables us to easily develop faster and exact variants of cube pruning for tree-to-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). 2 Motivating Example We begin by describing the problem that cube pruning addresses. Consider a synchronous context-free grammar (SCFG) that includes the following rules: A —* (A 0 B 1 , A 0 B 1 ) (1) B —* (A 0 B 1 , B 1 A 0 ) (2) A —* (B 0 A 1 , c B 0 b A 1 ) (3) B —* (B 0 A 1 , B 0 A 1 ) (4) Figure 1 shows CKY decoding in progress. CKY is a bottom-up algorithm that works by building objects known as items, over increasingly larger spans of an input sentence (in the context of SCFG decoding, the items represent partial translations of the input sentence). To limit runn</context>
<context position="15249" citStr="Galley et al., 2006" startWordPosition="2747" endWordPosition="2750">e pruning begin to reveal themselves. On the one hand, rule and item nodes are associated with strong but inadmissible heuristics (the short explanation for why cube pruning is an inexact algorithm). On the other hand, subspan and precondition nodes are associated with weak trivial heuristics. This should be regarded neither as a surprise nor a criticism, considering cube pruning’s origins in hierarchical phrase-based MT models (Chiang, 2007), which have only a small number of distinct nonterminals. But the situation is much different in treeto-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). Transducer-based MT relies on SCFGs with large nonterminal sets. Binarizing the grammars (Zhang et al., 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. A key benefit of the heuristic search viewpoint is that it is well positioned to take advantage of such insights into the structure of a particular decoding problem. In the case of transducer-based MT, the large set of preconditions encourages us to introduce a nontrivial heuristic for the precondition nodes. The inclusion of these heuristics into the CP search will enable</context>
<context position="18992" citStr="Galley et al., 2006" startWordPosition="3437" endWordPosition="3440">6.1 3.9 92 35.9 2.4 345 37.2 7.9 200 37.3 5.4 520 37.7 13.4 302 37.7 8.5 725 38.2 17.1 407 38.0 10.7 1092 38.3 27.1 619 38.2 16.3 1812 38.6 45.9 1064 38.5 27.7 Table 1: Results of standard and augmented cube pruning. The number of (thousands of) search nodes visited is given along with BLEU and average time to decode one sentence, in seconds. Figure 6: Nodes visited by standard and augmented cube pruning. We also apply analogous heuristics to the subspan nodes. 5.2 Experimental setup We evaluated all of the algorithms in this paper on a syntax-based Arabic-English translation system based on (Galley et al., 2006), with rules extracted from 200 million words of parallel data from NIST 2008 and GALE data collections, and with a 4- gram language model trained on 1 billion words of monolingual English data from the LDC Gigaword corpus. We evaluated the system’s performance on the NIST 2008 test corpus, which consists of 1357 Arabic sentences from a mixture of newswire and web domains, with four English reference translations. We report BLEU scores (Papineni et al., 2002) on untokenized, recapitalized output. 5.3 Results for Augmented Cube Pruning The results for augmented cube pruning are compared against</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic models. In Proceedings of ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="983" citStr="Huang and Chiang, 2007" startWordPosition="156" endWordPosition="159">ecific search space with specific heuristics. We use this insight to develop faster and exact variants of cube pruning. 1 Introduction In recent years, an intense research focus on machine translation (MT) has raised the quality of MT systems to the degree that they are now viable for a variety of real-world applications. Because of this, the research community has turned its attention to a major drawback of such systems: they are still quite slow. Recent years have seen a flurry of innovative techniques designed to tackle this problem. These include cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), early pruning (Moore and Quirk, 2007), closing spans (Roark and Hollingshead, 2008; Roark and Hollingshead, 2009), coarse-to-fine methods (Petrov et al., 2008), pervasive laziness (Pust and Knight, 2009), and many more. This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they work. Many apply only to specialized MT situations. Without a deeper understanding</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Faster beam-search decoding for phrasal statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofMT Summit XI.</booktitle>
<contexts>
<context position="1022" citStr="Moore and Quirk, 2007" startWordPosition="162" endWordPosition="165">tics. We use this insight to develop faster and exact variants of cube pruning. 1 Introduction In recent years, an intense research focus on machine translation (MT) has raised the quality of MT systems to the degree that they are now viable for a variety of real-world applications. Because of this, the research community has turned its attention to a major drawback of such systems: they are still quite slow. Recent years have seen a flurry of innovative techniques designed to tackle this problem. These include cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), early pruning (Moore and Quirk, 2007), closing spans (Roark and Hollingshead, 2008; Roark and Hollingshead, 2009), coarse-to-fine methods (Petrov et al., 2008), pervasive laziness (Pust and Knight, 2009), and many more. This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they work. Many apply only to specialized MT situations. Without a deeper understanding of these methods, it is difficult for </context>
</contexts>
<marker>Moore, Quirk, 2007</marker>
<rawString>Robert C. Moore and Chris Quirk. 2007. Faster beam-search decoding for phrasal statistical machine translation. In Proceedings ofMT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19455" citStr="Papineni et al., 2002" startWordPosition="3517" endWordPosition="3521">des. 5.2 Experimental setup We evaluated all of the algorithms in this paper on a syntax-based Arabic-English translation system based on (Galley et al., 2006), with rules extracted from 200 million words of parallel data from NIST 2008 and GALE data collections, and with a 4- gram language model trained on 1 billion words of monolingual English data from the LDC Gigaword corpus. We evaluated the system’s performance on the NIST 2008 test corpus, which consists of 1357 Arabic sentences from a mixture of newswire and web domains, with four English reference translations. We report BLEU scores (Papineni et al., 2002) on untokenized, recapitalized output. 5.3 Results for Augmented Cube Pruning The results for augmented cube pruning are compared against cube pruning in Table 1. The data Figure 7: Time spent by standard and augmented cube pruning, average seconds per sentence. Standard CP Augmented CP subspan 12936 12792 precondition 851458 379954 rule 33734 33331 item 119703 118889 goal 74618 74159 TOTAL 1092449 619125 BLEU 38.33 38.22 Table 2: Breakdown of visited search nodes by type (for a fixed beam size). from that table are also plotted in Figure 6 and Figure 7. Each line gives the number of nodes vis</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<date>1984</date>
<publisher>Heuristics. Addison-Wesley.</publisher>
<contexts>
<context position="1948" citStr="Pearl, 1984" startWordPosition="319" endWordPosition="320">ue brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they work. Many apply only to specialized MT situations. Without a deeper understanding of these methods, it is difficult for the practitioner to combine them and adapt them to new use cases. In this paper, we attempt to bring some clarity to the situation by taking a closer look at one of these existing methods. Specifically, we cast the popular technique of cube pruning (Chiang, 2007) in the well-understood terms of heuristic search (Pearl, 1984). We show that cube pruning is essentially equivalent to A* search on a specific search space with specific heuristics. This simple observation affords a deeper insight into how and why cube pruning works. We show how this insight enables us to easily develop faster and exact variants of cube pruning for tree-to-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). 2 Motivating Example We begin by describing the problem that cube pruning addresses. Consider a synchronous context-free grammar (SCFG) that includes the following rules: A —* (A 0 B 1 , A 0 B 1</context>
</contexts>
<marker>Pearl, 1984</marker>
<rawString>Judea Pearl. 1984. Heuristics. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1144" citStr="Petrov et al., 2008" startWordPosition="179" endWordPosition="182">esearch focus on machine translation (MT) has raised the quality of MT systems to the degree that they are now viable for a variety of real-world applications. Because of this, the research community has turned its attention to a major drawback of such systems: they are still quite slow. Recent years have seen a flurry of innovative techniques designed to tackle this problem. These include cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), early pruning (Moore and Quirk, 2007), closing spans (Roark and Hollingshead, 2008; Roark and Hollingshead, 2009), coarse-to-fine methods (Petrov et al., 2008), pervasive laziness (Pust and Knight, 2009), and many more. This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they work. Many apply only to specialized MT situations. Without a deeper understanding of these methods, it is difficult for the practitioner to combine them and adapt them to new use cases. In this paper, we attempt to bring some clarity to the s</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pust</author>
<author>Kevin Knight</author>
</authors>
<title>Faster mt decoding through pervasive laziness.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="1188" citStr="Pust and Knight, 2009" startWordPosition="185" endWordPosition="188">has raised the quality of MT systems to the degree that they are now viable for a variety of real-world applications. Because of this, the research community has turned its attention to a major drawback of such systems: they are still quite slow. Recent years have seen a flurry of innovative techniques designed to tackle this problem. These include cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), early pruning (Moore and Quirk, 2007), closing spans (Roark and Hollingshead, 2008; Roark and Hollingshead, 2009), coarse-to-fine methods (Petrov et al., 2008), pervasive laziness (Pust and Knight, 2009), and many more. This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they work. Many apply only to specialized MT situations. Without a deeper understanding of these methods, it is difficult for the practitioner to combine them and adapt them to new use cases. In this paper, we attempt to bring some clarity to the situation by taking a closer look at one of t</context>
</contexts>
<marker>Pust, Knight, 2009</marker>
<rawString>Michael Pust and Kevin Knight. 2009. Faster mt decoding through pervasive laziness. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity contextfree inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>745--752</pages>
<contexts>
<context position="1067" citStr="Roark and Hollingshead, 2008" startWordPosition="169" endWordPosition="172">ster and exact variants of cube pruning. 1 Introduction In recent years, an intense research focus on machine translation (MT) has raised the quality of MT systems to the degree that they are now viable for a variety of real-world applications. Because of this, the research community has turned its attention to a major drawback of such systems: they are still quite slow. Recent years have seen a flurry of innovative techniques designed to tackle this problem. These include cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), early pruning (Moore and Quirk, 2007), closing spans (Roark and Hollingshead, 2008; Roark and Hollingshead, 2009), coarse-to-fine methods (Petrov et al., 2008), pervasive laziness (Pust and Knight, 2009), and many more. This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they work. Many apply only to specialized MT situations. Without a deeper understanding of these methods, it is difficult for the practitioner to combine them and adapt th</context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2008. Classifying chart cells for quadratic complexity contextfree inference. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 745–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Linear complexity context-free parsing pipelines via chart constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>647--655</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1098" citStr="Roark and Hollingshead, 2009" startWordPosition="173" endWordPosition="176">e pruning. 1 Introduction In recent years, an intense research focus on machine translation (MT) has raised the quality of MT systems to the degree that they are now viable for a variety of real-world applications. Because of this, the research community has turned its attention to a major drawback of such systems: they are still quite slow. Recent years have seen a flurry of innovative techniques designed to tackle this problem. These include cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), early pruning (Moore and Quirk, 2007), closing spans (Roark and Hollingshead, 2008; Roark and Hollingshead, 2009), coarse-to-fine methods (Petrov et al., 2008), pervasive laziness (Pust and Knight, 2009), and many more. This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they work. Many apply only to specialized MT situations. Without a deeper understanding of these methods, it is difficult for the practitioner to combine them and adapt them to new use cases. In this pa</context>
</contexts>
<marker>Roark, Hollingshead, 2009</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2009. Linear complexity context-free parsing pipelines via chart constraints. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 647–655, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weixiong Zhang</author>
<author>Richard E Korf</author>
</authors>
<title>Performance of linear-space search algorithms.</title>
<date>1995</date>
<journal>Artificial Intelligence,</journal>
<volume>79</volume>
<issue>2</issue>
<marker>Zhang, Korf, 1995</marker>
<rawString>Weixiong Zhang and Richard E. Korf. 1995. Performance of linear-space search algorithms. Artificial Intelligence, 79(2):241–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="15382" citStr="Zhang et al., 2006" startWordPosition="2767" endWordPosition="2770">e short explanation for why cube pruning is an inexact algorithm). On the other hand, subspan and precondition nodes are associated with weak trivial heuristics. This should be regarded neither as a surprise nor a criticism, considering cube pruning’s origins in hierarchical phrase-based MT models (Chiang, 2007), which have only a small number of distinct nonterminals. But the situation is much different in treeto-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). Transducer-based MT relies on SCFGs with large nonterminal sets. Binarizing the grammars (Zhang et al., 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. A key benefit of the heuristic search viewpoint is that it is well positioned to take advantage of such insights into the structure of a particular decoding problem. In the case of transducer-based MT, the large set of preconditions encourages us to introduce a nontrivial heuristic for the precondition nodes. The inclusion of these heuristics into the CP search will enable A* to eliminate certain preconditions from consideration, giving us a speedup. For this reason we call this strategy augmented cube </context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 256–263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>