<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021001">
<title confidence="0.996969">
Improving Dependency Parsers with Supertags
</title>
<author confidence="0.998412">
Hiroki Ouchi Kevin Duh Yuji Matsumoto
</author>
<affiliation confidence="0.998219">
Computational Linguistics Laboratory
Nara Institute of Science and Technology
</affiliation>
<email confidence="0.958851">
{ouchi.hiroki.nt6, kevinduh, matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.997078" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999540333333333">
Transition-based dependency parsing sys-
tems can utilize rich feature representa-
tions. However, in practice, features are
generally limited to combinations of lexi-
cal tokens and part-of-speech tags. In this
paper, we investigate richer features based
on supertags, which represent lexical tem-
plates extracted from dependency struc-
ture annotated corpus. First, we develop
two types of supertags that encode infor-
mation about head position and depen-
dency relations in different levels of granu-
larity. Then, we propose a transition-based
dependency parser that incorporates the
predictions from a CRF-based supertagger
as new features. On standard English Penn
Treebank corpus, we show that our su-
pertag features achieve parsing improve-
ments of 1.3% in unlabeled attachment,
2.07% root attachment, and 3.94% in com-
plete tree accuracy.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999787039215686">
One significant advantage of transition-based de-
pendency parsing (Yamada and Matsumoto, 2003;
Nivre et al, 2007, Goldberg and Elhadad, 2010;
Huang and Sagae, 2010) is that they can utilize
rich feature representations. However, in prac-
tice, current state-of-the-art parsers generally uti-
lize only features that are based on lexical tokens
and part-of-speech (POS) tags. In this paper, we
argue that more complex features that capture fine-
grained syntactic phenomenon and long-distance
dependencies represent a simple and effective way
to improve transition-based dependency parsers.
We focus on defining supertags for English de-
pendency parsing. Supertags, which are lexical
templates extracted from dependency structure an-
notated corpus, encode linguistically rich infor-
mation that imposes complex constraints in a lo-
cal context (Bangalore and Joshi, 1999). While
supertags have been used in frameworks based
on lexicalized grammars, e.g. Lexicalized Tree-
Adjoining Grammar (LTAG), Head-driven Phrase
Structure Grammar (HPSG) and Combinatory
Categorial Grammar (CCG), they have scarcely
been utilized for dependency parsing so far.
Previous work by Foth et al (2006) demon-
strate that supertags improve German dependency
parsing under a Weighted Constraint Dependency
Grammar (WCDG). Recent work by Ambati et al
(2013) show that supertags based on CCG lexi-
con improves transition-based dependency parsing
for Hindi. In particular, they argue that supertags
can improve long distance dependencies (e.g. co-
ordination, relative clause) in a morphologically-
rich free-word-order language. Zhang et. al.
(2010) define supertags that incorporate that long-
distance dependency information for the purpose
of HPSG parsing. All these works suggest
the promising synergy between dependency pars-
ing and supertagging. Our main contributions
are: (1) an investigation of supertags that work
well for English dependency parsing, and (2) a
novel transition-based parser that effectively uti-
lizes such supertag features.
In the following, we first describe our supertag
design (Section 2) and parser (Section 3). Su-
pertagging and parsing experiments on the Penn
Treebank (Marcus et al., 1993) are shown in Sec-
tion 4. We show that using automatically predicted
supertags, our parser can achieve improvements of
1.3% in unlabeled attachment, 2.07% root attach-
ment, and 3.94% in complete tree accuracy.
</bodyText>
<sectionHeader confidence="0.97759" genericHeader="method">
2 Supertag Design
</sectionHeader>
<bodyText confidence="0.9996395">
The main challenge with designing supertags is
finding the right balance between granularity and
predictability. Ideally, we would like to increase
the granularity of the supertags in order capture
</bodyText>
<page confidence="0.996725">
154
</page>
<note confidence="0.9633755">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 154–158,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.994916">
Figure 1: Example sentence
</figureCaption>
<table confidence="0.999799444444445">
Word Model 1 Model 2
No VMOD/R VMOD/R
, P/R P/R
it SUB/R SUB/R
was ROOT+L R ROOT+SUB/L PRD/R
n’t VMOD/L VMOD/L
Black NMOD/R NMOD/R
Monday PRD/L+L PRD/L+L
. P/L P/L
</table>
<tableCaption confidence="0.999865">
Table 1: Model 1 &amp; 2 supertags for Fig. 1.
</tableCaption>
<bodyText confidence="0.955811">
more fine-grained syntactic information, but large
tagsets tend to be more difficult to predict auto-
matically. We describe two supertag designs with
different levels of granularity in the following, fo-
cusing on incorporating syntactic features that we
believe are important for dependency parsing.
For easy exposition, consider the example sen-
tence in Figure 1. Our first supertag design, Model
1, represents syntactic information that shows the
relative position (direction) of the head of a word,
such as left (L) or right (R). If a word has root as its
head, we consider it as no direction. In addition,
dependency relation labels of heads are added. For
instance, ’No’ in the example in Figure 1 has its
head in the right direction with a label ’VMOD’,
so its supertag can be represented as ’VMOD/R’.
This kind of information essentially provides clues
about the role of the word in sentence.
On top of this, we also add information about
whether a word has any left or right dependents.
For instance, the word ’Monday’ has a left de-
pendent ’Black’, so we encode it as ’PRD/L+L’,
where the part before ’+’ specifies the head in-
formation (’PRD/L’) and the part afterwards (’L’)
specifies the position of the dependent (’L’ for left,
’R’ for right). When a word has its dependents
in both left and right directions, such as the word
’was’ in Figure 1, we combine them using ’ ’,
as in: ’ROOT+L R’. On our Penn Treebank data,
Model 1 has 79 supertags.
unigrams of supertags
for p in pi−2, pi−1, pi, pi+1, wpsp, tpsp
pi+2, pi+3
bigrams of supertags
for p, q in (pi, pi+1), spsq, tpsq, sptq,
(pi, pi+2), (pi−1, pi), (pi−1, wpsq, spwq
pi+2), (pi+1, pi+2)
head-dependent of supertags
for p, q in (pi, pi+1), wpshpwqsldq,
(pi, pi+2), (pi−1, pi), (pi−1, tpshptqsldq,
</bodyText>
<equation confidence="0.448477166666667">
pi+2), (pi+1, pi+2) wpsrdpwqshq,
tpsrdptqshq
Table 2: Proposed supertag feature templates.
w = word; t = POS-tag; s = supertag; sh = head part
of supertag; sld = left dependent part of supertag;
srd = right dependent part of supertag
</equation>
<bodyText confidence="0.99997508">
In Model 2, we further add dependency relation
labels of obligatory dependents of verbs. Here we
define obligatory dependents of verbs as depen-
dents which have the following dependency rela-
tion labels, ’SUB’, ’OBJ’, ’PRD’ and ’VC’. If a
label of a dependent is not any of the obligatory
dependent labels, the supertag encodes only the
information of direction of the dependents (same
as Model 1). For instance, ’was’ in the exam-
ple sentence has an obligatory dependent with a
label ’SUB’ in the left direction and ’PRD’ in
the right direction, so its supertag is represented
as ’ROOT+SUB/L PRD/R’. If a verb has multi-
ple obligatory dependents in the same direction,
its supertag encodes them in sequence; if a verb
takes a subject and two objects, we may have
’X/X+SUB/L OBJ/R OBJ/R’. The number of su-
pertags of Model 2 is 312.
Our Model 2 is similar to Model F of Foth et
al. (2006) except that they define objects of prepo-
sitions and conjunctions as obligatory as well as
verbs. However, we define only dependents of
verbs because verbs play the most important role
for constructing syntactic trees and we would like
to decrease the number of supertags.
</bodyText>
<sectionHeader confidence="0.993288" genericHeader="method">
3 Supertags as Features in a
</sectionHeader>
<subsectionHeader confidence="0.979607">
Transition-based Dependency Parser
</subsectionHeader>
<bodyText confidence="0.9982572">
In this work, we adopt the Easy-First parser of
(Goldberg and Elhadad, 2010), a highly-accurate
transition-based dependency parser. We describe
how we incorporate supertag features in the Easy-
First framework, though it can be done similarly
</bodyText>
<page confidence="0.996072">
155
</page>
<bodyText confidence="0.999029591836735">
for other transition-based frameworks like left-to-
right arc-eager and arc-standard models (Nivre et
al., 2006; Yamada and Matsumoto, 2003).
In the Easy-First algorithm, a dependency tree
is constructed by two kinds of actions: ATTACH-
LEFT(i) and ATTACHRIGHT(i) to a list of par-
tial tree structures p1,...,pk initialized with the n
words of the sentence w1,...,wn. ATTACHLEFT(i)
attaches (pi, p+1) and removes pi+1 from the par-
tial tree list. ATTACHRIGHT(i) attaches (pi+1, pi)
and removes pi from the partial tree list. Features
are extracted from the attachment point as well as
two neighboring structures: pi−2, pi−1, pi, pi+1,
pi+2, pi+s. Table 2 summarizes the supertag fea-
tures we extract from this neighborhood; these are
appended to the original baseline features based
on POS/word in Goldberg and Elhadad (2010).
For a partial tree structure p, features are de-
fined based on information in its head: we use
wp to refer to the surface word form of the head
word of p, tp to refer to the head word’s POS
tag, and sp to refer to the head word’s supertag.
Further, we not only use a supertag as is, but
split each supertag into subparts. For instance,
the supertag ’ROOT+SUB/L PRD/R’ is split into
’ROOT’, ’SUB/L’ and ’PRD/R’, a supertag rep-
resenting the supertag head information shp, su-
pertag left dependent information sldp, and su-
pertag right dependent information srdp.
For the unigram features, we use information
within a single partial structure, such as conjunc-
tion of head word and its supertag (wpsp), con-
junction of head word’s POS tag and its supertag
(tpsp). To consider more context, bigram features
look at pairs of partial structures. For each (p, q)
pair of structures in pi−2, pi−1, pi, pi+1, pi+2, we
look at e.g. conjunctions of supertags (spsq).
Finally, head information of a partial struc-
ture and dependent information of another partial
structure are combined as ”head-dependent fea-
tures” in order to check for consistency in head-
dependent relations. For instance, in Table 1
the supertag for the word ’Black’ has head part
’NMOD/R’ wanting to attach right and the su-
pertag for the word ’Monday’ has dependent part
’L’ wanting something to the left; they are likely
to be attached by our parser because of the consis-
tency in head-dependent direction. These features
are used in conjunction with word and POS-tag.
</bodyText>
<table confidence="0.942789666666667">
Model # tags Dev Test
Model1 79 87.81 88.12
Model2 312 87.22 87.13
</table>
<tableCaption confidence="0.993387333333333">
Table 3: Supertag accuracy evaluated on develop-
ment and test set. Dev = development set, PTB 22;
Test = test set, PTB 23
</tableCaption>
<sectionHeader confidence="0.998834" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999828">
To evaluate the effectiveness of supertags as fea-
tures, we perform experiments on the Penn Tree-
bank (PTB), converted into dependency format
with Penn2Malt1. Adopting standard approach,
we split PTB sections 2-21 for training, section 22
for development and 23 for testing. We assigned
POS-tags to the training data by ten-fold jackknif-
ing following Huang and Sagae (2010). Develop-
ment and test sets are automatically tagged by the
tagger trained on the training set.
</bodyText>
<subsectionHeader confidence="0.998711">
4.1 Supertagging Experiments
</subsectionHeader>
<bodyText confidence="0.999807074074074">
We use the training data set to train a supertagger
of each model using Conditional Random Fields
(CRF) and the test data set to evaluate the accu-
racies. We use version 0.12 of CRFsuite2 for our
CRF implementation. First-order transitions, and
word/POS of uni, bi and trigrams in a 7-word win-
dow surrounding the target word are used as fea-
tures. Table 3 shows the result of the supertagging
accuracies. The supertag accuracies are around
87-88% for both models, suggesting that most of
the supertags can be effectively learned by stan-
dard CRFs. The tagger takes 0.001 and 0.005 sec-
ond per sentence for Model 1 and 2 respectively.
In our error analysis, we find it is challeng-
ing to assign correct supertags for obligatory
dependents of Model 2. In the test set, the
number of the supertags encoding obligatory de-
pendents is 5432 and its accuracy is 74.61%
(The accuracy of the corresponding supertags in
Model 1 is 82.18%). Among them, it is es-
pecially difficult to predict the supertags encod-
ing obligatory dependents with a head informa-
tion of subordination conjunction ’SBAR’, such as
’SBAR/L+SUB/L PRD/R’. The accuracy of such
supertags is around 60% (e.g., the accuracy of
a supertag ’SBAR/L+SUB/L PRD/R’ is 57.78%),
while the supertags encoding dependents with a la-
</bodyText>
<footnote confidence="0.9999825">
1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.jar
2http://www.chokkan.org/software/crfsuite/
</footnote>
<page confidence="0.990731">
156
</page>
<table confidence="0.9995984">
feature Model1 Model2
baseline 90.25 90.25
+unigram of supertag 90.59 90.76
+bigram of supertag 91.37 91.08
+head-dependent 91.22 91.28
</table>
<tableCaption confidence="0.937341">
Table 4: Unlabeled attachment scores (UAS) on
</tableCaption>
<table confidence="0.9570382">
the development set for each feature template.
Model UAS Root Complete
baseline 90.05 91.10 37.41
Model 1 91.35 93.17 41.35
Model 2 91.23 92.72 41.35
</table>
<tableCaption confidence="0.970228">
Table 5: Accuracies for English dependency pars-
</tableCaption>
<bodyText confidence="0.885804769230769">
ing on the test set. UAS = unlabeled attachment
score; Root = root attachment score; Complete =
the percentage of sentences in which all tokens
were assigned their correct heads.
bel ’VC’ are assigned almost correctly (e.g., the
accuracy of ’VC/L+VC/R’ is 97.41%). A verb
within a subordinating clause usually has the sub-
ordinating conjunction as its head and it tends
to be long-range dependency, which is harder to
predict. ’VC’ represents verb complements. A
gerund and a past participle is often a dependent
of the immediate front verb, so it is not so difficult
to identify the dependency relation.
</bodyText>
<subsectionHeader confidence="0.991893">
4.2 Dependency Parsing Experiments
</subsectionHeader>
<bodyText confidence="0.999942288461539">
First, we evaluate the effectiveness of the feature
templates proposed in Section 3. Following the
same procedure as our POS tagger, we first assign
supertags to the training data by ten-fold jackknif-
ing, then train our Easy-First dependency parser
on these predicted supertags. For development and
test sets, we assign supertags based on a supertag-
ger trained on the whole training data.
Table 4 shows the effect of new supertag fea-
tures on the development data. We start with the
baseline features, and incrementally add the uni-
grams, bigrams, and head-dependent feature tem-
plates. For Model 1 we observe that adding uni-
gram features improve the baseline UAS slightly
by 0.34% while additionally adding bigram fea-
tures give larger improvements of 0.78%. On the
other hand, for Model 2 unigram features make
bigger contribution on improvements by 0.51%
than bigram ones 0.32%. One possible expla-
nation is that because each supertag of Model 2
encodes richer syntactic information, an individ-
ual tag can make bigger contribution on improve-
ments than Model 1 as a unigram feature. How-
ever, since supertags of Model 2 can be erroneous
and noisy combination of multiple supertags, such
as bigram features, can propagate errors.
Using all features, the accuracy of the accu-
racy of Model 2 improved further by 0.20%, while
Model 1 dropped by 0.15%. It is unclear why
Model 1 accuracy dropped, but one hypothesis is
that coarse-grained supertags may conflate some
head-dependent. The development set UAS for
combinations of all features are 91.22% (Model 1)
and 91.28% (Model 2), corresponding to 0.97%
and 1.03% improvement over the baseline.
Next, we show the parsing accuracies on the
test set, using all unigram, bigram, and head-
dependents supertag features. The UAS3, Root
attachment scores, and Complete accuracy are
shown in Table 5. Both Model 1 and 2 outperform
the baseline in all metrics. UAS improvements
for both models are statistically significant under
the McNemar test, p &lt; 0.05 (difference between
Model 1 and 2 is not significant). Notably, Model
1 achieves parsing improvements of 1.3% in un-
labeled attachment, 2.07% root attachment, and
3.94% in complete accuracy. Comparing Model
1 to baseline, attachment improvements binned by
distance to head are as follows: +0.54 F1 for dis-
tance 1, +0.81 for distance 2, +2.02 for distance
3 to 6, +2.95 for distance 7 or more, implying su-
pertags are helpful for long distance dependencies.
</bodyText>
<sectionHeader confidence="0.999653" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999937333333333">
We have demonstrated the effectiveness of su-
pertags as features for English transition-based de-
pendency parsing. In previous work, syntactic in-
formation, such as a head and dependents of a
word, cannot be used as features before partial tree
structures are constructed (Zhang and Nivre, 2011;
Goldberg and Elhadad, 2010). By using supertags
as features, we can utilize fine-grained syntactic
information without waiting for partial trees to be
built, and they contribute to improvement of ac-
curacies of English dependency parsing. In future
work, we would like to develop parsers that di-
rectly integrate supertag ambiguity in the parsing
decision, and to investigate automatic pattern min-
ing approaches to supertag design.
</bodyText>
<footnote confidence="0.992881">
3For comparison, MaltParser and MSTParser with base-
line features is 88.68% and 91.37% UAS respectively
</footnote>
<page confidence="0.996956">
157
</page>
<sectionHeader confidence="0.99831" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935897959184">
Bharat R Ambati, Tejaswini Deoskar and Mark Steed-
man. 2013. Using CCG categories to improve Hindi
dependency parsing. In Proceedings of ACL, pages
604-609, Sofia, Bulgaria, August.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
puational Linguistics, 25(2):237-265.
Kilian Foth, Tomas By, and Wolfgang Menzel. 2006.
Guiding a Constraint Dependency Parser with Su-
pertags. In Proceedings of COLING/ACL 2006,
pages 289-296, Sydney, Australia, July.
Yoav Goldberg and Michael Elhadad. 2010. An Effi-
cient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Proceedings of HLT/NAACL,
pages 742-750, Los Angeles, California, June.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL, pages 1077-1086, Uppsala,
Sweden, July.
Mitchell. P. Marcus, Beatrice Santorini and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313-330
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryiˇgit, Sandra K¨ubler, Svetoslav,
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95-135
Joakim Nivre, Johan Hall, Jens Nilsson, G¨ulsen Eryiˇgit
and Svetoslav, Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of CoNLL, pages 221-
225, New York, USA.
N Okazaki. 2007. CRFsuite: a fast imple-
mentation of Conditional Random Fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In
Proceedings of IWPT, Nancy, France.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Feaures.
In Proceedings of ACL, pages 188-193, Porland,
Oregon, June.
Yao-zhong Zhang, Takuya Matsuzaki and Jun’ichi Tsu-
jii. 2010. A Simple Approach for HPSG Supertag-
ging Using Dependency Information. In Proceed-
ings of HLT/NAACL, pages 645-648, Los Angeles,
California, June.
</reference>
<page confidence="0.997088">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880275">
<title confidence="0.999947">Improving Dependency Parsers with Supertags</title>
<author confidence="0.995097">Hiroki Ouchi Kevin Duh Yuji Matsumoto</author>
<affiliation confidence="0.995045">Computational Linguistics Nara Institute of Science and Technology</affiliation>
<email confidence="0.913863">kevinduh,</email>
<abstract confidence="0.998741181818182">Transition-based dependency parsing systems can utilize rich feature representations. However, in practice, features are generally limited to combinations of lexical tokens and part-of-speech tags. In this paper, we investigate richer features based on supertags, which represent lexical templates extracted from dependency structure annotated corpus. First, we develop two types of supertags that encode information about head position and dependency relations in different levels of granularity. Then, we propose a transition-based dependency parser that incorporates the predictions from a CRF-based supertagger as new features. On standard English Penn Treebank corpus, we show that our supertag features achieve parsing improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% in complete tree accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bharat R Ambati</author>
<author>Tejaswini Deoskar</author>
<author>Mark Steedman</author>
</authors>
<title>Using CCG categories to improve Hindi dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>604--609</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2380" citStr="Ambati et al (2013)" startWordPosition="334" endWordPosition="337">m dependency structure annotated corpus, encode linguistically rich information that imposes complex constraints in a local context (Bangalore and Joshi, 1999). While supertags have been used in frameworks based on lexicalized grammars, e.g. Lexicalized TreeAdjoining Grammar (LTAG), Head-driven Phrase Structure Grammar (HPSG) and Combinatory Categorial Grammar (CCG), they have scarcely been utilized for dependency parsing so far. Previous work by Foth et al (2006) demonstrate that supertags improve German dependency parsing under a Weighted Constraint Dependency Grammar (WCDG). Recent work by Ambati et al (2013) show that supertags based on CCG lexicon improves transition-based dependency parsing for Hindi. In particular, they argue that supertags can improve long distance dependencies (e.g. coordination, relative clause) in a morphologicallyrich free-word-order language. Zhang et. al. (2010) define supertags that incorporate that longdistance dependency information for the purpose of HPSG parsing. All these works suggest the promising synergy between dependency parsing and supertagging. Our main contributions are: (1) an investigation of supertags that work well for English dependency parsing, and (</context>
</contexts>
<marker>Ambati, Deoskar, Steedman, 2013</marker>
<rawString>Bharat R Ambati, Tejaswini Deoskar and Mark Steedman. 2013. Using CCG categories to improve Hindi dependency parsing. In Proceedings of ACL, pages 604-609, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Compuational Linguistics,</journal>
<pages>25--2</pages>
<contexts>
<context position="1920" citStr="Bangalore and Joshi, 1999" startWordPosition="268" endWordPosition="271">practice, current state-of-the-art parsers generally utilize only features that are based on lexical tokens and part-of-speech (POS) tags. In this paper, we argue that more complex features that capture finegrained syntactic phenomenon and long-distance dependencies represent a simple and effective way to improve transition-based dependency parsers. We focus on defining supertags for English dependency parsing. Supertags, which are lexical templates extracted from dependency structure annotated corpus, encode linguistically rich information that imposes complex constraints in a local context (Bangalore and Joshi, 1999). While supertags have been used in frameworks based on lexicalized grammars, e.g. Lexicalized TreeAdjoining Grammar (LTAG), Head-driven Phrase Structure Grammar (HPSG) and Combinatory Categorial Grammar (CCG), they have scarcely been utilized for dependency parsing so far. Previous work by Foth et al (2006) demonstrate that supertags improve German dependency parsing under a Weighted Constraint Dependency Grammar (WCDG). Recent work by Ambati et al (2013) show that supertags based on CCG lexicon improves transition-based dependency parsing for Hindi. In particular, they argue that supertags c</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Compuational Linguistics, 25(2):237-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Foth</author>
<author>Tomas By</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Guiding a Constraint Dependency Parser with Supertags.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL 2006,</booktitle>
<pages>289--296</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2229" citStr="Foth et al (2006)" startWordPosition="312" endWordPosition="315">nsition-based dependency parsers. We focus on defining supertags for English dependency parsing. Supertags, which are lexical templates extracted from dependency structure annotated corpus, encode linguistically rich information that imposes complex constraints in a local context (Bangalore and Joshi, 1999). While supertags have been used in frameworks based on lexicalized grammars, e.g. Lexicalized TreeAdjoining Grammar (LTAG), Head-driven Phrase Structure Grammar (HPSG) and Combinatory Categorial Grammar (CCG), they have scarcely been utilized for dependency parsing so far. Previous work by Foth et al (2006) demonstrate that supertags improve German dependency parsing under a Weighted Constraint Dependency Grammar (WCDG). Recent work by Ambati et al (2013) show that supertags based on CCG lexicon improves transition-based dependency parsing for Hindi. In particular, they argue that supertags can improve long distance dependencies (e.g. coordination, relative clause) in a morphologicallyrich free-word-order language. Zhang et. al. (2010) define supertags that incorporate that longdistance dependency information for the purpose of HPSG parsing. All these works suggest the promising synergy between </context>
<context position="6988" citStr="Foth et al. (2006)" startWordPosition="1096" endWordPosition="1099">any of the obligatory dependent labels, the supertag encodes only the information of direction of the dependents (same as Model 1). For instance, ’was’ in the example sentence has an obligatory dependent with a label ’SUB’ in the left direction and ’PRD’ in the right direction, so its supertag is represented as ’ROOT+SUB/L PRD/R’. If a verb has multiple obligatory dependents in the same direction, its supertag encodes them in sequence; if a verb takes a subject and two objects, we may have ’X/X+SUB/L OBJ/R OBJ/R’. The number of supertags of Model 2 is 312. Our Model 2 is similar to Model F of Foth et al. (2006) except that they define objects of prepositions and conjunctions as obligatory as well as verbs. However, we define only dependents of verbs because verbs play the most important role for constructing syntactic trees and we would like to decrease the number of supertags. 3 Supertags as Features in a Transition-based Dependency Parser In this work, we adopt the Easy-First parser of (Goldberg and Elhadad, 2010), a highly-accurate transition-based dependency parser. We describe how we incorporate supertag features in the EasyFirst framework, though it can be done similarly 155 for other transiti</context>
</contexts>
<marker>Foth, By, Menzel, 2006</marker>
<rawString>Kilian Foth, Tomas By, and Wolfgang Menzel. 2006. Guiding a Constraint Dependency Parser with Supertags. In Proceedings of COLING/ACL 2006, pages 289-296, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>742--750</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="1202" citStr="Goldberg and Elhadad, 2010" startWordPosition="165" endWordPosition="168">we develop two types of supertags that encode information about head position and dependency relations in different levels of granularity. Then, we propose a transition-based dependency parser that incorporates the predictions from a CRF-based supertagger as new features. On standard English Penn Treebank corpus, we show that our supertag features achieve parsing improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% in complete tree accuracy. 1 Introduction One significant advantage of transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre et al, 2007, Goldberg and Elhadad, 2010; Huang and Sagae, 2010) is that they can utilize rich feature representations. However, in practice, current state-of-the-art parsers generally utilize only features that are based on lexical tokens and part-of-speech (POS) tags. In this paper, we argue that more complex features that capture finegrained syntactic phenomenon and long-distance dependencies represent a simple and effective way to improve transition-based dependency parsers. We focus on defining supertags for English dependency parsing. Supertags, which are lexical templates extracted from dependency structure annotated corpus, </context>
<context position="7401" citStr="Goldberg and Elhadad, 2010" startWordPosition="1162" endWordPosition="1165"> supertag encodes them in sequence; if a verb takes a subject and two objects, we may have ’X/X+SUB/L OBJ/R OBJ/R’. The number of supertags of Model 2 is 312. Our Model 2 is similar to Model F of Foth et al. (2006) except that they define objects of prepositions and conjunctions as obligatory as well as verbs. However, we define only dependents of verbs because verbs play the most important role for constructing syntactic trees and we would like to decrease the number of supertags. 3 Supertags as Features in a Transition-based Dependency Parser In this work, we adopt the Easy-First parser of (Goldberg and Elhadad, 2010), a highly-accurate transition-based dependency parser. We describe how we incorporate supertag features in the EasyFirst framework, though it can be done similarly 155 for other transition-based frameworks like left-toright arc-eager and arc-standard models (Nivre et al., 2006; Yamada and Matsumoto, 2003). In the Easy-First algorithm, a dependency tree is constructed by two kinds of actions: ATTACHLEFT(i) and ATTACHRIGHT(i) to a list of partial tree structures p1,...,pk initialized with the n words of the sentence w1,...,wn. ATTACHLEFT(i) attaches (pi, p+1) and removes pi+1 from the partial t</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing. In Proceedings of HLT/NAACL, pages 742-750, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1077--1086</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1226" citStr="Huang and Sagae, 2010" startWordPosition="169" endWordPosition="172">rtags that encode information about head position and dependency relations in different levels of granularity. Then, we propose a transition-based dependency parser that incorporates the predictions from a CRF-based supertagger as new features. On standard English Penn Treebank corpus, we show that our supertag features achieve parsing improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% in complete tree accuracy. 1 Introduction One significant advantage of transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre et al, 2007, Goldberg and Elhadad, 2010; Huang and Sagae, 2010) is that they can utilize rich feature representations. However, in practice, current state-of-the-art parsers generally utilize only features that are based on lexical tokens and part-of-speech (POS) tags. In this paper, we argue that more complex features that capture finegrained syntactic phenomenon and long-distance dependencies represent a simple and effective way to improve transition-based dependency parsers. We focus on defining supertags for English dependency parsing. Supertags, which are lexical templates extracted from dependency structure annotated corpus, encode linguistically ri</context>
<context position="10484" citStr="Huang and Sagae (2010)" startWordPosition="1670" endWordPosition="1673">s are used in conjunction with word and POS-tag. Model # tags Dev Test Model1 79 87.81 88.12 Model2 312 87.22 87.13 Table 3: Supertag accuracy evaluated on development and test set. Dev = development set, PTB 22; Test = test set, PTB 23 4 Experiments To evaluate the effectiveness of supertags as features, we perform experiments on the Penn Treebank (PTB), converted into dependency format with Penn2Malt1. Adopting standard approach, we split PTB sections 2-21 for training, section 22 for development and 23 for testing. We assigned POS-tags to the training data by ten-fold jackknifing following Huang and Sagae (2010). Development and test sets are automatically tagged by the tagger trained on the training set. 4.1 Supertagging Experiments We use the training data set to train a supertagger of each model using Conditional Random Fields (CRF) and the test data set to evaluate the accuracies. We use version 0.12 of CRFsuite2 for our CRF implementation. First-order transitions, and word/POS of uni, bi and trigrams in a 7-word window surrounding the target word are used as features. Table 3 shows the result of the supertagging accuracies. The supertag accuracies are around 87-88% for both models, suggesting th</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL, pages 1077-1086, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="3236" citStr="Marcus et al., 1993" startWordPosition="459" endWordPosition="462">ich free-word-order language. Zhang et. al. (2010) define supertags that incorporate that longdistance dependency information for the purpose of HPSG parsing. All these works suggest the promising synergy between dependency parsing and supertagging. Our main contributions are: (1) an investigation of supertags that work well for English dependency parsing, and (2) a novel transition-based parser that effectively utilizes such supertag features. In the following, we first describe our supertag design (Section 2) and parser (Section 3). Supertagging and parsing experiments on the Penn Treebank (Marcus et al., 1993) are shown in Section 4. We show that using automatically predicted supertags, our parser can achieve improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% in complete tree accuracy. 2 Supertag Design The main challenge with designing supertags is finding the right balance between granularity and predictability. Ideally, we would like to increase the granularity of the supertags in order capture 154 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 154–158, Gothenburg, Sweden, April 26-30 2014. c�2014 Ass</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell. P. Marcus, Beatrice Santorini and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Atanas Chanev</author>
<author>G¨ulsen Eryiˇgit</author>
<author>Sandra K¨ubler</author>
<author>Marinov Svetoslav</author>
<author>Erwin Marsi</author>
</authors>
<title>Maltparser: A language-independent system for data-driven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<pages>13--2</pages>
<marker>Nivre, Hall, Nilsson, Chanev, Eryiˇgit, K¨ubler, Svetoslav, Marsi, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryiˇgit, Sandra K¨ubler, Svetoslav, Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95-135</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>G¨ulsen Eryiˇgit</author>
<author>Marinov Svetoslav</author>
</authors>
<title>Labeled pseudoprojective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>221--225</pages>
<location>New York, USA.</location>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Svetoslav, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, G¨ulsen Eryiˇgit and Svetoslav, Marinov. 2006. Labeled pseudoprojective dependency parsing with support vector machines. In Proceedings of CoNLL, pages 221-225, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of Conditional Random Fields</title>
<date>2007</date>
<marker>Okazaki, 2007</marker>
<rawString>N Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields (CRFs). http://www.chokkan.org/software/crfsuite/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="1155" citStr="Yamada and Matsumoto, 2003" startWordPosition="157" endWordPosition="160"> dependency structure annotated corpus. First, we develop two types of supertags that encode information about head position and dependency relations in different levels of granularity. Then, we propose a transition-based dependency parser that incorporates the predictions from a CRF-based supertagger as new features. On standard English Penn Treebank corpus, we show that our supertag features achieve parsing improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% in complete tree accuracy. 1 Introduction One significant advantage of transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre et al, 2007, Goldberg and Elhadad, 2010; Huang and Sagae, 2010) is that they can utilize rich feature representations. However, in practice, current state-of-the-art parsers generally utilize only features that are based on lexical tokens and part-of-speech (POS) tags. In this paper, we argue that more complex features that capture finegrained syntactic phenomenon and long-distance dependencies represent a simple and effective way to improve transition-based dependency parsers. We focus on defining supertags for English dependency parsing. Supertags, which are lexical templates extract</context>
<context position="7708" citStr="Yamada and Matsumoto, 2003" startWordPosition="1205" endWordPosition="1208">rbs. However, we define only dependents of verbs because verbs play the most important role for constructing syntactic trees and we would like to decrease the number of supertags. 3 Supertags as Features in a Transition-based Dependency Parser In this work, we adopt the Easy-First parser of (Goldberg and Elhadad, 2010), a highly-accurate transition-based dependency parser. We describe how we incorporate supertag features in the EasyFirst framework, though it can be done similarly 155 for other transition-based frameworks like left-toright arc-eager and arc-standard models (Nivre et al., 2006; Yamada and Matsumoto, 2003). In the Easy-First algorithm, a dependency tree is constructed by two kinds of actions: ATTACHLEFT(i) and ATTACHRIGHT(i) to a list of partial tree structures p1,...,pk initialized with the n words of the sentence w1,...,wn. ATTACHLEFT(i) attaches (pi, p+1) and removes pi+1 from the partial tree list. ATTACHRIGHT(i) attaches (pi+1, pi) and removes pi from the partial tree list. Features are extracted from the attachment point as well as two neighboring structures: pi−2, pi−1, pi, pi+1, pi+2, pi+s. Table 2 summarizes the supertag features we extract from this neighborhood; these are appended to</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H Yamada and Y Matsumoto. 2003. Statistical dependency analysis using support vector machines. In Proceedings of IWPT, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based Dependency Parsing with Rich Non-local Feaures.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>188--193</pages>
<location>Porland, Oregon,</location>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Feaures. In Proceedings of ACL, pages 188-193, Porland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yao-zhong Zhang</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A Simple Approach for HPSG Supertagging Using Dependency Information.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>645--648</pages>
<location>Los Angeles, California,</location>
<marker>Zhang, Matsuzaki, Tsujii, 2010</marker>
<rawString>Yao-zhong Zhang, Takuya Matsuzaki and Jun’ichi Tsujii. 2010. A Simple Approach for HPSG Supertagging Using Dependency Information. In Proceedings of HLT/NAACL, pages 645-648, Los Angeles, California, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>