<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.998856">
Online Plagiarism Detection Through Exploiting Lexical, Syntactic, and
Semantic Information
</title>
<author confidence="0.990512">
Wan-Yu Lin Nanyun Peng Chun-Chao Yen Shou-de Lin
</author>
<affiliation confidence="0.840041">
Graduate Institute of
Networking and
Multimedia, National
Taiwan University
</affiliation>
<figure confidence="0.3834075">
r99944016@csie
.ntu.edu.tw
Institute of
Computational
Linguistic, Peking
University
pengnanyun@pku
.edu.cn
</figure>
<affiliation confidence="0.5806875">
Graduate Institute of
Networking and
Multimedia, National
Taiwan University
r96944016@csie
.ntu.edu.tw
Graduate Institute of
Networking and
Multimedia, National
Taiwan University
</affiliation>
<email confidence="0.7730645">
sdlin@csie.ntu
.edu.tw
</email>
<sectionHeader confidence="0.991448" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992945">
In this paper, we introduce a framework that
identifies online plagiarism by exploiting lexical,
syntactic and semantic features that includes
duplication-gram, reordering and alignment of
words, POS and phrase tags, and semantic
similarity of sentences. We establish an ensemble
framework to combine the predictions of each
model. Results demonstrate that our system can
not only find considerable amount of real-world
online plagiarism cases but also outperforms
several state-of-the-art algorithms and commercial
software.
</bodyText>
<sectionHeader confidence="0.998309" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.955922">
Plagiarism Detection, Lexical, Syntactic, Semantic
</keyword>
<sectionHeader confidence="0.999749" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999982472222222">
Online plagiarism, the action of trying to create a
new piece of writing by copying, reorganizing or
rewriting others’ work identified through search
engines, is one of the most commonly seen
misusage of the highly matured web technologies.
As implied by the experiment conducted by
(Braumoeller and Gaines, 2001), a powerful
plagiarism detection system can effectively
discourage people from plagiarizing others’ work.
A common strategy people adopt for online-
plagiarism detection is as follows. First they
identify several suspicious sentences from the
write-up and feed them one by one as a query to a
search engine to obtain a set of documents. Then
human reviewers can manually examine whether
these documents are truly the sources of the
suspicious sentences. While it is quite
straightforward and effective, the limitation of this
strategy is obvious. First, since the length of search
query is limited, suspicious sentences are usually
queried and examined independently. Therefore, it
is harder to identify document level plagiarism
than sentence level plagiarism. Second, manually
checking whether a query sentence plagiarizes
certain websites requires specific domain and
language knowledge as well as considerable
amount of energy and time. To overcome the
above shortcomings, we introduce an online
plagiarism detection system using natural language
processing techniques to simulate the above
reverse-engineering approach. We develop an
ensemble framework that integrates lexical,
syntactic and semantic features to achieve this goal.
Our system is language independent and we have
implemented both Chinese and English versions
for evaluation.
</bodyText>
<sectionHeader confidence="0.99991" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.994995333333333">
Plagiarism detection has been widely discussed in
the past decades (Zou et al., 2010). Table 1.
summarizes some of them:
</bodyText>
<table confidence="0.980580384615385">
Author Comparison Similarity Function
Unit
Brin et al., Word + Percentage of matching
1995 Sentence sentences.
White and Sentence Average overlap ratio of
Joy, 2004 the sentence pairs using 2
pre-defined thresholds.
Niezgoda A human Sliding windows ranked
and Way, defined by the average length per
2006 sliding word.
window
Cedeno and Sentence + Overlap percentage of n-
Rosso, 2009 n-gram gram in the sentence pairs.
</table>
<page confidence="0.981255">
145
</page>
<note confidence="0.8343875">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 145–150,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<table confidence="0.997873636363636">
Pera and Ng, Sentence A pre-defined
2010 resemblance function
based on word correlation
factor.
Stamatatos, Passage Overlap percentage of
2011 stopword n-grams.
Grman and Passage Matching percentage of
Ravas, 2011 words with given
thresholds on both ratio
and absolute number of
words in passage.
</table>
<tableCaption confidence="0.999931">
Table 1. Summary of related works
</tableCaption>
<bodyText confidence="0.999510214285714">
Comparing to those systems, our system exploits
more sophisticated syntactic and semantic
information to simulate what plagiarists are trying
to do.
There are several online or charged/free
downloadable plagiarism detection systems such as
Turnitin, EVE2, Docol©c, and CATPPDS which
detect mainly verbatim copy. Others such as
Microsoft Plagiarism Detector (MPD), Safeassign,
Copyscape and VeriGuide, claim to be capable of
detecting obfuscations. Unfortunately those
commercial systems do not reveal the detail
strategies used, therefore it is hard to judge and
reproduce their results for comparison.
</bodyText>
<sectionHeader confidence="0.98872" genericHeader="method">
3. Methodology
</sectionHeader>
<figureCaption confidence="0.996569">
Figure 1. Detection Flow
</figureCaption>
<bodyText confidence="0.955601">
The data flow is shown above in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999634">
3.1 Query a Search Engine
</subsectionHeader>
<bodyText confidence="0.9999583">
We first break down each article into a series of
queries to query a search engine. Several systems
such as (Liu at al., 2007) have proposed a similar
idea. The main difference between our method and
theirs is that we send unquoted queries rather than
quoted ones. We do not require the search results
to completely match to the query sentence. This
strategy allows us to not only identify the
copy/paste type of plagiarism but also re-write/edit
type of plagiarism.
</bodyText>
<subsectionHeader confidence="0.999835">
3.2 Sentence-based Plagiarism Detection
</subsectionHeader>
<bodyText confidence="0.999972363636364">
Since not all outputs of a search engine contain an
exact copy of the query, we need a model to
quantify how likely each of them is the source of
plagiarism. For better efficiency, our experiment
exploits the snippet of a search output to represent
the whole document. That is, we want to measure
how likely a snippet is the plagiarized source of the
query. We designed several models which utilized
rich lexical, syntactic and semantic features to
pursue this goal, and the details are discussed
below.
</bodyText>
<subsubsectionHeader confidence="0.978278">
3.2.1 Igram Matching (IM)
</subsubsectionHeader>
<bodyText confidence="0.99997675">
One straightforward measure is to exploit the n-
gram similarity between source and target texts.
We first enumerate all n-grams in source, and then
calculate the overlap percentage with the n-grams
in the target. The larger n is, the harder for this
feature to detect plagiarism with insertion,
replacement, and deletion. In the experiment, we
choose n=2.
</bodyText>
<subsectionHeader confidence="0.830188">
3.2.2 Reordering of Words (RW)
</subsectionHeader>
<bodyText confidence="0.999925">
Plagiarism can come from the reordering of words.
We argue that the permutation distance between S1
and S2 is an important indicator for reordered
plagiarism. The permutation distance is defined as
the minimum number of pair-wise exchanging of
matched words needed to transform a sentence, S2,
to contain the same order of matched words as
another sentence, S1. As mentioned in (Sörensena
and Sevaux, 2005), the permutation distance can
be calculated by the following expression
</bodyText>
<equation confidence="0.99605">
𝑑 𝑆1,𝑆2 = 𝑧𝑖𝑗
𝑛−1 𝑛
𝑖=1 𝑗 =𝑖+1
</equation>
<bodyText confidence="0.743482">
where
</bodyText>
<equation confidence="0.989799666666667">
1, 𝑖𝑓 𝑆1 𝑗 &gt; 𝑆1 𝑖 𝑎𝑛𝑑 𝑆2 𝑗 &lt; 𝑆2 𝑖
𝑧𝑖𝑗 =
k 0, 𝑜𝑡𝑕𝑒𝑟𝑤𝑖𝑠𝑒
</equation>
<bodyText confidence="0.950035714285714">
S1(i) and S2(i) are indices of the ith matched
word in sentences S1 and S2 respectively and n is
the number of matched words between the
sentences S1 and S2. Let μ = n2− n
2 be the
normalized term, which is the maximum possible
distance between S1 and S2, then the reordering
</bodyText>
<page confidence="0.994242">
146
</page>
<bodyText confidence="0.7108935">
score of the two sentences, expressed as s(S1, S2),
will be s (S1, S2 ) = 1 −
d (S1,S2)
µ
</bodyText>
<subsectionHeader confidence="0.817099">
3.2.3 Alignment of Words (AW)
</subsectionHeader>
<bodyText confidence="0.999454636363636">
Besides reordering, plagiarists often insert or
delete words in a sentence. We try to model such
behavior by finding the alignment of two word
sequences. We perform the alignment using a
dynamic programming method as mentioned in
(Wagner and Fischer, 1975).
However, such alignment score does not reflect
the continuity of the matched words, which can be
an important cue to identify plagiarism. To
overcome such drawback, we modify the score as
below.
</bodyText>
<equation confidence="0.994347">
|M|−1
New Alignment Score = Y- Gi
i=1
|M|−1
1
</equation>
<bodyText confidence="0.97961825">
where Gi = # o f words between (M i,M i+1 )+1
M is the list of matched words, and Mi is the ith
matched word in M. This implies we prefer fewer
unmatched words in between two matched ones.
</bodyText>
<subsubsectionHeader confidence="0.961546">
3.2.4 POS and Phrase Tags of Words (PT, PP)
</subsubsectionHeader>
<bodyText confidence="0.998938">
Exploiting only lexical features can sometimes
result in some false positive cases because two sets
of matched words can play different roles in the
sentences. See S1 and S2 in Table 2. as a possible
false positive case.
</bodyText>
<table confidence="0.926546333333333">
S1: The man likes the woman
S2: The woman is like the man
Word S1: Tag S2: Tag S1: Phrase S2: Phrase
man NN NN NP PP
like VBZ IN VP PP
woman NN NN VP NP
</table>
<tableCaption confidence="0.9914115">
Table 2. An example of matched words with different
tags and phrases
</tableCaption>
<bodyText confidence="0.9742748125">
Therefore, we further explore syntactic features
for plagiarism detection. To achieve this goal, we
utilize a parser to obtain POS and phrase tags of
the words. Then we design an equation to measure
the tag/phrase similarity.
num (matc hed words with identical tag)
Sim = num (matched words)
We paid special attention to the case that
transforms a sentence from an active form to a
passive-form or vice versa. A subject originally in
a Noun Phrase can become a Preposition Phrase,
i.e. “by ...”, in the passive form while the object in
a Verb Phrase can become a new subject in a Noun
Phrase. Here we utilize the Stanford Dependency
provided by Stanford Parser to match the
tag/phrase between active and passive sentences.
</bodyText>
<subsubsectionHeader confidence="0.95945">
3.2.5 Semantic Similarity (LDA)
</subsubsectionHeader>
<bodyText confidence="0.999497941176471">
Plagiarists, sometimes, change words or phrases to
those with similar meanings. While previous works
(Y. Lin et al., 2006) often explore semantic
similarity using lexical databases such as WordNet
to find synonyms, we exploit a topic model,
specifically latent Dirichlet allocation (LDA, D. M.
Blei et al., 2003), to extract the semantic features
of sentences. Given a set of documents represented
by their word sequences, and a topic number n,
LDA learns the word distribution for each topic
and the topic distribution for each document which
maximize the likelihood of the word co-occurrence
in a document. The topic distribution is often taken
as semantics of a document. We use LDA to obtain
the topic distribution of a query and a candidate
snippet, and compare the cosine similarity of them
as a measure of their semantic similarity.
</bodyText>
<subsectionHeader confidence="0.99786">
3.3 Ensemble Similarity Scores
</subsectionHeader>
<bodyText confidence="0.999993083333333">
Up to this point, for each snippet the system
generates six similarity scores to measure the
degree of plagiarism in different aspects. In this
stage, we propose two strategies to linearly
combine the scores to make better prediction. The
first strategy utilizes each model’s predictability
(e.g. accuracy) as the weight to linearly combine
the scores. In other words, the models that perform
better individually will obtain higher weights. In
the second strategy we exploit a learning model (in
the experiment section we use Liblinear) to learn
the weights directly.
</bodyText>
<subsectionHeader confidence="0.988748">
3.4 Document Level Plagiarism Detection
</subsectionHeader>
<bodyText confidence="0.99994725">
For each query from the input article, our system
assigns a degree-of-plagiarism score to some
plausible source URLs. Then, for each URL, the
system sums up all the scores it obtains as the final
score for document-level degree-of-plagiarism. We
set up a cutoff threshold to obtain the most
plausible URLs. At the end, our system highlights
the suspicious areas of plagiarism for display.
</bodyText>
<page confidence="0.99852">
147
</page>
<sectionHeader confidence="0.997702" genericHeader="method">
4. Evaluation
</sectionHeader>
<bodyText confidence="0.999771333333333">
We evaluate our system from two different angles.
We first evalaute the sentence level plagirism
detection using the PAN corpus in English. We
then evaluate the capability of the full system to
detect on-line plagiarism cases using annotated
results in Chinese.
</bodyText>
<subsectionHeader confidence="0.979342">
4.1 Sentence-based Evaluations
</subsectionHeader>
<bodyText confidence="0.999990153846154">
We want to compare our model with the state-of-
the-art methods, in particular the winning entries in
plagiarism detection competition in PAN 1 .
However, the competition in PAN is designed for
off-line plagiarism detection; the entries did not
exploit an IR system to search the Web like we do.
Nevertheless, we can still compare the core
component of our system, the sentence-based
measuring model with that of other systems. To
achieve such goal, we first randomly sampled 370
documents from PAN-2011 external plagiarism
corpus (M. Potthast et al., 2010) containing 2882
labeled plagiarism cases.
To obtain high-quality negative examples for
evaluation, we built a full-text index on the corpus
using Lucene package. Then we use the suspicious
passages as queries to search the whole dataset
using Lucene. Since there is length limitation in
Lucene (as well as in the real search engines), we
further break the 2882 plagiarism cases into 6477
queries. We then extract the top 30 snippets
returned by the search engine as the potential
negative candidates for each plagiarism case. Note
that for each suspicious passage, there is only one
target passage (given by the ground truth) that is
considered as a positive plagiarism case in this data,
and it can be either among these 30 cases or not.
However, we union these 30 cases with the ground
truth as a set, and use our (as well as the
competitors’) models to rank the degree-of-
plagiarism for all the candidates. We then evaluate
the rank by the area-under-PR-curve (AUC) score.
We compared our system with the winning entry of
PAN 2011 (Grman and Ravas, 2011) and the
stopword ngram model that claims to perform
better than this winning entry by Stamatatos (2011).
The results of each individual model and ensemble
using 5-fold cross validation are listed in Table 3.
It shows that NM is the best individual model, and
</bodyText>
<footnote confidence="0.665851">
1 The website of PAN-2011 is http://pan.webis.de/
</footnote>
<bodyText confidence="0.835904">
an ensemble of three features outperforms the
state-of-the-art by 26%.
</bodyText>
<table confidence="0.9922775">
IM RW AW PT PP LDA
0.876 0.596 0.537 0.551 0.521 0.596
Ours ensemble Pan-11 Stopword
Champion Igram
AUC 0.882 0.620 0.596
(NM+RW+PP)
</table>
<tableCaption confidence="0.9756745">
Table 3. (a) AUC for each individual model (b) AUC of
our ensemble and other state-of-the-art algorithms
</tableCaption>
<subsectionHeader confidence="0.994282">
4.2 Evaluating the Full System
</subsectionHeader>
<bodyText confidence="0.9998598">
To evaluate the overall system, we manually
collect 60 real-world review articles from the
Internet for books (20), movies (20), and music
albums (20). Unfortunately for an online system
like ours, there is no ground truth available for
recall measure. We conduct two differement
evalautions. First we use the 60 articles as inputs to
our system, ask 5 human annotators to check
whether the articles returned by our system can be
considered as plagiarism. Among all 60 review
articles, our system identifies a considerablely high
number of copy/paste articles, 231 in total.
However, identifying this type of plagiarism is
trivial, and has been done by many similar tools.
Instead we focus on the so-called smart-plagiarism
which cannot be found through quoting a query in
a search engine. Table 4. shows the precision of
the smart-plagiarism articles returned by our
system. The precision is very high and outperforms
a commertial tool Microsoft Plagiarism Detector.
</bodyText>
<table confidence="0.9987386">
Book Movie Music
Ours 280/288 88/110 979/1033
MPD (97%) (80%) (95%)
44/53 123/172 120/161
(83%) (72%) (75%)
</table>
<tableCaption confidence="0.999918">
Table 4. Precision of Smart Plagiarism
</tableCaption>
<bodyText confidence="0.999833857142857">
In the second evaluation, we first choose 30
reviews randomly. Then we use each of them as
queries into Google and retrieve a total of 5636
pieces of snippet candidates. We then ask 63
human beings to annotate whether those snippets
represent plagiarism cases of the original review
article. Eventually we have obtained an annotated
</bodyText>
<page confidence="0.996255">
148
</page>
<bodyText confidence="0.999894333333333">
dataset and found a total of 502 plagiarized
candidates with 4966 innocent ones for evalaution.
Table 5. shows the average AUC of 5-fold cross
validation. The results show that our method
outperforms the Pan-11 winner slightly, and much
better than the Stopword Ngram.
</bodyText>
<table confidence="0.991311428571429">
IM RW AW PT PP LDA
0.904 0.778 0.874 0.734 0.622 0.581
Ours ensemble Pan-11 Stopword
Champion Igram
AUC 0.919 0.893 0.568
(NM+RW+AW
+PT+PP+LDA)
</table>
<tableCaption confidence="0.970888">
Table 5. (a) AUC for each individual model (b) AUC of
our ensemble and other state-of-the-art algorithms
</tableCaption>
<subsectionHeader confidence="0.990678">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999970161290323">
There is some inconsistency of the performance of
single features in these two experiments. The main
reason we believe is that the plagiarism cases were
created in very different manners. Plagiarism cases
in PAN external source are created artificially
through word insertions, deletions, reordering and
synonym substitutions. As a result, features such as
word alignment and reordering do not perform
well because they did not consider the existence of
synonym word replacement. On the other hand,
real-world plagiarism cases returned by Google are
those with matching-words, and we can find better
performance for AW.
The performances of syntactic and semantic
features, namely PT, PP and LDA, are consistently
inferior than other features. It is because they often
introduce false-positives as there are some non-
plagiarism cases that might have highly overlapped
syntactic or semantic tags. Nevertheless,
experiments also show that these features can
improve the overall accuracy in ensemble.
We also found that the stopword Igram model
is not applicable universally. For one thing, it is
less suitable for on-line plagiarism detection, as the
length limitation for queries diminishes the
usability of stopword n-grams. For another,
Chinese seems to be a language that does not rely
as much on stopwords as the latin languages do to
maintain its syntax structure.
Samples of our system’s finding can be found
here, http://tinyurl.com/6pnhurz
</bodyText>
<sectionHeader confidence="0.997123" genericHeader="method">
5. Online Demo System
</sectionHeader>
<bodyText confidence="0.869214166666667">
We developed an online demos system using
JAVA (JDK 1.7). The system currently supports
the detection of documents in both English and
Chinese. Users can either upload the plain text file
of a suspicious document, or copy/paste the
content onto the text area, as shown below in
</bodyText>
<figureCaption confidence="0.894397166666667">
Figure 2.
Figure 2. Input Screen-Shot
Then the system will output some URLs and
snippets as the potential source of plagiarism. (see
Figure 3.)
Figure 3. Output Screen-Shot
</figureCaption>
<sectionHeader confidence="0.998347" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.9996557">
Comparing with other online plagiarism
detection systems, ours exploit more sophisticated
features by modeling how human beings plagiarize
online sources. We have exploited sentence-level
plagiarism detection on lexical, syntactic and
semantic levels. Another noticeable fact is that our
approach is almost language independent. Given a
parser and a POS tagger of a language, our
framework can be extended to support plagiarism
detection for that language.
</bodyText>
<page confidence="0.998674">
149
</page>
<sectionHeader confidence="0.981207" genericHeader="references">
7. References
</sectionHeader>
<reference confidence="0.999355253012048">
Salha Alzahrani, Naomie Salim, and Ajith Abraham,
“Understanding Plagiarism Linguistic Patterns,
Textual Features and Detection Methods “ in IEEE
Transactions on systems , man and cyberneticsPart C:
Applications and reviews, 2011
D. M. Blei, A. Y. Ng, M. I. Jordan, and J. Lafferty.
Latent dirichlet allocation. Journal of Machine
Learning Research, 3:2003, 2003.
Bear F. Braumoeller and Brian J. Gaines. 2001. Actions
Do Speak Louder Than Words: Deterring Plagiarism
with the Use of Plagiarism-Detection Software. In
Political Science &amp; Politics, 34(4):835-839.
Sergey Brin, James Davis, and Hector Garcia-molina.
1995. Copy Detection Mechanisms for Digital
Documents. In Proceedings of the ACM SIGMOD
Annual Conference, 24(2):398-409.
Alberto Barrón Cedeño and Paolo Rosso. 2009. On
Automatic Plagiarism Detection based on n-grams
Comparison. In Proceedings of the 31th European
Conference on IR Research on Advances in
Information Retrieval, ECIR 2009, LNCS 5478:696-
700, Springer-Verlag, and Berlin Heidelberg,
Jan Grman and Rudolf Ravas. 2011. Improved
implementation for finding text similarities in large
collections of data.In Proceedings of PAN 2011.
NamOh Kang, Alexander Gelbukh, and SangYong Han.
2006. PPChecker: Plagiarism Pattern Checker in
Document Copy Detection. In Proceedings of TSD-
2006, LNCS, 4188:661-667.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’ Shea, and Keeley Crockett. 2006. Sentence
Similarity Based on Semantic Nets and Corpus
Statistics. In Proceedings of the IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138-
1150.
Yi-Ting Liu, Heng-Rui Zhang, Tai-Wei Chen, and Wei-
Guang Teng. 2007. Extending Web Search for
Online Plagiarism Detection. In Proceedings of the
IEEE International Conference on Information Reuse
and Integration, IRI 2007.
Caroline Lyon, Ruth Barrett, and James Malcolm. 2004.
A Theoretical Basis to the Automated Detection of
Copying Between Texts, and its Practical
Implementation in the Ferret Plagiarism and
Collusion Detector. In Proceedings of Plagiarism:
Prevention, Practice and Policies 2004 Conference.
Sebastian Niezgoda and Thomas P. Way. 2006.
SNITCH: A Software Tool for Detecting Cut and
Paste Plagiarism. In Proceedings of the 37th SIGCSE
Technical Symposium on Computer Science
Education, p.51-55.
Maria Soledad Pera and Yiu-kai Ng. 2010. IOS Press
SimPaD: A Word-Similarity Sentence-Based
Plagiarism Detection Tool on Web Documents. In
Journal on Web Intelligence and Agent Systems, 9(1).
Xuan-Hieu Phan and Cam-Tu Nguyen. GibbsLDA++:
A C/C++ implementation of latent Dirichlet
allocation (LDA), 2007
Martin Potthast, Benno Stein, Alberto Barrón Cedeño,
and Paolo Rosso. An Evaluation Framework for
Plagiarism Detection. In 23rd International
Conference on Computational Linguistics (COLING
10), August 2010. Association for Computational
Linguistics.
Kenneth Sörensena and Marc Sevaux. 2005.
Permutation Distance Measures for Memetic
Algorithms with Population Management. In
Proceedings of 6th Metaheuristics International
Conference.
Efstathios Stamatatos, &amp;quot;Plagiarism Detection Based on
Structural Information&amp;quot; in Proceedings of the 20th
ACM international conference on Information and
knowledge management, CIKM&apos;11
Robert A. Wagner and Michael J. Fischer. 1975. The
String-to-string correction problem. In Journal of the
ACM, 21(1):168-173.
Daniel R. White and Mike S. Joy. 2004. Sentence-Based
Natural Language Plagiarism Detection. In Journal
on Educational Resources in Computing JERIC
Homepage archive, 4(4).
Du Zou, Wei-jiang Long, and Zhang Ling. 2010. A
Cluster-Based Plagiarism Detection Method. In Lab
Report for PAN at CLEF 2010.
</reference>
<page confidence="0.998304">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001288">
<title confidence="0.997441">Online Plagiarism Detection Through Exploiting Lexical, Syntactic, Semantic Information</title>
<author confidence="0.998231">Lin Nanyun Peng Yen Shou-de_Lin</author>
<affiliation confidence="0.71454625">Graduate Institute Networking Multimedia, Taiwan University</affiliation>
<email confidence="0.6972395">r99944016@csie.ntu.edu.tw</email>
<affiliation confidence="0.966559666666667">Institute Linguistic, University</affiliation>
<email confidence="0.780854">pengnanyun@pku.edu.cn</email>
<affiliation confidence="0.70702625">Graduate Institute Networking Multimedia, Taiwan University</affiliation>
<email confidence="0.7089995">r96944016@csie.ntu.edu.tw</email>
<affiliation confidence="0.7099155">Graduate Institute Networking Multimedia, Taiwan University</affiliation>
<email confidence="0.865109">sdlin@csie.ntu.edu.tw</email>
<abstract confidence="0.999908153846154">In this paper, we introduce a framework that identifies online plagiarism by exploiting lexical, syntactic and semantic features that includes duplication-gram, reordering and alignment of words, POS and phrase tags, and semantic similarity of sentences. We establish an ensemble framework to combine the predictions of each model. Results demonstrate that our system can not only find considerable amount of real-world online plagiarism cases but also outperforms several state-of-the-art algorithms and commercial software.</abstract>
<keyword confidence="0.7461115">Keywords Plagiarism Detection, Lexical, Syntactic, Semantic</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salha Alzahrani</author>
<author>Naomie Salim</author>
<author>Ajith Abraham</author>
</authors>
<title>Understanding Plagiarism Linguistic Patterns, Textual Features and Detection Methods “</title>
<date>2011</date>
<journal>in IEEE Transactions on</journal>
<marker>Alzahrani, Salim, Abraham, 2011</marker>
<rawString>Salha Alzahrani, Naomie Salim, and Ajith Abraham, “Understanding Plagiarism Linguistic Patterns, Textual Features and Detection Methods “ in IEEE Transactions on systems , man and cyberneticsPart C: Applications and reviews, 2011</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
<author>J Lafferty</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="9225" citStr="Blei et al., 2003" startWordPosition="1463" endWordPosition="1466">an become a Preposition Phrase, i.e. “by ...”, in the passive form while the object in a Verb Phrase can become a new subject in a Noun Phrase. Here we utilize the Stanford Dependency provided by Stanford Parser to match the tag/phrase between active and passive sentences. 3.2.5 Semantic Similarity (LDA) Plagiarists, sometimes, change words or phrases to those with similar meanings. While previous works (Y. Lin et al., 2006) often explore semantic similarity using lexical databases such as WordNet to find synonyms, we exploit a topic model, specifically latent Dirichlet allocation (LDA, D. M. Blei et al., 2003), to extract the semantic features of sentences. Given a set of documents represented by their word sequences, and a topic number n, LDA learns the word distribution for each topic and the topic distribution for each document which maximize the likelihood of the word co-occurrence in a document. The topic distribution is often taken as semantics of a document. We use LDA to obtain the topic distribution of a query and a candidate snippet, and compare the cosine similarity of them as a measure of their semantic similarity. 3.3 Ensemble Similarity Scores Up to this point, for each snippet the sy</context>
</contexts>
<marker>Blei, Ng, Jordan, Lafferty, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, M. I. Jordan, and J. Lafferty. Latent dirichlet allocation. Journal of Machine Learning Research, 3:2003, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bear F Braumoeller</author>
<author>Brian J Gaines</author>
</authors>
<title>Actions Do Speak Louder Than Words: Deterring Plagiarism with the Use of Plagiarism-Detection Software.</title>
<date>2001</date>
<booktitle>In Political Science &amp; Politics,</booktitle>
<pages>34--4</pages>
<contexts>
<context position="1450" citStr="Braumoeller and Gaines, 2001" startWordPosition="185" endWordPosition="188">ork to combine the predictions of each model. Results demonstrate that our system can not only find considerable amount of real-world online plagiarism cases but also outperforms several state-of-the-art algorithms and commercial software. Keywords Plagiarism Detection, Lexical, Syntactic, Semantic 1. Introduction Online plagiarism, the action of trying to create a new piece of writing by copying, reorganizing or rewriting others’ work identified through search engines, is one of the most commonly seen misusage of the highly matured web technologies. As implied by the experiment conducted by (Braumoeller and Gaines, 2001), a powerful plagiarism detection system can effectively discourage people from plagiarizing others’ work. A common strategy people adopt for onlineplagiarism detection is as follows. First they identify several suspicious sentences from the write-up and feed them one by one as a query to a search engine to obtain a set of documents. Then human reviewers can manually examine whether these documents are truly the sources of the suspicious sentences. While it is quite straightforward and effective, the limitation of this strategy is obvious. First, since the length of search query is limited, su</context>
</contexts>
<marker>Braumoeller, Gaines, 2001</marker>
<rawString>Bear F. Braumoeller and Brian J. Gaines. 2001. Actions Do Speak Louder Than Words: Deterring Plagiarism with the Use of Plagiarism-Detection Software. In Political Science &amp; Politics, 34(4):835-839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>James Davis</author>
<author>Hector Garcia-molina</author>
</authors>
<title>Copy Detection Mechanisms for Digital Documents.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACM SIGMOD Annual Conference,</booktitle>
<pages>24--2</pages>
<marker>Brin, Davis, Garcia-molina, 1995</marker>
<rawString>Sergey Brin, James Davis, and Hector Garcia-molina. 1995. Copy Detection Mechanisms for Digital Documents. In Proceedings of the ACM SIGMOD Annual Conference, 24(2):398-409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Barrón Cedeño</author>
<author>Paolo Rosso</author>
</authors>
<title>On Automatic Plagiarism Detection based on n-grams Comparison.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR 2009, LNCS 5478:696-700,</booktitle>
<location>Springer-Verlag, and Berlin Heidelberg,</location>
<marker>Cedeño, Rosso, 2009</marker>
<rawString>Alberto Barrón Cedeño and Paolo Rosso. 2009. On Automatic Plagiarism Detection based on n-grams Comparison. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR 2009, LNCS 5478:696-700, Springer-Verlag, and Berlin Heidelberg,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Grman</author>
<author>Rudolf Ravas</author>
</authors>
<title>Improved implementation for finding text similarities in large collections of data.In</title>
<date>2011</date>
<booktitle>Proceedings of PAN</booktitle>
<contexts>
<context position="12704" citStr="Grman and Ravas, 2011" startWordPosition="2028" endWordPosition="2031">eturned by the search engine as the potential negative candidates for each plagiarism case. Note that for each suspicious passage, there is only one target passage (given by the ground truth) that is considered as a positive plagiarism case in this data, and it can be either among these 30 cases or not. However, we union these 30 cases with the ground truth as a set, and use our (as well as the competitors’) models to rank the degree-ofplagiarism for all the candidates. We then evaluate the rank by the area-under-PR-curve (AUC) score. We compared our system with the winning entry of PAN 2011 (Grman and Ravas, 2011) and the stopword ngram model that claims to perform better than this winning entry by Stamatatos (2011). The results of each individual model and ensemble using 5-fold cross validation are listed in Table 3. It shows that NM is the best individual model, and 1 The website of PAN-2011 is http://pan.webis.de/ an ensemble of three features outperforms the state-of-the-art by 26%. IM RW AW PT PP LDA 0.876 0.596 0.537 0.551 0.521 0.596 Ours ensemble Pan-11 Stopword Champion Igram AUC 0.882 0.620 0.596 (NM+RW+PP) Table 3. (a) AUC for each individual model (b) AUC of our ensemble and other state-of-</context>
</contexts>
<marker>Grman, Ravas, 2011</marker>
<rawString>Jan Grman and Rudolf Ravas. 2011. Improved implementation for finding text similarities in large collections of data.In Proceedings of PAN 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NamOh Kang</author>
<author>Alexander Gelbukh</author>
<author>SangYong Han</author>
</authors>
<title>PPChecker: Plagiarism Pattern Checker in Document Copy Detection.</title>
<date>2006</date>
<booktitle>In Proceedings of TSD2006, LNCS,</booktitle>
<pages>4188--661</pages>
<marker>Kang, Gelbukh, Han, 2006</marker>
<rawString>NamOh Kang, Alexander Gelbukh, and SangYong Han. 2006. PPChecker: Plagiarism Pattern Checker in Document Copy Detection. In Proceedings of TSD2006, LNCS, 4188:661-667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’ Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity Based on Semantic Nets and Corpus Statistics.</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE Transactions on Knowledge and Data Engineering,</booktitle>
<pages>18--8</pages>
<marker>Li, McLean, Bandar, Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. O’ Shea, and Keeley Crockett. 2006. Sentence Similarity Based on Semantic Nets and Corpus Statistics. In Proceedings of the IEEE Transactions on Knowledge and Data Engineering, 18(8):1138-1150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Ting Liu</author>
<author>Heng-Rui Zhang</author>
<author>Tai-Wei Chen</author>
<author>WeiGuang Teng</author>
</authors>
<title>Extending Web Search for Online Plagiarism Detection.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Information Reuse and Integration, IRI</booktitle>
<marker>Liu, Zhang, Chen, Teng, 2007</marker>
<rawString>Yi-Ting Liu, Heng-Rui Zhang, Tai-Wei Chen, and WeiGuang Teng. 2007. Extending Web Search for Online Plagiarism Detection. In Proceedings of the IEEE International Conference on Information Reuse and Integration, IRI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>Ruth Barrett</author>
<author>James Malcolm</author>
</authors>
<title>A Theoretical Basis to the Automated Detection of Copying Between Texts, and its Practical Implementation in the Ferret Plagiarism and Collusion Detector.</title>
<date>2004</date>
<booktitle>In Proceedings of Plagiarism: Prevention, Practice and Policies</booktitle>
<note>Conference.</note>
<marker>Lyon, Barrett, Malcolm, 2004</marker>
<rawString>Caroline Lyon, Ruth Barrett, and James Malcolm. 2004. A Theoretical Basis to the Automated Detection of Copying Between Texts, and its Practical Implementation in the Ferret Plagiarism and Collusion Detector. In Proceedings of Plagiarism: Prevention, Practice and Policies 2004 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Niezgoda</author>
<author>Thomas P Way</author>
</authors>
<title>SNITCH: A Software Tool for Detecting Cut and Paste Plagiarism.</title>
<date>2006</date>
<booktitle>In Proceedings of the 37th SIGCSE Technical Symposium on Computer Science Education,</booktitle>
<pages>51--55</pages>
<marker>Niezgoda, Way, 2006</marker>
<rawString>Sebastian Niezgoda and Thomas P. Way. 2006. SNITCH: A Software Tool for Detecting Cut and Paste Plagiarism. In Proceedings of the 37th SIGCSE Technical Symposium on Computer Science Education, p.51-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Soledad Pera</author>
<author>Yiu-kai Ng</author>
</authors>
<title>SimPaD: A Word-Similarity Sentence-Based Plagiarism Detection Tool on Web Documents.</title>
<date>2010</date>
<journal>In Journal on Web Intelligence and Agent Systems,</journal>
<volume>9</volume>
<issue>1</issue>
<publisher>IOS Press</publisher>
<marker>Pera, Ng, 2010</marker>
<rawString>Maria Soledad Pera and Yiu-kai Ng. 2010. IOS Press SimPaD: A Word-Similarity Sentence-Based Plagiarism Detection Tool on Web Documents. In Journal on Web Intelligence and Agent Systems, 9(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
<author>Cam-Tu Nguyen</author>
</authors>
<title>GibbsLDA++: A C/C++ implementation of latent Dirichlet allocation (LDA),</title>
<date>2007</date>
<marker>Phan, Nguyen, 2007</marker>
<rawString>Xuan-Hieu Phan and Cam-Tu Nguyen. GibbsLDA++: A C/C++ implementation of latent Dirichlet allocation (LDA), 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Alberto Barrón Cedeño</author>
<author>Paolo Rosso</author>
</authors>
<title>An Evaluation Framework for Plagiarism Detection.</title>
<date>2010</date>
<booktitle>In 23rd International Conference on Computational Linguistics (COLING 10),</booktitle>
<contexts>
<context position="11647" citStr="Potthast et al., 2010" startWordPosition="1849" endWordPosition="1852">using annotated results in Chinese. 4.1 Sentence-based Evaluations We want to compare our model with the state-ofthe-art methods, in particular the winning entries in plagiarism detection competition in PAN 1 . However, the competition in PAN is designed for off-line plagiarism detection; the entries did not exploit an IR system to search the Web like we do. Nevertheless, we can still compare the core component of our system, the sentence-based measuring model with that of other systems. To achieve such goal, we first randomly sampled 370 documents from PAN-2011 external plagiarism corpus (M. Potthast et al., 2010) containing 2882 labeled plagiarism cases. To obtain high-quality negative examples for evaluation, we built a full-text index on the corpus using Lucene package. Then we use the suspicious passages as queries to search the whole dataset using Lucene. Since there is length limitation in Lucene (as well as in the real search engines), we further break the 2882 plagiarism cases into 6477 queries. We then extract the top 30 snippets returned by the search engine as the potential negative candidates for each plagiarism case. Note that for each suspicious passage, there is only one target passage (</context>
</contexts>
<marker>Potthast, Stein, Cedeño, Rosso, 2010</marker>
<rawString>Martin Potthast, Benno Stein, Alberto Barrón Cedeño, and Paolo Rosso. An Evaluation Framework for Plagiarism Detection. In 23rd International Conference on Computational Linguistics (COLING 10), August 2010. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Sörensena</author>
<author>Marc Sevaux</author>
</authors>
<title>Permutation Distance Measures for Memetic Algorithms with Population Management.</title>
<date>2005</date>
<booktitle>In Proceedings of 6th Metaheuristics International Conference.</booktitle>
<contexts>
<context position="6418" citStr="Sörensena and Sevaux, 2005" startWordPosition="951" endWordPosition="954">lap percentage with the n-grams in the target. The larger n is, the harder for this feature to detect plagiarism with insertion, replacement, and deletion. In the experiment, we choose n=2. 3.2.2 Reordering of Words (RW) Plagiarism can come from the reordering of words. We argue that the permutation distance between S1 and S2 is an important indicator for reordered plagiarism. The permutation distance is defined as the minimum number of pair-wise exchanging of matched words needed to transform a sentence, S2, to contain the same order of matched words as another sentence, S1. As mentioned in (Sörensena and Sevaux, 2005), the permutation distance can be calculated by the following expression 𝑑 𝑆1,𝑆2 = 𝑧𝑖𝑗 𝑛−1 𝑛 𝑖=1 𝑗 =𝑖+1 where 1, 𝑖𝑓 𝑆1 𝑗 &gt; 𝑆1 𝑖 𝑎𝑛𝑑 𝑆2 𝑗 &lt; 𝑆2 𝑖 𝑧𝑖𝑗 = k 0, 𝑜𝑡𝑒𝑟𝑤𝑖𝑠𝑒 S1(i) and S2(i) are indices of the ith matched word in sentences S1 and S2 respectively and n is the number of matched words between the sentences S1 and S2. Let μ = n2− n 2 be the normalized term, which is the maximum possible distance between S1 and S2, then the reordering 146 score of the two sentences, expressed as s(S1, S2), will be s (S1, S2 ) = 1 − d (S1,S2) µ 3.2.3 Alignment of Words (AW) Besides reordering, plagiarists oft</context>
</contexts>
<marker>Sörensena, Sevaux, 2005</marker>
<rawString>Kenneth Sörensena and Marc Sevaux. 2005. Permutation Distance Measures for Memetic Algorithms with Population Management. In Proceedings of 6th Metaheuristics International Conference.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>Plagiarism Detection Based on Structural Information&amp;quot;</title>
<booktitle>in Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM&apos;11</booktitle>
<marker>Stamatatos, </marker>
<rawString>Efstathios Stamatatos, &amp;quot;Plagiarism Detection Based on Structural Information&amp;quot; in Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM&apos;11</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fischer</author>
</authors>
<title>The String-to-string correction problem.</title>
<date>1975</date>
<journal>In Journal of the ACM,</journal>
<pages>21--1</pages>
<contexts>
<context position="7239" citStr="Wagner and Fischer, 1975" startWordPosition="1110" endWordPosition="1113">the ith matched word in sentences S1 and S2 respectively and n is the number of matched words between the sentences S1 and S2. Let μ = n2− n 2 be the normalized term, which is the maximum possible distance between S1 and S2, then the reordering 146 score of the two sentences, expressed as s(S1, S2), will be s (S1, S2 ) = 1 − d (S1,S2) µ 3.2.3 Alignment of Words (AW) Besides reordering, plagiarists often insert or delete words in a sentence. We try to model such behavior by finding the alignment of two word sequences. We perform the alignment using a dynamic programming method as mentioned in (Wagner and Fischer, 1975). However, such alignment score does not reflect the continuity of the matched words, which can be an important cue to identify plagiarism. To overcome such drawback, we modify the score as below. |M|−1 New Alignment Score = Y- Gi i=1 |M|−1 1 where Gi = # o f words between (M i,M i+1 )+1 M is the list of matched words, and Mi is the ith matched word in M. This implies we prefer fewer unmatched words in between two matched ones. 3.2.4 POS and Phrase Tags of Words (PT, PP) Exploiting only lexical features can sometimes result in some false positive cases because two sets of matched words can pla</context>
</contexts>
<marker>Wagner, Fischer, 1975</marker>
<rawString>Robert A. Wagner and Michael J. Fischer. 1975. The String-to-string correction problem. In Journal of the ACM, 21(1):168-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel R White</author>
<author>Mike S Joy</author>
</authors>
<title>Sentence-Based Natural Language Plagiarism Detection.</title>
<date>2004</date>
<booktitle>In Journal on Educational Resources in Computing JERIC Homepage archive,</booktitle>
<pages>4--4</pages>
<marker>White, Joy, 2004</marker>
<rawString>Daniel R. White and Mike S. Joy. 2004. Sentence-Based Natural Language Plagiarism Detection. In Journal on Educational Resources in Computing JERIC Homepage archive, 4(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Du Zou</author>
<author>Wei-jiang Long</author>
<author>Zhang Ling</author>
</authors>
<title>A Cluster-Based Plagiarism Detection Method. In Lab Report for PAN at CLEF</title>
<date>2010</date>
<contexts>
<context position="2896" citStr="Zou et al., 2010" startWordPosition="396" endWordPosition="399">in websites requires specific domain and language knowledge as well as considerable amount of energy and time. To overcome the above shortcomings, we introduce an online plagiarism detection system using natural language processing techniques to simulate the above reverse-engineering approach. We develop an ensemble framework that integrates lexical, syntactic and semantic features to achieve this goal. Our system is language independent and we have implemented both Chinese and English versions for evaluation. 2. Related Work Plagiarism detection has been widely discussed in the past decades (Zou et al., 2010). Table 1. summarizes some of them: Author Comparison Similarity Function Unit Brin et al., Word + Percentage of matching 1995 Sentence sentences. White and Sentence Average overlap ratio of Joy, 2004 the sentence pairs using 2 pre-defined thresholds. Niezgoda A human Sliding windows ranked and Way, defined by the average length per 2006 sliding word. window Cedeno and Sentence + Overlap percentage of nRosso, 2009 n-gram gram in the sentence pairs. 145 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 145–150, Jeju, Republic of Korea, 8-14 July 2012</context>
</contexts>
<marker>Zou, Long, Ling, 2010</marker>
<rawString>Du Zou, Wei-jiang Long, and Zhang Ling. 2010. A Cluster-Based Plagiarism Detection Method. In Lab Report for PAN at CLEF 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>