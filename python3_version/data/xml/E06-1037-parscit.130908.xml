<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.156849">
<title confidence="0.992704">
Using Reinforcement Learning to Build a Better Model of Dialogue State
</title>
<author confidence="0.9462">
Joel R. Tetreault Diane J. Litman
</author>
<affiliation confidence="0.850141666666667">
University of Pittsburgh University of Pittsburgh
Learning Research and Development Center Department of Computer Science &amp;
Pittsburgh PA, 15260, USA Learning Research and Development Center
</affiliation>
<email confidence="0.754737">
tetreaul@pitt.edu Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902157894737">
Given the growing complexity of tasks
that spoken dialogue systems are trying to
handle, Reinforcement Learning (RL) has
been increasingly used as a way of au-
tomatically learning the best policy for a
system to make. While most work has
focused on generating better policies for
a dialogue manager, very little work has
been done in using RL to construct a better
dialogue state. This paper presents a RL
approach for determining what dialogue
features are important to a spoken dia-
logue tutoring system. Our experiments
show that incorporating dialogue factors
such as dialogue acts, emotion, repeated
concepts and performance play a signifi-
cant role in tutoring and should be taken
into account when designing dialogue sys-
tems.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999834863636364">
This paper presents initial research toward the
long-term goal of designing a tutoring system that
can effectively adapt to the student. While most
work in Markov Decision Processes (MDPs) and
spoken dialogue have focused on building better
policies (Walker, 2000; Henderson et al., 2005), to
date very little empirical work has tested the utility
of adding specialized features to construct a better
dialogue state. We wish to show that adding more
complex factors to a representation of student state
is a worthwhile pursuit, since it alters what action
the tutor should make. The five dialogue factors
we explore are dialogue acts, certainty level, frus-
tration level, concept repetition, and student per-
formance. All five are factors that are not just
unique to the tutoring domain but are important
to dialogue systems in general. Our results show
that using these features, combined with the com-
mon baseline of student correctness, leads to a sig-
nificant change in the policies produced, and thus
should be taken into account when designing a
system.
</bodyText>
<sectionHeader confidence="0.995259" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999672103448276">
We follow past lines of research (such as (Singh
et al., 1999)) for describing a dialogue as a tra-
jectory within a Markov Decision Process (Sutton
and Barto, 1998). A MDP has four main com-
ponents: states, actions, a policy, which specifies
what is the best action to take in a state, and a re-
ward function which specifies the utility of each
state and the process as a whole. Dialogue man-
agement is easily described using a MDP because
one can consider the actions as actions made by
the system, the state as the dialogue context, and
a reward which for many dialogue systems tends
to be task completion success or dialogue length.
Typically the state is viewed as a vector of features
such as dialogue history, speech recognition con-
fidence, etc.
The goal of using MDPs is to determine the best
policy for a certain state and action space. That
is, we wish to find the best combination of states
and actions to maximize the reward at the end of
the dialogue. In most dialogues, the exact reward
for each state is not known immediately, in fact,
usually only the final reward is known at the end
of the dialogue. As long as we have a reward func-
tion, Reinforcement Learning allows one to auto-
matically compute the best policy. The following
recursive equation gives us a way of calculating
the expected cumulative value (V-value) of a state
(-value):
</bodyText>
<page confidence="0.996253">
289
</page>
<bodyText confidence="0.999684208333333">
Here is the best action for state at this
time, is the probability of getting from state to
via . This is multiplied by the sum of the re-
ward for that traversal plus the value of the new
state multiplied by a discount factor . ranges
between 0 and 1 and discounts the value of past
states. The policy iteration algorithm (Sutton and
Barto, 1998) iteratively updates the value of each
state V(s) based on the values of its neighboring
states. The iteration stops when each update yields
an epsilon difference (implying that V(s) has con-
verged) and we select the action that produces the
highest V-value for that state.
Normally one would want a dialogue system to
interact with users thousands of times to explore
the entire traversal space of the MDP, however in
practice that is very time-consuming. Instead, the
next best tactic is to train the MDP (that is, cal-
culate transition probabilities for getting from one
state to another, and the reward for each state) on
already collected data. Of course, the whole space
will not be considered, but if one reduces the size
of the state vector effectively, data size becomes
less of an issue (Singh et al., 2002).
</bodyText>
<sectionHeader confidence="0.995379" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.999960811594203">
For our study, we used an annotated corpus of
20 human-computer spoken dialogue tutoring ses-
sions. Each session consists of an interaction with
one student over 5 different college-level physics
problems, for a total of 100 dialogues. Before the
5 problems, the student is asked to read physics
material for 30 minutes and then take a pre-test
based on that material. Each problem begins with
the student writing out a short essay response to
the question posed by the computer tutor. The sys-
tem reads the essay and detects the problem areas
and then starts a dialogue with the student asking
questions regarding the confused concepts. Infor-
mally, the dialogue follows a question-answer for-
mat. Each of the dialogues has been manually au-
thored in advance meaning that the system has a
response based on the correctness of the student’s
last answer. Once the student has successfully an-
swered all the questions, he or she is asked to cor-
rect the initial essay. On average, each of the di-
alogues takes 20 minutes and contains 25 student
turns. Finally, the student is given a post-test sim-
ilar to the pre-test, from which we can calculate
their normalized learning gain:
Prior to our study, the corpus was then anno-
tated for Student and Tutor Moves (see Tables 1
and 2) which can be viewed as Dialogue Acts
(Forbes-Riley et al., 2005). Note that tutor and stu-
dent turns can consist of multiple utterances and
can thus be labeled with multiple moves. For ex-
ample, a tutor can give feedback and then ask a
question in the same turn. Whether to include
feedback will be the action choice addressed in
this paper since it is an interesting open ques-
tion in the Intelligent Tutoring Systems (ITS) com-
munity. Student Moves refer to the type of an-
swer a student gives. Answers that involve a con-
cept already introduced in the dialogue are called
Shallow, answers that involve a novel concept are
called Novel, “I don’t know” type answers are
called Assertions (As), and Deep answers refer to
answers that involve linking two concepts through
reasoning. In our study, we merge all non-Shallow
moves into a new move “Other.”
In addition to Student Moves, we annotated five
other features to include in our representation of
the student state. Two emotion related features
were annotated manually (Forbes-Riley and Lit-
man, 2005): certainty and frustration. Certainty
describes how confident a student seemed to be in
his answer, while frustration describes how frus-
trated the student seemed to be in his last response.
We include three other features for the Student
state that were extracted automatically. Correct-
ness says if the last student answer was correct or
incorrect. As noted above, this is what most cur-
rent tutoring systems use as their state. Percent
Correct is the percentage of questions in the cur-
rent problem the student has answered correctly so
far. Finally, if a student performs poorly when it
comes to a certain topic, the system may be forced
to repeat a description of that concept again (con-
cept repetition).
It should be noted that all the dialogues were
authored beforehand by physics experts. For ev-
ery turn there is a list of possible correct, incor-
rect and partially correct answers the student can
make, and then for each one of these student re-
sponses a link to the next turn. In addition to
</bodyText>
<page confidence="0.99445">
290
</page>
<table confidence="0.9995222">
State Parameters
Student Move Shallow (S)
Novel &amp; As &amp; Deep (O)
Certainty Certain, Uncertain, Neutral
Frustration Frustrated (F), Neutral (N),
Correctness Correct (C), Incorrect (I)
Partially Correct (PC)
Percent Correct 50-100% (High), 0-50% (Low)
Concept Repetition Concept is not repeated (0),
Concept is repeated (R)
</table>
<tableCaption confidence="0.99813">
Table 1: Student Features in Tutoring Corpus
</tableCaption>
<table confidence="0.999496833333333">
Action Parameters
Tutor Feedback Act Positive, Negative
Tutor Question Act Short Answer Question (SAQ)
Complex Answer Question (CAQ)
Tutor State Act Restatement, Recap, Hint
Expansion, Bottom Out
</table>
<tableCaption confidence="0.998899">
Table 2: Tutor Acts in Tutoring Corpus
</tableCaption>
<bodyText confidence="0.999927">
explaining physics concepts, the authors also in-
clude feedback and other types of helpful mea-
sures (such as hints or restatements) to help the
student along. These were not written with the
goal of how best to influence student state. Our
goal in this study is to automatically learn from
this corpus which state-action patterns evoke the
highest learning gain.
</bodyText>
<sectionHeader confidence="0.999036" genericHeader="method">
4 Infrastructure
</sectionHeader>
<bodyText confidence="0.99995475">
To test different hypotheses of what features best
approximate the student state and what are the best
actions for a tutor to consider, one must have a
flexible system that allows one to easily test dif-
ferent configurations of states and actions. To ac-
complish this, we designed a system similar to
the Reinforcement Learning for Dialogue Systems
(RLDS) (Singh et al., 1999). The system allows a
system designer to specify what features will com-
pose the state and actions as well as perform oper-
ations on each individual feature. For instance, the
tool allows the user to collapse features together
(such as collapsing all Question Acts together into
one) or quantize features that have continuous val-
ues (such as the number of utterances in the di-
alogue so far). These collapsing functions allow
the user to easily constrain the trajectory space. To
further reduce the search space for the MDP, our
tool allows the user to specify a threshold to com-
bine states that occur less than the threshold into a
single “threshold state.” In addition, the user can
specify a reward function and a discount factor,
For this study, we use a threshold of 50 and a
discount factor of 0.9, which is also what is com-
monly used in other RL models, such as (Framp-
ton and Lemon, 2005). For the dialogue reward
function, we did a median split on the 20 students
based on their normalized learning gain, which is
a standard evaluation metric in the Intelligent Tu-
toring Systems community. So 10 students and
their respective 5 dialogues were assigned a posi-
tive reward of +100 (high learners), and the other
10 students and their respective dialogues were as-
signed a negative reward of -100 (low learners). It
should be noted that a student’s 5 dialogues were
assigned the same reward since there was no way
to approximate their learning gain in the middle of
a session.
The output of the tool is a probability matrix
over the user-specified states and actions. This
matrix is then passed to an MDP toolkit (Chades et
al., 2005) written in Matlab.1 The toolkit performs
policy iteration and generates a policy as well as a
list of V-values for each state.
</bodyText>
<sectionHeader confidence="0.997051" genericHeader="method">
5 Experimental Method
</sectionHeader>
<bodyText confidence="0.999985777777778">
With the infrastructure created and the MDP pa-
rameters set, we can then move on to the goal of
this experiment - to see what sources of informa-
tion impact a tutoring dialogue system. First, we
need to develop a baseline to compare the effects
of adding more information. Second, we gener-
ate a new policy by adding the new information
source to the baseline state. However, since we
are currently not running any new experiments to
test our policy, or evaluating over user simulations,
we evaluate the reliability of our policies by look-
ing at how well they converge over time, that is, if
you incrementally add more data (ie. a student’s 5
dialogues) does the policy generated tend to stabi-
lize over time? And also, do the V-values for each
state stabilize over time as well? The intuition is
that if both the policies and V-values tend to con-
verge then we can be sure that the policy generated
is reasonable.
The first step in our experiment is to determine
a baseline. We use feedback as our system action
in our MDP. The action size is 3 (tutor can give
feedback (Feed), give feedback with another tutor
act (Mix), or give no feedback at all (NonFeed).
Examples from our corpus can be seen in Table 3.
It should be noted that “NonFeed” does not mean
that the student’s answer is not acknowledged, it
</bodyText>
<footnote confidence="0.9584675">
1MDP toolkit can be downloaded from
http://www.inra.fr/bia/T/MDPtoolbox/
</footnote>
<page confidence="0.98449">
291
</page>
<table confidence="0.9956086">
Case Tutor Moves Example Turn
Feed Pos “Super.”
Mix Pos, SAQ “Good. What is the direction of that force relative to your ft st?”
NonFeed Hint, CAQ “To analyze the pumpkin’s acceleration we will use Newton’s Second Law.
What is the deft nition of the law?”
</table>
<tableCaption confidence="0.998934">
Table 3: Tutor Action Examples
</tableCaption>
<bodyText confidence="0.999982263888889">
means that something more complex than a sim-
ple positive or negative phrase is given (such as a
Hint or Restatement). Currently, the system’s re-
sponse to a student depends only on whether or not
the student answered the last question correctly, so
we use correctness as the sole feature in our dia-
logue state. Recall that a student can either be cor-
rect, partially correct, or incorrect. Since partially
correct occurs infrequently compared to the other
two, we reduced the state size to two by combin-
ing Incorrect and Partially Correct into one state
(IPC) and keeping correct (C).
The third column of Table 4 has the resulting
learned MDP policy as well as the frequencies of
both states in the data. So for both states, the best
action for the tutor to make is to give feedback,
without knowing anything else about the student
state.
The second step in our experiment is to test
whether the policies generated are indeed reliable.
Normally, the best way to verify a policy is by con-
ducting experiments and seeing if the new policy
leads to a higher reward for the new dialogues. In
our context, this would entail running more sub-
jects with the augmented dialogue manager and
checking if the students had a higher learning gain
with the new policies. However, collecting data in
this fashion can take months. So, we take a differ-
ent tact of checking if the polices and values for
each state are indeed converging as we add data
to our MDP model. The intuition here is that if
both of those parameters were varying between a
corpus of 19 students to 20 students, then we can’t
assume that our policy is stable, and hence not re-
liable. However, if these parameters converged as
more data was added, this would indicate that the
MDP is reliable.
To test this out, we conducted a 20-fold cross-
averaging test over our corpus of 20 students.
Specifically, we made 20 random orderings of our
students to prevent any one ordering from giving a
false convergence. Each ordering was then chun-
ked into 20 cuts ranging from a size of 1 student, to
the entire corpus of 20 students. We then passed
each cut to our MDP infrastructure such that we
started with a corpus of just the first student of the
ordering and then determined a MDP policy for
that cut, then added another student to that original
corpus and reran our MDP system. We continue
this incremental addition of a student (5 dialogues)
until we completed all 20 students. So at the end,
we have 20 random orderings with 20 cuts each,
so 400 MDP trials were run. Finally, we average
the V-values of same size cuts together to produce
an average V-value for that cut size. The left-hand
graph in Figure 1 shows a plot of the average V-
values for each state against a cut. The state with
the plusses is the positive final state, and the one at
the bottom is the negative final state. However, we
are most concerned with how the non-final states
converge, which are the states in the middle. The
plot shows that for early cuts, there is a lot of in-
stability but then each state tends to stabilize after
cut 10. So this tells us that the V-values are fairly
stable and thus reliable when we derive policies
from the entire corpus of 20 students.
As a further test, we also check that the poli-
cies generated for each cut tend to stabilize over
time. That is, the differences between a policy at
a smaller cut and the final cut converge to zero as
more data is added. This “diffs” test is discussed
in more detail in Section 6.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999873571428571">
In this section, we investigate whether adding
more information to our student state will lead to
interesting policy changes. First, we add certainty
to our baseline of correctness, and then compare
this new baseline’s policy (henceforth Baseline 2)
with the policies generated when student moves,
frustration, concept repetition, and percent cor-
rectness are included. For each test, we employed
the same methodology as with the baseline case of
doing a 20-fold cross-averaging and examining if
the states’ V-values converge.
We first add certainty to correctness because
prior work (such as (Bhatt et al., 2004)) has shown
the importance of considering certainty in tutoring
</bodyText>
<page confidence="0.969639">
292
</page>
<figure confidence="0.999386233333333">
Correctness Correctness + Certainty
V−value 100
80
60
40
20
0
V−value
40
20
0
100
80
60
12 14 16 18 20
−20
−40
−60
−80
−100
0 2 4 6 8 10
B of students
−20
−40
−60
−80
−100
0 2 4 6 8 10
B of students
12 14 16 18 20
</figure>
<figureCaption confidence="0.999961">
Figure 1: Baseline 1 and Baseline 2 Convergence Plots
</figureCaption>
<bodyText confidence="0.999869388888889">
systems. For example, a student who is correct
and certain probably does not need a lot of feed-
back. But one that is correct but uncertain could
signal that the student is becoming doubtful or at
least confused about a concept. There are three
types of certainty: certain (cer), uncertain (unc),
and neutral (neu). Adding these to our state repre-
sentation increases state size from 2 to 6. The new
policy is shown in Table 4. The second and third
columns show the original baseline states and their
policies. The next column shows the new policy
when splitting the original state into the new three
states based on certainty, as well as the frequency
of the new state. So the first row can be interpreted
as if the student is correct and certain, one should
give no feedback; if the student is correct and neu-
tral, give feedback; and if the student is correct and
uncertain, give non-feedback.
</bodyText>
<table confidence="0.999680285714286">
# State Baseline +Certainty
1 C Feed (1308) cer: NonFeed (663)
neu: Feed (480)
unc: NonFeed (165)
2 IPC Feed (872) cer: NonFeed (251)
neu: Mix (377)
unc: NonFeed (244)
</table>
<tableCaption confidence="0.997699">
Table 4: Baseline Policies
</tableCaption>
<bodyText confidence="0.9998556875">
Our reasoning is that if a feature is important to
include in a state representation it should change
the policies of the old states. For example, if cer-
tainty did not impact how well students learned
(as deemed by the MDP) then the policies for
certainty, uncertainty, and neutral would be the
same as the original policy for Correct or Incor-
rect/Partially Correct, in this case they would be
Feed. However, the figures show otherwise as
when you add certainty to the state, only one new
state (C while being neutral) retains the old pol-
icy of having the tutor give feedback. The policies
which differ with the original are shown in bold.
So in general, the learned policy is that one
should not give feedback if the student is certain
or uncertain, but rather give some other form of
feedback such as a Hint or a Restatement perhaps.
But when the student is neutral with respect to cer-
tainty, one should give feedback. One way of in-
terpreting these results is that given our domain,
for students who are confident or not confident at
all in their last answer, there are better things to
say to improve their learning down the road than
“Great Job!” But if the student does not display a
lot of emotion, than one should use explicit posi-
tive or negative feedback to perhaps bolster their
confidence level.
The right hand graph in Figure 1 shows the con-
vergence plot for the baseline state with certainty.
It shows that as we add more data, the values for
each state converge. So in general, we can say that
the values for our Baseline 2 case are fairly stable.
Next, we add Student Moves, Frustration, Con-
cept Repetition, and Percent Correct features indi-
vidually to Baseline 2. The first graph in Figure
2 shows a plot of the convergence values for the
Percent Correct feature. We only show one con-
vergence plot since the other three are similar. The
result is that the V-values for all four converge af-
ter 14-15 students.
The second graph shows the differences in poli-
cies between the final cut of 20 students and all
smaller cuts. This check is necessary because
some states may exhibit stable V-values but actu-
ally be oscillating between two different policies
of equal values. So each point on the graph tells
us how many differences in policies there are be-
tween the cut in question and the final cut. For
</bodyText>
<page confidence="0.996497">
293
</page>
<bodyText confidence="0.998742882352941">
example, if the policy generated at cut 15 was to
give feedback for all states, and the policy at the fi-
nal cut was to give feedback for all but two states,
the “diff” for cut 15 would be two. So in the best
case, zero differences mean that the policies gen-
erated for both cuts are exactly the same. The
diff plots shows the differences decrease as data
is added and they exhibited very similar plots to
both Baseline cases. For cuts greater than 15, there
are still some differences but these are usually due
to low frequency states. So we can conclude that
since our policies are fairly stable they are worth
investigating in more detail.
In the remainder of this section, we look at the
differences between the Baseline 2 policies and
the policies generated by adding a new feature to
the Baseline 2 state. If adding a new feature actu-
ally does not really change what the tutor should
do (that is, the tutor will do the baseline policy
regardless of the new information), one can con-
clude that the feature is not worth including in a
student state. On the other hand, if adding the state
results in a much different policy, then the feature
is important to student modeling.
Student Move Feature The results of adding
Student Moves to Baseline 2 are shown in Table
5. Out of the 12 new states created, 7 deviate from
the original policy. The main trend is for the neu-
tral and uncertain states to give mixed feedback
after a student shallow move, and a non-feed re-
sponse when the student says something deep or
novel. When the student is certain, always give a
mixed response except in the case where he said
something Shallow and Correct.
</bodyText>
<table confidence="0.999850615384615">
# State Baseline New Policy
1 certain:C NonFeed S: NonFeed
O: Mix
2 certain:IPC NonFeed S: Mix
O: Mix
3 neutral:C Feed S: Feed
O: NonFeed
4 neutral:IPC Mix S: Mix
O: NonFeed
5 uncertain:C NonFeed S: Mix
O: NonFeed
6 uncertain:IPC NonFeed S: Mix
O: NonFeed
</table>
<tableCaption confidence="0.998827">
Table 5: Student Move Policies
</tableCaption>
<bodyText confidence="0.9911406875">
Concept Repetition Feature Table 6 shows
the new policy generated. Unlike the Student
Move policies which impacted all 6 of the base-
line states, Concept Repetition changes the poli-
cies for the first three baseline states resulting in
4 out of 12 new states differing from the baseline.
For states 1 through 4, the trend is that if the con-
cept has been repeated, the tutor should give feed-
back or a combination of feedback with another
Tutor Act. Intuitively, this seems clear because
if a concept were repeated it shows the student is
not understanding the concept completely and it
is neccessary to give them a little more feedback
than when they first see the concept. So, this test
indicates that keeping track of repeated concepts
has a significant impact on the policy generated.
</bodyText>
<table confidence="0.999821615384615">
# State Baseline New Policy
1 certain:C NonFeed 0: NonFeed
R: Feed
2 certain:IPC NonFeed 0: Mix
R: Mix
3 neutral:C Feed 0: Mix
R: Feed
4 neutral:IPC Mix 0: Mix
R: Mix
5 uncertain:C NonFeed 0: NonFeed
R: NonFeed
6 uncertain:IPC NonFeed 0: NonFeed
R: NonFeed
</table>
<tableCaption confidence="0.995707">
Table 6: Concept Repetition Policies
</tableCaption>
<bodyText confidence="0.99900332">
Frustration Feature Table 7 shows the new
policy generated. Comparing the baseline policy
with the new policy (which includes categories for
when the original state is either neutral or frus-
tration), shows that adding frustration changes the
policy for state 1, when the student is certain or
correct. In that case, the better option is to give
them positive feedback. For all other states, frus-
tration occurs with each of them so infrequently 2
that the resulting states appeared less than the our
threshold of 50 instances. As a result, these 5 frus-
tration states are grouped together in the “thresh-
old state” and our MDP found that the best policy
when in that state is to give no feedback. So the
two neutral states change when the student is frus-
trated. Interestingly, for students that are uncer-
tain, the policy does not change if they are frus-
trated or neutral. The trend is to always give Non-
Feedback.
Percent Correctness Feature Table 8 shows
the new policy generated for incorporating a sim-
ple model of current student performance within
the dialog. This feature, along with Frustration,
seems to impact the baseline the state least since
both only alter the policies for 3 of the 12 new
</bodyText>
<footnote confidence="0.958991">
2Only 225 out of 2180 student turns are marked as frus-
tration, while all the others are neutral
</footnote>
<page confidence="0.99676">
294
</page>
<figure confidence="0.9974044">
Percent Correctness Convergence Diffs for All 4 Features
14
12
10
8
6
4
2
0
0 2 4 6 8 10
12 14 16 18 20
Smoves
Concept
Percent Correct
Emotion
40
20
0
V−value
100
80
60
−20
−40
−60
−80
−100
0 2 4 6 8 10
12 14 16 18 20
B of students
</figure>
<figureCaption confidence="0.996159">
Figure 2: Percent Correct Convergence, and Diff Plots for all 4 Features
</figureCaption>
<table confidence="0.999568615384615">
# State Baseline New Policy
1 certain:C NonFeed N: NonFeed
F: Feed
2 certain:IPC NonFeed N: NonFeed
F: NonFeed
3 neutral:C Feed N: Feed
F: NonFeed
4 neutral:IPC Mix N: Mix
F: NonFeed
5 uncertain:C NonFeed N: NonFeed
F: NonFeed
6 uncertain:IPC NonFeed N: NonFeed
F: NonFeed
</table>
<tableCaption confidence="0.998308">
Table 7: Frustration Policies
</tableCaption>
<bodyText confidence="0.9996706">
states. States 3, 4, and 5 show a change in policy
for different parameters of correctness. One trend
seems to be that when a student has not been per-
forming well (L), to give a NonFeedback response
such as a hint or restatement.
</bodyText>
<table confidence="0.999725076923077">
# State Baseline New Policy
1 certain:C NonFeed H: NonFeed
L: NonFeed
2 certain:IPC NonFeed H: NonFeed
L: NonFeed
3 neutral:C Feed H: Feed
L: NonFeed
4 neutral:IPC Mix H: Mix
L: NonFeed
5 uncertain:C NonFeed H: Mix
L: NonFeed
6 uncertain:IPC NonFeed H: NonFeed
L: NonFeed
</table>
<tableCaption confidence="0.994897">
Table 8: % Correctness Policies
</tableCaption>
<sectionHeader confidence="0.999867" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999931944444444">
RL has been applied to improve dialogue sys-
tems in past work but very few approaches have
looked at which features are important to include
in the dialogue state. (Paek and Chickering, 2005)
showed how the state space can be learned from
data along with the policy. One result is that a
state space can be constrained by only using fea-
tures that are relevant to receiving a reward. Singh
et al. (1999) found an optimal dialogue length in
their domain, and showed that the number of in-
formation and distress attributes impact the state.
They take a different approach than the work here
in that they compare which feature values are opti-
mal for different points in the dialogue. Frampton
et al. (2005) is similar to ours in that they exper-
iment on including another dialogue feature into
their baseline system: the user’s last dialogue act,
which was found to produce a 52% increase in av-
erage reward. Williams et al. (2003) used Super-
vised Learning to select good state and action fea-
tures as an initial policy to bootstrap a RL-based
dialoge system. They found that their automati-
cally created state and action seeds outperformed
hand-crafted polices in a driving directions corpus.
In addition, there has been extensive work on cre-
ating new corpora via user simulations (such as
(Georgila et al., 2005)) to get around the possible
issue of not having enough data to train on. Our
results here indicate that a small training corpus is
actually acceptable to use in a MDP framework as
long as the state and action features are pruned ef-
fectively. The use of features such as context and
student moves is nothing new to the ITS commu-
nity however, such as the BEETLE system (Zinn
et al., 2005), but very little work has been done
using RL in developing tutoring systems.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="method">
8 Discussion
</sectionHeader>
<bodyText confidence="0.988197">
In this paper we showed that incorporating more
information into a representation of the student
state has an impact on what actions the tutor
should take. We first showed that despite not be-
</bodyText>
<page confidence="0.993306">
295
</page>
<bodyText confidence="0.999972488888889">
ing able to test on real users or simulated users just
yet, that our generated policies were indeed reli-
able since they converged in terms of the V-values
of each state and the policy for each state.
Next, we showed that all five features investi-
gated in this study were indeed important to in-
clude when constructing an estimation of the stu-
dent state. Student Moves, Certainty and Concept
Repetition were the most compelling since adding
them to their respective baseline states resulted in
major policy changes. Tracking the student’s frus-
tration levels and how correct the student had been
in the dialogue had the least impact on policies.
While these features (and their resulting poli-
cies) may appear unique to tutoring systems they
also generalize to dialogue systems as a whole.
Repeating a concept (whether it be a physics term
or travel information) is important because it is an
implicit signal that there might be some confusion
and a different action is needed when the concept
is repeated. Whether a student (or user) gives a
short answer or a good explanation can indicate to
the system how well the user is understanding sys-
tem questions. Emotion detection and adaptation
is a key issue for any spoken dialogue systems as
designers try to make the system as easy to use
for a student or trip-planner, etc. Frustration can
come from difficulty in questions or in the more
frequent problem for any dialogue system, speech
recognition errors, so the manner in dealing with
it will always be important. Percent Correctness
can be viewed as a specific instance of tracking
user performance such as if they are continously
answering questions properly or are confused by
what the system wants from them.
In terms of future work, we are currently an-
notating more human-computer dialogue data and
will triple the size of our test corpus allowing us
to 1. create more complicated states since more
states will have been explored and 2. test out
more complex tutor actions such as when to give
Hints and Restatements. Finally, we are in the pro-
cess of running this same experiment on a corpus
of human-human tutoring dialogues to compare if
human tutors have different policies.
</bodyText>
<sectionHeader confidence="0.998615" genericHeader="method">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99982925">
We would like to thank the ITSPOKE group
and the three anonymous reviewers for their in-
sight and comments. Support for the research re-
ported in this paper was provided by NSF grants
</bodyText>
<sectionHeader confidence="0.8780695" genericHeader="conclusions">
#0325054 and #0328431.
References
</sectionHeader>
<reference confidence="0.999851717391304">
K. Bhatt, M. Evens, and S. Argamon. 2004. Hedged
responses and expressions of affect in human/human
and human computer tutorial interactions. In Proc.
Cognitive Science.
I. Chades, M. Cros, F. Garcia, and R. Sabbadin. 2005.
Mdp toolbox v2.0 for matlab.
K. Forbes-Riley and D. Litman. 2005. Using bigrams
to identify relationships between student certainness
states and tutor responses in a spoken dialogue cor-
pus. In SIGDial.
K. Forbes-Riley, D. Litman, A. Huettner, and A. Ward.
2005. Dialogue-learning correlations in spoken dia-
logue tutoring. In AIED.
M. Frampton and O. Lemon. 2005. Reinforcement
learning of dialogue strategies using the user’s last
dialogue act. In IJCAI Wkshp. on K&amp;R in Practical
Dialogue Systems.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning user simulations for information state up-
date dialogue systems. In Interspeech.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hy-
brid reinforcement/supervised learning for dialogue
policies from communicator data. In IJCAI Wkshp.
on K&amp;R in Practical Dialogue Systems.
T. Paek and D. Chickering. 2005. The markov as-
sumption in spoken dialogue management. In 6th
SIGDial Workshop on Discourse and Dialogue.
S. Singh, M. Kearns, D. Litman, and M. Walker. 1999.
Reinforcement learning for spoken dialogue sys-
tems. In Proc. NIPS ’99.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue managment with reinforcement
learning: Experiments with the njfun system. JAIR,
16.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. The MIT Press.
M. Walker. 2000. An application of reinforcement
learning to dialogue strategy selection in a spoken
dialogue system for email. JAIR, 12.
J. Williams and S. Young. 2003. Using wizard-of-
oz simulations to bootstrap reinforcement learning-
based dialog management systems. In 4th SIGdial
Workshop on Discourse and Dialogue.
C. Zinn, J. Moore, and M. Core. 2005. Intelligent in-
formation presentation for tutoring systems. Intelli-
gent Information Presentation.
</reference>
<page confidence="0.998558">
296
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511343">
<title confidence="0.999794">Using Reinforcement Learning to Build a Better Model of Dialogue State</title>
<author confidence="0.999997">Joel R Tetreault Diane J Litman</author>
<affiliation confidence="0.999684">University of Pittsburgh University of Pittsburgh Learning Research and Development Center Department of Computer Science &amp;</affiliation>
<address confidence="0.7981815">Pittsburgh PA, 15260, USA Learning Research and Development Center PA, 15260, USA</address>
<email confidence="0.999884">litman@cs.pitt.edu</email>
<abstract confidence="0.9928602">Given the growing complexity of tasks that spoken dialogue systems are trying to handle, Reinforcement Learning (RL) has been increasingly used as a way of automatically learning the best policy for a system to make. While most work has focused on generating better policies for a dialogue manager, very little work has been done in using RL to construct a better dialogue state. This paper presents a RL approach for determining what dialogue features are important to a spoken dialogue tutoring system. Our experiments show that incorporating dialogue factors such as dialogue acts, emotion, repeated concepts and performance play a significant role in tutoring and should be taken into account when designing dialogue systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Bhatt</author>
<author>M Evens</author>
<author>S Argamon</author>
</authors>
<title>Hedged responses and expressions of affect in human/human and human computer tutorial interactions.</title>
<date>2004</date>
<booktitle>In Proc. Cognitive Science.</booktitle>
<contexts>
<context position="16924" citStr="Bhatt et al., 2004" startWordPosition="2900" endWordPosition="2903">esults In this section, we investigate whether adding more information to our student state will lead to interesting policy changes. First, we add certainty to our baseline of correctness, and then compare this new baseline’s policy (henceforth Baseline 2) with the policies generated when student moves, frustration, concept repetition, and percent correctness are included. For each test, we employed the same methodology as with the baseline case of doing a 20-fold cross-averaging and examining if the states’ V-values converge. We first add certainty to correctness because prior work (such as (Bhatt et al., 2004)) has shown the importance of considering certainty in tutoring 292 Correctness Correctness + Certainty V−value 100 80 60 40 20 0 V−value 40 20 0 100 80 60 12 14 16 18 20 −20 −40 −60 −80 −100 0 2 4 6 8 10 B of students −20 −40 −60 −80 −100 0 2 4 6 8 10 B of students 12 14 16 18 20 Figure 1: Baseline 1 and Baseline 2 Convergence Plots systems. For example, a student who is correct and certain probably does not need a lot of feedback. But one that is correct but uncertain could signal that the student is becoming doubtful or at least confused about a concept. There are three types of certainty: </context>
</contexts>
<marker>Bhatt, Evens, Argamon, 2004</marker>
<rawString>K. Bhatt, M. Evens, and S. Argamon. 2004. Hedged responses and expressions of affect in human/human and human computer tutorial interactions. In Proc. Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Chades</author>
<author>M Cros</author>
<author>F Garcia</author>
<author>R Sabbadin</author>
</authors>
<date>2005</date>
<note>Mdp toolbox v2.0 for matlab.</note>
<contexts>
<context position="11013" citStr="Chades et al., 2005" startWordPosition="1855" endWordPosition="1858">h is a standard evaluation metric in the Intelligent Tutoring Systems community. So 10 students and their respective 5 dialogues were assigned a positive reward of +100 (high learners), and the other 10 students and their respective dialogues were assigned a negative reward of -100 (low learners). It should be noted that a student’s 5 dialogues were assigned the same reward since there was no way to approximate their learning gain in the middle of a session. The output of the tool is a probability matrix over the user-specified states and actions. This matrix is then passed to an MDP toolkit (Chades et al., 2005) written in Matlab.1 The toolkit performs policy iteration and generates a policy as well as a list of V-values for each state. 5 Experimental Method With the infrastructure created and the MDP parameters set, we can then move on to the goal of this experiment - to see what sources of information impact a tutoring dialogue system. First, we need to develop a baseline to compare the effects of adding more information. Second, we generate a new policy by adding the new information source to the baseline state. However, since we are currently not running any new experiments to test our policy, or</context>
</contexts>
<marker>Chades, Cros, Garcia, Sabbadin, 2005</marker>
<rawString>I. Chades, M. Cros, F. Garcia, and R. Sabbadin. 2005. Mdp toolbox v2.0 for matlab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Using bigrams to identify relationships between student certainness states and tutor responses in a spoken dialogue corpus.</title>
<date>2005</date>
<booktitle>In SIGDial.</booktitle>
<contexts>
<context position="7024" citStr="Forbes-Riley and Litman, 2005" startWordPosition="1182" endWordPosition="1186">TS) community. Student Moves refer to the type of answer a student gives. Answers that involve a concept already introduced in the dialogue are called Shallow, answers that involve a novel concept are called Novel, “I don’t know” type answers are called Assertions (As), and Deep answers refer to answers that involve linking two concepts through reasoning. In our study, we merge all non-Shallow moves into a new move “Other.” In addition to Student Moves, we annotated five other features to include in our representation of the student state. Two emotion related features were annotated manually (Forbes-Riley and Litman, 2005): certainty and frustration. Certainty describes how confident a student seemed to be in his answer, while frustration describes how frustrated the student seemed to be in his last response. We include three other features for the Student state that were extracted automatically. Correctness says if the last student answer was correct or incorrect. As noted above, this is what most current tutoring systems use as their state. Percent Correct is the percentage of questions in the current problem the student has answered correctly so far. Finally, if a student performs poorly when it comes to a c</context>
</contexts>
<marker>Forbes-Riley, Litman, 2005</marker>
<rawString>K. Forbes-Riley and D. Litman. 2005. Using bigrams to identify relationships between student certainness states and tutor responses in a spoken dialogue corpus. In SIGDial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
<author>A Huettner</author>
<author>A Ward</author>
</authors>
<title>Dialogue-learning correlations in spoken dialogue tutoring.</title>
<date>2005</date>
<booktitle>In AIED.</booktitle>
<contexts>
<context position="6040" citStr="Forbes-Riley et al., 2005" startWordPosition="1016" endWordPosition="1019"> dialogues has been manually authored in advance meaning that the system has a response based on the correctness of the student’s last answer. Once the student has successfully answered all the questions, he or she is asked to correct the initial essay. On average, each of the dialogues takes 20 minutes and contains 25 student turns. Finally, the student is given a post-test similar to the pre-test, from which we can calculate their normalized learning gain: Prior to our study, the corpus was then annotated for Student and Tutor Moves (see Tables 1 and 2) which can be viewed as Dialogue Acts (Forbes-Riley et al., 2005). Note that tutor and student turns can consist of multiple utterances and can thus be labeled with multiple moves. For example, a tutor can give feedback and then ask a question in the same turn. Whether to include feedback will be the action choice addressed in this paper since it is an interesting open question in the Intelligent Tutoring Systems (ITS) community. Student Moves refer to the type of answer a student gives. Answers that involve a concept already introduced in the dialogue are called Shallow, answers that involve a novel concept are called Novel, “I don’t know” type answers are</context>
</contexts>
<marker>Forbes-Riley, Litman, Huettner, Ward, 2005</marker>
<rawString>K. Forbes-Riley, D. Litman, A. Huettner, and A. Ward. 2005. Dialogue-learning correlations in spoken dialogue tutoring. In AIED.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Frampton</author>
<author>O Lemon</author>
</authors>
<title>Reinforcement learning of dialogue strategies using the user’s last dialogue act.</title>
<date>2005</date>
<booktitle>In IJCAI Wkshp. on K&amp;R in Practical Dialogue Systems.</booktitle>
<contexts>
<context position="10271" citStr="Frampton and Lemon, 2005" startWordPosition="1726" endWordPosition="1730">ther into one) or quantize features that have continuous values (such as the number of utterances in the dialogue so far). These collapsing functions allow the user to easily constrain the trajectory space. To further reduce the search space for the MDP, our tool allows the user to specify a threshold to combine states that occur less than the threshold into a single “threshold state.” In addition, the user can specify a reward function and a discount factor, For this study, we use a threshold of 50 and a discount factor of 0.9, which is also what is commonly used in other RL models, such as (Frampton and Lemon, 2005). For the dialogue reward function, we did a median split on the 20 students based on their normalized learning gain, which is a standard evaluation metric in the Intelligent Tutoring Systems community. So 10 students and their respective 5 dialogues were assigned a positive reward of +100 (high learners), and the other 10 students and their respective dialogues were assigned a negative reward of -100 (low learners). It should be noted that a student’s 5 dialogues were assigned the same reward since there was no way to approximate their learning gain in the middle of a session. The output of t</context>
</contexts>
<marker>Frampton, Lemon, 2005</marker>
<rawString>M. Frampton and O. Lemon. 2005. Reinforcement learning of dialogue strategies using the user’s last dialogue act. In IJCAI Wkshp. on K&amp;R in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>J Henderson</author>
<author>O Lemon</author>
</authors>
<title>Learning user simulations for information state update dialogue systems.</title>
<date>2005</date>
<journal>In Interspeech.</journal>
<contexts>
<context position="27433" citStr="Georgila et al., 2005" startWordPosition="4790" endWordPosition="4793">pton et al. (2005) is similar to ours in that they experiment on including another dialogue feature into their baseline system: the user’s last dialogue act, which was found to produce a 52% increase in average reward. Williams et al. (2003) used Supervised Learning to select good state and action features as an initial policy to bootstrap a RL-based dialoge system. They found that their automatically created state and action seeds outperformed hand-crafted polices in a driving directions corpus. In addition, there has been extensive work on creating new corpora via user simulations (such as (Georgila et al., 2005)) to get around the possible issue of not having enough data to train on. Our results here indicate that a small training corpus is actually acceptable to use in a MDP framework as long as the state and action features are pruned effectively. The use of features such as context and student moves is nothing new to the ITS community however, such as the BEETLE system (Zinn et al., 2005), but very little work has been done using RL in developing tutoring systems. 8 Discussion In this paper we showed that incorporating more information into a representation of the student state has an impact on wh</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2005</marker>
<rawString>K. Georgila, J. Henderson, and O. Lemon. 2005. Learning user simulations for information state update dialogue systems. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>O Lemon</author>
<author>K Georgila</author>
</authors>
<title>Hybrid reinforcement/supervised learning for dialogue policies from communicator data.</title>
<date>2005</date>
<booktitle>In IJCAI Wkshp. on K&amp;R in Practical Dialogue Systems.</booktitle>
<contexts>
<context position="1401" citStr="Henderson et al., 2005" startWordPosition="209" endWordPosition="212">ach for determining what dialogue features are important to a spoken dialogue tutoring system. Our experiments show that incorporating dialogue factors such as dialogue acts, emotion, repeated concepts and performance play a significant role in tutoring and should be taken into account when designing dialogue systems. 1 Introduction This paper presents initial research toward the long-term goal of designing a tutoring system that can effectively adapt to the student. While most work in Markov Decision Processes (MDPs) and spoken dialogue have focused on building better policies (Walker, 2000; Henderson et al., 2005), to date very little empirical work has tested the utility of adding specialized features to construct a better dialogue state. We wish to show that adding more complex factors to a representation of student state is a worthwhile pursuit, since it alters what action the tutor should make. The five dialogue factors we explore are dialogue acts, certainty level, frustration level, concept repetition, and student performance. All five are factors that are not just unique to the tutoring domain but are important to dialogue systems in general. Our results show that using these features, combined </context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2005</marker>
<rawString>J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid reinforcement/supervised learning for dialogue policies from communicator data. In IJCAI Wkshp. on K&amp;R in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>D Chickering</author>
</authors>
<title>The markov assumption in spoken dialogue management.</title>
<date>2005</date>
<booktitle>In 6th SIGDial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="26319" citStr="Paek and Chickering, 2005" startWordPosition="4598" endWordPosition="4601"> that when a student has not been performing well (L), to give a NonFeedback response such as a hint or restatement. # State Baseline New Policy 1 certain:C NonFeed H: NonFeed L: NonFeed 2 certain:IPC NonFeed H: NonFeed L: NonFeed 3 neutral:C Feed H: Feed L: NonFeed 4 neutral:IPC Mix H: Mix L: NonFeed 5 uncertain:C NonFeed H: Mix L: NonFeed 6 uncertain:IPC NonFeed H: NonFeed L: NonFeed Table 8: % Correctness Policies 7 Related Work RL has been applied to improve dialogue systems in past work but very few approaches have looked at which features are important to include in the dialogue state. (Paek and Chickering, 2005) showed how the state space can be learned from data along with the policy. One result is that a state space can be constrained by only using features that are relevant to receiving a reward. Singh et al. (1999) found an optimal dialogue length in their domain, and showed that the number of information and distress attributes impact the state. They take a different approach than the work here in that they compare which feature values are optimal for different points in the dialogue. Frampton et al. (2005) is similar to ours in that they experiment on including another dialogue feature into the</context>
</contexts>
<marker>Paek, Chickering, 2005</marker>
<rawString>T. Paek and D. Chickering. 2005. The markov assumption in spoken dialogue management. In 6th SIGDial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>M Kearns</author>
<author>D Litman</author>
<author>M Walker</author>
</authors>
<title>Reinforcement learning for spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proc. NIPS ’99.</booktitle>
<contexts>
<context position="2244" citStr="Singh et al., 1999" startWordPosition="349" endWordPosition="352">hwhile pursuit, since it alters what action the tutor should make. The five dialogue factors we explore are dialogue acts, certainty level, frustration level, concept repetition, and student performance. All five are factors that are not just unique to the tutoring domain but are important to dialogue systems in general. Our results show that using these features, combined with the common baseline of student correctness, leads to a significant change in the policies produced, and thus should be taken into account when designing a system. 2 Background We follow past lines of research (such as (Singh et al., 1999)) for describing a dialogue as a trajectory within a Markov Decision Process (Sutton and Barto, 1998). A MDP has four main components: states, actions, a policy, which specifies what is the best action to take in a state, and a reward function which specifies the utility of each state and the process as a whole. Dialogue management is easily described using a MDP because one can consider the actions as actions made by the system, the state as the dialogue context, and a reward which for many dialogue systems tends to be task completion success or dialogue length. Typically the state is viewed </context>
<context position="9379" citStr="Singh et al., 1999" startWordPosition="1568" endWordPosition="1571">help the student along. These were not written with the goal of how best to influence student state. Our goal in this study is to automatically learn from this corpus which state-action patterns evoke the highest learning gain. 4 Infrastructure To test different hypotheses of what features best approximate the student state and what are the best actions for a tutor to consider, one must have a flexible system that allows one to easily test different configurations of states and actions. To accomplish this, we designed a system similar to the Reinforcement Learning for Dialogue Systems (RLDS) (Singh et al., 1999). The system allows a system designer to specify what features will compose the state and actions as well as perform operations on each individual feature. For instance, the tool allows the user to collapse features together (such as collapsing all Question Acts together into one) or quantize features that have continuous values (such as the number of utterances in the dialogue so far). These collapsing functions allow the user to easily constrain the trajectory space. To further reduce the search space for the MDP, our tool allows the user to specify a threshold to combine states that occur l</context>
<context position="26530" citStr="Singh et al. (1999)" startWordPosition="4638" endWordPosition="4641">d L: NonFeed 3 neutral:C Feed H: Feed L: NonFeed 4 neutral:IPC Mix H: Mix L: NonFeed 5 uncertain:C NonFeed H: Mix L: NonFeed 6 uncertain:IPC NonFeed H: NonFeed L: NonFeed Table 8: % Correctness Policies 7 Related Work RL has been applied to improve dialogue systems in past work but very few approaches have looked at which features are important to include in the dialogue state. (Paek and Chickering, 2005) showed how the state space can be learned from data along with the policy. One result is that a state space can be constrained by only using features that are relevant to receiving a reward. Singh et al. (1999) found an optimal dialogue length in their domain, and showed that the number of information and distress attributes impact the state. They take a different approach than the work here in that they compare which feature values are optimal for different points in the dialogue. Frampton et al. (2005) is similar to ours in that they experiment on including another dialogue feature into their baseline system: the user’s last dialogue act, which was found to produce a 52% increase in average reward. Williams et al. (2003) used Supervised Learning to select good state and action features as an initi</context>
</contexts>
<marker>Singh, Kearns, Litman, Walker, 1999</marker>
<rawString>S. Singh, M. Kearns, D. Litman, and M. Walker. 1999. Reinforcement learning for spoken dialogue systems. In Proc. NIPS ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>D Litman</author>
<author>M Kearns</author>
<author>M Walker</author>
</authors>
<title>Optimizing dialogue managment with reinforcement learning: Experiments with the njfun system.</title>
<date>2002</date>
<journal>JAIR,</journal>
<volume>16</volume>
<contexts>
<context position="4697" citStr="Singh et al., 2002" startWordPosition="784" endWordPosition="787">) and we select the action that produces the highest V-value for that state. Normally one would want a dialogue system to interact with users thousands of times to explore the entire traversal space of the MDP, however in practice that is very time-consuming. Instead, the next best tactic is to train the MDP (that is, calculate transition probabilities for getting from one state to another, and the reward for each state) on already collected data. Of course, the whole space will not be considered, but if one reduces the size of the state vector effectively, data size becomes less of an issue (Singh et al., 2002). 3 Corpus For our study, we used an annotated corpus of 20 human-computer spoken dialogue tutoring sessions. Each session consists of an interaction with one student over 5 different college-level physics problems, for a total of 100 dialogues. Before the 5 problems, the student is asked to read physics material for 30 minutes and then take a pre-test based on that material. Each problem begins with the student writing out a short essay response to the question posed by the computer tutor. The system reads the essay and detects the problem areas and then starts a dialogue with the student ask</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>S. Singh, D. Litman, M. Kearns, and M. Walker. 2002. Optimizing dialogue managment with reinforcement learning: Experiments with the njfun system. JAIR, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sutton</author>
<author>A Barto</author>
</authors>
<title>Reinforcement Learning.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2345" citStr="Sutton and Barto, 1998" startWordPosition="366" endWordPosition="369">xplore are dialogue acts, certainty level, frustration level, concept repetition, and student performance. All five are factors that are not just unique to the tutoring domain but are important to dialogue systems in general. Our results show that using these features, combined with the common baseline of student correctness, leads to a significant change in the policies produced, and thus should be taken into account when designing a system. 2 Background We follow past lines of research (such as (Singh et al., 1999)) for describing a dialogue as a trajectory within a Markov Decision Process (Sutton and Barto, 1998). A MDP has four main components: states, actions, a policy, which specifies what is the best action to take in a state, and a reward function which specifies the utility of each state and the process as a whole. Dialogue management is easily described using a MDP because one can consider the actions as actions made by the system, the state as the dialogue context, and a reward which for many dialogue systems tends to be task completion success or dialogue length. Typically the state is viewed as a vector of features such as dialogue history, speech recognition confidence, etc. The goal of usi</context>
<context position="3882" citStr="Sutton and Barto, 1998" startWordPosition="645" endWordPosition="648">s known at the end of the dialogue. As long as we have a reward function, Reinforcement Learning allows one to automatically compute the best policy. The following recursive equation gives us a way of calculating the expected cumulative value (V-value) of a state (-value): 289 Here is the best action for state at this time, is the probability of getting from state to via . This is multiplied by the sum of the reward for that traversal plus the value of the new state multiplied by a discount factor . ranges between 0 and 1 and discounts the value of past states. The policy iteration algorithm (Sutton and Barto, 1998) iteratively updates the value of each state V(s) based on the values of its neighboring states. The iteration stops when each update yields an epsilon difference (implying that V(s) has converged) and we select the action that produces the highest V-value for that state. Normally one would want a dialogue system to interact with users thousands of times to explore the entire traversal space of the MDP, however in practice that is very time-consuming. Instead, the next best tactic is to train the MDP (that is, calculate transition probabilities for getting from one state to another, and the re</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R. Sutton and A. Barto. 1998. Reinforcement Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
</authors>
<title>An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email.</title>
<date>2000</date>
<journal>JAIR,</journal>
<volume>12</volume>
<contexts>
<context position="1376" citStr="Walker, 2000" startWordPosition="207" endWordPosition="208">nts a RL approach for determining what dialogue features are important to a spoken dialogue tutoring system. Our experiments show that incorporating dialogue factors such as dialogue acts, emotion, repeated concepts and performance play a significant role in tutoring and should be taken into account when designing dialogue systems. 1 Introduction This paper presents initial research toward the long-term goal of designing a tutoring system that can effectively adapt to the student. While most work in Markov Decision Processes (MDPs) and spoken dialogue have focused on building better policies (Walker, 2000; Henderson et al., 2005), to date very little empirical work has tested the utility of adding specialized features to construct a better dialogue state. We wish to show that adding more complex factors to a representation of student state is a worthwhile pursuit, since it alters what action the tutor should make. The five dialogue factors we explore are dialogue acts, certainty level, frustration level, concept repetition, and student performance. All five are factors that are not just unique to the tutoring domain but are important to dialogue systems in general. Our results show that using </context>
</contexts>
<marker>Walker, 2000</marker>
<rawString>M. Walker. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. JAIR, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Using wizard-ofoz simulations to bootstrap reinforcement learningbased dialog management systems.</title>
<date>2003</date>
<booktitle>In 4th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<marker>Williams, Young, 2003</marker>
<rawString>J. Williams and S. Young. 2003. Using wizard-ofoz simulations to bootstrap reinforcement learningbased dialog management systems. In 4th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zinn</author>
<author>J Moore</author>
<author>M Core</author>
</authors>
<title>Intelligent information presentation for tutoring systems. Intelligent Information Presentation.</title>
<date>2005</date>
<contexts>
<context position="27820" citStr="Zinn et al., 2005" startWordPosition="4862" endWordPosition="4865">automatically created state and action seeds outperformed hand-crafted polices in a driving directions corpus. In addition, there has been extensive work on creating new corpora via user simulations (such as (Georgila et al., 2005)) to get around the possible issue of not having enough data to train on. Our results here indicate that a small training corpus is actually acceptable to use in a MDP framework as long as the state and action features are pruned effectively. The use of features such as context and student moves is nothing new to the ITS community however, such as the BEETLE system (Zinn et al., 2005), but very little work has been done using RL in developing tutoring systems. 8 Discussion In this paper we showed that incorporating more information into a representation of the student state has an impact on what actions the tutor should take. We first showed that despite not be295 ing able to test on real users or simulated users just yet, that our generated policies were indeed reliable since they converged in terms of the V-values of each state and the policy for each state. Next, we showed that all five features investigated in this study were indeed important to include when constructi</context>
</contexts>
<marker>Zinn, Moore, Core, 2005</marker>
<rawString>C. Zinn, J. Moore, and M. Core. 2005. Intelligent information presentation for tutoring systems. Intelligent Information Presentation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>