<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000390">
<title confidence="0.983722">
Dual Training and Dual Prediction for Polarity Classification
</title>
<author confidence="0.999794">
Rui Xia, Tao Wang, Xuelei Hu
</author>
<affiliation confidence="0.835998333333333">
Department of Computer Science
Nanjing University of
Science and Technology
</affiliation>
<email confidence="0.968476666666667">
rxia@njust.edu.cn,
linclonwang@163.com,
xlhu@njust.edu.cn
</email>
<author confidence="0.898232">
Shoushan Li
</author>
<affiliation confidence="0.898286">
NLP Lab
Department of
Computer Science
Soochow University
</affiliation>
<email confidence="0.6343295">
shoushan.li
@gmail.com
</email>
<author confidence="0.903951">
Chengqing Zong
</author>
<affiliation confidence="0.861672333333333">
National Lab of
Pattern Recognition
Institute of Automation
</affiliation>
<sectionHeader confidence="0.254976" genericHeader="abstract">
CAS
</sectionHeader>
<bodyText confidence="0.403174">
cqzong
@nlpr.ia.ac.cn
</bodyText>
<sectionHeader confidence="0.978073" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998956411764706">
Bag-of-words (BOW) is now the most popular
way to model text in machine learning based
sentiment classification. However, the perfor-
mance of such approach sometimes remains
rather limited due to some fundamental defi-
ciencies of the BOW model. In this paper, we
focus on the polarity shift problem, and pro-
pose a novel approach, called dual training and
dual prediction (DTDP), to address it. The
basic idea of DTDP is to first generate artifi-
cial samples that are polarity-opposite to the
original samples by polarity reversion, and
then leverage both the original and opposite
samples for (dual) training and (dual) predic-
tion. Experimental results on four datasets
demonstrate the effectiveness of the proposed
approach for polarity classification.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998822901639345">
The most popular text representation model in
machine learning based sentiment classification
is known as the bag-of-words (BOW) model,
where a piece of text is represented by an unor-
dered collection of words, based on which stand-
ard machine learning algorithms are employed as
classifiers. Although the BOW model is simple
and has achieved great successes in topic-based
text classification, it disrupts word order, breaks
the syntactic structures and discards some kinds
of semantic information that are possibly very
important for sentiment classification. Such dis-
advantages sometimes limit the performance of
sentiment classification systems.
A lot of subsequent work focused on feature
engineering that aims to find a set of effective
features based on the BOW representation. How-
ever, there still remain some problems that are
not well addressed. Out of them, the polarity
shift problem is the biggest one.
We refer to “polarity shift” as a linguistic phe-
nomenon that the sentiment orientation of a text
is reversed (from positive to negative or vice ver-
sa) because of some particular expressions called
polarity shifters. Negation words (e.g., “no”, “not”
and “don’t”) are the most important type of po-
larity shifter. For example, by adding a negation
word “don’t” to a positive text “I like this book”
in front of “like”, the orientation of the text is
reversed from positive to negative.
Naturally, handling polarity shift is very im-
portant for sentiment classification. However, the
BOW representations of two polarity-opposite
texts, e.g., “I like this book” and “I don’t like this
book”, are considered to be very similar by most
of machine learning algorithms. Although some
methods have been proposed in the literature to
address the polarity shift problem (Das and Chen,
2001; Pang et al., 2002; Na et al., 2004; Kenndey
and Inkpen, 2006; Ikeda et al., 2008; Li and
Huang, 2009; Li et al., 2010), the state-of-the-art
results are still far from satisfactory. For example,
the improvements are less than 2% after consid-
ering polarity shift in Li et al. (2010).
In this work, we propose a novel approach,
called dual training and dual prediction (DTDP),
to address the polarity shift problem. By taking
advantage of the unique nature of polarity classi-
fication, DTDP is motivated by first generating
artificial samples that are polarity-opposite to the
original ones. For example, given the original
sample “I don’t like this book. It is boring,” its
polarity-opposite version, “I like this book. It is
interesting”, is artificially generated. Second, the
original and opposite training samples are used
together for training a sentiment classifier (called
dual training), and the original and opposite test
samples are used together for prediction (called
dual prediction). Experimental results prove that
the procedure of DTDP is very effective at cor-
recting the training and prediction errors caused
</bodyText>
<page confidence="0.968068">
521
</page>
<bodyText confidence="0.62772275">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 521–525,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
by polarity shift, and it beats other alternative
methods of considering polarity shift.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999992186046512">
The lexicon-based sentiment classification sys-
tems can be easily modified to include polarity
shift. One common way is to directly reverse the
sentiment orientation of polarity-shifted words,
and then sum up the orientations word by word
(Hu and Liu, 2004; Kim and Hovy, 2004; Po-
lanyi and Zaenen, 2004; Kennedy and Inkpen,
2006). Wilson et al. (2005) discussed other com-
plex negation effects by using conjunctive and
dependency relations among polarity words. Alt-
hough handling polarity shift is easy and effec-
tive in term-counting systems, they rarely outper-
form the baselines of machine learning methods
(Kennedy, 2006).
The machine learning methods are generally
more effective for sentiment classification. How-
ever, it is difficult to handle polarity shift based
on the BOW model. Das and Chen (2001) pro-
posed a method by simply attaching “NOT” to
words in the scope of negation, so that in the text
“I don’t like book”, the word “like” is changed to
a new word “like-NOT”. There were also some
attempts to model polarity shift by using more
complex linguistic features (Na et al., 2004;
Kennedy and Inkpen, 2006). But the improve-
ments upon the baselines of machine learning
systems are very slight (less than 1%).
Ikeda et al. (2008) proposed a machine learn-
ing method, to model polarity-shifters for both
word-wise and sentence-wise sentiment classifi-
cation, based on a dictionary extracted from
General Inquirer. Li and Huang (2009) proposed
a method first to classify each sentence in a text
into a polarity-unshifted part and a polarity-
shifted part according to certain rules, then to
represent them as two bag-of-words for senti-
ment classification. Li et al. (2010) further pro-
posed a method to separate the shifted and un-
shifted text based on training a binary detector.
Classification models are then trained based on
each of the two parts. An ensemble of two com-
ponent parts is used at last to get the final polari-
ty of the whole text.
</bodyText>
<sectionHeader confidence="0.992089" genericHeader="method">
3 The Proposed Approach
</sectionHeader>
<bodyText confidence="0.99966475">
We first present the method for generating artifi-
cial polarity-opposite samples, and then intro-
duce the algorithm of dual training and dual pre-
diction (DTDP).
</bodyText>
<subsectionHeader confidence="0.9984825">
3.1 Generating Artificial Polarity-Opposite
Samples
</subsectionHeader>
<bodyText confidence="0.99961275">
Given an original sample and an antonym dic-
tionary (e.g., WordNet 1 ), a polarity-opposite
sample is generated artificially according to the
following rules:
</bodyText>
<listItem confidence="0.957238785714286">
1) Sentiment word reversion: All sentiment
words out of the scope of negation are re-
versed to their antonyms;
2) Handling negation: If there is a negation
expression, we first detect the scope of nega-
tion, and then remove the negation words
(e.g., “no”, “not”, and “don’t”). The senti-
ment words in the scope of negation are not
reversed;
3) Label reversion: The class label of the la-
beled sample is also reversed to its opposite
(i.e., Positive to Negative, or vice versa) as
the class label of newly generated samples
(called polarity-opposite samples).
</listItem>
<bodyText confidence="0.976072333333333">
Let us use a simple example to explain the
generation process. Given the original sample:
The original sample
Text: I don’t like this book. It is boring.
Label: Negative
According to Rule 1, “boring” is reversed to
its antonym “interesting”; According to Rule 2,
the negation word “don’t” is removed, and “like”
is not reversed; According to Rule 3, the class
label Negative is reversed to Positive. Finally, an
artificial polarity-opposite sample is generated:
The generated opposite sample
Text: I like this book. It is interesting.
Label: Positive
All samples in the training and test set are re-
versed to their polarity-opposite versions. We
refer to them as “opposite training set” and “op-
posite test set”, respectively.
</bodyText>
<subsectionHeader confidence="0.9999">
3.2 Dual Training and Dual Prediction
</subsectionHeader>
<bodyText confidence="0.936909888888889">
In this part, we introduce how to make use of the
original and opposite training/test data together
for dual training and dual prediction (DTDP).
Dual Training: Let D = {(xi, yi)}i_&apos; 1 and
D = {(xi, yi)}Ni= 1be the original and opposite
training set respectively, where denotes the
feature vector, denotes the class label, andN
denotes the size of training set. In dual training,
D U D are used together as training data to learn
</bodyText>
<footnote confidence="0.97294">
1 http://wordnet.princeton.edu/
</footnote>
<page confidence="0.991332">
522
</page>
<bodyText confidence="0.997038586206897">
a classification model. The size of training data
is doubled in dual training.
Suppose the example in Section 3.1 is used as
one training sample. As far as only the original
sample (“I don’t like this book. It is boring.”) is
considered, the feature “like” will be improperly
recognized as a negative indicator (since the
class label is Negative), ignoring the expression
of negation. Nevertheless, if the generated oppo-
site sample (“I like this book. It is interesting.”)
is also used for training, “like” will be learned
correctly, due to the removal of negation in sam-
ple reversion. Therefore, the procedure of dual
training can correct some learning errors caused
by polarity shift.
Dual Prediction: Given an already-trained
classification model, in dual prediction, the orig-
inal and opposite test samples are used together
for prediction. In dual prediction, when we pre-
dict the positive degree of a test sample, we
measure not only how positive the original test
sample is, but also how negative the opposite
sample is.
Let and denote the feature vector of the
original and opposite test samples respectively;
let and denote the predictions of
the original and opposite test sample, based on
the dual training model. The dual predicting
function is defined as:
,
,
where ( &lt; a &lt; ) is the weight of the oppo-
site prediction.
Now suppose the example in Section 3.1 is a
test sample. As far as only the original test sam-
ple (“I don’t like this book. It is boring.”) is used
for prediction, it is very likely that it is falsely
predicted as Positive, since “like” is a strong pos-
itive feature, despite that it is in the scope of ne-
gation. While in dual prediction, we still measure
the “sentiment-opposite” degree of the opposite
test sample (“I like this book. It is interesting.”).
Since negation is removed, it is very likely that
the opposite test sample is assigned with a high
positive score, which could compensate the pre-
diction errors of the original test sample.
Final Output: It should be noted that alt-
hough the artificially generated training and test-
ing data are helpful in most cases, they still pro-
duce some noises (e.g., some poorly generated
samples may violate the quality of the original
data set). Therefore, instead of using all dual
predictions as the final output, we use the origi-
nal prediction as an alternate, in case that
the dual prediction is not enough con-
fident, according to a confidence threshold . The
final output is defined as:
where .
</bodyText>
<sectionHeader confidence="0.992098" genericHeader="evaluation">
4 Experimental Study
</sectionHeader>
<subsectionHeader confidence="0.890756">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9996234">
The Multi-Domain Sentiment Datasets2 are used
for evaluations. They consist of product reviews
collected from four different domains: Book,
DVD, Electronics and Kitchen. Each of them
contains 1,000 positive and 1,000 negative re-
views. Each of the datasets is randomly spit into
5 folds, with four folds serving as training data,
and the remaining one fold serving as test data.
All of the following results are reported in terms
of an average of 5-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.969659">
4.2 Evaluated Systems
</subsectionHeader>
<bodyText confidence="0.999542">
We evaluate four machine learning systems that
are proposed to address polarity shift in docu-
ment-level polarity classification:
</bodyText>
<listItem confidence="0.987180636363636">
1) Baseline: standard machine learning meth-
ods based on the BOW model, without han-
dling polarity shift;
2) Das-2001: the method proposed by Das and
Chen (2001), where “NOT” is attached to the
words in the scope of negation as a prepro-
cessing step;
3) Li-2010: the approach proposed by Li et al.
(2010). The details of the algorithm is intro-
duced in related work;
4) DTDP: our approach proposed in Section 3.
</listItem>
<bodyText confidence="0.748664666666667">
The WordNet dictionary is used for sample
reversion. The empirical value of the param-
eter and are used in the evaluation.
</bodyText>
<subsectionHeader confidence="0.998763">
4.3 Comparison of the Evaluated Systems
</subsectionHeader>
<bodyText confidence="0.999906428571429">
In table 1, we report the classification accuracy
of four evaluated systems using unigram features.
We consider two widely-used classification algo-
rithms: SVM and Naïve Bayes. For SVM, the
LibSVM toolkit3 is used with a linear kernel and
the default penalty parameter. For Naïve Bayes,
the OpenPR-NB toolkit4 is used.
</bodyText>
<footnote confidence="0.999611333333333">
2 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
4 http://www.openpr.org.cn
</footnote>
<page confidence="0.992332">
523
</page>
<table confidence="0.999872714285714">
Dataset SVM Naïve Bayes
Baseline Das-2001 Li-2010 DTDP Baseline Das-2001 Li-2010 DTDP
Book 0.745 0.763 0.760 0.800 0.779 0.783 0.792 0.814
DVD 0.764 0.771 0.795 0.823 0.795 0.793 0.810 0.820
Electronics 0.796 0.813 0.812 0.828 0.815 0.827 0.824 0.841
Kitchen 0.822 0.820 0.844 0.849 0.830 0.847 0.840 0.859
Avg. 0.782 0.792 0.803 0.825 0.804 0.813 0.817 0.834
</table>
<tableCaption confidence="0.997629">
Table 1: Classification accuracy of different systems using unigram features
</tableCaption>
<table confidence="0.999961285714286">
Dataset SVM Naïve Bayes
Baseline Das-2001 Li-2010 DTDP Baseline Das-2001 Li-2010 DTDP
Book 0.775 0.777 0.788 0.818 0.811 0.815 0.822 0.840
DVD 0.790 0.793 0.809 0.828 0.824 0.826 0.837 0.868
Electronics 0.818 0.834 0.841 0.848 0.841 0.857 0.852 0.866
Kitchen 0.847 0.844 0.870 0.878 0.878 0.879 0.883 0.896
Avg. 0.808 0.812 0.827 0.843 0.839 0.844 0.849 0.868
</table>
<tableCaption confidence="0.999641">
Table 2: Classification accuracy of different systems using both unigram and bigram features
</tableCaption>
<bodyText confidence="0.999927428571429">
Compared to the Baseline system, the Das-
2001 approach achieves very slight improve-
ments (less than 1%). The performance of Li-
2010 is relatively effective: it improves the aver-
age score by 0.21% and 0.13% on SVM and Na-
ïve Bayes, respectively. Yet, the improvements
are still not satisfactory.
As for our approach (DTDP), the improve-
ments are remarkable. Compared to the Baseline
system, the average improvements are 4.3% and
3.0% on SVM and Naïve Bayes, respectively. In
comparison with the state-of-the-art (Li-2010),
the average improvement is 2.2% and 1.7% on
SVM and Naïve Bayes, respectively.
We also report the classification accuracy of
four systems using both unigrams and bigrams
features for classification in Table 2. From this
table, we can see that the performance of each
system is improved compared to that using uni-
grams. It is now relatively difficult to show im-
provements by incorporating polarity shift, be-
cause using bigrams already captured a part of
negations (e.g., “don’t like”).
The Das-2001 approach still shows very lim-
ited improvements (less than 0.5%), which
agrees with the reports in Pang et al. (2002). The
improvements of Li-2010 are also reduced: 1.9%
and 1% on SVM and Naïve Bayes, respectively.
Although the improvements of the previous
two systems are both limited, the performance of
our approach (DTDP) is still sound. It improves
the Baseline system by 3.7% and 2.9% on SVM
and Naïve Bayes, respectively, and outperforms
the state-of-the-art (Li-2010) by 1.6% and 1.9%
on SVM and Naïve Bayes, respectively.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999932133333333">
In this work, we propose a method, called dual
training and dual prediction (DTDP), to address
the polarity shift problem in sentiment classifica-
tion. The basic idea of DTDP is to generate arti-
ficial samples that are polarity-opposite to the
original samples, and to make use of both the
original and opposite samples for dual training
and dual prediction. Experimental studies show
that our DTDP algorithm is very effective for
sentiment classification and it beats other alterna-
tive methods of considering polarity shift.
One limitation of current work is that the tun-
ing of parameters in DTDP (such as and ) is
not well discussed. We will leave this issue to an
extended version.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999524214285714">
The research work is supported by the Jiangsu
Provincial Natural Science Foundation of China
(BK2012396), the Research Fund for the Doc-
toral Program of Higher Education of China
(20123219120025), and the Open Project Pro-
gram of the National Laboratory of Pattern
Recognition (NLPR). This work is also partly
supported by the Hi-Tech Research and Devel-
opment Program of China (2012AA011102 and
2012AA011101), the Program of Introducing
Talents of Discipline to Universities (B13022),
and the Open Project Program of the Jiangsu Key
Laboratory of Image and Video Understanding
for Social Safety (30920130122006).
</bodyText>
<page confidence="0.997051">
524
</page>
<sectionHeader confidence="0.99022" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833214285714">
S. Das and M. Chen. 2001. Yahoo! for Amazon:
Extracting market sentiment from stock mes-
sage boards. In Proceedings of the Asia Pacif-
ic Finance Association Annual Conference.
M. Hu and B. Liu. 2004. Mining opinion features
in customer reviews. In Proceedings of the
National Conference on Artificial Intelligence
(AAAI).
D. Ikeda, H. Takamura L. Ratinov M. Okumura.
2008. Learning to Shift the Polarity of Words
for Sentiment Classification. In Proceedings
of the International Joint Conference on Natu-
ral Language Processing (IJCNLP).
S. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceeding of the Inter-
national Conference on Computational Lin-
guistics (COLING).
A. Kennedy and D. Inkpen. 2006. Sentiment
classification of movie reviews using contex-
tual valence shifters. Computational Intelli-
gence, 22:110–125.
S. Li and C. Huang. 2009. Sentiment classifica-
tion considering negation and contrast transi-
tion. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Com-
putation (PACLIC).
S. Li, S. Lee, Y. Chen, C. Huang and G. Zhou.
2010. Sentiment Classification and Polarity
Shifting. In Proceeding of the International
Conference on Computational Linguistics
(COLING).
J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou.
2004. Effectiveness of simple linguistic pro-
cessing in automatic sentiment classification
of product reviews. In Proceeding of the Con-
ference of the International Society for
Knowledge Organization.
B. Pang, L. Lee, and S. Vaithyanathan. 2002.
Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
L. Polanyi and A. Zaenen. 2004. Contextual lex-
ical valence shifters. In Proceedings of the
AAAI Spring Symposium on Exploring Attitude
and Affect in Text, AAAI technical report.
P. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised
classification of reviews. In Proceeding of the
Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
T. Wilson, J. Wiebe, and P. Hoffmann. 2005.
Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of
the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
</reference>
<page confidence="0.998481">
525
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.089722">
<title confidence="0.999737">Dual Training and Dual Prediction for Polarity Classification</title>
<author confidence="0.999689">Rui Xia</author>
<author confidence="0.999689">Tao Wang</author>
<author confidence="0.999689">Xuelei</author>
<affiliation confidence="0.999956">Department of Computer Science Nanjing University</affiliation>
<title confidence="0.5269255">Science and Technology xlhu@njust.edu.cn</title>
<author confidence="0.99806">Shoushan Li</author>
<affiliation confidence="0.71867075">NLP Department Computer Soochow</affiliation>
<email confidence="0.997928">@gmail.com</email>
<author confidence="0.948091">Chengqing Zong</author>
<affiliation confidence="0.954434">National Lab Pattern Institute of</affiliation>
<email confidence="0.773222">@nlpr.ia.ac.cn</email>
<abstract confidence="0.998877555555556">Bag-of-words (BOW) is now the most popular way to model text in machine learning based sentiment classification. However, the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the BOW model. In this paper, we focus on the polarity shift problem, and propose a novel approach, called dual training and dual prediction (DTDP), to address it. The basic idea of DTDP is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion, and then leverage both the original and opposite samples for (dual) training and (dual) prediction. Experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Das</author>
<author>M Chen</author>
</authors>
<title>Yahoo! for Amazon: Extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proceedings of the Asia Pacific Finance Association Annual Conference.</booktitle>
<contexts>
<context position="2985" citStr="Das and Chen, 2001" startWordPosition="451" endWordPosition="454">t”) are the most important type of polarity shifter. For example, by adding a negation word “don’t” to a positive text “I like this book” in front of “like”, the orientation of the text is reversed from positive to negative. Naturally, handling polarity shift is very important for sentiment classification. However, the BOW representations of two polarity-opposite texts, e.g., “I like this book” and “I don’t like this book”, are considered to be very similar by most of machine learning algorithms. Although some methods have been proposed in the literature to address the polarity shift problem (Das and Chen, 2001; Pang et al., 2002; Na et al., 2004; Kenndey and Inkpen, 2006; Ikeda et al., 2008; Li and Huang, 2009; Li et al., 2010), the state-of-the-art results are still far from satisfactory. For example, the improvements are less than 2% after considering polarity shift in Li et al. (2010). In this work, we propose a novel approach, called dual training and dual prediction (DTDP), to address the polarity shift problem. By taking advantage of the unique nature of polarity classification, DTDP is motivated by first generating artificial samples that are polarity-opposite to the original ones. For examp</context>
<context position="5210" citStr="Das and Chen (2001)" startWordPosition="795" endWordPosition="798">words, and then sum up the orientations word by word (Hu and Liu, 2004; Kim and Hovy, 2004; Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006). Wilson et al. (2005) discussed other complex negation effects by using conjunctive and dependency relations among polarity words. Although handling polarity shift is easy and effective in term-counting systems, they rarely outperform the baselines of machine learning methods (Kennedy, 2006). The machine learning methods are generally more effective for sentiment classification. However, it is difficult to handle polarity shift based on the BOW model. Das and Chen (2001) proposed a method by simply attaching “NOT” to words in the scope of negation, so that in the text “I don’t like book”, the word “like” is changed to a new word “like-NOT”. There were also some attempts to model polarity shift by using more complex linguistic features (Na et al., 2004; Kennedy and Inkpen, 2006). But the improvements upon the baselines of machine learning systems are very slight (less than 1%). Ikeda et al. (2008) proposed a machine learning method, to model polarity-shifters for both word-wise and sentence-wise sentiment classification, based on a dictionary extracted from Ge</context>
<context position="11821" citStr="Das and Chen (2001)" startWordPosition="1900" endWordPosition="1903">itchen. Each of them contains 1,000 positive and 1,000 negative reviews. Each of the datasets is randomly spit into 5 folds, with four folds serving as training data, and the remaining one fold serving as test data. All of the following results are reported in terms of an average of 5-fold cross validation. 4.2 Evaluated Systems We evaluate four machine learning systems that are proposed to address polarity shift in document-level polarity classification: 1) Baseline: standard machine learning methods based on the BOW model, without handling polarity shift; 2) Das-2001: the method proposed by Das and Chen (2001), where “NOT” is attached to the words in the scope of negation as a preprocessing step; 3) Li-2010: the approach proposed by Li et al. (2010). The details of the algorithm is introduced in related work; 4) DTDP: our approach proposed in Section 3. The WordNet dictionary is used for sample reversion. The empirical value of the parameter and are used in the evaluation. 4.3 Comparison of the Evaluated Systems In table 1, we report the classification accuracy of four evaluated systems using unigram features. We consider two widely-used classification algorithms: SVM and Naïve Bayes. For SVM, the </context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>S. Das and M. Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="4661" citStr="Hu and Liu, 2004" startWordPosition="709" endWordPosition="712">effective at correcting the training and prediction errors caused 521 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 521–525, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics by polarity shift, and it beats other alternative methods of considering polarity shift. 2 Related Work The lexicon-based sentiment classification systems can be easily modified to include polarity shift. One common way is to directly reverse the sentiment orientation of polarity-shifted words, and then sum up the orientations word by word (Hu and Liu, 2004; Kim and Hovy, 2004; Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006). Wilson et al. (2005) discussed other complex negation effects by using conjunctive and dependency relations among polarity words. Although handling polarity shift is easy and effective in term-counting systems, they rarely outperform the baselines of machine learning methods (Kennedy, 2006). The machine learning methods are generally more effective for sentiment classification. However, it is difficult to handle polarity shift based on the BOW model. Das and Chen (2001) proposed a method by simply attaching “NOT” to wor</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining opinion features in customer reviews. In Proceedings of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ikeda</author>
<author>H Takamura L Ratinov M Okumura</author>
</authors>
<title>Learning to Shift the Polarity of Words for Sentiment Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<marker>Ikeda, Okumura, 2008</marker>
<rawString>D. Ikeda, H. Takamura L. Ratinov M. Okumura. 2008. Learning to Shift the Polarity of Words for Sentiment Classification. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceeding of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="4681" citStr="Kim and Hovy, 2004" startWordPosition="713" endWordPosition="716">cting the training and prediction errors caused 521 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 521–525, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics by polarity shift, and it beats other alternative methods of considering polarity shift. 2 Related Work The lexicon-based sentiment classification systems can be easily modified to include polarity shift. One common way is to directly reverse the sentiment orientation of polarity-shifted words, and then sum up the orientations word by word (Hu and Liu, 2004; Kim and Hovy, 2004; Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006). Wilson et al. (2005) discussed other complex negation effects by using conjunctive and dependency relations among polarity words. Although handling polarity shift is easy and effective in term-counting systems, they rarely outperform the baselines of machine learning methods (Kennedy, 2006). The machine learning methods are generally more effective for sentiment classification. However, it is difficult to handle polarity shift based on the BOW model. Das and Chen (2001) proposed a method by simply attaching “NOT” to words in the scope of n</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>S. Kim and E. Hovy. 2004. Determining the sentiment of opinions. In Proceeding of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kennedy</author>
<author>D Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<pages>22--110</pages>
<contexts>
<context position="4734" citStr="Kennedy and Inkpen, 2006" startWordPosition="722" endWordPosition="725">d 521 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 521–525, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics by polarity shift, and it beats other alternative methods of considering polarity shift. 2 Related Work The lexicon-based sentiment classification systems can be easily modified to include polarity shift. One common way is to directly reverse the sentiment orientation of polarity-shifted words, and then sum up the orientations word by word (Hu and Liu, 2004; Kim and Hovy, 2004; Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006). Wilson et al. (2005) discussed other complex negation effects by using conjunctive and dependency relations among polarity words. Although handling polarity shift is easy and effective in term-counting systems, they rarely outperform the baselines of machine learning methods (Kennedy, 2006). The machine learning methods are generally more effective for sentiment classification. However, it is difficult to handle polarity shift based on the BOW model. Das and Chen (2001) proposed a method by simply attaching “NOT” to words in the scope of negation, so that in the text “I don’t like book”, the</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>A. Kennedy and D. Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational Intelligence, 22:110–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>C Huang</author>
</authors>
<title>Sentiment classification considering negation and contrast transition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Pacific Asia Conference on Language, Information and Computation (PACLIC).</booktitle>
<contexts>
<context position="3087" citStr="Li and Huang, 2009" startWordPosition="471" endWordPosition="474"> a positive text “I like this book” in front of “like”, the orientation of the text is reversed from positive to negative. Naturally, handling polarity shift is very important for sentiment classification. However, the BOW representations of two polarity-opposite texts, e.g., “I like this book” and “I don’t like this book”, are considered to be very similar by most of machine learning algorithms. Although some methods have been proposed in the literature to address the polarity shift problem (Das and Chen, 2001; Pang et al., 2002; Na et al., 2004; Kenndey and Inkpen, 2006; Ikeda et al., 2008; Li and Huang, 2009; Li et al., 2010), the state-of-the-art results are still far from satisfactory. For example, the improvements are less than 2% after considering polarity shift in Li et al. (2010). In this work, we propose a novel approach, called dual training and dual prediction (DTDP), to address the polarity shift problem. By taking advantage of the unique nature of polarity classification, DTDP is motivated by first generating artificial samples that are polarity-opposite to the original ones. For example, given the original sample “I don’t like this book. It is boring,” its polarity-opposite version, “</context>
<context position="5845" citStr="Li and Huang (2009)" startWordPosition="902" endWordPosition="905">od by simply attaching “NOT” to words in the scope of negation, so that in the text “I don’t like book”, the word “like” is changed to a new word “like-NOT”. There were also some attempts to model polarity shift by using more complex linguistic features (Na et al., 2004; Kennedy and Inkpen, 2006). But the improvements upon the baselines of machine learning systems are very slight (less than 1%). Ikeda et al. (2008) proposed a machine learning method, to model polarity-shifters for both word-wise and sentence-wise sentiment classification, based on a dictionary extracted from General Inquirer. Li and Huang (2009) proposed a method first to classify each sentence in a text into a polarity-unshifted part and a polarityshifted part according to certain rules, then to represent them as two bag-of-words for sentiment classification. Li et al. (2010) further proposed a method to separate the shifted and unshifted text based on training a binary detector. Classification models are then trained based on each of the two parts. An ensemble of two component parts is used at last to get the final polarity of the whole text. 3 The Proposed Approach We first present the method for generating artificial polarity-opp</context>
</contexts>
<marker>Li, Huang, 2009</marker>
<rawString>S. Li and C. Huang. 2009. Sentiment classification considering negation and contrast transition. In Proceedings of the Pacific Asia Conference on Language, Information and Computation (PACLIC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>S Lee</author>
<author>Y Chen</author>
<author>C Huang</author>
<author>G Zhou</author>
</authors>
<title>Sentiment Classification and Polarity Shifting.</title>
<date>2010</date>
<booktitle>In Proceeding of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="3105" citStr="Li et al., 2010" startWordPosition="475" endWordPosition="478">like this book” in front of “like”, the orientation of the text is reversed from positive to negative. Naturally, handling polarity shift is very important for sentiment classification. However, the BOW representations of two polarity-opposite texts, e.g., “I like this book” and “I don’t like this book”, are considered to be very similar by most of machine learning algorithms. Although some methods have been proposed in the literature to address the polarity shift problem (Das and Chen, 2001; Pang et al., 2002; Na et al., 2004; Kenndey and Inkpen, 2006; Ikeda et al., 2008; Li and Huang, 2009; Li et al., 2010), the state-of-the-art results are still far from satisfactory. For example, the improvements are less than 2% after considering polarity shift in Li et al. (2010). In this work, we propose a novel approach, called dual training and dual prediction (DTDP), to address the polarity shift problem. By taking advantage of the unique nature of polarity classification, DTDP is motivated by first generating artificial samples that are polarity-opposite to the original ones. For example, given the original sample “I don’t like this book. It is boring,” its polarity-opposite version, “I like this book. </context>
<context position="6081" citStr="Li et al. (2010)" startWordPosition="941" endWordPosition="944">istic features (Na et al., 2004; Kennedy and Inkpen, 2006). But the improvements upon the baselines of machine learning systems are very slight (less than 1%). Ikeda et al. (2008) proposed a machine learning method, to model polarity-shifters for both word-wise and sentence-wise sentiment classification, based on a dictionary extracted from General Inquirer. Li and Huang (2009) proposed a method first to classify each sentence in a text into a polarity-unshifted part and a polarityshifted part according to certain rules, then to represent them as two bag-of-words for sentiment classification. Li et al. (2010) further proposed a method to separate the shifted and unshifted text based on training a binary detector. Classification models are then trained based on each of the two parts. An ensemble of two component parts is used at last to get the final polarity of the whole text. 3 The Proposed Approach We first present the method for generating artificial polarity-opposite samples, and then introduce the algorithm of dual training and dual prediction (DTDP). 3.1 Generating Artificial Polarity-Opposite Samples Given an original sample and an antonym dictionary (e.g., WordNet 1 ), a polarity-opposite </context>
<context position="11963" citStr="Li et al. (2010)" startWordPosition="1927" endWordPosition="1930">ing as training data, and the remaining one fold serving as test data. All of the following results are reported in terms of an average of 5-fold cross validation. 4.2 Evaluated Systems We evaluate four machine learning systems that are proposed to address polarity shift in document-level polarity classification: 1) Baseline: standard machine learning methods based on the BOW model, without handling polarity shift; 2) Das-2001: the method proposed by Das and Chen (2001), where “NOT” is attached to the words in the scope of negation as a preprocessing step; 3) Li-2010: the approach proposed by Li et al. (2010). The details of the algorithm is introduced in related work; 4) DTDP: our approach proposed in Section 3. The WordNet dictionary is used for sample reversion. The empirical value of the parameter and are used in the evaluation. 4.3 Comparison of the Evaluated Systems In table 1, we report the classification accuracy of four evaluated systems using unigram features. We consider two widely-used classification algorithms: SVM and Naïve Bayes. For SVM, the LibSVM toolkit3 is used with a linear kernel and the default penalty parameter. For Naïve Bayes, the OpenPR-NB toolkit4 is used. 2 http://www.</context>
</contexts>
<marker>Li, Lee, Chen, Huang, Zhou, 2010</marker>
<rawString>S. Li, S. Lee, Y. Chen, C. Huang and G. Zhou. 2010. Sentiment Classification and Polarity Shifting. In Proceeding of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Na</author>
<author>H Sui</author>
<author>C Khoo</author>
<author>S Chan</author>
<author>Y Zhou</author>
</authors>
<title>Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews.</title>
<date>2004</date>
<booktitle>In Proceeding of the Conference of the International Society for Knowledge Organization.</booktitle>
<contexts>
<context position="3021" citStr="Na et al., 2004" startWordPosition="459" endWordPosition="462">rity shifter. For example, by adding a negation word “don’t” to a positive text “I like this book” in front of “like”, the orientation of the text is reversed from positive to negative. Naturally, handling polarity shift is very important for sentiment classification. However, the BOW representations of two polarity-opposite texts, e.g., “I like this book” and “I don’t like this book”, are considered to be very similar by most of machine learning algorithms. Although some methods have been proposed in the literature to address the polarity shift problem (Das and Chen, 2001; Pang et al., 2002; Na et al., 2004; Kenndey and Inkpen, 2006; Ikeda et al., 2008; Li and Huang, 2009; Li et al., 2010), the state-of-the-art results are still far from satisfactory. For example, the improvements are less than 2% after considering polarity shift in Li et al. (2010). In this work, we propose a novel approach, called dual training and dual prediction (DTDP), to address the polarity shift problem. By taking advantage of the unique nature of polarity classification, DTDP is motivated by first generating artificial samples that are polarity-opposite to the original ones. For example, given the original sample “I don</context>
<context position="5496" citStr="Na et al., 2004" startWordPosition="848" endWordPosition="851">ng polarity shift is easy and effective in term-counting systems, they rarely outperform the baselines of machine learning methods (Kennedy, 2006). The machine learning methods are generally more effective for sentiment classification. However, it is difficult to handle polarity shift based on the BOW model. Das and Chen (2001) proposed a method by simply attaching “NOT” to words in the scope of negation, so that in the text “I don’t like book”, the word “like” is changed to a new word “like-NOT”. There were also some attempts to model polarity shift by using more complex linguistic features (Na et al., 2004; Kennedy and Inkpen, 2006). But the improvements upon the baselines of machine learning systems are very slight (less than 1%). Ikeda et al. (2008) proposed a machine learning method, to model polarity-shifters for both word-wise and sentence-wise sentiment classification, based on a dictionary extracted from General Inquirer. Li and Huang (2009) proposed a method first to classify each sentence in a text into a polarity-unshifted part and a polarityshifted part according to certain rules, then to represent them as two bag-of-words for sentiment classification. Li et al. (2010) further propos</context>
</contexts>
<marker>Na, Sui, Khoo, Chan, Zhou, 2004</marker>
<rawString>J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004. Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews. In Proceeding of the Conference of the International Society for Knowledge Organization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3004" citStr="Pang et al., 2002" startWordPosition="455" endWordPosition="458">ortant type of polarity shifter. For example, by adding a negation word “don’t” to a positive text “I like this book” in front of “like”, the orientation of the text is reversed from positive to negative. Naturally, handling polarity shift is very important for sentiment classification. However, the BOW representations of two polarity-opposite texts, e.g., “I like this book” and “I don’t like this book”, are considered to be very similar by most of machine learning algorithms. Although some methods have been proposed in the literature to address the polarity shift problem (Das and Chen, 2001; Pang et al., 2002; Na et al., 2004; Kenndey and Inkpen, 2006; Ikeda et al., 2008; Li and Huang, 2009; Li et al., 2010), the state-of-the-art results are still far from satisfactory. For example, the improvements are less than 2% after considering polarity shift in Li et al. (2010). In this work, we propose a novel approach, called dual training and dual prediction (DTDP), to address the polarity shift problem. By taking advantage of the unique nature of polarity classification, DTDP is motivated by first generating artificial samples that are polarity-opposite to the original ones. For example, given the origi</context>
<context position="14700" citStr="Pang et al. (2002)" startWordPosition="2351" endWordPosition="2354">0), the average improvement is 2.2% and 1.7% on SVM and Naïve Bayes, respectively. We also report the classification accuracy of four systems using both unigrams and bigrams features for classification in Table 2. From this table, we can see that the performance of each system is improved compared to that using unigrams. It is now relatively difficult to show improvements by incorporating polarity shift, because using bigrams already captured a part of negations (e.g., “don’t like”). The Das-2001 approach still shows very limited improvements (less than 0.5%), which agrees with the reports in Pang et al. (2002). The improvements of Li-2010 are also reduced: 1.9% and 1% on SVM and Naïve Bayes, respectively. Although the improvements of the previous two systems are both limited, the performance of our approach (DTDP) is still sound. It improves the Baseline system by 3.7% and 2.9% on SVM and Naïve Bayes, respectively, and outperforms the state-of-the-art (Li-2010) by 1.6% and 1.9% on SVM and Naïve Bayes, respectively. 5 Conclusions In this work, we propose a method, called dual training and dual prediction (DTDP), to address the polarity shift problem in sentiment classification. The basic idea of DTD</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>A Zaenen</author>
</authors>
<title>Contextual lexical valence shifters.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text, AAAI technical report.</booktitle>
<contexts>
<context position="4707" citStr="Polanyi and Zaenen, 2004" startWordPosition="717" endWordPosition="721">nd prediction errors caused 521 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 521–525, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics by polarity shift, and it beats other alternative methods of considering polarity shift. 2 Related Work The lexicon-based sentiment classification systems can be easily modified to include polarity shift. One common way is to directly reverse the sentiment orientation of polarity-shifted words, and then sum up the orientations word by word (Hu and Liu, 2004; Kim and Hovy, 2004; Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006). Wilson et al. (2005) discussed other complex negation effects by using conjunctive and dependency relations among polarity words. Although handling polarity shift is easy and effective in term-counting systems, they rarely outperform the baselines of machine learning methods (Kennedy, 2006). The machine learning methods are generally more effective for sentiment classification. However, it is difficult to handle polarity shift based on the BOW model. Das and Chen (2001) proposed a method by simply attaching “NOT” to words in the scope of negation, so that in the te</context>
</contexts>
<marker>Polanyi, Zaenen, 2004</marker>
<rawString>L. Polanyi and A. Zaenen. 2004. Contextual lexical valence shifters. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text, AAAI technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceeding of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceeding of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4756" citStr="Wilson et al. (2005)" startWordPosition="726" endWordPosition="729">st Annual Meeting of the Association for Computational Linguistics, pages 521–525, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics by polarity shift, and it beats other alternative methods of considering polarity shift. 2 Related Work The lexicon-based sentiment classification systems can be easily modified to include polarity shift. One common way is to directly reverse the sentiment orientation of polarity-shifted words, and then sum up the orientations word by word (Hu and Liu, 2004; Kim and Hovy, 2004; Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006). Wilson et al. (2005) discussed other complex negation effects by using conjunctive and dependency relations among polarity words. Although handling polarity shift is easy and effective in term-counting systems, they rarely outperform the baselines of machine learning methods (Kennedy, 2006). The machine learning methods are generally more effective for sentiment classification. However, it is difficult to handle polarity shift based on the BOW model. Das and Chen (2001) proposed a method by simply attaching “NOT” to words in the scope of negation, so that in the text “I don’t like book”, the word “like” is change</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>