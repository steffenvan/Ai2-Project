<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001997">
<title confidence="0.992673">
Adapting a Lexicalized-Grammar Parser to Contrasting Domains
</title>
<author confidence="0.99721">
Laura Rimell and Stephen Clark
</author>
<affiliation confidence="0.998253">
Oxford University Computing Laboratory
</affiliation>
<address confidence="0.9904795">
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
</address>
<email confidence="0.999779">
{laura.rimell,stephen.clark}@comlab.ox.ac.uk
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999386166666667">
Most state-of-the-art wide-coverage parsers
are trained on newspaper text and suffer a
loss of accuracy in other domains, making
parser adaptation a pressing issue. In this
paper we demonstrate that a CCG parser can
be adapted to two new domains, biomedical
text and questions for a QA system, by us-
ing manually-annotated training data at the
POS and lexical category levels only. This ap-
proach achieves parser accuracy comparable
to that on newspaper data without the need
for annotated parse trees in the new domain.
We find that retraining at the lexical category
level yields a larger performance increase for
questions than for biomedical text and analyze
the two datasets to investigate why different
domains might behave differently for parser
adaptation.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999769375">
Most state-of-the-art wide-coverage parsers are
based on the Penn Treebank (Marcus et al., 1993),
making such parsers highly tuned to newspaper text.
A pressing question facing the parsing community
is how to adapt these parsers to other domains, such
as biomedical research papers and web pages. A re-
lated question is how to improve the performance
of these parsers on constructions that are rare in the
Penn Treebank, such as questions. Questions are
particularly important since a question parser is a
component in most Question Answering (QA) sys-
tems (Harabagiu et al., 2001).
In this paper we investigate parser adaptation in
the context of lexicalized grammars, by using a
parser based on Combinatory Categorial Grammar
(CCG) (Steedman, 2000). A key property of CCG is
that it is lexicalized, meaning that each word in a
sentence is associated with an elementary syntactic
structure. In the case of CCG this is a lexical cate-
gory expressing subcategorization information. We
exploit this property of CCG by performing manual
annotation in the new domain, but only up to this
level of representation, where the annotation can be
carried out relatively quickly. Since CCG lexical cat-
egories are so expressive, many of the syntactic char-
acteristics of a domain are captured at this level.
The two domains we consider are the biomedical
domain and questions for a QA system. We use the
term “domain” somewhat loosely here, since ques-
tions are best described as a particular set of syn-
tactic constructions, rather than a set of documents
about a particular topic. However, we consider ques-
tion data to be interesting in the context of domain
adaptation for the following reasons: 1) there are
few examples in the Penn Treebank (PTB) and so
PTB parsers typically perform poorly on them; 2)
questions form a fairly homogeneous set with re-
spect to the syntactic constructions employed, and
it is an interesting question how easy it is to adapt a
parser to such data; and 3) QA is becoming an impor-
tant example of NLP technology, and question pars-
ing is an important task for QA systems.
The CCG parser we use (Clark and Curran, 2007b)
makes use of three levels of representation: one, a
POS tag level based on the fairly coarse-grained POS
tags in the Penn Treebank; two, a lexical category
level based on the more fine-grained CCG lexical cat-
egories, which are assigned to words by a CCG su-
</bodyText>
<page confidence="0.987534">
475
</page>
<note confidence="0.962069">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475–484,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999548">
pertagger; and three, a hierarchical level consisting
of CCG derivations. A key idea in this paper, follow-
ing a pilot study in Clark et al. (2004), is to perform
manual annotation only at the first two levels. Since
the lexical category level consists of sequences of
tags, rather than hierarchical derivations, the anno-
tation can be performed relatively quickly.
For the biomedical and question domains we
manually annotated approximately 1,000 and 2,000
sentences, respectively, with CCG lexical categories.
We also created a gold standard set of grammati-
cal relations (GR) in the Stanford format (de Marn-
effe et al., 2006), using 500 of the questions. For
the biomedical domain we used the BioInfer corpus
(Pyysalo et al., 2007a), an existing gold-standard GR
resource also in the Stanford format. We evaluated
the parser on both lexical category assignment and
recovery of GRs.
The results show that the domain adaptation ap-
proach used here is successful in two very different
domains, achieving parsing accuracy comparable to
state-of-the-art accuracy for newspaper text. The re-
sults also show, however, that the two domains have
different profiles with regard to the levels of repre-
sentation used by the parser. We find that simply re-
training the POS tagger used by the parser leads to a
large improvement in performance for the biomed-
ical domain, and that retraining the CCG supertag-
ger on the annotated biomedical data improves the
performance further. For the question data, retrain-
ing just the POS tagger also improves parser perfor-
mance, but retraining the supertagger has a much
greater effect. We perform some analysis of the two
datasets in order to explain the different behaviours
with regard to porting the CCG parser.
</bodyText>
<sectionHeader confidence="0.992668" genericHeader="method">
2 The CCG Parser
</sectionHeader>
<bodyText confidence="0.99995624137931">
The CCG parser is described in detail in Clark and
Curran (2007b) and so we provide only a brief de-
scription. The stages in the CCG parsing pipeline are
as follows. First, a maximum entropy POS tagger
assigns a single POS tag to each word in a sentence.
POS tags are fairly coarse-grained grammatical la-
bels indicating part-of-speech; the Penn Treebank
set, used here, contains approximately 50 labels.
Second, a maximum entropy supertagger assigns
CCG lexical categories to the words in the sentence.
Lexical categories can be thought of as fine-grained
POS tags expressing subcategorization information,
i.e. information about the argument frame of the
word. There are 425 categories in the set used by the
CCG parser. Supertagging was originally developed
for Lexicalized Tree Adjoining Grammar (Banga-
lore and Joshi, 1999), but has been particularly suc-
cessful for wide-coverage CCG parsing (Clark and
Curran, 2007b). Rather than assign a single category
to each word, the supertagger operates as a multi-
tagger, sometimes assigning more than one category
if the context is not sufficiently discriminating to
suggest a single tag (Curran et al., 2006). Since
the taggers have linear time complexity, the first two
stages can be performed extremely quickly.
Finally, the parsing stage combines the lexical cat-
egories, using a small set of combinatory rules that
are part of the grammar of CCG, and builds a packed
chart representation containing all the derivations
which can be built from the lexical categories. The
Viterbi algorithm efficiently finds the highest scor-
ing derivation from the packed chart, using a log-
linear model to score the derivations. The grammar
and training data for the newspaper version of the
CCG parser are obtained from CCGbank (Hocken-
maier and Steedman, 2007), a CCG version of the
Penn Treebank.
The aspect of the pipeline which is most relevant
to this paper is the supertagging phase. Figure 1
gives an example sentence from each target domain,
with the CCG lexical category assigned to each word
shown below the word, and the POS tag to the right.
Note that the categories contain a significant amount
of grammatical information, in particular subcatego-
rization information. The verb acts in the biomedi-
cal sentence, for example, looks for a prepositional
phrase (PP, as a linkage protein) to its right and a
noun phrase (NP, Talin) to its left, with the resulting
category a declarative sentence (S[dcl]).
Bangalore and Joshi (1999) refer to supertagging
as almost parsing, because once the correct lexical
categories have been assigned, the parser is left with
much less work to do. The CCG supertagger is not
able to assign a single category to each word with
extremely high accuracy — hence the need for it to
operate as a multi-tagger — but even in multi-tagger
mode it dramatically reduces the ambiguity passed
through to the parser (Clark and Curran, 2007b).
</bodyText>
<page confidence="0.97056">
476
</page>
<equation confidence="0.79540425">
Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|.
NP (S\NP)/(S\NP) (S[dcl]\NP)/PP PP/NP NP[nb]/N N/N N .
What|WDT king|NN signed|VBD the|DT Magna|NNP Carta|NNP ?|.
(S[wq]/(S[dcl]\NP))/N N (S[dcl]\NP)/NP NP[nb]/N N/N N .
</equation>
<figureCaption confidence="0.996921">
Figure 1: Example sentences with lexical category assignment.
</figureCaption>
<bodyText confidence="0.999965681818182">
The parser has been evaluated on DepBank (King
et al., 2003), using the GR scheme of Briscoe et
al. (2006), and it scores 82.4% labelled precision
and 81.2% labelled recall overall (Clark and Curran,
2007a). Section 4.4 describes how the CCG depen-
dencies can be mapped into the Stanford GR scheme
(de Marneffe et al., 2006) and gives the results of
evaluating the parser on biomedical and question GR
resources.
The CCG parser is particularly well suited to the
biomedical and question domains. First, use of CCG
allows recovery of long-distance dependencies. In
the sentence What does target heart rate mean?, the
word What is an underlying object of the verb mean.
The parser recovers this information despite the dis-
tance between the two words. This capability is
crucial for question parsing, and also useful in the
biomedical field for extraction of relationships be-
tween biological entities. Additionally, the speed of
the parser (tens of sentences per second) is useful
for the large volumes of biomedical data that require
processing for biomedical text mining.
</bodyText>
<sectionHeader confidence="0.994406" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999974759259259">
Our approach to domain adaptation is to target the
coarser-grained, less syntactically complex, levels of
representation used by the parser, and to train new
models with manually annotated data at these levels.
The motivation for this approach is twofold. First,
accuracy at each stage of the pipeline depends on ac-
curacy at the earlier stages. If the POS tagger assigns
incorrect tags, it is unlikely that the supertagger will
be able to recover and produce the correct lexical
categories, since it relies heavily on POS tags as fea-
tures. Without the correct categories, the parser in
turn will be unable to find a correct parse.
In the sentence What year did the Vietnam War
end?, the newspaper-trained POS tagger incorrectly
assigns the POS tag NN (common noun) to the verb
end, since verb-final sentences are atypical for the
PTB. As a result, the supertagger is virtually cer-
tain (greater than 99% probability) that the correct
CCG lexical category for end is N (noun). The parser
then assigns the Vietnam War end the structure of a
noun phrase, and chooses an unusual subcategoriza-
tion frame for did in which it takes three arguments:
What, year, and the Vietnam War end.
In the sentence How many siblings does
she have?, on the other hand, the supertag-
ger assigns an incorrect category to the word
How despite it having the correct POS tag
(WRB for wh-adverb). The correct category is
((S[wq]/(S[q]/NP))/N)/(NP/N), which takes
many (category NP/N) and siblings (category N)
as arguments. Instead it is tagged as S[wq]/S[q],
the category for a sentential adverb (i.e. the man-
ner reading of how), which prevents a correct parse.
Our intention was that creating new training data at
the lower levels of representation would improve the
accuracy of the POS tagger and supertagger in the
target domains, thereby improving the accuracy of
later stages in the pipeline as well.
The second motivation for our approach is to re-
duce annotation overhead. Full syntactic deriva-
tions are costly to produce by hand. POS tags, how-
ever, are relatively easy to annotate; even an out-
of-domain tagger will provide a good starting point,
and manual correction is quick, especially in a do-
main without much unfamiliar vocabulary. CCG lex-
ical categories require more expertise, but our ex-
perience shows that an out-of-domain supertagger
can again provide a starting point for correction, and
since the annotation is flat rather than hierarchical,
we hypothesize that it is not as difficult or time-
consuming as annotation of full derivations.
Our adaptation approach has been partially ex-
plored in previous work which targets one or another
of the different levels of representation.
</bodyText>
<page confidence="0.992003">
477
</page>
<bodyText confidence="0.999985966666667">
Lease and Charniak (2005) obtained an improve-
ment in the accuracy of the Charniak (2000) parser,
as well as POS tagging accuracy, when applied to
the biomedical domain, by training a new POS tag-
ger model with a combination of newspaper and
biomedical data. The parser improvement was due
solely to the new POS tagger, without retraining the
parser model. Since the Charniak parser does not
use a lexicalized grammar with an intermediate level
of representation, any further improvements would
have to come from the parser model itself.
Clark et al. (2004) obtained an improvement in
CCG supertagging accuracy for What-questions by
training a new supertagger model with a combina-
tion of newspaper and question data annotated with
CCG lexical categories. Because a question resource
annotated with GRs was not available, they did not
perform a parser evaluation, and the effects of the
POS tagging level were not compared to the lexi-
cal category level. In this paper, we extend the pi-
lot experiments performed by Clark et al. (2004) in
four ways. First, we use a larger corpus of TREC
questions covering additional question types, thus
extending the experiments to the question domain
more broadly, as well as to the biomedical domain.
Second, we create a gold standard GR resource en-
abling a full parser evaluation on question data.
Third, we show that the POS level is important for
adaptation, reinforcing the work of Lease and Char-
niak (2005). A key finding of the present paper is
that the combination of retraining at the POS tag and
lexical category levels provides additional improve-
ments beyond those gained by retraining at a single
level. Finally, we provide analysis comparing the
adaptation methodology for question and biomedi-
cal data.
Hara et al. (2007) followed a similar approach to
Clark et al. (2004), using the parser of Ninomiya
et al. (2006), a version of the Enju parser (Miyao
and Tsujii, 2005). Enju is based on HPSG, a lex-
icalized grammar formalism. They obtained an im-
provement in parsing accuracy in the biomedical do-
main by training a new probabilistic model of lexi-
cal entry assignments on a combination of newspa-
per and biomedical data without changing the orig-
inal newspaper-trained parsing model. Hara et al.
(2007) did not consider the role of POS tagging. The
lexical category data in Hara et al. (2007) was de-
rived from a gold standard treebank, while the an-
notation of lexical categories in this paper was per-
formed without reference to gold standard syntactic
derivations.
Judge et al. (2006) produced a corpus of 4,000
questions annotated with syntactic trees, and ob-
tained an improvement in parsing accuracy for
Bikel’s reimplementation of the Collins parser
(Collins, 1997) by training a new parser model with
a combination of newspaper and question data. Our
approach differs in retraining only at the levels of
representation below parse trees.
</bodyText>
<sectionHeader confidence="0.998175" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.927909">
4.1 Resources
</subsectionHeader>
<bodyText confidence="0.9999678125">
We have used a combination of existing resources
and new, manually annotated data. The baseline POS
tagger, supertagger, and parser are trained on WSJ
Sections 02-21 of CCGbank. The baseline perfor-
mance at each level of representation is on WSJ Sec-
tion 00 of CCGbank, which contains 1913 sentences
and approximately 45,000 words.
For the biomedical domain, we trained the POS
tagger on gold-standard POS tags from GENIA (Kim
et al., 2003), a corpus of 2,000 MEDLINE abstracts
containing a total of approximately 18,500 sentences
and 440,000 words. We also annotated the first
1,000 sentences of GENIA with CCG lexical cate-
gories. This set of 1,000 sentences, containing ap-
proximately 27,000 words, was used for POS tagger
evaluation and for development and evaluation of a
new supertagger model. For parser evaluation, we
used BioInfer (Pyysalo et al., 2007a), a corpus of
MEDLINE abstracts (on a different topic from those
in GENIA) containing 1,100 sentences, and with syn-
tactic dependencies encoded as grammatical rela-
tions in the Stanford GR format. We used the same
evaluation set of 500 sentences as in Pyysalo et al.
(2007b), and the remaining 600 for development of
the mapping to Stanford format. Two parsers have
already been evaluated on BioInfer, which makes it
a useful resource for comparative evaluation.
For the question domain, we extended the dataset
described in Clark et al. (2004). That dataset con-
tained 1,171 questions beginning with the word
What, from the TREC 9-12 competitions (2000-
2003), manually POS tagged and annotated with
</bodyText>
<page confidence="0.995205">
478
</page>
<bodyText confidence="0.999271214285715">
CCG lexical categories. We annotated all the addi-
tional TREC question types and improved the exist-
ing annotation, for a total of 1,828 sentences. We ad-
ditionally annotated a random subset of 500 of these
with GRs in the Stanford format. This subset served
as our evaluation set at all levels of representation. It
contains approximately 4,000 words, fewer than the
other domains because of the significantly shorter
sentence lengths of typical questions. The remain-
ing 1,328 sentences were used as training data. A
set of about a dozen sentences from the evaluation
and training sets were used to develop the mapping
to Stanford format for lexical categories not occur-
ring in the biomedical data.
</bodyText>
<subsectionHeader confidence="0.989466">
4.2 POS tagger
</subsectionHeader>
<bodyText confidence="0.9999915">
We began by training new models at the POS tag
level of representation. All datasets use the PTB
tagset. As a baseline, we used the original WSJ 02-
21 model on the biomedical and question datasets.
For comparison we also evaluated on Section 00 us-
ing the WSJ-trained model.
For the question data, the new POS tagger was
trained on CCGbank Sections 02-21 plus ten copies
of the 1,328 training sentences. The WSJ data pro-
vides additional robustness and wide grammatical
coverage, and the weighting factor of ten was chosen
in preliminary experiments to prevent the newspaper
data from “overwhelming” the question data. For
the biomedical data, the new POS tagger was trained
on the full GENIA corpus, minus the first 1,000 sen-
tences. GENIA is large enough that combination with
the newspaper data was not needed.
Table 1 gives the results. For both of the new do-
mains the performance of the WSJ model decreased
compared to Section 00, but the retrained model per-
formed at least as well as the WSJ model did on 00.1
Improving the POS tagger performance has a posi-
tive effect on the performance of the supertagger and
parser, which will be discussed in Sections 4.3-4.4.
</bodyText>
<footnote confidence="0.8972722">
1Since GENIA does not use the proper noun tag, NNP, for
names of genes and other biomedical entities, all figures in
this paper collapse the NNP-NN distinction where relevant for
biomedical data. The question data uses NNP and the distinc-
tion is not collapsed.
</footnote>
<table confidence="0.98594175">
WSJ 02-21 Retrained
Sec. 00 96.7
92.2 97.1
93.4 98.7
</table>
<tableCaption confidence="0.992814">
Table 1: POS tagger accuracy (%) for original and re-
trained models.
</tableCaption>
<table confidence="0.993200833333333">
Orig Retrained Retrained
pipeline POS POS and
super
Sec. 00
71.6 74.0 92.1
89.0 91.2 93.0
</table>
<tableCaption confidence="0.9947105">
Table 2: Supertagging accuracy (%) and the effect of re-
training the POS model and the supertagger model.
</tableCaption>
<subsectionHeader confidence="0.985627">
4.3 Supertagger
</subsectionHeader>
<bodyText confidence="0.999977068965517">
We next trained new models at the CCG lexical cat-
egory level. The training data consisted of manu-
ally annotated biomedical and question sentences;
specifically, lexical categories were automatically
assigned by the original parsing pipeline and then
manually corrected. Whenever possible we used
categories from the parser’s original set of 425, al-
though occasionally it was necessary to use a new
category for a syntactic construction not occurring
in CCGbank Sections 02-21. (The parser can be con-
figured to recognize additional categories.) Question
data in particular requires the use of categories that
are rare or unseen in CCGbank.
For the questions, the new supertagger model,
like the POS tagger, was trained on WSJ 02-21 plus
ten copies of the 1,328 training sentences. For the
biomedical data, a ten-fold cross-validation was per-
formed, training each supertagger model on WSJ 02-
21 plus ten copies of 90% of the 1,000 annotated
sentences. Table 2 gives the supertagger accuracy
with and without the retrained POS and supertagger
models. The figure for the retrained biomedical su-
pertagger is the average of the ten-fold split.
The results show an improvement in accuracy of
lexical category assignment solely from retraining
the POS tagger, and an additional improvement from
retraining the supertagger. Supertagger accuracy for
the two domains with a retrained supertagger was
comparable, and in both cases was at least as high
</bodyText>
<figure confidence="0.9545445">
Qus
Bio
Qus
Bio
91.5 — —
479
What car company invented the Edsel?
(nsubj invented company)
(det Edsel the)
(dobj invented Edsel)
(det company What)
(nn company car)
</figure>
<figureCaption confidence="0.99976">
Figure 2: Example of grammatical relations in the Stan-
ford grammatical relation format.
</figureCaption>
<bodyText confidence="0.997704333333333">
as for the original pipeline on Section 00. The ques-
tion data started from a much lower baseline figure,
however.
</bodyText>
<subsectionHeader confidence="0.977653">
4.4 Parser
</subsectionHeader>
<bodyText confidence="0.9999367">
We evaluated the parser on the 500 questions anno-
tated with Stanford GRs and on the 500 evaluation
sentences from the BioInfer corpus. We used the
original newspaper pipeline, a pipeline with a re-
trained POS tagger, and a pipeline with both a re-
trained POS tagger and supertagger.
In order to perform these evaluations we devel-
ooped a mapping from the parser’s native CCG syn-
tactic dependencies to GRs in the Stanford format.
The mapping was based on the same principles as
the mapping that produces GR output in the style
of Briscoe et al. (2006). These principles are dis-
cussed in detail in Clark and Curran (2007a); in
summary, the argument slots in the CCG dependen-
cies are mapped to argument slots in Stanford GRs,
a fairly complex, many-to-many mapping. An ad-
ditional post-processing script applies some manu-
ally developed rules to bring the output closer to the
Stanford format. Figure 2 gives an example of Stan-
ford GRs, where the label of the relation is followed
by two arguments, head and dependent.
Table 3 gives the results of the parser evaluation
on GRs. Since the parser model was not retrained,
the improvements in accuracy are due solely to the
new POS and supertaggers. The results are given as
an F-score over labelled GRs.2
The F-scores given in Table 3 are only for sen-
tences for which a parse was found. However, there
were also improvements in coverage with the re-
trained models. For the question data, parser cov-
</bodyText>
<footnote confidence="0.811982">
2Only GRs at the lowest level of the Stanford hierarchy were
considered in the evaluation; more generic relations such as de-
pendent were not considered.
</footnote>
<note confidence="0.3019305">
Orig POS New POS New POS
and super and super
</note>
<table confidence="0.9937755">
Qus 64.4 69.4 86.6
BioInfer 76.0 80.4 81.5
</table>
<tableCaption confidence="0.9882485">
Table 3: Parser F-score on grammatical relations and the
effect of retraining the POS and supertagger models.
</tableCaption>
<bodyText confidence="0.99989435">
erage was 94% for the original pipeline and the
pipeline with just the retrained POS tagger, and
99.6% with the retrained POS and supertaggers. For
the biomedical data, coverage was 97.2% for the
original pipeline, 99.0% for the pipeline with the re-
trained POS tagger, and 99.8% for the pipeline with
the retrained POS and supertaggers.
The final accuracy for both domains is in the same
range as that of the original parser on newspaper
data (81.8%) (Clark and Curran, 2007b), although
the results are not directly comparable, since the
newspaper resource uses a different GR scheme. For
the BioInfer corpus, the final accuracy is also in
line with results reported in the literature for other
parsers (Pyysalo et al., 2007b). (No comparable GR
results are available for questions.) A score in this
range is thought to be near the upper bound when
evaluating a CCG parser on GRs, since some loss is
inherent in the mapping to GRs (Clark and Curran,
2007a).
</bodyText>
<sectionHeader confidence="0.994405" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999979411764706">
Although domain adaptation was successful for both
of our target domains, the impact of the different
levels of representation on parsing accuracy was not
uniform. Table 3 shows that retraining the POS tag-
ger accounted for a greater proportion of the im-
provement on biomedical data, while retraining the
supertagger accounted for a much greater proportion
on questions. In this section we discuss some of the
differences between the domains which may have
contributed to their behaviour in this regard, with
the intention of highlighting attributes that may be
relevant for domain adaptation in general.
Informally, we believe that the main difference
between newspaper and biomedical text is vocabu-
lary, and that their syntactic structures are essentially
similar (with some isolated exceptions, such as more
frequent use of parentheses and comma-separated
</bodyText>
<page confidence="0.995481">
480
</page>
<table confidence="0.939221875">
Tag Errors Freq confused
Qus WDT 129 WP
VB 46 NN, VBP
NNP 33 JJ, NN
NN 32 JJ, NNP
Bio NN 801 JJ, CD
Table 4: JJ 268 NN, VBN
VBN 113 JJ, VBD
</table>
<note confidence="0.495732">
FW 95 NN, IN
Tags with the most frequent errors by the
</note>
<figureCaption confidence="0.665202">
newspaper-trained POS tagger and the tags they were
most frequently confused with.
</figureCaption>
<bodyText confidence="0.998450916666667">
lists in biomedical text). Once the POS tagger had
been retrained for biomedical text, accounting for
unfamiliar vocabulary, the original supertagger al-
ready performed well. The main difference between
newspaper and question data, on the other hand, is
syntactic. Retraining the POS tagger for questions
therefore had less effect; even with the correct POS
tags the supertagger was unable to assign the correct
lexical categories. Since lexical categories encode
syntactic information, the domain with the more di-
vergent syntax is likely to benefit most from new
training data at the lexical category level.
</bodyText>
<subsectionHeader confidence="0.995492">
5.1 POS tagger
</subsectionHeader>
<bodyText confidence="0.9997532">
Table 1 showed that the accuracy of the newspaper-
trained POS tagger was in the same range for both
biomedical and question data. However, the distri-
bution of errors was different. Table 4 shows the tags
with the most frequent errors, accounting for about
75% of all POS tag errors in each domain, and the
tags that they were most frequently confused with.
For the question data, the most frequent error was
tagging a wh-determiner (WDT) as a wh-pronoun
(WP). A determiner combines with a noun to form
a noun phrase, as in the sentence What Liverpool
club spawned the Beatles?. A pronoun, on the other
hand, is a noun phrase in its own right, as in What
are the colors of the German flag?. This tagger er-
ror arises from the fact that the word What occurs
only once in WSJ 02-21 with a WDT tag. The sec-
ond most common error was on bare verbs (VB), be-
cause the newspaper model gives a low probability
of bare verbs occurring in sentence-final position, or
not directly following an auxiliary.
</bodyText>
<table confidence="0.8556764">
Unknown word Unknown
rate word-POS rate
Sec. 00 3.8 4.4
Qus 7.5 8.3
Bio 23.6 25.3
</table>
<tableCaption confidence="0.991556">
Table 5: Unknown word rate and word-POS tag pair rate
(%) compared to WSJ 02-21 (by token).
</tableCaption>
<bodyText confidence="0.999957657894737">
For the biomedical data, the most frequent errors
by far were confusions of noun (NN) and adjective
(JJ). This is most likely due to the prevalence of long
noun phrases in the biomedical data, such as major
histocompatibility complex class II molecules. Al-
though the words preceding the head noun are rec-
ognized as nominal modifiers, the classification into
noun and adjective is difficult, especially when the
word is previously unseen. There were also prob-
lems distinguishing verbal past participles (VBN)
from adjectives (JJ) and identifying foreign words
(FW), for example the phrase in vitro.
The fact that the newspaper-trained POS tagger
performed comparably in the two target domains
(Table 1) is surprising, since their lexical profiles
are quite different. Lease and Charniak (2005) dis-
cussed unknown word rate as a predictor of POS
tagger accuracy. However, the unknown word rate
compared with WSJ 02-21 is much higher for the
biomedical data than for the question data, as seen
in Table 5. (The unknown word rate for the question
data is still higher than that for WSJ 00, which may
be due to the high proportion of proper nouns in the
question data.)
Some POS tagging errors can be attributed, not
to an unknown word, but to the use of a known
word with an unfamiliar tag (as in the WDT exam-
ple above). However, it is not the case that the ques-
tion data contains many known words with unknown
tags, since the rate of unknown word-tag pairs is also
much higher for biomedical than for question data,
as seen in the rightmost column of Table 5.
We do know that the newspaper-trained POS tag-
ger performs better on unknown words for biomedi-
cal (84.7%) than for question data (80.4%). We hy-
pothesize that the syntactic context of the biomed-
ical data, being more similar to newspaper data,
provides more information for the POS tagger in
</bodyText>
<page confidence="0.996038">
481
</page>
<table confidence="0.9927767">
WSJ 02-21 New train-
ing sets
3-grams
Sec. 00 0.4 0.7
Qus 3.6 0.5
Bio 0.7
5-grams
Sec. 00 12.1 7.4
Qus 22.0 9.2
Bio 10.9
</table>
<tableCaption confidence="0.8396625">
Table 6: Unknown POS n-gram rate (%) compared to WSJ
02-21, and when in-domain data is added (by token).
biomedical than in question data. Syntactic differ-
ences are discussed in the next section.
</tableCaption>
<subsectionHeader confidence="0.991941">
5.2 Supertagger
</subsectionHeader>
<bodyText confidence="0.9998981">
To quantify the syntactic distance between domains,
we propose using the unknown POS n-gram rate
compared to WSJ Sections 02-21. In the absence of
parse trees, POS n-grams can serve as a rough proxy
for the syntactic characteristics of a domain, reflect-
ing local word order configurations. POS n-grams
have been used in document modeling for text cate-
gorization (Baayen et al., 1996; Argamon-Engelson
et al., 1998), but we believe our proposed use of the
unknown POS n-gram rate is novel.
The leftmost column of Table 6 gives the un-
known POS trigram and 5-gram rates compared to
WSJ Sections 02-21. The rates for the biomedical
data are quite similar to those for Section 00. The
question data, however, shows higher rates of un-
known POS n-grams.
For both biomedical and question data, adding in-
domain data to the training set makes its syntactic
profile more like that of the evaluation set. The right-
most column of Table 6 shows the unknown POS n-
gram rates compared to the datasets used for training
the new supertagger models, consisting of WSJ 02-
21 plus annotated question or biomedical data. (For
the biomedical data, the figures are averages of the
same ten-fold split used for evaluation). It can be
seen that adding in-domain data reduces the rate of
unknown POS n-grams to about the same level ob-
served for newspaper text.
The unknown POS n-gram rate requires POS
tagged data for a new domain and thus cannot be
</bodyText>
<figure confidence="0.5787605">
3-grams 5-grams
18 19
8 5
16 13
</figure>
<tableCaption confidence="0.995285333333333">
Table 7: Number of the 20 most frequent POS n-grams
that are also in the 20 most frequent POS n-grams of WSJ
Sections 02-21.
</tableCaption>
<table confidence="0.9931114">
WSJ 02-21 Bio Qus
. — — JJ NN NN — — WP
IN DT NN IN JJ NN — WP VBZ
NN . — NN IN JJ — — WDT
DT JJ NN NNS IN NN WP VBZ DT
</table>
<tableCaption confidence="0.983701">
Table 8: Four most frequent POS trigrams for WSJ 02-
21; four most frequent POS trigrams for biomedical and
question data that are not in the 20 most frequent for WSJ
02-21. The dash represents the sentence boundary.
</tableCaption>
<bodyText confidence="0.999582423076923">
used with unlabelled data. However, since POS tag-
ging is relatively inexpensive, it might be possible to
use this rate as one measure of syntactic distance be-
tween a training corpus and a target domain, prior to
undertaking parser domain adaptation. The measure
does not capture all aspects of syntactic distance,
however. As pointed out by an anonymous reviewer,
if the syntactic tree structures are similar across do-
mains but lexical distributions are different – e.g. a
large number of words with unfamiliar categories in
the new domain – this measure will not be sensitive
to the difference.
Another useful measure for comparing domain
adaptation in the biomedical and question domains
is frequent POS n-grams. Table 7 shows how many
of the 20 most frequent POS n-grams in each dataset
overlap with the 20 most frequent POS n-grams in
WSJ 02-21. It can be seen that the overlap is the
highest for Section 00, but much lower for the ques-
tion data than for the biomedical data, again demon-
strating that the question data makes frequent use of
syntactic constructions which are rare in the PTB.
Table 8 shows the four most frequent POS tri-
grams in WSJ Sections 02-21,3 and the four most
frequent POS trigrams in the biomedical and ques-
tion data that are not among the 20 most frequent
</bodyText>
<footnote confidence="0.985163">
3Collapsing the NNP-NN distinction yields a slightly differ-
ent set.
</footnote>
<note confidence="0.825286333333333">
Sec. 00
Qus
Bio
</note>
<page confidence="0.98834">
482
</page>
<bodyText confidence="0.9999435">
for WSJ 02-21. The frequent question trigrams in-
clude two sentence-initial question words as well as
the pattern — WP VBZ, occurring in sentences be-
ginning with e.g. What is or Who is. Though not
among the top four, the pattern VB . —, represent-
ing a sentence-final bare verb, is also frequent. The
most frequent biomedical POS trigrams are not dra-
matically different from the newspaper trigrams, but
do appear to reflect the prevalence of NPs and PPs
in the data.
One final measure of syntactic distance is the
frequency with which CCG lexical categories that
are rare or unseen in CCGbank are used in a do-
main. It is typical to use a few such categories,
even for in-domain data, for unusual syntactic con-
structions, but each one is usually used only a hand-
ful of times. The question data is unique in the
frequency with which previously rare or unseen
categories are required. For example, the unseen
category (S[wq]/S[q])/N, representing the word
What in a question such as What day did Nintendo
64 come out? is used 11 times in the evaluation
set; the rare category (S[wq]/(S[dcl]\NP))/N,
used in subject questions like Which river runs
through Dublin?, is used 61 times; and the rare cat-
egory (S[q]/(S[pss]\NP))/NP, representing pas-
sive verbs in sentences like What is Jane Goodall
known for?, is used 59 times.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997995">
We have targeted lower levels of representation in
order to adapt a lexicalized-grammar parser to two
new domains, biomedical text and questions. Al-
though each of the lower levels has been targeted in-
dependently in previous work, this is the first study
that examines both levels together to determine how
they affect parsing accuracy. We achieved an accu-
racy on grammatical relations in the same range as
that of the original parser for newspaper text, with-
out requiring costly annotation of full parse trees.
Both biomedical and question data are domains in
which there is an immediate need for accurate pars-
ing. The question dataset is in some ways an ex-
treme example for domain adaptation, since the sen-
tences are syntactically uniform; on the other hand,
it is of interest as a set of constructions where the
parser initially performed poorly, and is a realistic
parsing challenge in the context of QA systems.
Interestingly, although an increase in accuracy at
each stage of the pipeline did yield an increase at
the following stage, these increases were not uni-
form across the two domains. The new POS tagger
model was responsible for most of the improvement
in parsing for the biomedical domain, while the new
supertagger model was necessary to see a large im-
provement in the question domain. We attribute this
to the fact that question syntax is significantly differ-
ent from newspaper syntax. We expect these consid-
erations to apply to any lexicalized-grammar parser.
Of course, it would be useful to have a way of
predicting which level of annotation would be most
effective for adapting to a new domain before the an-
notation begins. The utility of measures such as un-
known word rate (which can be performed with un-
labelled data) and unknown POS n-gram rate (which
can be performed with only POS tags) is not yet suffi-
ciently clear to rely on them as predictive measures,
but it seems a fruitful avenue for future work to in-
vestigate the importance of such measures for parser
domain adaptation.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999846">
We would like to thank Marie-Catherine de Marn-
effe for advice on the use of the Stanford GR for-
mat, Sampo Pyysalo for sharing information about
the BioInfer corpus, and Mark Steedman for advice
on encoding question data in CCG. We would also
like to thank three anonymous reviewers for their
suggestions. This work was supported by EPSRC
grant EP/E035698/1: Accurate and Efficient Parsing
of Biomedical Text.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996651916666667">
Shlomo Argamon-Engelson, Moshe Koppel, and Galit
Avneri. 1998. Style-based text categorization: What
newspaper am I reading? In Proceedings of AAAI
Workshop on Learning for Text Categorization, pages
1–4.
Harald Baayen, Hans Van Halteren, and Fiona Tweedie.
1996. Outside the cave of shadows: Using syntactic
annotation to enhance authorship attribution. Literary
and Linguistic Computing, 11(3):121–131.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237–265.
</reference>
<page confidence="0.990953">
483
</page>
<reference confidence="0.999788979381444">
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the Interactive Demo Session of the Joint Con-
ference of the International Committee on Computa-
tional Linguistics and the Association for Computa-
tional Linguistics (COLING/ACL-06), Sydney, Aus-
trailia.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the
NAACL, pages 132–139, Seattle, WA.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Meeting of the ACL,
pages 248–255, Prague, Czech Republic.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
Stephen Clark, Mark Steedman, and James R. Curran.
2004. Object-extraction and question-parsing using
CCG. In Proceedings of the EMNLP Conference,
pages 111–118, Barcelona, Spain.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Meeting of the ACL, pages 16–23, Madrid, Spain.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proceedings of the Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06), pages 697–704, Sydney, Aus-
trailia.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th LREC Conference, pages 449–454,
Genoa, Italy.
Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an HPSG
parser. In Proceedings of IWPT, pages 11–22, Prague,
Czech Republic.
Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana
Girju, Vasile Rus, and Paul Morarescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Meeting of the ACL, pages 274–281, Toulose, France.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355–396.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proceedings of the Joint Conference of
the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics, pages 497–504, Sydney, Australia.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun’ichi
Tsujii. 2003. GENIA corpus – a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19:i180–i182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 Dependency Bank. In Proceedings of the 4th
International Workshop on Linguistically Interpreted
Corpora, Budapest, Hungary.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In Proceedings of the Second
International Joint Conference on Natural Language
Processing (IJCNLP-05), Jeju Island, Korea.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proceedings of the 43rd meeting of the ACL,
pages 83–90, University of Michigan, Ann Arbor.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Ex-
tremely lexicalized models for accurate and fast HPSG
parsing. In Proceedings of the EMNLP Conference.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio
Salakoski. 2007a. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8:50.
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007b. On the unification of syntactic annotations un-
der the stanford dependency scheme: A case study on
BioInfer and GENIA. In ACL’07 workshop on Biolog-
ical, translational, and clinical language processing,
pages 25–32, Prague, Czech Republic.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
</reference>
<page confidence="0.998962">
484
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553500">
<title confidence="0.999667">Adapting a Lexicalized-Grammar Parser to Contrasting Domains</title>
<author confidence="0.8354">Rimell</author>
<affiliation confidence="0.998973">Oxford University Computing</affiliation>
<address confidence="0.805795">Wolfson Building, Parks Oxford, OX1 3QD,</address>
<abstract confidence="0.996751736842105">Most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains, making parser adaptation a pressing issue. In this we demonstrate that a can be adapted to two new domains, biomedical and questions for a by using manually-annotated training data at the lexical category levels only. This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain. We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon-Engelson</author>
<author>Moshe Koppel</author>
<author>Galit Avneri</author>
</authors>
<title>Style-based text categorization: What newspaper am I reading?</title>
<date>1998</date>
<booktitle>In Proceedings of AAAI Workshop on Learning for Text Categorization,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="29347" citStr="Argamon-Engelson et al., 1998" startWordPosition="4855" endWordPosition="4858">0.9 Table 6: Unknown POS n-gram rate (%) compared to WSJ 02-21, and when in-domain data is added (by token). biomedical than in question data. Syntactic differences are discussed in the next section. 5.2 Supertagger To quantify the syntactic distance between domains, we propose using the unknown POS n-gram rate compared to WSJ Sections 02-21. In the absence of parse trees, POS n-grams can serve as a rough proxy for the syntactic characteristics of a domain, reflecting local word order configurations. POS n-grams have been used in document modeling for text categorization (Baayen et al., 1996; Argamon-Engelson et al., 1998), but we believe our proposed use of the unknown POS n-gram rate is novel. The leftmost column of Table 6 gives the unknown POS trigram and 5-gram rates compared to WSJ Sections 02-21. The rates for the biomedical data are quite similar to those for Section 00. The question data, however, shows higher rates of unknown POS n-grams. For both biomedical and question data, adding indomain data to the training set makes its syntactic profile more like that of the evaluation set. The rightmost column of Table 6 shows the unknown POS ngram rates compared to the datasets used for training the new supe</context>
</contexts>
<marker>Argamon-Engelson, Koppel, Avneri, 1998</marker>
<rawString>Shlomo Argamon-Engelson, Moshe Koppel, and Galit Avneri. 1998. Style-based text categorization: What newspaper am I reading? In Proceedings of AAAI Workshop on Learning for Text Categorization, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Hans Van Halteren</author>
<author>Fiona Tweedie</author>
</authors>
<title>Outside the cave of shadows: Using syntactic annotation to enhance authorship attribution.</title>
<date>1996</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<volume>11</volume>
<issue>3</issue>
<marker>Baayen, Van Halteren, Tweedie, 1996</marker>
<rawString>Harald Baayen, Hans Van Halteren, and Fiona Tweedie. 1996. Outside the cave of shadows: Using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing, 11(3):121–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="6158" citStr="Bangalore and Joshi, 1999" startWordPosition="986" endWordPosition="990">r assigns a single POS tag to each word in a sentence. POS tags are fairly coarse-grained grammatical labels indicating part-of-speech; the Penn Treebank set, used here, contains approximately 50 labels. Second, a maximum entropy supertagger assigns CCG lexical categories to the words in the sentence. Lexical categories can be thought of as fine-grained POS tags expressing subcategorization information, i.e. information about the argument frame of the word. There are 425 categories in the set used by the CCG parser. Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). Rather than assign a single category to each word, the supertagger operates as a multitagger, sometimes assigning more than one category if the context is not sufficiently discriminating to suggest a single tag (Curran et al., 2006). Since the taggers have linear time complexity, the first two stages can be performed extremely quickly. Finally, the parsing stage combines the lexical categories, using a small set of combinatory rules that are part of the grammar of CCG, and builds a packed chart repr</context>
<context position="7802" citStr="Bangalore and Joshi (1999)" startWordPosition="1255" endWordPosition="1258">the pipeline which is most relevant to this paper is the supertagging phase. Figure 1 gives an example sentence from each target domain, with the CCG lexical category assigned to each word shown below the word, and the POS tag to the right. Note that the categories contain a significant amount of grammatical information, in particular subcategorization information. The verb acts in the biomedical sentence, for example, looks for a prepositional phrase (PP, as a linkage protein) to its right and a noun phrase (NP, Talin) to its left, with the resulting category a declarative sentence (S[dcl]). Bangalore and Joshi (1999) refer to supertagging as almost parsing, because once the correct lexical categories have been assigned, the parser is left with much less work to do. The CCG supertagger is not able to assign a single category to each word with extremely high accuracy — hence the need for it to operate as a multi-tagger — but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). 476 Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|. NP (S\NP)/(S\NP) (S[dcl]\NP)/PP PP/NP NP[nb]/N N/N N . What|WDT king|NN signed|VBD the|DT Magna|N</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Interactive Demo Session of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<location>Sydney, Austrailia.</location>
<contexts>
<context position="8643" citStr="Briscoe et al. (2006)" startWordPosition="1388" endWordPosition="1391">word with extremely high accuracy — hence the need for it to operate as a multi-tagger — but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). 476 Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|. NP (S\NP)/(S\NP) (S[dcl]\NP)/PP PP/NP NP[nb]/N N/N N . What|WDT king|NN signed|VBD the|DT Magna|NNP Carta|NNP ?|. (S[wq]/(S[dcl]\NP))/N N (S[dcl]\NP)/NP NP[nb]/N N/N N . Figure 1: Example sentences with lexical category assignment. The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). Section 4.4 describes how the CCG dependencies can be mapped into the Stanford GR scheme (de Marneffe et al., 2006) and gives the results of evaluating the parser on biomedical and question GR resources. The CCG parser is particularly well suited to the biomedical and question domains. First, use of CCG allows recovery of long-distance dependencies. In the sentence What does target heart rate mean?, the word What is an underlying object of the verb mean. The parser recovers this information de</context>
<context position="21616" citStr="Briscoe et al. (2006)" startWordPosition="3530" endWordPosition="3533">stion data started from a much lower baseline figure, however. 4.4 Parser We evaluated the parser on the 500 questions annotated with Stanford GRs and on the 500 evaluation sentences from the BioInfer corpus. We used the original newspaper pipeline, a pipeline with a retrained POS tagger, and a pipeline with both a retrained POS tagger and supertagger. In order to perform these evaluations we develooped a mapping from the parser’s native CCG syntactic dependencies to GRs in the Stanford format. The mapping was based on the same principles as the mapping that produces GR output in the style of Briscoe et al. (2006). These principles are discussed in detail in Clark and Curran (2007a); in summary, the argument slots in the CCG dependencies are mapped to argument slots in Stanford GRs, a fairly complex, many-to-many mapping. An additional post-processing script applies some manually developed rules to bring the output closer to the Stanford format. Figure 2 gives an example of Stanford GRs, where the label of the relation is followed by two arguments, head and dependent. Table 3 gives the results of the parser evaluation on GRs. Since the parser model was not retrained, the improvements in accuracy are du</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the Interactive Demo Session of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06), Sydney, Austrailia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="12378" citStr="Charniak (2000)" startWordPosition="2002" endWordPosition="2003">n is quick, especially in a domain without much unfamiliar vocabulary. CCG lexical categories require more expertise, but our experience shows that an out-of-domain supertagger can again provide a starting point for correction, and since the annotation is flat rather than hierarchical, we hypothesize that it is not as difficult or timeconsuming as annotation of full derivations. Our adaptation approach has been partially explored in previous work which targets one or another of the different levels of representation. 477 Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser, as well as POS tagging accuracy, when applied to the biomedical domain, by training a new POS tagger model with a combination of newspaper and biomedical data. The parser improvement was due solely to the new POS tagger, without retraining the parser model. Since the Charniak parser does not use a lexicalized grammar with an intermediate level of representation, any further improvements would have to come from the parser model itself. Clark et al. (2004) obtained an improvement in CCG supertagging accuracy for What-questions by training a new supertagger model with a combination of ne</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the NAACL, pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalismindependent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the ACL,</booktitle>
<pages>248--255</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3142" citStr="Clark and Curran, 2007" startWordPosition="503" endWordPosition="506">rather than a set of documents about a particular topic. However, we consider question data to be interesting in the context of domain adaptation for the following reasons: 1) there are few examples in the Penn Treebank (PTB) and so PTB parsers typically perform poorly on them; 2) questions form a fairly homogeneous set with respect to the syntactic constructions employed, and it is an interesting question how easy it is to adapt a parser to such data; and 3) QA is becoming an important example of NLP technology, and question parsing is an important task for QA systems. The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG su475 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475–484, Honolulu, October 2008.c�2008 Association for Computational Linguistics pertagger; and three, a hierarchical level consisting of CCG derivations. A key idea in this paper, following a pilot study in Clark et al. (2004), is to perform </context>
<context position="5396" citStr="Clark and Curran (2007" startWordPosition="868" endWordPosition="871">ion used by the parser. We find that simply retraining the POS tagger used by the parser leads to a large improvement in performance for the biomedical domain, and that retraining the CCG supertagger on the annotated biomedical data improves the performance further. For the question data, retraining just the POS tagger also improves parser performance, but retraining the supertagger has a much greater effect. We perform some analysis of the two datasets in order to explain the different behaviours with regard to porting the CCG parser. 2 The CCG Parser The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. The stages in the CCG parsing pipeline are as follows. First, a maximum entropy POS tagger assigns a single POS tag to each word in a sentence. POS tags are fairly coarse-grained grammatical labels indicating part-of-speech; the Penn Treebank set, used here, contains approximately 50 labels. Second, a maximum entropy supertagger assigns CCG lexical categories to the words in the sentence. Lexical categories can be thought of as fine-grained POS tags expressing subcategorization information, i.e. information about the argument frame of the word. Th</context>
<context position="8231" citStr="Clark and Curran, 2007" startWordPosition="1329" endWordPosition="1332">a prepositional phrase (PP, as a linkage protein) to its right and a noun phrase (NP, Talin) to its left, with the resulting category a declarative sentence (S[dcl]). Bangalore and Joshi (1999) refer to supertagging as almost parsing, because once the correct lexical categories have been assigned, the parser is left with much less work to do. The CCG supertagger is not able to assign a single category to each word with extremely high accuracy — hence the need for it to operate as a multi-tagger — but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). 476 Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|. NP (S\NP)/(S\NP) (S[dcl]\NP)/PP PP/NP NP[nb]/N N/N N . What|WDT king|NN signed|VBD the|DT Magna|NNP Carta|NNP ?|. (S[wq]/(S[dcl]\NP))/N N (S[dcl]\NP)/NP NP[nb]/N N/N N . Figure 1: Example sentences with lexical category assignment. The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). Section 4.4 describes how the CCG dependencies can be mapped into the Stanford GR sche</context>
<context position="21684" citStr="Clark and Curran (2007" startWordPosition="3542" endWordPosition="3545">Parser We evaluated the parser on the 500 questions annotated with Stanford GRs and on the 500 evaluation sentences from the BioInfer corpus. We used the original newspaper pipeline, a pipeline with a retrained POS tagger, and a pipeline with both a retrained POS tagger and supertagger. In order to perform these evaluations we develooped a mapping from the parser’s native CCG syntactic dependencies to GRs in the Stanford format. The mapping was based on the same principles as the mapping that produces GR output in the style of Briscoe et al. (2006). These principles are discussed in detail in Clark and Curran (2007a); in summary, the argument slots in the CCG dependencies are mapped to argument slots in Stanford GRs, a fairly complex, many-to-many mapping. An additional post-processing script applies some manually developed rules to bring the output closer to the Stanford format. Figure 2 gives an example of Stanford GRs, where the label of the relation is followed by two arguments, head and dependent. Table 3 gives the results of the parser evaluation on GRs. Since the parser model was not retrained, the improvements in accuracy are due solely to the new POS and supertaggers. The results are given as a</context>
<context position="23331" citStr="Clark and Curran, 2007" startWordPosition="3825" endWordPosition="3828">4.4 69.4 86.6 BioInfer 76.0 80.4 81.5 Table 3: Parser F-score on grammatical relations and the effect of retraining the POS and supertagger models. erage was 94% for the original pipeline and the pipeline with just the retrained POS tagger, and 99.6% with the retrained POS and supertaggers. For the biomedical data, coverage was 97.2% for the original pipeline, 99.0% for the pipeline with the retrained POS tagger, and 99.8% for the pipeline with the retrained POS and supertaggers. The final accuracy for both domains is in the same range as that of the original parser on newspaper data (81.8%) (Clark and Curran, 2007b), although the results are not directly comparable, since the newspaper resource uses a different GR scheme. For the BioInfer corpus, the final accuracy is also in line with results reported in the literature for other parsers (Pyysalo et al., 2007b). (No comparable GR results are available for questions.) A score in this range is thought to be near the upper bound when evaluating a CCG parser on GRs, since some loss is inherent in the mapping to GRs (Clark and Curran, 2007a). 5 Analysis Although domain adaptation was successful for both of our target domains, the impact of the different lev</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007a. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Meeting of the ACL, pages 248–255, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="3142" citStr="Clark and Curran, 2007" startWordPosition="503" endWordPosition="506">rather than a set of documents about a particular topic. However, we consider question data to be interesting in the context of domain adaptation for the following reasons: 1) there are few examples in the Penn Treebank (PTB) and so PTB parsers typically perform poorly on them; 2) questions form a fairly homogeneous set with respect to the syntactic constructions employed, and it is an interesting question how easy it is to adapt a parser to such data; and 3) QA is becoming an important example of NLP technology, and question parsing is an important task for QA systems. The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG su475 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475–484, Honolulu, October 2008.c�2008 Association for Computational Linguistics pertagger; and three, a hierarchical level consisting of CCG derivations. A key idea in this paper, following a pilot study in Clark et al. (2004), is to perform </context>
<context position="5396" citStr="Clark and Curran (2007" startWordPosition="868" endWordPosition="871">ion used by the parser. We find that simply retraining the POS tagger used by the parser leads to a large improvement in performance for the biomedical domain, and that retraining the CCG supertagger on the annotated biomedical data improves the performance further. For the question data, retraining just the POS tagger also improves parser performance, but retraining the supertagger has a much greater effect. We perform some analysis of the two datasets in order to explain the different behaviours with regard to porting the CCG parser. 2 The CCG Parser The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. The stages in the CCG parsing pipeline are as follows. First, a maximum entropy POS tagger assigns a single POS tag to each word in a sentence. POS tags are fairly coarse-grained grammatical labels indicating part-of-speech; the Penn Treebank set, used here, contains approximately 50 labels. Second, a maximum entropy supertagger assigns CCG lexical categories to the words in the sentence. Lexical categories can be thought of as fine-grained POS tags expressing subcategorization information, i.e. information about the argument frame of the word. Th</context>
<context position="8231" citStr="Clark and Curran, 2007" startWordPosition="1329" endWordPosition="1332">a prepositional phrase (PP, as a linkage protein) to its right and a noun phrase (NP, Talin) to its left, with the resulting category a declarative sentence (S[dcl]). Bangalore and Joshi (1999) refer to supertagging as almost parsing, because once the correct lexical categories have been assigned, the parser is left with much less work to do. The CCG supertagger is not able to assign a single category to each word with extremely high accuracy — hence the need for it to operate as a multi-tagger — but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). 476 Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|. NP (S\NP)/(S\NP) (S[dcl]\NP)/PP PP/NP NP[nb]/N N/N N . What|WDT king|NN signed|VBD the|DT Magna|NNP Carta|NNP ?|. (S[wq]/(S[dcl]\NP))/N N (S[dcl]\NP)/NP NP[nb]/N N/N N . Figure 1: Example sentences with lexical category assignment. The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). Section 4.4 describes how the CCG dependencies can be mapped into the Stanford GR sche</context>
<context position="21684" citStr="Clark and Curran (2007" startWordPosition="3542" endWordPosition="3545">Parser We evaluated the parser on the 500 questions annotated with Stanford GRs and on the 500 evaluation sentences from the BioInfer corpus. We used the original newspaper pipeline, a pipeline with a retrained POS tagger, and a pipeline with both a retrained POS tagger and supertagger. In order to perform these evaluations we develooped a mapping from the parser’s native CCG syntactic dependencies to GRs in the Stanford format. The mapping was based on the same principles as the mapping that produces GR output in the style of Briscoe et al. (2006). These principles are discussed in detail in Clark and Curran (2007a); in summary, the argument slots in the CCG dependencies are mapped to argument slots in Stanford GRs, a fairly complex, many-to-many mapping. An additional post-processing script applies some manually developed rules to bring the output closer to the Stanford format. Figure 2 gives an example of Stanford GRs, where the label of the relation is followed by two arguments, head and dependent. Table 3 gives the results of the parser evaluation on GRs. Since the parser model was not retrained, the improvements in accuracy are due solely to the new POS and supertaggers. The results are given as a</context>
<context position="23331" citStr="Clark and Curran, 2007" startWordPosition="3825" endWordPosition="3828">4.4 69.4 86.6 BioInfer 76.0 80.4 81.5 Table 3: Parser F-score on grammatical relations and the effect of retraining the POS and supertagger models. erage was 94% for the original pipeline and the pipeline with just the retrained POS tagger, and 99.6% with the retrained POS and supertaggers. For the biomedical data, coverage was 97.2% for the original pipeline, 99.0% for the pipeline with the retrained POS tagger, and 99.8% for the pipeline with the retrained POS and supertaggers. The final accuracy for both domains is in the same range as that of the original parser on newspaper data (81.8%) (Clark and Curran, 2007b), although the results are not directly comparable, since the newspaper resource uses a different GR scheme. For the BioInfer corpus, the final accuracy is also in line with results reported in the literature for other parsers (Pyysalo et al., 2007b). (No comparable GR results are available for questions.) A score in this range is thought to be near the upper bound when evaluating a CCG parser on GRs, since some loss is inherent in the mapping to GRs (Clark and Curran, 2007a). 5 Analysis Although domain adaptation was successful for both of our target domains, the impact of the different lev</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007b. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
</authors>
<title>Object-extraction and question-parsing using CCG.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="3726" citStr="Clark et al. (2004)" startWordPosition="597" endWordPosition="600">rser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG su475 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475–484, Honolulu, October 2008.c�2008 Association for Computational Linguistics pertagger; and three, a hierarchical level consisting of CCG derivations. A key idea in this paper, following a pilot study in Clark et al. (2004), is to perform manual annotation only at the first two levels. Since the lexical category level consists of sequences of tags, rather than hierarchical derivations, the annotation can be performed relatively quickly. For the biomedical and question domains we manually annotated approximately 1,000 and 2,000 sentences, respectively, with CCG lexical categories. We also created a gold standard set of grammatical relations (GR) in the Stanford format (de Marneffe et al., 2006), using 500 of the questions. For the biomedical domain we used the BioInfer corpus (Pyysalo et al., 2007a), an existing </context>
<context position="12845" citStr="Clark et al. (2004)" startWordPosition="2077" endWordPosition="2080">s one or another of the different levels of representation. 477 Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser, as well as POS tagging accuracy, when applied to the biomedical domain, by training a new POS tagger model with a combination of newspaper and biomedical data. The parser improvement was due solely to the new POS tagger, without retraining the parser model. Since the Charniak parser does not use a lexicalized grammar with an intermediate level of representation, any further improvements would have to come from the parser model itself. Clark et al. (2004) obtained an improvement in CCG supertagging accuracy for What-questions by training a new supertagger model with a combination of newspaper and question data annotated with CCG lexical categories. Because a question resource annotated with GRs was not available, they did not perform a parser evaluation, and the effects of the POS tagging level were not compared to the lexical category level. In this paper, we extend the pilot experiments performed by Clark et al. (2004) in four ways. First, we use a larger corpus of TREC questions covering additional question types, thus extending the experim</context>
<context position="14107" citStr="Clark et al. (2004)" startWordPosition="2286" endWordPosition="2289">s well as to the biomedical domain. Second, we create a gold standard GR resource enabling a full parser evaluation on question data. Third, we show that the POS level is important for adaptation, reinforcing the work of Lease and Charniak (2005). A key finding of the present paper is that the combination of retraining at the POS tag and lexical category levels provides additional improvements beyond those gained by retraining at a single level. Finally, we provide analysis comparing the adaptation methodology for question and biomedical data. Hara et al. (2007) followed a similar approach to Clark et al. (2004), using the parser of Ninomiya et al. (2006), a version of the Enju parser (Miyao and Tsujii, 2005). Enju is based on HPSG, a lexicalized grammar formalism. They obtained an improvement in parsing accuracy in the biomedical domain by training a new probabilistic model of lexical entry assignments on a combination of newspaper and biomedical data without changing the original newspaper-trained parsing model. Hara et al. (2007) did not consider the role of POS tagging. The lexical category data in Hara et al. (2007) was derived from a gold standard treebank, while the annotation of lexical categ</context>
<context position="16617" citStr="Clark et al. (2004)" startWordPosition="2696" endWordPosition="2699">tagger model. For parser evaluation, we used BioInfer (Pyysalo et al., 2007a), a corpus of MEDLINE abstracts (on a different topic from those in GENIA) containing 1,100 sentences, and with syntactic dependencies encoded as grammatical relations in the Stanford GR format. We used the same evaluation set of 500 sentences as in Pyysalo et al. (2007b), and the remaining 600 for development of the mapping to Stanford format. Two parsers have already been evaluated on BioInfer, which makes it a useful resource for comparative evaluation. For the question domain, we extended the dataset described in Clark et al. (2004). That dataset contained 1,171 questions beginning with the word What, from the TREC 9-12 competitions (2000- 2003), manually POS tagged and annotated with 478 CCG lexical categories. We annotated all the additional TREC question types and improved the existing annotation, for a total of 1,828 sentences. We additionally annotated a random subset of 500 of these with GRs in the Stanford format. This subset served as our evaluation set at all levels of representation. It contains approximately 4,000 words, fewer than the other domains because of the significantly shorter sentence lengths of typi</context>
</contexts>
<marker>Clark, Steedman, Curran, 2004</marker>
<rawString>Stephen Clark, Mark Steedman, and James R. Curran. 2004. Object-extraction and question-parsing using CCG. In Proceedings of the EMNLP Conference, pages 111–118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="15002" citStr="Collins, 1997" startWordPosition="2437" endWordPosition="2438">ntry assignments on a combination of newspaper and biomedical data without changing the original newspaper-trained parsing model. Hara et al. (2007) did not consider the role of POS tagging. The lexical category data in Hara et al. (2007) was derived from a gold standard treebank, while the annotation of lexical categories in this paper was performed without reference to gold standard syntactic derivations. Judge et al. (2006) produced a corpus of 4,000 questions annotated with syntactic trees, and obtained an improvement in parsing accuracy for Bikel’s reimplementation of the Collins parser (Collins, 1997) by training a new parser model with a combination of newspaper and question data. Our approach differs in retraining only at the levels of representation below parse trees. 4 Experiments and Results 4.1 Resources We have used a combination of existing resources and new, manually annotated data. The baseline POS tagger, supertagger, and parser are trained on WSJ Sections 02-21 of CCGbank. The baseline performance at each level of representation is on WSJ Section 00 of CCGbank, which contains 1913 sentences and approximately 45,000 words. For the biomedical domain, we trained the POS tagger on </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Meeting of the ACL, pages 16–23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>697--704</pages>
<location>Sydney, Austrailia.</location>
<contexts>
<context position="6486" citStr="Curran et al., 2006" startWordPosition="1039" endWordPosition="1042">ht of as fine-grained POS tags expressing subcategorization information, i.e. information about the argument frame of the word. There are 425 categories in the set used by the CCG parser. Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). Rather than assign a single category to each word, the supertagger operates as a multitagger, sometimes assigning more than one category if the context is not sufficiently discriminating to suggest a single tag (Curran et al., 2006). Since the taggers have linear time complexity, the first two stages can be performed extremely quickly. Finally, the parsing stage combines the lexical categories, using a small set of combinatory rules that are part of the grammar of CCG, and builds a packed chart representation containing all the derivations which can be built from the lexical categories. The Viterbi algorithm efficiently finds the highest scoring derivation from the packed chart, using a loglinear model to score the derivations. The grammar and training data for the newspaper version of the CCG parser are obtained from CC</context>
</contexts>
<marker>Curran, Clark, Vadas, 2006</marker>
<rawString>James R. Curran, Stephen Clark, and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06), pages 697–704, Sydney, Austrailia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th LREC Conference,</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th LREC Conference, pages 449–454, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadayoshi Hara</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an HPSG parser.</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>11--22</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="14056" citStr="Hara et al. (2007)" startWordPosition="2277" endWordPosition="2280">experiments to the question domain more broadly, as well as to the biomedical domain. Second, we create a gold standard GR resource enabling a full parser evaluation on question data. Third, we show that the POS level is important for adaptation, reinforcing the work of Lease and Charniak (2005). A key finding of the present paper is that the combination of retraining at the POS tag and lexical category levels provides additional improvements beyond those gained by retraining at a single level. Finally, we provide analysis comparing the adaptation methodology for question and biomedical data. Hara et al. (2007) followed a similar approach to Clark et al. (2004), using the parser of Ninomiya et al. (2006), a version of the Enju parser (Miyao and Tsujii, 2005). Enju is based on HPSG, a lexicalized grammar formalism. They obtained an improvement in parsing accuracy in the biomedical domain by training a new probabilistic model of lexical entry assignments on a combination of newspaper and biomedical data without changing the original newspaper-trained parsing model. Hara et al. (2007) did not consider the role of POS tagging. The lexical category data in Hara et al. (2007) was derived from a gold stand</context>
</contexts>
<marker>Hara, Miyao, Tsujii, 2007</marker>
<rawString>Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an HPSG parser. In Proceedings of IWPT, pages 11–22, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>Razvan Bunescu</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
<author>Paul Morarescu</author>
</authors>
<title>The role of lexico-semantic feedback in open-domain textual question-answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Meeting of the ACL,</booktitle>
<pages>274--281</pages>
<location>Toulose, France.</location>
<contexts>
<context position="1591" citStr="Harabagiu et al., 2001" startWordPosition="238" endWordPosition="241">ently for parser adaptation. 1 Introduction Most state-of-the-art wide-coverage parsers are based on the Penn Treebank (Marcus et al., 1993), making such parsers highly tuned to newspaper text. A pressing question facing the parsing community is how to adapt these parsers to other domains, such as biomedical research papers and web pages. A related question is how to improve the performance of these parsers on constructions that are rare in the Penn Treebank, such as questions. Questions are particularly important since a question parser is a component in most Question Answering (QA) systems (Harabagiu et al., 2001). In this paper we investigate parser adaptation in the context of lexicalized grammars, by using a parser based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). A key property of CCG is that it is lexicalized, meaning that each word in a sentence is associated with an elementary syntactic structure. In the case of CCG this is a lexical category expressing subcategorization information. We exploit this property of CCG by performing manual annotation in the new domain, but only up to this level of representation, where the annotation can be carried out relatively quickly. Since CCG lex</context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Morarescu, 2001</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana Girju, Vasile Rus, and Paul Morarescu. 2001. The role of lexico-semantic feedback in open-domain textual question-answering. In Proceedings of the 39th Meeting of the ACL, pages 274–281, Toulose, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="7124" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1141" endWordPosition="1145"> the taggers have linear time complexity, the first two stages can be performed extremely quickly. Finally, the parsing stage combines the lexical categories, using a small set of combinatory rules that are part of the grammar of CCG, and builds a packed chart representation containing all the derivations which can be built from the lexical categories. The Viterbi algorithm efficiently finds the highest scoring derivation from the packed chart, using a loglinear model to score the derivations. The grammar and training data for the newspaper version of the CCG parser are obtained from CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. The aspect of the pipeline which is most relevant to this paper is the supertagging phase. Figure 1 gives an example sentence from each target domain, with the CCG lexical category assigned to each word shown below the word, and the POS tag to the right. Note that the categories contain a significant amount of grammatical information, in particular subcategorization information. The verb acts in the biomedical sentence, for example, looks for a prepositional phrase (PP, as a linkage protein) to its right and a noun phrase (NP, Talin) to its left, with the </context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Judge</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Questionbank: Creating a corpus of parse-annotated questions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics,</booktitle>
<pages>497--504</pages>
<location>Sydney, Australia.</location>
<marker>Judge, Cahill, van Genabith, 2006</marker>
<rawString>John Judge, Aoife Cahill, and Josef van Genabith. 2006. Questionbank: Creating a corpus of parse-annotated questions. In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics, pages 497–504, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Teteisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA corpus – a semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<contexts>
<context position="15654" citStr="Kim et al., 2003" startWordPosition="2541" endWordPosition="2544">h a combination of newspaper and question data. Our approach differs in retraining only at the levels of representation below parse trees. 4 Experiments and Results 4.1 Resources We have used a combination of existing resources and new, manually annotated data. The baseline POS tagger, supertagger, and parser are trained on WSJ Sections 02-21 of CCGbank. The baseline performance at each level of representation is on WSJ Section 00 of CCGbank, which contains 1913 sentences and approximately 45,000 words. For the biomedical domain, we trained the POS tagger on gold-standard POS tags from GENIA (Kim et al., 2003), a corpus of 2,000 MEDLINE abstracts containing a total of approximately 18,500 sentences and 440,000 words. We also annotated the first 1,000 sentences of GENIA with CCG lexical categories. This set of 1,000 sentences, containing approximately 27,000 words, was used for POS tagger evaluation and for development and evaluation of a new supertagger model. For parser evaluation, we used BioInfer (Pyysalo et al., 2007a), a corpus of MEDLINE abstracts (on a different topic from those in GENIA) containing 1,100 sentences, and with syntactic dependencies encoded as grammatical relations in the Stan</context>
</contexts>
<marker>Kim, Ohta, Teteisi, Tsujii, 2003</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun’ichi Tsujii. 2003. GENIA corpus – a semantically annotated corpus for bio-textmining. Bioinformatics, 19:i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The PARC 700 Dependency Bank.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="8597" citStr="King et al., 2003" startWordPosition="1379" endWordPosition="1382">t able to assign a single category to each word with extremely high accuracy — hence the need for it to operate as a multi-tagger — but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). 476 Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|. NP (S\NP)/(S\NP) (S[dcl]\NP)/PP PP/NP NP[nb]/N N/N N . What|WDT king|NN signed|VBD the|DT Magna|NNP Carta|NNP ?|. (S[wq]/(S[dcl]\NP))/N N (S[dcl]\NP)/NP NP[nb]/N N/N N . Figure 1: Example sentences with lexical category assignment. The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). Section 4.4 describes how the CCG dependencies can be mapped into the Stanford GR scheme (de Marneffe et al., 2006) and gives the results of evaluating the parser on biomedical and question GR resources. The CCG parser is particularly well suited to the biomedical and question domains. First, use of CCG allows recovery of long-distance dependencies. In the sentence What does target heart rate mean?, the word What is an underlying object of the verb</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700 Dependency Bank. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
</authors>
<title>Parsing biomedical literature.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Joint Conference on Natural Language Processing (IJCNLP-05),</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="12315" citStr="Lease and Charniak (2005)" startWordPosition="1989" endWordPosition="1992">of-domain tagger will provide a good starting point, and manual correction is quick, especially in a domain without much unfamiliar vocabulary. CCG lexical categories require more expertise, but our experience shows that an out-of-domain supertagger can again provide a starting point for correction, and since the annotation is flat rather than hierarchical, we hypothesize that it is not as difficult or timeconsuming as annotation of full derivations. Our adaptation approach has been partially explored in previous work which targets one or another of the different levels of representation. 477 Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser, as well as POS tagging accuracy, when applied to the biomedical domain, by training a new POS tagger model with a combination of newspaper and biomedical data. The parser improvement was due solely to the new POS tagger, without retraining the parser model. Since the Charniak parser does not use a lexicalized grammar with an intermediate level of representation, any further improvements would have to come from the parser model itself. Clark et al. (2004) obtained an improvement in CCG supertagging accuracy for What-questio</context>
<context position="13734" citStr="Lease and Charniak (2005)" startWordPosition="2224" endWordPosition="2228">they did not perform a parser evaluation, and the effects of the POS tagging level were not compared to the lexical category level. In this paper, we extend the pilot experiments performed by Clark et al. (2004) in four ways. First, we use a larger corpus of TREC questions covering additional question types, thus extending the experiments to the question domain more broadly, as well as to the biomedical domain. Second, we create a gold standard GR resource enabling a full parser evaluation on question data. Third, we show that the POS level is important for adaptation, reinforcing the work of Lease and Charniak (2005). A key finding of the present paper is that the combination of retraining at the POS tag and lexical category levels provides additional improvements beyond those gained by retraining at a single level. Finally, we provide analysis comparing the adaptation methodology for question and biomedical data. Hara et al. (2007) followed a similar approach to Clark et al. (2004), using the parser of Ninomiya et al. (2006), a version of the Enju parser (Miyao and Tsujii, 2005). Enju is based on HPSG, a lexicalized grammar formalism. They obtained an improvement in parsing accuracy in the biomedical dom</context>
<context position="27545" citStr="Lease and Charniak (2005)" startWordPosition="4533" endWordPosition="4536">s in the biomedical data, such as major histocompatibility complex class II molecules. Although the words preceding the head noun are recognized as nominal modifiers, the classification into noun and adjective is difficult, especially when the word is previously unseen. There were also problems distinguishing verbal past participles (VBN) from adjectives (JJ) and identifying foreign words (FW), for example the phrase in vitro. The fact that the newspaper-trained POS tagger performed comparably in the two target domains (Table 1) is surprising, since their lexical profiles are quite different. Lease and Charniak (2005) discussed unknown word rate as a predictor of POS tagger accuracy. However, the unknown word rate compared with WSJ 02-21 is much higher for the biomedical data than for the question data, as seen in Table 5. (The unknown word rate for the question data is still higher than that for WSJ 00, which may be due to the high proportion of proper nouns in the question data.) Some POS tagging errors can be attributed, not to an unknown word, but to the use of a known word with an unfamiliar tag (as in the WDT example above). However, it is not the case that the question data contains many known words</context>
</contexts>
<marker>Lease, Charniak, 2005</marker>
<rawString>Matthew Lease and Eugene Charniak. 2005. Parsing biomedical literature. In Proceedings of the Second International Joint Conference on Natural Language Processing (IJCNLP-05), Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1108" citStr="Marcus et al., 1993" startWordPosition="159" endWordPosition="162">cal text and questions for a QA system, by using manually-annotated training data at the POS and lexical category levels only. This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain. We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation. 1 Introduction Most state-of-the-art wide-coverage parsers are based on the Penn Treebank (Marcus et al., 1993), making such parsers highly tuned to newspaper text. A pressing question facing the parsing community is how to adapt these parsers to other domains, such as biomedical research papers and web pages. A related question is how to improve the performance of these parsers on constructions that are rare in the Penn Treebank, such as questions. Questions are particularly important since a question parser is a component in most Question Answering (QA) systems (Harabagiu et al., 2001). In this paper we investigate parser adaptation in the context of lexicalized grammars, by using a parser based on C</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd meeting of the ACL,</booktitle>
<pages>83--90</pages>
<institution>University of Michigan,</institution>
<location>Ann Arbor.</location>
<contexts>
<context position="14206" citStr="Miyao and Tsujii, 2005" startWordPosition="2304" endWordPosition="2307">ll parser evaluation on question data. Third, we show that the POS level is important for adaptation, reinforcing the work of Lease and Charniak (2005). A key finding of the present paper is that the combination of retraining at the POS tag and lexical category levels provides additional improvements beyond those gained by retraining at a single level. Finally, we provide analysis comparing the adaptation methodology for question and biomedical data. Hara et al. (2007) followed a similar approach to Clark et al. (2004), using the parser of Ninomiya et al. (2006), a version of the Enju parser (Miyao and Tsujii, 2005). Enju is based on HPSG, a lexicalized grammar formalism. They obtained an improvement in parsing accuracy in the biomedical domain by training a new probabilistic model of lexical entry assignments on a combination of newspaper and biomedical data without changing the original newspaper-trained parsing model. Hara et al. (2007) did not consider the role of POS tagging. The lexical category data in Hara et al. (2007) was derived from a gold standard treebank, while the annotation of lexical categories in this paper was performed without reference to gold standard syntactic derivations. Judge e</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd meeting of the ACL, pages 83–90, University of Michigan, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast HPSG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP Conference.</booktitle>
<contexts>
<context position="14151" citStr="Ninomiya et al. (2006)" startWordPosition="2294" endWordPosition="2297">d, we create a gold standard GR resource enabling a full parser evaluation on question data. Third, we show that the POS level is important for adaptation, reinforcing the work of Lease and Charniak (2005). A key finding of the present paper is that the combination of retraining at the POS tag and lexical category levels provides additional improvements beyond those gained by retraining at a single level. Finally, we provide analysis comparing the adaptation methodology for question and biomedical data. Hara et al. (2007) followed a similar approach to Clark et al. (2004), using the parser of Ninomiya et al. (2006), a version of the Enju parser (Miyao and Tsujii, 2005). Enju is based on HPSG, a lexicalized grammar formalism. They obtained an improvement in parsing accuracy in the biomedical domain by training a new probabilistic model of lexical entry assignments on a combination of newspaper and biomedical data without changing the original newspaper-trained parsing model. Hara et al. (2007) did not consider the role of POS tagging. The lexical category data in Hara et al. (2007) was derived from a gold standard treebank, while the annotation of lexical categories in this paper was performed without re</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Extremely lexicalized models for accurate and fast HPSG parsing. In Proceedings of the EMNLP Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Juho Heimonen</author>
<author>Jari Bj¨orne</author>
<author>Jorma Boberg</author>
<author>Jouni J¨arvinen</author>
<author>Tapio Salakoski</author>
</authors>
<title>BioInfer: A corpus for information extraction in the biomedical domain.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<marker>Pyysalo, Ginter, Heimonen, Bj¨orne, Boberg, J¨arvinen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio Salakoski. 2007a. BioInfer: A corpus for information extraction in the biomedical domain. BMC Bioinformatics, 8:50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Veronika Laippala</author>
<author>Katri Haverinen</author>
<author>Juho Heimonen</author>
<author>Tapio Salakoski</author>
</authors>
<title>On the unification of syntactic annotations under the stanford dependency scheme: A case study on BioInfer and GENIA.</title>
<date>2007</date>
<booktitle>In ACL’07 workshop on Biological, translational, and clinical language processing,</booktitle>
<pages>25--32</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4310" citStr="Pyysalo et al., 2007" startWordPosition="689" endWordPosition="692">a pilot study in Clark et al. (2004), is to perform manual annotation only at the first two levels. Since the lexical category level consists of sequences of tags, rather than hierarchical derivations, the annotation can be performed relatively quickly. For the biomedical and question domains we manually annotated approximately 1,000 and 2,000 sentences, respectively, with CCG lexical categories. We also created a gold standard set of grammatical relations (GR) in the Stanford format (de Marneffe et al., 2006), using 500 of the questions. For the biomedical domain we used the BioInfer corpus (Pyysalo et al., 2007a), an existing gold-standard GR resource also in the Stanford format. We evaluated the parser on both lexical category assignment and recovery of GRs. The results show that the domain adaptation approach used here is successful in two very different domains, achieving parsing accuracy comparable to state-of-the-art accuracy for newspaper text. The results also show, however, that the two domains have different profiles with regard to the levels of representation used by the parser. We find that simply retraining the POS tagger used by the parser leads to a large improvement in performance for</context>
<context position="16073" citStr="Pyysalo et al., 2007" startWordPosition="2607" endWordPosition="2610"> on WSJ Section 00 of CCGbank, which contains 1913 sentences and approximately 45,000 words. For the biomedical domain, we trained the POS tagger on gold-standard POS tags from GENIA (Kim et al., 2003), a corpus of 2,000 MEDLINE abstracts containing a total of approximately 18,500 sentences and 440,000 words. We also annotated the first 1,000 sentences of GENIA with CCG lexical categories. This set of 1,000 sentences, containing approximately 27,000 words, was used for POS tagger evaluation and for development and evaluation of a new supertagger model. For parser evaluation, we used BioInfer (Pyysalo et al., 2007a), a corpus of MEDLINE abstracts (on a different topic from those in GENIA) containing 1,100 sentences, and with syntactic dependencies encoded as grammatical relations in the Stanford GR format. We used the same evaluation set of 500 sentences as in Pyysalo et al. (2007b), and the remaining 600 for development of the mapping to Stanford format. Two parsers have already been evaluated on BioInfer, which makes it a useful resource for comparative evaluation. For the question domain, we extended the dataset described in Clark et al. (2004). That dataset contained 1,171 questions beginning with </context>
<context position="23581" citStr="Pyysalo et al., 2007" startWordPosition="3865" endWordPosition="3868">ith the retrained POS and supertaggers. For the biomedical data, coverage was 97.2% for the original pipeline, 99.0% for the pipeline with the retrained POS tagger, and 99.8% for the pipeline with the retrained POS and supertaggers. The final accuracy for both domains is in the same range as that of the original parser on newspaper data (81.8%) (Clark and Curran, 2007b), although the results are not directly comparable, since the newspaper resource uses a different GR scheme. For the BioInfer corpus, the final accuracy is also in line with results reported in the literature for other parsers (Pyysalo et al., 2007b). (No comparable GR results are available for questions.) A score in this range is thought to be near the upper bound when evaluating a CCG parser on GRs, since some loss is inherent in the mapping to GRs (Clark and Curran, 2007a). 5 Analysis Although domain adaptation was successful for both of our target domains, the impact of the different levels of representation on parsing accuracy was not uniform. Table 3 shows that retraining the POS tagger accounted for a greater proportion of the improvement on biomedical data, while retraining the supertagger accounted for a much greater proportion</context>
</contexts>
<marker>Pyysalo, Ginter, Laippala, Haverinen, Heimonen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Veronika Laippala, Katri Haverinen, Juho Heimonen, and Tapio Salakoski. 2007b. On the unification of syntactic annotations under the stanford dependency scheme: A case study on BioInfer and GENIA. In ACL’07 workshop on Biological, translational, and clinical language processing, pages 25–32, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1760" citStr="Steedman, 2000" startWordPosition="265" endWordPosition="266"> newspaper text. A pressing question facing the parsing community is how to adapt these parsers to other domains, such as biomedical research papers and web pages. A related question is how to improve the performance of these parsers on constructions that are rare in the Penn Treebank, such as questions. Questions are particularly important since a question parser is a component in most Question Answering (QA) systems (Harabagiu et al., 2001). In this paper we investigate parser adaptation in the context of lexicalized grammars, by using a parser based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). A key property of CCG is that it is lexicalized, meaning that each word in a sentence is associated with an elementary syntactic structure. In the case of CCG this is a lexical category expressing subcategorization information. We exploit this property of CCG by performing manual annotation in the new domain, but only up to this level of representation, where the annotation can be carried out relatively quickly. Since CCG lexical categories are so expressive, many of the syntactic characteristics of a domain are captured at this level. The two domains we consider are the biomedical domain an</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>