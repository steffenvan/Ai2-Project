<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012841">
<title confidence="0.9960385">
Genre Independent Subgroup Detection in Online Discussion Threads: A
Pilot Study of Implicit Attitude using Latent Textual Semantics
</title>
<author confidence="0.648">
Pradeep Dasigi Weiwei Guo Mona Diab
</author>
<email confidence="0.846824">
pd2359@columbia.edu weiwei@cs.columbia.edu mdiab@ccls.columbia.edu
</email>
<affiliation confidence="0.599954">
Center for Computational Learning Systems, Columbia University
</affiliation>
<sectionHeader confidence="0.966735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913260869565">
We describe an unsupervised approach to
the problem of automatically detecting sub-
groups of people holding similar opinions in
a discussion thread. An intuitive way of iden-
tifying this is to detect the attitudes of discus-
sants towards each other or named entities or
topics mentioned in the discussion. Sentiment
tags play an important role in this detection,
but we also note another dimension to the de-
tection of people’s attitudes in a discussion: if
two persons share the same opinion, they tend
to use similar language content. We consider
the latter to be an implicit attitude. In this pa-
per, we investigate the impact of implicit and
explicit attitude in two genres of social media
discussion data, more formal wikipedia dis-
cussions and a debate discussion forum that
is much more informal. Experimental results
strongly suggest that implicit attitude is an im-
portant complement for explicit attitudes (ex-
pressed via sentiment) and it can improve the
sub-group detection performance independent
of genre.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999697583333333">
There has been a significant increase in discus-
sion forum data in online media recently. Most of
such discussion threads have a clear debate compo-
nent in them with varying levels of formality. Auto-
matically identifying the groups of discussants with
similar attitudes, or subgroup detection, is an inter-
esting problem which allows for a better understand-
ing of the data in this genre in a manner that could
directly benefit Opinion Mining research as well as
Community Mining from Social Networks.
A straight-forward approach to this problem is
to apply Opinion Mining techniques, and extract
</bodyText>
<page confidence="0.994192">
65
</page>
<bodyText confidence="0.999968305555556">
each discussant’s attitudes towards other discussants
and entities being discussed. But the challenge is
that Opinion Mining is not mature enough to ex-
tract all the correct opinions of discussants. In ad-
dition, without domain knowledge, using unsuper-
vised techniques to do this is quite challenging.
On observing interactions from these threads, we
believe that there is another dimension of attitude
which is expressed implicitly. We find that people
sharing the same opinion tend to speak about the
same topics even though they do not explicitly ex-
press their sentiment. We refer to this as Implicit
Attitude. One such example may be seen in the two
posts in Table 1. It can be seen that even though dis-
cussants A and B do not express explicit sentiments,
they hold similar views. Hence it can be said that
there is an agreement in their implicit attitudes.
Attempting to find a surface level word similar-
ity between posts of two discussants is not sufficient
as there are typically few overlapping words shared
among the posts. This is quite significant a problem
especially given the relative short context of posts.
Accordingly, in this work, we attempt to model the
implicit latent similarity between posts as a means of
identifying the implicit attitudes among discussants.
We apply variants on Latent Dirichelet Allocation
(LDA) based topic models to the problem (Blei et
al., 2003).
Our goal is identify subgroups with respect to dis-
cussants’ attitudes towards each other, the entities
and topics in a discussion forum. To our knowl-
edge, this is the first attempt at using text similar-
ity as an indication of user attitudes. We investigate
the influence of the explicit and implicit attitudes on
two genres of data, one more formal than the other.
We find an interesting trend. Explicit attitude alone
</bodyText>
<note confidence="0.4698055">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 65–69,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999897666666667">
as a feature is more useful than implicit attitude in
identifying sub-groups in informal data. But in the
case of formal data, implicit attitude yields better re-
sults. This may be due to the fact that in informal
data, strong subjective opinions about entities/events
or towards other discussants are expressed more ex-
plicitly. This is generally not the case in the formal
genre where ideas do not have as much sentiment as-
sociated with them, and hence the opinions are more
“implicit”. Finally, we observe that combining both
kinds of features improves performance of our sys-
tems for both genres.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999770846153846">
Substantial research exists in the fields of Opin-
ion Identification and Community Mining that is re-
lated to our current work. (Ganapathibhotla and
Liu, 2008) deal with the problem of finding opin-
ions from comparative sentences. Many previous
research efforts related to Opinion Target Identifi-
cation (Hu and Liu, 2004; Kobayashi et al., 2007;
Jakob and Gurevych, 2010), focus on the domain of
product reviews where they exploit the genre in mul-
tiple ways. Somasundaran and Wiebe (2009) used
unsupervised methods to identify stances in online
debates. They mine the web to find associations
indicative of opinions and combine them with dis-
course information. Their problem essentially deals
with the debate genre and finding the stance of an in-
dividual given two options. Ours is a more general
problem since we deal with discussion data in gen-
eral and not debates on specific topics. Hence our
aim is to identify multiple groups, not just two.
In terms of Sentiment Analysis, the work done by
Hassan et al.(2010) in using part-of-speech and de-
pendency structures to identify polarities of attitudes
is similar to our work. But they predict binary po-
larities in attitudes, and our goal of identification of
sub-groups is a more general problem in that we aim
at identifying multiple subgroups.
</bodyText>
<sectionHeader confidence="0.996132" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.9971565">
We tackle the problem using Vector Space Mod-
eling techniques to represent the discussion threads.
Each vector represents a discussant in the thread cre-
ating an Attitude Profile (AP). We use a clustering
algorithm to partition the vector space of APs into
multiple sub-groups. The idea is that resulting clus-
ters would comprise sub-groups of discussants with
similar attitudes.
</bodyText>
<subsectionHeader confidence="0.999748">
3.1 Basic Features
</subsectionHeader>
<bodyText confidence="0.999915774193548">
We use two basic features, namely Negative and
Positive sentiment towards specific discussants and
entities like in the work done by (Abu-Jbara et al.,
2012). We start off by determining sentences that
express attitude in the thread, attitude sentences
(AS). We use OpinionFinder (Wilson et al., 2005)
which employs negative and positive polarity cues.
For determining discussant sentiment, we need to
first identify who the target of their sentiment is: an-
other discussant, or an entity, where an entity could
be a topic or a person not participating in the dis-
cussion. Sentiment toward another discussant:
This is quite challenging since explicit sentiment ex-
pressed in a post is not necessarily directed towards
another discussant to whom it is a reply. It is pos-
sible that a discussant may be replying to another
poster but expressing an attitude towards a third en-
tity or discussant. However as a simplifying assump-
tion, similar to the work of (Hassan et al., 2010),
we adopt the view that replies in the sentences that
are determined to be attitudinal and contain second-
person pronouns (you, your, yourself) are assumed
to be directed towards the recipients of the replies.
Sentiment toward an entity: We again adopt a sim-
plifying view by modeling all the named entities in
a sentence without heeding the roles these entities
play, i.e. whether they are targets or not. Accord-
ingly, we extract all the named entities in a sentence
using Stanford’s Name Entity Recognizer (Finkel et
al., 2005). We only focus on Person and Organiza-
tion named entities.
</bodyText>
<subsectionHeader confidence="0.9999">
3.2 Extracting Implicit Attitudes
</subsectionHeader>
<bodyText confidence="0.999664909090909">
We define implicit attitudes as the semantic sim-
ilarity between texts comprising discussant utter-
ances or posts in a thread. We cannot find enough
overlapping words between posts, since some posts
are very short. Hence we apply LDA (Blei et al.,
2003) on texts to extract latent semantics of texts.
We split text into sentences, i.e., each sentence is
treated as a single document. Accordingly, each sen-
tence is represented as a K-dimension vector. By
computing the similarity on these vectors, we obtain
a more accurate semantic similarity.
</bodyText>
<page confidence="0.745197">
66
</page>
<tableCaption confidence="0.527827">
A: There are a few other directors in the history of cinema who have achieved such a singular and consistent worldview as Kubrick.
His films are very philosophically deep, they say something about everything, war, crime, relationships, humanity, etc.
B: All of his films show the true human nature of man and their inner fights and all of them are very
philosophical. Alfred was good in suspense and all, but his work is not as deep as Kubrick’s
Table 1: Example of Agreement based on Implicit Attitude
</tableCaption>
<table confidence="0.998636857142857">
WIKI CD
Median No. of Discussants (n) 6 29
� 2 4
Predicted No. Clusters
J
of (� 2 �)
Median No. of Actual Classes 3 3
</table>
<tableCaption confidence="0.992487">
Table 2: Number of Clusters
</tableCaption>
<subsectionHeader confidence="0.99965">
3.3 Clustering Attitude Space
</subsectionHeader>
<bodyText confidence="0.999988214285714">
A tree-based (hierarchical) clustering algorithm,
SLINK (Sibson, 1973) is used to cluster the vec-
tor space. Cosine Similarity between the vectors is
used as the inter-data point similarity measure for
clustering.1 We choose the number of clusters to be
FV/n2 ], described as the rule of thumb by (Mardia et
al., 1979), where n is the number of discussants in
the group. This rule seems to be validated by the fact
that in the data sets with which we experiment, we
note that the predicted number of clusters according
to this rule and the classes identified in the gold data
are very close as illustrated in Table 2. On average
we note that the gold data has the number of classes
per thread to be roughly 2-5.
</bodyText>
<sectionHeader confidence="0.998398" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999842846153846">
We use data from two online forums - Cre-
ate Debate [CD]2 and discussions from Wikipedia
[WIKI]3. There is a significant difference in the kind
of discussions in these two sources. Our WIKI data
comprises 117 threads crawled from Wikipedia. It is
relatively formal with short threads. It does not have
much negative polarity and discussants essentially
discuss the Wikipedia page in question. Hence it is
closer to an academic discussion forum. The threads
are manually annotated with sub-group information.
Given a thread, the annotator is asked to identify if
there are any sub-groups among the discussants with
similar opinions, and if yes, the membership of those
</bodyText>
<footnote confidence="0.973828727272727">
1We also experimented with K-means (MacQueen, 1967)
and found that it yields worse results compared to SLINK.
There is a fundamental difference between the two algorithms.
Where as K-Means does a random initialization of clusters,
SLINK is a deterministic algorithm. The difference in the per-
formance may be attributed to the fact that the number of initial
data points is too small for random initialization. Hence, tree
based clustering algorithms are more well suited for the current
task.
2http://www.createdebate.com
3en.wikipedia.org
</footnote>
<table confidence="0.99976">
Property WIKI CD
Threads 117 34
Posts per Thread 15.5 112
Sentences per Post 4.5 7.7
Tokens per Post 78.9 118.3
Word Types per Post 11.1 10.6
Discussants per Thread 6.5 34.15
Entities Discovered per Thread 6.15 32.7
</table>
<tableCaption confidence="0.998613">
Table 3: Data Statistics
</tableCaption>
<bodyText confidence="0.998237727272727">
subgroups.
On the other hand, CD is a forum where people
debate a specific topic. The CD data we use com-
prises 34 threads. It is more informal (with per-
vasive negative language and personal insults) than
WIKI and has longer threads. It is closer to the de-
bate genre. It has a poll associated with every de-
bate. The votes cast by the discussants in the poll
are used as the class labels for our experiments. De-
tailed statistics related to both the data sets and a
comparison can be found in Table 3.
</bodyText>
<sectionHeader confidence="0.999053" genericHeader="method">
5 Experimental Conditions
</sectionHeader>
<bodyText confidence="0.991932">
The following three features represent discussant
attitudes:
</bodyText>
<listItem confidence="0.998896333333333">
• Sentiment towards other discussants (SD) - This
corresponds to 2 * n dimensions in the Attitude Pro-
file (AP) vector, n being the number of discussants
in the thread. This is because there are two polari-
ties and n possible targets. The value representing
this feature is the number of sentences with the re-
spective polarity – negative or positive – towards the
particular discussant.
• Sentiment towards entities in discussion (SE) -
Number of dimensions corresponding to this feature
is 2*e, where e is the number of entities discovered.
Similar to SD, the value taken by this feature is the
number of sentences in which that specific polarity
is shown by the discussant towards the entity.
• Implicit Attitude (IA) - n * t dimensions are ex-
</listItem>
<bodyText confidence="0.714848">
pressed using this feature, where t is the number of
topics that the topic model contains. This means that
the AP of every discussant contains the topic model
distribution of his/her interactions with every other
member in the thread. Hence, the topics in the inter-
ation between the given discussant and other mem-
bers in the thread are being modeled here. Accord-
</bodyText>
<page confidence="0.998854">
67
</page>
<bodyText confidence="0.999962333333334">
ingly, high vector similarity due to IA between two
members in a thread means that they discussed sim-
ilar topics with the same people in the thread. In
our experiments, we set t = 50. We use the Gibbs
sampling based LDA (Griffiths and Steyvers, 2004).
The LDA model is built on definitions of two online
dictionaries WordNet, and Wiktionary, in addition
to the Brown corpus (BC). To create more context,
each sentence from BC is treated as a document.
The whole corpus contains 393,667 documents and
5,080,369 words.
The degree of agreement among discussants in
terms of these three features is used to identify sub-
groups among them. Our experiments are aimed at
investigating the effect of explicit attitude features
(SD and SE) in comparison with implicit feature
(IA) and how they perform when combined. So
the experimental conditions are: the three features
in isolation, each of the explicit features SD and SE
together with IA, and then all three features together.
SWD-BASE: As a baseline, we employ a simple
word frequency based model to capture topic dis-
tribution, Surface Word Distribution (SWD). SWD
is still topic modeling in the vector space, but the di-
mensions of the vectors are the frequencies of all the
unique words used by the discussant in question.
RAND-BASE: We also apply a very simple base-
line using random assignment of discussants to
groups, however the number of clusters is deter-
mined by the rule of thumb described in Section 3.3.
</bodyText>
<sectionHeader confidence="0.99876" genericHeader="evaluation">
6 Results and Analysis
</sectionHeader>
<bodyText confidence="0.9985950625">
Three metrics are used for evaluation, as de-
scribed in (Manning et al., 2008): Purity, Entropy
and F-measure. Table 4 shows the results of the
9 experimental conditions. The following observa-
tions can be made: All the individual conditions SD,
SE and IA clearly outperform SWD-BASE. All the
experimental conditions outperform RAND-BASE
which indicates that using clustering is contributing
positively to the problem. SE performs worse than
SD across both datasets CD and WIKI. This may
be due to two reasons: Firstly, since the problem
is of clustering the discussant space, SD should be
a better indicator than SE. Secondly, as seen from
the comparison in Table 5, there are more polarized
sentences indicating SD than SE. IA clearly outper-
forms SD, SE and SD+SE in the case of WIKI. In
</bodyText>
<table confidence="0.9995386">
Property WIKI CD
Positive Sentences towards Discussants 5.15 17.94
Negative Sentences towards Discussants 6.75 40.38
Positive Sentences towards Entities 1.65 8.85
Negative Sentences towards Entities 1.59 8.53
</table>
<tableCaption confidence="0.7772455">
Table 5: Statistics of the Attitudinal Sentences per
each Thread in the two data sets
</tableCaption>
<bodyText confidence="0.999522545454546">
the case of CD, it is exactly the opposite. This is an
interesting result and we believe it is mainly due to
the genre of the data. Explicit expression of senti-
ment usually increases with the increase in the in-
formal nature of discussions. Hence IA is more use-
ful in WIKI which is more formal compared to CD,
where there is less overt sentiment expression. We
note the same trend with the SWD-BASE where per-
formance on WIKI is much better than its perfor-
mance on CD. This also suggests that WIKI might
be an easier data set. A qualitative comparison of the
inter-discussant relations can be gleaned from Ta-
ble 5. There is significantly more negative language
than positive language in CD when compared with
the ratios of negative to positive language in WIKI,
which are almost the same. The best results over-
all are yielded from the combination of IA with SD
and SE, the implicit and explicit features together for
both data sets, which suggests that Implicit and ex-
plicit attitude features complement each other cap-
turing more information than each of them individ-
ually.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999998076923077">
We proposed the use of LDA based topic mod-
eling as an implicit agreement feature for the task
of identifying similar attitudes in online discussions.
We specifically applied latent modeling to the prob-
lem of sub-group detection. We compared this with
explicit sentiment features in different genres both
in isolation and in combination. We highlighted the
difference in genre in the datasets and the necessity
for capturing different forms of information from
them for the task at hand. The best yielding con-
dition in both the dat sets combines implicit and ex-
plicit features suggesting that there is a complemen-
tarity between the two tpes of feaures.
</bodyText>
<sectionHeader confidence="0.969822" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.98341175">
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab.
</bodyText>
<page confidence="0.997425">
68
</page>
<table confidence="0.999891090909091">
Condition WIKI CD
Purity Entropy F-measure Purity Entropy F-measure
RAND-BASE 0.6745 0.5629 0.6523 0.3986 0.9664 0.407
SWD-BASE 0.7716 0.4746 0.6455 0.4514 0.9319 0.4322
SD 0.8342 0.3602 0.667 0.8243 0.3942 0.5964
SE 0.8265 0.3829 0.6554 0.7933 0.4216 0.5818
SD+SE 0.8346 0.3614 0.6649 0.82 0.3851 0.6039
IA 0.8527 0.3209 0.6993 0.787 0.3993 0.5891
SD+IA 0.8532 0.3199 0.6977 0.8487 0.3328 0.6152
SE+IA 0.8525 0.3216 0.7015 0.7884 0.3986 0.591
SD+SE+IA 0.8572 0.3104 0.7032 0.8608 0.3149 0.6251
</table>
<tableCaption confidence="0.999019">
Table 4: Experimental Results
</tableCaption>
<sectionHeader confidence="0.99518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999713737704918">
Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and
Dragomir Radev. 2012. Subgroup detection in ideo-
logical discussions. In Proceedings of the 5oth Annual
Meeting of ACL.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics (Coling 2008).
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What’s with the attitude? identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing,.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning.
J. MacQueen. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings
of Fifth Berkeley Symposium on Mathematical Statis-
tics and Probability.
Christopher D. Manning, Prabhakar Raghavan, , and Hin-
rich Schtze. 2008.. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY,USA.
K. V. Mardia, J. T. Kent, and J. M. Bibby. 1979. Multi-
variate Analysis. Publisher.
R. Sibson. 1973. Slink: An optimally efficient algorithm
for the single-link cluster method. In The Computer
Journal (1973) 16 (1): 30-34.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, JanyceWiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT/EMNLP 2005 Demonstration.
</reference>
<page confidence="0.999308">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.944558">
<title confidence="0.9860035">Genre Independent Subgroup Detection in Online Discussion Threads: A Pilot Study of Implicit Attitude using Latent Textual Semantics</title>
<author confidence="0.999055">Pradeep Dasigi Weiwei Guo Mona Diab</author>
<email confidence="0.995281">pd2359@columbia.eduweiwei@cs.columbia.edumdiab@ccls.columbia.edu</email>
<affiliation confidence="0.995096">Center for Computational Learning Systems, Columbia University</affiliation>
<abstract confidence="0.999231208333333">We describe an unsupervised approach to the problem of automatically detecting subgroups of people holding similar opinions in a discussion thread. An intuitive way of identifying this is to detect the attitudes of discussants towards each other or named entities or topics mentioned in the discussion. Sentiment tags play an important role in this detection, but we also note another dimension to the detection of people’s attitudes in a discussion: if two persons share the same opinion, they tend to use similar language content. We consider the latter to be an implicit attitude. In this paper, we investigate the impact of implicit and explicit attitude in two genres of social media discussion data, more formal wikipedia discussions and a debate discussion forum that is much more informal. Experimental results strongly suggest that implicit attitude is an important complement for explicit attitudes (expressed via sentiment) and it can improve the sub-group detection performance independent of genre.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Pradeep Dasigi</author>
<author>Mona Diab</author>
<author>Dragomir Radev</author>
</authors>
<title>Subgroup detection in ideological discussions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 5oth Annual Meeting of ACL.</booktitle>
<contexts>
<context position="6404" citStr="Abu-Jbara et al., 2012" startWordPosition="1023" endWordPosition="1026"> problem in that we aim at identifying multiple subgroups. 3 Approach We tackle the problem using Vector Space Modeling techniques to represent the discussion threads. Each vector represents a discussant in the thread creating an Attitude Profile (AP). We use a clustering algorithm to partition the vector space of APs into multiple sub-groups. The idea is that resulting clusters would comprise sub-groups of discussants with similar attitudes. 3.1 Basic Features We use two basic features, namely Negative and Positive sentiment towards specific discussants and entities like in the work done by (Abu-Jbara et al., 2012). We start off by determining sentences that express attitude in the thread, attitude sentences (AS). We use OpinionFinder (Wilson et al., 2005) which employs negative and positive polarity cues. For determining discussant sentiment, we need to first identify who the target of their sentiment is: another discussant, or an entity, where an entity could be a topic or a person not participating in the discussion. Sentiment toward another discussant: This is quite challenging since explicit sentiment expressed in a post is not necessarily directed towards another discussant to whom it is a reply. </context>
</contexts>
<marker>Abu-Jbara, Dasigi, Diab, Radev, 2012</marker>
<rawString>Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and Dragomir Radev. 2012. Subgroup detection in ideological discussions. In Proceedings of the 5oth Annual Meeting of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="3321" citStr="Blei et al., 2003" startWordPosition="522" endWordPosition="525">lar views. Hence it can be said that there is an agreement in their implicit attitudes. Attempting to find a surface level word similarity between posts of two discussants is not sufficient as there are typically few overlapping words shared among the posts. This is quite significant a problem especially given the relative short context of posts. Accordingly, in this work, we attempt to model the implicit latent similarity between posts as a means of identifying the implicit attitudes among discussants. We apply variants on Latent Dirichelet Allocation (LDA) based topic models to the problem (Blei et al., 2003). Our goal is identify subgroups with respect to discussants’ attitudes towards each other, the entities and topics in a discussion forum. To our knowledge, this is the first attempt at using text similarity as an indication of user attitudes. We investigate the influence of the explicit and implicit attitudes on two genres of data, one more formal than the other. We find an interesting trend. Explicit attitude alone Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 65–69, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational</context>
<context position="8089" citStr="Blei et al., 2003" startWordPosition="1302" endWordPosition="1305"> simplifying view by modeling all the named entities in a sentence without heeding the roles these entities play, i.e. whether they are targets or not. Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005). We only focus on Person and Organization named entities. 3.2 Extracting Implicit Attitudes We define implicit attitudes as the semantic similarity between texts comprising discussant utterances or posts in a thread. We cannot find enough overlapping words between posts, since some posts are very short. Hence we apply LDA (Blei et al., 2003) on texts to extract latent semantics of texts. We split text into sentences, i.e., each sentence is treated as a single document. Accordingly, each sentence is represented as a K-dimension vector. By computing the similarity on these vectors, we obtain a more accurate semantic similarity. 66 A: There are a few other directors in the history of cinema who have achieved such a singular and consistent worldview as Kubrick. His films are very philosophically deep, they say something about everything, war, crime, relationships, humanity, etc. B: All of his films show the true human nature of man a</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7745" citStr="Finkel et al., 2005" startWordPosition="1246" endWordPosition="1249">iscussant. However as a simplifying assumption, similar to the work of (Hassan et al., 2010), we adopt the view that replies in the sentences that are determined to be attitudinal and contain secondperson pronouns (you, your, yourself) are assumed to be directed towards the recipients of the replies. Sentiment toward an entity: We again adopt a simplifying view by modeling all the named entities in a sentence without heeding the roles these entities play, i.e. whether they are targets or not. Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005). We only focus on Person and Organization named entities. 3.2 Extracting Implicit Attitudes We define implicit attitudes as the semantic similarity between texts comprising discussant utterances or posts in a thread. We cannot find enough overlapping words between posts, since some posts are very short. Hence we apply LDA (Blei et al., 2003) on texts to extract latent semantics of texts. We split text into sentences, i.e., each sentence is treated as a single document. Accordingly, each sentence is represented as a K-dimension vector. By computing the similarity on these vectors, we obtain a </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murthy Ganapathibhotla</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinions in comparative sentences.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<contexts>
<context position="4704" citStr="Ganapathibhotla and Liu, 2008" startWordPosition="747" endWordPosition="750">tude yields better results. This may be due to the fact that in informal data, strong subjective opinions about entities/events or towards other discussants are expressed more explicitly. This is generally not the case in the formal genre where ideas do not have as much sentiment associated with them, and hence the opinions are more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given</context>
</contexts>
<marker>Ganapathibhotla, Liu, 2008</marker>
<rawString>Murthy Ganapathibhotla and Bing Liu. 2008. Mining opinions in comparative sentences. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101</pages>
<contexts>
<context position="13172" citStr="Griffiths and Steyvers, 2004" startWordPosition="2173" endWordPosition="2176">ensions are expressed using this feature, where t is the number of topics that the topic model contains. This means that the AP of every discussant contains the topic model distribution of his/her interactions with every other member in the thread. Hence, the topics in the interation between the given discussant and other members in the thread are being modeled here. Accord67 ingly, high vector similarity due to IA between two members in a thread means that they discussed similar topics with the same people in the thread. In our experiments, we set t = 50. We use the Gibbs sampling based LDA (Griffiths and Steyvers, 2004). The LDA model is built on definitions of two online dictionaries WordNet, and Wiktionary, in addition to the Brown corpus (BC). To create more context, each sentence from BC is treated as a document. The whole corpus contains 393,667 documents and 5,080,369 words. The degree of agreement among discussants in terms of these three features is used to identify subgroups among them. Our experiments are aimed at investigating the effect of explicit attitude features (SD and SE) in comparison with implicit feature (IA) and how they perform when combined. So the experimental conditions are: the thr</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
</authors>
<title>What’s with the attitude? identifying sentences with attitude in online discussions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,.</booktitle>
<contexts>
<context position="7217" citStr="Hassan et al., 2010" startWordPosition="1158" endWordPosition="1161">s. For determining discussant sentiment, we need to first identify who the target of their sentiment is: another discussant, or an entity, where an entity could be a topic or a person not participating in the discussion. Sentiment toward another discussant: This is quite challenging since explicit sentiment expressed in a post is not necessarily directed towards another discussant to whom it is a reply. It is possible that a discussant may be replying to another poster but expressing an attitude towards a third entity or discussant. However as a simplifying assumption, similar to the work of (Hassan et al., 2010), we adopt the view that replies in the sentences that are determined to be attitudinal and contain secondperson pronouns (you, your, yourself) are assumed to be directed towards the recipients of the replies. Sentiment toward an entity: We again adopt a simplifying view by modeling all the named entities in a sentence without heeding the roles these entities play, i.e. whether they are targets or not. Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005). We only focus on Person and Organization named entities. 3.2 Extractin</context>
</contexts>
<marker>Hassan, Qazvinian, Radev, 2010</marker>
<rawString>Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev. 2010. What’s with the attitude? identifying sentences with attitude in online discussions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<contexts>
<context position="4864" citStr="Hu and Liu, 2004" startWordPosition="772" endWordPosition="775">e explicitly. This is generally not the case in the formal genre where ideas do not have as much sentiment associated with them, and hence the opinions are more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given two options. Ours is a more general problem since we deal with discussion data in general and not debates on specific topics. Hence our aim is to identify mult</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using anaphora resolution to improve opinion target identification in movie reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers.</booktitle>
<contexts>
<context position="4915" citStr="Jakob and Gurevych, 2010" startWordPosition="780" endWordPosition="783">se in the formal genre where ideas do not have as much sentiment associated with them, and hence the opinions are more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given two options. Ours is a more general problem since we deal with discussion data in general and not debates on specific topics. Hence our aim is to identify multiple groups, not just two. In terms of Sentiment An</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010. Using anaphora resolution to improve opinion target identification in movie reviews. In Proceedings of the ACL 2010 Conference Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="4888" citStr="Kobayashi et al., 2007" startWordPosition="776" endWordPosition="779"> is generally not the case in the formal genre where ideas do not have as much sentiment associated with them, and hence the opinions are more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given two options. Ours is a more general problem since we deal with discussion data in general and not debates on specific topics. Hence our aim is to identify multiple groups, not just tw</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J MacQueen</author>
</authors>
<title>Some methods for classification and analysis of multivariate observations.</title>
<date>1967</date>
<booktitle>In Proceedings of Fifth Berkeley Symposium on Mathematical Statistics and Probability.</booktitle>
<contexts>
<context position="10498" citStr="MacQueen, 1967" startWordPosition="1717" endWordPosition="1718">is a significant difference in the kind of discussions in these two sources. Our WIKI data comprises 117 threads crawled from Wikipedia. It is relatively formal with short threads. It does not have much negative polarity and discussants essentially discuss the Wikipedia page in question. Hence it is closer to an academic discussion forum. The threads are manually annotated with sub-group information. Given a thread, the annotator is asked to identify if there are any sub-groups among the discussants with similar opinions, and if yes, the membership of those 1We also experimented with K-means (MacQueen, 1967) and found that it yields worse results compared to SLINK. There is a fundamental difference between the two algorithms. Where as K-Means does a random initialization of clusters, SLINK is a deterministic algorithm. The difference in the performance may be attributed to the fact that the number of initial data points is too small for random initialization. Hence, tree based clustering algorithms are more well suited for the current task. 2http://www.createdebate.com 3en.wikipedia.org Property WIKI CD Threads 117 34 Posts per Thread 15.5 112 Sentences per Post 4.5 7.7 Tokens per Post 78.9 118.3</context>
</contexts>
<marker>MacQueen, 1967</marker>
<rawString>J. MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of Fifth Berkeley Symposium on Mathematical Statistics and Probability.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY,USA.</location>
<marker>Manning, Raghavan, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, , and Hinrich Schtze. 2008.. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY,USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K V Mardia</author>
<author>J T Kent</author>
<author>J M Bibby</author>
</authors>
<title>Multivariate Analysis.</title>
<date>1979</date>
<publisher>Publisher.</publisher>
<contexts>
<context position="9379" citStr="Mardia et al., 1979" startWordPosition="1524" endWordPosition="1527">as good in suspense and all, but his work is not as deep as Kubrick’s Table 1: Example of Agreement based on Implicit Attitude WIKI CD Median No. of Discussants (n) 6 29 � 2 4 Predicted No. Clusters J of (� 2 �) Median No. of Actual Classes 3 3 Table 2: Number of Clusters 3.3 Clustering Attitude Space A tree-based (hierarchical) clustering algorithm, SLINK (Sibson, 1973) is used to cluster the vector space. Cosine Similarity between the vectors is used as the inter-data point similarity measure for clustering.1 We choose the number of clusters to be FV/n2 ], described as the rule of thumb by (Mardia et al., 1979), where n is the number of discussants in the group. This rule seems to be validated by the fact that in the data sets with which we experiment, we note that the predicted number of clusters according to this rule and the classes identified in the gold data are very close as illustrated in Table 2. On average we note that the gold data has the number of classes per thread to be roughly 2-5. 4 Data We use data from two online forums - Create Debate [CD]2 and discussions from Wikipedia [WIKI]3. There is a significant difference in the kind of discussions in these two sources. Our WIKI data compr</context>
</contexts>
<marker>Mardia, Kent, Bibby, 1979</marker>
<rawString>K. V. Mardia, J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sibson</author>
</authors>
<title>Slink: An optimally efficient algorithm for the single-link cluster method.</title>
<date>1973</date>
<booktitle>In The Computer Journal</booktitle>
<volume>16</volume>
<issue>1</issue>
<pages>30--34</pages>
<contexts>
<context position="9132" citStr="Sibson, 1973" startWordPosition="1482" endWordPosition="1483">ilms are very philosophically deep, they say something about everything, war, crime, relationships, humanity, etc. B: All of his films show the true human nature of man and their inner fights and all of them are very philosophical. Alfred was good in suspense and all, but his work is not as deep as Kubrick’s Table 1: Example of Agreement based on Implicit Attitude WIKI CD Median No. of Discussants (n) 6 29 � 2 4 Predicted No. Clusters J of (� 2 �) Median No. of Actual Classes 3 3 Table 2: Number of Clusters 3.3 Clustering Attitude Space A tree-based (hierarchical) clustering algorithm, SLINK (Sibson, 1973) is used to cluster the vector space. Cosine Similarity between the vectors is used as the inter-data point similarity measure for clustering.1 We choose the number of clusters to be FV/n2 ], described as the rule of thumb by (Mardia et al., 1979), where n is the number of discussants in the group. This rule seems to be validated by the fact that in the data sets with which we experiment, we note that the predicted number of clusters according to this rule and the classes identified in the gold data are very close as illustrated in Table 2. On average we note that the gold data has the number </context>
</contexts>
<marker>Sibson, 1973</marker>
<rawString>R. Sibson. 1973. Slink: An optimally efficient algorithm for the single-link cluster method. In The Computer Journal (1973) 16 (1): 30-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="5032" citStr="Somasundaran and Wiebe (2009)" startWordPosition="800" endWordPosition="803"> more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given two options. Ours is a more general problem since we deal with discussion data in general and not debates on specific topics. Hence our aim is to identify multiple groups, not just two. In terms of Sentiment Analysis, the work done by Hassan et al.(2010) in using part-of-speech and dependency structures to identify polarities</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Yejin Choi JanyceWiebe</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP 2005 Demonstration.</booktitle>
<contexts>
<context position="6548" citStr="Wilson et al., 2005" startWordPosition="1045" endWordPosition="1048"> discussion threads. Each vector represents a discussant in the thread creating an Attitude Profile (AP). We use a clustering algorithm to partition the vector space of APs into multiple sub-groups. The idea is that resulting clusters would comprise sub-groups of discussants with similar attitudes. 3.1 Basic Features We use two basic features, namely Negative and Positive sentiment towards specific discussants and entities like in the work done by (Abu-Jbara et al., 2012). We start off by determining sentences that express attitude in the thread, attitude sentences (AS). We use OpinionFinder (Wilson et al., 2005) which employs negative and positive polarity cues. For determining discussant sentiment, we need to first identify who the target of their sentiment is: another discussant, or an entity, where an entity could be a topic or a person not participating in the discussion. Sentiment toward another discussant: This is quite challenging since explicit sentiment expressed in a post is not necessarily directed towards another discussant to whom it is a reply. It is possible that a discussant may be replying to another poster but expressing an attitude towards a third entity or discussant. However as a</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, JanyceWiebe, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, JanyceWiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Opinionfinder: A system for subjectivity analysis. In Proceedings of HLT/EMNLP 2005 Demonstration.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>