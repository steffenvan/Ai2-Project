<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.336016">
<title confidence="0.992565">
Hebrew Dependency Parsing: Initial Results
</title>
<author confidence="0.996374">
Yoav Goldberg and Michael Elhadad
</author>
<affiliation confidence="0.9994965">
Ben Gurion University of the Negev
Department of Computer Science
</affiliation>
<address confidence="0.813587">
POB 653 Be’er Sheva, 84105, Israel
</address>
<email confidence="0.999056">
{yoavg,elhadad}@cs.bgu.ac.il
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838545454546">
We describe a newly available Hebrew
Dependency Treebank, which is extracted
from the Hebrew (constituency) Tree-
bank. We establish some baseline un-
labeled dependency parsing performance
on Hebrew, based on two state-of-the-art
parsers, MST-parser and MaltParser. The
evaluation is performed both in an artifi-
cial setting, in which the data is assumed
to be properly morphologically segmented
and POS-tagged, and in a real-world set-
ting, in which the parsing is performed on
automatically segmented and POS-tagged
text. We present an evaluation measure
that takes into account the possibility of
incompatible token segmentation between
the gold standard and the parsed data.
Results indicate that (a) MST-parser per-
forms better on Hebrew data than Malt-
Parser, and (b) both parsers do not make
good use of morphological information
when parsing Hebrew.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937571428572">
Hebrew is a Semitic language with rich morpho-
logical structure and free constituent order.
Previous computational work addressed unsu-
pervised Hebrew POS tagging and unknown word
resolution (Adler, 2007), Hebrew NP-chunking
(Goldberg et al., 2006), and Hebrew constituency
parsing (Tsarfaty, 2006; Golderg et al., 2009).
Here, we focus on Hebrew dependency parsing.
Dependency-parsing got a lot of research at-
tention lately, in part due to two CoNLL shared
tasks focusing on multilingual dependency parsing
(Buchholz and Erwin, 2006; Nivre et al., 2007).
These tasks include relatively many parsing re-
sults for Arabic, a Semitic language similar to He-
brew. However, parsing accuracies for Arabic usu-
ally lag behind non-semitic languages. Moreover,
while there are many published results, we could
not find any error analysis or even discussion of
the results of Arabic dependency parsing models,
or the specific properties of Arabic making it easy
or hard to parse in comparison to other languages.
Our aim is to evaluate current state-of-the-art
dependency parsers and approaches on Hebrew
dependency parsing, to understand some of the
difficulties in parsing a Semitic language, and to
establish a strong baseline for future work.
We present the first published results on Depen-
dency Parsing of Hebrew.
Some aspects that make Hebrew challenging
from a parsing perspective are:
Affixation Common prepositions, conjunctions
and articles are prefixed to the following word,
and pronominal elements often appear as suffixes.
The segmentation of prefixes and suffixes is of-
ten ambiguous and must be determined in a spe-
cific context only. In term of dependency pars-
ing, this means that the dependency relations oc-
cur not between space-delimited tokens, but in-
stead between sub-token elements which we’ll re-
fer to as segments. Furthermore, any mistakes in
the underlying token segmentations are sure to be
reflected in the parsing accuracy.
Relatively free constituent order The ordering
of constituents inside a phrase is relatively free.
This is most notably apparent in the verbal phrases
and sentential levels. In particular, while most sen-
tences follow an SVO order, OVS and VSO con-
figurations are also possible. Verbal arguments
can appear before or after the verb, and in many
ordering. For example, the message “went from
Israel to Thailand” can be expressed as “went to
Thailand from Israel”, “to Thailand went from Is-
rael”, “from Israel went to Thailand”, “from Israel
to Thailand went” and “to Thailand from Israel
went”. This results in long and flat VP and S struc-
tures and a fair amount of sparsity, which suggests
</bodyText>
<page confidence="0.981736">
129
</page>
<bodyText confidence="0.984429533333333">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 129–133,
Paris, October 2009. c�2009 Association for Computational Linguistics
that a dependency representations might be more
suitable to Hebrew than a constituency one.
Rich templatic morphology Hebrew has a
very productive morphological structure, which
is based on a root+template system. The pro-
ductive morphology results in many distinct word
forms and a high out-of-vocabulary rate, which
makes it hard to reliably estimate lexical param-
eters from annotated corpora. The root+template
system (combined with the unvocalized writing
system) makes it hard to guess the morphological
analyses of an unknown word based on its prefix
and suffix, as usually done in other languages.
Unvocalized writing system Most vowels are
not marked in everyday Hebrew text, which re-
sults in a very high level of lexical and morpho-
logical ambiguity. Some tokens can admit as many
as 15 distinct readings, and the average number of
possible morphological analyses per token in He-
brew text is 2.7, compared to 1.4 in English (Adler,
2007). This means that on average, every token is
ambiguous with respect to its POS and morpho-
logical features.
Agreement Hebrew grammar forces morpho-
logical agreement between Adjectives and Nouns
(which should agree in Gender and Number and
definiteness), and between Subjects and Verbs
(which should agree in Gender and Number).
</bodyText>
<sectionHeader confidence="0.959945" genericHeader="method">
2 Hebrew Dependency Treebank
</sectionHeader>
<bodyText confidence="0.999297047619047">
Our experiments are based on the Hebrew De-
pendency Treebank (henceforth DepTB), which
we derived from Version 2 of the Hebrew
Constituency Treebank (Guthmann et al., 2009)
(henceforth TBv2). We briefly discuss the conver-
sion process and the resulting Treebank:
Parent-child dependencies TBv2 marks sev-
eral kinds of dependencies, indicating the mother-
daughter percolation of features such as number,
gender, definiteness and accusativity. See (Guth-
mann et al., 2009) for the details. We follow
TBv2’s HEAD, MAJOR and MULTIPLE depen-
dency marking in our-head finding rules. When
these markings are not available we use head find-
ing rules in the spirit of Collins. The head-finding
rules were developed by Reut Tsarfaty and used
in (Tsarfaty and Sima’an, 2008). We slightly ex-
tended them to handle previously unhandled cases.
Some conventions in TBv2 annotations resulted in
bad dependency structures. We identified these
constructions and transformed the tree structure,
</bodyText>
<figureCaption confidence="0.999997">
Figure 1: Coordinated Verbs
Figure 2: Coordinated Sentence
</figureCaption>
<bodyText confidence="0.99988">
either manually or automatically, prior to the de-
pendency extraction process.
The conversion process revealed some errors
and inconsistencies in TBv2, which we fixed.
We take relativizers as the head S and SBAR,
and prepositions as the heads of PPs. In the case
the parent of a word X is an empty element, we
take the parent of the empty element as the par-
ent of X instead. While this may result in non-
projective structures, in practice all but 34 of the
resulting trees are projective.
We take conjunctions to be the head of a coordi-
nated structure, resulting in dependency structures
such as the one in Figures 1 and 2. Notice how
in Figure 1 the parent of the subject “xוה/He” is
the coordinator “ו/and”, and not one of the verbs.
While this makes things harder for the parser, we
find this representation to be much cleaner and
more expressive than the usual approach in which
the first coordinated element is taken as the head
of the coordinated structure.1
Dependency labels TBv2 marks 3 kinds of
functional relations: Subject, Object and Comple-
mentizer. We use these in our conversion pro-
cess, and label dependencies as being SBJ, OBJ
or CMP, as indicated in TBv2. We also trivially
mark the ROOT dependency, and introduce the re-
lations INF PREP, AT INF POS INF RB INF be-
tween a base word and its suffix for the cases of
suffix-inflected prepositions, accusative suffixes,
possessive suffixes and inflected-adverbs, respec-
tively. Still, most dependency relations remain un-
labeled. We are currently seeking a method of re-
liably labeling the remaining edges with a rich set
</bodyText>
<footnote confidence="0.979258666666667">
1A possible alternative would be to allow multiple par-
ents, as done in (de Marneffe et al., 2006), but current parsing
algorithms require the output to be tree structured.
</footnote>
<page confidence="0.99395">
130
</page>
<bodyText confidence="0.999923428571429">
of relations. However, in the current work we fo-
cus on the unlabeled dependency structure.
POS tags The Hebrew Treebank follows a syn-
tactic tagging scheme, while other Hebrew re-
sources prefer a more morphological/dictionary-
based scheme. For a discussion of these two tag-
ging schemes in the context of parsing, see (Gold-
erg et al., 2009). In DepTB, we kept the two
tagsets, and each token has two POS tags asso-
ciated with it. However, as current dependency
parsers rely on an external POS tagger, we per-
formed all of our experiments only with the mor-
phological tagset, which is what our tagger pro-
duces.
</bodyText>
<sectionHeader confidence="0.998042" genericHeader="method">
3 The Parsing Models
</sectionHeader>
<bodyText confidence="0.999914083333333">
To establish some baseline results for Hebrew de-
pendency parsing, we experiment with two pars-
ing models, the graph-based MST-parser (Mc-
Donald, 2006) and the transition-based MaltParser
(Nivre et al., 2006). These two parsers repre-
sent the current mainstream approaches for de-
pendency parsing, and each was shown to pro-
vide state-of-the-art results on many languages
(CoNLL Shared Task 2006, 2007).
Briefly, a graph-based parsing model works by
assigning a score to every possible attachment be-
tween a pair (or a triple, for a second-order model)
of words, and then inferring a global tree struc-
ture that maximizes the sum of these local scores.
Transition-based models work by building the de-
pendency graph in a sequence of steps, where each
step is dependent on the next input word(s), the
previous decisions, and the current state of the
parser. For more details about these parsing mod-
els as well as a discussion on the relative benefits
of each model, see (McDonald and Nivre, 2007).
Contrary to constituency-based parsers, depen-
dency parsing models expect a morphologically
segmented and POS tagged text as input.
</bodyText>
<sectionHeader confidence="0.999873" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999764482142857">
Data We follow the train-test-dev split estab-
lished in (Tsarfaty and Sima’an, 2008). Specifi-
cally, we use Sections 2-12 (sentences 484-5724)
of the Hebrew Dependency Treebank as our train-
ing set, and report results on parsing the develop-
ment set, Section 1 (sentences 0-483). We do not
evaluate on the test set in this work.
The data in the Treebank is segmented and
POS-tagged. All of the models were trained on the
gold-standard segmented and tagged data. When
evaluating the parsing models, we perform two
sets of evaluations. The first one is an oracle ex-
periment, assuming gold segmentation and tag-
ging is available. The second one is a real-world
experiment, in which we segment and POS-tag the
test-set sentences using the morphological disam-
biguator described in (Adler, 2007; Goldberg et
al., 2008) prior to parsing.
Parsers and parsing models We use the freely
available implementation of MaltParser2 and
MSTParser3, with default settings for each of the
parsers.
For MaltParser, we experiment both with the de-
fault feature representation (MALT) and the fea-
ture representation used for parsing Arabic in
CoNLL 2006 and 2007 multilingual dependency
parsing shared tasks (MALT-ARA).
For MST parser, we experimented with first-
order (MST1) and second-order (MST2) models.
We varied the amount of lexical information
available to the parser. Each of the parsers was
trained on 3 datasets: LEXFULL, in which all the
lexical items are available, LEX20, in which lexi-
cal items appearing less than 20 times in the train-
ing data were replaced by an OOV token, and
LEX100 in which we kept only lexical items ap-
pearing more than 100 times in training.
We also wanted to control the effect of the rich
morphological information available in Hebrew
(gender and number marking, person, and so on).
To this end, we trained and tested each model ei-
ther with all the available morphological informa-
tion (+MORPH) or without any morphological in-
formation (-MORPH).
Evaluation Measure We evaluate the resulting
parses in terms of unlabeled accuracy – the percent
of correctly identified (child,parent) pairs4. To be
precise, we calculate:
number of correctly identified pairs
number of pairs in gold parse
For the oracle case in which the gold-standard
token segmentation is available for the parser, this
is the same as the traditional unlabeled-accuracy
evaluation metric. However, in the real-word set-
ting in which the token segmentation is done auto-
matically, the yields of the gold-standard and the
</bodyText>
<footnote confidence="0.995501">
2http://w3.msi.vxu.se/∼jha/maltparser/
3http://sourceforge.net/projects/mstparser/
4All the results are macro averaged. The micro-averaged
numbers are about 2 percents higher for all cases.
</footnote>
<page confidence="0.990728">
131
</page>
<table confidence="0.997664857142857">
MALT MALT-ARA
80.77 80.32
79.69 79.40
78.66 78.56
80.77 80.73
79.69 79.84
78.66 78.56
</table>
<tableCaption confidence="0.9945685">
Table 1: Unlabeled dependency accuracy with
oracle token segmentation and POS-tagging.
</tableCaption>
<table confidence="0.999486">
MALT MALT-ARA
73.03 72.94
72.04 71.88
70.93 70.73
73.03 73.43
72.04 72.30
70.93 70.97
</table>
<tableCaption confidence="0.9860185">
Table 2: Unlabeled dependency accuracy with
automatic token segmentation and POS-tagging.
</tableCaption>
<bodyText confidence="0.9953885">
automatic parse may differ, and one needs to de-
cide how to handle the cases in which one or more
elements in the identified (child,parent) pair are
not present in the gold-standard parse. Our evalua-
tion metric penalizes these cases by regarding any
such case as a mistake.
</bodyText>
<sectionHeader confidence="0.999056" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<bodyText confidence="0.997376555555556">
Results are presented in Tables 1 and 2.
It seems that the graph-based parsers perform
better than the transitions-based ones. We at-
tribute this to 2 factors: first, our representa-
tion of coordinated structure is hard to capture
with a greedy local search as performed by a
transition-based parser, because we need to de-
fer many attachment decisions until the final co-
ordinator is revealed. The global inference of the
graph-based parser is much more robust to these
kinds of structure. Indeed, when evaluating the
gold-morphology, fully-lexicalized models on a
subset of the test-set (314 sentences) which does
not have coordinated structures, the accuracy of
MALT improves in 3.98% absolute (from 80.77 to
84.75), while MST improves only in 2.66% abso-
lute (from 83.60 to 86.26). Coordination is hard
for both parsing models, but more so to the transi-
tion based MALT.
Second, it might be hard for a transition-based
parser to handle the free constituent order of He-
brew, as it has no means of generalizing from the
training set to various possible constituent order-
ing. The graph-based parser’s features and infer-
ence method do not take constituent order into ac-
count, making it more suitable for free constituent
order language.
As expected, the Second-order graph based
models perform better than the first-order ones.
Surprisingly, the Arabic-optimized feature-set do
not perform better than the English one for the
transition-based parsers. Overall, morphological
information seems to contribute very little (if at
all) to any of the parsers in the gold-morphology
(oracle) setting. MALTARA gets some benefit
from the morphological information in the fully-
lexicalized case, while the MST variants benefit
from morphology in the lexically-pruned models.
Overall, full lexicalization is not needed. In-
deed, less lexicalized LEX20 2nd-order graph-
based models perform better than the fully lexi-
calized ones. This strengthens our intuition that
robust lexical statistics are hard to acquire from
small annotated corpora, even more so for a lan-
guage with productive morphology such as He-
brew.
Moving from the oracle morphological disam-
biguation to an automatic one greatly hurts the per-
formance of all the models. This is in line with re-
sults for Hebrew constituency parsing, where go-
ing from gold segmentation to a parser derived one
caused a similar drop in accuracy (Golderg et al.,
2009). This suggests that we should either strive
to improve the tagging accuracy, or perform joint
inference for parsing and morphological disam-
biguation. We believe the later would be a better
way to go, but it is currently unsupported in state-
of-the-art dependency parsing algorithms.
Interestingly, in the automatic morphological
disambiguation setting MALTARA benefits a little
from the addition of morpological features, while
the MST models perform better without these fea-
tures.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9996704">
We presented the first results for unlabeled de-
pendency parsing of Hebrew, with two state-of-
the-art dependency parsing models of different
families. We experimented both with gold mor-
phological information, and with an automatically
derived one. It seems that graph-based models
have a slight edge in parsing Hebrew over current
transition-based ones. Both model families are not
currently making good use of morphological infor-
mation.
</bodyText>
<figure confidence="0.997235566666667">
Features MST1 MST2
Full Lex
Lex 20
Lex 100
Full Lex
Lex 20
Lex 100
83.60 84.31
82.99 84.52
82.56 83.12
83.60 84.39
83.60 84.77
83.23 83.80
-MORPH
+MORPH
Features MST1 MST2
Full Lex
Lex 20
Lex 100
Full Lex
Lex 20
Lex 100
75.64 76.38
75.48 76.41
74.97 75.49
73.90 74.62
73.56 74.41
72.90 73.78
-MORPH
+MORPH
</figure>
<page confidence="0.985269">
132
</page>
<sectionHeader confidence="0.997557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999823977777778">
Meni Adler. 2007. Hebrew Morphological Disam-
biguation: An Unsupervised Stochastic Word-based
Approach. Ph.D. thesis, Ben-Gurion University of
the Negev, Beer-Sheva, Israel.
Sabine Buchholz and Marsi Erwin. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of LREC.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2006. Noun phrase chunking in hebrew: Influence
of lexical and morphological features. In Proc. of
COLING/ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-Taggers
(when given a good start). In Proc. of ACL.
Yoav Golderg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing
performance using a wide coverage lexicon, fuzzy
tag-set mapping, and EM-HMM-based lexical prob-
abilities. In Proc of EACL.
Noemie Guthmann, Yuval Krymolowski, Adi Milea,
and Yoad Winter. 2009. Automatic annotation of
morpho-syntactic dependencies in a modern hebrew
treebank. In Proc of TLT.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proc. of EMNLP.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proc. of LREC.
Joakim Nivre, Johan Hall, Sandra Kubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of the EMNLP-CoNLL.
Reut Tsarfaty and Khalil Sima’an. 2008. Relational-
realizational parsing. In Proc. of CoLING, August.
Reut Tsarfaty. 2006. Integrated morphological and
syntactic disambiguation for modern hebrew. In
Proceedings of ACL-SRW.
</reference>
<page confidence="0.999147">
133
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.292500">
<title confidence="0.8105895">Hebrew Dependency Parsing: Initial Results Goldberg</title>
<author confidence="0.460165">Ben Gurion University of the</author>
<affiliation confidence="0.973863">Department of Computer</affiliation>
<address confidence="0.812658">POB 653 Be’er Sheva, 84105,</address>
<abstract confidence="0.99834052173913">We describe a newly available Hebrew Dependency Treebank, which is extracted from the Hebrew (constituency) Treebank. We establish some baseline unlabeled dependency parsing performance on Hebrew, based on two state-of-the-art parsers, MST-parser and MaltParser. The evaluation is performed both in an artificial setting, in which the data is assumed to be properly morphologically segmented and POS-tagged, and in a real-world setting, in which the parsing is performed on automatically segmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than Malt- Parser, and (b) both parsers do not make good use of morphological information when parsing Hebrew.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
</authors>
<title>Hebrew Morphological Disambiguation: An Unsupervised Stochastic Word-based Approach.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Ben-Gurion University of the Negev,</institution>
<location>Beer-Sheva, Israel.</location>
<contexts>
<context position="1284" citStr="Adler, 2007" startWordPosition="186" endWordPosition="187">g is performed on automatically segmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not fi</context>
<context position="4819" citStr="Adler, 2007" startWordPosition="743" endWordPosition="744">o reliably estimate lexical parameters from annotated corpora. The root+template system (combined with the unvocalized writing system) makes it hard to guess the morphological analyses of an unknown word based on its prefix and suffix, as usually done in other languages. Unvocalized writing system Most vowels are not marked in everyday Hebrew text, which results in a very high level of lexical and morphological ambiguity. Some tokens can admit as many as 15 distinct readings, and the average number of possible morphological analyses per token in Hebrew text is 2.7, compared to 1.4 in English (Adler, 2007). This means that on average, every token is ambiguous with respect to its POS and morphological features. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree in Gender and Number and definiteness), and between Subjects and Verbs (which should agree in Gender and Number). 2 Hebrew Dependency Treebank Our experiments are based on the Hebrew Dependency Treebank (henceforth DepTB), which we derived from Version 2 of the Hebrew Constituency Treebank (Guthmann et al., 2009) (henceforth TBv2). We briefly discuss the conversion process and the resu</context>
<context position="10496" citStr="Adler, 2007" startWordPosition="1676" endWordPosition="1677">nk as our training set, and report results on parsing the development set, Section 1 (sentences 0-483). We do not evaluate on the test set in this work. The data in the Treebank is segmented and POS-tagged. All of the models were trained on the gold-standard segmented and tagged data. When evaluating the parsing models, we perform two sets of evaluations. The first one is an oracle experiment, assuming gold segmentation and tagging is available. The second one is a real-world experiment, in which we segment and POS-tag the test-set sentences using the morphological disambiguator described in (Adler, 2007; Goldberg et al., 2008) prior to parsing. Parsers and parsing models We use the freely available implementation of MaltParser2 and MSTParser3, with default settings for each of the parsers. For MaltParser, we experiment both with the default feature representation (MALT) and the feature representation used for parsing Arabic in CoNLL 2006 and 2007 multilingual dependency parsing shared tasks (MALT-ARA). For MST parser, we experimented with firstorder (MST1) and second-order (MST2) models. We varied the amount of lexical information available to the parser. Each of the parsers was trained on 3</context>
</contexts>
<marker>Adler, 2007</marker>
<rawString>Meni Adler. 2007. Hebrew Morphological Disambiguation: An Unsupervised Stochastic Word-based Approach. Ph.D. thesis, Ben-Gurion University of the Negev, Beer-Sheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Marsi Erwin</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1613" citStr="Buchholz and Erwin, 2006" startWordPosition="233" endWordPosition="236">arsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, or the specific properties of Arabic making it easy or hard to parse in comparison to other languages. Our aim is to evaluate current state-of-the-art dependency parsers and approaches on Hebrew dependency parsing, to understand some of</context>
</contexts>
<marker>Buchholz, Erwin, 2006</marker>
<rawString>Sabine Buchholz and Marsi Erwin. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>Noun phrase chunking in hebrew: Influence of lexical and morphological features.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL.</booktitle>
<contexts>
<context position="1328" citStr="Goldberg et al., 2006" startWordPosition="190" endWordPosition="193">gmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of </context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2006</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2006. Noun phrase chunking in hebrew: Influence of lexical and morphological features. In Proc. of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-Taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10520" citStr="Goldberg et al., 2008" startWordPosition="1678" endWordPosition="1681">ining set, and report results on parsing the development set, Section 1 (sentences 0-483). We do not evaluate on the test set in this work. The data in the Treebank is segmented and POS-tagged. All of the models were trained on the gold-standard segmented and tagged data. When evaluating the parsing models, we perform two sets of evaluations. The first one is an oracle experiment, assuming gold segmentation and tagging is available. The second one is a real-world experiment, in which we segment and POS-tag the test-set sentences using the morphological disambiguator described in (Adler, 2007; Goldberg et al., 2008) prior to parsing. Parsers and parsing models We use the freely available implementation of MaltParser2 and MSTParser3, with default settings for each of the parsers. For MaltParser, we experiment both with the default feature representation (MALT) and the feature representation used for parsing Arabic in CoNLL 2006 and 2007 multilingual dependency parsing shared tasks (MALT-ARA). For MST parser, we experimented with firstorder (MST1) and second-order (MST2) models. We varied the amount of lexical information available to the parser. Each of the parsers was trained on 3 datasets: LEXFULL, in w</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM can find pretty good HMM POS-Taggers (when given a good start). In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Golderg</author>
<author>Reut Tsarfaty</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities.</title>
<date>2009</date>
<booktitle>In Proc of EACL.</booktitle>
<contexts>
<context position="1400" citStr="Golderg et al., 2009" startWordPosition="200" endWordPosition="203">into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, or the specific propert</context>
<context position="8292" citStr="Golderg et al., 2009" startWordPosition="1308" endWordPosition="1312">remain unlabeled. We are currently seeking a method of reliably labeling the remaining edges with a rich set 1A possible alternative would be to allow multiple parents, as done in (de Marneffe et al., 2006), but current parsing algorithms require the output to be tree structured. 130 of relations. However, in the current work we focus on the unlabeled dependency structure. POS tags The Hebrew Treebank follows a syntactic tagging scheme, while other Hebrew resources prefer a more morphological/dictionarybased scheme. For a discussion of these two tagging schemes in the context of parsing, see (Golderg et al., 2009). In DepTB, we kept the two tagsets, and each token has two POS tags associated with it. However, as current dependency parsers rely on an external POS tagger, we performed all of our experiments only with the morphological tagset, which is what our tagger produces. 3 The Parsing Models To establish some baseline results for Hebrew dependency parsing, we experiment with two parsing models, the graph-based MST-parser (McDonald, 2006) and the transition-based MaltParser (Nivre et al., 2006). These two parsers represent the current mainstream approaches for dependency parsing, and each was shown </context>
<context position="15435" citStr="Golderg et al., 2009" startWordPosition="2451" endWordPosition="2454">ll lexicalization is not needed. Indeed, less lexicalized LEX20 2nd-order graphbased models perform better than the fully lexicalized ones. This strengthens our intuition that robust lexical statistics are hard to acquire from small annotated corpora, even more so for a language with productive morphology such as Hebrew. Moving from the oracle morphological disambiguation to an automatic one greatly hurts the performance of all the models. This is in line with results for Hebrew constituency parsing, where going from gold segmentation to a parser derived one caused a similar drop in accuracy (Golderg et al., 2009). This suggests that we should either strive to improve the tagging accuracy, or perform joint inference for parsing and morphological disambiguation. We believe the later would be a better way to go, but it is currently unsupported in stateof-the-art dependency parsing algorithms. Interestingly, in the automatic morphological disambiguation setting MALTARA benefits a little from the addition of morpological features, while the MST models perform better without these features. 6 Conclusions We presented the first results for unlabeled dependency parsing of Hebrew, with two state-ofthe-art depe</context>
</contexts>
<marker>Golderg, Tsarfaty, Adler, Elhadad, 2009</marker>
<rawString>Yoav Golderg, Reut Tsarfaty, Meni Adler, and Michael Elhadad. 2009. Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities. In Proc of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noemie Guthmann</author>
<author>Yuval Krymolowski</author>
<author>Adi Milea</author>
<author>Yoad Winter</author>
</authors>
<title>Automatic annotation of morpho-syntactic dependencies in a modern hebrew treebank.</title>
<date>2009</date>
<booktitle>In Proc of TLT.</booktitle>
<contexts>
<context position="5345" citStr="Guthmann et al., 2009" startWordPosition="822" endWordPosition="825">morphological analyses per token in Hebrew text is 2.7, compared to 1.4 in English (Adler, 2007). This means that on average, every token is ambiguous with respect to its POS and morphological features. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree in Gender and Number and definiteness), and between Subjects and Verbs (which should agree in Gender and Number). 2 Hebrew Dependency Treebank Our experiments are based on the Hebrew Dependency Treebank (henceforth DepTB), which we derived from Version 2 of the Hebrew Constituency Treebank (Guthmann et al., 2009) (henceforth TBv2). We briefly discuss the conversion process and the resulting Treebank: Parent-child dependencies TBv2 marks several kinds of dependencies, indicating the motherdaughter percolation of features such as number, gender, definiteness and accusativity. See (Guthmann et al., 2009) for the details. We follow TBv2’s HEAD, MAJOR and MULTIPLE dependency marking in our-head finding rules. When these markings are not available we use head finding rules in the spirit of Collins. The head-finding rules were developed by Reut Tsarfaty and used in (Tsarfaty and Sima’an, 2008). We slightly e</context>
</contexts>
<marker>Guthmann, Krymolowski, Milea, Winter, 2009</marker>
<rawString>Noemie Guthmann, Yuval Krymolowski, Adi Milea, and Yoad Winter. 2009. Automatic annotation of morpho-syntactic dependencies in a modern hebrew treebank. In Proc of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9566" citStr="McDonald and Nivre, 2007" startWordPosition="1525" endWordPosition="1528">ages (CoNLL Shared Task 2006, 2007). Briefly, a graph-based parsing model works by assigning a score to every possible attachment between a pair (or a triple, for a second-order model) of words, and then inferring a global tree structure that maximizes the sum of these local scores. Transition-based models work by building the dependency graph in a sequence of steps, where each step is dependent on the next input word(s), the previous decisions, and the current state of the parser. For more details about these parsing models as well as a discussion on the relative benefits of each model, see (McDonald and Nivre, 2007). Contrary to constituency-based parsers, dependency parsing models expect a morphologically segmented and POS tagged text as input. 4 Experiments Data We follow the train-test-dev split established in (Tsarfaty and Sima’an, 2008). Specifically, we use Sections 2-12 (sentences 484-5724) of the Hebrew Dependency Treebank as our training set, and report results on parsing the development set, Section 1 (sentences 0-483). We do not evaluate on the test set in this work. The data in the Treebank is segmented and POS-tagged. All of the models were trained on the gold-standard segmented and tagged d</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Training and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8728" citStr="McDonald, 2006" startWordPosition="1386" endWordPosition="1388">hile other Hebrew resources prefer a more morphological/dictionarybased scheme. For a discussion of these two tagging schemes in the context of parsing, see (Golderg et al., 2009). In DepTB, we kept the two tagsets, and each token has two POS tags associated with it. However, as current dependency parsers rely on an external POS tagger, we performed all of our experiments only with the morphological tagset, which is what our tagger produces. 3 The Parsing Models To establish some baseline results for Hebrew dependency parsing, we experiment with two parsing models, the graph-based MST-parser (McDonald, 2006) and the transition-based MaltParser (Nivre et al., 2006). These two parsers represent the current mainstream approaches for dependency parsing, and each was shown to provide state-of-the-art results on many languages (CoNLL Shared Task 2006, 2007). Briefly, a graph-based parsing model works by assigning a score to every possible attachment between a pair (or a triple, for a second-order model) of words, and then inferring a global tree structure that maximizes the sum of these local scores. Transition-based models work by building the dependency graph in a sequence of steps, where each step i</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nillson</author>
</authors>
<title>MaltParser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="8785" citStr="Nivre et al., 2006" startWordPosition="1393" endWordPosition="1396">al/dictionarybased scheme. For a discussion of these two tagging schemes in the context of parsing, see (Golderg et al., 2009). In DepTB, we kept the two tagsets, and each token has two POS tags associated with it. However, as current dependency parsers rely on an external POS tagger, we performed all of our experiments only with the morphological tagset, which is what our tagger produces. 3 The Parsing Models To establish some baseline results for Hebrew dependency parsing, we experiment with two parsing models, the graph-based MST-parser (McDonald, 2006) and the transition-based MaltParser (Nivre et al., 2006). These two parsers represent the current mainstream approaches for dependency parsing, and each was shown to provide state-of-the-art results on many languages (CoNLL Shared Task 2006, 2007). Briefly, a graph-based parsing model works by assigning a score to every possible attachment between a pair (or a triple, for a second-order model) of words, and then inferring a global tree structure that maximizes the sum of these local scores. Transition-based models work by building the dependency graph in a sequence of steps, where each step is dependent on the next input word(s), the previous decis</context>
</contexts>
<marker>Nivre, Hall, Nillson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nillson. 2006. MaltParser: A data-driven parser-generator for dependency parsing. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra Kubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proc. of the EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1634" citStr="Nivre et al., 2007" startWordPosition="237" endWordPosition="240">e of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, or the specific properties of Arabic making it easy or hard to parse in comparison to other languages. Our aim is to evaluate current state-of-the-art dependency parsers and approaches on Hebrew dependency parsing, to understand some of the difficulties in </context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc. of the EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Sima’an</author>
</authors>
<title>Relationalrealizational parsing.</title>
<date>2008</date>
<booktitle>In Proc. of CoLING,</booktitle>
<marker>Tsarfaty, Sima’an, 2008</marker>
<rawString>Reut Tsarfaty and Khalil Sima’an. 2008. Relationalrealizational parsing. In Proc. of CoLING, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
</authors>
<title>Integrated morphological and syntactic disambiguation for modern hebrew.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-SRW.</booktitle>
<contexts>
<context position="1377" citStr="Tsarfaty, 2006" startWordPosition="198" endWordPosition="199">sure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, </context>
</contexts>
<marker>Tsarfaty, 2006</marker>
<rawString>Reut Tsarfaty. 2006. Integrated morphological and syntactic disambiguation for modern hebrew. In Proceedings of ACL-SRW.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>