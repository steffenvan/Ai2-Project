<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.415788">
CONE: Metrics for Automatic Evaluation of Named Entity
Co-reference Resolution
</title>
<author confidence="0.755148">
Bo Lin, Rushin Shah, Robert Frederking, Anatole Gershman
</author>
<affiliation confidence="0.7794875">
Language Technologies Institute, School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.731322">
5000 Forbes Ave., PA 15213, USA
</address>
<email confidence="0.991111">
{bolin,rnshah,ref,anatoleg}@cs.cmu.edu
</email>
<sectionHeader confidence="0.982116" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.97935915625">
Human annotation for Co-reference Resolu-
tion (CRR) is labor intensive and costly, and
only a handful of annotated corpora are cur-
rently available. However, corpora with
Named Entity (NE) annotations are widely
available. Also, unlike current CRR systems,
state-of-the-art NER systems have very high
accuracy and can generate NE labels that are
very close to the gold standard for unlabeled
corpora. We propose a new set of metrics col-
lectively called CONE for Named Entity Co-
reference Resolution (NE-CRR) that use a
subset of gold standard annotations, with the
advantage that this subset can be easily ap-
proximated using NE labels when gold stan-
dard CRR annotations are absent. We define
CONE B3 and CONE CEAF metrics based on
the traditional B3 and CEAF metrics and show
that CONE B3 and CONE CEAF scores of any
CRR system on any dataset are highly corre-
lated with its B3 and CEAF scores respectively.
We obtain correlation factors greater than 0.6
for all CRR systems across all datasets, and a
best-case correlation factor of 0.8. We also
present a baseline method to estimate the gold
standard required by CONE metrics, and show
that CONE B3 and CONE CEAF scores using
this estimated gold standard are also correlated
with B3 and CEAF scores respectively. We
thus demonstrate the suitability of CONE
B3and CONE CEAF for automatic evaluation
of NE-CRR.
</bodyText>
<sectionHeader confidence="0.992399" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999574595744681">
Co-reference resolution (CRR) is the problem of
determining whether two entity mentions in a
text refer to the same entity in real world or not.
Noun Phrase CRR (NP-CRR) considers all noun
phrases as entities, while Named Entity CRR
restricts itself to noun phrases that describe a
Named Entity. In this paper, we consider the task
of Named Entity CRR (NE-CRR) only. Most, if
not all, recent efforts in the field of CRR have
concentrated on machine-learning based ap-
proaches. Many of them formulate the problem
as a pair-wise binary classification task, in which
possible co-reference between every pair of men-
tions is considered, and produce chains of co-
referring mentions for each entity as their output.
One of the most important problems in CRR is
the evaluation of CRR results. Different evalua-
tion metrics have been proposed for this task. B-
cubed (Bagga and Baldwin, 1998) and CEAF
(Luo, 2005) are the two most popular metrics;
they compute Precision, Recall and F1 measure
between matched equivalent classes and use
weighted sums of Precision, Recall and F1 to
produce a global score. Like all metrics, B3 and
CEAF require gold standard annotations; howev-
er, gold standard CRR annotations are scarce,
because producing such annotations involves a
substantial amount of human effort since it re-
quires an in-depth knowledge of linguistics and a
high level of understanding of the particular text.
Consequently, very few corpora with gold stan-
dard CRR annotations are available (NIST, 2003;
MUC-6, 1995; Agirre, 2007). By contrast, gold
standard Named Entity (NE) annotations are easy
to produce; indeed, there are many NE annotated
corpora of different sizes and genres. Similarly,
there are few CRR systems and even the best
scores obtained by them are only in the region of
F1 = 0.5 - 0.6. There are only four such CRR
systems freely available, to the best of our know-
ledge (Bengston and Roth, 2007; Versley et al.,
2008; Baldridge and Torton, 2004; Baldwin and
Carpenter, 2003). In comparison, there are nu-
merous Named Entity recognition (NER) sys-
tems, both general-purpose and specialized, and
many of them achieve scores better than F1 =
0.95 (Ratinov and Roth, 2009; Finkel et al.,
</bodyText>
<page confidence="0.455844">
136
</page>
<note confidence="0.9958495">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136–144,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99920275">
2005). Although these facts can be partly attri-
buted to the ‘hardness’ of CRR compared to
NER, they also reflect the substantial gap be-
tween NER and CRR research. In this paper, we
present a set of metrics, collectively called
CONE, that leverage widely available NER sys-
tems and resources and tools for the task of eva-
luating co-reference resolution systems. The ba-
sic idea behind CONE is to predict a CRR sys-
tem’s performance for the task of full NE-CRR
on some dataset using its performance for the
subtask of named mentions extraction and group-
ing (NMEG) on that dataset. The advantage of
doing so is that measuring NE-CRR performance
requires the co-reference information of all men-
tions of a Named Entity, including named men-
tions, nominal and pronominal references, while
measuring the NMEG performance only requires
co-reference information of named mentions of a
NE, and this information is relatively easy to ob-
tain automatically even in the absence of gold
standard annotations. We compute correlation
between CONE B3, B3, CONE CEAF and CEAF
scores for various CRR systems on various gold-
standard annotated datasets and show that the
CONE B3 and B3 scores are highly correlated for
all such combinations of CRR systems and data-
sets, as are CONE CEAF and CEAF scores, with
a best-case correlation of 0.8. We produce esti-
mated gold standard annotations for the Enron
email corpus, since no actual gold standard CRR
annotations exist for it, and then use CONE B3
and CONE CEAF with these estimated gold
standard annotations to compare the performance
of various NE-CRR systems on this corpus. No
such comparison has been previously performed
for the Enron corpus.
We adopt the same terminology as in (Luo,
2005): a mention refers to each individual phrase
and an entity refers to the equivalence class or
co-reference chain with several mentions. This
allows us to note some differences between NE-
CRR and NP-CRR. NE-CRR involves indentify-
ing named entities and extracting their co-
referring mentions; equivalences classes without
any NEs are not considered. NE-CRR is thus
clearly a subset of NP-CRR, where all co-
referring mentions and equivalence classes are
considered. However, we focus on NE-CRR be-
cause it is currently a more active research area
than NP-CRR and a better fit for target applica-
tions such as text forensics and web mining, and
also because it is more amenable to the automatic
evaluation approach that we propose.
The research questions that motivate our work
are:
</bodyText>
<listItem confidence="0.997546375">
(1) Is it possible to use only NER resources to
evaluate NE-CRR systems? If so, how is this
problem formulated?
(2) How does one perform evaluation in a way
that is accurate and automatic with least hu-
man intervention?
(3) How does one perform evaluation on large
unlabeled datasets?
</listItem>
<bodyText confidence="0.99982845">
We show that our CONE metrics achieve good
results and represent a promising first step to-
ward answering these questions.
The rest of the paper is organized as follows. We
present related work in the field of automatic
evaluation methods for natural language
processing tasks in Section 2. In Section 3, we
give an overview of the standard metrics current-
ly used for evaluating co-reference resolution.
We define our new metrics CONE B3 and CONE
CEAF in Section 4. In section 5, we provide ex-
perimental results that illustrate the performance
of CONE B3 and CONE CEAF compared to B3
and CEAF respectively. In Section 6, we give an
example of the application of CONE metrics by
evaluating NE-CRR systems on an unlabeled
dataset, and discuss possible drawbacks and ex-
tensions of these metrics. Finally, in section 7 we
present our conclusions and ideas for future
work.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999975142857143">
There has been a substantial amount of research
devoted to automatic evaluation for natural lan-
guage processing, especially tasks involving lan-
guage generation. The BLEU score (Papineni et
al., 2002) proposed for evaluating machine trans-
lation results is the best known example of this.
It uses n-gram statistics between machine gener-
ated results and references. It inspired the
ROUGE metric (Lin and Hovy, 2003) and other
methods (Louis and Nenkova, 2009) to perform
automatic evaluation of text summarization. Both
these metrics have show strong correlation be-
tween automatic evaluation results and human
judgments. The two metrics successfully reduce
the need for human judgment and help speed up
research by allowing large-scale evaluation.
Another example is the alignment entropy (Per-
vouchine et al., 2009) for evaluating translitera-
tion alignment. It reduces the need for alignment
gold standard and highly correlates with transli-
teration system performance. Thus it is able to
</bodyText>
<page confidence="0.824824">
137
</page>
<bodyText confidence="0.999977153846154">
serve as a good metric for transliteration align-
ment. We contrast our work with (Stoyanov et al.,
2009), who show that the co-reference resolution
problem can be separated into different parts ac-
cording to the type of the mention. Some parts
are relatively easy to solve. The resolver per-
forms equally well in each part across datasets.
They use the statistics of mentions in different
parts with test results on other datasets as a pre-
dictor for unseen datasets, and obtain promising
results with good correlations. We approach the
problem from a different perspective. In our
work, we show the correlation between the
scores on traditional metrics and scores on our
CONE metrics, and show how to automatically
estimate the gold standard required by CONE
metrics. Thus our method is able to predict the
co-reference resolution performance without
gold standard at all. We base our new metrics on
the standard B3 and CEAF metrics used for com-
puting CRR scores. (Vilian et al., 1995; Bagga
and Baldwin, 1998; Luo, 2005). B3 and CEAF
are believed to be more discriminative and inter-
pretable than earlier metrics and are widely
adopted especially for machine-learning based
approaches.
</bodyText>
<sectionHeader confidence="0.918235" genericHeader="method">
3 Standard Metrics: B3 and CEAF
</sectionHeader>
<bodyText confidence="0.999914133333333">
We now provide an overview of the standard B3
and CEAF metrics used to evaluate CRR sys-
tems. Both metrics assume that a CRR system
produces a set of equivalence classes {O} and
assigns each mention to only one class. Let Oi be
the class to which the ith mention was assigned
by the system. We also assume that we have a set
of correct equivalence classes {G} (the gold
standard). Let Gi be the gold standard class to
which the ith mention should belong. Let Ni de-
note the number of mentions in Oi which are also
in Gi – the correct mentions. B3 computes the
presence rate of correct mentions in the same
equivalent classes. The individual precision and
recall score is defined as follows:
</bodyText>
<equation confidence="0.991397833333333">
P
|i
R
 |G;
i Ni ; N;
O
</equation>
<bodyText confidence="0.996712666666667">
Here |Oi |and |Gi |are the cardinalities of sets Oi
and Gi.
The final precision and recall scores are:
</bodyText>
<equation confidence="0.983326">
n n
wiPi R= wiRi
i=1
</equation>
<bodyText confidence="0.999977909090909">
Here, in the simplest case the weight wi is set to
1/n, equal for all mentions.
CEAF (Luo, 2005) produces the optimal
matching between output classes and true classes
first, with the constraint that one true class, Gi,
can be mapped to at most one output class, say
Of(i) and vice versa. This can be solved by the
KM algorithm (Kuhn, 1955; Munkres, 1957) for
maximum matching in a bipartite graph. CEAF
then computes the precision and recall score as
follows:
</bodyText>
<equation confidence="0.9451465">
( 0
M a f,
( )

i j
i j
</equation>
<bodyText confidence="0.999022">
We use the terms Mi,j from CEAF to re-write B3,
its formulas then reduce to:
</bodyText>
<equation confidence="0.95125325">


2
i
</equation>
<bodyText confidence="0.9997992">
We can see that B3 simply iterates through all
pairs of matchings instead of considering the one
to one mappings as CEAF does. Thus, B3 com-
putes the weighted sum of the F-measures for
each individual mention which helps alleviate the
bias in the pure link-based F-measure, while
CEAF computes the same as B3 but enforces at
most one matched equivalence class for every
class in the system output and gold standard out-
put.
</bodyText>
<sectionHeader confidence="0.456514" genericHeader="method">
4 CONE B3 and CONE CEAF Metrics:
</sectionHeader>
<bodyText confidence="0.999612764705882">
We now formally define the new CONE B3 and
CONE CEAF metrics that we propose for
automatic evaluation of NE-CRR systems.
Let G denote the set of gold standard
annotations and O denote the output of an NE-
CRR system. Let Gi denote the equivalent class
of entity i in the gold standard and Oj denote the
equivalence class for entity j in the system output.
Also let Gij denote the jth mention in the
equivalence class of entity i in the gold standard
and Oij denote the jth mention in the system
output.
As described earlier, the standard B3 and CEAF
metrics evaluate scores using G and O and can
be thought of as functions of the form B3(G, O).
and CEAF(G, O) respectively. Let us use
Score(G, O) to collectively refer to both these
</bodyText>
<figure confidence="0.987868171428572">
Ma


Mi ,
a
a
f
a
a
P
Oa
i
Ga

a
R

a
j
1
P
Oi
Oi

2
1
Mi,
j
R
Gi
Gi
P

i1
138
</figure>
<bodyText confidence="0.999630333333334">
functions. An equivalence class Gi in G may
contain three types of mentions: named mentions
gNMij, nominal mentions gNOij, and pronominal
mentions gPRij. Similarly, we can define oNMij,
oNOij and oPRij for a class Oi in O. Now for each
gold standard equivalence class Gi and system
output equivalence class Oi, we define the
following sets GNMi and ONMi:
In other words, GNMi and ONMi are the subsets of
Gi and Oi containing all named mentions and no
mentions of any other type.
Let GNM denote the set of all such equivalance
classes GNMi and ONM denote the set of all
equivalence classes ONMi. It is clear that GNM and
ONM are pruned versions of the gold standard
annotations and system output respectively.
We now define CONE B3 and CONE CEAF as
follows:
</bodyText>
<equation confidence="0.8991825">
CONE B3 = B3(GNM, ONM)
CONE CEAF = CEAF(GNM, ONM)
</equation>
<bodyText confidence="0.999951625">
Following our previous notation, we denote
CONE B3 and CONE CEAF collectively as
Score(GNM, ONM). We observe that Score(GNM,
ONM) measures a NE-CRR system’s
performance for the NE-CRR subtask of named
mentions extraction and grouping (NMEG). We
find that Score(GNM, ONM) is highly correlated
with Score(G, O) for all the freely available NE-
CRR systems over various datasets. This
provides the neccessary justification for the use
of Score(GNM, ONM).
We use SYNERGY (Shah et al., 2010), an
ensemble NER system that combines the UIUC
NER (Ritanov and Roth, 2009) and Stanford
NER (Finkel et al., 2005) systems, to produce
GNM and ONM from G and O by selecting named
mentions. However, any other good NER system
would serve the same purpose.
We see that while standard evaluation metrics
require the use of G, i.e. the full set of NE-CRR
gold standard annotations including named,
nominal and pronimal mentions, CONE metrics
require only GNM, i.e. gold standard annotations
consisting of named mentions only. The key
advantage of using CONE metrics is that GNM
can be automatically approximated using an
NER system with a good degree of accuracy.
This is because state-of-the-art NER systems
achieve near-optimal performance, exceeding F1
= 0.95 in many cases, and after obtaining their
output, the task of estimating GNM reduces to
simply clustering it to seperate mentions of
diffrerent real-world entities. This clustering can
be thought of as a form of named entity matching,
which is not a very hard problem. There exist
systems that perform such matching in a
sophisticated manner with a high degree of
accuracy. We use simple heuristics such as exact
matching, word matches, matches between in-
itials, etc. to design such a matching system
ourselves and use it to obtain estimates of GNM,
say GNM-approx. We then calculate CONE B3 and
CONE CEAF scores using GNM-approx instead of
GNM; in other words, we perform fully automatic
evaluation of NE-CRR systems by using
Score(GNM-approx, ONM) instead of Score(GNM,
ONM). In order to show the validity of this
evaluation, we calculate the correlation between
the Score(GNM-approx, ONM) and Score(G, O) for
different NE-CRR systems across different
datasets and find that they are indeed correlated.
CONE thus makes automatic evaluation of NE-
CRR systems possible. By leveraging the widely
available named entity resources, it reduces the
need for gold standard annotations in the
evaluation process.
</bodyText>
<subsectionHeader confidence="0.992915">
4.1 Analysis
</subsectionHeader>
<bodyText confidence="0.960242333333333">
There are two major kinds of errors that affect
the performance of NE-CRR systems for the full
NE-CRR task:
</bodyText>
<listItem confidence="0.9930018">
• Missing Named Entity (MNE): If a named
mention is missing from the system output,
it is very likely that its nearby nominal and
anaphoric mentions will be lost, too
• Incorrectly grouped Named Entity (IGNE):
</listItem>
<bodyText confidence="0.987183777777778">
Even if the named mention is correctly iden-
tified with its nearby nominal and anaphoric
mentions to form a chain, it is still possible
to misclassify the named mentions and its
co-reference chain
Consider the following example of these two
types of errors. Here, the alphabets represent the
named mentions and numbers represent other
type of mentions:
Gold standard, G: (A, B, C, 1, 2, 3, 4)
Output from System 1, O1: (A, B, 1, 2, 3)
Output from System 2, O2: (A, C, 1, 2, 4), (B, 3)
O1 shows an example of an MNE error, while
O2 shows an example of an IGNE error.
Both these types of errors are in fact rooted in
named mention extraction and grouping
(NMEG). Therefore, we hypothesize that they
must be preserved in a NE-CRR system’s output
</bodyText>
<page confidence="0.772485">
139
</page>
<bodyText confidence="0.999789">
for the subtask of named mentions extraction and
grouping (NMEG) and will be reflected in the
CONE B3 and CONE CEAF metrics that eva-
luate scores for this subtask. Consider the follow-
ing extension of the previous example:
</bodyText>
<construct confidence="0.441166333333333">
GNM: (A, B, C)
O1NM: (A, B)
O2NM: (A, C), (B)
</construct>
<bodyText confidence="0.999392862068965">
In order to support the hypothesis that CONE
metrics evaluated using (GNM, ONM) represent an
effective substitute for standard metrics that use
(G, O), we compute entity level correlation be-
tween the corresponding CONE and standard
metrics. For example, in the case of CEAF /
CONE CEAF Precision, we calculate correlation
between the following quantities:
and
We observe that the MNE error in O1 is pre-
served in O1NM, and the IGNE error in O2 is pre-
served in O2NM. Empirically we sample several
output files in our experiments and observe the
same phenomena. Therefore, we argue that it is
possible to capture the two major kinds of errors
described by considering only GNM and ONM in-
stead of G and O.
We now provide a more detailed theoretical
analysis of the CONE metrics. For a given NE-
CRR system and dataset, consider the system
output O and gold standard annotation G. Let P
and R indicate precision and recall scores ob-
tained by evaluating O against G, using CEAF. If
we replace both G and O with their subsets GNM
and ONM respectively, such that GNM and ONM
contain only named mentions, we can modify the
equations for precision and recall for CEAF to
derive the following equations for precision PNM
and recall RNM for CONE CEAF:
</bodyText>
<equation confidence="0.7658992">

( )
( )
Sum{GNM }

G
NM
i
P
NM

i
Sum{ONM }
M
i
NM
i,
f
i
R
NM

i
Sum{GNM}
M
NM
i,
f
i
The corresponding equations for CONE B3 Pre-
cision are:

M
NM
i,
j
2
P
NM

i
O
NM
j
i
 Sum{ONM I
E
M
NM
_,
i
2
R&apos;
E
_
R
NM
i
_
X Sum{RNM}
</equation>
<bodyText confidence="0.700563666666667">
We perform this experiment with the LBJ and
BART CRR systems on the ACE Phase 2 corpus.
We illustrate the correlation results in Figure 1.
</bodyText>
<figureCaption confidence="0.995868">
Figure 1. Correlation between and -
</figureCaption>
<bodyText confidence="0.981342235294118">
Entity Level CEAF Precision
From Figure 1, we can see that the two
measures are highly correlated. In fact, we find
that the Pearson’s correlation coefficient (Soper
et al., 1917; Cohen, 1988) is 0.73. The points
lining up on the x-axis and y=1.0 represent very
small equivalence classes and are a form of noise;
their removal doesn’t affect this coefficient. To
show that this strong correlation is not a
statistical anomaly, we also compute entity-level
correlation using (Gi - GNMi, Oj - ONMj) and (Gi,
Oj) instead of (GNM i, ONMj) and (Gi, Oj) and find
that the coefficient drops to 0.03, which is
obviously not correlated at all.
We now know and P— are highly correlated.
Assume the correlation is linear, with the
following equation:
</bodyText>
<equation confidence="0.990051833333333">
 a i 
P NM
P = � = �
P i
i
Pi 
</equation>
<bodyText confidence="0.998684">
where α and R are the linear regression
parameters.
</bodyText>
<equation confidence="0.892609666666667">
Thus
(aPNMi +,3)= naPNM + n,3
i
</equation>
<bodyText confidence="0.849863">
Here, n is the number of equivalence classes.
We conclude that the overall CEAF Precision
and CONE CEAF Precision should be highly
</bodyText>
<page confidence="0.687305">
140
</page>
<bodyText confidence="0.99972468">
correlated too. We repeat this experiment with
CEAF / CONE CEAF Recall, B3 / CONE B3
Precision and B3 / CONE B3 Recall and obtain
similar results, allowing us to conclude that these
sets of measures should also be highly correlated.
We note here some generally accepted
terminology regarding correlation: If two
quantities have a Pearson’s correlation
coefficient greater than 0.7, they are considered
&amp;quot;strongly correlated&amp;quot;, if their correlation is
between 0.5 and 0.7, they are considered &amp;quot;highly
correlated&amp;quot;, if it is between 0.3 and 0.5, they are
considered &amp;quot;correlated&amp;quot;, and otherwise they are
considered &amp;quot;not correlated&amp;quot;.
It is important to note that like all automatic
evaluation metrics, CONE B3 and CONE CEAF
too can be easily ‘cheated’, e.g. a NE-CRR sys-
tem that performs NER and named entity match-
ing well but does not even detect and classify
anaphora or nominal mentions would nonethe-
less score highly on these metrics. A possible
solution to this problem would be to create gold
standard annotations for a small subset of the
data, call these annotations G’, and report two
scores: B3 / CEAF (G’), and CONE B3 / CONE
CEAF (GNM-approx). Discrepancies between these
two scores would enable the detection of such
‘cheating’. A related point is that designers of
NE-CRR systems should not optimize for CONE
metrics alone, since by using GNM-approx (or GNM
where gold standard annotations are available),
these metrics are obviously biased towards
named mentions. This issue can also be ad-
dressed by having gold standard annotations G’
for a small subset. One could then train a system
by optimizing both B3 / CEAF (G’) and CONE
B3 / CONE CEAF (GNM-approx). This can be
thought of as a form of semi-supervised learning,
and may be useful in areas such as domain adap-
tation, where we could use some annotated test-
set in a standard domain, e.g. newswire as the
smaller set and an unlabeled large testset from
some other domain, such as e-mail or biomedical
documents. An interesting future direction is to
monitor the effectiveness of our metrics over
time. As co-reference resolution systems evolve
in strength, our metrics might be less effective,
however this could be a good indicator to discri-
minate on different subtasks the improvements
gained by the co-reference resolution systems.
</bodyText>
<sectionHeader confidence="0.980467" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999428944444444">
We present experimental results in support of the
validity and effectiveness of CONE metrics. As
mentioned earlier, we used the following four
publicly available CRR systems: UIUC’s LBJ
system (L), BART from JHU Summer Workshop
(B), LingPipe from Alias-i (LP), and OpenNLP
(OP) (Bengston and Roth, 2007; Versley et al.,
2008; Baldridge and Torton, 2004; Baldwin and
Carpenter, 2003). All these CRR systems per-
form Noun Phrase co-reference resolution (NP-
CRR), not NE-CRR. So, we must first eliminate
all equivalences classes that do not contain any
named mentions. We do so using the SYNERGY
NER system to separate named mentions from
unnamed ones. Note that this must not be con-
fused with the use of SYNERGY to produce GNM
and ONM from G and O respectively. For that task,
all equivalence classes in G and O already con-
tain at least one named mention and we remove
all unnamed mentions from each class. This
process effectively converts the NP-CRR results
of these systems into NE-CRR ones. We use the
ACE Phase 2 NWIRE and ACE 2005 English
datasets. We avoid using the ACE 2004 and
MUC6 datasets because the UIUC LBJ system
was trained on ACE 2004 (Bengston and Roth,
2008), while BART and LingPipe were trained
on MUC6. There are 29 files in the test set of
ACE Phrase 2 and 81 files in ACE 2005, sum-
ming up to 120 files with around 50,000 tokens
with 5000 valid co-reference mentions. Tables 1
and 2 show the Pearson’s correlation coefficients
between CONE metric scores of the type
Score(GNM, ONM) and standard metric scores of
the type Score(G, O) for combinations of various
CRR systems and datasets.
</bodyText>
<table confidence="0.998748333333333">
B3/CONE B3 CEAF/CONE CEAF
P R F1 P R F1
L 0.82 0.71 0.7 0.81 0.71 0.77
B 0.85 0.5 0.66 0.71 0.61 0.68
LP 0.84 0.66 0.67 0.74 0.71 0.73
OP 0.31 0.57 0.61 0.79 0.72 0.79
</table>
<tableCaption confidence="0.994474">
Table 1. GNM: Correlation on ACE Phase 2
</tableCaption>
<table confidence="0.999885666666667">
B3/CONE B3 CEAF/CONE CEAF
P R F1 P R F1
L 0.6 0.62 0.62 0.75 0.61 0.68
B 0.74 0.82 0.84 0.72 0.68 0.67
LP 0.91 0.65 0.73 0.44 0.57 0.53
OP 0.48 0.77 0.8 0.54 0.67 0.65
</table>
<tableCaption confidence="0.999594">
Table 2. GNM: Correlation on ACE 2005
</tableCaption>
<bodyText confidence="0.957543">
We observe from Tables 1 and 2 that CONE B3
and CONE CEAF scores are highly correlated
</bodyText>
<page confidence="0.820037">
141
</page>
<bodyText confidence="0.999868266666667">
with B3 and CEAF scores respectively, and this
holds true for Precision, Recall and F1 scores, for
all combinations of CRR systems and datasets.
This justifies our assumption that a system’s per-
formance for the subtask of NMEG is a good
predictor of its performance for the full task of
NE-CRR. These correlation coefficients are
graphically illustrated in Figures 2 and 3.
We now use our baseline named entity matching
method to automatically generate estimated gold
standard annotations GNM-approx and recalculate
CONE CEAF and CONE B3 scores using GNM-
approx instead of GNM. Tables 3 and 4 show the
correlation coefficients between the new CONE
scores and the standard metric scores.
</bodyText>
<table confidence="0.998933666666667">
B3/CONE B3 CEAF/CONE CEAF
P R F1 P R F1
L 0.31 0.23 0.22 0.33 0.55 0.56
B 0.71 0.44 0.43 0.61 0.63 0.71
LP 0.57 0.43 0.49 0.36 0.25 0.31
OP 0.1 0.6 0.64 0.35 0.53 0.53
</table>
<tableCaption confidence="0.963991">
Table 3. GNM-approx: Correlation on ACE Phase 2
</tableCaption>
<table confidence="0.999870166666667">
B3/CONE B3 CEAF/CONE CEAF
P R F1 P R F1
L 0.33 0.32 0.42 0.22 0.34 0.36
B 0.25 0.66 0.65 0.2 0.45 0.37
LP 0.19 0.33 0.34 0.77 0.68 0.72
OP 0.26 0.66 0.67 0.28 0.42 0.38
</table>
<tableCaption confidence="0.895106">
Table 4. GNM-approx: Correlation on ACE Phase 2
</tableCaption>
<bodyText confidence="0.9996554">
We observe from Tables 3 and 4 that these corre-
lation factors are encouraging, but not as good as
those in Tables 1 and 2. All the corresponding
CONE B3 and CONE CEAF scores are corre-
lated, but very few are highly correlated. We
should note however that our baseline system to
create GNM-approx uses relatively simple clustering
methods and heuristics. It is easy to observe that
a sophisticated named entity matching system
would produce a GNM-approx that better approx-
imates GNM than our baseline method, and CONE
B3 and CONE CEAF scores calculated using this
GNM-approx would be more correlated with stan-
dard B3 and CEAF scores.
We note from the above results that correlations
scores are very similar across different systems
and datasets. In order to formalize this assertion,
we calculate correlation scores in a system-
independent and data-independent manner. We
combine all the data points across all four differ-
ent systems and plot them in Figure 2 and 3 for
ACE Phase 2 NWIRE corpus and in Figure 4 and
5 for ACE 2005 corpus respectively. We illu-
strate only F1 scores; the results for precision
and recall are similar.
</bodyText>
<figureCaption confidence="0.960086">
Figure 2. Correlation between B3 F1 and CONE
B3 F1 for all systems on ACE 2
Figure 3. Correlation between CEAF F1 and
CONE CEAF F1 for all systems on ACE 2
</figureCaption>
<bodyText confidence="0.9989892">
Figure 2 reflects a Pearson’s correlation coeffi-
cient of 0.70, suggesting that all the B3 F1 and
CONE B3 F1 scores for different systems are
highly correlated and that CONE B3 F1 does not
bias towards any particular system. Figure 3 re-
flects a Pearson’s correlation coefficient of 0.83,
providing similar evidence for the system-
independence of correlation between CEAF F1
and CONE CEAF F1 scores. Figures 4 and 5
corresponding to ACE 2005 reflect similar corre-
lation coefficients of 0.89 and 0.82, and thus
support the idea that the correlations between B3
F1 and CONE B3 F1, as well as between CEAF
F1and CONE CEAF F1, are dataset-independent
in addition to being system-independent.
</bodyText>
<page confidence="0.546298">
142
</page>
<figureCaption confidence="0.900572">
Figure 4. Correlation between B3 F1 and CONE
B3 F1 for all systems on ACE 2005
Figure 5. Correlation between CEAF F1 and
CONE CEAF F1 for all systems on ACE 2005
</figureCaption>
<sectionHeader confidence="0.968806" genericHeader="evaluation">
6 Application and Discussion
</sectionHeader>
<bodyText confidence="0.99990425">
To illustrate the applicability of CONE metrics,
we consider the Enron e-mail corpus. It is of a
different genre than the newswire corpora that
CRR systems are usually trained on, and no CRR
gold standard annotations exist for it. Conse-
quently, no CRR systems have been evaluated on
it so far. We used CONE B3 and CONE CEAF to
evaluate and compare the NE-CRR performance
of various CRR systems on a subset of the Enron
e-mail corpus (Klimt and Yang, 2004) that was
cleaned and stripped of spam messages. We re-
port the results in Table 5.
</bodyText>
<table confidence="0.999357">
CONE B3 CONE CEAF
P R F1 P R F1
L 0.43 0.21 0.23 0.31 0.17 0.21
B 0.26 0.18 0.2 0.26 0.16 0.2
LP 0.61 0.51 0.53 0.58 0.53 0.54
OP 0.19 0.03 0.05 0.11 0.02 0.04
</table>
<tableCaption confidence="0.996843">
Table 5. GNM-approx Scores on Enron corpus
</tableCaption>
<bodyText confidence="0.999833875">
We find that LingPipe is the best of all the sys-
tems we considered, and LBJ is slightly ahead of
BART in all measures. We suspect that since
LingPipe is a commercial system, it may have
extra training resources in the form of non-
traditional corpora. Nevertheless, we believe our
method is robust and scalable for large corpora
without NE-CRR gold standard annotations.
</bodyText>
<sectionHeader confidence="0.964236" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999997666666667">
We propose the CONE B3 and CONE CEAF me-
trics for automatic evaluation of Named Entity
Co-reference Resolution (NE-CRR). These me-
trics measures a NE-CRR system’s performance
on the subtask of named mentions extraction and
grouping (NMEG) and use it to estimate the sys-
tem’s performance on the full task of NE-CRR.
We show that CONE B3 and CONE CEAF
scores of various systems across different data-
sets are strongly correlated with their standard B3
and CEAF scores respectively. The advantage of
CONE metrics compared to standard ones is that
instead of the full gold standard data G, they only
require a subset GNM of named mentions which
even if not available can be closely approximated
by using a state-of-the-art NER system and clus-
tering its results. Although we use a simple base-
line algorithm for producing the approximate
gold standard GNM-approx, CONE B3 and CONE
CEAF scores of various systems obtained using
this GNM-approx still prove to be correlated with
their standard B3 and CEAF scores obtained us-
ing the full gold standard G. CONE metrics thus
reduce the need of expensive labeled corpora.
We use CONE B3 and CONE CEAF to evaluate
the NE-CRR performance of various CRR sys-
tems on a subset of the Enron email corpus, for
which no gold standard annotations exist and no
such evaluations have been performed so far. In
the future, we intend to use more sophisticated
named entity matching schemes to produce better
approximate gold standards GNM-approx. We also
intend to use the CONE metrics to evaluate NE-
CRR systems on new datasets in domains such as
chat, email, biomedical literature, etc. where very
few corpora with gold standard annotations exist.
</bodyText>
<sectionHeader confidence="0.995203" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999973">
We would like to thank Prof. Ani Nenkova from
the University of Pennsylvania for her talk about
automatic evaluation for text summarization at
the spring 2010 CMU LTI Colloquium and ano-
nymous reviewers for insightful comments.
</bodyText>
<page confidence="0.931358">
143
</page>
<sectionHeader confidence="0.993181" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952763157895">
E. Agirre, L. Màrquez and R. Wicentowski, Eds.
2007. Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval).
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. Proceedings of LREC
Workshop on Linguistic Coreference.
J. Baldridge and T. Morton. 2004. OpenNLP.
http://opennlp.sourceforge.net/.
B. Baldwin and B. Carpenter. 2003. LingPipe. Alias-i.
E. Bengtson and D. Roth. 2008. Understanding the
Value of Features for Coreference Resolution. Pro-
ceedings of EMNLP.
J. Cohen. 1988. Statistical power analysis for the be-
havioral sciences. (2nd ed.)
A.K. Elmagarmid, P.G. Ipeirotis and V.S. Verykios.
2007. Duplicate Record Detection: A Survey. IEEE
Transactions on Knowledge and Data Engineering,
v.19 n.1, 2007.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Informa-
tion Extraction Systems by Gibbs Sampling. Pro-
ceedings of ACL.
B. Klimt and Y. Yang. 2004. The Enron corpus: A
new dataset for email classification research. Pro-
ceedings of ECML.
H.W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics
Quarterly, 2(83).
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics.
Proceedings of HLT-NAACL.
C. Lin and F.J. Och. 2004. Automatic evaluation of
machine translation quality using longest common
subsequence and skip-bigram statistics. Proceed-
ings of ACL.
A. Louis and A. Nenkova. 2009. Automatically Eva-
luating Content Selection in Summarization with-
out Human Models. Proceedings of EMNLP, pages
306–314, Singapore, 6-7 August 2009.
X. Luo. 2005. On coreference resolution performance
metrics. Proceedings of EMNLP.
MUC-6. 1995. Proceedings of the Sixth Understand-
ing Conference (MUC-6).
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. Journal of SIAM, 5:32-38.
NIST. 2003. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
K Papineni, S Roukos, T Ward and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. Proceedings of ACL.
V. Pervouchine, H. Li and B. Lin. 2009. Translitera-
tion alignment. Proceedings of ACL.
L. Ratinov and D. Roth. 2009. Design Challenges and
Misconceptions in Named Entity Recognition.
Proceedings of CoNLL.
R. Shah, B. Lin, A. Gershman and R. Frederking.
2010. SYNERGY: a named entity recognition sys-
tem for resource-scarce languages such as Swahili
using online machine translation. Proceedings of
LREC Workshop on African Language Technology.
H.E. Soper, A.W. Young, B.M. Cave, A. Lee and K.
Pearson. 1917. On the distribution of the correla-
tion coefficient in small samples. Appendix II to
the papers of &amp;quot;Student&amp;quot; and R. A. Fisher. A co-
operative study. Biometrika, 11, 328-413.
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff.
2009. Conundrums in Noun Phrase Coreference
Resolution: Making Sense of the State-of-the-Art.
Proceedings of ACL.
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman, A.
Jern, J. Smith, X. Yang and A. Moschitti. 2008.
BART: A Modular Toolkit for Coreference Reso-
lution. Proceedings of EMNLP.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L.
Hirschman. 1995. A model-theoretic coreference
scoring scheme. Proceedings of MUC 6.
</reference>
<page confidence="0.947177">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806990">
<title confidence="0.994512">CONE: Metrics for Automatic Evaluation of Named Co-reference Resolution</title>
<author confidence="0.999065">Bo Lin</author>
<author confidence="0.999065">Rushin Shah</author>
<author confidence="0.999065">Robert Frederking</author>
<author confidence="0.999065">Anatole</author>
<affiliation confidence="0.9880145">Language Technologies Institute, School of Computer Carnegie Mellon</affiliation>
<address confidence="0.999866">5000 Forbes Ave., PA 15213, USA</address>
<email confidence="0.99979">bolin@cs.cmu.edu</email>
<email confidence="0.99979">rnshah@cs.cmu.edu</email>
<email confidence="0.99979">ref@cs.cmu.edu</email>
<email confidence="0.99979">anatoleg@cs.cmu.edu</email>
<abstract confidence="0.993772727272727">Human annotation for Co-reference Resolution (CRR) is labor intensive and costly, and only a handful of annotated corpora are currently available. However, corpora with Named Entity (NE) annotations are widely available. Also, unlike current CRR systems, state-of-the-art NER systems have very high accuracy and can generate NE labels that are very close to the gold standard for unlabeled corpora. We propose a new set of metrics collectively called CONE for Named Entity Coreference Resolution (NE-CRR) that use a subset of gold standard annotations, with the advantage that this subset can be easily approximated using NE labels when gold standard CRR annotations are absent. We define and CONE CEAF metrics based on traditional and CEAF metrics and show CONE and CONE CEAF scores of any CRR system on any dataset are highly correwith its and CEAF scores respectively. We obtain correlation factors greater than 0.6 for all CRR systems across all datasets, and a best-case correlation factor of 0.8. We also present a baseline method to estimate the gold standard required by CONE metrics, and show CONE and CONE CEAF scores using this estimated gold standard are also correlated and CEAF scores respectively. We thus demonstrate the suitability of CONE CONE CEAF for automatic evaluation of NE-CRR.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>L Màrquez</author>
<author>R Wicentowski</author>
<author>Eds</author>
</authors>
<date>2007</date>
<booktitle>Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval).</booktitle>
<marker>Agirre, Màrquez, Wicentowski, Eds, 2007</marker>
<rawString>E. Agirre, L. Màrquez and R. Wicentowski, Eds. 2007. Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for Scoring Coreference Chains.</title>
<date>1998</date>
<booktitle>Proceedings of LREC Workshop on Linguistic Coreference.</booktitle>
<contexts>
<context position="2553" citStr="Bagga and Baldwin, 1998" startWordPosition="407" endWordPosition="410">oun phrases that describe a Named Entity. In this paper, we consider the task of Named Entity CRR (NE-CRR) only. Most, if not all, recent efforts in the field of CRR have concentrated on machine-learning based approaches. Many of them formulate the problem as a pair-wise binary classification task, in which possible co-reference between every pair of mentions is considered, and produce chains of coreferring mentions for each entity as their output. One of the most important problems in CRR is the evaluation of CRR results. Different evaluation metrics have been proposed for this task. Bcubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) are the two most popular metrics; they compute Precision, Recall and F1 measure between matched equivalent classes and use weighted sums of Precision, Recall and F1 to produce a global score. Like all metrics, B3 and CEAF require gold standard annotations; however, gold standard CRR annotations are scarce, because producing such annotations involves a substantial amount of human effort since it requires an in-depth knowledge of linguistics and a high level of understanding of the particular text. Consequently, very few corpora with gold standard CRR annotations are availa</context>
<context position="9681" citStr="Bagga and Baldwin, 1998" startWordPosition="1576" endWordPosition="1579">nt parts with test results on other datasets as a predictor for unseen datasets, and obtain promising results with good correlations. We approach the problem from a different perspective. In our work, we show the correlation between the scores on traditional metrics and scores on our CONE metrics, and show how to automatically estimate the gold standard required by CONE metrics. Thus our method is able to predict the co-reference resolution performance without gold standard at all. We base our new metrics on the standard B3 and CEAF metrics used for computing CRR scores. (Vilian et al., 1995; Bagga and Baldwin, 1998; Luo, 2005). B3 and CEAF are believed to be more discriminative and interpretable than earlier metrics and are widely adopted especially for machine-learning based approaches. 3 Standard Metrics: B3 and CEAF We now provide an overview of the standard B3 and CEAF metrics used to evaluate CRR systems. Both metrics assume that a CRR system produces a set of equivalence classes {O} and assigns each mention to only one class. Let Oi be the class to which the ith mention was assigned by the system. We also assume that we have a set of correct equivalence classes {G} (the gold standard). Let Gi be t</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for Scoring Coreference Chains. Proceedings of LREC Workshop on Linguistic Coreference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>T Morton</author>
</authors>
<date>2004</date>
<journal>LingPipe. Alias-i.</journal>
<marker>Baldridge, Morton, 2004</marker>
<rawString>J. Baldridge and T. Morton. 2004. OpenNLP. http://opennlp.sourceforge.net/. B. Baldwin and B. Carpenter. 2003. LingPipe. Alias-i.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bengtson</author>
<author>D Roth</author>
</authors>
<title>Understanding the Value of Features for Coreference Resolution.</title>
<date>2008</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<marker>Bengtson, Roth, 2008</marker>
<rawString>E. Bengtson and D. Roth. 2008. Understanding the Value of Features for Coreference Resolution. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>Statistical power analysis for the behavioral sciences.</title>
<date>1988</date>
<note>(2nd ed.)</note>
<contexts>
<context position="19009" citStr="Cohen, 1988" startWordPosition="3266" endWordPosition="3267"> CONE CEAF:  ( ) ( ) Sum{GNM }  G NM i P NM  i Sum{ONM } M i NM i, f i R NM  i Sum{GNM} M NM i, f i The corresponding equations for CONE B3 Precision are:  M NM i, j 2 P NM  i O NM j i  Sum{ONM I E M NM _, i 2 R&apos; E _ R NM i _ X Sum{RNM} We perform this experiment with the LBJ and BART CRR systems on the ACE Phase 2 corpus. We illustrate the correlation results in Figure 1. Figure 1. Correlation between and - Entity Level CEAF Precision From Figure 1, we can see that the two measures are highly correlated. In fact, we find that the Pearson’s correlation coefficient (Soper et al., 1917; Cohen, 1988) is 0.73. The points lining up on the x-axis and y=1.0 represent very small equivalence classes and are a form of noise; their removal doesn’t affect this coefficient. To show that this strong correlation is not a statistical anomaly, we also compute entity-level correlation using (Gi - GNMi, Oj - ONMj) and (Gi, Oj) instead of (GNM i, ONMj) and (Gi, Oj) and find that the coefficient drops to 0.03, which is obviously not correlated at all. We now know and P— are highly correlated. Assume the correlation is linear, with the following equation:  a i  P NM P = � = � P i i Pi  where α and R are </context>
</contexts>
<marker>Cohen, 1988</marker>
<rawString>J. Cohen. 1988. Statistical power analysis for the behavioral sciences. (2nd ed.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Elmagarmid</author>
<author>P G Ipeirotis</author>
<author>V S Verykios</author>
</authors>
<title>Duplicate Record Detection: A Survey.</title>
<date>2007</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>19</volume>
<pages>1</pages>
<marker>Elmagarmid, Ipeirotis, Verykios, 2007</marker>
<rawString>A.K. Elmagarmid, P.G. Ipeirotis and V.S. Verykios. 2007. Duplicate Record Detection: A Survey. IEEE Transactions on Knowledge and Data Engineering, v.19 n.1, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="13979" citStr="Finkel et al., 2005" startWordPosition="2373" endWordPosition="2376">EAF = CEAF(GNM, ONM) Following our previous notation, we denote CONE B3 and CONE CEAF collectively as Score(GNM, ONM). We observe that Score(GNM, ONM) measures a NE-CRR system’s performance for the NE-CRR subtask of named mentions extraction and grouping (NMEG). We find that Score(GNM, ONM) is highly correlated with Score(G, O) for all the freely available NECRR systems over various datasets. This provides the neccessary justification for the use of Score(GNM, ONM). We use SYNERGY (Shah et al., 2010), an ensemble NER system that combines the UIUC NER (Ritanov and Roth, 2009) and Stanford NER (Finkel et al., 2005) systems, to produce GNM and ONM from G and O by selecting named mentions. However, any other good NER system would serve the same purpose. We see that while standard evaluation metrics require the use of G, i.e. the full set of NE-CRR gold standard annotations including named, nominal and pronimal mentions, CONE metrics require only GNM, i.e. gold standard annotations consisting of named mentions only. The key advantage of using CONE metrics is that GNM can be automatically approximated using an NER system with a good degree of accuracy. This is because state-of-the-art NER systems achieve ne</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J.R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Klimt</author>
<author>Y Yang</author>
</authors>
<title>The Enron corpus: A new dataset for email classification research.</title>
<date>2004</date>
<booktitle>Proceedings of ECML.</booktitle>
<contexts>
<context position="27981" citStr="Klimt and Yang, 2004" startWordPosition="4840" endWordPosition="4843">ween B3 F1 and CONE B3 F1 for all systems on ACE 2005 Figure 5. Correlation between CEAF F1 and CONE CEAF F1 for all systems on ACE 2005 6 Application and Discussion To illustrate the applicability of CONE metrics, we consider the Enron e-mail corpus. It is of a different genre than the newswire corpora that CRR systems are usually trained on, and no CRR gold standard annotations exist for it. Consequently, no CRR systems have been evaluated on it so far. We used CONE B3 and CONE CEAF to evaluate and compare the NE-CRR performance of various CRR systems on a subset of the Enron e-mail corpus (Klimt and Yang, 2004) that was cleaned and stripped of spam messages. We report the results in Table 5. CONE B3 CONE CEAF P R F1 P R F1 L 0.43 0.21 0.23 0.31 0.17 0.21 B 0.26 0.18 0.2 0.26 0.16 0.2 LP 0.61 0.51 0.53 0.58 0.53 0.54 OP 0.19 0.03 0.05 0.11 0.02 0.04 Table 5. GNM-approx Scores on Enron corpus We find that LingPipe is the best of all the systems we considered, and LBJ is slightly ahead of BART in all measures. We suspect that since LingPipe is a commercial system, it may have extra training resources in the form of nontraditional corpora. Nevertheless, we believe our method is robust and scalable for l</context>
</contexts>
<marker>Klimt, Yang, 2004</marker>
<rawString>B. Klimt and Y. Yang. 2004. The Enron corpus: A new dataset for email classification research. Proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<volume>2</volume>
<issue>83</issue>
<contexts>
<context position="11067" citStr="Kuhn, 1955" startWordPosition="1833" endWordPosition="1834">rate of correct mentions in the same equivalent classes. The individual precision and recall score is defined as follows: P |i R |G; i Ni ; N; O Here |Oi |and |Gi |are the cardinalities of sets Oi and Gi. The final precision and recall scores are: n n wiPi R= wiRi i=1 Here, in the simplest case the weight wi is set to 1/n, equal for all mentions. CEAF (Luo, 2005) produces the optimal matching between output classes and true classes first, with the constraint that one true class, Gi, can be mapped to at most one output class, say Of(i) and vice versa. This can be solved by the KM algorithm (Kuhn, 1955; Munkres, 1957) for maximum matching in a bipartite graph. CEAF then computes the precision and recall score as follows: ( 0 M a f, ( )  i j i j We use the terms Mi,j from CEAF to re-write B3, its formulas then reduce to:   2 i We can see that B3 simply iterates through all pairs of matchings instead of considering the one to one mappings as CEAF does. Thus, B3 computes the weighted sum of the F-measures for each individual mention which helps alleviate the bias in the pure link-based F-measure, while CEAF computes the same as B3 but enforces at most one matched equivalence class for eve</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>H.W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(83).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="8098" citStr="Lin and Hovy, 2003" startWordPosition="1322" endWordPosition="1325"> evaluating NE-CRR systems on an unlabeled dataset, and discuss possible drawbacks and extensions of these metrics. Finally, in section 7 we present our conclusions and ideas for future work. 2 Related Work There has been a substantial amount of research devoted to automatic evaluation for natural language processing, especially tasks involving language generation. The BLEU score (Papineni et al., 2002) proposed for evaluating machine translation results is the best known example of this. It uses n-gram statistics between machine generated results and references. It inspired the ROUGE metric (Lin and Hovy, 2003) and other methods (Louis and Nenkova, 2009) to perform automatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine et al., 2009) for evaluating transliteration alignment. It reduces the need for alignment gold standard and highly correlates with transliteration system performance. Thus it is able to 137 serve as a good metric </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>F J Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>Proceedings of ACL.</booktitle>
<marker>Lin, Och, 2004</marker>
<rawString>C. Lin and F.J. Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatically Evaluating Content Selection in Summarization without Human Models.</title>
<date>2009</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>306--314</pages>
<contexts>
<context position="8142" citStr="Louis and Nenkova, 2009" startWordPosition="1329" endWordPosition="1332">led dataset, and discuss possible drawbacks and extensions of these metrics. Finally, in section 7 we present our conclusions and ideas for future work. 2 Related Work There has been a substantial amount of research devoted to automatic evaluation for natural language processing, especially tasks involving language generation. The BLEU score (Papineni et al., 2002) proposed for evaluating machine translation results is the best known example of this. It uses n-gram statistics between machine generated results and references. It inspired the ROUGE metric (Lin and Hovy, 2003) and other methods (Louis and Nenkova, 2009) to perform automatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine et al., 2009) for evaluating transliteration alignment. It reduces the need for alignment gold standard and highly correlates with transliteration system performance. Thus it is able to 137 serve as a good metric for transliteration alignment. We contrast o</context>
</contexts>
<marker>Louis, Nenkova, 2009</marker>
<rawString>A. Louis and A. Nenkova. 2009. Automatically Evaluating Content Selection in Summarization without Human Models. Proceedings of EMNLP, pages 306–314, Singapore, 6-7 August 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2574" citStr="Luo, 2005" startWordPosition="413" endWordPosition="414">ntity. In this paper, we consider the task of Named Entity CRR (NE-CRR) only. Most, if not all, recent efforts in the field of CRR have concentrated on machine-learning based approaches. Many of them formulate the problem as a pair-wise binary classification task, in which possible co-reference between every pair of mentions is considered, and produce chains of coreferring mentions for each entity as their output. One of the most important problems in CRR is the evaluation of CRR results. Different evaluation metrics have been proposed for this task. Bcubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) are the two most popular metrics; they compute Precision, Recall and F1 measure between matched equivalent classes and use weighted sums of Precision, Recall and F1 to produce a global score. Like all metrics, B3 and CEAF require gold standard annotations; however, gold standard CRR annotations are scarce, because producing such annotations involves a substantial amount of human effort since it requires an in-depth knowledge of linguistics and a high level of understanding of the particular text. Consequently, very few corpora with gold standard CRR annotations are available (NIST, 2003; MUC-</context>
<context position="5745" citStr="Luo, 2005" startWordPosition="935" endWordPosition="936">nnotated datasets and show that the CONE B3 and B3 scores are highly correlated for all such combinations of CRR systems and datasets, as are CONE CEAF and CEAF scores, with a best-case correlation of 0.8. We produce estimated gold standard annotations for the Enron email corpus, since no actual gold standard CRR annotations exist for it, and then use CONE B3 and CONE CEAF with these estimated gold standard annotations to compare the performance of various NE-CRR systems on this corpus. No such comparison has been previously performed for the Enron corpus. We adopt the same terminology as in (Luo, 2005): a mention refers to each individual phrase and an entity refers to the equivalence class or co-reference chain with several mentions. This allows us to note some differences between NECRR and NP-CRR. NE-CRR involves indentifying named entities and extracting their coreferring mentions; equivalences classes without any NEs are not considered. NE-CRR is thus clearly a subset of NP-CRR, where all coreferring mentions and equivalence classes are considered. However, we focus on NE-CRR because it is currently a more active research area than NP-CRR and a better fit for target applications such as</context>
<context position="9693" citStr="Luo, 2005" startWordPosition="1580" endWordPosition="1581">s on other datasets as a predictor for unseen datasets, and obtain promising results with good correlations. We approach the problem from a different perspective. In our work, we show the correlation between the scores on traditional metrics and scores on our CONE metrics, and show how to automatically estimate the gold standard required by CONE metrics. Thus our method is able to predict the co-reference resolution performance without gold standard at all. We base our new metrics on the standard B3 and CEAF metrics used for computing CRR scores. (Vilian et al., 1995; Bagga and Baldwin, 1998; Luo, 2005). B3 and CEAF are believed to be more discriminative and interpretable than earlier metrics and are widely adopted especially for machine-learning based approaches. 3 Standard Metrics: B3 and CEAF We now provide an overview of the standard B3 and CEAF metrics used to evaluate CRR systems. Both metrics assume that a CRR system produces a set of equivalence classes {O} and assigns each mention to only one class. Let Oi be the class to which the ith mention was assigned by the system. We also assume that we have a set of correct equivalence classes {G} (the gold standard). Let Gi be the gold stan</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On coreference resolution performance metrics. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of the Sixth Understanding Conference (MUC-6).</booktitle>
<marker>1995</marker>
<rawString>MUC-6. 1995. Proceedings of the Sixth Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>Journal of SIAM,</journal>
<pages>5--32</pages>
<contexts>
<context position="11083" citStr="Munkres, 1957" startWordPosition="1835" endWordPosition="1836">ect mentions in the same equivalent classes. The individual precision and recall score is defined as follows: P |i R |G; i Ni ; N; O Here |Oi |and |Gi |are the cardinalities of sets Oi and Gi. The final precision and recall scores are: n n wiPi R= wiRi i=1 Here, in the simplest case the weight wi is set to 1/n, equal for all mentions. CEAF (Luo, 2005) produces the optimal matching between output classes and true classes first, with the constraint that one true class, Gi, can be mapped to at most one output class, say Of(i) and vice versa. This can be solved by the KM algorithm (Kuhn, 1955; Munkres, 1957) for maximum matching in a bipartite graph. CEAF then computes the precision and recall score as follows: ( 0 M a f, ( )  i j i j We use the terms Mi,j from CEAF to re-write B3, its formulas then reduce to:   2 i We can see that B3 simply iterates through all pairs of matchings instead of considering the one to one mappings as CEAF does. Thus, B3 computes the weighted sum of the F-measures for each individual mention which helps alleviate the bias in the pure link-based F-measure, while CEAF computes the same as B3 but enforces at most one matched equivalence class for every class in the </context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>J. Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of SIAM, 5:32-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE evaluation plan.</title>
<date>2003</date>
<note>www.nist.gov/speech/tests/ace/index.htm.</note>
<contexts>
<context position="3168" citStr="NIST, 2003" startWordPosition="506" endWordPosition="507">CEAF (Luo, 2005) are the two most popular metrics; they compute Precision, Recall and F1 measure between matched equivalent classes and use weighted sums of Precision, Recall and F1 to produce a global score. Like all metrics, B3 and CEAF require gold standard annotations; however, gold standard CRR annotations are scarce, because producing such annotations involves a substantial amount of human effort since it requires an in-depth knowledge of linguistics and a high level of understanding of the particular text. Consequently, very few corpora with gold standard CRR annotations are available (NIST, 2003; MUC-6, 1995; Agirre, 2007). By contrast, gold standard Named Entity (NE) annotations are easy to produce; indeed, there are many NE annotated corpora of different sizes and genres. Similarly, there are few CRR systems and even the best scores obtained by them are only in the region of F1 = 0.5 - 0.6. There are only four such CRR systems freely available, to the best of our knowledge (Bengston and Roth, 2007; Versley et al., 2008; Baldridge and Torton, 2004; Baldwin and Carpenter, 2003). In comparison, there are numerous Named Entity recognition (NER) systems, both general-purpose and special</context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>NIST. 2003. The ACE evaluation plan. www.nist.gov/speech/tests/ace/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="7885" citStr="Papineni et al., 2002" startWordPosition="1288" endWordPosition="1291">n 4. In section 5, we provide experimental results that illustrate the performance of CONE B3 and CONE CEAF compared to B3 and CEAF respectively. In Section 6, we give an example of the application of CONE metrics by evaluating NE-CRR systems on an unlabeled dataset, and discuss possible drawbacks and extensions of these metrics. Finally, in section 7 we present our conclusions and ideas for future work. 2 Related Work There has been a substantial amount of research devoted to automatic evaluation for natural language processing, especially tasks involving language generation. The BLEU score (Papineni et al., 2002) proposed for evaluating machine translation results is the best known example of this. It uses n-gram statistics between machine generated results and references. It inspired the ROUGE metric (Lin and Hovy, 2003) and other methods (Louis and Nenkova, 2009) to perform automatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K Papineni, S Roukos, T Ward and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Pervouchine</author>
<author>H Li</author>
<author>B Lin</author>
</authors>
<title>Transliteration alignment.</title>
<date>2009</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="8498" citStr="Pervouchine et al., 2009" startWordPosition="1380" endWordPosition="1384">t al., 2002) proposed for evaluating machine translation results is the best known example of this. It uses n-gram statistics between machine generated results and references. It inspired the ROUGE metric (Lin and Hovy, 2003) and other methods (Louis and Nenkova, 2009) to perform automatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine et al., 2009) for evaluating transliteration alignment. It reduces the need for alignment gold standard and highly correlates with transliteration system performance. Thus it is able to 137 serve as a good metric for transliteration alignment. We contrast our work with (Stoyanov et al., 2009), who show that the co-reference resolution problem can be separated into different parts according to the type of the mention. Some parts are relatively easy to solve. The resolver performs equally well in each part across datasets. They use the statistics of mentions in different parts with test results on other data</context>
</contexts>
<marker>Pervouchine, Li, Lin, 2009</marker>
<rawString>V. Pervouchine, H. Li and B. Lin. 2009. Transliteration alignment. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design Challenges and Misconceptions in Named Entity Recognition.</title>
<date>2009</date>
<booktitle>Proceedings of CoNLL.</booktitle>
<contexts>
<context position="3851" citStr="Ratinov and Roth, 2009" startWordPosition="620" endWordPosition="623">d Entity (NE) annotations are easy to produce; indeed, there are many NE annotated corpora of different sizes and genres. Similarly, there are few CRR systems and even the best scores obtained by them are only in the region of F1 = 0.5 - 0.6. There are only four such CRR systems freely available, to the best of our knowledge (Bengston and Roth, 2007; Versley et al., 2008; Baldridge and Torton, 2004; Baldwin and Carpenter, 2003). In comparison, there are numerous Named Entity recognition (NER) systems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95 (Ratinov and Roth, 2009; Finkel et al., 136 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136–144, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2005). Although these facts can be partly attributed to the ‘hardness’ of CRR compared to NER, they also reflect the substantial gap between NER and CRR research. In this paper, we present a set of metrics, collectively called CONE, that leverage widely available NER systems and resources and tools for the task of evaluating co-reference resolution systems. The basic idea behind CONE is to predict a CRR system’s performan</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design Challenges and Misconceptions in Named Entity Recognition. Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Shah</author>
<author>B Lin</author>
<author>A Gershman</author>
<author>R Frederking</author>
</authors>
<title>SYNERGY: a named entity recognition system for resource-scarce languages such as Swahili using online machine translation.</title>
<date>2010</date>
<booktitle>Proceedings of LREC Workshop on African Language Technology.</booktitle>
<contexts>
<context position="13864" citStr="Shah et al., 2010" startWordPosition="2353" endWordPosition="2356">ons and system output respectively. We now define CONE B3 and CONE CEAF as follows: CONE B3 = B3(GNM, ONM) CONE CEAF = CEAF(GNM, ONM) Following our previous notation, we denote CONE B3 and CONE CEAF collectively as Score(GNM, ONM). We observe that Score(GNM, ONM) measures a NE-CRR system’s performance for the NE-CRR subtask of named mentions extraction and grouping (NMEG). We find that Score(GNM, ONM) is highly correlated with Score(G, O) for all the freely available NECRR systems over various datasets. This provides the neccessary justification for the use of Score(GNM, ONM). We use SYNERGY (Shah et al., 2010), an ensemble NER system that combines the UIUC NER (Ritanov and Roth, 2009) and Stanford NER (Finkel et al., 2005) systems, to produce GNM and ONM from G and O by selecting named mentions. However, any other good NER system would serve the same purpose. We see that while standard evaluation metrics require the use of G, i.e. the full set of NE-CRR gold standard annotations including named, nominal and pronimal mentions, CONE metrics require only GNM, i.e. gold standard annotations consisting of named mentions only. The key advantage of using CONE metrics is that GNM can be automatically appro</context>
</contexts>
<marker>Shah, Lin, Gershman, Frederking, 2010</marker>
<rawString>R. Shah, B. Lin, A. Gershman and R. Frederking. 2010. SYNERGY: a named entity recognition system for resource-scarce languages such as Swahili using online machine translation. Proceedings of LREC Workshop on African Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H E Soper</author>
<author>A W Young</author>
<author>B M Cave</author>
<author>A Lee</author>
<author>K Pearson</author>
</authors>
<title>On the distribution of the correlation coefficient in small samples. Appendix II to the papers of &amp;quot;Student&amp;quot;</title>
<date>1917</date>
<journal>Biometrika,</journal>
<volume>11</volume>
<pages>328--413</pages>
<contexts>
<context position="18995" citStr="Soper et al., 1917" startWordPosition="3262" endWordPosition="3265">M and recall RNM for CONE CEAF:  ( ) ( ) Sum{GNM }  G NM i P NM  i Sum{ONM } M i NM i, f i R NM  i Sum{GNM} M NM i, f i The corresponding equations for CONE B3 Precision are:  M NM i, j 2 P NM  i O NM j i  Sum{ONM I E M NM _, i 2 R&apos; E _ R NM i _ X Sum{RNM} We perform this experiment with the LBJ and BART CRR systems on the ACE Phase 2 corpus. We illustrate the correlation results in Figure 1. Figure 1. Correlation between and - Entity Level CEAF Precision From Figure 1, we can see that the two measures are highly correlated. In fact, we find that the Pearson’s correlation coefficient (Soper et al., 1917; Cohen, 1988) is 0.73. The points lining up on the x-axis and y=1.0 represent very small equivalence classes and are a form of noise; their removal doesn’t affect this coefficient. To show that this strong correlation is not a statistical anomaly, we also compute entity-level correlation using (Gi - GNMi, Oj - ONMj) and (Gi, Oj) instead of (GNM i, ONMj) and (Gi, Oj) and find that the coefficient drops to 0.03, which is obviously not correlated at all. We now know and P— are highly correlated. Assume the correlation is linear, with the following equation:  a i  P NM P = � = � P i i Pi  wher</context>
</contexts>
<marker>Soper, Young, Cave, Lee, Pearson, 1917</marker>
<rawString>H.E. Soper, A.W. Young, B.M. Cave, A. Lee and K. Pearson. 1917. On the distribution of the correlation coefficient in small samples. Appendix II to the papers of &amp;quot;Student&amp;quot; and R. A. Fisher. A cooperative study. Biometrika, 11, 328-413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>N Gilbert</author>
<author>C Cardie</author>
<author>E Riloff</author>
</authors>
<date>2009</date>
<booktitle>Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art. Proceedings of ACL.</booktitle>
<contexts>
<context position="8778" citStr="Stoyanov et al., 2009" startWordPosition="1426" endWordPosition="1429">tomatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine et al., 2009) for evaluating transliteration alignment. It reduces the need for alignment gold standard and highly correlates with transliteration system performance. Thus it is able to 137 serve as a good metric for transliteration alignment. We contrast our work with (Stoyanov et al., 2009), who show that the co-reference resolution problem can be separated into different parts according to the type of the mention. Some parts are relatively easy to solve. The resolver performs equally well in each part across datasets. They use the statistics of mentions in different parts with test results on other datasets as a predictor for unseen datasets, and obtain promising results with good correlations. We approach the problem from a different perspective. In our work, we show the correlation between the scores on traditional metrics and scores on our CONE metrics, and show how to autom</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 2009. Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Versley</author>
<author>S P Ponzetto</author>
<author>M Poesio</author>
<author>V Eidelman</author>
<author>A Jern</author>
<author>J Smith</author>
<author>X Yang</author>
<author>A Moschitti</author>
</authors>
<title>BART: A Modular Toolkit for Coreference Resolution.</title>
<date>2008</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3602" citStr="Versley et al., 2008" startWordPosition="581" endWordPosition="584">an in-depth knowledge of linguistics and a high level of understanding of the particular text. Consequently, very few corpora with gold standard CRR annotations are available (NIST, 2003; MUC-6, 1995; Agirre, 2007). By contrast, gold standard Named Entity (NE) annotations are easy to produce; indeed, there are many NE annotated corpora of different sizes and genres. Similarly, there are few CRR systems and even the best scores obtained by them are only in the region of F1 = 0.5 - 0.6. There are only four such CRR systems freely available, to the best of our knowledge (Bengston and Roth, 2007; Versley et al., 2008; Baldridge and Torton, 2004; Baldwin and Carpenter, 2003). In comparison, there are numerous Named Entity recognition (NER) systems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95 (Ratinov and Roth, 2009; Finkel et al., 136 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136–144, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2005). Although these facts can be partly attributed to the ‘hardness’ of CRR compared to NER, they also reflect the substantial gap between NER and CRR research. In this paper</context>
<context position="22454" citStr="Versley et al., 2008" startWordPosition="3845" endWordPosition="3848">effectiveness of our metrics over time. As co-reference resolution systems evolve in strength, our metrics might be less effective, however this could be a good indicator to discriminate on different subtasks the improvements gained by the co-reference resolution systems. 5 Experimental Results We present experimental results in support of the validity and effectiveness of CONE metrics. As mentioned earlier, we used the following four publicly available CRR systems: UIUC’s LBJ system (L), BART from JHU Summer Workshop (B), LingPipe from Alias-i (LP), and OpenNLP (OP) (Bengston and Roth, 2007; Versley et al., 2008; Baldridge and Torton, 2004; Baldwin and Carpenter, 2003). All these CRR systems perform Noun Phrase co-reference resolution (NPCRR), not NE-CRR. So, we must first eliminate all equivalences classes that do not contain any named mentions. We do so using the SYNERGY NER system to separate named mentions from unnamed ones. Note that this must not be confused with the use of SYNERGY to produce GNM and ONM from G and O respectively. For that task, all equivalence classes in G and O already contain at least one named mention and we remove all unnamed mentions from each class. This process effectiv</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, Moschitti, 2008</marker>
<rawString>Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman, A. Jern, J. Smith, X. Yang and A. Moschitti. 2008. BART: A Modular Toolkit for Coreference Resolution. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>Proceedings of MUC 6.</booktitle>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. Proceedings of MUC 6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>