<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001070">
<title confidence="0.983201">
Non-Orthogonal Explicit Semantic Analysis
</title>
<author confidence="0.960227">
Nitish Aggarwal Kartik Asooja Georgeta Bordea Paul Buitelaar
</author>
<affiliation confidence="0.9842335">
Insight Centre for Data Analytics
National University of Ireland
</affiliation>
<address confidence="0.472494">
Galway, Ireland
</address>
<email confidence="0.77089">
firstname.lastname@insight-centre.org
</email>
<sectionHeader confidence="0.992293" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994802">
Explicit Semantic Analysis (ESA) utilizes the
Wikipedia knowledge base to represent the se-
mantics of a word by a vector where every
dimension refers to an explicitly defined con-
cept like a Wikipedia article. ESA inherently
assumes that Wikipedia concepts are orthog-
onal to each other, therefore, it considers that
two words are related only if they co-occur in
the same articles. However, two words can
be related to each other even if they appear
separately in related articles rather than co-
occurring in the same articles. This leads to
a need for extending the ESA model to con-
sider the relatedness between the explicit con-
cepts (i.e. Wikipedia articles in Wikipedia
based implementation) for computing textual
relatedness. In this paper, we present Non-
Orthogonal ESA (NESA) which represents
more fine grained semantics of a word as a
vector of explicit concept dimensions, where
every such concept dimension further consti-
tutes a semantic vector built in another vec-
tor space. Thus, NESA considers the concept
correlations in computing the relatedness be-
tween two words. We explore different ap-
proaches to compute the concept correlation
weights, and compare these approaches with
other existing methods. Furthermore, we eval-
uate our model NESA on several word related-
ness benchmarks showing that it outperforms
the state of the art methods.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896571428572">
Significance of quantifying relatedness between two
natural language texts has been shown in various
tasks which deal with information retrieval (IR),
natural language processing (NLP), or other related
fields. The semantics of a word can be obtained
from existing lexical resources like WordNet and
FrameNet. However, such lexical resources require
domain expertise for defining the hierarchical
structure, which makes their creation very expen-
sive. Therefore, distributional semantic models
(DSMs) have achieved much attention as they uti-
lize available document collections like Wikipedia,
and do not depend upon human expertise (Harris,
1954). DSMs represent the semantics of a word by
transforming it to a high dimensional distributional
vector in a predefined concept space. Many models
have been proposed that derive this concept space
by using explicit concepts or implicit concepts.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007) utilizes the concepts which
are explicitly derived under human cognition like
Wikipedia concepts (articles). However, Latent
Semantic Analysis (LSA) derives a latent concept
space by performing dimensionality reduction
(Landauer et al., 1998).
Gabrilovich and Markovitch (2007) introduced
ESA model in which Wikipedia and Open Directory
Project were used to obtain the explicit concepts,
however, Wikipedia has been a popular choice in
further ESA implementations (Polajnar et al., 2013;
Gottron et al., 2011; Aggarwal et al., 2014). ESA
represents the semantics of a word with a high di-
mensional vector over the Wikipedia concepts. The
tf-idf weight of the word with the textual content
under a Wikipedia concept reflects the magnitude
</bodyText>
<page confidence="0.944063">
92
</page>
<note confidence="0.906178">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 92–100,
Denver, Colorado, June 4–5, 2015.
</note>
<tableCaption confidence="0.999904">
Table 1: Top 5 Wikipedia concepts for “football” and “soccer” in the ESA vector
</tableCaption>
<table confidence="0.9294885">
# football soccer
1 FIFA History of soccer in the United States
2 Football Soccer in the United States
3 History of association football United States Soccer Federation
4 Football in England North American Soccer League (196884)
5 Association football United Soccer Leagues
</table>
<bodyText confidence="0.99988306122449">
of the corresponding vector dimension. To obtain
the semantic relatedness between two words, it
computes the vector dot product between their vec-
tors. ESA considers the dimensions as orthogonal
to each other. For instance, the synonyms like
“soccer” and “football” are highly related, however,
they may not co-occur together in many Wikipedia
articles. Table 1 shows that the top 5 Wikipedia
concepts retrieved for “football” and “soccer” do
not share any concept, however, the concepts may
exhibit relatedness to each other. Consequently,
ESA model assumes that words can be related only
if they co-occur in the same articles. However,
two words can also be related even if they do not
share the same articles at all, but appear in the
related ones. LSA resolves the orthogonality issue
to some extent by building latent concept space
in an unsupervised way (Landauer et al., 1998).
However, the resulting latent concepts are not as
clearly interpretable as the human-labeled concepts
in the ESA model. Previous studies (Gabrilovich
and Markovitch, 2007; Cimiano et al., 2009; Hassan
and Mihalcea, 2011) show that ESA performs better
than LSA for computing text relatedness. Therefore,
it is important to consider the relatedness between
dimensions in the ESA model, rather than con-
sidering them orthogonal, and also without losing
the explicit property of ESA model at the same time.
In this paper, we present Non-Orthogonal ESA
(NESA) model, an extension to ESA, which also
uses relatedness between the explicit concepts for
computing semantic relatedness between texts. The
concepts in ESA model are clearly interpretable and
they refer to the title of Wikipedia articles. This
characteristic provides an opportunity to investigate
different concept relatedness measures, such as
relatedness between articles’ content (document
relatedness) or relatedness between corresponding
Wikipedia titles. In order to investigate the perfor-
mance of these concept relatedness measures, we
evaluate them on an entity relatedness benchmark
called KORE (Hoffart et al., 2012) as Wikipedia
article title generally refers to an entity.
We then apply the different approaches for
computing concept relatedness in our model NESA
to compute text relatedness. We evaluate NESA
on several word relatedness benchmarks to verify
whether considering non-orthogonality in ESA
model improves its performance.
</bodyText>
<sectionHeader confidence="0.999759" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.975777">
2.1 Text Relatedness
</subsectionHeader>
<bodyText confidence="0.999987090909091">
In recent years, there have been a variety of efforts
to develop semantic relatedness measures. Classical
approaches assess the relatedness scores by using
existing knowledge bases or corpus statistics.
Lexical resources such as WordNet or Roget the-
saurus (Jarmasz and Szpakowicz, 2004) are used as
knowledge bases to compute the relatedness scores
between two words. Most of these approaches
make use of the hierarchical structure present in the
lexical resources. For instance, Hirst and St-Onge
(1998), Leacock and Chodorow (1998), and Wu
and Palmer (1994) utilize the edges that define
taxonomic relations between words; Banerjee and
Pedersen (2002) computes the scores by obtaining
the overlap between glosses associated with the
words; and some of the other approaches (Resnik,
1995; Lin, 1998) use corpus evidence with the
taxonomic structure of WordNet. These approaches
are limited to perform only for the lexical entries
and thus do not work with non-dictionary words.
Moreover, these measures rely on the manually
constructed lexical resources and they are not
</bodyText>
<page confidence="0.993525">
93
</page>
<bodyText confidence="0.991273568181818">
portable to multiple languages due to unavailability Strube and Ponzetto (2006) proposed WikiRelate
of lexical resources in multiple languages. that counts the edges between two concepts in
Corpus-based methods such as LSA (Landauer Wikipedia link structure, and also considers the
et al., 1998), Latent Dirichlet Allocation (LDA) depth of a concept in the Wikipedia category struc-
(Blei et al., 2003), and ESA (Gabrilovich and ture. Witten and Milne (2008) applied Google dis-
Markovitch, 2007) employ statistical models to tance metric (Cilibrasi and Vitanyi, 2007) on incom-
build the semantic profile of a word. LSA and ing links in Wikipedia. Hoffart at el. (2012) utilized
LDA generate unsupervised topics from a textual the textual content associated with the Wikipedia
corpus, and represent the semantics of a word by concepts. It observes the partial overlap between the
its distribution over these topics. LSA performs concepts (key-phrases) appearing in the article con-
singular value decomposition (SVD) to obtain a tent. The above mentioned approaches mainly ex-
latent concept space. On the contrary, ESA directly ploit the article content or Wikipedia link structure
uses supervised topics such as Wikipedia concepts for computing concept relatedness. In this paper, we
that are built manually, and considers that every also utilize the distributional information of the title
concept is orthogonal to each other. Polajnar at and hyperlinks for computing concept relatedness.
el. (2013) proposed an approach to improve ESA 3 Non-Orthogonal Explicit Semantic
by considering the concept relatedness using word Analysis
overlap in Wikipedia articles’ content. Radinsky at To compute text relatedness, NESA uses relatedness
el. (2011) introduced Temporal Semantic Analysis between the dimensions of the distributional vec-
(TSA) also considers the concept relatedness in tors to overcome the orthogonality in ESA model.
ESA model, which is computed by using their In addition to represent the words as distributional
temporal distribution over the NewYork Times news vectors, where each dimension is associated with a
archives from the last 100 years. Although, these Wikipedia concept as in ESA model, NESA also uti-
approaches consider relatedness between explicit lizes a square matrix Cn,n (n is the total number of
concepts (Polajnar et al., 2013; Radinsky et al., dimensions) containing the correlation weights be-
2011) and improve the accuracy, however, either tween the dimensions. Thus, to obtain the related-
they define a weak concept relatedness measure or ness score between the words w1 and w2, NESA
require an external corpus statistics. Our approach formulates the measure as follows:
takes inspiration from them and uses more advanced relNESA(w1,w2) = w1T1,n.Cn,n.w2n,1 (1)
concept relatedness measures that rely on the same where w1n,1 and w2n,1 are the corresponding distri-
corpus statistics, which is used to build the ESA butional vectors consisting of n dimensions. Every
model. concept dimension can be further semantically inter-
2.2 Concept Relatedness preted as a distributional vector in some other vector
As NESA model requires a concept relatedness mea- space of m dimensions. This transformation allows
sure to overcome orthogonality, we address here the computation of the correlation weights between
the existing methods of computing it (Strube and the concept dimensions. Thus, a transformation ma-
Ponzetto, 2006; Witten and Milne, 2008; Polajnar trix Em,n can be built, where each column corre-
et al., 2013). Most of these approaches rely on sponds to a transformation vector for each concept
Wikipedia and its derived knowledge bases such as dimension. Using the matrix Em,n, we can com-
DBpedia1, YAGO2 and FreeBase3. These knowl- pute the matrix Cn,n by multiplying Em,n with its
edge bases provide immense amount of informa- transpose as in equation 2. In the next section, we
tion about millions of concepts or entities which discuss the different approaches used for computing
can be utilized for computing concept relatedness. Cn,n containing the relatedness between the concept
dimensions .
</bodyText>
<figure confidence="0.7214006">
Cn,n = ETn,m.Em,n (2)
1http://dbpedia.org/About
2http://yago-knowledge.org/
3https://www.freebase.com/
94
</figure>
<sectionHeader confidence="0.891986" genericHeader="method">
4 Computing Concept Relatedness
</sectionHeader>
<bodyText confidence="0.9998125">
NESA requires the relatedness scores between
Wikipedia concepts (articles), therefore we present
the different approaches for computing Cn,n ma-
trix using Em,n. Every Wikipedia article consists
of different fields to represent the semantics of the
concept dimensions, such as Wikipedia title, textual
description and hyperlinks. We utilize this infor-
mation to implement four different concept related-
ness measures: VSM-Text, VSM-Hyperlinks, ESA-
WikiTitle, and DiSER. These approaches represent
the semantics of a concept with a distributional vec-
tor of m dimensions. All such vectors combined as
column vectors for n concept dimensions form the
matrix Em,n.
</bodyText>
<subsectionHeader confidence="0.997958">
4.1 VSM-Text
</subsectionHeader>
<bodyText confidence="0.9999893">
This approach is based on plain Vector Space Model
(VSM) for text. It calculates the relatedness scores
between concepts by taking word overlap between
their corresponding Wikipedia article content. The
concept is transformed to a column vector mx1,
where m is the total number of unique words in the
Wikipedia corpus. The magnitude of each dimen-
sion is calculated on the basis of the number of oc-
currences of the different words in the associated
Wikipedia article content.
</bodyText>
<subsectionHeader confidence="0.993512">
4.2 VSM-Hyperlink
</subsectionHeader>
<bodyText confidence="0.9999103">
Similar to the VSM-Text, this approach calculates
the concept relatedness by taking the overlap be-
tween the hyperlinks present in their corresponding
Wikipedia articles’ content. The concept is trans-
formed to a column vector mx1, where m is the total
number of hyperlinks in the whole Wikipedia. The
magnitude of each dimension is calculated on the
basis of the number of occurrences of the different
hyperlinks in the associated Wikipedia article con-
tent.
</bodyText>
<subsectionHeader confidence="0.998229">
4.3 ESA-WikiTitle
</subsectionHeader>
<bodyText confidence="0.999277857142857">
One intuitive way of obtaining concept relatedness
scores is by using ESA itself for calculating the re-
latedness between the concepts. We use the associ-
ated Wikipedia article title for this purpose. ESA
represents the semantics of a word with a high
dimensional vector over the Wikipedia concepts.
Therefore, each concept dimension is transformed
into a column vector of mx1, where m is the to-
tal number of Wikipedia concepts. The magni-
tude of each dimension is computed by using the
term frequency (tf) and inverse document frequency
(idf) for the terms appearing in the Wikipedia arti-
cle title over the Wikipedia corpus (Gabrilovich and
Markovitch, 2007).
</bodyText>
<subsectionHeader confidence="0.973064">
4.4 DiSER
</subsectionHeader>
<bodyText confidence="0.999838928571429">
Distributional Semantics for Entity Relatedness
(DiSER) (Aggarwal and Buitelaar, 2014) is a model
for computing relatedness scores between entities.
DiSER considers every Wikipedia concept as an en-
tity. Therefore, it can be used for computing concept
relatedness matrix Cn,n, as required by the NESA
model. In contrast to text relatedness measures
based on DSMs such as ESA, which do not distin-
guish between entity and text, DiSER differentiate
between entity and its surface forms by using unique
hyperlinks referring to entities in Wikipedia for en-
coding entities while building DSMs. It uses the
distributional information of such hyperlinks only
over the whole Wikipedia corpus for representing a
concept by a high dimensional distributional vector.
Therefore, each concept dimension is transformed
into a column vector of mx1, where m is the total
number of Wikipedia concepts. The magnitude of
each dimension is computed by using the concept
frequency (ef) and inverse document frequency (idf)
for an concept in the Wikipedia corpus. The concept
frequency (cf) is a slight variation of term frequency.
It computes the frequency of a concept appearing as
hyperlink in the Wikipedia articles. To obtain the
DiSER based relatedness scores between Wikipedia
concepts, we use Entity Relatedness Graph (EnRG)4
(Aggarwal et al., 2015), which is a focused related
entities explorer based on DiSER scores.
</bodyText>
<sectionHeader confidence="0.970192" genericHeader="method">
5 Evaluation of Concept Relatedness
Measures
</sectionHeader>
<bodyText confidence="0.9938342">
In this section, we evaluate the different approaches
defined for computing concept relatedness measures
in the previous section. For our evaluation, we use
the snapshot of English Wikipedia from 1st October,
2013. This snapshot consists of 13,872,614 articles,
</bodyText>
<footnote confidence="0.98538">
4EnRG demo: http://enrg.insight-centre.org
</footnote>
<page confidence="0.994974">
95
</page>
<bodyText confidence="0.999692822222222">
in which 5,934,022 are Wikipedia redirects. We equal to the total number of Wikipedia articles i.e.
filtered out all the namespace5 pages by using the 3,635,833.
articles’ titles as they have specific namespace 5.1 Dataset
patterns. There are 3,571,206 namespace pages in In order to evaluate the concept relatedness mea-
this snapshot. We remove all those articles which sures, we performed our experiments on the gold
contain less than 100 unique words or less than standard benchmark dataset KORE (Hoffart et al.,
5 hyperlinks; such articles are too specific and 2012). The KORE dataset consists of 21 seed
may generate some noise. We perform further Wikipedia concepts selected from the YAGO knowl-
filtering by removing all the articles if their titles edge base6. Every seed concept has a ranked list
are numbers like “19”, dates like “June 1”, or if the of 20 related Wikipedia concepts. In order to
title starts with “list”. We finally obtain a total of build this dataset, 20 concept candidates are se-
3,635,833 Wikipedia articles for our experiment. lected and ranked by human evaluators on crowd-
We implement all the concept relatedness measures sourcing platforms to give the relative comparison
by using these obtained Wikipedia articles. between two candidates against the corresponding
VSM-Text represents the semantics of a con- seed Wikipedia concept. For instance, human evalu-
cept with a column vector of mx1, where m is the ators provide their judgement if “Mark Zuckerberg”
total number of unique words appear in Wikipedia. is more related to “Facebook” than “Sean Parker”.
Wikipedia contains more than 2.5 billion unique With the answers for such binary questions, a ranked
words, therefore, to reduce the matrix size, we use list is prepared for every seed Wikipedia concept.
only 5 million most frequent words. ESA-WikiTitle The KORE dataset7 consists of 21 seed candidates,
represents the semantics of a concept with a col- thus forming 420 concept pairs with their related-
umn vector of mx1, where m is 3,635,833 in our ness scores assigned by 15 human evaluators.
implementation. In order to obtain the hyperlinks 5.2 Experiment
for VSM-Hyperlink and DiSER, we retain only We compare the concept relatedness measures de-
those text segments which have manually defined scribed in section 4 against other existing methods.
links provided by Wikipedia volunteers. However, Hoffart at el. (2012) proposed KORE and KPCS
the volunteers may not create the link for every which use the article content to compute the con-
surface form appearing in the article content. For cept relatedness. They use Mutual Information (MI-
instance, “Apple” occurs 213 times in “Steve Jobs” weight) to capture the importance of the hyperlink
Wikipedia page in our corpus, but only 7 out of for a Wikipedia concept. To evaluate the concept
these 213 are linked to the “Apple Inc.” Wikipedia relatedness measures using KORE dataset, we com-
page. The term frequency of “Apple” is calculated pute the concept relatedness scores for all the con-
without considering the partial string matches, cept pairs and rank the list of 20 candidates for each
for example, we do not count if “apple” appears seed Wikipedia concept. We calculated Spearman
as a substring of any annotated text segment like Rank correlation between the gold standard dataset
“Apple Store” or “Apple Lisa”. To obtain the actual and the results obtained from VSM-Text, VSM-
frequency of every hyperlink for computing the Hyperlink, ESA-WikiTitle and DiSER.
magnitude of the dimension, we apply “one sense 5.3 Results and Discussion
per discourse” heuristic (Gale et al., 1992), which Experimental results are shown in Table 2. We
assumes that a term tends to have the same meaning compare our results with the other existing methods
in the same discourse. We link every additional of computing concept relatedness: WLM, KORE,
un-linked occurrence of the text segment with the and KPCS. WLM is the Wikipedia Link-based ap-
same hyperlink appearing most of the times for proach by Witten and Milne (2008). KPCS and
the same segment in the article. The total number
of hyperlinks possible in our corpus would be
</bodyText>
<figure confidence="0.563392333333333">
6http://datahub.io/dataset/yago
7http://www.mpi-inf.mpg.de/yago-naga/aida
5http://en.wikipedia.org/wiki/Wikipedia:Namespace
96
tent are very influential features for concept related-
ness measures.
</figure>
<tableCaption confidence="0.9299895">
Table 2: Spearman rank correlation of concept related-
ness measures with gold standard
</tableCaption>
<table confidence="0.975236">
Concept Relatedness Spearman Rank
Measures Correlation with human
VSM-Text 0.510
VSM-Hyperlink 0.637
ESA 0.661
DiSER 0.781
WLM 0.610
KPCS 0.698
KORE 0.673
</table>
<bodyText confidence="0.999574117647058">
KORE are the approaches proposed in (Hoffart et
al., 2012), where KPCS is the cosine similarity on
MI-weighted keyphrases while KORE represents the
keyphrase overlap relatedness. These keyphrases
can be the text segment with hyperlinks in the article
content. Therefore, KPCS is a similar approach to
VSM-Hyperlink, besides KPCS assigns MI-weights
to capture the generality and specificity of concept
in the Wikipedia article. Many concepts in the gold
standard dataset are defined by ambiguous surface
forms such as “NeXT” and “Nice”, or they have
ambiguous text segments in their surface forms like
“Jobs” in “Steve Jobs” and “Guitar” in the “Guitar
Hero” video game. Therefore, the effect of using
only hyperlinks can be observed with the remarkable
difference between the results obtained by ESA and
DiSER. DiSER improves the accuracy over ESA by
20%. These scores illustrate that ESA fails in gen-
erating the appropriate semantic profiles for ambigu-
ous terms. VSM-Text does not capture the semantics
of Wikipedia concepts as the textual description in
Wikipedia article also contains generic terms which
are not enough to specify the precisely semantics
of Wikipedia concepts. Therefore, VSM-Hyperlink
achieved noticeable improvement over VSM-Text as
VSM-Hyperlink builds the semantic profile by us-
ing hyperlinks in the article content. These hyper-
links are created by Wikipedia volunteers, therefore,
it can be assumed that the text segments which are
linked to other Wikipedia article, are more important
than un-linked ones. However, KPCS and KORE
achieved significantly higher accuracy in compari-
son to VSM-Hyperlink, which indicates that gener-
ality and specificity of hyperlinks in the article con-
</bodyText>
<sectionHeader confidence="0.674582" genericHeader="method">
6 Evaluation of NESA for Word
Relatedness
</sectionHeader>
<bodyText confidence="0.999928">
In this section, we evaluate NESA for word relat-
edness. We experiment by using different concept
relatedness measures as explained in section 4 for
building the C,,,,,,, in NESA model as shown in equa-
tions 1 and 2. We use the same filtered Wikipedia
articles as used for evaluating the concept related-
ness measures in the previous section.
</bodyText>
<subsectionHeader confidence="0.982102">
6.1 Dataset
</subsectionHeader>
<bodyText confidence="0.952752766666667">
We use 6 different word relatedness benchmarks to
evaluate NESA.
WN353 consists of 353 word pairs annotated
by 13-15 human experts on a scale of 0-10. 0 refers
to un-related and 10 stands for highly related or
identical. This dataset mainly contains generic
words like “money”, “drink”, “movie”, etc.. It
also contains named entities such as “Jerusalem”,
“Palestinian” and “Israel”, which makes this dataset
more challenging for approaches that use only the
lexical resources.
WN353Rel and WN353Sim datasets are the
subsets of WN353. As WN353 contains similar and
related word pairs, Agirre at el. (2009) refine the
WN353 gold standard by splitting it in two parts:
related word pairs and similar word pairs. The
notion of similarity and relatedness are defined as
follow: two words are similar if they are connected
through the taxonomic relations like synonym or
hyponym in lexical resources, while two words can
be considered related if they are connected through
other relations such as meronym and holonym.
For instance, “football” and “soccer” are two
similar words while “computer” and “software”
can be considered as related. Finally, WN353Rel
and WN353Sim contain 252 and 203 word pairs
respectively.
MC30 is the dataset build by Miller and Charles
(1991) that contains the selected word pairs of
WN353. The relatedness scores of these words are
</bodyText>
<page confidence="0.999733">
97
</page>
<tableCaption confidence="0.999695">
Table 3: Spearman rank correlation of relatedness measures with gold standard datasets
</tableCaption>
<table confidence="0.9999825">
# WN353 WN353Rel WN353Sim MC30 RG65 MT287
H&amp;S 0.347 0.142 0.497 0.811 0.813 0.278
L&amp;C 0.302 0.172 0.412 0.793 0.823 0.284
Lesk 0.337 0.125 0.511 0.583 0.5466 0.271
W&amp;P 0.316 0.131 0.461 0.784 0.807 0.331
Resnik 0.353 0.184 0.535 0.693 0.731 0.234
J&amp;C 0.317 0.089 0.442 0.820 0.804 0.296
Lin 0.348 0.154 0.483 0.750 0.788 0.286
Roget 0.415 - - - - 0.856 0.804 - -
SSA 0.629 - - - - 0.810 0.830 - -
Polajnar et al. 0.664 - - - - - - - - - -
ESA 0.660 0.643 0.663 0.765 0.826 0.507
NESA (VSM-Text) 0.666 0.648 0.669 0.768 0.827 0.509
NESA (VSM-Hyperlink) 0.670 0.649 0.672 0.768 0.828 0.516
NESA (ESA-WikiTitle) 0.681 0.652 0.684 0.774 0.830 0.541
NESA (DiSER) 0.696 0.663 0.719 0.784 0.839 0.572
</table>
<bodyText confidence="0.958353909090909">
provided by 38 human experts on a scale of 0-4.
RG65 is a collection of 65 non-technical word
pairs. These word pairs are annotated by 51 human
experts (see for more detail (Rubenstein and Good-
enough, 1965)).
MT287 is a relatively newer dataset that con-
tains 287 word pairs. This dataset is prepared
mainly to study the effect of temporal distribution
(Radinsky et al., 2011) of a word over several
years. The relatedness scores of the word pairs are
obtained from 15-20 mechanical turkers.
</bodyText>
<subsectionHeader confidence="0.986873">
6.2 Experiment
</subsectionHeader>
<bodyText confidence="0.9978048">
We compare the NESA model with other state of
the art methods of calculating word relatedness: Ex-
plicit Semantic Analysis (ESA), Salient Semantic
Analysis (SSA), and several WordNet-based simi-
larity measures. Hassan and Mihalcea (2011) re-
ported SSA performance on WN353, MC30 and
RG65 datasets as shown in table 3. The WordNet-
based similarity measures are implemented using
WS4J (WordNet Similarity for Java)8 library built
on WordNet 3.0.
</bodyText>
<footnote confidence="0.926063">
8https://code.google.com/p/ws4j/
</footnote>
<subsectionHeader confidence="0.808627">
6.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999987851851852">
Table 3 shows the results of the NESA model
with different concept relatedness approaches and
other state of the art methods of calculating word
relatedness. The knowledge-based methods that use
lexical resources like WordNet or Roget thesaurus
(Jarmasz and Szpakowicz, 2004), achieve higher
accuracy if the words in benchmark datasets are
available in the knowledge bases. For instance,
WordNet-based measures (H&amp;S (Hirst and St-Onge,
1998), L&amp;C (Leacock and Chodorow, 1998), Lesk
(Banerjee and Pedersen, 2002), W&amp;P (Wu and
Palmer, 1994), Resnik (Resnik, 1995) J&amp;C (Jiang
and Conrath, 1997), Lin (Lin, 1998)) and Roget
thesaurus-based measure (Jarmasz and Szpakowicz,
2004) achieved higher accuracy on MC30 and
RG65 datasets. However, these approaches may not
fit well for the datasets that contain non-dictionary
words, therefore, the accuracy of knowledge-based
measures decrease significantly on other datasets.
Corpus-based measures ESA and SSA achieved
higher scores than knowledge-based methods on
WN353, WN353Rel, WN353Sim and MT287
datasets. Moreover, corpus-based methods per-
formed comparable to knowledge-based methods
on MC30 and RG65. Most of the knowledge-based
measures use the taxonomic relations for comput-
ing word relatedness. Therefore, these measures
</bodyText>
<page confidence="0.989285">
98
</page>
<bodyText confidence="0.993363906976744">
obtained poor results on WN353Rel in contrast References
to WN353Sim dataset. However, corpus-based Nitish Aggarwal and Paul Buitelaar. 2014. Wikipedia-
measures performed well for both type of relations based distributional semantics for entity relatedness.
i.e. similarity and relatedness. In 2014 AAAI Fall Symposium Series.
The NESA model combined with any concept Nitish Aggarwal, Kartik Asooja, and Paul Buitelaar.
relatedness measure outperforms ESA for all the 2014. Exploring esa to improve word relatedness.
word relatedness benchmark datasets. It shows Lexical and Computational Semantics (* SEM 2014),
that considering non-orthogonality between explicit 51.
concepts in ESA model improves the accuracy. Nitish Aggarwal, Kartik Asooja, Housam Ziad, and Paul
NESA-VSM-Hyperlink performs better than Buitelaar. 2015. Who are the american vegans related
NESA-VSM-Text implying that considering only to brad pitt? exploring related entities. In 24th Inter-
the hyperlinks from the article content works better national World Wide Web Conference (WWW 2015),
than taking the overlap of whole content. NESA- Florence, Italy.
ESA-WikiTitle and NESA-DiSER achieved higher Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
scores than both NESA-VSM-Text and NESA- Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A
VSM-Hyperlink. It shows that the distributional study on similarity and relatedness using distributional
representation of the article title captures the se- and wordnet-based approaches. In Proceedings of Hu-
mantic information better than considering only the man Language Technologies: The 2009 Annual Con-
corresponding article content. Another interesting ference of the North American Chapter of the Associ-
thing to note is that the correlation scores obtained ation for Computational Linguistics, pages 19–27.
by NESA model with the four concept relatedness Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
measures follow the same order in table 3 as of the lesk algorithm for word sense disambiguation using
correlation scores obtained in evaluating concept wordnet. In Computational linguistics and intelligent
relatedness shown in table 2. It represents the con- text processing, pages 136–145. Springer.
sistency of proposed concept relatedness measures David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
in two different experiment settings. NESA-DiSER 2003. Latent dirichlet allocation. J. Mach. Learn.
achieved the highest correlation scores in all the Res., 3:993–1022.
word relatedness benchmark datasets. Rudi L Cilibrasi and Paul MB Vitanyi. 2007. The google
7 Conclusion similarity distance. Knowledge and Data Engineering,
We presented Non-Orthogonal ESA which intro- IEEE Transactions on, 19(3):370–383.
duces the relatedness between the explicit concepts Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
in the ESA model for computing semantic related- Sorg, and Steffen Staab. 2009. Explicit versus la-
ness, without compromising with the explicit prop- tent concept models for cross-language information re-
erty of the ESA concept space. We showed that the trieval. In IJCAI, volume 9, pages 1513–1518.
word relatedness results vary with the different con- Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
cept relatedness measures. NESA outperformed all puting semantic relatedness using wikipedia-based ex-
state of the art methods, in particular, NESA-DiSER plicit semantic analysis. In Proceedings of the 20th
achieved the highest correlation with the gold stan- IJCAI, pages 1606–1611.
dard. We also evaluated the different concept relat- William A Gale, Kenneth W Church, and David
edness measures using benchmark dataset KORE, in Yarowsky. 1992. One sense per discourse. In Pro-
which DiSER outperformed all others. ceedings of the workshop on Speech and Natural Lan-
8 Acknowledgements guage, pages 233–237. ACL.
This work has been funded by a research grant from Thomas Gottron, Maik Anderka, and Benno Stein. 2011.
</bodyText>
<reference confidence="0.985867383333333">
Science Foundation Ireland (SFI) under Grant Num- Insights into explicit semantic analysis. In Proceed-
ber SFI/12/RC/2289 (INSIGHT). ings of the 20th CIKM, pages 1961–1964. ACM.
99 Zellig Harris. 1954. Distributional structure. In Word 10
(23), pages 146–162.
Samer Hassan and Rada Mihalcea. 2011. Semantic re-
latedness using salient semantic analysis. In AAAI.
Graeme Hirst and David St-Onge. 1998. Lexical chains
as representations of context for the detection and cor-
rection of malapropisms. WordNet: An electronic lex-
ical database, 305:305–332.
Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Mar-
tin Theobald, and Gerhard Weikum. 2012. Kore:
Keyphrase overlap relatedness for entity disambigua-
tion. In Proceedings of the 21st ACM international
conference on Information and knowledge manage-
ment, pages 545–554. ACM.
Mario Jarmasz and Stan Szpakowicz. 2004. Rogets
thesaurus and semantic similarity1. Recent Advances
in Natural Language Processing III: Selected Papers
from RANLP, 2003:111.
Jay J Jiang and David W Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy.
arXiv preprint cmp-lg/9709008.
T K Landauer, P. W. Foltz, and D Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259–284.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In ICML, volume 98, pages 296–304.
George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
cognitive processes, 6(1):1–28.
Tamara Polajnar, Nitish Aggarwal, Kartik Asooja, and
Paul Buitelaar. 2013. Improving esa with docu-
ment similarity. In Advances in Information Retrieval,
pages 582–593. Springer.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,
and Shaul Markovitch. 2011. A word at a time: com-
puting word relatedness using temporal semantic anal-
ysis. In 20th WWW, pages 337–346.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. arXiv preprint
cmp-lg/9511007.
Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 8(10):627–633.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In AAAI, volume 6, pages 1419–1424.
I Witten and David Milne. 2008. An effective, low-
cost measure of semantic relatedness obtained from
wikipedia links. In Proceeding of AAAI Workshop on
Wikipedia and Artificial Intelligence: an Evolving Syn-
ergy, AAAI Press, Chicago, USA, pages 25–30.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on ACL, pages 133–138. ACL.
</reference>
<page confidence="0.990741">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684317">
<title confidence="0.999673">Non-Orthogonal Explicit Semantic Analysis</title>
<author confidence="0.978635">Nitish Aggarwal Kartik Asooja Georgeta Bordea Paul</author>
<affiliation confidence="0.990717">Insight Centre for Data National University of</affiliation>
<address confidence="0.723629">Galway,</address>
<email confidence="0.999322">firstname.lastname@insight-centre.org</email>
<abstract confidence="0.99902753125">Explicit Semantic Analysis (ESA) utilizes the Wikipedia knowledge base to represent the semantics of a word by a vector where every dimension refers to an explicitly defined concept like a Wikipedia article. ESA inherently assumes that Wikipedia concepts are orthogonal to each other, therefore, it considers that two words are related only if they co-occur in the same articles. However, two words can be related to each other even if they appear separately in related articles rather than cooccurring in the same articles. This leads to a need for extending the ESA model to consider the relatedness between the explicit concepts (i.e. Wikipedia articles in Wikipedia based implementation) for computing textual relatedness. In this paper, we present Non- Orthogonal ESA (NESA) which represents more fine grained semantics of a word as a vector of explicit concept dimensions, where every such concept dimension further constitutes a semantic vector built in another vector space. Thus, NESA considers the concept correlations in computing the relatedness between two words. We explore different approaches to compute the concept correlation weights, and compare these approaches with other existing methods. Furthermore, we evaluate our model NESA on several word relatedness benchmarks showing that it outperforms the state of the art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>