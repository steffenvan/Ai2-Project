<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000829">
<title confidence="0.665711">
Predicting Strong Associations on the Basis of Corpus Data
Yves Peirsman Dirk Geeraerts
</title>
<author confidence="0.630782">
Research Foundation – Flanders &amp; QLVL, University of Leuven
</author>
<affiliation confidence="0.978931">
QLVL, University of Leuven Leuven, Belgium
</affiliation>
<email confidence="0.7634535">
Leuven, Belgium dirk.geeraerts@arts.kuleuven.be
yves.peirsman@arts.kuleuven.be
</email>
<sectionHeader confidence="0.993657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976388888889">
Current approaches to the prediction of
associations rely on just one type of in-
formation, generally taking the form of
either word space models or collocation
measures. At the moment, it is an open
question how these approaches compare
to one another. In this paper, we will
investigate the performance of these two
types of models and that of a new ap-
proach based on compounding. The best
single predictor is the log-likelihood ratio,
followed closely by the document-based
word space model. We will show, how-
ever, that an ensemble method that com-
bines these two best approaches with the
compounding algorithm achieves an in-
crease in performance of almost 30% over
the current state of the art.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974639344263">
Associations are words that immediately come to
mind when people hear or read a given cue word.
For instance, a word like pepper calls up salt,
and wave calls up sea. Aitchinson (2003) and
Schulte im Walde and Melinger (2005) show that
such associations can be motivated by a number
of factors, from semantic similarity to colloca-
tion. Current computational models of associa-
tion, however, tend to focus on one of these, by us-
ing either collocation measures (Michelbacher et
al., 2007) or word space models (Sahlgren, 2006;
Peirsman et al., 2008). To this day, two gen-
eral problems remain. First, the literature lacks
a comprehensive comparison between these gen-
eral types of models. Second, we are still looking
for an approach that combines several sources of
information, so as to correctly predict a larger va-
riety of associations.
Most computational models of semantic rela-
tions aim to model semantic similarity in particu-
lar (Landauer and Dumais, 1997; Lin, 1998; Pad´o
and Lapata, 2007). In Natural Language Process-
ing, these models have applications in fields like
query expansion, thesaurus extraction, informa-
tion retrieval, etc. Similarly, in Cognitive Science,
such models have helped explain neural activa-
tion (Mitchell et al., 2008), sentence and discourse
comprehension (Burgess et al., 1998; Foltz, 1996;
Landauer and Dumais, 1997) and priming patterns
(Lowe and McDonald, 2000), to name just a few
examples. However, there are a number of appli-
cations and research fields that will surely bene-
fit from models that target the more general phe-
nomenon of association. For instance, automat-
ically predicted associations may prove useful in
models of information scent, which seek to ex-
plain the paths that users follow in their search
for relevant information on the web (Chi et al.,
2001). After all, if the visitor of a web shop
clicks on music to find the prices of iPods, this
behaviour is motivated by an associative relation
different from similarity. Other possible applica-
tions lie in the field of models of text coherence
(Landauer and Dumais, 1997) and automated es-
say grading (Kakkonen et al., 2005). In addition,
all research in Cognitive Science that we have re-
ferred to above could benefit from computational
models of association in order to study the effects
of association in comparison to those of similarity.
Our article is structured as follows. In sec-
tion 2, we will discuss the phenomenon of asso-
ciation and introduce the variety of relations that
it is motivated by. Parallel to these relations, sec-
tion 3 presents the three basic types of approaches
that we use to predict strong associations. Sec-
tion 4 will first compare the results of these three
approaches, for a total of 43 models. Section 5
will then show how these results can be improved
by the combination of several models in an ensem-
ble. Finally, section 6 wraps up with conclusions
and an outlook for future research.
</bodyText>
<note confidence="0.922959">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 648–656,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.995607">
648
</page>
<bodyText confidence="0.657669666666667">
cue association
amfibie (‘amphibian’) kikker (‘frog’)
peper (‘pepper’) zout (‘salt’)
roodborstje (‘robin’) vogel (‘bird’)
granaat (‘grenade’) oorlog (‘war’)
helikopter (‘helicopter’) vliegen (‘to fly’)
</bodyText>
<equation confidence="0.649473">
werk (‘job’) geld (‘money’)
acteur (‘actor’) film (‘film’)
cello (‘cello’) muziek (‘music’)
kruk (‘stool’) bar (‘bar’)
</equation>
<tableCaption confidence="0.8690505">
Table 1: Examples of cues and their strongest as-
sociation.
</tableCaption>
<sectionHeader confidence="0.985408" genericHeader="introduction">
2 Associations
</sectionHeader>
<bodyText confidence="0.9999844">
There are several reasons why a word may be asso-
ciated to its cue. According to Aitchinson (2003),
the four major types of associations are, in or-
der of frequency, co-ordination (co-hyponyms like
pepper and salt), collocation (like salt and wa-
ter), superordination (insect as a hypernym of but-
terfly) and synonymy (like starved and hungry).
As a result, a computational model that is able to
predict associations accurately has to deal with a
wide range of semantic relations. Past systems,
however, generally use only one type of informa-
tion (Wettler et al., 2005; Sahlgren, 2006; Michel-
bacher et al., 2007; Peirsman et al., 2008; Wand-
macher et al., 2008), which suggests that they are
relatively restricted in the number of associations
they will find.
In this article, we will focus on a set of Dutch
cue words and their single strongest association,
collected from a large psycholinguistic experi-
ment. Table 1 gives a few examples of such cue–
association pairs. It illustrates the different types
of linguistic phenomena that an association may
be motivated by. The first three word pairs are
based on similarity. In this case, strong associ-
ations can be hyponyms (as in amphibian–frog),
co-hyponyms (as in pepper–salt) or hypernyms of
their cue (as in robin–bird). The next three pairs
represent semantic links where no relation of sim-
ilarity plays a role. Instead, the associations seem
to be motivated by a topical relation to their cue,
which is possibly reflected by their frequent co-
occurrence in a corpus. The final three word pairs
suggest that morphological factors might play a
role, too. Often, a cue and its association form
the building blocks of a compound, and it is possi-
ble that one part of a compound calls up the other.
The examples show that the process of compound-
ing can go in either direction: the compound may
consist of cue plus association (as in cellomuziek
‘cello music’), or of association plus cue (as in
filmacteur ‘film actor’). While it is not clear if it
is the compounds themselves that motivate the as-
sociation, or whether it is just the topical relation
between their two parts, they might still be able to
help identify strong associations.
</bodyText>
<sectionHeader confidence="0.996448" genericHeader="method">
3 Approaches
</sectionHeader>
<bodyText confidence="0.999868">
Motivated by the three types of cue–association
pairs that we identified in Table 1, we study three
sources of information (two types of distributional
information, and one type of morphological infor-
mation) that may provide corpus-based evidence
for strong associatedness: collocation measures,
word space models and compounding.
</bodyText>
<subsectionHeader confidence="0.998793">
3.1 Collocation measures
</subsectionHeader>
<bodyText confidence="0.999675416666667">
Probably the most straightforward way to pre-
dict strong associations is to assume that a cue
and its strong association often co-occur in text.
As a result, we can use collocation measures
like point-wise mutual information (Church and
Hanks, 1989) or the log-likelihood ratio (Dunning,
1993) to predict the strong association for a given
cue. Point-wise mutual information (PMI) tells
us if two words w1 and w2 occur together more or
less often than expected on the basis of their indi-
vidual frequencies and the independence assump-
tion:
</bodyText>
<equation confidence="0.940928636363636">
P (w1, w2)
PMI(w1,w2) = log2 P(w1) ∗ P(w2)
The log-likelihood ratio compares the like-
lihoods L of the independence hypothesis (i.e.,
p = P(w2|w1) = P(w2|¬w1)) and the de-
pendence hypothesis (i.e., p1 = P(w2|w1) =6
P(w2|¬w1) = p2), under the assumption that the
words in a text are binomially distributed:
L(P(w2|w1); p) ∗ L(P(w2|¬w1); p)
log λ = log
L(P(w2|w1); p1) ∗ L(P(w2|¬w1); p2)
</equation>
<subsectionHeader confidence="0.999497">
3.2 Word Space Models
</subsectionHeader>
<bodyText confidence="0.99947375">
A respectable proportion (in our data about 18%)
of the strong associations are motivated by se-
mantic similarity to their cue. They can be syn-
onyms, hyponyms, hypernyms, co-hyponyms or
</bodyText>
<page confidence="0.998756">
649
</page>
<bodyText confidence="0.999939642857143">
antonyms. Collocation measures, however, are not
specifically targeted towards the discovery of se-
mantic similarity. Instead, they model similarity
mainly as a side effect of collocation. Therefore
we also investigated a large set of computational
models that were specifically developed for the
discovery of semantic similarity. These so-called
word space models or distributional models of lex-
ical semantics are motivated by the distributional
hypothesis, which claims that semantically simi-
lar words appear in similar contexts. As a result,
they model each word in terms of its contexts in
a corpus, as a so-called context vector. Distribu-
tional similarity is then operationalized as the sim-
ilarity between two such context vectors. These
models will thus look for possible associations by
searching words with a context vector similar to
the given cue.
Crucial in the implementation of word space
models is their definition of context. In the cur-
rent literature, there are basically three popular ap-
proaches. Document-based models use some sort
of textual entity as features (Landauer and Du-
mais, 1997; Sahlgren, 2006). Their context vec-
tors note what documents, paragraphs, articles or
similar stretches of text a target word appears in.
Without dimensionality reduction, in these mod-
els two words will be distributionally similar if
they often occur together in the same paragraph,
for instance. This approach still bears some simi-
larity to the collocation measures above, since it
relies on the direct co-occurrence of two words
in text. Second, syntax-based models focus on
the syntactic relationships in which a word takes
part (Lin, 1998). Here two words will be sim-
ilar when they often appear in the same syntac-
tic roles, like subject of fly. Third, word-
based models simply use as features the words
that appear in the context of the target, without
considering the syntactic relations between them.
Context is thus defined as the set of n words
around the target (Sahlgren, 2006). Obviously, the
choice of context size will again have a major in-
fluence on the behaviour of the model. Syntax-
based and word-based models differ from collo-
cation measures and document-based models in
that they do not search for words that co-occur
directly. Instead, they look for words that often
occur together with the same context words or
syntactic relations. Even though all these models
were originally developed to model semantic sim-
ilarity relations, syntax-based models have been
shown to favour such relations more than word-
based and document-based models, which might
capture more associative relationships (Sahlgren,
2006; Van der Plas, 2008).
</bodyText>
<subsectionHeader confidence="0.999553">
3.3 Compounding
</subsectionHeader>
<bodyText confidence="0.9999912">
As we have argued before, one characteristic of
cues and their strong associations is that they can
sometimes be combined into a compound. There-
fore we developed a third approach which dis-
covers for every cue the words in the corpus that
in combination with it lead to an existing com-
pound. Since in Dutch compounds are generally
written as one word, this is relatively easy. We at-
tached each candidate association to the cue (both
in the combination cue+association and associ-
ation+cue), following a number of simple mor-
phological rules for compounding. We then de-
termined if any of these hypothetical compounds
occurred in the corpus. The possible associa-
tions that led to an observed compound were then
ranked according to the frequency of that com-
pound.1 Note that, for languages where com-
pounds are often spelled as two words, like En-
glish, our approach will have to recognize multi-
word units to deal with this issue.
</bodyText>
<subsectionHeader confidence="0.996848">
3.4 Previous research
</subsectionHeader>
<bodyText confidence="0.999990571428571">
In previous research, most attention has gone out
to the first two of our models. Sahlgren (2006)
tries to find associations with word space mod-
els. He argues that document-based models are
better suited to the discovery of associations than
word-based ones. In addition, Sahlgren (2006) as
well as Peirsman et al. (2008) show that in word-
based models, large context sizes are more effec-
tive than small ones. This supports Wandmacher
et al.’s (2008) model of associations, which uses a
context size of 75 words to the left and right of the
target. However, Peirsman et al. (2008) find that
word-based distributional models are clearly out-
performed by simple collocation measures, par-
ticularly the log-likelihood ratio. Such colloca-
tion measures are also used by Michelbacher et al.
(2007) in their classification of asymmetric associ-
ations. They show the chi-square metric to be a ro-
bust classifier of associations as either symmetric
or asymmetric, while a measure based on condi-
tional probabilities is particularly suited to model
</bodyText>
<footnote confidence="0.9858335">
1If both compounds cue+association and association+cue
occurred in the corpus, their frequencies were summed.
</footnote>
<page confidence="0.987344">
650
</page>
<figure confidence="0.978608615384615">
median rank of most frequent association
2 5 10 20 50 100
●
●
● word−based no stoplist
word−based stoplist
pmi statistic
log−likelihood statistic
compound−based
syntax−based
document−based
2 4 6 8 10
context size
</figure>
<figureCaption confidence="0.99997">
Figure 1: Median rank of the strong associations.
</figureCaption>
<bodyText confidence="0.999744875">
the magnitude of asymmetry. In a similar vein,
Wettler et al. (2005) successfully predict associa-
tions on the basis of co-occurrence in text, in the
framework of associationist learning theory. De-
spite this wealth of systems, it is an open question
how their results compare to each other. More-
over, a model that combines several of these sys-
tems might outperform any basic approach.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999228">
Our experiments were inspired by the association
prediction task at the ESSLLI-2008 workshop on
distributional models. We will first present this
precise setup and then go into the results and their
implications.
</bodyText>
<subsectionHeader confidence="0.982912">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999968657894737">
Our data was the Twente Nieuws Corpus (TwNC),
which contains 300 million words of Dutch news-
paper articles. This corpus was compiled at the
University of Twente and subsequently parsed by
the Alpino parser at the University of Gronin-
gen (van Noord, 2006). The newspaper arti-
cles in the corpus served as the contextual fea-
tures for the document-based system; the depen-
dency triples output by Alpino were used as in-
put for the syntax-based approach. These syntactic
features of the type subject of fly covered
eight syntactic relations — subject, direct object,
prepositional complement, adverbial prepositional
phrase, adjective modification, PP postmodifica-
tion, apposition and coordination. Finally, the col-
location measures and word-based distributional
models took into account context sizes ranging
from one to ten words to the left and right of the
target.
Because of its many parameters, the precise im-
plementation of the word space models deserves a
bit more attention. In all cases, we used the con-
text vectors in their full dimensionality. While this
is somewhat of an exception in the literature, it
has been argued that the full dimensionality leads
to the best results for word-based models at least
(Bullinaria and Levy, 2007). For the syntax-based
and word-based approaches, we only took into ac-
count features that occurred at least two times to-
gether with the target. For the word-based models,
we experimented with the use of a stoplist, which
allowed us to exclude semantically “empty” words
as features. The simple co-occurrence frequencies
in the context vectors were replaced by the point-
wise mutual information between the target and
the feature (Bullinaria and Levy, 2007; Van der
Plas, 2008). The similarity between two vectors
was operationalized as the cosine of the angle be-
</bodyText>
<page confidence="0.989859">
651
</page>
<table confidence="0.999152933333333">
similar related, not similar
mean med rank1 mean med rank1
16.4 4 23% 25.2 9 10%
12.8 2 41% 18.0 3 31%
16.3 4 22% 61.9 70 2%
10.7 3 27% 36.9 17 12%
10.1 3 26% 20.2 4 26%
80.7 101 5% 51.9 26 12%
models
pmi context 10
log-likelihood ratio context 10
syntax-based
word-based context 10 stoplist
document-based
compounding
</table>
<tableCaption confidence="0.9891605">
Table 2: Performance of the models on semantically similar cue-association pairs and related but not
similar pairs.
</tableCaption>
<bodyText confidence="0.994413311111111">
med = median; rank1 = number of associations at rank 1
tween them. This measure is more or less stan-
dard in the literature and leads to state-of-the-art
results (Sch¨utze, 1998; Pad´o and Lapata, 2007;
Bullinaria and Levy, 2007). While the cosine is a
symmetric measure, however, association strength
is asymmetric. For example, snelheid (‘speed’)
triggered auto (‘car’) no fewer than 55 times in
the experiment, whereas auto evoked snelheid a
mere 3 times. Like Michelbacher et al. (2007), we
solve this problem by focusing not on the similar-
ity score itself, but on the rank of the association in
the list of nearest neighbours to the cue. We thus
expect that auto will have a much higher rank in
the list of nearest neighbours to snelheid than vice
versa.
Our Gold Standard was based on a large-scale
psycholinguistic experiment conducted at the Uni-
versity of Leuven (De Deyne and Storms, 2008).
In this experiment, participants were asked to list
three different associations for all cue words they
were presented with. Each of the 1425 cues was
given to at least 82 participants, resulting in a to-
tal of 381,909 responses. From this set, we took
only noun cues with a single strong association.
This means we found the most frequent associ-
ation to each cue, and only included the pair in
the test set if the association occurred at least 1.5
times more often than the second most frequent
one. This resulted in a final test set of 593 cue-
association pairs. Next we brought together all the
associations in a set of candidate associations, and
complemented it with 1000 random words from
the corpus with a frequency of at least 200. From
these candidate words, we had each model select
the 100 highest scoring ones (the nearest neigh-
bours). Performance was then expressed as the
median and mean rank of the strongest association
in this list. Associations absent from the list auto-
matically received a rank of 101. Thus, the lower
the rank, the better the performance of the system.
While there are obviously many more ways of as-
sembling a test set and scoring the several systems,
we found these all gave very similar results to the
ones reported here.
</bodyText>
<subsectionHeader confidence="0.604207">
4.2 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999939">
The median ranks of the strong associations for all
models are plotted in Figure 1. The means show
the same pattern, but give a less clear indication of
the number of associations that were suggested in
the top n most likely candidates. The most suc-
cessful approach is the log-likelihood ratio (me-
dian 3 with a context size of 10, mean 16.6),
followed by the document-based model (median
4, mean 18.4) and point-wise mutual informa-
tion (median 7 with a context size of 10, mean
23.1). Next in line are the word-based distribu-
tional models with and without a stoplist (high-
est medians at 11 and 12, highest means at 30.9
and 33.3, respectively), and then the syntax-based
word space model (median 42, mean 51.1). The
worst performance is recorded for the compound-
ing approach (median 101, mean 56.7). Overall,
corpus-based approaches that rely on direct co-
occurrence thus seem most appropriate for the pre-
diction of strong associations to a cue. This is
probably a result of two factors. First, collocation
itself is an important motivation for human asso-
ciations (Aitchinson, 2003). Second, while col-
location approaches in themselves do not target
semantic similarity, semantically similar associa-
tions are often also collocates to their cues. This is
particularly the case for co-hyponyms, like pepper
and salt, which score very high both in terms of
collocation and in terms of similarity.
Let us discuss the results of all models in a bit
</bodyText>
<page confidence="0.997538">
652
</page>
<figureCaption confidence="0.999117">
Figure 2: Performance of the models in three cue and association frequency bands.
</figureCaption>
<figure confidence="0.996825045454545">
median rank of strongest association
1 2 5 10 20 50 100
median rank of strongest association
1 2 5 10 20 50 100
association frequency
cue frequency
●
●
●
high mid low
Index
●
●
high mid low
Index
● pmi context 10
log−likelihood context 10
syntax−based
word−based context 10 stoplist
document−based
compounding
●
</figure>
<bodyText confidence="0.998064071428572">
more detail. A first factor of interest is the dif-
ference between associations that are similar to
their cue and those which are related but not simi-
lar. Most of our models show a crucial difference
in performance with respect to these two classes.
The most important results are given in Table 2.
The log-likelihood ratio gives the highest number
of associations at rank 1 for both classes. Par-
ticularly surprising is its strong performance with
respect to semantic similarity, since this relation
is only a side effect of collocation. In fact, the
log-likelihood ratio scores better at predicting se-
mantically similar associations than related but not
similar associations. Its performance moreover
lies relatively close to that of the word space mod-
els, which were specifically developed to model
semantic similarity. This underpins the observa-
tion that even associations that are semantically
similar to their cues are still highly motivated by
direct co-occurrence in text. Interestingly, only the
compounding approach has a clear preference for
associations that are related to their cue, but not
similar.
A second factor that influences the performance
of the models is frequency. In order to test its
precise impact, we split up the cues and their as-
sociations in three frequency bands of compara-
ble size. For the cues, we constructed a band
for words with a frequency of less than 500 in
the corpus (low), between 500 and 2,500 (mid)
and more than 2,500 (high). For the associations,
we had bands for words with a frequency of less
than 7,500 (low), between 7,500 and 20,000 (mid)
and more than 20,000 (high). Figure 2 shows
the performance of the most important models in
these frequency bands. With respect to cue fre-
quency, the word space models and compound-
ing approach suffer most from low frequencies
and hence, data sparseness. The log-likelihood
ratio is much more robust, while point-wise mu-
tual information even performs better with low-
frequency cues, although it does not yet reach
the performance of the document-based system
or the log-likelihood ratio. With respect to asso-
ciation frequency, the picture is different. Here
the word-based distributional models and PMI per-
form better with low-frequency associations. The
document-based approach is largely insensitive to
association frequency, while the log-likelihood ra-
tio suffers slightly from low frequencies. The per-
formance of the compounding approach decreases
most. What is particularly interesting about this
plot is that it points towards an important differ-
ence between the log-likelihood ratio and point-
wise mutual information. In its search for nearest
neighbours to a given cue word, the log-likelihood
ratio favours frequent words. This is an advanta-
geous feature in the prediction of strong associa-
tions, since people tend to give frequent words as
associations. PMI, like the syntax-based and word-
based models, lacks this characteristic. It therefore
fails to discover mid- and high-frequency associa-
tions in particular.
Finally, despite the similarity in results between
the log-likelihood ratio and the document-based
word space model, there exists substantial varia-
tion in the associations that they predict success-
fully. Table 3 gives an overview of the top ten as-
sociations that are predicted better by one model
than the other, according to the difference be-
</bodyText>
<page confidence="0.998616">
653
</page>
<bodyText confidence="0.7390715">
model cue–association pairs
document-based model cue–billiards, amphibian–frog, fair–doughnut ball, sperm whale–sea,
map–trip, avocado–green, carnivore–meat, one-wheeler–circus,
wallet–money, pinecone–wood
log-likelihood ratio top–toy, oven–hot, sorbet–ice cream, rhubarb–sour, poppy–red,
knot–rope, pepper–red, strawberry–red, massage–oil, raspberry–red
</bodyText>
<tableCaption confidence="0.918609">
Table 3: A comparison of the document-based model and the log-likelihood ratio on the basis of the
</tableCaption>
<bodyText confidence="0.98200244">
cue–target pairs with the largest difference in log ranks between the two approaches.
tween the models in the logarithm of the rank of
the association. The log-likelihood ratio seems
to be biased towards “characteristics” of the tar-
get. For instance, it finds the strong associative
relation between poppy, pepper, strawberry, rasp-
berry and their shared colour red much better than
the document-based model, just like it finds the re-
latedness between oven and hot and rhubarb and
sour. The document-based model recovers more
associations that display a strong topical connec-
tion with their cue word. This is thanks to its re-
liance on direct co-occurrence within a large con-
text, which makes it less sensitive to semantic sim-
ilarity than word-based models. It also appears to
have less of a bias toward frequent words than the
log-likelihood ratio. Note, for instance, the pres-
ence of doughnut ball (or smoutebol in Dutch) as
the third nearest neighbour to fair, despite the fact
it occurs only once (!) in the corpus. This com-
plementarity between our two most successful ap-
proaches suggests that a combination of the two
may lead to even better results. We therefore in-
vestigated the benefits of a committee-based or en-
semble approach.
</bodyText>
<sectionHeader confidence="0.847657" genericHeader="method">
5 Ensemble-based prediction of strong
associations
</sectionHeader>
<bodyText confidence="0.999964396226415">
Given the varied nature of cue–association rela-
tions, it could be beneficial to develop a model that
relies on more than one type of information. En-
semble methods have already proved their effec-
tiveness in the related area of automatic thesaurus
extraction (Curran, 2002), where semantic similar-
ity is the target relation. Curran (2002) explored
three ways of combining multiple ordered sets of
words: (1) mean, taking the mean rank of each
word over the ensemble; (2) harmonic, taking the
harmonic mean; (3) mixture, calculating the mean
similarity score for each word. We will study only
the first two of these approaches, as the different
metrics of our models cannot simply be combined
in a mean relatedness score. More particularly, we
will experiment with ensembles taking the (har-
monic) mean of the natural logarithm of the ranks,
since we found these to perform better than those
working with the original ranks.2
Table 4 compares the results of the most im-
portant ensembles with that of the single best ap-
proach, the log-likelihood ratio with a context size
of 10. By combining the two best approaches
from the previous section, the log-likelihood ra-
tio and the document-based model, we already
achieve a substantial increase in performance. The
mean rank of the association goes from 3 to 2,
the mean from 16.6 to 13.1 and the number of
strong associations with rank 1 climbs from 194
to 223. This is a statistically significant increase
(one-tailed paired Wilcoxon test, W = 30866,
p = .0002). Adding another word space model
to the ensemble, either a word-based or syntax-
based model, brings down performance. However,
the addition of the compound model does lead to a
clear gain in performance. This ensemble finds the
strongest association at a median rank of 2, and a
mean of 11.8. In total, 249 strong associations (out
of a total 593) are presented as the best candidate
by the model — an increase of 28.4% compared
to the log-likelihood ratio. Hence, despite its poor
performance as a simple model, the compound-
based approach can still give useful information
about the strong association of a cue word when
combined with other models. Based on the origi-
nal ranks, the increase from the previous ensem-
ble is not statistically significant (W = 23929,
p = .31). If we consider differences at the start
of the neighbour list more important and compare
the logarithms of the ranks, however, the increase
becomes significant (W = 29787.5, p = 0.0008).
Its precise impact should thus further be investi-
gated.
</bodyText>
<footnote confidence="0.9759865">
2In the case of the harmonic mean, we actually take the
logarithm of rank+1, in order to avoid division by zero.
</footnote>
<page confidence="0.993504">
654
</page>
<table confidence="0.96780275">
mean harmonic mean
med mean rank1 med mean rank1
3 16.6 194
2 13.1 223 3 13.4 211
3 13.8 182 3 14.2 187
3 14.4 179 4 14.7 184
2 11.8 249 2 12.2 221
systems
</table>
<equation confidence="0.9841544">
loglik10 (baseline)
loglik10 + doc
loglik10 + doc + word10
loglik10 + doc + syn
loglik10 + doc + comp
</equation>
<tableCaption confidence="0.657745">
Table 4: Results of ensemble methods.
</tableCaption>
<equation confidence="0.953859">
loglik10 = log-likelihood ratio with context size 10;
doc = document-based model;
word10 = word-based model with context size 10 and a stoplist;
syn = syntax-based model;
comp = compound-based model;
med = median; rank1 = number of associations at rank 1
</equation>
<bodyText confidence="0.999992333333333">
Let us finally take a look at the types of strong
associations that still tend to receive a low rank in
this ensemble system. The first group consists of
adjectives that refer to an inherent characteristic of
the cue word that is rarely mentioned in text. This
is the case for tennis ball–yellow, cheese–yellow,
grapefruit–bitter. The second type brings together
polysemous cues whose strongest association re-
lates to a different sense than that represented by
its corpus-based nearest neighbour. This applies
to Dutch kant, which is polysemous between side
and lace. Its strongest association, Bruges, is
clearly related to the latter meaning, but its corpus-
based neighbours ball and water suggest the for-
mer. The third type reflects human encyclopaedic
knowledge that is less central to the semantics of
the cue word. Examples are police–blue, love–red,
or triangle–maths. In many of these cases, it ap-
pears that the failure of the model to recover the
strong associations results from corpus limitations
rather than from the model itself.
</bodyText>
<sectionHeader confidence="0.996359" genericHeader="conclusions">
6 Conclusions and future research
</sectionHeader>
<bodyText confidence="0.999982194444444">
In this paper, we explored three types of basic ap-
proaches to the prediction of strong associations
to a given cue. Collocation measures like the log-
likelihood ratio simply recover those words that
strongly collocate with the cue. Word space mod-
els look for words that appear in similar contexts,
defined as documents, context words or syntac-
tic relations. The compounding approach, finally,
searches for words that combine with the target to
form a compound. The log-likelihood ratio with
a large context size emerged as the best predic-
tor of strong association, followed closely by the
document-based word space model. Moreover,
we showed that an ensemble method combining
the log-likelihood ratio, the document-based word
space model and the compounding approach, out-
performed any of the basic methods by almost
30%.
In a number of ways, this paper is only a first
step towards the successful modelling of cue-
association relations. First, the newspaper cor-
pus that served as our data has some restrictions,
particularly with respect to diversity of genres. It
would be interesting to investigate to what degree
a more general corpus — a web corpus, for in-
stance — would be able to accurately predict a
wider range of associations. Second, the mod-
els themselves might benefit from some additional
features. For instance, we are curious to find
out what the influence of dimensionality reduction
would be, particularly for document-based word
space models. Finally, we would like to extend
our test set from strong associations to more asso-
ciations for a given target, in order to investigate
how well the discussed models predict relative as-
sociation strength.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999566666666667">
Jean Aitchinson. 2003. Words in the Mind. An Intro-
duction to the Mental Lexicon. Blackwell, Oxford.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
haviour Research Methods, 39:510–526.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: Words, sentences,
discourse. Discourse Processes, 25:211–257.
</reference>
<page confidence="0.986024">
655
</page>
<reference confidence="0.999750555555556">
Ed H. Chi, Peter Pirolli, Kim Chen, and James Pitkow.
2001. Using information scent to model user infor-
mation needs and actions on the web. In Proceed-
ings of the ACM Conference on Human Factors and
Computing Systems (CHI 2001), pages 490–497.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicog-
raphy. In Proceedings of ACL-27, pages 76–83.
James R. Curran. 2002. Ensemble methods for au-
tomatic thesaurus extraction. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), pages 222–229.
Simon De Deyne and Gert Storms. 2008. Word asso-
ciations: Norms for 1,424 Dutch words in a contin-
uous task. Behaviour Research Methods, 40:198–
205.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19:61–74.
Peter W. Foltz. 1996. Latent Semantic Analysis for
text-based research. Behaviour Research Methods,
Instruments, and Computers, 29:197–202.
Tuomo Kakkonen, Niko Myller, Jari Timonen, and
Erkki Sutinen. 2005. Automatic essay grading with
probabilistic latent semantic analysis. In Proceed-
ings of the 2nd Workshop on Building Educational
Applications Using NLP, pages 29–36.
Thomas K. Landauer and Susan T. Dumais. 1997. A
solution to Plato’s problem: The Latent Semantic
Analysis theory of acquisition, induction and rep-
resentation of knowledge. Psychological Review,
104(2):211–240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768–774, Montreal, Canada.
Will Lowe and Scott McDonald. 2000. The di-
rect route: Mediated priming in semantic space.
In Proceedings of COGSCI 2000, pages 675–680.
Lawrence Erlbaum Associates.
Lukas Michelbacher, Stefan Evert, and Hinrich
Sch¨utze. 2007. Asymmetric association measures.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-07).
Tom M. Mitchell, Svetlana V. Shinkareva, An-
drew Carlson, Kai-Min Chang, Vicente L. Malva,
Robert A. Mason, and Marcel Adam Just. 2008.
Predicting human brain activity associated with the
meanings of nouns. Science, 320:1191–1195.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts.
2008. Size matters. Tight and loose context defini-
tions in English word space models. In Proceedings
of the ESSLLI Workshop on Distributional Lexical
Semantics, pages 9–16.
Magnus Sahlgren. 2006. The Word-Space Model.
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations Between Words
in High-dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.
Sabine Schulte im Walde and Alissa Melinger. 2005.
Identifying semantic relations and functional prop-
erties of human verb associations. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 612–619.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
124.
Lonneke Van der Plas. 2008. Automatic Lexico-
Semantic Acquisition for Question Answering.
Ph.D. thesis, University of Groningen, Groningen,
The Netherlands.
Gertjan van Noord. 2006. At last parsing is now oper-
ational. In Piet Mertens, C´edrick Fairon, Anne Dis-
ter, and Patrick Watrin, editors, Verbum Ex Machina.
Actes de la 13e Conf´erence sur le Traitement Au-
tomatique des Langues Naturelles (TALN), pages
20–42.
Tonio Wandmacher, Ekaterina Ovchinnikova, and
Theodore Alexandrov. 2008. Does Latent Seman-
tic Analysis reflect human associations? In Pro-
ceedings of the ESSLLI Workshop on Distributional
Lexical Semantics, pages 63–70.
Manfred Wettler, Reinhard Rapp, and Peter Sedlmeier.
2005. Free word associations correspond to contigu-
ities between words in texts. Journal of Quantitative
Linguistics, 12(2/3):111–122.
</reference>
<page confidence="0.998906">
656
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.345102">
<title confidence="0.999839">Predicting Strong Associations on the Basis of Corpus Data</title>
<author confidence="0.995396">Yves Peirsman Dirk Geeraerts</author>
<affiliation confidence="0.772520333333333">Research Foundation – Flanders &amp; QLVL, University of Leuven QLVL, University of Leuven Leuven, Belgium Belgium dirk.geeraerts@arts.kuleuven.be</affiliation>
<email confidence="0.763695">yves.peirsman@arts.kuleuven.be</email>
<abstract confidence="0.997458421052631">Current approaches to the prediction of associations rely on just one type of information, generally taking the form of either word space models or collocation measures. At the moment, it is an open question how these approaches compare to one another. In this paper, we will investigate the performance of these two types of models and that of a new approach based on compounding. The best single predictor is the log-likelihood ratio, followed closely by the document-based word space model. We will show, however, that an ensemble method that combines these two best approaches with the compounding algorithm achieves an increase in performance of almost 30% over the current state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Aitchinson</author>
</authors>
<title>Words in the Mind. An Introduction to the Mental Lexicon.</title>
<date>2003</date>
<location>Blackwell, Oxford.</location>
<contexts>
<context position="1175" citStr="Aitchinson (2003)" startWordPosition="184" endWordPosition="185">estigate the performance of these two types of models and that of a new approach based on compounding. The best single predictor is the log-likelihood ratio, followed closely by the document-based word space model. We will show, however, that an ensemble method that combines these two best approaches with the compounding algorithm achieves an increase in performance of almost 30% over the current state of the art. 1 Introduction Associations are words that immediately come to mind when people hear or read a given cue word. For instance, a word like pepper calls up salt, and wave calls up sea. Aitchinson (2003) and Schulte im Walde and Melinger (2005) show that such associations can be motivated by a number of factors, from semantic similarity to collocation. Current computational models of association, however, tend to focus on one of these, by using either collocation measures (Michelbacher et al., 2007) or word space models (Sahlgren, 2006; Peirsman et al., 2008). To this day, two general problems remain. First, the literature lacks a comprehensive comparison between these general types of models. Second, we are still looking for an approach that combines several sources of information, so as to </context>
<context position="4581" citStr="Aitchinson (2003)" startWordPosition="731" endWordPosition="732">ter of the ACL, pages 648–656, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 648 cue association amfibie (‘amphibian’) kikker (‘frog’) peper (‘pepper’) zout (‘salt’) roodborstje (‘robin’) vogel (‘bird’) granaat (‘grenade’) oorlog (‘war’) helikopter (‘helicopter’) vliegen (‘to fly’) werk (‘job’) geld (‘money’) acteur (‘actor’) film (‘film’) cello (‘cello’) muziek (‘music’) kruk (‘stool’) bar (‘bar’) Table 1: Examples of cues and their strongest association. 2 Associations There are several reasons why a word may be associated to its cue. According to Aitchinson (2003), the four major types of associations are, in order of frequency, co-ordination (co-hyponyms like pepper and salt), collocation (like salt and water), superordination (insect as a hypernym of butterfly) and synonymy (like starved and hungry). As a result, a computational model that is able to predict associations accurately has to deal with a wide range of semantic relations. Past systems, however, generally use only one type of information (Wettler et al., 2005; Sahlgren, 2006; Michelbacher et al., 2007; Peirsman et al., 2008; Wandmacher et al., 2008), which suggests that they are relatively</context>
<context position="19316" citStr="Aitchinson, 2003" startWordPosition="3154" endWordPosition="3155">e of 10, mean 23.1). Next in line are the word-based distributional models with and without a stoplist (highest medians at 11 and 12, highest means at 30.9 and 33.3, respectively), and then the syntax-based word space model (median 42, mean 51.1). The worst performance is recorded for the compounding approach (median 101, mean 56.7). Overall, corpus-based approaches that rely on direct cooccurrence thus seem most appropriate for the prediction of strong associations to a cue. This is probably a result of two factors. First, collocation itself is an important motivation for human associations (Aitchinson, 2003). Second, while collocation approaches in themselves do not target semantic similarity, semantically similar associations are often also collocates to their cues. This is particularly the case for co-hyponyms, like pepper and salt, which score very high both in terms of collocation and in terms of similarity. Let us discuss the results of all models in a bit 652 Figure 2: Performance of the models in three cue and association frequency bands. median rank of strongest association 1 2 5 10 20 50 100 median rank of strongest association 1 2 5 10 20 50 100 association frequency cue frequency ● ● ●</context>
</contexts>
<marker>Aitchinson, 2003</marker>
<rawString>Jean Aitchinson. 2003. Words in the Mind. An Introduction to the Mental Lexicon. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behaviour Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="15050" citStr="Bullinaria and Levy, 2007" startWordPosition="2423" endWordPosition="2426">ase, adjective modification, PP postmodification, apposition and coordination. Finally, the collocation measures and word-based distributional models took into account context sizes ranging from one to ten words to the left and right of the target. Because of its many parameters, the precise implementation of the word space models deserves a bit more attention. In all cases, we used the context vectors in their full dimensionality. While this is somewhat of an exception in the literature, it has been argued that the full dimensionality leads to the best results for word-based models at least (Bullinaria and Levy, 2007). For the syntax-based and word-based approaches, we only took into account features that occurred at least two times together with the target. For the word-based models, we experimented with the use of a stoplist, which allowed us to exclude semantically “empty” words as features. The simple co-occurrence frequencies in the context vectors were replaced by the pointwise mutual information between the target and the feature (Bullinaria and Levy, 2007; Van der Plas, 2008). The similarity between two vectors was operationalized as the cosine of the angle be651 similar related, not similar mean m</context>
<context position="16278" citStr="Bullinaria and Levy, 2007" startWordPosition="2631" endWordPosition="2634"> rank1 mean med rank1 16.4 4 23% 25.2 9 10% 12.8 2 41% 18.0 3 31% 16.3 4 22% 61.9 70 2% 10.7 3 27% 36.9 17 12% 10.1 3 26% 20.2 4 26% 80.7 101 5% 51.9 26 12% models pmi context 10 log-likelihood ratio context 10 syntax-based word-based context 10 stoplist document-based compounding Table 2: Performance of the models on semantically similar cue-association pairs and related but not similar pairs. med = median; rank1 = number of associations at rank 1 tween them. This measure is more or less standard in the literature and leads to state-of-the-art results (Sch¨utze, 1998; Pad´o and Lapata, 2007; Bullinaria and Levy, 2007). While the cosine is a symmetric measure, however, association strength is asymmetric. For example, snelheid (‘speed’) triggered auto (‘car’) no fewer than 55 times in the experiment, whereas auto evoked snelheid a mere 3 times. Like Michelbacher et al. (2007), we solve this problem by focusing not on the similarity score itself, but on the rank of the association in the list of nearest neighbours to the cue. We thus expect that auto will have a much higher rank in the list of nearest neighbours to snelheid than vice versa. Our Gold Standard was based on a large-scale psycholinguistic experim</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behaviour Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curt Burgess</author>
<author>Kay Livesay</author>
<author>Kevin Lund</author>
</authors>
<date>1998</date>
<booktitle>Explorations in context space: Words, sentences, discourse. Discourse Processes,</booktitle>
<pages>25--211</pages>
<contexts>
<context position="2297" citStr="Burgess et al., 1998" startWordPosition="357" endWordPosition="360">cond, we are still looking for an approach that combines several sources of information, so as to correctly predict a larger variety of associations. Most computational models of semantic relations aim to model semantic similarity in particular (Landauer and Dumais, 1997; Lin, 1998; Pad´o and Lapata, 2007). In Natural Language Processing, these models have applications in fields like query expansion, thesaurus extraction, information retrieval, etc. Similarly, in Cognitive Science, such models have helped explain neural activation (Mitchell et al., 2008), sentence and discourse comprehension (Burgess et al., 1998; Foltz, 1996; Landauer and Dumais, 1997) and priming patterns (Lowe and McDonald, 2000), to name just a few examples. However, there are a number of applications and research fields that will surely benefit from models that target the more general phenomenon of association. For instance, automatically predicted associations may prove useful in models of information scent, which seek to explain the paths that users follow in their search for relevant information on the web (Chi et al., 2001). After all, if the visitor of a web shop clicks on music to find the prices of iPods, this behaviour is</context>
</contexts>
<marker>Burgess, Livesay, Lund, 1998</marker>
<rawString>Curt Burgess, Kay Livesay, and Kevin Lund. 1998. Explorations in context space: Words, sentences, discourse. Discourse Processes, 25:211–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ed H Chi</author>
<author>Peter Pirolli</author>
<author>Kim Chen</author>
<author>James Pitkow</author>
</authors>
<title>Using information scent to model user information needs and actions on the web.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM Conference on Human Factors and Computing Systems (CHI</booktitle>
<pages>490--497</pages>
<contexts>
<context position="2793" citStr="Chi et al., 2001" startWordPosition="440" endWordPosition="443">have helped explain neural activation (Mitchell et al., 2008), sentence and discourse comprehension (Burgess et al., 1998; Foltz, 1996; Landauer and Dumais, 1997) and priming patterns (Lowe and McDonald, 2000), to name just a few examples. However, there are a number of applications and research fields that will surely benefit from models that target the more general phenomenon of association. For instance, automatically predicted associations may prove useful in models of information scent, which seek to explain the paths that users follow in their search for relevant information on the web (Chi et al., 2001). After all, if the visitor of a web shop clicks on music to find the prices of iPods, this behaviour is motivated by an associative relation different from similarity. Other possible applications lie in the field of models of text coherence (Landauer and Dumais, 1997) and automated essay grading (Kakkonen et al., 2005). In addition, all research in Cognitive Science that we have referred to above could benefit from computational models of association in order to study the effects of association in comparison to those of similarity. Our article is structured as follows. In section 2, we will d</context>
</contexts>
<marker>Chi, Pirolli, Chen, Pitkow, 2001</marker>
<rawString>Ed H. Chi, Peter Pirolli, Kim Chen, and James Pitkow. 2001. Using information scent to model user information needs and actions on the web. In Proceedings of the ACM Conference on Human Factors and Computing Systems (CHI 2001), pages 490–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of ACL-27,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="7285" citStr="Church and Hanks, 1989" startWordPosition="1171" endWordPosition="1174">ions. 3 Approaches Motivated by the three types of cue–association pairs that we identified in Table 1, we study three sources of information (two types of distributional information, and one type of morphological information) that may provide corpus-based evidence for strong associatedness: collocation measures, word space models and compounding. 3.1 Collocation measures Probably the most straightforward way to predict strong associations is to assume that a cue and its strong association often co-occur in text. As a result, we can use collocation measures like point-wise mutual information (Church and Hanks, 1989) or the log-likelihood ratio (Dunning, 1993) to predict the strong association for a given cue. Point-wise mutual information (PMI) tells us if two words w1 and w2 occur together more or less often than expected on the basis of their individual frequencies and the independence assumption: P (w1, w2) PMI(w1,w2) = log2 P(w1) ∗ P(w2) The log-likelihood ratio compares the likelihoods L of the independence hypothesis (i.e., p = P(w2|w1) = P(w2|¬w1)) and the dependence hypothesis (i.e., p1 = P(w2|w1) =6 P(w2|¬w1) = p2), under the assumption that the words in a text are binomially distributed: L(P(w2</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information and lexicography. In Proceedings of ACL-27, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>Ensemble methods for automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2002),</booktitle>
<pages>222--229</pages>
<contexts>
<context position="25444" citStr="Curran, 2002" startWordPosition="4118" endWordPosition="4119">ighbour to fair, despite the fact it occurs only once (!) in the corpus. This complementarity between our two most successful approaches suggests that a combination of the two may lead to even better results. We therefore investigated the benefits of a committee-based or ensemble approach. 5 Ensemble-based prediction of strong associations Given the varied nature of cue–association relations, it could be beneficial to develop a model that relies on more than one type of information. Ensemble methods have already proved their effectiveness in the related area of automatic thesaurus extraction (Curran, 2002), where semantic similarity is the target relation. Curran (2002) explored three ways of combining multiple ordered sets of words: (1) mean, taking the mean rank of each word over the ensemble; (2) harmonic, taking the harmonic mean; (3) mixture, calculating the mean similarity score for each word. We will study only the first two of these approaches, as the different metrics of our models cannot simply be combined in a mean relatedness score. More particularly, we will experiment with ensembles taking the (harmonic) mean of the natural logarithm of the ranks, since we found these to perform b</context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>James R. Curran. 2002. Ensemble methods for automatic thesaurus extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), pages 222–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon De Deyne</author>
<author>Gert Storms</author>
</authors>
<title>Word associations: Norms for 1,424 Dutch words in a continuous task.</title>
<date>2008</date>
<journal>Behaviour Research Methods,</journal>
<volume>40</volume>
<pages>205</pages>
<marker>De Deyne, Storms, 2008</marker>
<rawString>Simon De Deyne and Gert Storms. 2008. Word associations: Norms for 1,424 Dutch words in a continuous task. Behaviour Research Methods, 40:198– 205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--61</pages>
<contexts>
<context position="7329" citStr="Dunning, 1993" startWordPosition="1179" endWordPosition="1180">e–association pairs that we identified in Table 1, we study three sources of information (two types of distributional information, and one type of morphological information) that may provide corpus-based evidence for strong associatedness: collocation measures, word space models and compounding. 3.1 Collocation measures Probably the most straightforward way to predict strong associations is to assume that a cue and its strong association often co-occur in text. As a result, we can use collocation measures like point-wise mutual information (Church and Hanks, 1989) or the log-likelihood ratio (Dunning, 1993) to predict the strong association for a given cue. Point-wise mutual information (PMI) tells us if two words w1 and w2 occur together more or less often than expected on the basis of their individual frequencies and the independence assumption: P (w1, w2) PMI(w1,w2) = log2 P(w1) ∗ P(w2) The log-likelihood ratio compares the likelihoods L of the independence hypothesis (i.e., p = P(w2|w1) = P(w2|¬w1)) and the dependence hypothesis (i.e., p1 = P(w2|w1) =6 P(w2|¬w1) = p2), under the assumption that the words in a text are binomially distributed: L(P(w2|w1); p) ∗ L(P(w2|¬w1); p) log λ = log L(P(w</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19:61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
</authors>
<title>Latent Semantic Analysis for text-based research.</title>
<date>1996</date>
<journal>Behaviour Research Methods, Instruments, and Computers,</journal>
<pages>29--197</pages>
<contexts>
<context position="2310" citStr="Foltz, 1996" startWordPosition="361" endWordPosition="362">king for an approach that combines several sources of information, so as to correctly predict a larger variety of associations. Most computational models of semantic relations aim to model semantic similarity in particular (Landauer and Dumais, 1997; Lin, 1998; Pad´o and Lapata, 2007). In Natural Language Processing, these models have applications in fields like query expansion, thesaurus extraction, information retrieval, etc. Similarly, in Cognitive Science, such models have helped explain neural activation (Mitchell et al., 2008), sentence and discourse comprehension (Burgess et al., 1998; Foltz, 1996; Landauer and Dumais, 1997) and priming patterns (Lowe and McDonald, 2000), to name just a few examples. However, there are a number of applications and research fields that will surely benefit from models that target the more general phenomenon of association. For instance, automatically predicted associations may prove useful in models of information scent, which seek to explain the paths that users follow in their search for relevant information on the web (Chi et al., 2001). After all, if the visitor of a web shop clicks on music to find the prices of iPods, this behaviour is motivated by</context>
</contexts>
<marker>Foltz, 1996</marker>
<rawString>Peter W. Foltz. 1996. Latent Semantic Analysis for text-based research. Behaviour Research Methods, Instruments, and Computers, 29:197–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tuomo Kakkonen</author>
<author>Niko Myller</author>
<author>Jari Timonen</author>
<author>Erkki Sutinen</author>
</authors>
<title>Automatic essay grading with probabilistic latent semantic analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>29--36</pages>
<contexts>
<context position="3114" citStr="Kakkonen et al., 2005" startWordPosition="495" endWordPosition="498">rely benefit from models that target the more general phenomenon of association. For instance, automatically predicted associations may prove useful in models of information scent, which seek to explain the paths that users follow in their search for relevant information on the web (Chi et al., 2001). After all, if the visitor of a web shop clicks on music to find the prices of iPods, this behaviour is motivated by an associative relation different from similarity. Other possible applications lie in the field of models of text coherence (Landauer and Dumais, 1997) and automated essay grading (Kakkonen et al., 2005). In addition, all research in Cognitive Science that we have referred to above could benefit from computational models of association in order to study the effects of association in comparison to those of similarity. Our article is structured as follows. In section 2, we will discuss the phenomenon of association and introduce the variety of relations that it is motivated by. Parallel to these relations, section 3 presents the three basic types of approaches that we use to predict strong associations. Section 4 will first compare the results of these three approaches, for a total of 43 models</context>
</contexts>
<marker>Kakkonen, Myller, Timonen, Sutinen, 2005</marker>
<rawString>Tuomo Kakkonen, Niko Myller, Jari Timonen, and Erkki Sutinen. 2005. Automatic essay grading with probabilistic latent semantic analysis. In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP, pages 29–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1948" citStr="Landauer and Dumais, 1997" startWordPosition="307" endWordPosition="310">on. Current computational models of association, however, tend to focus on one of these, by using either collocation measures (Michelbacher et al., 2007) or word space models (Sahlgren, 2006; Peirsman et al., 2008). To this day, two general problems remain. First, the literature lacks a comprehensive comparison between these general types of models. Second, we are still looking for an approach that combines several sources of information, so as to correctly predict a larger variety of associations. Most computational models of semantic relations aim to model semantic similarity in particular (Landauer and Dumais, 1997; Lin, 1998; Pad´o and Lapata, 2007). In Natural Language Processing, these models have applications in fields like query expansion, thesaurus extraction, information retrieval, etc. Similarly, in Cognitive Science, such models have helped explain neural activation (Mitchell et al., 2008), sentence and discourse comprehension (Burgess et al., 1998; Foltz, 1996; Landauer and Dumais, 1997) and priming patterns (Lowe and McDonald, 2000), to name just a few examples. However, there are a number of applications and research fields that will surely benefit from models that target the more general ph</context>
<context position="9275" citStr="Landauer and Dumais, 1997" startWordPosition="1489" endWordPosition="1493">ntically similar words appear in similar contexts. As a result, they model each word in terms of its contexts in a corpus, as a so-called context vector. Distributional similarity is then operationalized as the similarity between two such context vectors. These models will thus look for possible associations by searching words with a context vector similar to the given cue. Crucial in the implementation of word space models is their definition of context. In the current literature, there are basically three popular approaches. Document-based models use some sort of textual entity as features (Landauer and Dumais, 1997; Sahlgren, 2006). Their context vectors note what documents, paragraphs, articles or similar stretches of text a target word appears in. Without dimensionality reduction, in these models two words will be distributionally similar if they often occur together in the same paragraph, for instance. This approach still bears some similarity to the collocation measures above, since it relies on the direct co-occurrence of two words in text. Second, syntax-based models focus on the syntactic relationships in which a word takes part (Lin, 1998). Here two words will be similar when they often appear i</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL98,</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1959" citStr="Lin, 1998" startWordPosition="311" endWordPosition="312">odels of association, however, tend to focus on one of these, by using either collocation measures (Michelbacher et al., 2007) or word space models (Sahlgren, 2006; Peirsman et al., 2008). To this day, two general problems remain. First, the literature lacks a comprehensive comparison between these general types of models. Second, we are still looking for an approach that combines several sources of information, so as to correctly predict a larger variety of associations. Most computational models of semantic relations aim to model semantic similarity in particular (Landauer and Dumais, 1997; Lin, 1998; Pad´o and Lapata, 2007). In Natural Language Processing, these models have applications in fields like query expansion, thesaurus extraction, information retrieval, etc. Similarly, in Cognitive Science, such models have helped explain neural activation (Mitchell et al., 2008), sentence and discourse comprehension (Burgess et al., 1998; Foltz, 1996; Landauer and Dumais, 1997) and priming patterns (Lowe and McDonald, 2000), to name just a few examples. However, there are a number of applications and research fields that will surely benefit from models that target the more general phenomenon of</context>
<context position="9818" citStr="Lin, 1998" startWordPosition="1577" endWordPosition="1578"> some sort of textual entity as features (Landauer and Dumais, 1997; Sahlgren, 2006). Their context vectors note what documents, paragraphs, articles or similar stretches of text a target word appears in. Without dimensionality reduction, in these models two words will be distributionally similar if they often occur together in the same paragraph, for instance. This approach still bears some similarity to the collocation measures above, since it relies on the direct co-occurrence of two words in text. Second, syntax-based models focus on the syntactic relationships in which a word takes part (Lin, 1998). Here two words will be similar when they often appear in the same syntactic roles, like subject of fly. Third, wordbased models simply use as features the words that appear in the context of the target, without considering the syntactic relations between them. Context is thus defined as the set of n words around the target (Sahlgren, 2006). Obviously, the choice of context size will again have a major influence on the behaviour of the model. Syntaxbased and word-based models differ from collocation measures and document-based models in that they do not search for words that co-occur directly</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLINGACL98, pages 768–774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
<author>Scott McDonald</author>
</authors>
<title>The direct route: Mediated priming in semantic space.</title>
<date>2000</date>
<booktitle>In Proceedings of COGSCI 2000,</booktitle>
<pages>675--680</pages>
<contexts>
<context position="2385" citStr="Lowe and McDonald, 2000" startWordPosition="370" endWordPosition="373">ion, so as to correctly predict a larger variety of associations. Most computational models of semantic relations aim to model semantic similarity in particular (Landauer and Dumais, 1997; Lin, 1998; Pad´o and Lapata, 2007). In Natural Language Processing, these models have applications in fields like query expansion, thesaurus extraction, information retrieval, etc. Similarly, in Cognitive Science, such models have helped explain neural activation (Mitchell et al., 2008), sentence and discourse comprehension (Burgess et al., 1998; Foltz, 1996; Landauer and Dumais, 1997) and priming patterns (Lowe and McDonald, 2000), to name just a few examples. However, there are a number of applications and research fields that will surely benefit from models that target the more general phenomenon of association. For instance, automatically predicted associations may prove useful in models of information scent, which seek to explain the paths that users follow in their search for relevant information on the web (Chi et al., 2001). After all, if the visitor of a web shop clicks on music to find the prices of iPods, this behaviour is motivated by an associative relation different from similarity. Other possible applicat</context>
</contexts>
<marker>Lowe, McDonald, 2000</marker>
<rawString>Will Lowe and Scott McDonald. 2000. The direct route: Mediated priming in semantic space. In Proceedings of COGSCI 2000, pages 675–680. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lukas Michelbacher</author>
<author>Stefan Evert</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Asymmetric association measures.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-07).</booktitle>
<marker>Michelbacher, Evert, Sch¨utze, 2007</marker>
<rawString>Lukas Michelbacher, Stefan Evert, and Hinrich Sch¨utze. 2007. Asymmetric association measures. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
<author>Svetlana V Shinkareva</author>
<author>Andrew Carlson</author>
<author>Kai-Min Chang</author>
<author>Vicente L Malva</author>
<author>Robert A Mason</author>
<author>Marcel Adam Just</author>
</authors>
<title>Predicting human brain activity associated with the meanings of nouns.</title>
<date>2008</date>
<journal>Science,</journal>
<pages>320--1191</pages>
<contexts>
<context position="2237" citStr="Mitchell et al., 2008" startWordPosition="349" endWordPosition="352">rehensive comparison between these general types of models. Second, we are still looking for an approach that combines several sources of information, so as to correctly predict a larger variety of associations. Most computational models of semantic relations aim to model semantic similarity in particular (Landauer and Dumais, 1997; Lin, 1998; Pad´o and Lapata, 2007). In Natural Language Processing, these models have applications in fields like query expansion, thesaurus extraction, information retrieval, etc. Similarly, in Cognitive Science, such models have helped explain neural activation (Mitchell et al., 2008), sentence and discourse comprehension (Burgess et al., 1998; Foltz, 1996; Landauer and Dumais, 1997) and priming patterns (Lowe and McDonald, 2000), to name just a few examples. However, there are a number of applications and research fields that will surely benefit from models that target the more general phenomenon of association. For instance, automatically predicted associations may prove useful in models of information scent, which seek to explain the paths that users follow in their search for relevant information on the web (Chi et al., 2001). After all, if the visitor of a web shop cl</context>
</contexts>
<marker>Mitchell, Shinkareva, Carlson, Chang, Malva, Mason, Just, 2008</marker>
<rawString>Tom M. Mitchell, Svetlana V. Shinkareva, Andrew Carlson, Kai-Min Chang, Vicente L. Malva, Robert A. Mason, and Marcel Adam Just. 2008. Predicting human brain activity associated with the meanings of nouns. Science, 320:1191–1195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Kris Heylen</author>
<author>Dirk Geeraerts</author>
</authors>
<title>Size matters. Tight and loose context definitions in English word space models.</title>
<date>2008</date>
<booktitle>In Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1537" citStr="Peirsman et al., 2008" startWordPosition="241" endWordPosition="244">performance of almost 30% over the current state of the art. 1 Introduction Associations are words that immediately come to mind when people hear or read a given cue word. For instance, a word like pepper calls up salt, and wave calls up sea. Aitchinson (2003) and Schulte im Walde and Melinger (2005) show that such associations can be motivated by a number of factors, from semantic similarity to collocation. Current computational models of association, however, tend to focus on one of these, by using either collocation measures (Michelbacher et al., 2007) or word space models (Sahlgren, 2006; Peirsman et al., 2008). To this day, two general problems remain. First, the literature lacks a comprehensive comparison between these general types of models. Second, we are still looking for an approach that combines several sources of information, so as to correctly predict a larger variety of associations. Most computational models of semantic relations aim to model semantic similarity in particular (Landauer and Dumais, 1997; Lin, 1998; Pad´o and Lapata, 2007). In Natural Language Processing, these models have applications in fields like query expansion, thesaurus extraction, information retrieval, etc. Simila</context>
<context position="5114" citStr="Peirsman et al., 2008" startWordPosition="816" endWordPosition="819"> several reasons why a word may be associated to its cue. According to Aitchinson (2003), the four major types of associations are, in order of frequency, co-ordination (co-hyponyms like pepper and salt), collocation (like salt and water), superordination (insect as a hypernym of butterfly) and synonymy (like starved and hungry). As a result, a computational model that is able to predict associations accurately has to deal with a wide range of semantic relations. Past systems, however, generally use only one type of information (Wettler et al., 2005; Sahlgren, 2006; Michelbacher et al., 2007; Peirsman et al., 2008; Wandmacher et al., 2008), which suggests that they are relatively restricted in the number of associations they will find. In this article, we will focus on a set of Dutch cue words and their single strongest association, collected from a large psycholinguistic experiment. Table 1 gives a few examples of such cue– association pairs. It illustrates the different types of linguistic phenomena that an association may be motivated by. The first three word pairs are based on similarity. In this case, strong associations can be hyponyms (as in amphibian–frog), co-hyponyms (as in pepper–salt) or hy</context>
<context position="12101" citStr="Peirsman et al. (2008)" startWordPosition="1954" endWordPosition="1957">sible associations that led to an observed compound were then ranked according to the frequency of that compound.1 Note that, for languages where compounds are often spelled as two words, like English, our approach will have to recognize multiword units to deal with this issue. 3.4 Previous research In previous research, most attention has gone out to the first two of our models. Sahlgren (2006) tries to find associations with word space models. He argues that document-based models are better suited to the discovery of associations than word-based ones. In addition, Sahlgren (2006) as well as Peirsman et al. (2008) show that in wordbased models, large context sizes are more effective than small ones. This supports Wandmacher et al.’s (2008) model of associations, which uses a context size of 75 words to the left and right of the target. However, Peirsman et al. (2008) find that word-based distributional models are clearly outperformed by simple collocation measures, particularly the log-likelihood ratio. Such collocation measures are also used by Michelbacher et al. (2007) in their classification of asymmetric associations. They show the chi-square metric to be a robust classifier of associations as eit</context>
</contexts>
<marker>Peirsman, Heylen, Geeraerts, 2008</marker>
<rawString>Yves Peirsman, Kris Heylen, and Dirk Geeraerts. 2008. Size matters. Tight and loose context definitions in English word space models. In Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model. Using Distributional Analysis to Represent Syntagmatic and Paradigmatic Relations Between Words in High-dimensional Vector Spaces.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University,</institution>
<location>Stockholm,</location>
<contexts>
<context position="1513" citStr="Sahlgren, 2006" startWordPosition="239" endWordPosition="240"> an increase in performance of almost 30% over the current state of the art. 1 Introduction Associations are words that immediately come to mind when people hear or read a given cue word. For instance, a word like pepper calls up salt, and wave calls up sea. Aitchinson (2003) and Schulte im Walde and Melinger (2005) show that such associations can be motivated by a number of factors, from semantic similarity to collocation. Current computational models of association, however, tend to focus on one of these, by using either collocation measures (Michelbacher et al., 2007) or word space models (Sahlgren, 2006; Peirsman et al., 2008). To this day, two general problems remain. First, the literature lacks a comprehensive comparison between these general types of models. Second, we are still looking for an approach that combines several sources of information, so as to correctly predict a larger variety of associations. Most computational models of semantic relations aim to model semantic similarity in particular (Landauer and Dumais, 1997; Lin, 1998; Pad´o and Lapata, 2007). In Natural Language Processing, these models have applications in fields like query expansion, thesaurus extraction, informatio</context>
<context position="5064" citStr="Sahlgren, 2006" startWordPosition="809" endWordPosition="810">ngest association. 2 Associations There are several reasons why a word may be associated to its cue. According to Aitchinson (2003), the four major types of associations are, in order of frequency, co-ordination (co-hyponyms like pepper and salt), collocation (like salt and water), superordination (insect as a hypernym of butterfly) and synonymy (like starved and hungry). As a result, a computational model that is able to predict associations accurately has to deal with a wide range of semantic relations. Past systems, however, generally use only one type of information (Wettler et al., 2005; Sahlgren, 2006; Michelbacher et al., 2007; Peirsman et al., 2008; Wandmacher et al., 2008), which suggests that they are relatively restricted in the number of associations they will find. In this article, we will focus on a set of Dutch cue words and their single strongest association, collected from a large psycholinguistic experiment. Table 1 gives a few examples of such cue– association pairs. It illustrates the different types of linguistic phenomena that an association may be motivated by. The first three word pairs are based on similarity. In this case, strong associations can be hyponyms (as in amph</context>
<context position="9292" citStr="Sahlgren, 2006" startWordPosition="1494" endWordPosition="1495">ar in similar contexts. As a result, they model each word in terms of its contexts in a corpus, as a so-called context vector. Distributional similarity is then operationalized as the similarity between two such context vectors. These models will thus look for possible associations by searching words with a context vector similar to the given cue. Crucial in the implementation of word space models is their definition of context. In the current literature, there are basically three popular approaches. Document-based models use some sort of textual entity as features (Landauer and Dumais, 1997; Sahlgren, 2006). Their context vectors note what documents, paragraphs, articles or similar stretches of text a target word appears in. Without dimensionality reduction, in these models two words will be distributionally similar if they often occur together in the same paragraph, for instance. This approach still bears some similarity to the collocation measures above, since it relies on the direct co-occurrence of two words in text. Second, syntax-based models focus on the syntactic relationships in which a word takes part (Lin, 1998). Here two words will be similar when they often appear in the same syntac</context>
<context position="10796" citStr="Sahlgren, 2006" startWordPosition="1736" endWordPosition="1737">ce of context size will again have a major influence on the behaviour of the model. Syntaxbased and word-based models differ from collocation measures and document-based models in that they do not search for words that co-occur directly. Instead, they look for words that often occur together with the same context words or syntactic relations. Even though all these models were originally developed to model semantic similarity relations, syntax-based models have been shown to favour such relations more than wordbased and document-based models, which might capture more associative relationships (Sahlgren, 2006; Van der Plas, 2008). 3.3 Compounding As we have argued before, one characteristic of cues and their strong associations is that they can sometimes be combined into a compound. Therefore we developed a third approach which discovers for every cue the words in the corpus that in combination with it lead to an existing compound. Since in Dutch compounds are generally written as one word, this is relatively easy. We attached each candidate association to the cue (both in the combination cue+association and association+cue), following a number of simple morphological rules for compounding. We the</context>
<context position="12067" citStr="Sahlgren (2006)" startWordPosition="1949" endWordPosition="1950">rred in the corpus. The possible associations that led to an observed compound were then ranked according to the frequency of that compound.1 Note that, for languages where compounds are often spelled as two words, like English, our approach will have to recognize multiword units to deal with this issue. 3.4 Previous research In previous research, most attention has gone out to the first two of our models. Sahlgren (2006) tries to find associations with word space models. He argues that document-based models are better suited to the discovery of associations than word-based ones. In addition, Sahlgren (2006) as well as Peirsman et al. (2008) show that in wordbased models, large context sizes are more effective than small ones. This supports Wandmacher et al.’s (2008) model of associations, which uses a context size of 75 words to the left and right of the target. However, Peirsman et al. (2008) find that word-based distributional models are clearly outperformed by simple collocation measures, particularly the log-likelihood ratio. Such collocation measures are also used by Michelbacher et al. (2007) in their classification of asymmetric associations. They show the chi-square metric to be a robust</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Using Distributional Analysis to Represent Syntagmatic and Paradigmatic Relations Between Words in High-dimensional Vector Spaces. Ph.D. thesis, Stockholm University, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Alissa Melinger</author>
</authors>
<title>Identifying semantic relations and functional properties of human verb associations.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>612--619</pages>
<contexts>
<context position="1216" citStr="Walde and Melinger (2005)" startWordPosition="189" endWordPosition="192"> two types of models and that of a new approach based on compounding. The best single predictor is the log-likelihood ratio, followed closely by the document-based word space model. We will show, however, that an ensemble method that combines these two best approaches with the compounding algorithm achieves an increase in performance of almost 30% over the current state of the art. 1 Introduction Associations are words that immediately come to mind when people hear or read a given cue word. For instance, a word like pepper calls up salt, and wave calls up sea. Aitchinson (2003) and Schulte im Walde and Melinger (2005) show that such associations can be motivated by a number of factors, from semantic similarity to collocation. Current computational models of association, however, tend to focus on one of these, by using either collocation measures (Michelbacher et al., 2007) or word space models (Sahlgren, 2006; Peirsman et al., 2008). To this day, two general problems remain. First, the literature lacks a comprehensive comparison between these general types of models. Second, we are still looking for an approach that combines several sources of information, so as to correctly predict a larger variety of ass</context>
</contexts>
<marker>Walde, Melinger, 2005</marker>
<rawString>Sabine Schulte im Walde and Alissa Melinger. 2005. Identifying semantic relations and functional properties of human verb associations. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 612–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>124</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke Van der Plas</author>
</authors>
<title>Automatic LexicoSemantic Acquisition for Question Answering.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Groningen, Groningen, The Netherlands.</institution>
<marker>Van der Plas, 2008</marker>
<rawString>Lonneke Van der Plas. 2008. Automatic LexicoSemantic Acquisition for Question Answering. Ph.D. thesis, University of Groningen, Groningen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At last parsing is now operational.</title>
<date>2006</date>
<booktitle>Verbum Ex Machina. Actes de la 13e Conf´erence sur le Traitement Automatique des Langues Naturelles (TALN),</booktitle>
<pages>20--42</pages>
<editor>In Piet Mertens, C´edrick Fairon, Anne Dister, and Patrick Watrin, editors,</editor>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At last parsing is now operational. In Piet Mertens, C´edrick Fairon, Anne Dister, and Patrick Watrin, editors, Verbum Ex Machina. Actes de la 13e Conf´erence sur le Traitement Automatique des Langues Naturelles (TALN), pages 20–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tonio Wandmacher</author>
<author>Ekaterina Ovchinnikova</author>
<author>Theodore Alexandrov</author>
</authors>
<title>Does Latent Semantic Analysis reflect human associations?</title>
<date>2008</date>
<booktitle>In Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="5140" citStr="Wandmacher et al., 2008" startWordPosition="820" endWordPosition="824">word may be associated to its cue. According to Aitchinson (2003), the four major types of associations are, in order of frequency, co-ordination (co-hyponyms like pepper and salt), collocation (like salt and water), superordination (insect as a hypernym of butterfly) and synonymy (like starved and hungry). As a result, a computational model that is able to predict associations accurately has to deal with a wide range of semantic relations. Past systems, however, generally use only one type of information (Wettler et al., 2005; Sahlgren, 2006; Michelbacher et al., 2007; Peirsman et al., 2008; Wandmacher et al., 2008), which suggests that they are relatively restricted in the number of associations they will find. In this article, we will focus on a set of Dutch cue words and their single strongest association, collected from a large psycholinguistic experiment. Table 1 gives a few examples of such cue– association pairs. It illustrates the different types of linguistic phenomena that an association may be motivated by. The first three word pairs are based on similarity. In this case, strong associations can be hyponyms (as in amphibian–frog), co-hyponyms (as in pepper–salt) or hypernyms of their cue (as i</context>
</contexts>
<marker>Wandmacher, Ovchinnikova, Alexandrov, 2008</marker>
<rawString>Tonio Wandmacher, Ekaterina Ovchinnikova, and Theodore Alexandrov. 2008. Does Latent Semantic Analysis reflect human associations? In Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Wettler</author>
<author>Reinhard Rapp</author>
<author>Peter Sedlmeier</author>
</authors>
<title>Free word associations correspond to contiguities between words in texts.</title>
<date>2005</date>
<journal>Journal of Quantitative Linguistics,</journal>
<pages>12--2</pages>
<contexts>
<context position="5048" citStr="Wettler et al., 2005" startWordPosition="805" endWordPosition="808">of cues and their strongest association. 2 Associations There are several reasons why a word may be associated to its cue. According to Aitchinson (2003), the four major types of associations are, in order of frequency, co-ordination (co-hyponyms like pepper and salt), collocation (like salt and water), superordination (insect as a hypernym of butterfly) and synonymy (like starved and hungry). As a result, a computational model that is able to predict associations accurately has to deal with a wide range of semantic relations. Past systems, however, generally use only one type of information (Wettler et al., 2005; Sahlgren, 2006; Michelbacher et al., 2007; Peirsman et al., 2008; Wandmacher et al., 2008), which suggests that they are relatively restricted in the number of associations they will find. In this article, we will focus on a set of Dutch cue words and their single strongest association, collected from a large psycholinguistic experiment. Table 1 gives a few examples of such cue– association pairs. It illustrates the different types of linguistic phenomena that an association may be motivated by. The first three word pairs are based on similarity. In this case, strong associations can be hypo</context>
<context position="13258" citStr="Wettler et al. (2005)" startWordPosition="2137" endWordPosition="2140">i-square metric to be a robust classifier of associations as either symmetric or asymmetric, while a measure based on conditional probabilities is particularly suited to model 1If both compounds cue+association and association+cue occurred in the corpus, their frequencies were summed. 650 median rank of most frequent association 2 5 10 20 50 100 ● ● ● word−based no stoplist word−based stoplist pmi statistic log−likelihood statistic compound−based syntax−based document−based 2 4 6 8 10 context size Figure 1: Median rank of the strong associations. the magnitude of asymmetry. In a similar vein, Wettler et al. (2005) successfully predict associations on the basis of co-occurrence in text, in the framework of associationist learning theory. Despite this wealth of systems, it is an open question how their results compare to each other. Moreover, a model that combines several of these systems might outperform any basic approach. 4 Experiments Our experiments were inspired by the association prediction task at the ESSLLI-2008 workshop on distributional models. We will first present this precise setup and then go into the results and their implications. 4.1 Setup Our data was the Twente Nieuws Corpus (TwNC), w</context>
</contexts>
<marker>Wettler, Rapp, Sedlmeier, 2005</marker>
<rawString>Manfred Wettler, Reinhard Rapp, and Peter Sedlmeier. 2005. Free word associations correspond to contiguities between words in texts. Journal of Quantitative Linguistics, 12(2/3):111–122.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>