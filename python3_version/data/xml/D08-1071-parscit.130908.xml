<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000236">
<title confidence="0.840715">
Cross-Task Knowledge-Constrained Self Training
</title>
<author confidence="0.99177">
Hal Daum´e III
</author>
<affiliation confidence="0.865588">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.993297">
me@hal3.name
</email>
<sectionHeader confidence="0.998562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833">
We present an algorithmic framework for
learning multiple related tasks. Our frame-
work exploits a form of prior knowledge that
relates the output spaces of these tasks. We
present PAC learning results that analyze the
conditions under which such learning is pos-
sible. We present results on learning a shal-
low parser and named-entity recognition sys-
tem that exploits our framework, showing con-
sistent improvements over baseline methods.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999469230769231">
When two NLP systems are run on the same data, we
expect certain constraints to hold between their out-
puts. This is a form of prior knowledge. We propose
a self-training framework that uses such information
to significantly boost the performance of one of the
systems. The key idea is to perform self-training
only on outputs that obey the constraints.
Our motivating example in this paper is the task
pair: named entity recognition (NER) and shallow
parsing (aka syntactic chunking). Consider a hid-
den sentence with known POS and syntactic struc-
ture below. Further consider four potential NER se-
quences for this sentence.
</bodyText>
<equation confidence="0.40885">
POS: NNP NNP VBD TO NNP NN
Chunk: [- NP -][-VP-][-PP-][-NP-][-NP-]
</equation>
<bodyText confidence="0.999788392857143">
wrong because we feel like named entities should
not be part of verb phrases. NER2 seems wrong be-
cause there is an NNP1 (proper noun) that is not part
of a named entity (word 5). NER3 is amiss because
we feel it is unlikely that a single name should span
more than one NP (last two words). NER4 has none
of these problems and seems quite reasonable. In
fact, for the hidden sentence, NER4 is correct2.
The remainder of this paper deals with the prob-
lem of formulating such prior knowledge into a
workable system. There are similarities between
our proposed model and both self-training and co-
training; background is given in Section 2. We
present a formal model for our approach and per-
form a simple, yet informative, analysis (Section 3).
This analysis allows us to define what good and
bad constraints are. Throughout, we use a running
example of NER using hidden Markov models to
show the efficacy of the method and the relation-
ship between the theory and the implementation. Fi-
nally, we present full-blown results on seven dif-
ferent NER data sets (one from CoNLL, six from
ACE), comparing our method to several competi-
tive baselines (Section 4). We see that for many of
these data sets, less than one hundred labeled NER
sentences are required to get state-of-the-art perfor-
mance, using a discriminative sequence labeling al-
gorithm (Daum´e III and Marcu, 2005).
</bodyText>
<note confidence="0.97109825">
NER1: [- Per -][- O -][-Org-][- 0 -] 2 Background
NER2: [- Per -][- O -][- O -][- O -][- O -]
NER3: [- Per -][- O -][- O -][- Org -] Self-training works by learning a model on a small
NER4: [- Per -][- O -][- O -][-Org-][- O -] amount of labeled data. This model is then evalu-
</note>
<footnote confidence="0.9125175">
Without ever seeing the actual sentence, can we 1When we refer to NNP, we also include NNPS.
guess which NER sequence is correct? NER1 seems 2The sentence is: “George Bush spoke to Congress today”
</footnote>
<page confidence="0.899673">
680
</page>
<note confidence="0.962544">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 680–688,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999780692307692">
ated on a large amount of unlabeled data. Its predic-
tions are assumed to be correct, and it is retrained
on the unlabeled data according to its own predic-
tions. Although there is little theoretical support
for self-training, it is relatively popular in the natu-
ral language processing community. Its success sto-
ries range from parsing (McClosky et al., 2006) to
machine translation (Ueffing, 2006). In some cases,
self-training takes into account model confidence.
Co-training (Yarowsky, 1995; Blum and Mitchell,
1998) is related to self-training, in that an algorithm
is trained on its own predictions. Where it differs is
that co-training learns two separate models (which
are typically assumed to be independent; for in-
stance by training with disjoint feature sets). These
models are both applied to a large repository of un-
labeled data. Examples on which these two mod-
els agree are extracted and treated as labeled for a
new round of training. In practice, one often also
uses a notion of model confidence and only extracts
agreed-upon examples for which both models are
confident. The original, and simplest analysis of co-
training is due to Blum and Mitchell (1998). It does
not take into account confidence (to do so requires a
significantly more detailed analysis (Dasgupta et al.,
2001)), but is useful for understanding the process.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="introduction">
3 Model
</sectionHeader>
<bodyText confidence="0.999971294117647">
We define a formal PAC-style (Valiant, 1994) model
that we call the “hints model”3. We have an instance
space X and two output spaces Y1 and Y2. We as-
sume two concept classes C1 and C2 for each output
space respectively. Let D be a distribution over X,
and f1 E C1 (resp., f2 E C2) be target functions. The
goal, of course, is to use a finite sample of examples
drawn from D (and labeled—perhaps with noise—
by f1 and f2) to “learn” h1 E C1 and h2 E C2, which
are good approximations to f1 and f2.
So far we have not made use of any notion of con-
straints. Our expectation is that if we constrain h1
and h2 to agree (vis-a-vis the example in the Intro-
duction), then we should need fewer labeled exam-
ples to learn either. (The agreement should “shrink”
the size of the corresponding hypothesis spaces.) To
formalize this, let X : Y1 x Y2 —* 10, 11 be a con-
</bodyText>
<footnote confidence="0.870111">
3The name comes from thinking of our knowledge-based
constraints as “hints” to a learner as to what it should do.
</footnote>
<bodyText confidence="0.875915666666667">
straint function. We say that two outputs y1 E Y1
and y2 E Y2 are compatible if X(y1, y2) = 1. We
need to assume that X is correct:
Definition 1. We say that X is correct with respect
to D, f1, f2 if whenever x has non-zero probability
under D, then X(f1(x), f2(x)) = 1.
</bodyText>
<sectionHeader confidence="0.755228" genericHeader="method">
RUNNING EXAMPLE
</sectionHeader>
<bodyText confidence="0.995821695652174">
In our example, Y1 is the space of all POS/chunk
sequences and Y2 is the space of all NER se-
quences. We assume that C1 and C2 are both
represented by HMMs over the appropriate state
spaces. The functions we are trying to learn are f1,
the “true” POS/chunk labeler and f2, the “true”
NER labeler. (Note that we assume f1 E C1,
which is obviously not true for language.)
Our constraint function X will require the follow-
ing for agreement: (1) any NNP must be part of a
named entity; (2) any named entity must be a sub-
sequence of a noun phrase. This is precisely the set
of constraints discussed in the introduction.
The question is: given this additional source of
knowledge (i.e., X), has the learning problem be-
come easier? That is, can we learn f2 (and/or f1) us-
ing significantly fewer labeled examples than if we
did not have X? Moreover, we have assumed that X
is correct, but is this enough? Intuitively, no: a func-
tion X that returns 1 regardless of its inputs is clearly
not useful. Given this, what other constraints must
be placed on X. We address these questions in Sec-
tions 3.3. However, first we define our algorithm.
</bodyText>
<subsectionHeader confidence="0.994998">
3.1 One-sided Learning with Hints
</subsectionHeader>
<bodyText confidence="0.999998666666667">
We begin by considering a simplified version of the
“learning with hints” problem. Suppose that all we
care about is learning f2. We have a small amount of
data labeled by f2 (call this D) and a large amount of
data labeled by f1 (call this Dunlab–”unlab” because
as far as f2 is concerned, it is unlabeled).
</bodyText>
<sectionHeader confidence="0.65656" genericHeader="method">
RUNNING EXAMPLE
</sectionHeader>
<footnote confidence="0.786193222222222">
In our example, this means that we have a small
amount of labeled NER data and a large amount of
labeled POS/chunk data. We use 3500 sentences
from CoNLL (Tjong Kim Sang and De Meulder,
2003) as the NER data and section 20-23 of the
WSJ (Marcus et al., 1993; Ramshaw and Marcus,
1995) as the POS/chunk data (8936 sentences). We
are only interested in learning to do NER. Details
of the exact HMM setup are in Section 4.2.
</footnote>
<page confidence="0.997925">
681
</page>
<bodyText confidence="0.9986715">
We call the following algorithm “One-Sided
Learning with Hints,” since it aims only to learn f2:
</bodyText>
<listItem confidence="0.998068166666667">
1: Learn h2 directly on D
2: For each example (x, y1) E Dunlab
3: Compute y2 = h2(x)
4: If χ(y1, y2), add (x, y2) to D
5: Relearn h2 on the (augmented) D
6: Go to (2) if desired
</listItem>
<sectionHeader confidence="0.71533" genericHeader="method">
RUNNING EXAMPLE
</sectionHeader>
<bodyText confidence="0.9998608">
In step 1, we train an NER HMM on CoNLL. On
test data, this model achieves an F-score of 50.8.
In step 2, we run this HMM on all the WSJ data,
and extract 3145 compatible examples. In step 3,
we retrain the HMM; the F-score rises to 58.9.
</bodyText>
<subsectionHeader confidence="0.996591">
3.2 Two-sided Learning with Hints
</subsectionHeader>
<bodyText confidence="0.999933333333333">
In the two-sided version, we assume that we have a
small amount of data labeled by f1 (call this D1), a
small amount of data labeled by f2 (call this D2) and
a large amount of unlabeled data (call this Dunlab).
The algorithm we propose for learning hypotheses
for both tasks is below:
</bodyText>
<listItem confidence="0.998566833333333">
1: Learn h1 on D1 and h2 on D2.
2: For each example x E Dunlab:
3: Compute y1 = h1(x) and y2 = h2(x)
4: If χ(y1, y2) add (x, y1) to D1, (x, y2) to D2
5: Relearn h1 on D1 and h2 on D2.
6: Go to (2) if desired
</listItem>
<sectionHeader confidence="0.754025" genericHeader="method">
RUNNING EXAMPLE
</sectionHeader>
<bodyText confidence="0.999987625">
We use 3500 examples from NER and 1000 from
WSJ. We use the remaining 18447 examples as
unlabeled data. The baseline HMMs achieve F-
scores of 50.8 and 76.3, respectively. In step 2, we
add 7512 examples to each data set. After step 3,
the new models achieve F-scores of 54.6 and 79.2,
respectively. The gain for NER is lower than be-
fore as it is trained against “noisy” syntactic labels.
</bodyText>
<subsectionHeader confidence="0.99906">
3.3 Analysis
</subsectionHeader>
<bodyText confidence="0.9998474">
Our goal is to prove that one-sided learning with
hints “works.” That is, if C2 is learnable from
large amounts of labeled data, then it is also learn-
able from small amounts of labeled data and large
amounts of f1-labeled data. This is formalized in
Theorem 1 (all proofs are in Appendix A). How-
ever, before stating the theorem, we must define an
“initial weakly-useful predictor” (terminology from
Blum and Mitchell(1998)), and the notion of noisy
PAC-learning in the structured domain.
</bodyText>
<construct confidence="0.95044225">
Definition 2. We say that h is a weakly-useful pre-
dictor of f if for all y: PrD [h(x) = y] &gt; c
and PrD [f(x) = y  |h(x) = y&apos; =� y] &gt;
PrD [f(x) = y] + c.
</construct>
<bodyText confidence="0.99296315">
This definition simply ensures that (1) h is non-
trivial: it assigns some non-zero probability to every
possible output; and (2) h is somewhat indicative of
f. In practice, we use the hypothesis learned on the
small amount of training data during step (1) of the
algorithm as the weakly useful predictor.
Definition 3. We say that C is PAC-learnable with
noise (in the structured setting) if there exists an
algorithm with the following properties. For any
c E C, any distribution D over X, any 0 &lt; η &lt;
1/ |Y|, any 0 &lt; c &lt; 1, any 0 &lt; δ &lt; 1 and any
η &lt; η0 &lt; 1/ |Y|, if the algorithm is given access
to examples drawn EXS1N(c, D) and inputs c, δ and
η0, then with probability at least 1 − δ, the algo-
rithm returns a hypothesis h E C with error at most
c. Here, EXSN(c, D) is a structured noise oracle,
which draws examples from D, labels them by c and
randomly replaces with another label with prob. η.
Note here the rather weak notion of noise: en-
tire structures are randomly changed, rather than in-
dividual labels. Furthermore, the error is 0/1 loss
over the entire structure. Collins (2001) establishes
learnability results for the class of hyperplane mod-
els under 0/1 loss. While not stated directly in terms
of PAC learnability, it is clear that his results apply.
Taskar et al. (2005) establish tighter bounds for the
case of Hamming loss. This suggests that the re-
quirement of 0/1 loss is weaker.
As suggested before, it is not sufficient for χ to
simply be correct (the constant 1 function is cor-
rect, but not useful). We need it to be discriminating,
made precise in the following definition.
Definition 4. We say the discrimination of χ for h0
is PrD[χ(f1(x), h0(x))]−1.
In other words, a constraint function is discrim-
inating when it is unlikely that our weakly-useful
predictor h0 chooses an output that satisfies the con-
straint. This means that if we do find examples (in
our unlabeled corpus) that satisfy the constraints,
they are likely to be “useful” to learning.
</bodyText>
<page confidence="0.993514">
682
</page>
<sectionHeader confidence="0.685475" genericHeader="method">
RUNNING EXAMPLE
</sectionHeader>
<bodyText confidence="0.9990891">
In the NER HMM, let h° be the HMM obtained by
training on the small labeled NER data set and f,
is the true syntactic labels. We approximate PrD
by an empirical estimate over the unlabeled distri-
bution. This gives a discrimination is 41.6 for the
constraint function defined previously. However, if
we compare against “weaker” constraint functions,
we see the appropriate trend. The value for the con-
straint based only on POS tags is 39.1 (worse) and
for the NP constraint alone is 27.0 (much worse).
</bodyText>
<construct confidence="0.8439872">
Theorem 1. Suppose C2 is PAC-learnable with
noise in the structured setting, h2 is a weakly use-
ful predictor of f2, and X is correct with respect to
D, f1, f2, h2, and has discrimination &gt; 2(|Y |− 1).
Then C2 is also PAC-learnable with one-sided hints.
</construct>
<bodyText confidence="0.999960736842105">
The way to interpret this theorem is that it tells
us that if the initial h2 we learn in step 1 of the one-
sided algorithm is “good enough” (in the sense that it
is weakly-useful), then we can use it as specified by
the remainder of the one-sided algorithm to obtain
an arbitrarily good h2 (via iterating).
The dependence on |Y |is the discrimination
bound for X is unpleasant for structured problems. If
we wish to find M unlabeled examples that satisfy
the hints, we’ll need a total of at least 2M(|Y |− 1)
total. This dependence can be improved as follows.
Suppose that our structure is represented by a graph
over vertices V , each of which can take a label from
a set Y . Then, |Y |= IY V 1, and our result requires
that X be discriminating on an order exponential in
V . Under the assumption that X decomposes over
the graph structure (true for our example) and that
C2 is PAC-learnable with per-vertex noise, then the
discrimination requirement drops to 2 |V  |(|Y  |− 1).
</bodyText>
<sectionHeader confidence="0.733144" genericHeader="method">
RUNNING EXAMPLE
</sectionHeader>
<bodyText confidence="0.999780941176471">
In NER, |Y  |= 9 and |V  |,: 26. This means
that the values from the previous example look not
quite so bad. In the 0/1 loss case, they are com-
pared to 1025; in the Hamming case, they are com-
pared to only 416. The ability of the one-sided al-
gorithm follows the same trends as the discrimi-
nation values. Recall the baseline performance is
50.8. With both constraints (and a discrimination
value of 41.6), we obtain a score of 58.9. With just
the POS constraint (discrimination of 39.1), we ob-
tain a score of 58.1. With just the NP constraint
(discrimination of 27.0, we obtain a score of 54.5.
The final question is how one-sided learning re-
lates to two-sided learning. The following definition
and easy corollary shows that they are related in the
obvious manner, but depends on a notion of uncor-
relation between h° and h2.
</bodyText>
<equation confidence="0.754933333333333">
Definition 5. We say that h1 and h2 are un-
correlated if PrD [h1(x) = y1  |h2(x) = y2, x] =
PrD [h1(x) = y1  |x].
</equation>
<bodyText confidence="0.9282063">
Corollary 1. Suppose C1 and C2 are both PAC-
learnable in the structured setting, h° and h2 are
weakly useful predictors of f1 and f2, and X is
correct with respect to D, f1, f2, h° and h2, and
has discrimination &gt; 4(|Y |− 1)2 (for 0/1 loss) or
&gt; 4 |V |2 (|Y |−1)2 (for Hamming loss), and that h°
and h2 are uncorrelated. Then C1 and C2 are also
PAC-learnable with two-sided hints.
Unfortunately, Corollary 1 depends quadratically
on the discrimination term, unlike Theorem 1.
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999795">
In this section, we describe our experimental results.
We have already discussed some of them in the con-
text of the running example. In Section 4.1, we
briefly describe the data sets we use. A full descrip-
tion of the HMM implementation and its results are
in Section 4.2. Finally, in Section 4.3, we present
results based on a competitive, discriminatively-
learned sequence labeling algorithm. All results for
NER and chunking are in terms of F-score; all re-
sults for POS tagging are accuracy.
</bodyText>
<subsectionHeader confidence="0.994014">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9999248">
Our results are based on syntactic data drawn from
the Penn Treebank (Marcus et al., 1993), specifi-
cally the portion used by CoNLL 2000 shared task
(Tjong Kim Sang and Buchholz, 2000). Our NER
data is from two sources. The first source is the
CoNLL 2003 shared task date (Tjong Kim Sang and
De Meulder, 2003) and the second source is the 2004
NIST Automatic Content Extraction (Weischedel,
2004). The ACE data constitute six separate data
sets from six domains: weblogs (wl), newswire
(nw), broadcast conversations (bc), United Nations
(un), direct telephone speech (dts) and broadcast
news (bn). Of these, bc, dts and bn are all speech
data sets. All the examples from the previous sec-
tions have been limited to the CoNLL data.
</bodyText>
<page confidence="0.996316">
683
</page>
<subsectionHeader confidence="0.942536">
4.2 HMM Results
</subsectionHeader>
<bodyText confidence="0.999954055555556">
The experiments discussed in the preceding sections
are based on a generative hidden Markov model for
both the NER and syntactic chunking/POS tagging
tasks. The HMMs constructed use first-order tran-
sitions and emissions. The emission vocabulary is
pruned so that any word that appears &lt; 1 time in the
training data is replaced by a unique *unknown*
token. The transition and emission probabilities are
smoothed with Dirichlet smoothing, α = 0.001 (this
was not-aggressively tuned by hand on one setting).
The HMMs are implemented as finite state models
in the Carmel toolkit (Graehl and Knight, 2002).
The various compatibility functions are also im-
plemented as finite state models. We implement
them as a transducer from POS/chunk labels to NER
labels (though through the reverse operation, they
can obviously be run in the opposite direction). The
construction is with a single state with transitions:
</bodyText>
<listItem confidence="0.99326725">
• (NNP,?) maps to B-* and I-*
• (?,B-NP) maps to B-* and O
• (?,I-NP) maps to B-*, I-* and O
• Single exception: (NNP,x), where x is not an NP
tag maps to anything (this is simply to avoid
empty composition problems). This occurs in
100 of the 212k words in the Treebank data and
more rarely in the automatically tagged data.
</listItem>
<subsectionHeader confidence="0.995076">
4.3 One-sided Discriminative Learning
</subsectionHeader>
<bodyText confidence="0.999710882352941">
In this section, we describe the results of one-sided
discriminative labeling with hints. We use the true
syntactic labels from the Penn Treebank to derive
the constraints (this is roughly 9000 sentences). We
use the LaSO sequence labeling software (Daum´e III
and Marcu, 2005), with its built-in feature set.
Our goal is to analyze two things: (1) what is the
effect of the amount of labeled NER data? (2) what
is the effect of the amount of labeled syntactic data
from which the hints are constructed?
To answer the first question, we keep the
amount of syntactic data fixed (at 8936 sentences)
and vary the amount of NER data in N E
1100, 200, 400, 800,1600}. We compare models
with and without the default gazetteer information
from the LaSO software. We have the following
models for comparison:
</bodyText>
<listItem confidence="0.991157">
• A default “Baseline” in which we simply train
the NER model without using syntax.
</listItem>
<table confidence="0.999452">
Hints Self-T Hints
vs Base vs Base vs Self-T
Win 29 20 24
Tie 6 12 11
Lose 0 3 0
</table>
<tableCaption confidence="0.9363105">
Table 1: Comparison between hints, self-training and the
(best) baseline for varying amount of labeled data.
</tableCaption>
<listItem confidence="0.9479368">
• In “POS-feature”, we do the same thing, but we
first label the NER data using a tagger/chunker
trained on the 8936 syntactic sentences. These
labels are used as features for the baseline.
• A “Self-training” setting where we use the
</listItem>
<bodyText confidence="0.9904935">
8936 syntactic sentences as “unlabeled,” label
them with our model, and then train on the
results. (This is equivalent to a hints model
where x(·, ·) = 1 is the constant 1 func-
tion.) We use model confidence as in Blum and
Mitchell (1998).4
The results are shown in Figure 1. The trends we
see are the following:
</bodyText>
<listItem confidence="0.97024">
• More data always helps.
• Self-training usually helps over the baseline
(though not always: for instance in wl and parts
of cts and bn).
• Adding the gazetteers help.
• Adding the syntactic features helps.
• Learning with hints, especially for &lt; 1000
</listItem>
<bodyText confidence="0.994499933333334">
training data points, helps significantly, even
over self-training.
We further compare the algorithms by looking at
how many training setting has each as the winner. In
particular, we compare both hints and self-training
to the two baselines, and then compare hints to self-
training. If results are not significant at the 95%
level (according to McNemar’s test), we call it a tie.
The results are in Table 1.
In our second set of experiments, we consider the
role of the syntactic data. For this experiment, we
hold the number of NER labeled sentences constant
(at N = 200) and vary the amount of syntactic data
in M E 1500, 1000, 2000, 4000, 8936}. The results
of these experiments are in Figure 2. The trends are:
</bodyText>
<listItem confidence="0.93286">
• The POS feature is relatively insensitive to the
amount of syntactic data—this is most likely
because it’s weight is discriminatively adjusted
</listItem>
<footnote confidence="0.884347">
4Results without confidence were significantly worse.
</footnote>
<page confidence="0.996318">
684
</page>
<figure confidence="0.999835542372881">
wl
0 1000 2000
un
0 1000 2000
nw
0 1000 2000
cts
0 1000 2000
conll
0 1000 2000
bn
0 1000 2000
bc
0 1000 2000
POS−feature
Hints (no gaz)
Baseline (no gaz)
Hints (w/ gaz)
Baseline (w/ gaz)
Self−train (no gaz)
Self−train (w/ gaz)
0.7
0.6
0.5
0.4
0.3
0.2
0.7
0.6
0.5
0.4
0.3
0.2
0.8
0.7
0.6
0.5
0.4
0.95
0.9
0.85
0.8
0.75
0.9
0.8
0.7
0.6
0.5
0.4
0.8
0.6
0.4
0.2
0
0.8
0.7
0.6
0.5
0.4
</figure>
<figureCaption confidence="0.999971">
Figure 1: Results of varying the amount of NER labeled data, for a fixed amount (M = 8936) of syntactic data.
</figureCaption>
<table confidence="0.990511">
Hints Self-T Hints
vs Base vs Base vs Self-T
Win 34 28 15
Tie 1 7 20
Lose 0 0 0
</table>
<tableCaption confidence="0.9866015">
Table 2: Comparison between hints, self-training and the
(best) baseline for varying amount of unlabeled data.
</tableCaption>
<bodyText confidence="0.834388">
by LaSO so that if the syntactic information is
bad, it is relatively ignored.
</bodyText>
<listItem confidence="0.86674">
• Self-training performance often degrades as the
amount of syntactic data increases.
• The performance of learning with hints in-
creases steadily with more syntactic data.
</listItem>
<bodyText confidence="0.999107923076923">
As before, we compare performance between the
different models, declaring a “tie” if the difference
is not statistically significant at the 95% level. The
results are in Table 2.
In experiments not reported here to save space,
we experimented with several additional settings. In
one, we weight the unlabeled data in various ways:
(1) to make it equal-weight to the labeled data; (2)
at 10% weight; (3) according to the score produced
by the first round of labeling. None of these had a
significant impact on scores; in a few cases perfor-
mance went up by « 1, in a few cases, performance
went down about the same amount.
</bodyText>
<subsectionHeader confidence="0.998989">
4.4 Two-sided Discriminative Learning
</subsectionHeader>
<bodyText confidence="0.985859761904762">
In this section, we explore the use of two-sided
discriminative learning to boost the performance of
our syntactic chunking, part of speech tagging, and
named-entity recognition software. We continue to
use LaSO (Daum´e III and Marcu, 2005) as the se-
quence labeling technique.
The results we present are based on attempting to
improve the performance of a state-of-the-art system
train on all of the training data. (This is in contrast to
the results in Section 4.3, in which the effect of us-
ing limited amounts of data was explored.) For the
POS tagging and syntactic chunking, we being with
all 8936 sentences of training data from CoNLL. For
the named entity recognition, we limit our presenta-
tion to results from the CoNLL 2003 NER shared
task. For this data, we have roughly 14k sentences
of training data, all of which are used. In both cases,
we reserve 10% as development data. The develop-
ment data is use to do early stopping in LaSO.
As unlabeled data, we use 1m sentences extracted
from the North American National Corpus of En-
</bodyText>
<page confidence="0.995141">
685
</page>
<figure confidence="0.999781383333333">
wl
0 5000 10000
un
0 5000 10000
nw
0 5000 10000
cts
0 5000 10000
conll
0 5000 10000
bn
0 5000 10000
bc
0 5000 10000
POS−feature
Hints (no gaz)
Baseline (no gaz)
Hints (w/ gaz)
Baseline (w/ gaz)
Self−train (no gaz)
Self−train (w/ gaz)
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.7
0.6
0.5
0.4
0.3
0.2
0.8
0.75
0.7
0.65
0.6
0.55
0.95
0.9
0.85
0.8
0.75
0.9
0.8
0.7
0.6
0.5
1
0.8
0.6
0.4
0.2
0.9
0.8
0.7
0.6
0.5
</figure>
<figureCaption confidence="0.999948">
Figure 2: Results of varying amount of syntactic data for a fixed amount of NER data (N = 200 sentences).
</figureCaption>
<bodyText confidence="0.999027769230769">
glish (previously used for self-training of parsers
(McClosky et al., 2006)). These 1m sentences were
selected by dev-set relativization against the union
of the two development data sets.
Following similar ideas to those presented by
Blum and Mitchell (1998), we employ two slight
modifications to the algorithm presented in Sec-
tion 3.2. First, in step (2b) instead of adding all
allowable instances to the labeled data set, we only
add the top R (for some hyper-parameter R), where
“top” is determined by average model confidence for
the two tasks. Second, Instead of using the full un-
labeled set to label at each iteration, we begin with
a random subset of 10R unlabeled examples and an-
other add random 10R every iteration.
We use the same baseline systems as in one-sided
learning: a Baseline that learns the two tasks inde-
pendently; a variant of the Baseline on which the
output of the POS/chunker is used as a feature for
the NER; a variant based on self-training; the hints-
based method. In all cases, we do use gazetteers. We
run the hints-based model for 10 iterations. For self-
training, we use 10R unlabeled examples (so that it
had access to the same amount of unlabeled data as
the hints-based learning after all 10 iterations). We
used three values of R: 50, 100, 500. We select the
</bodyText>
<table confidence="0.999366363636364">
Chunking NER
Baseline 94.2 87.5
w/POS N/A 88.0
Self-train
R = 50 94.2 88.0
R = 100 94.3 88.6
R = 500 94.1 88.4
Hints
R = 50 94.2 88.5
R = 100 94.3 89.1
R = 500 94.3 89.0
</table>
<tableCaption confidence="0.999948">
Table 3: Results on two-sided learning with hints.
</tableCaption>
<bodyText confidence="0.999786">
best-performing model (by the dev data) over these
ten iterations. The results are in Table 3.
As we can see, performance for syntactic chunk-
ing is relatively stagnant: there are no significant
improvements for any of the methods over the base-
line. This is not surprising: the form of the con-
straint function we use tells us a lot about the NER
task, but relatively little about the syntactic chunking
task. In particular, it tells us nothing about phrases
other than NPs. On the other hand, for NER, we see
that both self-training and learning with hints im-
prove over the baseline. The improvements are not
</bodyText>
<page confidence="0.996875">
686
</page>
<bodyText confidence="0.9950485">
enormous, but are significant (at the 95% level, as
measured by McNemar’s test). Unfortunately, the
improvements for learning with hints over the self-
training model are only significant at the 90% level.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999971394736842">
We have presented a method for simultaneously
learning two tasks using prior knowledge about the
relationship between their outputs. This is related
to joint inference (Daum´e III et al., 2006). How-
ever, we do not require that that a single data set
be labeled for multiple tasks. In all our examples,
we use separate data sets for shallow parsing as for
named-entity recognition. Although all our exper-
iments used the LaSO framework for sequence la-
beling, there is noting in our method that assumes
any particular learner; alternatives include: condi-
tional random fields (Lafferty et al., 2001), indepen-
dent predictors (Punyakanok and Roth, 2001), max-
margin Markov networks (Taskar et al., 2005), etc.
Our approach, both algorithmically and theoreti-
cally, is most related to ideas in co-training (Blum
and Mitchell, 1998). The key difference is that in
co-training, one assumes that the two “views” are
on the inputs; here, we can think of the two out-
put spaces as being the difference “views” and the
compatibility function χ being a method for recon-
ciling these two views. Like the pioneering work
of Blum and Mitchell, the algorithm we employ in
practice makes use of incrementally augmenting the
unlabeled data and using model confidence. Also
like that work, we do not currently have a theoret-
ical framework for this (more complex) model.5 It
would also be interesting to explore soft hints, where
the range of χ is [0, 1] rather than {0, 1}.
Recently, Ganchev et al. (2008) proposed a co-
regularization framework for learning across multi-
ple related tasks with different output spaces. Their
approach hinges on a constrained EM framework
and addresses a quite similar problem to that ad-
dressed by this paper. Chang et al. (2008) also
propose a “semisupervised” learning approach quite
similar to our own model. The show very promis-
ing results in the context of semantic role labeling.
</bodyText>
<footnote confidence="0.948716666666667">
5Dasgupta et al. (2001) proved, three years later, that a for-
mal model roughly equivalent to the actual Blum and Mitchell
algorithm does have solid theoretical foundations.
</footnote>
<bodyText confidence="0.999934">
Given the apparent (very!) recent interest in this
problem, it would be ideal to directly compare the
different approaches.
In addition to an analysis of the theoretical prop-
erties of the algorithm presented, the most com-
pelling avenue for future work is to apply this frame-
work to other task pairs. With a little thought, one
can imagine formulating compatibility functions be-
tween tasks like discourse parsing and summariza-
tion (Marcu, 2000), parsing and word alignment, or
summarization and information extraction.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999269">
Many thanks to three anonymous reviewers of this
papers whose suggestions greatly improved the
work and the presentation. This work was partially
funded by NSF grant IIS 0712764.
</bodyText>
<sectionHeader confidence="0.771263" genericHeader="method">
A Proofs
</sectionHeader>
<bodyText confidence="0.999756">
The proof of Theorem 1 closes follows that of Blum
and Mitchell (1998).
Proof (Theorem 1, sketch). Use the following nota-
tion: ck = PrD[h(x) = k], pl = PrD[f(x) = l],
ql|k = PrD[f(x) = l  |h(x) = k]. Denote by A
the set of outputs that satisfy the constraints. We are
interested in the probability that h(x) is erroneous,
given that h(x) satisfies the constraints:
</bodyText>
<equation confidence="0.9636365">
p (h(x) ∈ A\{l}  |f(x) = l)
X= p (h(x) = k  |f(x) = l) = X ckql|k/pl
kEA\{l} kEA\{l}
X ck(|Y |− 1 + � X 1/pl) ≤ 2 X ck(|Y |− 1)
≤ l54k kEA
kEA
</equation>
<bodyText confidence="0.998009333333333">
Here, the second step is Bayes’ rule plus definitions,
the third step is by the weak initial hypothesis as-
sumption, and the last step is by algebra. Thus, in
order to get a probability off error at most η, we need
PkEA ck = Pr[h(x) ∈ A] ≤ η/(2(|Y |− 1)).
The proof of Corollary 1 is straightforward.
Proof (Corollary 1, sketch). Write out the probabil-
ity of error as a double sum over true labels y1, y2
and predicted labels 91, 92 subject to χ(y1, 92). Use
the uncorrelation assumption and Bayes’ to split this
into the product two terms as in the proof of Theo-
rem 1. Bound as before.
</bodyText>
<page confidence="0.997647">
687
</page>
<sectionHeader confidence="0.998325" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874430232558">
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the Conference on Computational Learn-
ing Theory (COLT), pages 92–100.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In Proceedings of the National Conference
on Artificial Intelligence (AAAI).
Michael Collins. 2001. Parameter estimation for
statistical parsing models: Theory and practice of
distribution-free methods. In International Workshop
on Parsing Technologies (IWPT).
Sanjoy Dasgupta, Michael Littman, and David
McAllester. 2001. PAC generalization bounds
for co-training. In Advances in Neural Information
Processing Systems (NIPS).
Hal Daum´e III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the In-
ternational Conference on Machine Learning (ICML).
Hal Daum´e III, Andrew McCallum, Ryan McDonald,
Fernando Pereira, and Charles Sutton, editors. 2006.
Workshop on Computationally Hard Problems and
Joint Inference in Speech and Language Process-
ing. Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics and Human Language Technology
(NAACL/HLT).
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-view learning over structured and
non-identical outputs. In Proceedings of the Conver-
ence on Uncertainty in Artificial Intelligence (UAI).
Jonathan Graehl and Kevin Knight. 2002. Carmel fi-
nite state transducer package. http://www.isi.
edu/licensed-sw/carmel/.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning (ICML).
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press,
Cambridge, Massachusetts.
Mitch Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Conference of the North American Chapter
of the Association for Computational Linguistics and
Human Language Technology (NAACL/HLT).
Vasin Punyakanok and Dan Roth. 2001. The use of clas-
sifiers in sequential inference. In Advances in Neural
Information Processing Systems (NIPS).
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Pro-
ceedings of the Third ACL Workshop on Very Large
Corpora.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceedings
of the International Conference on Machine Learning
(ICML), pages 897–904.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 shared task: Chunk-
ing. In Proceedings of the Conference on Natural Lan-
guage Learning (CoNLL).
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of Conference on Computational Natural Language
Learning, pages 142–147.
Nicola Ueffing. 2006. Self-training for machine trans-
lation. In NIPS workshop on Machine Learning for
Multilingual Information Access.
Leslie G. Valiant. 1994. A theory of the learnable. An-
nual ACM Symposium on Theory of Computing, pages
436–445.
Ralph Weischedel, editor. 2004. Automatic Content Ex-
traction Workshop (ACE-2004), Alexandria, Virginia,
September 20–22.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
</reference>
<page confidence="0.997344">
688
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.811226">
<title confidence="0.999609">Cross-Task Knowledge-Constrained Self Training</title>
<author confidence="0.999214">Hal Daum´e</author>
<affiliation confidence="0.9993755">School of University of</affiliation>
<address confidence="0.912319">Salt Lake City, UT 84112</address>
<email confidence="0.894974">me@hal3.name</email>
<abstract confidence="0.998160545454545">We present an algorithmic framework for learning multiple related tasks. Our framework exploits a form of prior knowledge that relates the output spaces of these tasks. We present PAC learning results that analyze the conditions under which such learning is possible. We present results on learning a shallow parser and named-entity recognition system that exploits our framework, showing consistent improvements over baseline methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Conference on Computational Learning Theory (COLT),</booktitle>
<pages>92--100</pages>
<contexts>
<context position="3825" citStr="Blum and Mitchell, 1998" startWordPosition="634" endWordPosition="637">nguage Processing, pages 680–688, Honolulu, October 2008.c�2008 Association for Computational Linguistics ated on a large amount of unlabeled data. Its predictions are assumed to be correct, and it is retrained on the unlabeled data according to its own predictions. Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community. Its success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006). In some cases, self-training takes into account model confidence. Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident. The original, and simplest analy</context>
<context position="19177" citStr="Blum and Mitchell (1998)" startWordPosition="3430" endWordPosition="3433">Self-T Win 29 20 24 Tie 6 12 11 Lose 0 3 0 Table 1: Comparison between hints, self-training and the (best) baseline for varying amount of labeled data. • In “POS-feature”, we do the same thing, but we first label the NER data using a tagger/chunker trained on the 8936 syntactic sentences. These labels are used as features for the baseline. • A “Self-training” setting where we use the 8936 syntactic sentences as “unlabeled,” label them with our model, and then train on the results. (This is equivalent to a hints model where x(·, ·) = 1 is the constant 1 function.) We use model confidence as in Blum and Mitchell (1998).4 The results are shown in Figure 1. The trends we see are the following: • More data always helps. • Self-training usually helps over the baseline (though not always: for instance in wl and parts of cts and bn). • Adding the gazetteers help. • Adding the syntactic features helps. • Learning with hints, especially for &lt; 1000 training data points, helps significantly, even over self-training. We further compare the algorithms by looking at how many training setting has each as the winner. In particular, we compare both hints and self-training to the two baselines, and then compare hints to sel</context>
<context position="23819" citStr="Blum and Mitchell (1998)" startWordPosition="4263" endWordPosition="4266">az) Baseline (no gaz) Hints (w/ gaz) Baseline (w/ gaz) Self−train (no gaz) Self−train (w/ gaz) 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.7 0.6 0.5 0.4 0.3 0.2 0.8 0.75 0.7 0.65 0.6 0.55 0.95 0.9 0.85 0.8 0.75 0.9 0.8 0.7 0.6 0.5 1 0.8 0.6 0.4 0.2 0.9 0.8 0.7 0.6 0.5 Figure 2: Results of varying amount of syntactic data for a fixed amount of NER data (N = 200 sentences). glish (previously used for self-training of parsers (McClosky et al., 2006)). These 1m sentences were selected by dev-set relativization against the union of the two development data sets. Following similar ideas to those presented by Blum and Mitchell (1998), we employ two slight modifications to the algorithm presented in Section 3.2. First, in step (2b) instead of adding all allowable instances to the labeled data set, we only add the top R (for some hyper-parameter R), where “top” is determined by average model confidence for the two tasks. Second, Instead of using the full unlabeled set to label at each iteration, we begin with a random subset of 10R unlabeled examples and another add random 10R every iteration. We use the same baseline systems as in one-sided learning: a Baseline that learns the two tasks independently; a variant of the Base</context>
<context position="26727" citStr="Blum and Mitchell, 1998" startWordPosition="4768" endWordPosition="4771">ever, we do not require that that a single data set be labeled for multiple tasks. In all our examples, we use separate data sets for shallow parsing as for named-entity recognition. Although all our experiments used the LaSO framework for sequence labeling, there is noting in our method that assumes any particular learner; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc. Our approach, both algorithmically and theoretically, is most related to ideas in co-training (Blum and Mitchell, 1998). The key difference is that in co-training, one assumes that the two “views” are on the inputs; here, we can think of the two output spaces as being the difference “views” and the compatibility function χ being a method for reconciling these two views. Like the pioneering work of Blum and Mitchell, the algorithm we employ in practice makes use of incrementally augmenting the unlabeled data and using model confidence. Also like that work, we do not currently have a theoretical framework for this (more complex) model.5 It would also be interesting to explore soft hints, where the range of χ is </context>
<context position="28761" citStr="Blum and Mitchell (1998)" startWordPosition="5102" endWordPosition="5105">l properties of the algorithm presented, the most compelling avenue for future work is to apply this framework to other task pairs. With a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (Marcu, 2000), parsing and word alignment, or summarization and information extraction. Acknowledgments Many thanks to three anonymous reviewers of this papers whose suggestions greatly improved the work and the presentation. This work was partially funded by NSF grant IIS 0712764. A Proofs The proof of Theorem 1 closes follows that of Blum and Mitchell (1998). Proof (Theorem 1, sketch). Use the following notation: ck = PrD[h(x) = k], pl = PrD[f(x) = l], ql|k = PrD[f(x) = l |h(x) = k]. Denote by A the set of outputs that satisfy the constraints. We are interested in the probability that h(x) is erroneous, given that h(x) satisfies the constraints: p (h(x) ∈ A\{l} |f(x) = l) X= p (h(x) = k |f(x) = l) = X ckql|k/pl kEA\{l} kEA\{l} X ck(|Y |− 1 + � X 1/pl) ≤ 2 X ck(|Y |− 1) ≤ l54k kEA kEA Here, the second step is Bayes’ rule plus definitions, the third step is by the weak initial hypothesis assumption, and the last step is by algebra. Thus, in order t</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Conference on Computational Learning Theory (COLT), pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Lev Ratinov</author>
<author>Nicholas Rizzolo</author>
<author>Dan Roth</author>
</authors>
<title>Learning and inference with constraints.</title>
<date>2008</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="27639" citStr="Chang et al. (2008)" startWordPosition="4925" endWordPosition="4928">tchell, the algorithm we employ in practice makes use of incrementally augmenting the unlabeled data and using model confidence. Also like that work, we do not currently have a theoretical framework for this (more complex) model.5 It would also be interesting to explore soft hints, where the range of χ is [0, 1] rather than {0, 1}. Recently, Ganchev et al. (2008) proposed a coregularization framework for learning across multiple related tasks with different output spaces. Their approach hinges on a constrained EM framework and addresses a quite similar problem to that addressed by this paper. Chang et al. (2008) also propose a “semisupervised” learning approach quite similar to our own model. The show very promising results in the context of semantic role labeling. 5Dasgupta et al. (2001) proved, three years later, that a formal model roughly equivalent to the actual Blum and Mitchell algorithm does have solid theoretical foundations. Given the apparent (very!) recent interest in this problem, it would be ideal to directly compare the different approaches. In addition to an analysis of the theoretical properties of the algorithm presented, the most compelling avenue for future work is to apply this f</context>
</contexts>
<marker>Chang, Ratinov, Rizzolo, Roth, 2008</marker>
<rawString>Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and Dan Roth. 2008. Learning and inference with constraints. In Proceedings of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods.</title>
<date>2001</date>
<booktitle>In International Workshop on Parsing Technologies (IWPT).</booktitle>
<contexts>
<context position="11029" citStr="Collins (2001)" startWordPosition="1995" endWordPosition="1996">ion D over X, any 0 &lt; η &lt; 1/ |Y|, any 0 &lt; c &lt; 1, any 0 &lt; δ &lt; 1 and any η &lt; η0 &lt; 1/ |Y|, if the algorithm is given access to examples drawn EXS1N(c, D) and inputs c, δ and η0, then with probability at least 1 − δ, the algorithm returns a hypothesis h E C with error at most c. Here, EXSN(c, D) is a structured noise oracle, which draws examples from D, labels them by c and randomly replaces with another label with prob. η. Note here the rather weak notion of noise: entire structures are randomly changed, rather than individual labels. Furthermore, the error is 0/1 loss over the entire structure. Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss. While not stated directly in terms of PAC learnability, it is clear that his results apply. Taskar et al. (2005) establish tighter bounds for the case of Hamming loss. This suggests that the requirement of 0/1 loss is weaker. As suggested before, it is not sufficient for χ to simply be correct (the constant 1 function is correct, but not useful). We need it to be discriminating, made precise in the following definition. Definition 4. We say the discrimination of χ for h0 is PrD[χ(f1(x), h0(x))]−1. In other wor</context>
</contexts>
<marker>Collins, 2001</marker>
<rawString>Michael Collins. 2001. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In International Workshop on Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Michael Littman</author>
<author>David McAllester</author>
</authors>
<title>PAC generalization bounds for co-training.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="4601" citStr="Dasgupta et al., 2001" startWordPosition="764" endWordPosition="767">ch are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident. The original, and simplest analysis of cotraining is due to Blum and Mitchell (1998). It does not take into account confidence (to do so requires a significantly more detailed analysis (Dasgupta et al., 2001)), but is useful for understanding the process. 3 Model We define a formal PAC-style (Valiant, 1994) model that we call the “hints model”3. We have an instance space X and two output spaces Y1 and Y2. We assume two concept classes C1 and C2 for each output space respectively. Let D be a distribution over X, and f1 E C1 (resp., f2 E C2) be target functions. The goal, of course, is to use a finite sample of examples drawn from D (and labeled—perhaps with noise— by f1 and f2) to “learn” h1 E C1 and h2 E C2, which are good approximations to f1 and f2. So far we have not made use of any notion of c</context>
<context position="27819" citStr="Dasgupta et al. (2001)" startWordPosition="4954" endWordPosition="4957">theoretical framework for this (more complex) model.5 It would also be interesting to explore soft hints, where the range of χ is [0, 1] rather than {0, 1}. Recently, Ganchev et al. (2008) proposed a coregularization framework for learning across multiple related tasks with different output spaces. Their approach hinges on a constrained EM framework and addresses a quite similar problem to that addressed by this paper. Chang et al. (2008) also propose a “semisupervised” learning approach quite similar to our own model. The show very promising results in the context of semantic role labeling. 5Dasgupta et al. (2001) proved, three years later, that a formal model roughly equivalent to the actual Blum and Mitchell algorithm does have solid theoretical foundations. Given the apparent (very!) recent interest in this problem, it would be ideal to directly compare the different approaches. In addition to an analysis of the theoretical properties of the algorithm presented, the most compelling avenue for future work is to apply this framework to other task pairs. With a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (Marcu, 2000), parsi</context>
</contexts>
<marker>Dasgupta, Littman, McAllester, 2001</marker>
<rawString>Sanjoy Dasgupta, Michael Littman, and David McAllester. 2001. PAC generalization bounds for co-training. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: Approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing. Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technology (NAACL/HLT).</booktitle>
<editor>Hal Daum´e III, Andrew McCallum, Ryan McDonald, Fernando Pereira, and Charles Sutton, editors.</editor>
<marker>2006</marker>
<rawString>Hal Daum´e III, Andrew McCallum, Ryan McDonald, Fernando Pereira, and Charles Sutton, editors. 2006. Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing. Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technology (NAACL/HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Graca</author>
<author>John Blitzer</author>
<author>Ben Taskar</author>
</authors>
<title>Multi-view learning over structured and non-identical outputs.</title>
<date>2008</date>
<booktitle>In Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="27385" citStr="Ganchev et al. (2008)" startWordPosition="4884" endWordPosition="4887">aining, one assumes that the two “views” are on the inputs; here, we can think of the two output spaces as being the difference “views” and the compatibility function χ being a method for reconciling these two views. Like the pioneering work of Blum and Mitchell, the algorithm we employ in practice makes use of incrementally augmenting the unlabeled data and using model confidence. Also like that work, we do not currently have a theoretical framework for this (more complex) model.5 It would also be interesting to explore soft hints, where the range of χ is [0, 1] rather than {0, 1}. Recently, Ganchev et al. (2008) proposed a coregularization framework for learning across multiple related tasks with different output spaces. Their approach hinges on a constrained EM framework and addresses a quite similar problem to that addressed by this paper. Chang et al. (2008) also propose a “semisupervised” learning approach quite similar to our own model. The show very promising results in the context of semantic role labeling. 5Dasgupta et al. (2001) proved, three years later, that a formal model roughly equivalent to the actual Blum and Mitchell algorithm does have solid theoretical foundations. Given the appare</context>
</contexts>
<marker>Ganchev, Graca, Blitzer, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Joao Graca, John Blitzer, and Ben Taskar. 2008. Multi-view learning over structured and non-identical outputs. In Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Carmel finite state transducer package.</title>
<date>2002</date>
<note>http://www.isi. edu/licensed-sw/carmel/.</note>
<contexts>
<context position="16962" citStr="Graehl and Knight, 2002" startWordPosition="3041" endWordPosition="3044">ta. 683 4.2 HMM Results The experiments discussed in the preceding sections are based on a generative hidden Markov model for both the NER and syntactic chunking/POS tagging tasks. The HMMs constructed use first-order transitions and emissions. The emission vocabulary is pruned so that any word that appears &lt; 1 time in the training data is replaced by a unique *unknown* token. The transition and emission probabilities are smoothed with Dirichlet smoothing, α = 0.001 (this was not-aggressively tuned by hand on one setting). The HMMs are implemented as finite state models in the Carmel toolkit (Graehl and Knight, 2002). The various compatibility functions are also implemented as finite state models. We implement them as a transducer from POS/chunk labels to NER labels (though through the reverse operation, they can obviously be run in the opposite direction). The construction is with a single state with transitions: • (NNP,?) maps to B-* and I-* • (?,B-NP) maps to B-* and O • (?,I-NP) maps to B-*, I-* and O • Single exception: (NNP,x), where x is not an NP tag maps to anything (this is simply to avoid empty composition problems). This occurs in 100 of the 212k words in the Treebank data and more rarely in t</context>
</contexts>
<marker>Graehl, Knight, 2002</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2002. Carmel finite state transducer package. http://www.isi. edu/licensed-sw/carmel/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="26500" citStr="Lafferty et al., 2001" startWordPosition="4734" endWordPosition="4737">he 90% level. 5 Discussion We have presented a method for simultaneously learning two tasks using prior knowledge about the relationship between their outputs. This is related to joint inference (Daum´e III et al., 2006). However, we do not require that that a single data set be labeled for multiple tasks. In all our examples, we use separate data sets for shallow parsing as for named-entity recognition. Although all our experiments used the LaSO framework for sequence labeling, there is noting in our method that assumes any particular learner; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc. Our approach, both algorithmically and theoretically, is most related to ideas in co-training (Blum and Mitchell, 1998). The key difference is that in co-training, one assumes that the two “views” are on the inputs; here, we can think of the two output spaces as being the difference “views” and the compatibility function χ being a method for reconciling these two views. Like the pioneering work of Blum and Mitchell, the algorithm we employ in practice makes use of incrementally augmentin</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="28412" citStr="Marcu, 2000" startWordPosition="5051" endWordPosition="5052">gupta et al. (2001) proved, three years later, that a formal model roughly equivalent to the actual Blum and Mitchell algorithm does have solid theoretical foundations. Given the apparent (very!) recent interest in this problem, it would be ideal to directly compare the different approaches. In addition to an analysis of the theoretical properties of the algorithm presented, the most compelling avenue for future work is to apply this framework to other task pairs. With a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (Marcu, 2000), parsing and word alignment, or summarization and information extraction. Acknowledgments Many thanks to three anonymous reviewers of this papers whose suggestions greatly improved the work and the presentation. This work was partially funded by NSF grant IIS 0712764. A Proofs The proof of Theorem 1 closes follows that of Blum and Mitchell (1998). Proof (Theorem 1, sketch). Use the following notation: ck = PrD[h(x) = k], pl = PrD[f(x) = l], ql|k = PrD[f(x) = l |h(x) = k]. Denote by A the set of outputs that satisfy the constraints. We are interested in the probability that h(x) is erroneous, </context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7657" citStr="Marcus et al., 1993" startWordPosition="1340" endWordPosition="1343">m. 3.1 One-sided Learning with Hints We begin by considering a simplified version of the “learning with hints” problem. Suppose that all we care about is learning f2. We have a small amount of data labeled by f2 (call this D) and a large amount of data labeled by f1 (call this Dunlab–”unlab” because as far as f2 is concerned, it is unlabeled). RUNNING EXAMPLE In our example, this means that we have a small amount of labeled NER data and a large amount of labeled POS/chunk data. We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences). We are only interested in learning to do NER. Details of the exact HMM setup are in Section 4.2. 681 We call the following algorithm “One-Sided Learning with Hints,” since it aims only to learn f2: 1: Learn h2 directly on D 2: For each example (x, y1) E Dunlab 3: Compute y2 = h2(x) 4: If χ(y1, y2), add (x, y2) to D 5: Relearn h2 on the (augmented) D 6: Go to (2) if desired RUNNING EXAMPLE In step 1, we train an NER HMM on CoNLL. On test data, this model achieves an F-score of 50.8. In step 2, we run this HMM on all the WSJ dat</context>
<context position="15703" citStr="Marcus et al., 1993" startWordPosition="2833" endWordPosition="2836">eorem 1. 4 Experiments In this section, we describe our experimental results. We have already discussed some of them in the context of the running example. In Section 4.1, we briefly describe the data sets we use. A full description of the HMM implementation and its results are in Section 4.2. Finally, in Section 4.3, we present results based on a competitive, discriminativelylearned sequence labeling algorithm. All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy. 4.1 Data Sets Our results are based on syntactic data drawn from the Penn Treebank (Marcus et al., 1993), specifically the portion used by CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Our NER data is from two sources. The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). The ACE data constitute six separate data sets from six domains: weblogs (wl), newswire (nw), broadcast conversations (bc), United Nations (un), direct telephone speech (dts) and broadcast news (bn). Of these, bc, dts and bn are all speech data sets. All the examples from the previous section</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitch Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technology (NAACL/HLT).</booktitle>
<contexts>
<context position="3665" citStr="McClosky et al., 2006" startWordPosition="613" endWordPosition="616">uence is correct? NER1 seems 2The sentence is: “George Bush spoke to Congress today” 680 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 680–688, Honolulu, October 2008.c�2008 Association for Computational Linguistics ated on a large amount of unlabeled data. Its predictions are assumed to be correct, and it is retrained on the unlabeled data according to its own predictions. Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community. Its success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006). In some cases, self-training takes into account model confidence. Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practic</context>
<context position="23635" citStr="McClosky et al., 2006" startWordPosition="4235" endWordPosition="4238">he North American National Corpus of En685 wl 0 5000 10000 un 0 5000 10000 nw 0 5000 10000 cts 0 5000 10000 conll 0 5000 10000 bn 0 5000 10000 bc 0 5000 10000 POS−feature Hints (no gaz) Baseline (no gaz) Hints (w/ gaz) Baseline (w/ gaz) Self−train (no gaz) Self−train (w/ gaz) 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.7 0.6 0.5 0.4 0.3 0.2 0.8 0.75 0.7 0.65 0.6 0.55 0.95 0.9 0.85 0.8 0.75 0.9 0.8 0.7 0.6 0.5 1 0.8 0.6 0.4 0.2 0.9 0.8 0.7 0.6 0.5 Figure 2: Results of varying amount of syntactic data for a fixed amount of NER data (N = 200 sentences). glish (previously used for self-training of parsers (McClosky et al., 2006)). These 1m sentences were selected by dev-set relativization against the union of the two development data sets. Following similar ideas to those presented by Blum and Mitchell (1998), we employ two slight modifications to the algorithm presented in Section 3.2. First, in step (2b) instead of adding all allowable instances to the labeled data set, we only add the top R (for some hyper-parameter R), where “top” is determined by average model confidence for the two tasks. Second, Instead of using the full unlabeled set to label at each iteration, we begin with a random subset of 10R unlabeled e</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technology (NAACL/HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="26552" citStr="Punyakanok and Roth, 2001" startWordPosition="4741" endWordPosition="4744">ethod for simultaneously learning two tasks using prior knowledge about the relationship between their outputs. This is related to joint inference (Daum´e III et al., 2006). However, we do not require that that a single data set be labeled for multiple tasks. In all our examples, we use separate data sets for shallow parsing as for named-entity recognition. Although all our experiments used the LaSO framework for sequence labeling, there is noting in our method that assumes any particular learner; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc. Our approach, both algorithmically and theoretically, is most related to ideas in co-training (Blum and Mitchell, 1998). The key difference is that in co-training, one assumes that the two “views” are on the inputs; here, we can think of the two output spaces as being the difference “views” and the compatibility function χ being a method for reconciling these two views. Like the pioneering work of Blum and Mitchell, the algorithm we employ in practice makes use of incrementally augmenting the unlabeled data and using model confidence. Als</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>Vasin Punyakanok and Dan Roth. 2001. The use of classifiers in sequential inference. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="7684" citStr="Ramshaw and Marcus, 1995" startWordPosition="1344" endWordPosition="1347">ning with Hints We begin by considering a simplified version of the “learning with hints” problem. Suppose that all we care about is learning f2. We have a small amount of data labeled by f2 (call this D) and a large amount of data labeled by f1 (call this Dunlab–”unlab” because as far as f2 is concerned, it is unlabeled). RUNNING EXAMPLE In our example, this means that we have a small amount of labeled NER data and a large amount of labeled POS/chunk data. We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences). We are only interested in learning to do NER. Details of the exact HMM setup are in Section 4.2. 681 We call the following algorithm “One-Sided Learning with Hints,” since it aims only to learn f2: 1: Learn h2 directly on D 2: For each example (x, y1) E Dunlab 3: Compute y2 = h2(x) 4: If χ(y1, y2), add (x, y2) to D 5: Relearn h2 on the (augmented) D 6: Go to (2) if desired RUNNING EXAMPLE In step 1, we train an NER HMM on CoNLL. On test data, this model achieves an F-score of 50.8. In step 2, we run this HMM on all the WSJ data, and extract 3145 compati</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Vassil Chatalbashev</author>
<author>Daphne Koller</author>
<author>Carlos Guestrin</author>
</authors>
<title>Learning structured prediction models: A large margin approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>897--904</pages>
<contexts>
<context position="11226" citStr="Taskar et al. (2005)" startWordPosition="2026" endWordPosition="2029">ility at least 1 − δ, the algorithm returns a hypothesis h E C with error at most c. Here, EXSN(c, D) is a structured noise oracle, which draws examples from D, labels them by c and randomly replaces with another label with prob. η. Note here the rather weak notion of noise: entire structures are randomly changed, rather than individual labels. Furthermore, the error is 0/1 loss over the entire structure. Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss. While not stated directly in terms of PAC learnability, it is clear that his results apply. Taskar et al. (2005) establish tighter bounds for the case of Hamming loss. This suggests that the requirement of 0/1 loss is weaker. As suggested before, it is not sufficient for χ to simply be correct (the constant 1 function is correct, but not useful). We need it to be discriminating, made precise in the following definition. Definition 4. We say the discrimination of χ for h0 is PrD[χ(f1(x), h0(x))]−1. In other words, a constraint function is discriminating when it is unlikely that our weakly-useful predictor h0 chooses an output that satisfies the constraint. This means that if we do find examples (in our u</context>
<context position="26601" citStr="Taskar et al., 2005" startWordPosition="4749" endWordPosition="4752"> knowledge about the relationship between their outputs. This is related to joint inference (Daum´e III et al., 2006). However, we do not require that that a single data set be labeled for multiple tasks. In all our examples, we use separate data sets for shallow parsing as for named-entity recognition. Although all our experiments used the LaSO framework for sequence labeling, there is noting in our method that assumes any particular learner; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc. Our approach, both algorithmically and theoretically, is most related to ideas in co-training (Blum and Mitchell, 1998). The key difference is that in co-training, one assumes that the two “views” are on the inputs; here, we can think of the two output spaces as being the difference “views” and the compatibility function χ being a method for reconciling these two views. Like the pioneering work of Blum and Mitchell, the algorithm we employ in practice makes use of incrementally augmenting the unlabeled data and using model confidence. Also like that work, we do not currently have a theo</context>
</contexts>
<marker>Taskar, Chatalbashev, Koller, Guestrin, 2005</marker>
<rawString>Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. 2005. Learning structured prediction models: A large margin approach. In Proceedings of the International Conference on Machine Learning (ICML), pages 897–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="15796" citStr="Sang and Buchholz, 2000" startWordPosition="2849" endWordPosition="2852">eady discussed some of them in the context of the running example. In Section 4.1, we briefly describe the data sets we use. A full description of the HMM implementation and its results are in Section 4.2. Finally, in Section 4.3, we present results based on a competitive, discriminativelylearned sequence labeling algorithm. All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy. 4.1 Data Sets Our results are based on syntactic data drawn from the Penn Treebank (Marcus et al., 1993), specifically the portion used by CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Our NER data is from two sources. The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). The ACE data constitute six separate data sets from six domains: weblogs (wl), newswire (nw), broadcast conversations (bc), United Nations (un), direct telephone speech (dts) and broadcast news (bn). Of these, bc, dts and bn are all speech data sets. All the examples from the previous sections have been limited to the CoNLL data. 683 4.2 HMM Results The experiments discussed in the p</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of Conference on Computational Natural Language Learning,</booktitle>
<pages>142--147</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proceedings of Conference on Computational Natural Language Learning, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
</authors>
<title>Self-training for machine translation.</title>
<date>2006</date>
<booktitle>In NIPS workshop on Machine Learning for Multilingual Information Access.</booktitle>
<contexts>
<context position="3704" citStr="Ueffing, 2006" startWordPosition="620" endWordPosition="621">“George Bush spoke to Congress today” 680 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 680–688, Honolulu, October 2008.c�2008 Association for Computational Linguistics ated on a large amount of unlabeled data. Its predictions are assumed to be correct, and it is retrained on the unlabeled data according to its own predictions. Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community. Its success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006). In some cases, self-training takes into account model confidence. Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of mode</context>
</contexts>
<marker>Ueffing, 2006</marker>
<rawString>Nicola Ueffing. 2006. Self-training for machine translation. In NIPS workshop on Machine Learning for Multilingual Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>A theory of the learnable.</title>
<date>1994</date>
<booktitle>Annual ACM Symposium on Theory of Computing,</booktitle>
<pages>436--445</pages>
<contexts>
<context position="4701" citStr="Valiant, 1994" startWordPosition="782" endWordPosition="783">ls are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident. The original, and simplest analysis of cotraining is due to Blum and Mitchell (1998). It does not take into account confidence (to do so requires a significantly more detailed analysis (Dasgupta et al., 2001)), but is useful for understanding the process. 3 Model We define a formal PAC-style (Valiant, 1994) model that we call the “hints model”3. We have an instance space X and two output spaces Y1 and Y2. We assume two concept classes C1 and C2 for each output space respectively. Let D be a distribution over X, and f1 E C1 (resp., f2 E C2) be target functions. The goal, of course, is to use a finite sample of examples drawn from D (and labeled—perhaps with noise— by f1 and f2) to “learn” h1 E C1 and h2 E C2, which are good approximations to f1 and f2. So far we have not made use of any notion of constraints. Our expectation is that if we constrain h1 and h2 to agree (vis-a-vis the example in the</context>
</contexts>
<marker>Valiant, 1994</marker>
<rawString>Leslie G. Valiant. 1994. A theory of the learnable. Annual ACM Symposium on Theory of Computing, pages 436–445.</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<booktitle>Automatic Content Extraction Workshop (ACE-2004),</booktitle>
<editor>Ralph Weischedel, editor.</editor>
<location>Alexandria, Virginia,</location>
<marker>2004</marker>
<rawString>Ralph Weischedel, editor. 2004. Automatic Content Extraction Workshop (ACE-2004), Alexandria, Virginia, September 20–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3799" citStr="Yarowsky, 1995" startWordPosition="632" endWordPosition="633">ds in Natural Language Processing, pages 680–688, Honolulu, October 2008.c�2008 Association for Computational Linguistics ated on a large amount of unlabeled data. Its predictions are assumed to be correct, and it is retrained on the unlabeled data according to its own predictions. Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community. Its success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006). In some cases, self-training takes into account model confidence. Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident. The or</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>