<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.965393">
Joint Annotation of Search Queries
</title>
<author confidence="0.995031">
Michael Bendersky
</author>
<affiliation confidence="0.9987815">
Dept. of Computer Science
University of Massachusetts
</affiliation>
<address confidence="0.571181">
Amherst, MA
</address>
<email confidence="0.998195">
bemike@cs.umass.edu
</email>
<author confidence="0.989058">
W. Bruce Croft
</author>
<affiliation confidence="0.9986965">
Dept. of Computer Science
University of Massachusetts
</affiliation>
<address confidence="0.571086">
Amherst, MA
</address>
<email confidence="0.998013">
croft@cs.umass.edu
</email>
<author confidence="0.990467">
David A. Smith
</author>
<affiliation confidence="0.998739">
Dept. of Computer Science
University of Massachusetts
</affiliation>
<address confidence="0.57136">
Amherst, MA
</address>
<email confidence="0.998549">
dasmith@cs.umass.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999747045454545">
Marking up search queries with linguistic an-
notations such as part-of-speech tags, cap-
italization, and segmentation, is an impor-
tant part of query processing and understand-
ing in information retrieval systems. Due
to their brevity and idiosyncratic structure,
search queries pose a challenge to existing
NLP tools. To address this challenge, we
propose a probabilistic approach for perform-
ing joint query annotation. First, we derive
a robust set of unsupervised independent an-
notations, using queries and pseudo-relevance
feedback. Then, we stack additional classi-
fiers on the independent annotations, and ex-
ploit the dependencies between them to fur-
ther improve the accuracy, even with a very
limited amount of available training data. We
evaluate our method using a range of queries
extracted from a web search log. Experimen-
tal results verify the effectiveness of our ap-
proach for both short keyword queries, and
verbose natural language queries.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999862295454545">
Automatic mark-up of textual documents with lin-
guistic annotations such as part-of-speech tags, sen-
tence constituents, named entities, or semantic roles
is a common practice in natural language process-
ing (NLP). It is, however, much less common in in-
formation retrieval (IR) applications. Accordingly,
in this paper, we focus on annotating search queries
submitted by the users to a search engine.
There are several key differences between user
queries and the documents used in NLP (e.g., news
articles or web pages). As previous research shows,
these differences severely limit the applicability of
standard NLP techniques for annotating queries and
require development of novel annotation approaches
for query corpora (Bergsma and Wang, 2007; Barr et
al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li,
2010).
The most salient difference between queries and
documents is their length. Most search queries
are very short, and even longer queries are usually
shorter than the average written sentence. Due to
their brevity, queries often cannot be divided into
sub-parts, and do not provide enough context for
accurate annotations to be made using the stan-
dard NLP tools such as taggers, parsers or chun-
kers, which are trained on more syntactically coher-
ent textual units.
A recent analysis of web query logs by Bendersky
and Croft (2009) shows, however, that despite their
brevity, queries are grammatically diverse. Some
queries are keyword concatenations, some are semi-
complete verbal phrases and some are wh-questions.
It is essential for the search engine to correctly an-
notate the query structure, and the quality of these
query annotations has been shown to be a crucial
first step towards the development of reliable and
robust query processing, representation and under-
standing algorithms (Barr et al., 2008; Guo et al.,
2008; Guo et al., 2009; Manshadi and Li, 2009; Li,
2010).
However, in current query annotation systems,
even sentence-like queries are often hard to parse
and annotate, as they are prone to contain mis-
spellings and idiosyncratic grammatical structures.
</bodyText>
<page confidence="0.980466">
102
</page>
<note confidence="0.985107">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102–111,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.997332375">
(a) (b) (c)
Term CAP TAG SEG Term CAP TAG SEG Term CAP TAG SEG
who L X B kindred C N B shih C N B
won L V I where C X B tzu C N I
the L X B would C X I health L N B
2004 L X B i C X I problems L N I
kentucky C N B be C V I
derby C N I
</figure>
<figureCaption confidence="0.997382">
Figure 1: Examples of a mark-up scheme for annotating capitalization (L – lowercase, C – otherwise), POS tags (N –
noun, V – verb, X – otherwise) and segmentation (B/I – beginning of/inside the chunk).
</figureCaption>
<bodyText confidence="0.999967469387755">
They also tend to lack prepositions, proper punctu-
ation, or capitalization, since users (often correctly)
assume that these features are disregarded by the re-
trieval system.
In this paper, we propose a novel joint query an-
notation method to improve the effectiveness of ex-
isting query annotations, especially for longer, more
complex search queries. Most existing research fo-
cuses on using a single type of annotation for infor-
mation retrieval such as subject-verb-object depen-
dencies (Balasubramanian and Allan, 2009), named-
entity recognition (Guo et al., 2009), phrase chunk-
ing (Guo et al., 2008), or semantic labeling (Li,
2010).
In contrast, the main focus of this work is on de-
veloping a unified approach for performing reliable
annotations of different types. To this end, we pro-
pose a probabilistic method for performing a joint
query annotation. This method allows us to exploit
the dependency between different unsupervised an-
notations to further improve the accuracy of the en-
tire set of annotations. For instance, our method
can leverage the information about estimated parts-
of-speech tags and capitalization of query terms to
improve the accuracy of query segmentation.
We empirically evaluate the joint query annota-
tion method on a range of query types. Instead of
just focusing our attention on keyword queries, as
is often done in previous work (Barr et al., 2008;
Bergsma and Wang, 2007; Tan and Peng, 2008;
Guo et al., 2008), we also explore the performance
of our annotations with more complex natural lan-
guage search queries such as verbal phrases and wh-
questions, which often pose a challenge for IR appli-
cations (Bendersky et al., 2010; Kumaran and Allan,
2007; Kumaran and Carvalho, 2009; Lease, 2007).
We show that even with a very limited amount of
training data, our joint annotation method signifi-
cantly outperforms annotations that were done in-
dependently for these queries.
The rest of the paper is organized as follows. In
Section 2 we demonstrate several examples of an-
notated search queries. Then, in Section 3, we in-
troduce our joint query annotation method. In Sec-
tion 4 we describe two types of independent query
annotations that are used as input for the joint query
annotation. Section 5 details the related work and
Section 6 presents the experimental results. We draw
the conclusions from our work in Section 7.
</bodyText>
<sectionHeader confidence="0.967929" genericHeader="method">
2 Query Annotation Example
</sectionHeader>
<bodyText confidence="0.99883725">
To demonstrate a possible implementation of lin-
guistic annotation for search queries, Figure 1
presents a simple mark-up scheme, exemplified us-
ing three web search queries (as they appear in a
search log): (a) who won the 2004 kentucky derby,
(b) kindred where would i be, and (c) shih tzu health
problems. In this scheme, each query is marked-
up using three annotations: capitalization, POS tags,
and segmentation indicators.
Note that all the query terms are non-capitalized,
and no punctuation is provided by the user, which
complicates the query annotation process. While
the simple annotation described in Figure 1 can be
done with a very high accuracy for standard docu-
ment corpora, both previous work (Barr et al., 2008;
Bergsma and Wang, 2007; Jones and Fain, 2003)
and the experimental results in this paper indicate
that it is challenging to perform well on queries.
The queries in Figure 1 illustrate this point. Query
(a) in Figure 1 is a wh-question, and it contains
</bodyText>
<page confidence="0.998952">
103
</page>
<bodyText confidence="0.999532833333333">
a capitalized concept (“Kentucky Derby”), a single
verb, and four segments. Query (b) is a combination
of an artist name and a song title and should be inter-
preted as Kindred — “Where Would I Be”. Query (c)
is a concatenation of two short noun phrases: “Shih
Tzu” and “health problems”.
</bodyText>
<sectionHeader confidence="0.963134" genericHeader="method">
3 Joint Query Annotation
</sectionHeader>
<bodyText confidence="0.999459333333333">
Given a search query Q, which consists of a se-
quence of terms (q1, ... , qn), our goal is to anno-
tate it with an appropriate set of linguistic structures
ZQ. In this work, we assume that the set ZQ consists
of shallow sequence annotations zQ, each of which
takes the form
</bodyText>
<equation confidence="0.987497">
zQ = (ζ1, . . . , ζn).
</equation>
<bodyText confidence="0.997563666666667">
In other words, each symbol ζi ∈ zQ annotates a
single query term.
Many query annotations that are useful for IR
can be represented using this simple form, includ-
ing capitalization, POS tagging, phrase chunking,
named entity recognition, and stopword indicators,
to name just a few. For instance, Figure 1 demon-
strates an example of a set of annotations ZQ. In
this example,
</bodyText>
<equation confidence="0.695746">
ZQ = {CAP, TAG, SEG}.
</equation>
<bodyText confidence="0.9992738">
Most previous work on query annotation makes
the independence assumption — every annotation
zQ ∈ ZQ is done separately from the others. That is,
it is assumed that the optimal linguistic annotation
zQ is the annotation that has the highest probabil-
</bodyText>
<equation confidence="0.726751">
∗(I)
</equation>
<bodyText confidence="0.9990515">
ity given the query Q, regardless of the other anno-
tations in the set ZQ. Formally,
</bodyText>
<equation confidence="0.994173333333333">
zQ = argmax
∗(I)
ZQ
</equation>
<bodyText confidence="0.996607909090909">
The main shortcoming of this approach is in the
assumption that the linguistic annotations in the set
ZQ are independent. In practice, there are depen-
dencies between the different annotations, and they
can be leveraged to derive a better estimate of the
entire set of annotations.
For instance, imagine that we need to perform two
annotations: capitalization and POS tagging. Know-
ing that a query term is capitalized, we are more
likely to decide that it is a proper noun. Vice versa,
knowing that it is a preposition will reduce its proba-
bility of being capitalized. We would like to capture
this intuition in the annotation process.
To address the problem of joint query annotation,
we first assume that we have an initial set of annota-
tions Z∗(I)
Q , which were performed for query Q in-
dependently of one another (we will show an exam-
ple of how to derive such a set in Section 4). Given
the initial set Z∗(I)
Q , we are interested in obtaining
an annotation set Z∗(J)
</bodyText>
<equation confidence="0.8431676">
Q , which jointly optimizes the
probability of all the annotations, i.e.
Z∗(J)
Q = argmax
ZQ
</equation>
<bodyText confidence="0.985756">
If the initial set of estimations is reasonably ac-
curate, we can make the assumption that the anno-
tations in the set Z∗(J)
</bodyText>
<equation confidence="0.9469404">
Q are independent given the
initial estimates Z∗(I)
Q , allowing us to separately op-
timize the probability of each annotation z∗(J)
Q ∈
Z∗(J)
Q :
zQ = argmax p(zQ|Z∗(I)
∗(J) Q ). (2)
ZQ
</equation>
<bodyText confidence="0.9997384">
From Eq. 2, it is evident that the joint an-
notation task becomes that of finding some opti-
mal unobserved sequence (annotation z∗(J)
Q ), given
the observed sequences (independent annotation set
</bodyText>
<equation confidence="0.9411295">
Z∗(I)
Q ).
</equation>
<bodyText confidence="0.9940225">
Accordingly, we can directly use a supervised se-
quential probabilistic model such as CRF (Lafferty
et al., 2001) to find the optimal z∗(J)
Q . In this CRF
model, the optimal annotation z∗(J) Qis the label we
are trying to predict, and the set of independent an-
notations Z∗(I)
Q is used as the basis for the features
used for prediction. Figure 2 outlines the algorithm
for performing the joint query annotation.
As input, the algorithm receives a training set of
queries and their ground truth annotations. It then
produces a set of independent annotation estimates,
which are jointly used, together with the ground
truth annotations, to learn a CRF model for each an-
notation type. Finally, these CRF models are used
to predict annotations on a held-out set of queries,
which are the output of the algorithm.
</bodyText>
<equation confidence="0.665992">
p(zQ|Q) (1)
p(ZQ|Z∗(I)
Q ).
104
Input: Qt — training set of queries.
ZQt — ground truth annotations for the training set of queries.
Qh — held-out set of queries.
</equation>
<listItem confidence="0.94756">
(1) Obtain a set of independent annotation estimates Z∗(I)
Qt
(2) Initialize Z∗(J) ;
Qt
(3) for each z∗(I) Qt2 Z∗(I)
Qt �
/4) Z′ Z∗(I) \ z∗(I)
1 Qt Qt Qt
(5) Train a CRF model CRF(zQt) using zQt as a label and Z′ Qt as features.
(6) Predict annotation z∗Q(h ), using CRF(zQt).
</listItem>
<figure confidence="0.73552775">
(7) Z∗(J) Z∗(J) [ z∗(Jh)
Qh Qh Qh .
Output: Z∗(J) — predicted annotations for the held-out set of queries.
Qh
</figure>
<figureCaption confidence="0.998887">
Figure 2: Algorithm for performing joint query annotation.
</figureCaption>
<bodyText confidence="0.99937425">
Note that this formulation of joint query anno-
tation can be viewed as a stacked classification, in
which a second, more effective, classifier is trained
using the labels inferred by the first classifier as fea-
tures. Stacked classifiers were recently shown to be
an efficient and effective strategy for structured clas-
sification in NLP (Nivre and McDonald, 2008; Mar-
tins et al., 2008).
</bodyText>
<sectionHeader confidence="0.993643" genericHeader="method">
4 Independent Query Annotations
</sectionHeader>
<bodyText confidence="0.999980333333333">
While the joint annotation method proposed in Sec-
tion 3 is general enough to be applied to any set of
independent query annotations, in this work we fo-
cus on two previously proposed independent anno-
tation methods based on either the query itself, or
the top sentences retrieved in response to the query
(Bendersky et al., 2010). The main benefits of these
two annotation methods are that they can be easily
implemented using standard software tools, do not
require any labeled data, and provide reasonable an-
notation accuracy. Next, we briefly describe these
two independent annotation methods.
</bodyText>
<subsectionHeader confidence="0.98746">
4.1 Query-based estimation
</subsectionHeader>
<bodyText confidence="0.999422857142857">
The most straightforward way to estimate the con-
ditional probabilities in Eq. 1 is using the query it-
self. To make the estimation feasible, Bendersky et
al. (2010) take a bag-of-words approach, and assume
independence between both the query terms and the
corresponding annotation symbols. Thus, the inde-
pentent annotations in Eq. 1 are given by
</bodyText>
<equation confidence="0.862905">
((1,...,(-) iE(1,...,n)
</equation>
<bodyText confidence="0.981027">
Following Bendersky et al. (2010) we use a large
n-gram corpus (Brants and Franz, 2006) to estimate
p(ζi|qi) for annotating the query with capitalization
and segmentation mark-up, and a standard POS tag-
ger1 for part-of-speech tagging of the query.
</bodyText>
<subsectionHeader confidence="0.890467">
4.2 PRF-based estimation
</subsectionHeader>
<bodyText confidence="0.999958210526316">
Given a short, often ungrammatical query, it is hard
to accurately estimate the conditional probability in
Eq. 1 using the query terms alone. For instance, a
keyword query hawaiian falls, which refers to a lo-
cation, is inaccurately interpreted by a standard POS
tagger as a noun-verb pair. On the other hand, given
a sentence from a corpus that is relevant to the query
such as “Hawaiian Falls is a family-friendly water-
park”, the word “falls” is correctly identified by a
standard POS tagger as a proper noun.
Accordingly, the document corpus can be boot-
strapped in order to better estimate the query anno-
tation. To this end, Bendersky et al. (2010) employ
the pseudo-relevance feedback (PRF) — a method
that has a long record of success in IR for tasks such
as query expansion (Buckley, 1995; Lavrenko and
Croft, 2001).
In the most general form, given the set of all re-
trievable sentences r in the corpus C one can derive
</bodyText>
<equation confidence="0.992482">
p(zQ|Q) = � p(zQ|r)p(r|Q).
rEC
</equation>
<bodyText confidence="0.9804825">
Since for most sentences the conditional proba-
bility of relevance to the query p(r|Q) is vanish-
</bodyText>
<equation confidence="0.880827333333333">
�
z —
� _ argmax p(ζi |qi). (3)
</equation>
<footnote confidence="0.755349">
QRY) ingly small, the above can be closely approximated
1http://crftagger.sourceforge.net/
</footnote>
<page confidence="0.997175">
105
</page>
<bodyText confidence="0.968364848484849">
by considering only a set of sentences R, retrieved combines several independent annotations to im-
at top-k positions in response to the query Q. This prove the overall annotation accuracy. A similar ap-
yields proach was recently proposed by Guo et al. (2008).
p(zQ|Q) ≈ 1: p(zQ|r)p(r|Q). There are several key differences, however, between
rER the work presented here and their work.
Intuitively, the equation above models the query as First, Guo et al. (2008) focus on query refine-
a mixture of top-k retrieved sentences, where each ment (spelling corrections, word splitting, etc.) of
sentence is weighted by its relevance to the query. short keyword queries. Instead, we are interested
Furthermore, to make the estimation of the condi- in annotation of queries of different types, includ-
tional probability p(zQ|r) feasible, it is assumed that ing verbose natural language queries. While there
the symbols ζi in the annotation sequence are in- is an overlap between query refinement and annota-
dependent, given a sentence r. Note that this as- tion, the focus of the latter is on providing linguistic
sumption differs from the independence assumption information about existing queries (after initial re-
in Eq. 3, since here the annotation symbols are not finement has been performed). Such information is
independent given the query Q. especially important for more verbose and gramat-
Accordingly, the PRF-based estimate for indepen- ically complex queries. In addition, while all the
dent annotations in Eq. 1 is methods proposed by Guo et al. (2008) require large
zQPRF) = argmax 1: H amounts of training data (thousands of training ex-
(ζ1,...,ζ-) rERiE(1,...,n) p(ζi|r)p(r|Q). amples), our joint annotation method can be effec-
(4) tively trained with a minimal human labeling effort
Following Bendersky et al. (2010), an estimate of (several hundred training examples).
p(ζi|r) is a smoothed estimator that combines the An additional research area which is relevant to
information from the retrieved sentence r with the this paper is the work on joint structure model-
information about unigrams (for capitalization and ing (Finkel and Manning, 2009; Toutanova et al.,
POS tagging) and bigrams (for segmentation) from 2008) and stacked classification (Nivre and Mc-
a large n-gram corpus (Brants and Franz, 2006). Donald, 2008; Martins et al., 2008) in natural lan-
5 Related Work guage processing. These approaches have been
In recent years, linguistic annotation of search shown to be successful for tasks such as parsing and
queries has been receiving increasing attention as an named entity recognition in newswire data (Finkel
important step toward better query processing and and Manning, 2009) or semantic role labeling in the
understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al.,
includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon-
2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguis-
</bodyText>
<table confidence="0.685119512195122">
gen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap-
2008), part-of-speech and semantic tagging (Barr et plications.
al., 2008; Manshadi and Li, 2009; Li, 2010), named-
entity recognition (Guo et al., 2009; Lu et al., 2009;
Shen et al., 2008; Pas¸ca, 2007), abbreviation disam-
biguation (Wei et al., 2008) and stopword detection
(Lo et al., 2005; Jones and Fain, 2003).
Most of the previous work on query annotation
focuses on performing a particular annotation task
(e.g., segmentation or POS tagging) in isolation.
However, these annotations are often related, and
thus we take a joint annotation approach, which
106
6 Experiments
6.1 Experimental Setup
For evaluating the performance of our query anno-
tation methods, we use a random sample of 250
queries2 from a search log. This sample is manually
labeled with three annotations: capitalization, POS
tags, and segmentation, according to the description
of these annotations in Figure 1. In this set of 250
queries, there are 93 questions, 96 phrases contain-
2The annotations are available at
http://ciir.cs.umass.edu/∼bemike/data.html
CAP F1 (% impr) MQA (% impr)
i-QRY 0.641 (-/-) 0.779 (-/-)
i-PRF 0.711*(+10.9/-) 0.811*(+4.1/-)
j-QRY 0.6201(-3.3/-12.8) 0.805*(+3.3/-0.7)
j-PRF 0.718*(+12.0/+0.9) 0.840*1(+7.8/+3.6)
TAG
Acc. (% impr) MQA (% impr)
i-QRY 0.893 (-/-) 0.878 (-/-)
i-PRF 0.916*(+2.6/-) 0.914*(+4.1/-)
j-QRY 0.913*(+2.2/-0.3) 0.912*(+3.9/-0.2)
j-PRF 0.924*(+3.5/+0.9) 0.922*(+5.0/+0.9)
SEG
F1 (% impr) MQA (% impr)
i-QRY 0.694 (-/-) 0.672 (-/-)
i-PRF 0.753*(+8.5/-) 0.710*(+5.7/-)
j-QRY 0.817*1(+17.7/+8.5) 0.803*1(+19.5/+13.1)
j-PRF 0.819*1(+18.0/+8.8) 0.803*1(+19.5/+13.1)
</table>
<tableCaption confidence="0.785752">
Table 1: Summary of query annotation performance for
capitalization (CAP), POS tagging (TAG) and segmenta-
tion. Numbers in parentheses indicate % of improvement
</tableCaption>
<bodyText confidence="0.936040666666667">
over the i-QRY and i-PRF baselines, respectively. Best
result per measure and annotation is boldfaced. * and 1
denote statistically significant differences with i-QRY and
i-PRF, respectively.
ing a verb, and 61 short keyword queries (Figure 1
contains a single example of each of these types).
In order to test the effectiveness of the joint query
annotation, we compare four methods. In the first
two methods, i-QRY and i-PRF the three annotations
are done independently. Method i-QRY is based on
z*QRY ) estimator (Eq. 3). Method i-PRF is based
Q
on the z�(PRF) estimator (Eq. 4).
Q
The next two methods, j-QRY and j-PRF, are joint
annotation methods, which perform a joint optimiza-
tion over the entire set of annotations, as described
in the algorithm in Figure 2. j-QRY and j-PRF differ
in their choice of the initial independent annotation
set Z*(,) Qin line (1) of the algorithm (see Figure 2).
j-QRY uses only the annotations performed by i-
QRY (3 initial independent annotation estimates),
while j-PRF combines the annotations performed by
i-QRY with the annotations performed by i-PRF (6
initial annotation estimates). The CRF model train-
ing in line (6) of the algorithm is implemented using
CRF++ toolkit3.
</bodyText>
<footnote confidence="0.76635">
3http://crfpp.sourceforge.net/
</footnote>
<bodyText confidence="0.999900375">
The performance of the joint annotation methods
is estimated using a 10-fold cross-validation. In or-
der to test the statistical significance of improve-
ments attained by the proposed methods we use a
two-sided Fisher’s randomization test with 20,000
permutations. Results with p-value &lt; 0.05 are con-
sidered statistically significant.
For reporting the performance of our meth-
ods we use two measures. The first measure is
classification-oriented — treating the annotation de-
cision for each query term as a classification. In case
of capitalization and segmentation annotations these
decisions are binary and we compute the precision
and recall metrics, and report F1 — their harmonic
mean. In case of POS tagging, the decisions are
ternary, and hence we report the classification ac-
curacy.
We also report an additional, IR-oriented perfor-
mance measure. As is typical in IR, we propose
measuring the performance of the annotation meth-
ods on a per-query basis, to verify that the methods
have uniform impact across queries. Accordingly,
we report the mean of classification accuracies per
query (MQA). Formally, MQA is computed as
</bodyText>
<equation confidence="0.999067333333333">
�N
i=1 accQi
N �
</equation>
<bodyText confidence="0.999913">
where accQi is the classification accuracy for query
Qi, and N is the number of queries.
The empirical evaluation is conducted as follows.
In Section 6.2, we discuss the general performance
of the four annotation techniques, and compare the
effectiveness of independent and joint annotations.
In Section 6.3, we analyze the performance of the
independent and joint annotation methods by query
type. In Section 6.4, we compare the difficulty
of performing query annotations for different query
types. Finally, in Section 6.5, we compare the effec-
tiveness of the proposed joint annotation for query
segmentation with the existing query segmentation
methods.
</bodyText>
<subsectionHeader confidence="0.996223">
6.2 General Evaluation
</subsectionHeader>
<bodyText confidence="0.99997775">
Table 1 shows the summary of the performance of
the two independent and two joint annotation meth-
ods for the entire set of 250 queries. For independent
methods, we see that i-PRF outperforms i-QRY for
</bodyText>
<page confidence="0.997537">
107
</page>
<table confidence="0.999669083333333">
CAP Verbal Phrases Questions Keywords
F1 MQA F1 MQA F1 MQA
i-PRF 0.750 0.862 0.590 0.839 0.784 0.687
j-PRF 0.687*(-8.4%) 0.839*(-2.7%) 0.671*(+13.7%) 0.913*(+8.8%) 0.814 (+3.8%) 0.732* (+6.6%)
TAG Verbal Phrases Questions Keywords
Acc. MQA Acc. MQA Acc. MQA
i-PRF 0.908 0.908 0.932 0.935 0.880 0.890
j-PRF 0.904 (-0.4%) 0.906 (-0.2%) 0.951* (+2.1%) 0.953* (+1.9%) 0.893 (+1.5%) 0.900 (+1.1%)
SEG Verbal Phrases Questions Keywords
F1 MQA F1 MQA F1 MQA
i-PRF 0.751 0.700 0.740 0.700 0.816 0.747
j-PRF 0.772 (+2.8%) 0.742*(+6.0%) 0.858*(+15.9%) 0.838*(+19.7%) 0.844 (+3.4%) 0.853*(+14.2%)
</table>
<tableCaption confidence="0.973723666666667">
Table 2: Detailed analysis of the query annotation performance for capitalization (CAP), POS tagging (TAG) and
segmentation by query type. Numbers in parentheses indicate % of improvement over the i-PRF baseline. Best result
per measure and annotation is boldfaced. * denotes statistically significant differences with i-PRF.
</tableCaption>
<bodyText confidence="0.999889864864865">
all annotation types, using both performance mea-
sures.
In Table 1, we can also observe that the joint anno-
tation methods are, in all cases, better than the cor-
responding independent ones. The highest improve-
ments are attained by j-PRF, which always demon-
strates the best performance both in terms of F1 and
MQA. These results attest to both the importance of
doing a joint optimization over the entire set of an-
notations and to the robustness of the initial annota-
tions done by the i-PRF method. In all but one case,
the j-PRF method, which uses these annotations as
features, outperforms the j-QRY method that only
uses the annotation done by i-QRY.
The most significant improvements as a result of
joint annotation are observed for the segmentation
task. In this task, joint annotation achieves close to
20% improvement in MQA over the i-QRY method,
and more than 10% improvement in MQA over the i-
PRF method. These improvements indicate that the
segmentation decisions are strongly guided by cap-
italization and POS tagging. We also note that, in
case of segmentation, the differences in performance
between the two joint annotation methods, j-QRY
and j-PRF, are not significant, indicating that the
context of additional annotations in j-QRY makes up
for the lack of more robust pseudo-relevance feed-
back based features.
We also note that the lowest performance im-
provement as a result of joint annotation is evi-
denced for POS tagging. The improvements of joint
annotation method j-PRF over the i-PRF method are
less than 1%, and are not statistically significant.
This is not surprising, since the standard POS tag-
gers often already use bigrams and capitalization at
training time, and do not acquire much additional
information from other annotations.
</bodyText>
<subsectionHeader confidence="0.99868">
6.3 Evaluation by Query Type
</subsectionHeader>
<bodyText confidence="0.999781391304348">
Table 2 presents a detailed analysis of the perfor-
mance of the best independent (i-PRF) and joint (j-
PRF) annotation methods by the three query types
used for evaluation: verbal phrases, questions and
keyword queries. From the analysis in Table 2, we
note that the contribution of joint annotation varies
significantly across query types. For instance, us-
ing j-PRF always leads to statistically significant im-
provements over the i-PRF baseline for questions.
On the other hand, it is either statistically indistin-
guishable, or even significantly worse (in the case of
capitalization) than the i-PRF baseline for the verbal
phrases.
Table 2 also demonstrates that joint annotation
has a different impact on various annotations for the
same query type. For instance, j-PRF has a signif-
icant positive effect on capitalization and segmen-
tation for keyword queries, but only marginally im-
proves the POS tagging. Similarly, for the verbal
phrases, j-PRF has a significant positive effect only
for the segmentation annotation.
These variances in the performance of the j-PRF
method point to the differences in the structure be-
</bodyText>
<page confidence="0.994739">
108
</page>
<table confidence="0.883206166666667">
SEG F1 MQA
SEG-1 0.768 0.754
SEG-2 0.824∗ 0.787∗
j-PRF 0.819∗ (+6.7%/-0.6%) 0.803∗ (+6.5%/+2.1%)
Annotation Performance by Query Type
Verbal Phrases Questions Keyword Queries
</table>
<figureCaption confidence="0.984269333333333">
Figure 3: Comparative performance (in terms of F1 for
capitalization and segmentation and accuracy for POS
tagging) of the j-PRF method on the three query types.
</figureCaption>
<bodyText confidence="0.999929444444444">
tween the query types. While dependence between
the annotations plays an important role for question
and keyword queries, which often share a common
grammatical structure, this dependence is less use-
ful for verbal phrases, which have a more diverse
linguistic structure. Accordingly, a more in-depth
investigation of the linguistic structure of the verbal
phrase queries is an interesting direction for future
work.
</bodyText>
<subsectionHeader confidence="0.999172">
6.4 Annotation Difficulty
</subsectionHeader>
<bodyText confidence="0.999740277777778">
Recall that in our experiments, out of the overall 250
annotated queries, there are 96 verbal phrases, 93
questions and 61 keyword queries. Figure 3 shows a
plot that contrasts the relative performance for these
three query types of our best-performing joint an-
notation method, j-PRF, on capitalization, POS tag-
ging and segmentation annotation tasks. Next, we
analyze the performance profiles for the annotation
tasks shown in Figure 3.
For the capitalization task, the performance of j-
PRF on verbal phrases and questions is similar, with
the difference below 3%. The performance for key-
word queries is much higher — with improvement
over 20% compared to either of the other two types.
We attribute this increase to both a larger number
of positive examples in the short keyword queries
(a higher percentage of terms in keyword queries is
capitalized) and their simpler syntactic structure (ad-
</bodyText>
<tableCaption confidence="0.861249333333333">
Table 3: Comparison of the segmentation performance
of the j-PRF method to two state-of-the-art segmentation
methods. Numbers in parentheses indicate % of improve-
ment over the SEG-1 and SEG-2 baselines respectively.
Best result per measure and annotation is boldfaced. ∗
denotes statistically significant differences with SEG-1.
</tableCaption>
<bodyText confidence="0.999867392857143">
jacent terms in these queries are likely to have the
same case).
For the segmentation task, the performance is at
its best for the question and keyword queries, and at
its worst (with a drop of 11%) for the verbal phrases.
We hypothesize that this is due to the fact that ques-
tion queries and keyword queries tend to have repet-
itive structures, while the grammatical structure for
verbose queries is much more diverse.
For the tagging task, the performance profile is re-
versed, compared to the other two tasks — the per-
formance is at its worst for keyword queries, since
their grammatical structure significantly differs from
the grammatical structure of sentences in news arti-
cles, on which the POS tagger is trained. For ques-
tion queries the performance is the best (6% increase
over the keyword queries), since they resemble sen-
tences encountered in traditional corpora.
It is important to note that the results reported in
Figure 3 are based on training the joint annotation
model on all available queries with 10-fold cross-
validation. We might get different profiles if a sep-
arate annotation model was trained for each query
type. In our case, however, the number of queries
from each type is not sufficient to train a reliable
model. We leave the investigation of separate train-
ing of joint annotation models by query type to fu-
ture work.
</bodyText>
<subsectionHeader confidence="0.970201">
6.5 Additional Comparisons
</subsectionHeader>
<bodyText confidence="0.999882714285714">
In order to further evaluate the proposed joint an-
notation method, j-PRF, in this section we compare
its performance to other query annotation methods
previously reported in the literature. Unfortunately,
there is not much published work on query capi-
talization and query POS tagging that goes beyond
the simple query-based methods described in Sec-
</bodyText>
<figure confidence="0.9698318">
F1
60 65 70 75 80 85 90 95 100
TAG
SEG
CAP
</figure>
<page confidence="0.993515">
109
</page>
<bodyText confidence="0.99955375">
tion 4.1. The published work on the more advanced
methods usually requires access to large amounts of
proprietary user data such as query logs and clicks
(Barr et al., 2008; Guo et al., 2008; Guo et al., 2009).
Therefore, in this section we focus on recent work
on query segmentation (Bergsma and Wang, 2007;
Hagen et al., 2010). We compare the segmentation
effectiveness of our best performing method, j-PRF,
to that of these query segmentation methods.
The first method, SEG-1, was first proposed by
Hagen et al. (2010). It is currently the most effective
publicly disclosed unsupervised query segmentation
method. SEG-1 method requires an access to a large
web n-gram corpus (Brants and Franz, 2006). The
optimal segmentation for query Q, SQ, is then ob-
tained using
</bodyText>
<equation confidence="0.95732">
1: S� Q = argmax |s||s|count(s),
SESQ sES,|s|&gt;1
</equation>
<bodyText confidence="0.999951432432433">
where SQ is the set of all possible query segmenta-
tions, S is a possible segmentation, s is a segment
in S, and count(s) is the frequency of s in the web
n-gram corpus.
The second method, SEG-2, is based on a success-
ful supervised segmentation method, which was first
proposed by Bergsma and Wang (2007). SEG-2 em-
ploys a large set of features, and is pre-trained on the
query collection described by Bergsma and Wang
(2007). The features used by the SEG-2 method are
described by Bendersky et al. (2009), and include,
among others, n-gram frequencies in a sample of a
query log, web corpus and Wikipedia titles.
Table 3 demonstrates the comparison between the
j-PRF, SEG-1 and SEG-2 methods. When com-
pared to the SEG-1 baseline, j-PRF is significantly
more effective, even though it only employs bigram
counts (see Eq. 4), instead of the high-order n-grams
used by SEG-1, for computing the score of a seg-
mentation. This results underscores the benefit of
joint annotation, which leverages capitalization and
POS tagging to improve the quality of the segmen-
tation.
When compared to the SEG-2 baseline, j-PRF
and SEG-2 are statistically indistinguishable. SEG-2
posits a slightly better F1, while j-PRF has a better
MQA. This result demonstrates that the segmenta-
tion produced by the j-PRF method is as effective as
the segmentation produced by the current supervised
state-of-the-art segmentation methods, which em-
ploy external data sources and high-order n-grams.
The benefit of the j-PRF method compared to the
SEG-2 method, is that, simultaneously with the seg-
mentation, it produces several additional query an-
notations (in this case, capitalization and POS tag-
ging), eliminating the need to construct separate se-
quence classifiers for each annotation.
</bodyText>
<sectionHeader confidence="0.999086" genericHeader="evaluation">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99996980952381">
In this paper, we have investigated a joint approach
for annotating search queries with linguistic struc-
tures, including capitalization, POS tags and seg-
mentation. To this end, we proposed a probabilis-
tic approach for performing joint query annotation
that takes into account the dependencies that exist
between the different annotation types.
Our experimental findings over a range of queries
from a web search log unequivocally point to the su-
periority of the joint annotation methods over both
query-based and pseudo-relevance feedback based
independent annotation methods. These findings in-
dicate that the different annotations are mutually-
dependent.
We are encouraged by the success of our joint
query annotation technique, and intend to pursue the
investigation of its utility for IR applications. In the
future, we intend to research the use of joint query
annotations for additional IR tasks, e.g., for con-
structing better query formulations for ranking al-
gorithms.
</bodyText>
<sectionHeader confidence="0.997474" genericHeader="conclusions">
8 Acknowledgment
</sectionHeader>
<bodyText confidence="0.999896166666667">
This work was supported in part by the Center for In-
telligent Information Retrieval and in part by ARRA
NSF IIS-9014442. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect those of the sponsor.
</bodyText>
<page confidence="0.99679">
110
</page>
<sectionHeader confidence="0.995848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999796255319149">
Niranjan Balasubramanian and James Allan. 2009. Syn-
tactic query models for restatement retrieval. In Proc.
of SPIRE, pages 143–155.
Cory Barr, Rosie Jones, and Moira Regelson. 2008. The
linguistic structure of english web-search queries. In
Proc. of EMNLP, pages 1021–1030.
Michael Bendersky and W. Bruce Croft. 2009. Analysis
of long queries in a large scale search log. In Proc. of
Workshop on Web Search Click Data, pages 8–14.
Michael Bendersky, David Smith, and W. Bruce Croft.
2009. Two-stage query segmentation for information
retrieval. In Proc. of SIGIR, pages 810–811.
Michael Bendersky, W. Bruce Croft, and David A. Smith.
2010. Structural annotation of search queries using
pseudo-relevance feedback. In Proc. of CIKM, pages
1537–1540.
Shane Bergsma and Qin I. Wang. 2007. Learning noun
phrase query segmentation. In Proc. ofEMNLP, pages
819–826.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Chris Buckley. 1995. Automatic query expansion using
SMART. In Proc. of TREC-3, pages 69–80.
Jenny R. Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc.
ofNAACL, pages 326–334.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Proc. of SIGIR, pages 379–386.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In Proc. of SIGIR,
pages 267–274.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Braeutigam. 2010. The power of naive query
segmentation. In Proc. of SIGIR, pages 797–798.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Br¨autigam. 2011. Query segmentation re-
visited. In Proc. of WWW, pages 97–106.
Rosie Jones and Daniel C. Fain. 2003. Query word dele-
tion prediction. In Proc. of SIGIR, pages 435–436.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proc. of WWW, pages 387–396.
Giridhar Kumaran and James Allan. 2007. A case for
shorter queries, and helping user create them. In Proc.
ofNAACL, pages 220–227.
Giridhar Kumaran and Vitor R. Carvalho. 2009. Re-
ducing long queries using query quality predictors. In
Proc. of SIGIR, pages 564–571.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proc. ofICML, pages 282–289.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance
based language models. In Proc. ofSIGIR, pages 120–
127.
Matthew Lease. 2007. Natural language processing for
information retrieval: the time is ripe (again). In Pro-
ceedings of PIKM.
Xiao Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proc. of ACL, pages 1337–
1345, Morristown, NJ, USA.
Rachel T. Lo, Ben He, and Iadh Ounis. 2005. Auto-
matically building a stopword list for an information
retrieval system. In Proc. ofDIR.
Yumao Lu, Fuchun Peng, Gilad Mishne, Xing Wei, and
Benoit Dumoulin. 2009. Improving Web search rel-
evance with semantic features. In Proc. of EMNLP,
pages 648–657.
Mehdi Manshadi and Xiao Li. 2009. Semantic Tagging
of Web Search Queries. In Proc. of ACL, pages 861–
869.
Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP, pages 157–166.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. ofACL, pages 950–958.
Marius Pas¸ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
CIKM, pages 683–690.
Dou Shen, Toby Walkery, Zijian Zhengy, Qiang Yangz,
and Ying Li. 2008. Personal name classification in
web queries. In Proc. of WSDM, pages 149–158.
Bin Tan and Fuchun Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In Proc. of WWW, pages 347–356.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34:161–191,
June.
Xing Wei, Fuchun Peng, and Benoit Dumoulin. 2008.
Analyzing web text association to disambiguate abbre-
viation in queries. In Proc. of SIGIR, pages 751–752.
</reference>
<page confidence="0.9988">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.238759">
<title confidence="0.999562">Joint Annotation of Search Queries</title>
<author confidence="0.996532">Michael</author>
<affiliation confidence="0.9998725">Dept. of Computer University of</affiliation>
<address confidence="0.557309">Amherst,</address>
<email confidence="0.999768">bemike@cs.umass.edu</email>
<author confidence="0.999724">W Bruce</author>
<affiliation confidence="0.999868">Dept. of Computer University of</affiliation>
<address confidence="0.550024">Amherst,</address>
<email confidence="0.999799">croft@cs.umass.edu</email>
<author confidence="0.943235">A David</author>
<affiliation confidence="0.99983">Dept. of Computer University of</affiliation>
<address confidence="0.841345">Amherst,</address>
<email confidence="0.999966">dasmith@cs.umass.edu</email>
<abstract confidence="0.999377217391304">Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Niranjan Balasubramanian</author>
<author>James Allan</author>
</authors>
<title>Syntactic query models for restatement retrieval.</title>
<date>2009</date>
<booktitle>In Proc. of SPIRE,</booktitle>
<pages>143--155</pages>
<contexts>
<context position="4552" citStr="Balasubramanian and Allan, 2009" startWordPosition="730" endWordPosition="733"> otherwise), POS tags (N – noun, V – verb, X – otherwise) and segmentation (B/I – beginning of/inside the chunk). They also tend to lack prepositions, proper punctuation, or capitalization, since users (often correctly) assume that these features are disregarded by the retrieval system. In this paper, we propose a novel joint query annotation method to improve the effectiveness of existing query annotations, especially for longer, more complex search queries. Most existing research focuses on using a single type of annotation for information retrieval such as subject-verb-object dependencies (Balasubramanian and Allan, 2009), namedentity recognition (Guo et al., 2009), phrase chunking (Guo et al., 2008), or semantic labeling (Li, 2010). In contrast, the main focus of this work is on developing a unified approach for performing reliable annotations of different types. To this end, we propose a probabilistic method for performing a joint query annotation. This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations. For instance, our method can leverage the information about estimated partsof-speech tags and capitalizatio</context>
</contexts>
<marker>Balasubramanian, Allan, 2009</marker>
<rawString>Niranjan Balasubramanian and James Allan. 2009. Syntactic query models for restatement retrieval. In Proc. of SPIRE, pages 143–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cory Barr</author>
<author>Rosie Jones</author>
<author>Moira Regelson</author>
</authors>
<title>The linguistic structure of english web-search queries.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1021--1030</pages>
<contexts>
<context position="2079" citStr="Barr et al., 2008" startWordPosition="304" endWordPosition="307"> roles is a common practice in natural language processing (NLP). It is, however, much less common in information retrieval (IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, tha</context>
<context position="5412" citStr="Barr et al., 2008" startWordPosition="871" endWordPosition="874">pes. To this end, we propose a probabilistic method for performing a joint query annotation. This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations. For instance, our method can leverage the information about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries. The rest of the paper is organized as follows. In Section 2 we demonstrate s</context>
<context position="7136" citStr="Barr et al., 2008" startWordPosition="1158" endWordPosition="1161">mple mark-up scheme, exemplified using three web search queries (as they appear in a search log): (a) who won the 2004 kentucky derby, (b) kindred where would i be, and (c) shih tzu health problems. In this scheme, each query is markedup using three annotations: capitalization, POS tags, and segmentation indicators. Note that all the query terms are non-capitalized, and no punctuation is provided by the user, which complicates the query annotation process. While the simple annotation described in Figure 1 can be done with a very high accuracy for standard document corpora, both previous work (Barr et al., 2008; Bergsma and Wang, 2007; Jones and Fain, 2003) and the experimental results in this paper indicate that it is challenging to perform well on queries. The queries in Figure 1 illustrate this point. Query (a) in Figure 1 is a wh-question, and it contains 103 a capitalized concept (“Kentucky Derby”), a single verb, and four segments. Query (b) is a combination of an artist name and a song title and should be interpreted as Kindred — “Where Would I Be”. Query (c) is a concatenation of two short noun phrases: “Shih Tzu” and “health problems”. 3 Joint Query Annotation Given a search query Q, which </context>
<context position="30513" citStr="Barr et al., 2008" startWordPosition="4957" endWordPosition="4960">y type to future work. 6.5 Additional Comparisons In order to further evaluate the proposed joint annotation method, j-PRF, in this section we compare its performance to other query annotation methods previously reported in the literature. Unfortunately, there is not much published work on query capitalization and query POS tagging that goes beyond the simple query-based methods described in SecF1 60 65 70 75 80 85 90 95 100 TAG SEG CAP 109 tion 4.1. The published work on the more advanced methods usually requires access to large amounts of proprietary user data such as query logs and clicks (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009). Therefore, in this section we focus on recent work on query segmentation (Bergsma and Wang, 2007; Hagen et al., 2010). We compare the segmentation effectiveness of our best performing method, j-PRF, to that of these query segmentation methods. The first method, SEG-1, was first proposed by Hagen et al. (2010). It is currently the most effective publicly disclosed unsupervised query segmentation method. SEG-1 method requires an access to a large web n-gram corpus (Brants and Franz, 2006). The optimal segmentation for query Q, SQ, is then obtained using 1: </context>
</contexts>
<marker>Barr, Jones, Regelson, 2008</marker>
<rawString>Cory Barr, Rosie Jones, and Moira Regelson. 2008. The linguistic structure of english web-search queries. In Proc. of EMNLP, pages 1021–1030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bendersky</author>
<author>W Bruce Croft</author>
</authors>
<title>Analysis of long queries in a large scale search log.</title>
<date>2009</date>
<booktitle>In Proc. of Workshop on Web Search Click Data,</booktitle>
<pages>8--14</pages>
<contexts>
<context position="2659" citStr="Bendersky and Croft (2009)" startWordPosition="401" endWordPosition="404">ora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically diverse. Some queries are keyword concatenations, some are semicomplete verbal phrases and some are wh-questions. It is essential for the search engine to correctly annotate the query structure, and the quality of these query annotations has been shown to be a crucial first step towards the development of reliable and robust query processing, representation and understanding algorithms (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009; Manshadi and Li, 2009; Li, 2010). However, in current query annotation systems, even</context>
</contexts>
<marker>Bendersky, Croft, 2009</marker>
<rawString>Michael Bendersky and W. Bruce Croft. 2009. Analysis of long queries in a large scale search log. In Proc. of Workshop on Web Search Click Data, pages 8–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bendersky</author>
<author>David Smith</author>
<author>W Bruce Croft</author>
</authors>
<title>Two-stage query segmentation for information retrieval.</title>
<date>2009</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>810--811</pages>
<contexts>
<context position="31661" citStr="Bendersky et al. (2009)" startWordPosition="5153" endWordPosition="5156">2006). The optimal segmentation for query Q, SQ, is then obtained using 1: S� Q = argmax |s||s|count(s), SESQ sES,|s|&gt;1 where SQ is the set of all possible query segmentations, S is a possible segmentation, s is a segment in S, and count(s) is the frequency of s in the web n-gram corpus. The second method, SEG-2, is based on a successful supervised segmentation method, which was first proposed by Bergsma and Wang (2007). SEG-2 employs a large set of features, and is pre-trained on the query collection described by Bergsma and Wang (2007). The features used by the SEG-2 method are described by Bendersky et al. (2009), and include, among others, n-gram frequencies in a sample of a query log, web corpus and Wikipedia titles. Table 3 demonstrates the comparison between the j-PRF, SEG-1 and SEG-2 methods. When compared to the SEG-1 baseline, j-PRF is significantly more effective, even though it only employs bigram counts (see Eq. 4), instead of the high-order n-grams used by SEG-1, for computing the score of a segmentation. This results underscores the benefit of joint annotation, which leverages capitalization and POS tagging to improve the quality of the segmentation. When compared to the SEG-2 baseline, j-</context>
</contexts>
<marker>Bendersky, Smith, Croft, 2009</marker>
<rawString>Michael Bendersky, David Smith, and W. Bruce Croft. 2009. Two-stage query segmentation for information retrieval. In Proc. of SIGIR, pages 810–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bendersky</author>
<author>W Bruce Croft</author>
<author>David A Smith</author>
</authors>
<title>Structural annotation of search queries using pseudo-relevance feedback.</title>
<date>2010</date>
<booktitle>In Proc. of CIKM,</booktitle>
<pages>1537--1540</pages>
<contexts>
<context position="2120" citStr="Bendersky et al., 2010" startWordPosition="312" endWordPosition="315">al language processing (NLP). It is, however, much less common in information retrieval (IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are gram</context>
<context position="5690" citStr="Bendersky et al., 2010" startWordPosition="919" endWordPosition="922">d can leverage the information about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries. The rest of the paper is organized as follows. In Section 2 we demonstrate several examples of annotated search queries. Then, in Section 3, we introduce our joint query annotation method. In Section 4 we describe two types of independent query annotations that are used as input for the joint query annotation. Section 5 details the related work and Sec</context>
<context position="12495" citStr="Bendersky et al., 2010" startWordPosition="2110" endWordPosition="2113">ffective, classifier is trained using the labels inferred by the first classifier as features. Stacked classifiers were recently shown to be an efficient and effective strategy for structured classification in NLP (Nivre and McDonald, 2008; Martins et al., 2008). 4 Independent Query Annotations While the joint annotation method proposed in Section 3 is general enough to be applied to any set of independent query annotations, in this work we focus on two previously proposed independent annotation methods based on either the query itself, or the top sentences retrieved in response to the query (Bendersky et al., 2010). The main benefits of these two annotation methods are that they can be easily implemented using standard software tools, do not require any labeled data, and provide reasonable annotation accuracy. Next, we briefly describe these two independent annotation methods. 4.1 Query-based estimation The most straightforward way to estimate the conditional probabilities in Eq. 1 is using the query itself. To make the estimation feasible, Bendersky et al. (2010) take a bag-of-words approach, and assume independence between both the query terms and the corresponding annotation symbols. Thus, the indepe</context>
<context position="14082" citStr="Bendersky et al. (2010)" startWordPosition="2365" endWordPosition="2368">matical query, it is hard to accurately estimate the conditional probability in Eq. 1 using the query terms alone. For instance, a keyword query hawaiian falls, which refers to a location, is inaccurately interpreted by a standard POS tagger as a noun-verb pair. On the other hand, given a sentence from a corpus that is relevant to the query such as “Hawaiian Falls is a family-friendly waterpark”, the word “falls” is correctly identified by a standard POS tagger as a proper noun. Accordingly, the document corpus can be bootstrapped in order to better estimate the query annotation. To this end, Bendersky et al. (2010) employ the pseudo-relevance feedback (PRF) — a method that has a long record of success in IR for tasks such as query expansion (Buckley, 1995; Lavrenko and Croft, 2001). In the most general form, given the set of all retrievable sentences r in the corpus C one can derive p(zQ|Q) = � p(zQ|r)p(r|Q). rEC Since for most sentences the conditional probability of relevance to the query p(r|Q) is vanish� z — � _ argmax p(ζi |qi). (3) QRY) ingly small, the above can be closely approximated 1http://crftagger.sourceforge.net/ 105 by considering only a set of sentences R, retrieved combines several inde</context>
<context position="16424" citStr="Bendersky et al. (2010)" startWordPosition="2742" endWordPosition="2745">initial rein Eq. 3, since here the annotation symbols are not finement has been performed). Such information is independent given the query Q. especially important for more verbose and gramatAccordingly, the PRF-based estimate for indepen- ically complex queries. In addition, while all the dent annotations in Eq. 1 is methods proposed by Guo et al. (2008) require large zQPRF) = argmax 1: H amounts of training data (thousands of training ex(ζ1,...,ζ-) rERiE(1,...,n) p(ζi|r)p(r|Q). amples), our joint annotation method can be effec(4) tively trained with a minimal human labeling effort Following Bendersky et al. (2010), an estimate of (several hundred training examples). p(ζi|r) is a smoothed estimator that combines the An additional research area which is relevant to information from the retrieved sentence r with the this paper is the work on joint structure modelinformation about unigrams (for capitalization and ing (Finkel and Manning, 2009; Toutanova et al., POS tagging) and bigrams (for segmentation) from 2008) and stacked classification (Nivre and Mca large n-gram corpus (Brants and Franz, 2006). Donald, 2008; Martins et al., 2008) in natural lan5 Related Work guage processing. These approaches have b</context>
</contexts>
<marker>Bendersky, Croft, Smith, 2010</marker>
<rawString>Michael Bendersky, W. Bruce Croft, and David A. Smith. 2010. Structural annotation of search queries using pseudo-relevance feedback. In Proc. of CIKM, pages 1537–1540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Qin I Wang</author>
</authors>
<title>Learning noun phrase query segmentation.</title>
<date>2007</date>
<booktitle>In Proc. ofEMNLP,</booktitle>
<pages>819--826</pages>
<contexts>
<context position="2060" citStr="Bergsma and Wang, 2007" startWordPosition="300" endWordPosition="303">ed entities, or semantic roles is a common practice in natural language processing (NLP). It is, however, much less common in information retrieval (IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) </context>
<context position="5436" citStr="Bergsma and Wang, 2007" startWordPosition="875" endWordPosition="878">e propose a probabilistic method for performing a joint query annotation. This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations. For instance, our method can leverage the information about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries. The rest of the paper is organized as follows. In Section 2 we demonstrate several examples of annot</context>
<context position="7160" citStr="Bergsma and Wang, 2007" startWordPosition="1162" endWordPosition="1165">, exemplified using three web search queries (as they appear in a search log): (a) who won the 2004 kentucky derby, (b) kindred where would i be, and (c) shih tzu health problems. In this scheme, each query is markedup using three annotations: capitalization, POS tags, and segmentation indicators. Note that all the query terms are non-capitalized, and no punctuation is provided by the user, which complicates the query annotation process. While the simple annotation described in Figure 1 can be done with a very high accuracy for standard document corpora, both previous work (Barr et al., 2008; Bergsma and Wang, 2007; Jones and Fain, 2003) and the experimental results in this paper indicate that it is challenging to perform well on queries. The queries in Figure 1 illustrate this point. Query (a) in Figure 1 is a wh-question, and it contains 103 a capitalized concept (“Kentucky Derby”), a single verb, and four segments. Query (b) is a combination of an artist name and a song title and should be interpreted as Kindred — “Where Would I Be”. Query (c) is a concatenation of two short noun phrases: “Shih Tzu” and “health problems”. 3 Joint Query Annotation Given a search query Q, which consists of a sequence o</context>
<context position="30648" citStr="Bergsma and Wang, 2007" startWordPosition="4981" endWordPosition="4984">s section we compare its performance to other query annotation methods previously reported in the literature. Unfortunately, there is not much published work on query capitalization and query POS tagging that goes beyond the simple query-based methods described in SecF1 60 65 70 75 80 85 90 95 100 TAG SEG CAP 109 tion 4.1. The published work on the more advanced methods usually requires access to large amounts of proprietary user data such as query logs and clicks (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009). Therefore, in this section we focus on recent work on query segmentation (Bergsma and Wang, 2007; Hagen et al., 2010). We compare the segmentation effectiveness of our best performing method, j-PRF, to that of these query segmentation methods. The first method, SEG-1, was first proposed by Hagen et al. (2010). It is currently the most effective publicly disclosed unsupervised query segmentation method. SEG-1 method requires an access to a large web n-gram corpus (Brants and Franz, 2006). The optimal segmentation for query Q, SQ, is then obtained using 1: S� Q = argmax |s||s|count(s), SESQ sES,|s|&gt;1 where SQ is the set of all possible query segmentations, S is a possible segmentation, s i</context>
</contexts>
<marker>Bergsma, Wang, 2007</marker>
<rawString>Shane Bergsma and Qin I. Wang. 2007. Learning noun phrase query segmentation. In Proc. ofEMNLP, pages 819–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1.</booktitle>
<contexts>
<context position="13246" citStr="Brants and Franz, 2006" startWordPosition="2226" endWordPosition="2229">t require any labeled data, and provide reasonable annotation accuracy. Next, we briefly describe these two independent annotation methods. 4.1 Query-based estimation The most straightforward way to estimate the conditional probabilities in Eq. 1 is using the query itself. To make the estimation feasible, Bendersky et al. (2010) take a bag-of-words approach, and assume independence between both the query terms and the corresponding annotation symbols. Thus, the indepentent annotations in Eq. 1 are given by ((1,...,(-) iE(1,...,n) Following Bendersky et al. (2010) we use a large n-gram corpus (Brants and Franz, 2006) to estimate p(ζi|qi) for annotating the query with capitalization and segmentation mark-up, and a standard POS tagger1 for part-of-speech tagging of the query. 4.2 PRF-based estimation Given a short, often ungrammatical query, it is hard to accurately estimate the conditional probability in Eq. 1 using the query terms alone. For instance, a keyword query hawaiian falls, which refers to a location, is inaccurately interpreted by a standard POS tagger as a noun-verb pair. On the other hand, given a sentence from a corpus that is relevant to the query such as “Hawaiian Falls is a family-friendly</context>
<context position="16916" citStr="Brants and Franz, 2006" startWordPosition="2818" endWordPosition="2821">, our joint annotation method can be effec(4) tively trained with a minimal human labeling effort Following Bendersky et al. (2010), an estimate of (several hundred training examples). p(ζi|r) is a smoothed estimator that combines the An additional research area which is relevant to information from the retrieved sentence r with the this paper is the work on joint structure modelinformation about unigrams (for capitalization and ing (Finkel and Manning, 2009; Toutanova et al., POS tagging) and bigrams (for segmentation) from 2008) and stacked classification (Nivre and Mca large n-gram corpus (Brants and Franz, 2006). Donald, 2008; Martins et al., 2008) in natural lan5 Related Work guage processing. These approaches have been In recent years, linguistic annotation of search shown to be successful for tasks such as parsing and queries has been receiving increasing attention as an named entity recognition in newswire data (Finkel important step toward better query processing and and Manning, 2009) or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in </context>
<context position="31043" citStr="Brants and Franz, 2006" startWordPosition="5042" endWordPosition="5045">ss to large amounts of proprietary user data such as query logs and clicks (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009). Therefore, in this section we focus on recent work on query segmentation (Bergsma and Wang, 2007; Hagen et al., 2010). We compare the segmentation effectiveness of our best performing method, j-PRF, to that of these query segmentation methods. The first method, SEG-1, was first proposed by Hagen et al. (2010). It is currently the most effective publicly disclosed unsupervised query segmentation method. SEG-1 method requires an access to a large web n-gram corpus (Brants and Franz, 2006). The optimal segmentation for query Q, SQ, is then obtained using 1: S� Q = argmax |s||s|count(s), SESQ sES,|s|&gt;1 where SQ is the set of all possible query segmentations, S is a possible segmentation, s is a segment in S, and count(s) is the frequency of s in the web n-gram corpus. The second method, SEG-2, is based on a successful supervised segmentation method, which was first proposed by Bergsma and Wang (2007). SEG-2 employs a large set of features, and is pre-trained on the query collection described by Bergsma and Wang (2007). The features used by the SEG-2 method are described by Bende</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
</authors>
<title>Automatic query expansion using SMART.</title>
<date>1995</date>
<booktitle>In Proc. of TREC-3,</booktitle>
<pages>69--80</pages>
<contexts>
<context position="14225" citStr="Buckley, 1995" startWordPosition="2392" endWordPosition="2393"> falls, which refers to a location, is inaccurately interpreted by a standard POS tagger as a noun-verb pair. On the other hand, given a sentence from a corpus that is relevant to the query such as “Hawaiian Falls is a family-friendly waterpark”, the word “falls” is correctly identified by a standard POS tagger as a proper noun. Accordingly, the document corpus can be bootstrapped in order to better estimate the query annotation. To this end, Bendersky et al. (2010) employ the pseudo-relevance feedback (PRF) — a method that has a long record of success in IR for tasks such as query expansion (Buckley, 1995; Lavrenko and Croft, 2001). In the most general form, given the set of all retrievable sentences r in the corpus C one can derive p(zQ|Q) = � p(zQ|r)p(r|Q). rEC Since for most sentences the conditional probability of relevance to the query p(r|Q) is vanish� z — � _ argmax p(ζi |qi). (3) QRY) ingly small, the above can be closely approximated 1http://crftagger.sourceforge.net/ 105 by considering only a set of sentences R, retrieved combines several independent annotations to imat top-k positions in response to the query Q. This prove the overall annotation accuracy. A similar apyields proach w</context>
</contexts>
<marker>Buckley, 1995</marker>
<rawString>Chris Buckley. 1995. Automatic query expansion using SMART. In Proc. of TREC-3, pages 69–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc. ofNAACL,</booktitle>
<pages>326--334</pages>
<contexts>
<context position="16755" citStr="Finkel and Manning, 2009" startWordPosition="2793" endWordPosition="2796">oposed by Guo et al. (2008) require large zQPRF) = argmax 1: H amounts of training data (thousands of training ex(ζ1,...,ζ-) rERiE(1,...,n) p(ζi|r)p(r|Q). amples), our joint annotation method can be effec(4) tively trained with a minimal human labeling effort Following Bendersky et al. (2010), an estimate of (several hundred training examples). p(ζi|r) is a smoothed estimator that combines the An additional research area which is relevant to information from the retrieved sentence r with the this paper is the work on joint structure modelinformation about unigrams (for capitalization and ing (Finkel and Manning, 2009; Toutanova et al., POS tagging) and bigrams (for segmentation) from 2008) and stacked classification (Nivre and Mca large n-gram corpus (Brants and Franz, 2006). Donald, 2008; Martins et al., 2008) in natural lan5 Related Work guage processing. These approaches have been In recent years, linguistic annotation of search shown to be successful for tasks such as parsing and queries has been receiving increasing attention as an named entity recognition in newswire data (Finkel important step toward better query processing and and Manning, 2009) or semantic role labeling in the understanding. The </context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny R. Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proc. ofNAACL, pages 326–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiafeng Guo</author>
<author>Gu Xu</author>
<author>Hang Li</author>
<author>Xueqi Cheng</author>
</authors>
<title>A unified and discriminative model for query refinement.</title>
<date>2008</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>379--386</pages>
<contexts>
<context position="3155" citStr="Guo et al., 2008" startWordPosition="478" endWordPosition="481"> trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically diverse. Some queries are keyword concatenations, some are semicomplete verbal phrases and some are wh-questions. It is essential for the search engine to correctly annotate the query structure, and the quality of these query annotations has been shown to be a crucial first step towards the development of reliable and robust query processing, representation and understanding algorithms (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009; Manshadi and Li, 2009; Li, 2010). However, in current query annotation systems, even sentence-like queries are often hard to parse and annotate, as they are prone to contain misspellings and idiosyncratic grammatical structures. 102 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102–111, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics (a) (b) (c) Term CAP TAG SEG Term CAP TAG SEG Term CAP TAG SEG who L X B kindred C N B shih C N B won L V I where C X B tzu C N I the L X B would C X I health</context>
<context position="4632" citStr="Guo et al., 2008" startWordPosition="744" endWordPosition="747">inside the chunk). They also tend to lack prepositions, proper punctuation, or capitalization, since users (often correctly) assume that these features are disregarded by the retrieval system. In this paper, we propose a novel joint query annotation method to improve the effectiveness of existing query annotations, especially for longer, more complex search queries. Most existing research focuses on using a single type of annotation for information retrieval such as subject-verb-object dependencies (Balasubramanian and Allan, 2009), namedentity recognition (Guo et al., 2009), phrase chunking (Guo et al., 2008), or semantic labeling (Li, 2010). In contrast, the main focus of this work is on developing a unified approach for performing reliable annotations of different types. To this end, we propose a probabilistic method for performing a joint query annotation. This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations. For instance, our method can leverage the information about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically e</context>
<context position="14866" citStr="Guo et al. (2008)" startWordPosition="2499" endWordPosition="2502">01). In the most general form, given the set of all retrievable sentences r in the corpus C one can derive p(zQ|Q) = � p(zQ|r)p(r|Q). rEC Since for most sentences the conditional probability of relevance to the query p(r|Q) is vanish� z — � _ argmax p(ζi |qi). (3) QRY) ingly small, the above can be closely approximated 1http://crftagger.sourceforge.net/ 105 by considering only a set of sentences R, retrieved combines several independent annotations to imat top-k positions in response to the query Q. This prove the overall annotation accuracy. A similar apyields proach was recently proposed by Guo et al. (2008). p(zQ|Q) ≈ 1: p(zQ|r)p(r|Q). There are several key differences, however, between rER the work presented here and their work. Intuitively, the equation above models the query as First, Guo et al. (2008) focus on query refinea mixture of top-k retrieved sentences, where each ment (spelling corrections, word splitting, etc.) of sentence is weighted by its relevance to the query. short keyword queries. Instead, we are interested Furthermore, to make the estimation of the condi- in annotation of queries of different types, includtional probability p(zQ|r) feasible, it is assumed that ing verbose n</context>
<context position="16158" citStr="Guo et al. (2008)" startWordPosition="2702" endWordPosition="2705">nce are in- is an overlap between query refinement and annotadependent, given a sentence r. Note that this as- tion, the focus of the latter is on providing linguistic sumption differs from the independence assumption information about existing queries (after initial rein Eq. 3, since here the annotation symbols are not finement has been performed). Such information is independent given the query Q. especially important for more verbose and gramatAccordingly, the PRF-based estimate for indepen- ically complex queries. In addition, while all the dent annotations in Eq. 1 is methods proposed by Guo et al. (2008) require large zQPRF) = argmax 1: H amounts of training data (thousands of training ex(ζ1,...,ζ-) rERiE(1,...,n) p(ζi|r)p(r|Q). amples), our joint annotation method can be effec(4) tively trained with a minimal human labeling effort Following Bendersky et al. (2010), an estimate of (several hundred training examples). p(ζi|r) is a smoothed estimator that combines the An additional research area which is relevant to information from the retrieved sentence r with the this paper is the work on joint structure modelinformation about unigrams (for capitalization and ing (Finkel and Manning, 2009; T</context>
<context position="17571" citStr="Guo et al., 2008" startWordPosition="2922" endWordPosition="2925">n natural lan5 Related Work guage processing. These approaches have been In recent years, linguistic annotation of search shown to be successful for tasks such as parsing and queries has been receiving increasing attention as an named entity recognition in newswire data (Finkel important step toward better query processing and and Manning, 2009) or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS</context>
<context position="30531" citStr="Guo et al., 2008" startWordPosition="4961" endWordPosition="4964">rk. 6.5 Additional Comparisons In order to further evaluate the proposed joint annotation method, j-PRF, in this section we compare its performance to other query annotation methods previously reported in the literature. Unfortunately, there is not much published work on query capitalization and query POS tagging that goes beyond the simple query-based methods described in SecF1 60 65 70 75 80 85 90 95 100 TAG SEG CAP 109 tion 4.1. The published work on the more advanced methods usually requires access to large amounts of proprietary user data such as query logs and clicks (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009). Therefore, in this section we focus on recent work on query segmentation (Bergsma and Wang, 2007; Hagen et al., 2010). We compare the segmentation effectiveness of our best performing method, j-PRF, to that of these query segmentation methods. The first method, SEG-1, was first proposed by Hagen et al. (2010). It is currently the most effective publicly disclosed unsupervised query segmentation method. SEG-1 method requires an access to a large web n-gram corpus (Brants and Franz, 2006). The optimal segmentation for query Q, SQ, is then obtained using 1: S� Q = argmax |s||</context>
</contexts>
<marker>Guo, Xu, Li, Cheng, 2008</marker>
<rawString>Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008. A unified and discriminative model for query refinement. In Proc. of SIGIR, pages 379–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiafeng Guo</author>
<author>Gu Xu</author>
<author>Xueqi Cheng</author>
<author>Hang Li</author>
</authors>
<title>Named entity recognition in query.</title>
<date>2009</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>267--274</pages>
<contexts>
<context position="3173" citStr="Guo et al., 2009" startWordPosition="482" endWordPosition="485">yntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically diverse. Some queries are keyword concatenations, some are semicomplete verbal phrases and some are wh-questions. It is essential for the search engine to correctly annotate the query structure, and the quality of these query annotations has been shown to be a crucial first step towards the development of reliable and robust query processing, representation and understanding algorithms (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009; Manshadi and Li, 2009; Li, 2010). However, in current query annotation systems, even sentence-like queries are often hard to parse and annotate, as they are prone to contain misspellings and idiosyncratic grammatical structures. 102 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102–111, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics (a) (b) (c) Term CAP TAG SEG Term CAP TAG SEG Term CAP TAG SEG who L X B kindred C N B shih C N B won L V I where C X B tzu C N I the L X B would C X I health L N B 2004 L X B </context>
<context position="4596" citStr="Guo et al., 2009" startWordPosition="737" endWordPosition="740">nd segmentation (B/I – beginning of/inside the chunk). They also tend to lack prepositions, proper punctuation, or capitalization, since users (often correctly) assume that these features are disregarded by the retrieval system. In this paper, we propose a novel joint query annotation method to improve the effectiveness of existing query annotations, especially for longer, more complex search queries. Most existing research focuses on using a single type of annotation for information retrieval such as subject-verb-object dependencies (Balasubramanian and Allan, 2009), namedentity recognition (Guo et al., 2009), phrase chunking (Guo et al., 2008), or semantic labeling (Li, 2010). In contrast, the main focus of this work is on developing a unified approach for performing reliable annotations of different types. To this end, we propose a probabilistic method for performing a joint query annotation. This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations. For instance, our method can leverage the information about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of </context>
<context position="17884" citStr="Guo et al., 2009" startWordPosition="2976" endWordPosition="2979"> processing and and Manning, 2009) or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which 106 6 Experiments 6.1 Experimental Setup For evaluating the performance of our query annotation methods, we use a random sample of 250 queries2 from a search log. This sample is manually labe</context>
<context position="30550" citStr="Guo et al., 2009" startWordPosition="4965" endWordPosition="4968"> Comparisons In order to further evaluate the proposed joint annotation method, j-PRF, in this section we compare its performance to other query annotation methods previously reported in the literature. Unfortunately, there is not much published work on query capitalization and query POS tagging that goes beyond the simple query-based methods described in SecF1 60 65 70 75 80 85 90 95 100 TAG SEG CAP 109 tion 4.1. The published work on the more advanced methods usually requires access to large amounts of proprietary user data such as query logs and clicks (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009). Therefore, in this section we focus on recent work on query segmentation (Bergsma and Wang, 2007; Hagen et al., 2010). We compare the segmentation effectiveness of our best performing method, j-PRF, to that of these query segmentation methods. The first method, SEG-1, was first proposed by Hagen et al. (2010). It is currently the most effective publicly disclosed unsupervised query segmentation method. SEG-1 method requires an access to a large web n-gram corpus (Brants and Franz, 2006). The optimal segmentation for query Q, SQ, is then obtained using 1: S� Q = argmax |s||s|count(s), SESQ sE</context>
</contexts>
<marker>Guo, Xu, Cheng, Li, 2009</marker>
<rawString>Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named entity recognition in query. In Proc. of SIGIR, pages 267–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Hagen</author>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Christof Braeutigam</author>
</authors>
<title>The power of naive query segmentation.</title>
<date>2010</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>797--798</pages>
<contexts>
<context position="30669" citStr="Hagen et al., 2010" startWordPosition="4985" endWordPosition="4988"> performance to other query annotation methods previously reported in the literature. Unfortunately, there is not much published work on query capitalization and query POS tagging that goes beyond the simple query-based methods described in SecF1 60 65 70 75 80 85 90 95 100 TAG SEG CAP 109 tion 4.1. The published work on the more advanced methods usually requires access to large amounts of proprietary user data such as query logs and clicks (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009). Therefore, in this section we focus on recent work on query segmentation (Bergsma and Wang, 2007; Hagen et al., 2010). We compare the segmentation effectiveness of our best performing method, j-PRF, to that of these query segmentation methods. The first method, SEG-1, was first proposed by Hagen et al. (2010). It is currently the most effective publicly disclosed unsupervised query segmentation method. SEG-1 method requires an access to a large web n-gram corpus (Brants and Franz, 2006). The optimal segmentation for query Q, SQ, is then obtained using 1: S� Q = argmax |s||s|count(s), SESQ sES,|s|&gt;1 where SQ is the set of all possible query segmentations, S is a possible segmentation, s is a segment in S, and</context>
</contexts>
<marker>Hagen, Potthast, Stein, Braeutigam, 2010</marker>
<rawString>Matthias Hagen, Martin Potthast, Benno Stein, and Christof Braeutigam. 2010. The power of naive query segmentation. In Proc. of SIGIR, pages 797–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Hagen</author>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Christof Br¨autigam</author>
</authors>
<title>Query segmentation revisited.</title>
<date>2011</date>
<booktitle>In Proc. of WWW,</booktitle>
<pages>97--106</pages>
<marker>Hagen, Potthast, Stein, Br¨autigam, 2011</marker>
<rawString>Matthias Hagen, Martin Potthast, Benno Stein, and Christof Br¨autigam. 2011. Query segmentation revisited. In Proc. of WWW, pages 97–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Daniel C Fain</author>
</authors>
<title>Query word deletion prediction.</title>
<date>2003</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>435--436</pages>
<contexts>
<context position="7183" citStr="Jones and Fain, 2003" startWordPosition="1166" endWordPosition="1169">e web search queries (as they appear in a search log): (a) who won the 2004 kentucky derby, (b) kindred where would i be, and (c) shih tzu health problems. In this scheme, each query is markedup using three annotations: capitalization, POS tags, and segmentation indicators. Note that all the query terms are non-capitalized, and no punctuation is provided by the user, which complicates the query annotation process. While the simple annotation described in Figure 1 can be done with a very high accuracy for standard document corpora, both previous work (Barr et al., 2008; Bergsma and Wang, 2007; Jones and Fain, 2003) and the experimental results in this paper indicate that it is challenging to perform well on queries. The queries in Figure 1 illustrate this point. Query (a) in Figure 1 is a wh-question, and it contains 103 a capitalized concept (“Kentucky Derby”), a single verb, and four segments. Query (b) is a combination of an artist name and a song title and should be interpreted as Kindred — “Where Would I Be”. Query (c) is a concatenation of two short noun phrases: “Shih Tzu” and “health problems”. 3 Joint Query Annotation Given a search query Q, which consists of a sequence of terms (q1, ... , qn),</context>
<context position="18046" citStr="Jones and Fain, 2003" startWordPosition="3004" endWordPosition="3007">va et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which 106 6 Experiments 6.1 Experimental Setup For evaluating the performance of our query annotation methods, we use a random sample of 250 queries2 from a search log. This sample is manually labeled with three annotations: capitalization, POS tags, and segmentation, according to the description of these annotations in Figure 1. In this set of 250 queries,</context>
</contexts>
<marker>Jones, Fain, 2003</marker>
<rawString>Rosie Jones and Daniel C. Fain. 2003. Query word deletion prediction. In Proc. of SIGIR, pages 435–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Benjamin Rey</author>
<author>Omid Madani</author>
<author>Wiley Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proc. of WWW,</booktitle>
<pages>387--396</pages>
<contexts>
<context position="17553" citStr="Jones et al., 2006" startWordPosition="2918" endWordPosition="2921">tins et al., 2008) in natural lan5 Related Work guage processing. These approaches have been In recent years, linguistic annotation of search shown to be successful for tasks such as parsing and queries has been receiving increasing attention as an named entity recognition in newswire data (Finkel important step toward better query processing and and Manning, 2009) or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., s</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proc. of WWW, pages 387–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>James Allan</author>
</authors>
<title>A case for shorter queries, and helping user create them.</title>
<date>2007</date>
<booktitle>In Proc. ofNAACL,</booktitle>
<pages>220--227</pages>
<contexts>
<context position="5715" citStr="Kumaran and Allan, 2007" startWordPosition="923" endWordPosition="926">mation about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries. The rest of the paper is organized as follows. In Section 2 we demonstrate several examples of annotated search queries. Then, in Section 3, we introduce our joint query annotation method. In Section 4 we describe two types of independent query annotations that are used as input for the joint query annotation. Section 5 details the related work and Section 6 presents the exper</context>
</contexts>
<marker>Kumaran, Allan, 2007</marker>
<rawString>Giridhar Kumaran and James Allan. 2007. A case for shorter queries, and helping user create them. In Proc. ofNAACL, pages 220–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>Vitor R Carvalho</author>
</authors>
<title>Reducing long queries using query quality predictors.</title>
<date>2009</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>564--571</pages>
<contexts>
<context position="5743" citStr="Kumaran and Carvalho, 2009" startWordPosition="927" endWordPosition="930">rtsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries. The rest of the paper is organized as follows. In Section 2 we demonstrate several examples of annotated search queries. Then, in Section 3, we introduce our joint query annotation method. In Section 4 we describe two types of independent query annotations that are used as input for the joint query annotation. Section 5 details the related work and Section 6 presents the experimental results. We draw the</context>
</contexts>
<marker>Kumaran, Carvalho, 2009</marker>
<rawString>Giridhar Kumaran and Vitor R. Carvalho. 2009. Reducing long queries using query quality predictors. In Proc. of SIGIR, pages 564–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ofICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="10445" citStr="Lafferty et al., 2001" startWordPosition="1752" endWordPosition="1755">he initial set of estimations is reasonably accurate, we can make the assumption that the annotations in the set Z∗(J) Q are independent given the initial estimates Z∗(I) Q , allowing us to separately optimize the probability of each annotation z∗(J) Q ∈ Z∗(J) Q : zQ = argmax p(zQ|Z∗(I) ∗(J) Q ). (2) ZQ From Eq. 2, it is evident that the joint annotation task becomes that of finding some optimal unobserved sequence (annotation z∗(J) Q ), given the observed sequences (independent annotation set Z∗(I) Q ). Accordingly, we can directly use a supervised sequential probabilistic model such as CRF (Lafferty et al., 2001) to find the optimal z∗(J) Q . In this CRF model, the optimal annotation z∗(J) Qis the label we are trying to predict, and the set of independent annotations Z∗(I) Q is used as the basis for the features used for prediction. Figure 2 outlines the algorithm for performing the joint query annotation. As input, the algorithm receives a training set of queries and their ground truth annotations. It then produces a set of independent annotation estimates, which are jointly used, together with the ground truth annotations, to learn a CRF model for each annotation type. Finally, these CRF models are </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ofICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance based language models.</title>
<date>2001</date>
<booktitle>In Proc. ofSIGIR,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="14252" citStr="Lavrenko and Croft, 2001" startWordPosition="2394" endWordPosition="2397">efers to a location, is inaccurately interpreted by a standard POS tagger as a noun-verb pair. On the other hand, given a sentence from a corpus that is relevant to the query such as “Hawaiian Falls is a family-friendly waterpark”, the word “falls” is correctly identified by a standard POS tagger as a proper noun. Accordingly, the document corpus can be bootstrapped in order to better estimate the query annotation. To this end, Bendersky et al. (2010) employ the pseudo-relevance feedback (PRF) — a method that has a long record of success in IR for tasks such as query expansion (Buckley, 1995; Lavrenko and Croft, 2001). In the most general form, given the set of all retrievable sentences r in the corpus C one can derive p(zQ|Q) = � p(zQ|r)p(r|Q). rEC Since for most sentences the conditional probability of relevance to the query p(r|Q) is vanish� z — � _ argmax p(ζi |qi). (3) QRY) ingly small, the above can be closely approximated 1http://crftagger.sourceforge.net/ 105 by considering only a set of sentences R, retrieved combines several independent annotations to imat top-k positions in response to the query Q. This prove the overall annotation accuracy. A similar apyields proach was recently proposed by Guo</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance based language models. In Proc. ofSIGIR, pages 120– 127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
</authors>
<title>Natural language processing for information retrieval: the time is ripe (again).</title>
<date>2007</date>
<booktitle>In Proceedings of PIKM.</booktitle>
<contexts>
<context position="5757" citStr="Lease, 2007" startWordPosition="931" endWordPosition="932">lization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries. The rest of the paper is organized as follows. In Section 2 we demonstrate several examples of annotated search queries. Then, in Section 3, we introduce our joint query annotation method. In Section 4 we describe two types of independent query annotations that are used as input for the joint query annotation. Section 5 details the related work and Section 6 presents the experimental results. We draw the conclusions f</context>
</contexts>
<marker>Lease, 2007</marker>
<rawString>Matthew Lease. 2007. Natural language processing for information retrieval: the time is ripe (again). In Proceedings of PIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
</authors>
<title>Understanding the semantic structure of noun phrase queries.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1337--1345</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2131" citStr="Li, 2010" startWordPosition="316" endWordPosition="317">NLP). It is, however, much less common in information retrieval (IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically d</context>
<context position="4665" citStr="Li, 2010" startWordPosition="751" endWordPosition="752"> prepositions, proper punctuation, or capitalization, since users (often correctly) assume that these features are disregarded by the retrieval system. In this paper, we propose a novel joint query annotation method to improve the effectiveness of existing query annotations, especially for longer, more complex search queries. Most existing research focuses on using a single type of annotation for information retrieval such as subject-verb-object dependencies (Balasubramanian and Allan, 2009), namedentity recognition (Guo et al., 2009), phrase chunking (Guo et al., 2008), or semantic labeling (Li, 2010). In contrast, the main focus of this work is on developing a unified approach for performing reliable annotations of different types. To this end, we propose a probabilistic method for performing a joint query annotation. This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations. For instance, our method can leverage the information about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotatio</context>
<context position="17841" citStr="Li, 2010" startWordPosition="2971" endWordPosition="2972">l important step toward better query processing and and Manning, 2009) or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which 106 6 Experiments 6.1 Experimental Setup For evaluating the performance of our query annotation methods, we use a random sample of 250 queries2 from</context>
</contexts>
<marker>Li, 2010</marker>
<rawString>Xiao Li. 2010. Understanding the semantic structure of noun phrase queries. In Proc. of ACL, pages 1337– 1345, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel T Lo</author>
<author>Ben He</author>
<author>Iadh Ounis</author>
</authors>
<title>Automatically building a stopword list for an information retrieval system.</title>
<date>2005</date>
<booktitle>In Proc. ofDIR.</booktitle>
<contexts>
<context position="18023" citStr="Lo et al., 2005" startWordPosition="3000" endWordPosition="3003">n corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which 106 6 Experiments 6.1 Experimental Setup For evaluating the performance of our query annotation methods, we use a random sample of 250 queries2 from a search log. This sample is manually labeled with three annotations: capitalization, POS tags, and segmentation, according to the description of these annotations in Figure 1. In t</context>
</contexts>
<marker>Lo, He, Ounis, 2005</marker>
<rawString>Rachel T. Lo, Ben He, and Iadh Ounis. 2005. Automatically building a stopword list for an information retrieval system. In Proc. ofDIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yumao Lu</author>
<author>Fuchun Peng</author>
<author>Gilad Mishne</author>
<author>Xing Wei</author>
<author>Benoit Dumoulin</author>
</authors>
<title>Improving Web search relevance with semantic features.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>648--657</pages>
<contexts>
<context position="2096" citStr="Lu et al., 2009" startWordPosition="308" endWordPosition="311">practice in natural language processing (NLP). It is, however, much less common in information retrieval (IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their b</context>
<context position="17901" citStr="Lu et al., 2009" startWordPosition="2980" endWordPosition="2983">d Manning, 2009) or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which 106 6 Experiments 6.1 Experimental Setup For evaluating the performance of our query annotation methods, we use a random sample of 250 queries2 from a search log. This sample is manually labeled with three an</context>
</contexts>
<marker>Lu, Peng, Mishne, Wei, Dumoulin, 2009</marker>
<rawString>Yumao Lu, Fuchun Peng, Gilad Mishne, Xing Wei, and Benoit Dumoulin. 2009. Improving Web search relevance with semantic features. In Proc. of EMNLP, pages 648–657.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Manshadi</author>
<author>Xiao Li</author>
</authors>
<title>Semantic Tagging of Web Search Queries.</title>
<date>2009</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>861--869</pages>
<contexts>
<context position="3196" citStr="Manshadi and Li, 2009" startWordPosition="486" endWordPosition="489">ent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically diverse. Some queries are keyword concatenations, some are semicomplete verbal phrases and some are wh-questions. It is essential for the search engine to correctly annotate the query structure, and the quality of these query annotations has been shown to be a crucial first step towards the development of reliable and robust query processing, representation and understanding algorithms (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009; Manshadi and Li, 2009; Li, 2010). However, in current query annotation systems, even sentence-like queries are often hard to parse and annotate, as they are prone to contain misspellings and idiosyncratic grammatical structures. 102 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102–111, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics (a) (b) (c) Term CAP TAG SEG Term CAP TAG SEG Term CAP TAG SEG who L X B kindred C N B shih C N B won L V I where C X B tzu C N I the L X B would C X I health L N B 2004 L X B i C X I problems L N I </context>
<context position="17830" citStr="Manshadi and Li, 2009" startWordPosition="2967" endWordPosition="2970">in newswire data (Finkel important step toward better query processing and and Manning, 2009) or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which 106 6 Experiments 6.1 Experimental Setup For evaluating the performance of our query annotation methods, we use a random sample of 250 qu</context>
</contexts>
<marker>Manshadi, Li, 2009</marker>
<rawString>Mehdi Manshadi and Xiao Li. 2009. Semantic Tagging of Web Search Queries. In Proc. of ACL, pages 861– 869.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>157--166</pages>
<contexts>
<context position="12134" citStr="Martins et al., 2008" startWordPosition="2048" endWordPosition="2052">d Z′ Qt as features. (6) Predict annotation z∗Q(h ), using CRF(zQt). (7) Z∗(J) Z∗(J) [ z∗(Jh) Qh Qh Qh . Output: Z∗(J) — predicted annotations for the held-out set of queries. Qh Figure 2: Algorithm for performing joint query annotation. Note that this formulation of joint query annotation can be viewed as a stacked classification, in which a second, more effective, classifier is trained using the labels inferred by the first classifier as features. Stacked classifiers were recently shown to be an efficient and effective strategy for structured classification in NLP (Nivre and McDonald, 2008; Martins et al., 2008). 4 Independent Query Annotations While the joint annotation method proposed in Section 3 is general enough to be applied to any set of independent query annotations, in this work we focus on two previously proposed independent annotation methods based on either the query itself, or the top sentences retrieved in response to the query (Bendersky et al., 2010). The main benefits of these two annotation methods are that they can be easily implemented using standard software tools, do not require any labeled data, and provide reasonable annotation accuracy. Next, we briefly describe these two ind</context>
<context position="16953" citStr="Martins et al., 2008" startWordPosition="2824" endWordPosition="2827">fec(4) tively trained with a minimal human labeling effort Following Bendersky et al. (2010), an estimate of (several hundred training examples). p(ζi|r) is a smoothed estimator that combines the An additional research area which is relevant to information from the retrieved sentence r with the this paper is the work on joint structure modelinformation about unigrams (for capitalization and ing (Finkel and Manning, 2009; Toutanova et al., POS tagging) and bigrams (for segmentation) from 2008) and stacked classification (Nivre and Mca large n-gram corpus (Brants and Franz, 2006). Donald, 2008; Martins et al., 2008) in natural lan5 Related Work guage processing. These approaches have been In recent years, linguistic annotation of search shown to be successful for tasks such as parsing and queries has been receiving increasing attention as an named entity recognition in newswire data (Finkel important step toward better query processing and and Manning, 2009) or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proc. of EMNLP, pages 157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>950--958</pages>
<contexts>
<context position="12111" citStr="Nivre and McDonald, 2008" startWordPosition="2044" endWordPosition="2047">t) using zQt as a label and Z′ Qt as features. (6) Predict annotation z∗Q(h ), using CRF(zQt). (7) Z∗(J) Z∗(J) [ z∗(Jh) Qh Qh Qh . Output: Z∗(J) — predicted annotations for the held-out set of queries. Qh Figure 2: Algorithm for performing joint query annotation. Note that this formulation of joint query annotation can be viewed as a stacked classification, in which a second, more effective, classifier is trained using the labels inferred by the first classifier as features. Stacked classifiers were recently shown to be an efficient and effective strategy for structured classification in NLP (Nivre and McDonald, 2008; Martins et al., 2008). 4 Independent Query Annotations While the joint annotation method proposed in Section 3 is general enough to be applied to any set of independent query annotations, in this work we focus on two previously proposed independent annotation methods based on either the query itself, or the top sentences retrieved in response to the query (Bendersky et al., 2010). The main benefits of these two annotation methods are that they can be easily implemented using standard software tools, do not require any labeled data, and provide reasonable annotation accuracy. Next, we briefly</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proc. ofACL, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In Proc. of CIKM,</booktitle>
<pages>683--690</pages>
<marker>Pas¸ca, 2007</marker>
<rawString>Marius Pas¸ca. 2007. Weakly-supervised discovery of named entities using web search queries. In Proc. of CIKM, pages 683–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Toby Walkery</author>
<author>Zijian Zhengy</author>
<author>Qiang Yangz</author>
<author>Ying Li</author>
</authors>
<title>Personal name classification in web queries.</title>
<date>2008</date>
<booktitle>In Proc. of WSDM,</booktitle>
<pages>149--158</pages>
<contexts>
<context position="17920" citStr="Shen et al., 2008" startWordPosition="2984" endWordPosition="2987">or semantic role labeling in the understanding. The literature on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which 106 6 Experiments 6.1 Experimental Setup For evaluating the performance of our query annotation methods, we use a random sample of 250 queries2 from a search log. This sample is manually labeled with three annotations: capitali</context>
</contexts>
<marker>Shen, Walkery, Zhengy, Yangz, Li, 2008</marker>
<rawString>Dou Shen, Toby Walkery, Zijian Zhengy, Qiang Yangz, and Ying Li. 2008. Personal name classification in web queries. In Proc. of WSDM, pages 149–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Tan</author>
<author>Fuchun Peng</author>
</authors>
<title>Unsupervised query segmentation using generative language models and Wikipedia.</title>
<date>2008</date>
<booktitle>In Proc. of WWW,</booktitle>
<pages>347--356</pages>
<contexts>
<context position="5456" citStr="Tan and Peng, 2008" startWordPosition="879" endWordPosition="882">c method for performing a joint query annotation. This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations. For instance, our method can leverage the information about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries. The rest of the paper is organized as follows. In Section 2 we demonstrate several examples of annotated search queries.</context>
</contexts>
<marker>Tan, Peng, 2008</marker>
<rawString>Bin Tan and Fuchun Peng. 2008. Unsupervised query segmentation using generative language models and Wikipedia. In Proc. of WWW, pages 347–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--161</pages>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34:161–191, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Wei</author>
<author>Fuchun Peng</author>
<author>Benoit Dumoulin</author>
</authors>
<title>Analyzing web text association to disambiguate abbreviation in queries.</title>
<date>2008</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>751--752</pages>
<contexts>
<context position="17983" citStr="Wei et al., 2008" startWordPosition="2993" endWordPosition="2996">on query annotation Penn Treebank and Brown corpus (Toutanova et al., includes query segmentation (Bergsma and Wang, 2008). Similarly to this work in NLP, we demon2007; Jones et al., 2006; Guo et al., 2008; Ha- strate that a joint approach for modeling the linguisgen et al., 2010; Hagen et al., 2011; Tan and Peng, tic query structure can also be beneficial for IR ap2008), part-of-speech and semantic tagging (Barr et plications. al., 2008; Manshadi and Li, 2009; Li, 2010), namedentity recognition (Guo et al., 2009; Lu et al., 2009; Shen et al., 2008; Pas¸ca, 2007), abbreviation disambiguation (Wei et al., 2008) and stopword detection (Lo et al., 2005; Jones and Fain, 2003). Most of the previous work on query annotation focuses on performing a particular annotation task (e.g., segmentation or POS tagging) in isolation. However, these annotations are often related, and thus we take a joint annotation approach, which 106 6 Experiments 6.1 Experimental Setup For evaluating the performance of our query annotation methods, we use a random sample of 250 queries2 from a search log. This sample is manually labeled with three annotations: capitalization, POS tags, and segmentation, according to the descriptio</context>
</contexts>
<marker>Wei, Peng, Dumoulin, 2008</marker>
<rawString>Xing Wei, Fuchun Peng, and Benoit Dumoulin. 2008. Analyzing web text association to disambiguate abbreviation in queries. In Proc. of SIGIR, pages 751–752.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>