<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.74841">
ON THE COMPLEXITY OF ID/LP PARSING1
</title>
<author confidence="0.953406">
G. Edward Barton, Jr.
</author>
<sectionHeader confidence="0.544202666666667" genericHeader="method">
MIT Artificial Intelligence Laboratory
545 Technology Square
Cambridge, MA 02139
</sectionHeader>
<bodyText confidence="0.9899785">
Modern linguistic theory attributes surface complexity to interacting subsystems of constraints. For
instance, the ID/LP grammar formalism separates constraints on immediate dominance from those on
linear order. An ID/LP parsing algorithm by Shieber shows how to use ID and LP constraints directly
in language processing, without expanding them into an intermediate context-free &amp;quot;object grammar&amp;quot;.
However, Shieber&apos;s purported runtime bound underestimates the difficulty of ID/LP parsing. ID/LP
parsing is actually NP-complete, and the worst-case runtime of Shieber&apos;s algorithm is actually expo-
nential in grammar size. The growth of parser data structures causes the difficulty. Some computa-
tional and linguistic implications follow; in particular, it is important to note that, despite its potential
for combinatorial explosion, Shieber&apos;s algorithm remains better than the alternative of parsing an
expanded object grammar.
</bodyText>
<sectionHeader confidence="0.99778" genericHeader="method">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999637421052632">
It is common in recent linguistic theories for various
surface characteristics of a language to be described in
terms of several different kinds of underlying constraints.
ID/LP grammars involve immediate-dominance rules and
linear-order constraints; more broadly, GPSG systems
can also involve feature relationships and metarules
(Gazdar et al. 1985). The tree adjunction grammars of
Kroch and Joshi (1985) separate the statement of local
constraints from the projection of those constraints to
larger structures. The GB-framework of Chomsky
(1981:5) identifies the subtheories of bounding, govern-
ment, 0-marking, binding, Case, and control. When
several independent constraints are involved, a system
that explicitly multiplies out their effects is large, cumber-
some, and uninformative.2 If done properly, the disen-
tanglement of different kinds of constraints can result in
shorter and more illuminating language descriptions.
With any such modular framework, two questions
immediately arise: how can the various constraints be
put back together in parsing, and what are the computa-
tional characteristics of the process? One approach is to
compile a large object grammar that expresses the
combined effects of the constraints in a more familiar
format such as an ordinary context-free grammar (CFG).
The context-free object grammar can then be parsed with
Earley&apos;s (1970) algorithm or any of several other well-
known procedures with known computational character-
istics.
However, in order to apply this method, it is necessary
to expand out the effects of everything that falls outside
the strict context-free format: rule schemas, metarules,
ID rules, LP constraints, feature instantiations, case-
marking constraints, etc. The standard algorithms oper-
ate on CFGs, not on extended variants of them.
Unfortunately, the object grammar may be huge after the
effects of all nonstandard devices have been expanded
out. Estimates of the object-grammar size for typical
systems vary from hundreds or thousands3 up to trillions
of rules (Shieber 1983:4). With some formalisms, the
context-free object-grammar approach is not even possi-
ble because the object grammar would be infinite (Shie-
ber 1985:145). Grammar size matters beyond questions
of elegance and clumsiness, for it typically affects proc-
essing complexity. Berwick and Weinberg (1982) argue
that the effects of grammar size can actually dominate
complexity for a relevant range of input lengths.
Given the disadvantages of multiplying out the effects
of separate systems of constraints, Shieber&apos;s (1983) work
on direct parsing leads in a welcome direction. Shieber
considers how one might do parsing with ID/LP gram-
mars, which involve two orthogonal kinds of rules. ID
rules constrain immediate dominance irrespective of
constituent order (&amp;quot;a sentence can be composed of V
with NP and SBAR complements&amp;quot;), while LP rules
Copyright 1 985 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that
the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy
otherwise, or to republish, requires a fee and/or specific permission.
</bodyText>
<page confidence="0.41141">
0362-613X/85/040205-218$03.00
</page>
<note confidence="0.614026">
Computational Linguistics, Volume 11, Number 4, October-December 1985 205
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<bodyText confidence="0.999830092105263">
constrain linear precedence among the daughters of any
node (&amp;quot;if V and SBAR are sisters, then V must precede
SBAR&amp;quot;). Shieber shows how Earley&apos;s (1970) algorithm
for parsing context-free grammars (CFGs) can be
adapted to use the constraints of ID/LP grammars direct-
ly, without the combinatorially explosive step of convert-
ing the ID/LP grammar into standard context-free form.
Instead of multiplying out all of the possible surface
interactions among the ID and LP rules, Shieber&apos;s algo-
rithm applies them one step at a time as needed. Surely
this should work better in a parsing application than
applying Earley&apos;s algorithm to an expanded grammar
with trillions of rules, since the worst-case time complexi-
ty of Earley&apos;s algorithm is proportional to the square of
the grammar size!
Shieber&apos;s general approach is on the right track. On
pain of having a large and cumbersome rule system, the
parser designer should first look to linguistics to find the
correct set of constraints on syntactic structure, then
discover how to apply some form of those constraints in
parsing without multiplying out all possible surface
manifestations of their effects.
Nonetheless, nagging doubts about computational
complexity remain. Although Shieber (1983:15) claims
that his algorithm is identical to Earley&apos;s in time complex-
ity, it seems almost too much to hope for that the size of
an ID/LP grammar should enter into the time complexity
of ID/LP parsing in exactly the same way that the size of
a CFG enters into the time complexity of CFG parsing.
An ID/LP grammar G can enjoy a huge size advantage
over a context-free grammar G&apos; for the same language;
for example, if G contains only the rule S ID abcde,
the corresponding G&apos; contains 5! = 120 rules. In effect,
the claim that Shieber&apos;s algorithm has the same time
complexity as Earley&apos;s algorithm means that this tremen-
dously increased brevity of expression comes free (up to
a constant). The paucity of supporting argument in
Shieber&apos;s article does little to allay these doubts:
We will not present a rigorous demonstration of time
complexity, but it should be clear from the close relation
between the presented algorithm and Earley&apos;s that the
complexity is that of Earley&apos;s algorithm. In the worst
case, where the LP rules always specify a unique ordering
for the right-hand side of every ID rule, the presented
algorithm reduces to Earley&apos;s algorithm. Since, given the
grammar, checking the LP rules takes constant time, the
time complexity of the presented algorithm is identical to
Earley&apos;s. . . . That is, it is 0(1 G 12 n3), where I G 1 is
the size of the grammar (number of ID rules) and n is the
length of the input. (:141)
Many questions remain; for example, why should a situ-
ation of maximal constraint represent the worst case, as
Shieber claimsr
The following sections will investigate the complexity
of ID/LP parsing in more detail. In brief, the outcome is
that Shieber&apos;s direct-parsing algorithm usually does have
a time advantage over the use of Earley&apos;s algorithm on
the expanded CFG, but that it blows up in the worst case.
The claim of 0(IG I 2 n3) time complexity is mistaken; in
fact, the worst-case time complexity of ID/LP parsing
cannot be bounded by any polynomial in the size of the
grammar and input, unless S&apos; = ./1.9. ID/LP parsing is
NP-complete.
As it turns out, the complexity of ID/LP parsing has its
source in the immediate-domination rules rather than the
linear precedence constraints. Consequently, the prece-
dence constraints will be neglected. Attention will be
focused on unordered context-free grammars (UCFGs),
which are exactly like standard context-free grammars
except that when a rule is used in a derivation, the
symbols on its right-hand side are considered to be unor-
dered and hence may be written in any order. UCFGs
represent the special case of ID/LP grammars in which
there are no LP constraints. Shieber&apos;s ID/LP algorithm
can be used to parse UCFGs simply by ignoring all refer-
ences to LP constraints.
</bodyText>
<sectionHeader confidence="0.990691" genericHeader="method">
2 GENERALIZING EARLEY&apos;S ALGORITHM
</sectionHeader>
<bodyText confidence="0.999164333333333">
Shieber generalizes Earley&apos;s algorithm by modifying the
progress datum that tracks progress through a rule. The
Earley algorithm uses the position of a dot to track .linear
advancement through an ordered sequence of constitu-
ents. The major predicates and operations on such
dotted rules are these:
</bodyText>
<listItem confidence="0.984342416666667">
• A dotted rule is initialized with the dot at the left edge,
as in X -* .ABC.
• A dotted rule is advanced across a terminal or nonter-
minal that was predicted and has been located in the
input by simply moving the dot to the right. For exam-
ple, X -4. A.BC is advanced across a B by moving the
dot to obtain X AB.0 .
• A dotted rule is complete iff the dot is at the right edge.
For example, X ABC. is complete.
• A dotted rule predicts a terminal or nonterminal iff the
dot is immediately before the terminal or nonterminal.
For example, X A.BC predicts B.
</listItem>
<bodyText confidence="0.99795825">
UCFG rules differ from CFG rules only in that the right-
hand sides represent unordered multisets (that is, sets
with repeated elements allowed). It is thus appropriate to
use successive accumulation of set elements in place of
linear advancement through a sequence. In essence,
Shieber&apos;s algorithm replaces the standard operations on
dotted rules with corresponding operations on what will
be called dotted UCFG rules.5
</bodyText>
<listItem confidence="0.86362975">
• A dotted UCFG rule is initialized with the empty multi-
set before the dot and the entire multiset of right-hand
elements after the dot, as in X -3. {} • {A, B, Cl.
• A dotted UCFG rule is advanced across a terminal or
nonterminal that was predicted and has been located in
the input by simply moving one element from the
multiset after the dot to the multiset before the dot.
For example, X -1. {A} • {B, C} is advanced across a B
</listItem>
<bodyText confidence="0.871669">
by moving the B to obtain X {A, B} • {C}. Similar-
</bodyText>
<page confidence="0.95974">
206 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<equation confidence="0.265924333333333">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
ly, X -•• {A } • {B, C, CI may be advanced across a C to
obtain X -* {A, C} {11, C}.
</equation>
<listItem confidence="0.9812008">
• A dotted UCFG rule is complete iff the multiset after
the dot is empty. For example, X {A, B, Cl {} is
complete.
• A dotted UCFG rule predicts a terminal or nonterminal
iff the terminal or nonterminal is a member of the
</listItem>
<bodyText confidence="0.991402454545455">
multiset after the dot. For example, X {A I {B,
predicts B and C.
Given these replacements for operations on dotted rules,
Shieber&apos;s algorithm operates in the same way as Earley&apos;s
algorithm. As usual, each state in the parser&apos;s state sets
consists of a dotted rule tracking progress through a
constituent plus the interword position defining the
constituent&apos;s left edge (Earley 1970:95, omitting looka-
head). The left-edge position is also referred to as the
return pointer because of its role in the complete opera-
tion of the parser.
</bodyText>
<sectionHeader confidence="0.998368" genericHeader="method">
3 THE ADVANTAGES OF SHIEBER&apos;S ALGORITHM
</sectionHeader>
<bodyText confidence="0.999909111111111">
The first question to ask is whether Shieber&apos;s algorithm
saves anything. Is it faster to use Shieber&apos;s algorithm on
a UCFG than to use Earley&apos;s algorithm on the corre-
sponding expanded CFG? Consider the UCFG Gi that
has only the single rule S abcde. The corresponding
CFG G&apos;i has 120 rules spelling out all the permutations
of abcde: S abcde, S abced, and so forth. If the
string abcde is parsed using Shieber&apos;s algorithm directly
on G1 the state sets of the parser remain sma11.6
</bodyText>
<equation confidence="0.9983665">
So : [S I • fa,b,c,d,e}, 0]
Si : [S {al • {b,c,d,e}, 0]
S2 : [S fa,bj ic,d,e1, 0]
S3 : [S {a,b,c} • fd,e1, 0]
: [S la,b,c,d1 • fel, 0]
S5 : [S {a,b,c,d,e} • [ 1, 0]
</equation>
<bodyText confidence="0.99976252631579">
In contrast, consider what happens if the same string is
parsed using Earley&apos;s algorithm on the expanded CFG
with its 120 rules. As Figure 1 illustrates, the state sets
of the Earley parser are much larger. In state set Si, the
Earley parser uses 4! = 24 states to spell out all the
possible orders in which the remaining symbols fb,c,d,e1
could appear. Shieber&apos;s modified parser does not spell
them out, but uses the single state [S {a} • {b,c,d,e}, 0]
to summarize them all. Shieber&apos;s algorithm should thus be
faster, since both parsers work by successively processing
all of the states in the state sets.
Similar examples show that the Shieber parser can
enjoy an arbitrarily large advantage over the use of the
Earley parser on the expanded CFG. Instead of multiply-
ing out all surface appearances ahead of time to produce
an expanded CFG, Shieber&apos;s algorithm works out the
possibilities one step at a time, as needed. This can be an
advantage because not all of the possibilities may arise
with a particular input.
</bodyText>
<equation confidence="0.996408230769231">
(a) [S {a} • {b,c,d,e}, 0]
(b) [S a.edcb, 0] [S a.ecbd, 0]
[S a.decb, 0] [S a.cebd, 0]
[S a.ecdb, 0] [S a.ebcd, 0]
[S a.cedb, 0] [S a.becd, 0]
[S a.dceb, 0] [S a.cbed, 0]
[S a.cdeb, 0] [S a.bced, 0]
[S a.edbc, 0] [S a.dcbe, 0]
[S a.debc, 0] [S a.cdbe, 0]
[S a.ebdc, 0] [S a.dbce, 0]
[S a.bedc, 0] [S a.bdce, 0]
[S a.dbec, 0] [S a.cbde, 0]
[S a.bdec, 0] [S a.bcde, 0]
</equation>
<figureCaption confidence="0.889234">
Figure 1. The use of the Shieber parser on a UCFG can
</figureCaption>
<bodyText confidence="0.936316666666667">
enjoy a large advantage over the use of the Earley parser
on the corresponding expanded CFG. After having proc-
essed the terminal a while parsing the string abcde as
discussed in the text, the Shieber parser uses the single
state shown in (a) to keep track of the same information
for which the Earley parser uses the 24 states in (b).
</bodyText>
<sectionHeader confidence="0.9954915" genericHeader="method">
4 COMBINATORIAL EXPLOSION WITH
SHIEBER&apos;S ALGORITHM
</sectionHeader>
<bodyText confidence="0.9975923">
The answer to the first question is yes, then: it can be
more efficient to use Shieber&apos;s parser than to use the
Earley parser on an expanded object grammar. The
second question to ask is whether Shieber&apos;s parser always
enjoys a large advantage. Does the algorithm blow up in
difficult cases?
In the presence of lexical ambiguity, Shieber&apos;s algo-
rithm can suffer from combinatorial explosion. Consider
the following UCFG, G2, in which x is five-ways ambig-
uous:
</bodyText>
<figure confidence="0.9222286">
S ABCDE
A a I x
B b I x
D d I x
• -&gt;elx
</figure>
<bodyText confidence="0.965785363636364">
What happens if Shieber&apos;s algorithm is used to parse the
string xxxxa according to this grammar? After the first
three occurrences of x have been processed, the state set
of Shieber&apos;s parser will reflect the possibility that any
three of the phrases A, B, C, D, and E might have been
encountered in the input and any two of them might
remain to be parsed. There will be (5) = 10 states
3
reflecting progress through the rule expanding S, in addi-
tion to 5 states reflecting phrase completion and 10 states
reflecting phrase prediction (not shown):
</bodyText>
<table confidence="0.942264833333334">
Computational Linguistics, Volume 11, Number 4, October-December 1985 207
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
S3: [S -a• {A,B,C} • [D,E1, 0]
[S {A,B,D} • [C,E}, 01
[S {A,C,D} • [B,E}, 0]
[S {B,C,D1 • {A,E}, 01
[S -4. {A,B,E} • {C,D}, 01
[S {A,C,E} • [B,D}, 0]
[S {B,C,E} • [A,D}, 0]
[S {A,D,E} • [B,C1, 01
[S [B,D,E1 • [A,C}, 0]
[S {C,D,E} {A,B}, 01
</table>
<bodyText confidence="0.9566045">
In cases like this, Shieber&apos;s algorithm enumerates all of
the combinations of k elements taken i at a time, where k
is the rule length and i is the number of elements already
processed. Thus it can be combinatorially explosive.
It is important to note that even in this case Shieber&apos;s
algorithm wins out over parsing the expanded CFG with
Earley&apos;s algorithm. After the same input symbols have
been processed, the state set of the Earley parser will
reflect the same possibilities as the state set of the Shie-
ber parser: any three of the required phrases might have
been located, while any two of them might remain to be
parsed. However, the Earley parser has a less concise
representation to work with. In place of the state involv-
ing S {A,B,C} {D,E}, for instance, there will be 3! •
2! = 12 states involving S ABC.DE, S BCA.ED,
and so forth. Instead of a total of 25 states, the Earley
state set will contain 135 = 12 10 + 15 states.7
In the above case, although the parser could not be
sure of the categorial identities of the phrases parsed, at
least there was no uncertainty about the number of
phrases and their extent. We can make matters even
worse for the parser by introducing uncertainty in those
areas as well. Let G3 be the result of replacing every x in
G2 with the empty string e:
</bodyText>
<figure confidence="0.8279425">
S A B CDE
A -&gt; a I e
B-bjr
C -4. c 1 E
D d I e
E e I c
</figure>
<bodyText confidence="0.999560928571429">
Then an A, for instance, can be either an a or nothing.
Before any input has been read, the first state set Sc, in
Shieber&apos;s parser must reflect the possibility that the
correct parse may include any of the 25 = 32 possible
subsets of A, B, C, D, and E as empty initial constituents.
For example, So must include [S {A,B,C,D,E} • { }, 0]
because the input might turn out to be the null string.
Similarly, it must include [S {A,C,E} • {B,D}, 01
because the input might turn out to be bd or db. Counting
all possible subsets in addition to other states having to
do with predictions, completions, and the parser&apos;s start
symbol, there are 44 states in S. (There are 338 states
in the corresponding state when the expanded CFG 3 is
used.)
</bodyText>
<sectionHeader confidence="0.895328" genericHeader="method">
5 THE SOURCE OF THE DIFFICULTY
</sectionHeader>
<bodyText confidence="0.999976125">
Why is Shieber&apos;s algorithm potentially exponential in
grammar size despite its &amp;quot;close relation&amp;quot; to Earley&apos;s algo-
rithm, which has time complexity polynomial in grammar
size? The answer lies in the size of the state space that
each parser uses. Relative to grammar size, Shieber&apos;s
algorithm involves a much larger bound than Earley&apos;s
algorithm on the number of states in a state set. Since
the main task of the Earley parser is to perform scan,
predict, and complete operations on the states in each
state set (Earley 1970:97), an explosion in the size of the
state sets will be fatal to any small runtime bound.
Given a CFG Ga, how many possible dotted rules are
there? Resulting from each rule X -* A, ... A„ there are
k+1 possible dotted rules. Then the number of possible
dotted rules is bounded by 1 GoI , if this notation is taken
to mean the number of symbols that it takes to write Go
down. An Earley state is a pair [r,i], where r is a dotted
rule and i is an interword position ranging from 0 to the
length n of the input string. Because of these limits, no
state set in the Earley parser can contain more than
0( 1 GoI-n) (distinct) states.
The limited size of a state set allows an 0(1 Ga I 2 • n3)
bound to be placed on the runtime of the Earley parser.
Informally, the argument (due to Earley) runs as follows.
The scan operation on a state can be done in constant
time; the scan operations in a state set thus contribute no
more than 0(1 GaI • n) computational steps. All of the
predict operations in a state set taken together can add no
more states than the number of rules in the grammar,
bounded by 1 Gal , since a nonterminal needs to be
expanded only once in a state set regardless of how many
times it is predicted; hence the predict operations need
not take more than 0( 1 GoI • n+ I Gal) = 0(1 G aI • n)
steps. Finally, there are the complete operations to be
considered. A given completion can do no worse than
advancing every state in the state set indicated by the
return pointer. Therefore, any bound k on state set size
leads to a bound of k2 on the number of steps it takes to
do all the completions in a state set. Here k =
0(1 GI •n), so the complete operations in a state set can
take at most 0(1 G I 2 • n2) steps. Overall, then, it takes
no more than 0(1 Ga I 2 n2) steps to process one state set
and no more than 0(1 GaI2 • n3) steps for the Earley
parser to process them all.
In Shieber&apos;s parser, though, the state sets can grow
much larger relative to grammar size. Given a UCFG G b,
how many possible dotted UCFG rules are there? Result-
ing from a rule X - A1 ... A, there are not k+1 possible
dotted rules tracking linear advancement, but 2k possible
dotted UCFG rules tracking accumulation of set elements.
In the worst case, the grammar contains only one rule
and k is on the order of I Gb I; hence the number of
possible dotted UCFG rules for the whole grammar is not
bounded by 1 GbI , but by 2 I Gb I . (The bound can be
reached; recall that exponentially many dotted rules are
created in the processing of G3 from section 4.)
</bodyText>
<page confidence="0.941995">
208 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<note confidence="0.538967">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<bodyText confidence="0.999914076923077">
Informally speaking, the reason why Shieber&apos;s parser
sometimes suffers from combinatorial explosion is that
there are exponentially more possible ways to progress
through an unordered rule expansion than an ordered
one. When disambiguating information is scarce, the
parser must keep track of all of them. In the more gener-
al task of parsing ID/LP grammars, the most tractable
case occurs when constraint from the LP relation is
strong enough to force a unique ordering for every rule
expansion. Under such conditions, Shieber&apos;s parser
reduces to Earley&apos;s. However, the case of strong
constraint represents the best case computationally, rather
than the worst case as Shieber (1983:14) claims.
</bodyText>
<sectionHeader confidence="0.995518" genericHeader="method">
6 ID/LP PARSING IS INHERENTLY DIFFICULT
</sectionHeader>
<bodyText confidence="0.999886166666667">
The worst-case time complexity of Shieber&apos;s algorithm is
exponential in grammar size rather than quadratic as
Shieber (1983:15) believed. Did Shieber simply choose a
poor algorithm, or is ID/LP parsing inherently difficult in
the general case? In fact, the simpler problem of recog-
nizing sentences according to a UCFG is NP-complete.8
Consequently, unless .9 = &lt;4;9, no algorithm for ID/LP
parsing can have a runtime bound that is polynomial in
the size of the grammar and input.
The proof of NP-completeness involves reducing the
vertex cover problem (Garey and Johnson 1979:46) to
the UCFG recognition problem. Through careful
construction of the grammar and input string, it is possi-
ble to &amp;quot;trick&amp;quot; the parser into solving a known hard prob-
lem. The vertex cover problem involves finding a small
set of vertices in a graph with the property that every
edge of the graph has at least one endpoint in the set.
Figure 2 shows a trivial example.
</bodyText>
<figure confidence="0.909837">
a
e3
</figure>
<figureCaption confidence="0.995858">
Figure 2. This graph illustrates a trivial instance of the
vertex cover problem. The set {c,c/} is a vertex cover of
size 2.
</figureCaption>
<bodyText confidence="0.9999485">
To construct a grammar that encodes the question of
whether the graph in Figure 2 has a vertex cover of size
2, first take the vertex names a, b, c, and d as the alpha-
bet. Take START as the start symbol. Take H1 through
H4 as special symbols, one per edge; also take U and D
as special dummy symbols.
Next, write the rules corresponding to the edges of the
graph. Edge el runs from a to c, so include the rules H,
-* a and H1 c. Encode the other edges similarly
Rules expanding the dummy symbols are also needed.
Dummy symbol D will be used to soak up excess input
symbols, so D -3. a through D d should be rules.
Dummy symbol U will also be used to soak up excess
input symbols, but U will be allowed to match only when
there are four occurrences in a row of the same symbol
(one occurrence for each edge). Take U aaaa, U
bbbb, U cccc, and U dddd as the rules expanding U.
Now, what does it take for the graph to have a vertex
cover of size k = 2? One way to get a vertex cover is to
go through the list of edges and underline one endpoint
of each edge. If the vertex cover is to be of size 2, the
underlining must be done in such a way that only two
distinct vertices are ever touched in the process. Alterna-
tively, since there are 4 vertices in all, the vertex cover
will be of size 2 if there are 4-2=2 vertices left untouched
in the underlining process. This method of finding a
vertex cover can be translated into a UCFG rule as
follows:
</bodyText>
<sectionHeader confidence="0.357907" genericHeader="method">
START 111112H3H4UUDDDD
</sectionHeader>
<bodyText confidence="0.935662555555556">
That is, each H-symbol is supposed to match the name of
one of the endpoints of the corresponding edge, in
accordance with the rules expanding the H-symbols.
Each U-symbol is supposed to correspond to a vertex
that was left untouched by the H-matching, and the
D-symbols are just there for bookkeeping. Figure 3 lists
the complete grammar that encodes the vertex-cover
problem of Figure 2.
START H1H2H3H4U UDDD D
</bodyText>
<figure confidence="0.730238333333333">
H1 a I c
H2 b I c
H3 c Id
H4 bid
aaaa I bbbb I cccc I dddd
-3. aibicid
</figure>
<figureCaption confidence="0.880078">
Figure 3. For k = 2, the construction described in the
text transforms the vertex-cover problem of Figure 2 into
this UCFG. A parse exists for the string
aaaabbbbccccdddd iff the graph in the previous figure has
a vertex cover of size &lt; 2.
</figureCaption>
<bodyText confidence="0.97099175">
To make all of this work properly, take
a = aaaabbbbccccdddd
as the input string to be parsed. (In general, for every
vertex name x, include in a a contiguous run of occur-
rences of x, one occurrence for each edge in the graph.)
The grammar encodes the underlining procedure by
requiring each H-symbol to match one of its endpoints in
a. Since the right-hand side of the START rule is unor-
dered, the grammar allows an H-symbol to match
anywhere in the input, hence to match any vertex name
(subject to interference from other rules that have
already matched). Furthermore, since there is one occur-
</bodyText>
<page confidence="0.733299">
Computational Linguistics, Volume 11, Number 4, October-December 1985
209
</page>
<note confidence="0.74527">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<bodyText confidence="0.999975485714286">
rence of each vertex name for every edge, all of the
edges could conceivably be matched up with the same
vertex; that is, it&apos;s impossible to run out of vertex-name
occurrences. Consequently, the grammar will allow
either endpoint of an edge to be &amp;quot;underlined&amp;quot;. The
parser will have to figure out which endpoints to choose
— in other words, which vertex cover to select. However,
the grammar also requires two occurrences of U to match
somewhere. U can only match four contiguous identical
input symbols that have not been matched in any other
way, and thus if the parser chooses a vertex cover that is
too large, the U-symbols will not match and the parse
will fail. The proper number of D-symbols is given by
the length of the input string, minus the number of edges
in the graph (to account for the H-matches), minus k
times the number of edges (to account for the
U-matches): in this case, 16 — 4 — (2.4) = 4, as illus-
trated in the START rule.
The net result of this construction is that in order to
decide whether a is in the language generated by the
UCFG, the parser must in effect search for a vertex cover
of size 2 or less.9 If a parse exists, an appropriate vertex
cover can be read off from beneath the H-symbols in the
parse tree; conversely, if an appropriate vertex cover
exists, it indicates how to construct a parse. Figure 4
shows the parse tree that encodes a solution to the
vertex-cover problem of Figure 2.
The construction shows that vertex-cover problem is
reducible to UCFG recognition. Furthermore, the
construction of the grammar and input string can be
carried out in polynomial time. Consequently, UCFG
recognition and the more general task of ID/LP parsing
must be computationally difficult. For a more careful
and detailed treatment of the reduction and its correct-
ness, see the appendix.
</bodyText>
<sectionHeader confidence="0.984854" genericHeader="method">
7 COMPUTATIONAL IMPLICATIONS
</sectionHeader>
<bodyText confidence="0.9988661">
The reduction of Vertex Cover shows that the ID/LP
parsing problem is NP-complete. Unless .9 = e/K1 the
time complexity of ID/LP parsing cannot be bounded by
any polynomial in the size of the grammar and input.10
An immediate conclusion is that complexity analysis must
be done carefully: despite its similarity to Earley&apos;s algo-
rithm, Shieber&apos;s algorithm does not have complexity
0(1 G 1 2 • n3). For some choices of grammar and input,
its internal structures undergo exponential growth. Other
consequences also follow.
</bodyText>
<subsectionHeader confidence="0.945039">
7.1 PARSING THE OBJECT GRAMMAR
</subsectionHeader>
<bodyText confidence="0.999908263157895">
Even in the face of its combinatorially explosive worst-
case behavior, Shieber&apos;s algorithm should not be imme-
diately cast aside. Despite the fact that it sometimes
blows up, it still has an advantage over the alternative of
parsing the expanded object grammar. One interpreta-
tion of the NP-completeness result is that the general
case of ID/LP parsing is inherently difficult; hence it
should not be surprising that Shieber&apos;s algorithm for solv-
ing that problem can sometimes suffer from combina-
torial explosion. More significant is the fact that parsing
with the expanded CFG blows up in cases that should not
be difficult. There is nothing inherently difficult about
parsing the language that consists of all permutations of
the string abcde, but while parsing that language the
Earley parser can use 24 states or more to encode what
the Shieber parser encodes in only one (section 3). To
put the point another way, the significant fact is not that
the Shieber parser can blow up; it is that the use of an
expanded CFG blows up unnecessarily.
</bodyText>
<figure confidence="0.827130666666667">
START
Hs Hs DH4 DDD
a a a a b b b b c d d dd
</figure>
<figureCaption confidence="0.647147333333333">
Figure 4. The grammar of Figure 3, which encodes the the vertex-cover problem of Figure 2, generates the string a =
aaaabbbbccccdddd according to this parse tree. The vertex cover ic,d1 can be read off from the parse tree as the set of
elements dominated by H-symbols.
</figureCaption>
<page confidence="0.90318">
210 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<note confidence="0.712249">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<subsectionHeader confidence="0.964704">
7.2 IS PRECOMPILATION POSSIBLE?
</subsectionHeader>
<bodyText confidence="0.999988659090909">
The present reduction of Vertex Cover to ID/LP Parsing
involves constructing a grammar and input string that
both depend on the problem to be solved. Consequently,
the reduction does not rule out the possibility that
through clever programming one might concentrate most
of the computational difficulty of ID/LP parsing into a
separate precompilation stage, dependent on the grammar
but independent of the input. According to this optimis-
tic scenario, the entire procedure of preprocessing the
grammar and parsing the input string would be as diffi-
cult as any NP-complete problem, but after precompila-
tion, the time required for parsing a particular input
would be bounded by a polynomial in grammar size and
sentence length.
Regarding the case immediately at hand, Shieber&apos;s
modified Earley algorithm has no precompilation step.11
The complexity result implied by the reduction thus
applies with full force; any possible precompilation phase
has yet to be proposed. Moreover, it is by no means
clear that a clever precompilation step is even possible; it
depends on exactly how I G I and n enter into the
complexity function for ID/LP parsing. If n enters as a
factor multiplying an exponential, precompilation cannot
help enough to ensure that the parsing phase will run in
polynomial time.
For example, suppose some parsing problem is known
to require 21 G I • n3 steps for solution.12 If one is willing to
spend, say, 10 • 21G1 steps in the precompilation phase, is
it possible to reduce parsing-phase complexity to some-
thing like I G I8 • n3? The answer is no. Since by
hypothesis it takes at least 21GI • n3 steps to solve the
problem, there must be at least 21 G I • n3 — 10 • 21 G I steps
left to perform after the precompilation phase. The
parameter n is necessarily absent from the precompilation
complexity, hence the term 21 G n3 will eventually domi-
nate.
In a related vein, suppose the precompilation step is
conversion from ID/LP to CFG form and the runtime
step is the use of the Earley parser on the expanded CFG.
Although the precompilation step does a potentially
exponential amount of work in producing G&apos; from G,
another exponential factor still shows up at runtime
because I G&apos; I in the complexity bound I G&apos; 12 • n3 is
exponentially larger than the original I G I .
</bodyText>
<subsectionHeader confidence="0.768646">
7.3 POLYNOMINAL-TIME PARSING
OF A FIXED GRAMMAR
</subsectionHeader>
<bodyText confidence="0.998129333333333">
As noted above, both grammar and input in the current
vertex-cover reduction depend on the vertex-cover prob-
lem to be solved. The NP-completeness result would be
strengthened if there were a reduction that used the same
fixed grammar for all vertex-cover problems, for it would
then be possible to prove that a precompilation phase
would be of little avail. However, unless .9 = it is
impossible to design such a reduction. Since grammar
size is not considered to be a parameter of a fixed-gram-
</bodyText>
<subsectionHeader confidence="0.597994">
Computational Linguistics, Volume 11, Number 4, October-December 1985
</subsectionHeader>
<bodyText confidence="0.999973348837209">
mar parsing problem, the use of the Earley parser on the
object grammar constitutes a polynomial-time algorithm
for solving the fixed-grammar ID/LP parsing problem.
Although ID/LP parsing for a fixed grammar can
therefore be done in cubic time, that fact represents little
more than an accounting trick. The object grammar G&apos;
corresponding to a practical ID/LP grammar would be
huge, and if I G&apos; I 2 • n3 complexity is too slow, then it
remains too slow when I G&apos; I 2 is regarded as a constant.
The practical irrelevance of polynomial-time parsing
for a fixed grammar sheds some light on another question
that is sometimes asked. Can&apos;t we have our cake and eat
it too by using the ID/LP grammar G directly when we
want to see linguistic generalizations, but parsing the
object grammar G&apos; when we want efficient parsing?
After all, the Earley algorithm runs in cubic time based
on the length of the input string, and its dependence on
grammar size is only I G&apos; I 2-
Essentially, the answer is that using the object gram-
mar doesn&apos;t help. The reduction shows that it&apos;s not
always easy to process the ID/LP form of the grammar,
but it is no easier to use the Earley algorithm on the
expanded form. As the examples that have been
presented clearly illustrate, both the Shieber parser and
the Earley parser for a given language can end up with
state sets that contain large numbers of elements. The
object grammar does not promote efficient processing;
the Shieber parser operating on the ID/LP grammar can
often do better than the Earley parser operating on the
object grammar, because of its more concise represen-
tation (section 4).
The Earley-algorithm grammar-size factor I G&apos; I 2
looks smaller than the Shieber factor 21G1 until one
recalls that G&apos; can be exponentially larger than G. In
other words, we can hide the factor 21G 1 inside I G&apos; I ,
but that doesn&apos;t make it any smaller. Thus parsing is
likely to take a great many steps even if we parse the
object grammar, which might mistakenly be thought to
be more efficient than direct parsing. If an algorithm
runs too slowly, it doesn&apos;t make it faster if we cover up
the exponential factor and make it a constant K, and it&apos;s
unlikely that ID/LP parsing can be done quickly in the
general case.
</bodyText>
<subsectionHeader confidence="0.972921">
7.4 THE POWER OF THE UCFG FORMALISM
</subsectionHeader>
<bodyText confidence="0.999964615384615">
The Vertex Cover reduction also helps pin down the
computational power of the UCFG formalism. As G1 and
G&apos;1 in section 3 illustrated, a UCFG (or an ID/LP gram-
mar) can enjoy considerable brevity of expression
compared to the equivalent CFG. The NP-completeness
result illuminates this property in two ways. First, the
result shows that this brevity of expression is sufficient to
allow an instance of any problem in A&apos;S?&apos; to be stated in a
UCFG that is only polynomially larger than the original
problem instance. In contrast, if an attempt is made to
replicate the current reduction with a CFG rather than
UCFG, the necessity of spelling out all the orders in
which the H-, U-, and D-symbols might appear makes
</bodyText>
<page confidence="0.982077">
211
</page>
<note confidence="0.456484">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<bodyText confidence="0.9999445">
the CFG more than polynomially larger than the problem
instance. Consequently, the reduction fails to establish
NP-completeness, which indeed does not hold. Second,
the result shows that the increased expressive power does
not come free; while the CFG recognition problem can be
solved in time 0( I G I 2&apos;n3) unless g = Atg, the general
UCFG recognition problem cannot be solved in polyno-
mial time.
The details of the reduction show how powerful a
single UCFG rule can be. If the UCFG formalism is
extended to permit ordinary CFG rules in addition to
rules with unordered expansions, the grammar that
expresses a vertex-cover problem needs only one UCFG
rule, although that rule may need to be arbitrarily long.&apos;3
</bodyText>
<subsectionHeader confidence="0.829375">
7.5 THE ROLE OF CONSTRAINT
</subsectionHeader>
<bodyText confidence="0.999729416666667">
Finally, the discussion of section 5 illustrates the way in
which the weakening of constraints can often make a
problem computationally more difficult. It might erro-
neously be thought that weak constraints represent the
best case in computational terms, for &amp;quot;weak&amp;quot; constraints
sound easy to verify. However, oftentimes the weaken-
ing of constraint multiplies the number of possibilities
that must be considered in the course of solving a prob-
lem. In the case at hand, the removal of constraints on
the order in which constituents can appear causes the
dependence of parsing complexity on grammar size to
grow from I G 2 to 216I.
</bodyText>
<sectionHeader confidence="0.973419" genericHeader="method">
8 LINGUISTIC IMPLICATIONS
</sectionHeader>
<bodyText confidence="0.999871105882354">
The key factors that cause difficulty in ID/LP parsing are
familiar to linguistic theory. GB-theory and GPSG both
permit the existence of constituents that are empty on the
surface, and thus in principle they both allow the kind of
pathology illustrated by G3 in section 4, subject to amel-
ioration by additional constraints. Similarly, every
current theory acknowledges lexical ambiguity, a key
ingredient of the vertex-cover reduction. Though the
reduction illuminates the power of certain mechanisms
and formal devices, the direct implications of the
NP-completeness result for grammatical theory are few.
The reduction does expose the weakness of attempts
to link context-free generative power directly to efficient
parsability. Consider, for instance, Gazdar&apos;s (1981:155)
claim that the use of a formalism with only context-free
power can help explain the rapidity of human sentence
processing:
Suppose . . . that the permitted class of generative gram-
mars constituted a subset of those phrase structure gram-
mars capable only of generating context-free languages.
Such a move would have two important metatheoretical
consequences, one having to do with learnability, the
other with processability . . . . We would have the begin-
nings of an explanation for the obvious, but largely
ignored, fact that humans process the utterances they
hear very rapidly. Sentences of a context-free language
are provably parsable in a time which is proportional to
the Cube of the length of the sentence or less.
As the arguments and examples in this paper have illus-
trated, context-free generative power does not guarantee
efficient parsability. Every ID/LP grammar technically
generates a context-free language, but the potentially
large size of the corresponding CFG means that we can&apos;t
count on that fact to give us efficient parsing. Thus it is
impossible to sustain this particular argument for the
advantages of such formalisms as (early) GPSG over
other linguistic theories; instead, GPSG and other modern
theories seem to be (very roughly) in the same boat with
respect to complexity. In such a situation, the linguistic
merits of various theories are more important than
complexity results. (See Berwick (1982), Berwick and
Weinberg (1982, 1984), and Ristad (1985) for further
discussion.)
The reduction does not rule out the use of formalisms
that decouple ID and LP constraints; note that Shieber&apos;s
direct parsing algorithm wins out over the use of the
object grammar However, if we assume that natural
languages are efficiently parsable (EP), then computa-
tional difficulties in parsing a formalism do indicate that
the formalism itself does not tell the whole story. That is,
they point out that the range of possible languages has
been incorrectly characterized: the additional constraints
that guarantee efficient parsability remain unstated.
Since the general case of parsing ID/LP grammars is
computationally difficult, if the linguistically relevant
ID/LP grammars are to be efficiently parsable, there
must be additional factors that guarantee a certain
amount of constraint from some source.&apos;4 (Constraints
beyond the bare ID/LP formalism are required on linguis-
tic grounds as well.) Note that the subset principle of
language acquisition (cf. Berwick and Weinberg
1984:233) would lead the language learner to initially
hypothesize strong order constraints, to be weakened
only in response to positive evidence.
However, there are other potential ways to guarantee
efficient parsability. It might turn out that the principles
and parameters of the best grammatical theory permit
languages that are not efficiently parsable in the worst
case — just as grammatical theory permits sentences that
are deeply center-embedded (Miller and Chomsky
1963).15 In such a situation, difficult languages or
sentences would not be expected to turn up in general
use, precisely because they would be difficult to process.16
The factors that guarantee efficient parsability would not
be part of grammatical theory because they would result
from extragrammatical factors, i.e. the resource limita-
tions of the language-processing mechanisms. This &amp;quot;easy
way out&amp;quot; is not automatically available, depending as it
does on a detailed account of processing mechanisms.
For example, in the Earley parser, the difficulty of pars-
ing a construction can vary widely with the amount of
lookahead used (if any). Like any other theory, an
explanation based on resource limitations must make the
right predictions about which constructions will be diffi-
cult to parse.
</bodyText>
<page confidence="0.956823">
212 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<note confidence="0.750482">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<bodyText confidence="0.999681454545455">
In the same way, the language-acquisition procedure
could potentially be the source of some constraints rele-
vant to efficient parsability. Perhaps not all of the
languages permitted by the principles and parameters of
syntactic theory are accessible in the sense that they can
potentially be constructed by the language-acquisition
component. It is to be expected that language-acquisi-
tion mechanisms will be subject to various kinds of limi-
tations just as all other mental mechanisms are. Again,
however, concrete conclusions must await a detailed
proposal.
</bodyText>
<sectionHeader confidence="0.916349" genericHeader="conclusions">
9 APPENDIX
</sectionHeader>
<bodyText confidence="0.999054333333333">
This appendix contains the details of a more careful
reduction of the vertex-cover problem to the UCFG
recognition problem. This version of the reduction
establishes that the difficulty of UCFG recognition is not
due either to the possibility of empty constituents
(E-rules) or to the possibility of repeated symbols in rules
(i.e., to the use of multisets rather than sets). Conse-
quently, it is somewhat different from and more complex
than the one sketched in the text.
</bodyText>
<subsectionHeader confidence="0.748123">
9.1 DEFINING UNORDERED CONTEXT-FREE GRAMMARS
</subsectionHeader>
<figure confidence="0.930717285714286">
Definition: An unordered CFG (UCFG) is a quadruple (N,
2, R, 5), where
(a) N is a finite set of nonterminals.
(b) E disjoint from N is a finite, nonempty set of terminal
symbols.
(c) R is a nonempty set of rules (A, a), where A E N and
a E (N U E )*. The rule (A, a) may be written as A
a.
(d) S c N is the start symbol.
Convention: The grammar G and its components N,1, R,
S need not be explicitly mentioned when clear from
context.
Convention: Unless otherwise noted,
(a) A, A&apos;, At, ... denote elements of N;
</figure>
<listItem confidence="0.95845975">
(b) a, a&apos;, at ... denote elements of 2;
(c) X, Y, X&apos;, Y&apos;, X,, Y, ... denote elements of N U E;
(d) a, u, u&apos;, u,, ... denote elements of E*;
(e) a, 0, y, denote elements of (NU E)*.
</listItem>
<construct confidence="0.872728888888889">
Definition G = (N, 2, R, S) is E-free iff for every (A, a)
E R, I a I 0 0.
Definition: G = (N, 2, R, S) is branching iff for some (A,
a) E R, I al &gt; 1.
Definition: G = (N, 2, R, S) is duplicate-free iff for
every (A, a) E R, a = and for all ij E [1,n], Y,
= Y iff i=j.
Definition: G = (N, 2, R, S) is simple iff it is E-free,
duplicate-free, and branching.
</construct>
<bodyText confidence="0.610239">
Note. The notion of a simple UCFG is introduced in
order to help pin down the source of any computational
</bodyText>
<subsectionHeader confidence="0.693609">
Computational Linguistics, Volume 11, Number 4, October-December 1985
</subsectionHeader>
<bodyText confidence="0.7443312">
difficulties associated with UCFGs. For example, since
simple UCFGs are restricted to be duplicate-free, a diffi-
culty that arises with simple UCFGs cannot result from
the possibility that a symbol may occur more than once
on the right-hand side of a rule.
Definition: 16,44)t4)a4) (by r) just in case (for some) r =
(A&apos;, Y1 ... E R and for some permutation p of [1,n], A
= A&apos; and a = y,(1) Y. If E E*, also write
cbAtp./m 04.
Definition: L(G) = fa E 2*: S cy}
</bodyText>
<subsectionHeader confidence="0.194268">
Definition: An n-step derivation of Ap from is a sequence
</subsectionHeader>
<bodyText confidence="0.762604666666667">
ct),,) such that 00, =4 = and for all i E [0,
n-1], (pi If it is also true for all i that 0, lm
say that the derivation is leftmost.
</bodyText>
<subsectionHeader confidence="0.792157">
9.2 DEFINING THE COMPUTATIONAL PROBLEMS
</subsectionHeader>
<construct confidence="0.692861875">
Definition: A possible instance of the problem VERTEX
COVER is a triple ( V,E,k), where (V,E.) is a finite graph
with at least one edge and at least two vertices, k E N,
and k &lt; I V1.17 VERTEX COVER itself consists of all
possible instances (V,E,k) such that for some V g v,
1 V I &lt; k and for all edges e E, at least one endpoint
of e is in V. (Figure 5 gives an example of a VERTEX
COVER instance.)
</construct>
<equation confidence="0.989565333333333">
v= [ v, w,x,y,z1
E= feeeeef
1, 2, 3, 4, 5&apos; ee 6&apos; 7
</equation>
<bodyText confidence="0.8051475">
with the e as indicated
k= 3
</bodyText>
<figureCaption confidence="0.925182">
Figure 5. The triple ( V,E,k) is an instance of VERTEX
</figureCaption>
<footnote confidence="0.834099571428571">
COVER. The set I/ = {v,x,z,pf;} is a vertex cover of size
k=3.
Fact: VERTEX COVER is NP-complete. (Garey and
Johnson 1979:46)
Definition: A possible instance of the problem SIMPLE
UCFG RECOGNITION is a pair (G, a), where G is a
simple UCFG and a E 2*. SIMPLE UCFG
</footnote>
<page confidence="0.986793">
213
</page>
<note confidence="0.540905">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<bodyText confidence="0.960696333333333">
RECOGNITION itself consists of all possible instances (G,
a) such that a E L(G).
Notation: Take II II to be any reasonable measure of the
encoded input length for a computational problem;
continue to use I • I for set cardinality and string length.
It is reasonable to require that if S is a set, k E N, and
ISI&gt; k, then II S II &gt; II k ; that is, the encoding of
numbers is better than unary. It is also reasonable to
require that (..., x, &gt;11 &gt; Ilx 11 .
</bodyText>
<subsectionHeader confidence="0.994912">
9.3 THE UCFG RECOGNITION PROBLEM IS IN NP
</subsectionHeader>
<bodyText confidence="0.8424524">
Lemma 9.1: Let (00, ..., ok) be a shortest leftmost deri-
vation of ok from 00 in a branching c-free UCFG. If k &gt;
IN +1 then I Oki &gt; I 001 •
Proof. There exists some sequence of rules (A0, ad ...
k-1, ak_i) such that for all i E [0, k-1], lm c51+1 by
a,). Since G is c-free, Ic5,-FiI I0, I always.
Case]. For some i, 1a11 &gt; 1. Then I+I &gt; I 0, I .
Hence I Oki &lt; I 001 •
Case 2. For every i, I a, I = 1. Then there exist u, Y
such that for every i E [0, k-2], there is A&apos; E N such
that 0,+1 = u A&apos; ,y. Suppose the A&apos;, are all distinct. Then
I NI &gt; k-1, hence I NI +1 &gt; k, hence I NI +1 &gt;
I NI +1, which is impossible. Hence for some ij [0,
k-2], i &lt;I, A&apos;, = 1. Hence = j+i, since [1,1] has
only one permutation. Then 00, ..., çt,,op&amp;quot;, Ok) is a
leftmost derivation of ok from 00 and has length less than
k, which is also impossible.
Then I4kI &gt; k01.El
Corollary 9.2: If G is a branching c-free UCFG and a E
MG), then a has a leftmost derivation of length at most
lal •mwherem= INI+2.
Proof. Let (cio, ok) be a shortest leftmost derivation
of a from S. Suppose k &gt; IaI • m. Consider the sub-
derivations
(c60, , 4).)(o., ,
</bodyText>
<equation confidence="0.8319548">
(4)(101-U.m, ••• ••• ,(15k)•
Each one except the last has m steps and m &gt; INI +1.
Then by lemma,
I irk a .rn I &gt; I c6•( )•rn I
&gt; • • • &gt; Om I &gt; I (1,0 I = 1.
</equation>
<bodyText confidence="0.935925">
Then I a I &gt; 1+ I a I , which is impossible. Hence k &lt;
la 1 m.
Lemma 9.3: II = SIMPLE UCFG RECOGNITION is in
the computational class .ArY.
Proof. Let G = (N, E, R, 5) be a simple UCFG and a E
E*. Consider the following nondeterministic algorithm
with input (G, a):
Step 1. Write down 00 = S.
Step 2. Perform the following steps for i from 0 to I a I •
m-1, where m = INI +2.
</bodyText>
<listItem confidence="0.892836142857143">
(a) Express 0, as u,A,y, by finding the leftmost nontermi-
nal, or loop if impossible.
(b) Guess a rule (A, Y„) E R and a permutation
p, of [1,k1], or loop if there is no such rule.
(c) Write down 0,4.1 = u, Y„ .. • 1c, P1(k) Y, -
(d) If 0,4_1 = a then halt.
Step 3. Loop.
</listItem>
<bodyText confidence="0.998957769230769">
It should be apparent that the algorithm runs in time at
worst polynomial in 11 (G, a&gt; ii; note that the length of 0,
increases by at most a constant amount on each iteration.
Assume (G, a) c H. Then a has a leftmost derivation of
length at most IaI • m by Corollary 9.2; hence the
nondeterministic algorithm will be able to guess it and
will halt. Conversely, suppose the algorithm halts on
input (G, a). On the iteration when the algorithm halts,
the sequence (sbo, • • • , 0,+1) will constitute a leftmost deri-
vation of a from S; hence a E L(G) and ( G, a) E 11.
Then there is a nondeterministic algorithm that runs in
polynomial time and accepts exactly H. Hence
H E
</bodyText>
<subsectionHeader confidence="0.993081">
9.4 THE UCFG RECOGNITION PROBLEM IS NP-COMPLETE
</subsectionHeader>
<bodyText confidence="0.9985065">
Lemma 9.4: Let ( V,E,k) = (V,{e,}, k) be a possible
instance of VERTEX COVER. Then it is possible to
construct, in time polynomial in ii Vii, 11E11 , and k, a
simple UCFG G(V,E,k) and a string a(V,E,k) such that
</bodyText>
<equation confidence="0.6888775">
(G(V,E,k), Cr(V,E,k)) E SIMPLE UCFG RECOGNITION
iff (V,E,k) E VERTEX COVER.
</equation>
<bodyText confidence="0.988158555555556">
Proof. Construct G(V,E,k) as follows. Let the set N of
nonterminals consist of the following symbols not in V:
START, U, D,
H, for i E [1, 1E1 ],
1.1, for i E [1, I VI -1c],
D, for i E [1, 1E1 &apos;(k-i)].
ii Nil will be at worst polynomial in II Eli, ii Vii, and k for
a reasonable length measure. Define the terminal vocab-
ulary E to consist of subscripted symbols as follows:
</bodyText>
<equation confidence="0.847997">
E = fa, : a E V,i€ [1, I E ]i•
</equation>
<bodyText confidence="0.997984">
Designate START as the start symbol. Include the
following as members of the rule set R:
</bodyText>
<page confidence="0.965309">
214 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<figure confidence="0.438067916666667">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
(a) Include the rule
START ... H
• D
(b) For each e, E E, include the rules
{li, -* a,: a an endpoint of e,}.
(c) For each i c [1, I VI -k], include the rule U, -■ U.
Also include the rules
U al ... alEi : a E V).
(d) For each i E [1, I El • (k-l.)], include the rule Di
D. Also include the rules
{D -* a : a e El.
</figure>
<bodyText confidence="0.996534">
Take G(V,E,k) to be (N, E, R, START). (Figure 6 shows
the results of applying this construction to the VERTEX
COVER instance of Figure 5.)
Let h : [1, 1 VI]-* V be some standard enumeration of
the elements of V. Construct a(V,E,k) as h(1)1
</bodyText>
<equation confidence="0.782612">
h(1)1E1 h(1 V1)1 • • h(1 VI) 1E1 thus a(V,E,k) will
have length IEI I VI.
</equation>
<bodyText confidence="0.999853305555556">
It is easy to see that 0 KG(V,E,k), cr(V,E,k)&gt;I1 will, be at
worst polynomial in 0 Eli, II Vii, and k for reasonable
II • II . It will also be possible to construct the grammar and
string in polynomial time. Finally, note that given the
definition of a possible instance of VERTEX COVER, the
grammar will be branching, E-free, and duplicate-free,
hence simple.
Now suppose (V,E,k) E VERTEX COVER. Then there
exist V g. V and f : E V such that IVI &lt; k and for
every e c E, f(e) is an endpoint of e. E is nonempty by
hypothesis and V must hit every edge, hence I V I
cannot be zero. Construct a parse tree for a (V,E,k)
according to G(V,E,k) as follows.
Step I. Number the elements of V — Vi as ix, : i E [1,
1 v—Vi 1 11. For each x, where i &lt; IVI-k, construct a
node dominating the substring (x,)1 (x1)151 of a(V,E,k)
and label it U. Then construct a node dominating only
the U-node and label it U,. Note that the available
symbols U, are numbered from 1 to I VI -k, so it is
impossible to run out of U-symbols. Also, IVI &lt;k and
k, so
all of the U-symbols will be used. Finally, note that U
al ... al E is a rule for any a E S and that U, U is a
rule for any U,.
Step 2. For each e, E E, construct a node dominating the
(unique) occurrence of f(e,), E a(V,E,k) and label it H..
Step 2 cannot conflict with step 1 because f(e1) E
hence f(e1) V — V. Different parts of step 2 cannot
conflict with each other because each one affects a
symbol with a different subscript. Also note that f(e1) is
an endpoint of e, and that H a, is a rule for any e, E E
and a an endpoint of e,.
Step 3. Number all occurrences of terminals in a(V,E,k)
that were not attached in step 1 or step 2. For the ith
such occurrence, construct a node dominating the occur-
rence and label it D. Then construct another node domi-
</bodyText>
<equation confidence="0.9882484375">
START 111 H2 H3 H4 H5 H6 H7 Ul U2 D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 Dll D12 D13 D14
-&gt; V IW
1 1
H4 -* W4 I Z4
H7 X 1
7 7
U
U4 U
H2 -P. V2 I y2
H5 -1. X5 I y5
U2 U
H3 14&apos;3 1 X3
H6 Y6 I Z6
U3 U
V1 V2 V3 V4 V5. V6 V7 I W1 W2 W3 W4 W5 W6 W7 1
Y1 Y2 Y3 Y4 Y5 Y6 Y7 I Z1 Z2 Z3 Z4 Z5 Z6 Z7
</equation>
<figure confidence="0.937314769230769">
D2
D5
D8
Dii
D14
X1 X2 X3 X4 X5 X6 X7
D3 D
D6 D
D9 D
D12 D
v1lv21v3Iv4Iv5Iv6Iv7I wilw2Iw3Iw4Iw5Iw6Iw7I
X11x21 X3 1x4 1x5 1x6 I X7 I y1lY2IY3 I Y4 I Y5 I Y6 I Y7 I
z1IZ21 z3IZ41Z51/6 Iz7
</figure>
<figureCaption confidence="0.9935526">
Figure 6. The construction of Lemma 9.4 produces this grammar when applied to the VERTEX COVER problem of
Figure 5. The H-symbols ensure that the solution that is found must hit each of the edge s, while the U-symbols ensure
that enough elements of V remain untouched to satisfy the requirement I V I k. The D-symbols are dummies that
absorb excess input symbols. A shorter grammar than this will suffice if the grammar is not required to be duplicate-
free.
</figureCaption>
<note confidence="0.2707505">
Computational Linguistics, Volume 11, Number 4, October-December 1985 215
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<bodyText confidence="0.976642833333333">
nating the D-node and label it D„ Note that the stock of
D-symbols runs from 1 to (k-1) • IEI. Exactly
(I VI —k) I EJ syrhbols of a(V,E,k) were accounted for
in step 1. Also, exactly 1 E I symbols were accounted for
in step 2. The length of a(V,E,k) is IVI IEl, hence
exactly
</bodyText>
<equation confidence="0.898092">
Vi • lEi —(lVl —k)• IEI — IEI
=IVI • IEI — IVI • IEI +k• IEI — IEI
=(k-1) • 1 E 1
</equation>
<bodyText confidence="0.9287255">
symbols remain at the beginning of step 3. D a is a
rule for any a in E; D -).D is a rule for any De
</bodyText>
<figureCaption confidence="0.670555333333333">
Step 4. Finally, construct a node labeled START that
dominates all of the H, U„ and D nodes constructed in
steps 1, 2, and 3. The rule
</figureCaption>
<equation confidence="0.582597">
START -+ ... • .
</equation>
<bodyText confidence="0.9708874">
is in the grammar. Note also that nodes labeled
Hi,&apos; were constructed in step 2, nodes labeled
U1 vi_k were constructed in step 1, and nodes labeled D1,
•• • DIE1.(k-1) were constructed in step 3. Hence the appli-
cation of the rule is in accord with the grammar. Then
a(V,E,k) E L(G). (Figure 7 illustrates the application of
this parse-tree construction procedure to the grammar
and input string derived -from the VERTEX COVER
example in Figure 5.)
Conversely, suppose a(V,E,k) e L(G). Then the deriva-
tion of a(V,E,k) from START must begin with the appli-
cation of the rule
START-. H1... Hi E . . . v . . . Di E I • (k-1)
and each H., must later be expanded as some subscripted
terminal g(H). Define f(ei) to be g(H) without the
</bodyText>
<figure confidence="0.765474">
START
V1 V2 V3 V4 Vg Vg V7 W1tV2W3W41V5W6W7 21 X2 X3 24 X&amp; X6 x7 Y1Y2Y3Y4Y5Y13Y7 X1 Z2 Z3 Z4 Zg Z6 Z2
</figure>
<figureCaption confidence="0.993783666666667">
Figure 7. This parse tree shows how the grammar shown in Figure 6 can generate the string a(V,E,k) constructed in
Lemma 9.4 for the VERTEX COVER problem of Figure 5. The corresponding VERTEX COVER solution V. = v,x,z}
and its intersection with the edges can be read off by noticing which terminals the H-symbols dominate.
</figureCaption>
<figure confidence="0.69908675">
DDDDD U DD
DD
DDD
H2D1 D2D3D4Dg U1 D0D7H3D8H5D9D10 U2 D11D12D13H4D14116H7
</figure>
<page confidence="0.807329">
216 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
<note confidence="0.653354">
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
</note>
<bodyText confidence="0.994643333333333">
subscript; then by construction of the grammar, f(e) is
an endpoint of e, for all e, E E. Define V = {f(0 : e, E
E}; then it is apparent that V g V and that V contains at
least one endpoint of e, for all e, E E. Also, each U, for i
E [1, I VI —k] must be expanded as U, then as some
substring (a,), (a,)1E1 of a(V,E,k).18 Since the
substrings dominated by the H, and U, must all be
disjoint, and since there are only I E I subscripted occur-
rences of any single symbol from V in a(V,E,k), there
must be I VI — k distinct elements of V that are not
dominated in any of their subscripted versions by any H,.
Then I V — VI &gt; I VI — k. Since in addition V g V&apos; ,
</bodyText>
<equation confidence="0.401218">
I V I &lt; k. Then (V,E,k) in VERTEX COVER. 0
Theorem 1: SIMPLE UCFG RECOGNITION is NP-
complete.
</equation>
<bodyText confidence="0.855060882352941">
Proof. SIMPLE UCFG RECOGNITION is in the class 4r.9
by Lemma 9.3, hence a polynomial-time reduction of
VERTEX COVER to SIMPLE UCFG RECOGNITION is
sufficient. Let (V,E,k) be a possible instance of VERTEX
COVER. Let G be G(V,E,k) and a be (V,E,k) as
constructed in Lemma 9.4. Note that G is simple.
The construction of G and a can, by lemma, be carried
out at time at worst polynomial in II Eli, II Vii, and k.
Also by lemma (G,0&amp;quot;) E SIMPLE UCFG RECOGNITION
iff (V,E,k) E VERTEX COVER. k is not polynomial in
ii k II under a reasonable encoding scheme. However,
IEI &gt; k, hence II El ii k ; also II (V,E,k) II II Eli
hence II ( V,E,k) II &gt; k, all by properties assumed to hold
of II &apos; Then G and a can in fact be constructed in time
at worst polynomial in II(V,E,k)
Hence the VERTEX COVER problem is polynomial-time
reduced to SIMPLE UCFG RECOGNITION. 0
</bodyText>
<sectionHeader confidence="0.994264" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.99874164516129">
Barton, G. Edward, Jr. 1984 Toward a Principle-Based Parser. A.I.
Memo No. 788, M.I.T. Artificial Intelligence Laboratory,
Cambridge, Massachusetts.
Berwick, Robert C. 1982 Computational Complexity and Lexical-
Functional Grammar. American Journal of Computational Linguistics
8(3-4): 97-109.
Berwick, Robert C. and Weinberg, Amy S. 1982 Parsing Efficiency,
Computational Complexity, and the Evaluation of Grammatical
Theories. Linguistic Inquiry 13(2): 165-191.
Berwick, Robert C. and Weinberg, Amy S. 1984 The Grammatical
Basis of Linguistic Performance. M.I.T. Press, Cambridge, Massa-
chusetts.
Chomsky, Noam A. 1980. Rules and Representations. Columbia
University Press, New York, New York.
Chomsky, Noam A. 1981 Lectures on Government and Binding. Foris
Publications, Dordrecht, Holland.
Earley, Jay 1970 An Efficient Context-Free Parsing Algorithm.
Communications of the ACM 13(2): 94-102.
Garey, Michael R. and Johnson, David S. 1979 Computers and Intract-
ability. W. H. Freeman and Co., San Francisco, California.
Gazdar, Gerald 1981 Unbounded Dependencies and Coordinate
Structure. Linguistic Inquiry 12(2): 155-184.
Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan
1985 Generalized Phrase Structure Grammar. Basil Blackwell,
Oxford, U.K.
Guerssel, Mohamed; Hale, Kenneth; Laughren, Mary; Levin, Beth; and
White Eagle, Josie 1985 A Cross-Linguistic Study of Transitivity
Alternations. Paper presented at the Parasession on Causatives and
Agentivity at the Twenty-First Regional Meeting of the Chicago
Linguistic Society, April 1985.
Hoperoft, John E. and Ullman, Jeffrey D. 1979 Introduction to Auto-
mata Theory, Languages, and Computation. Addison-Wesley, Read-
ing, Massachusetts.
Kroch, Anthony S. and Joshi, Aravind K. 1985 The Linguistic Rele-
vance of Tree Adjoining Grammars. Technical Report No.
MS-CIS-85-16, Department of Computer and Information Science,
Moore School, University of Pennsylvania, Philadelphia, Pennsylva-
nia.
Levin, Beth; and Rappaport, Malka 1985 The Formation of Adjectival
Passives. Lexicon Project Working Papers #2, M.I.T. Center for
Cognitive Science, Cambridge, Massachusetts.
Miller, George A. and Chomsky, Noam A. 1963 Finitary Models of
Language Users. In: Luce, R. D.; Bush, R. R.; and Galanter, E.,
Eds., Handbook of Mathematical Psychology, vol. II. John Wiley and
Sons, New York, New York: 419-492.
Ristad, Eric S. 1985 GPSG-Recognition is NP-Hard. A.I. Memo No.
837, M.I.T. Artificial Intelligence Laboratory, Cambridge, Massa-
chusetts.
Shieber, Stuart M. 1983 Direct Parsing of ID/LP Grammars. Techni-
cal Report 291R, SRI International, Menlo Park, California. Also
appears in Linguistics and Philosophy 7(2).
Shieber, Stuart M. 1985 Using Restriction to Extend Parsing Algo-
rithms for Complex Feature—Based Formalisms. ACL-85 confer-
ence proceedings, pp. 145-152.
NOTES
1. This report describes research done at the Artificial Intel-
ligence Laboratory of the Massachusetts Institute of
Technology. Support for the Laboratory&apos;s artificial intelli-
gence research has been provided in part by the Advanced
Research Projects Agency of the Department of Defense
under Office of Naval Research contract
N00014-80-C-0505. Partial support for the author&apos;s
</reference>
<bodyText confidence="0.848872111111111">
graduate studies was provided by the Fannie and John
Hertz Foundation.
Useful guidance and commentary during the writing of
this paper have been provided by Bob Berwick, Michael
Sipser, and Joyce Friedman. The paper was also
improved in response to interesting remarks from the
anonymous referees for Computational Linguistics. The
author gratefully acknowledges the assistance of Blythe
Heepe in preparing the fitures.
</bodyText>
<reference confidence="0.793744851351352">
2. See Barton (1984) for discussion.
3. This estimate is from an anonymous referee.
4. See section 5; it is in fact the best case.
5. Shieber&apos;s representation differs in some ways from the
representation used here, which was developed independ-
ently by the author. The differences are generally ines-
sential, but see note 7.
6. The states related to the auxiliary start symbol and
endmarker that are added by some versions of the Earley
parser have been omitted for simplicity.
7. In contrast to the representation illustrated here, Shieber&apos;s
representation actually suffers to some extent from the
same problem. Shieber (1983:10) uses an ordered
sequence instead of a multiset before the dot; consequent-
ly, in place of the state involving S {A,B,C1 • {D,E},
Shieber would have the 3! = 6 states involving S a •
{D,E}, where a ranges over the six permutations of ABC.
8. Recognition is simpler than parsing because a recognizer
is not required to recover the structure of an input string,
but only to decide whether the string is in the language
Computational Linguistics, Volume 11, Number 4, October-December 1985 217
G. Edward Barton, Jr. On the Complexity of ID/LP Parsing
generated by the grammar: that is, whether or not there
exists a parse.
9. If the vertex cover is smaller than expected, the
D-symbols will soak up the extra contiguous runs that
could have been matched by more U-symbols.
10. Even assuming 8&apos; it does not follow that the time
complexity must be exponential, though it seems likely to
be. There are functions such as ni&amp;quot;&amp;quot; that fall between
polynomials and exponentials. See Hoperoft and Ullman
(1979:341).
11. Shieber (1983:15 n. 6) mentions a possible precompila-
tion step, but it is concerned with the LP relation rather
than the ID rules.
12. It is not known whether the worst-case complexity of
ID/LP parsing is exponential, since more generally it is not
known for sure that .9 #
13. The complexity of ID/LP parsing drops as maximum rule
length drops, but so does the succinctness advantage of an
ID/LP grammar over a standard CFG.
14. In the GB-framework of Chomsky (1981), for instance,
the syntactic expression of unordered 0-grids at the X
level is constrained by the principles of Case theory. In a
related framework, the limited possibilities for projection
from &amp;quot;lexical-conceptual structure&amp;quot; to syntactic argument
structure combine with Case-assignment rules to severely
restrict the possible configurations (Guerssel et al. 1985,
Levin 1985). See also Berwick&apos;s (1982) discussion of
constraints that could be placed on another grammatical
formalism — lexical-functional grammar — to avoid a simi-
lar intractability result.
15. Indeed, one may not conclude a priori that all the
sentences of every language permitted by linguistic theory
are algorithmically parsable at all (Chomsky 1980). This
is true for a variety of reasons. Imagine, for instance, that
linguistic theory allowed the strings ruled out by filters to
be specified by complex enumerators. Then the strings of
a language would be defined in part by subtracting off an
r.e. set, which could lead to nonrecursiveness because the
complement of an r.e. set is not always r.e. But even if
nonrecursive, the set of strings would be perfectly well-de-
fined.
16. It is often anecdotally remarked that languages that allow
relatively free word order tend to make heavy use of
inflections. A rich inflectional system can supply parsing
constraints that make up for the lack of ordering
constraints; thus the situation we do not find is the
computationally difficult case of weak constraint.
17. This formulation differs trivially from the one cited by
Garey and Johnson.
18. The grammar would allow the substring (a), ... (a)1,1 to
appear in any permutation, but in a(V,E,k) it appears only
in the indicated order.
</reference>
<page confidence="0.936585">
218 Computational Linguistics, Volume 11, Number 4, October-December 1985
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.765122">
<title confidence="0.922917">COMPLEXITY OF</title>
<author confidence="0.99038">Barton</author>
<affiliation confidence="0.999986">Intelligence Laboratory</affiliation>
<address confidence="0.9977885">545 Technology Square Cambridge, MA 02139</address>
<abstract confidence="0.9820882">Modern linguistic theory attributes surface complexity to interacting subsystems of constraints. For instance, the ID/LP grammar formalism separates constraints on immediate dominance from those on linear order. An ID/LP parsing algorithm by Shieber shows how to use ID and LP constraints directly in language processing, without expanding them into an intermediate context-free &amp;quot;object grammar&amp;quot;. However, Shieber&apos;s purported runtime bound underestimates the difficulty of ID/LP parsing. ID/LP parsing is actually NP-complete, and the worst-case runtime of Shieber&apos;s algorithm is actually exponential in grammar size. The growth of parser data structures causes the difficulty. Some computational and linguistic implications follow; in particular, it is important to note that, despite its potential for combinatorial explosion, Shieber&apos;s algorithm remains better than the alternative of parsing an expanded object grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Edward Barton</author>
</authors>
<title>Toward a Principle-Based Parser.</title>
<date>1984</date>
<booktitle>A.I. Memo No. 788, M.I.T. Artificial Intelligence Laboratory,</booktitle>
<location>Cambridge, Massachusetts.</location>
<marker>Barton, 1984</marker>
<rawString>Barton, G. Edward, Jr. 1984 Toward a Principle-Based Parser. A.I. Memo No. 788, M.I.T. Artificial Intelligence Laboratory, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
</authors>
<date>1982</date>
<journal>Computational Complexity and LexicalFunctional Grammar. American Journal of Computational Linguistics</journal>
<volume>8</volume>
<issue>3</issue>
<pages>97--109</pages>
<contexts>
<context position="38678" citStr="Berwick (1982)" startWordPosition="6689" endWordPosition="6690">oes not guarantee efficient parsability. Every ID/LP grammar technically generates a context-free language, but the potentially large size of the corresponding CFG means that we can&apos;t count on that fact to give us efficient parsing. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as (early) GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1982, 1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber&apos;s direct parsing algorithm wins out over the use of the object grammar However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself does not tell the whole story. That is, they point out that the range of possible languages has been incorrectly characterized: the additional constraints that guarantee e</context>
</contexts>
<marker>Berwick, 1982</marker>
<rawString>Berwick, Robert C. 1982 Computational Complexity and LexicalFunctional Grammar. American Journal of Computational Linguistics 8(3-4): 97-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
<author>Amy S Weinberg</author>
</authors>
<title>Parsing Efficiency, Computational Complexity, and the Evaluation of Grammatical Theories.</title>
<date>1982</date>
<journal>Linguistic Inquiry</journal>
<volume>13</volume>
<issue>2</issue>
<pages>165--191</pages>
<contexts>
<context position="3414" citStr="Berwick and Weinberg (1982)" startWordPosition="489" endWordPosition="492">traints, etc. The standard algorithms operate on CFGs, not on extended variants of them. Unfortunately, the object grammar may be huge after the effects of all nonstandard devices have been expanded out. Estimates of the object-grammar size for typical systems vary from hundreds or thousands3 up to trillions of rules (Shieber 1983:4). With some formalisms, the context-free object-grammar approach is not even possible because the object grammar would be infinite (Shieber 1985:145). Grammar size matters beyond questions of elegance and clumsiness, for it typically affects processing complexity. Berwick and Weinberg (1982) argue that the effects of grammar size can actually dominate complexity for a relevant range of input lengths. Given the disadvantages of multiplying out the effects of separate systems of constraints, Shieber&apos;s (1983) work on direct parsing leads in a welcome direction. Shieber considers how one might do parsing with ID/LP grammars, which involve two orthogonal kinds of rules. ID rules constrain immediate dominance irrespective of constituent order (&amp;quot;a sentence can be composed of V with NP and SBAR complements&amp;quot;), while LP rules Copyright 1 985 by the Association for Computational Linguistics</context>
<context position="38706" citStr="Berwick and Weinberg (1982" startWordPosition="6691" endWordPosition="6694">e efficient parsability. Every ID/LP grammar technically generates a context-free language, but the potentially large size of the corresponding CFG means that we can&apos;t count on that fact to give us efficient parsing. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as (early) GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1982, 1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber&apos;s direct parsing algorithm wins out over the use of the object grammar However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself does not tell the whole story. That is, they point out that the range of possible languages has been incorrectly characterized: the additional constraints that guarantee efficient parsability remain </context>
</contexts>
<marker>Berwick, Weinberg, 1982</marker>
<rawString>Berwick, Robert C. and Weinberg, Amy S. 1982 Parsing Efficiency, Computational Complexity, and the Evaluation of Grammatical Theories. Linguistic Inquiry 13(2): 165-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
<author>Amy S Weinberg</author>
</authors>
<title>The Grammatical Basis of Linguistic Performance.</title>
<date>1984</date>
<publisher>M.I.T. Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="39747" citStr="Berwick and Weinberg 1984" startWordPosition="6848" endWordPosition="6851">he whole story. That is, they point out that the range of possible languages has been incorrectly characterized: the additional constraints that guarantee efficient parsability remain unstated. Since the general case of parsing ID/LP grammars is computationally difficult, if the linguistically relevant ID/LP grammars are to be efficiently parsable, there must be additional factors that guarantee a certain amount of constraint from some source.&apos;4 (Constraints beyond the bare ID/LP formalism are required on linguistic grounds as well.) Note that the subset principle of language acquisition (cf. Berwick and Weinberg 1984:233) would lead the language learner to initially hypothesize strong order constraints, to be weakened only in response to positive evidence. However, there are other potential ways to guarantee efficient parsability. It might turn out that the principles and parameters of the best grammatical theory permit languages that are not efficiently parsable in the worst case — just as grammatical theory permits sentences that are deeply center-embedded (Miller and Chomsky 1963).15 In such a situation, difficult languages or sentences would not be expected to turn up in general use, precisely because</context>
</contexts>
<marker>Berwick, Weinberg, 1984</marker>
<rawString>Berwick, Robert C. and Weinberg, Amy S. 1984 The Grammatical Basis of Linguistic Performance. M.I.T. Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam A Chomsky</author>
</authors>
<title>Rules and Representations.</title>
<date>1980</date>
<publisher>Columbia University Press,</publisher>
<location>New York, New York.</location>
<marker>Chomsky, 1980</marker>
<rawString>Chomsky, Noam A. 1980. Rules and Representations. Columbia University Press, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam A Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding. Foris Publications,</booktitle>
<location>Dordrecht, Holland.</location>
<contexts>
<context position="1626" citStr="Chomsky (1981" startWordPosition="225" endWordPosition="226">lternative of parsing an expanded object grammar. 1 INTRODUCTION It is common in recent linguistic theories for various surface characteristics of a language to be described in terms of several different kinds of underlying constraints. ID/LP grammars involve immediate-dominance rules and linear-order constraints; more broadly, GPSG systems can also involve feature relationships and metarules (Gazdar et al. 1985). The tree adjunction grammars of Kroch and Joshi (1985) separate the statement of local constraints from the projection of those constraints to larger structures. The GB-framework of Chomsky (1981:5) identifies the subtheories of bounding, government, 0-marking, binding, Case, and control. When several independent constraints are involved, a system that explicitly multiplies out their effects is large, cumbersome, and uninformative.2 If done properly, the disentanglement of different kinds of constraints can result in shorter and more illuminating language descriptions. With any such modular framework, two questions immediately arise: how can the various constraints be put back together in parsing, and what are the computational characteristics of the process? One approach is to compil</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, Noam A. 1981 Lectures on Government and Binding. Foris Publications, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM</journal>
<volume>13</volume>
<issue>2</issue>
<pages>94--102</pages>
<contexts>
<context position="11080" citStr="Earley 1970" startWordPosition="1792" endWordPosition="1793">1, C}. • A dotted UCFG rule is complete iff the multiset after the dot is empty. For example, X {A, B, Cl {} is complete. • A dotted UCFG rule predicts a terminal or nonterminal iff the terminal or nonterminal is a member of the multiset after the dot. For example, X {A I {B, predicts B and C. Given these replacements for operations on dotted rules, Shieber&apos;s algorithm operates in the same way as Earley&apos;s algorithm. As usual, each state in the parser&apos;s state sets consists of a dotted rule tracking progress through a constituent plus the interword position defining the constituent&apos;s left edge (Earley 1970:95, omitting lookahead). The left-edge position is also referred to as the return pointer because of its role in the complete operation of the parser. 3 THE ADVANTAGES OF SHIEBER&apos;S ALGORITHM The first question to ask is whether Shieber&apos;s algorithm saves anything. Is it faster to use Shieber&apos;s algorithm on a UCFG than to use Earley&apos;s algorithm on the corresponding expanded CFG? Consider the UCFG Gi that has only the single rule S abcde. The corresponding CFG G&apos;i has 120 rules spelling out all the permutations of abcde: S abcde, S abced, and so forth. If the string abcde is parsed using Shieber</context>
<context position="17797" citStr="Earley 1970" startWordPosition="3037" endWordPosition="3038">n the corresponding state when the expanded CFG 3 is used.) 5 THE SOURCE OF THE DIFFICULTY Why is Shieber&apos;s algorithm potentially exponential in grammar size despite its &amp;quot;close relation&amp;quot; to Earley&apos;s algorithm, which has time complexity polynomial in grammar size? The answer lies in the size of the state space that each parser uses. Relative to grammar size, Shieber&apos;s algorithm involves a much larger bound than Earley&apos;s algorithm on the number of states in a state set. Since the main task of the Earley parser is to perform scan, predict, and complete operations on the states in each state set (Earley 1970:97), an explosion in the size of the state sets will be fatal to any small runtime bound. Given a CFG Ga, how many possible dotted rules are there? Resulting from each rule X -* A, ... A„ there are k+1 possible dotted rules. Then the number of possible dotted rules is bounded by 1 GoI , if this notation is taken to mean the number of symbols that it takes to write Go down. An Earley state is a pair [r,i], where r is a dotted rule and i is an interword position ranging from 0 to the length n of the input string. Because of these limits, no state set in the Earley parser can contain more than 0</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay 1970 An Efficient Context-Free Parsing Algorithm. Communications of the ACM 13(2): 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Garey</author>
<author>David S Johnson</author>
</authors>
<date>1979</date>
<journal>Computers</journal>
<location>San Francisco, California.</location>
<contexts>
<context position="21786" citStr="Garey and Johnson 1979" startWordPosition="3764" endWordPosition="3767">laims. 6 ID/LP PARSING IS INHERENTLY DIFFICULT The worst-case time complexity of Shieber&apos;s algorithm is exponential in grammar size rather than quadratic as Shieber (1983:15) believed. Did Shieber simply choose a poor algorithm, or is ID/LP parsing inherently difficult in the general case? In fact, the simpler problem of recognizing sentences according to a UCFG is NP-complete.8 Consequently, unless .9 = &lt;4;9, no algorithm for ID/LP parsing can have a runtime bound that is polynomial in the size of the grammar and input. The proof of NP-completeness involves reducing the vertex cover problem (Garey and Johnson 1979:46) to the UCFG recognition problem. Through careful construction of the grammar and input string, it is possible to &amp;quot;trick&amp;quot; the parser into solving a known hard problem. The vertex cover problem involves finding a small set of vertices in a graph with the property that every edge of the graph has at least one endpoint in the set. Figure 2 shows a trivial example. a e3 Figure 2. This graph illustrates a trivial instance of the vertex cover problem. The set {c,c/} is a vertex cover of size 2. To construct a grammar that encodes the question of whether the graph in Figure 2 has a vertex cover o</context>
<context position="44802" citStr="Garey and Johnson 1979" startWordPosition="7760" endWordPosition="7763"> the problem VERTEX COVER is a triple ( V,E,k), where (V,E.) is a finite graph with at least one edge and at least two vertices, k E N, and k &lt; I V1.17 VERTEX COVER itself consists of all possible instances (V,E,k) such that for some V g v, 1 V I &lt; k and for all edges e E, at least one endpoint of e is in V. (Figure 5 gives an example of a VERTEX COVER instance.) v= [ v, w,x,y,z1 E= feeeeef 1, 2, 3, 4, 5&apos; ee 6&apos; 7 with the e as indicated k= 3 Figure 5. The triple ( V,E,k) is an instance of VERTEX COVER. The set I/ = {v,x,z,pf;} is a vertex cover of size k=3. Fact: VERTEX COVER is NP-complete. (Garey and Johnson 1979:46) Definition: A possible instance of the problem SIMPLE UCFG RECOGNITION is a pair (G, a), where G is a simple UCFG and a E 2*. SIMPLE UCFG 213 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing RECOGNITION itself consists of all possible instances (G, a) such that a E L(G). Notation: Take II II to be any reasonable measure of the encoded input length for a computational problem; continue to use I • I for set cardinality and string length. It is reasonable to require that if S is a set, k E N, and ISI&gt; k, then II S II &gt; II k ; that is, the encoding of numbers is better than unary. It </context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, Michael R. and Johnson, David S. 1979 Computers and Intractability. W. H. Freeman and Co., San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Unbounded Dependencies and Coordinate Structure.</title>
<date>1981</date>
<journal>Linguistic Inquiry</journal>
<volume>12</volume>
<issue>2</issue>
<pages>155--184</pages>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, Gerald 1981 Unbounded Dependencies and Coordinate Structure. Linguistic Inquiry 12(2): 155-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey K Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar. Basil Blackwell,</title>
<date>1985</date>
<location>Oxford, U.K.</location>
<contexts>
<context position="1429" citStr="Gazdar et al. 1985" startWordPosition="194" endWordPosition="197">culty. Some computational and linguistic implications follow; in particular, it is important to note that, despite its potential for combinatorial explosion, Shieber&apos;s algorithm remains better than the alternative of parsing an expanded object grammar. 1 INTRODUCTION It is common in recent linguistic theories for various surface characteristics of a language to be described in terms of several different kinds of underlying constraints. ID/LP grammars involve immediate-dominance rules and linear-order constraints; more broadly, GPSG systems can also involve feature relationships and metarules (Gazdar et al. 1985). The tree adjunction grammars of Kroch and Joshi (1985) separate the statement of local constraints from the projection of those constraints to larger structures. The GB-framework of Chomsky (1981:5) identifies the subtheories of bounding, government, 0-marking, binding, Case, and control. When several independent constraints are involved, a system that explicitly multiplies out their effects is large, cumbersome, and uninformative.2 If done properly, the disentanglement of different kinds of constraints can result in shorter and more illuminating language descriptions. With any such modular </context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan 1985 Generalized Phrase Structure Grammar. Basil Blackwell, Oxford, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Guerssel</author>
<author>Kenneth Hale</author>
<author>Mary Laughren</author>
<author>Beth Levin</author>
<author>White Eagle</author>
</authors>
<title>A Cross-Linguistic Study of Transitivity Alternations. Paper presented at the Parasession on Causatives and Agentivity at the Twenty-First Regional Meeting of the Chicago Linguistic Society,</title>
<date>1985</date>
<marker>Guerssel, Hale, Laughren, Levin, Eagle, 1985</marker>
<rawString>Guerssel, Mohamed; Hale, Kenneth; Laughren, Mary; Levin, Beth; and White Eagle, Josie 1985 A Cross-Linguistic Study of Transitivity Alternations. Paper presented at the Parasession on Causatives and Agentivity at the Twenty-First Regional Meeting of the Chicago Linguistic Society, April 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hoperoft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>Hoperoft, John E. and Ullman, Jeffrey D. 1979 Introduction to Automata Theory, Languages, and Computation. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony S Kroch</author>
<author>Aravind K Joshi</author>
</authors>
<title>The Linguistic Relevance of Tree Adjoining Grammars.</title>
<date>1985</date>
<tech>Technical Report No. MS-CIS-85-16,</tech>
<institution>Department of Computer and Information Science, Moore School, University of Pennsylvania,</institution>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1485" citStr="Kroch and Joshi (1985)" startWordPosition="203" endWordPosition="206"> follow; in particular, it is important to note that, despite its potential for combinatorial explosion, Shieber&apos;s algorithm remains better than the alternative of parsing an expanded object grammar. 1 INTRODUCTION It is common in recent linguistic theories for various surface characteristics of a language to be described in terms of several different kinds of underlying constraints. ID/LP grammars involve immediate-dominance rules and linear-order constraints; more broadly, GPSG systems can also involve feature relationships and metarules (Gazdar et al. 1985). The tree adjunction grammars of Kroch and Joshi (1985) separate the statement of local constraints from the projection of those constraints to larger structures. The GB-framework of Chomsky (1981:5) identifies the subtheories of bounding, government, 0-marking, binding, Case, and control. When several independent constraints are involved, a system that explicitly multiplies out their effects is large, cumbersome, and uninformative.2 If done properly, the disentanglement of different kinds of constraints can result in shorter and more illuminating language descriptions. With any such modular framework, two questions immediately arise: how can the </context>
</contexts>
<marker>Kroch, Joshi, 1985</marker>
<rawString>Kroch, Anthony S. and Joshi, Aravind K. 1985 The Linguistic Relevance of Tree Adjoining Grammars. Technical Report No. MS-CIS-85-16, Department of Computer and Information Science, Moore School, University of Pennsylvania, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>Rappaport</author>
</authors>
<title>The Formation of Adjectival Passives. Lexicon Project Working Papers #2,</title>
<date>1985</date>
<institution>M.I.T. Center for Cognitive Science,</institution>
<location>Malka</location>
<marker>Levin, Rappaport, 1985</marker>
<rawString>Levin, Beth; and Rappaport, Malka 1985 The Formation of Adjectival Passives. Lexicon Project Working Papers #2, M.I.T. Center for Cognitive Science, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Noam A Chomsky</author>
</authors>
<title>Finitary Models of Language Users.</title>
<date>1963</date>
<booktitle>Handbook of Mathematical Psychology,</booktitle>
<volume>vol.</volume>
<pages>419--492</pages>
<publisher>II. John Wiley and Sons,</publisher>
<location>In:</location>
<contexts>
<context position="40223" citStr="Miller and Chomsky 1963" startWordPosition="6918" endWordPosition="6921">e ID/LP formalism are required on linguistic grounds as well.) Note that the subset principle of language acquisition (cf. Berwick and Weinberg 1984:233) would lead the language learner to initially hypothesize strong order constraints, to be weakened only in response to positive evidence. However, there are other potential ways to guarantee efficient parsability. It might turn out that the principles and parameters of the best grammatical theory permit languages that are not efficiently parsable in the worst case — just as grammatical theory permits sentences that are deeply center-embedded (Miller and Chomsky 1963).15 In such a situation, difficult languages or sentences would not be expected to turn up in general use, precisely because they would be difficult to process.16 The factors that guarantee efficient parsability would not be part of grammatical theory because they would result from extragrammatical factors, i.e. the resource limitations of the language-processing mechanisms. This &amp;quot;easy way out&amp;quot; is not automatically available, depending as it does on a detailed account of processing mechanisms. For example, in the Earley parser, the difficulty of parsing a construction can vary widely with the </context>
</contexts>
<marker>Miller, Chomsky, 1963</marker>
<rawString>Miller, George A. and Chomsky, Noam A. 1963 Finitary Models of Language Users. In: Luce, R. D.; Bush, R. R.; and Galanter, E., Eds., Handbook of Mathematical Psychology, vol. II. John Wiley and Sons, New York, New York: 419-492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric S Ristad</author>
</authors>
<title>GPSG-Recognition is NP-Hard.</title>
<date>1985</date>
<booktitle>A.I. Memo No. 837, M.I.T. Artificial Intelligence Laboratory,</booktitle>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="38732" citStr="Ristad (1985)" startWordPosition="6697" endWordPosition="6698">ammar technically generates a context-free language, but the potentially large size of the corresponding CFG means that we can&apos;t count on that fact to give us efficient parsing. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as (early) GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1982, 1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber&apos;s direct parsing algorithm wins out over the use of the object grammar However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself does not tell the whole story. That is, they point out that the range of possible languages has been incorrectly characterized: the additional constraints that guarantee efficient parsability remain unstated. Since the genera</context>
</contexts>
<marker>Ristad, 1985</marker>
<rawString>Ristad, Eric S. 1985 GPSG-Recognition is NP-Hard. A.I. Memo No. 837, M.I.T. Artificial Intelligence Laboratory, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Direct Parsing of ID/LP Grammars.</title>
<date>1983</date>
<tech>Technical Report 291R,</tech>
<institution>SRI International, Menlo Park, California.</institution>
<note>Also appears in Linguistics and Philosophy 7(2).</note>
<contexts>
<context position="3119" citStr="Shieber 1983" startWordPosition="449" endWordPosition="450">with known computational characteristics. However, in order to apply this method, it is necessary to expand out the effects of everything that falls outside the strict context-free format: rule schemas, metarules, ID rules, LP constraints, feature instantiations, casemarking constraints, etc. The standard algorithms operate on CFGs, not on extended variants of them. Unfortunately, the object grammar may be huge after the effects of all nonstandard devices have been expanded out. Estimates of the object-grammar size for typical systems vary from hundreds or thousands3 up to trillions of rules (Shieber 1983:4). With some formalisms, the context-free object-grammar approach is not even possible because the object grammar would be infinite (Shieber 1985:145). Grammar size matters beyond questions of elegance and clumsiness, for it typically affects processing complexity. Berwick and Weinberg (1982) argue that the effects of grammar size can actually dominate complexity for a relevant range of input lengths. Given the disadvantages of multiplying out the effects of separate systems of constraints, Shieber&apos;s (1983) work on direct parsing leads in a welcome direction. Shieber considers how one might </context>
<context position="5713" citStr="Shieber (1983" startWordPosition="847" endWordPosition="848">o an expanded grammar with trillions of rules, since the worst-case time complexity of Earley&apos;s algorithm is proportional to the square of the grammar size! Shieber&apos;s general approach is on the right track. On pain of having a large and cumbersome rule system, the parser designer should first look to linguistics to find the correct set of constraints on syntactic structure, then discover how to apply some form of those constraints in parsing without multiplying out all possible surface manifestations of their effects. Nonetheless, nagging doubts about computational complexity remain. Although Shieber (1983:15) claims that his algorithm is identical to Earley&apos;s in time complexity, it seems almost too much to hope for that the size of an ID/LP grammar should enter into the time complexity of ID/LP parsing in exactly the same way that the size of a CFG enters into the time complexity of CFG parsing. An ID/LP grammar G can enjoy a huge size advantage over a context-free grammar G&apos; for the same language; for example, if G contains only the rule S ID abcde, the corresponding G&apos; contains 5! = 120 rules. In effect, the claim that Shieber&apos;s algorithm has the same time complexity as Earley&apos;s algorithm me</context>
<context position="21158" citStr="Shieber (1983" startWordPosition="3668" endWordPosition="3669">rs from combinatorial explosion is that there are exponentially more possible ways to progress through an unordered rule expansion than an ordered one. When disambiguating information is scarce, the parser must keep track of all of them. In the more general task of parsing ID/LP grammars, the most tractable case occurs when constraint from the LP relation is strong enough to force a unique ordering for every rule expansion. Under such conditions, Shieber&apos;s parser reduces to Earley&apos;s. However, the case of strong constraint represents the best case computationally, rather than the worst case as Shieber (1983:14) claims. 6 ID/LP PARSING IS INHERENTLY DIFFICULT The worst-case time complexity of Shieber&apos;s algorithm is exponential in grammar size rather than quadratic as Shieber (1983:15) believed. Did Shieber simply choose a poor algorithm, or is ID/LP parsing inherently difficult in the general case? In fact, the simpler problem of recognizing sentences according to a UCFG is NP-complete.8 Consequently, unless .9 = &lt;4;9, no algorithm for ID/LP parsing can have a runtime bound that is polynomial in the size of the grammar and input. The proof of NP-completeness involves reducing the vertex cover pro</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, Stuart M. 1983 Direct Parsing of ID/LP Grammars. Technical Report 291R, SRI International, Menlo Park, California. Also appears in Linguistics and Philosophy 7(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Using Restriction to Extend Parsing Algorithms for Complex Feature—Based Formalisms.</title>
<date>1985</date>
<booktitle>ACL-85 conference proceedings,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="3266" citStr="Shieber 1985" startWordPosition="470" endWordPosition="472">s outside the strict context-free format: rule schemas, metarules, ID rules, LP constraints, feature instantiations, casemarking constraints, etc. The standard algorithms operate on CFGs, not on extended variants of them. Unfortunately, the object grammar may be huge after the effects of all nonstandard devices have been expanded out. Estimates of the object-grammar size for typical systems vary from hundreds or thousands3 up to trillions of rules (Shieber 1983:4). With some formalisms, the context-free object-grammar approach is not even possible because the object grammar would be infinite (Shieber 1985:145). Grammar size matters beyond questions of elegance and clumsiness, for it typically affects processing complexity. Berwick and Weinberg (1982) argue that the effects of grammar size can actually dominate complexity for a relevant range of input lengths. Given the disadvantages of multiplying out the effects of separate systems of constraints, Shieber&apos;s (1983) work on direct parsing leads in a welcome direction. Shieber considers how one might do parsing with ID/LP grammars, which involve two orthogonal kinds of rules. ID rules constrain immediate dominance irrespective of constituent ord</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, Stuart M. 1985 Using Restriction to Extend Parsing Algorithms for Complex Feature—Based Formalisms. ACL-85 conference proceedings, pp. 145-152.</rawString>
</citation>
<citation valid="false">
<authors>
<author>NOTES</author>
</authors>
<title>This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Laboratory&apos;s artificial intelligence research has been provided</title>
<booktitle>in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract</booktitle>
<pages>00014--80</pages>
<note>Partial support for the author&apos;s</note>
<marker>NOTES, </marker>
<rawString>NOTES 1. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Laboratory&apos;s artificial intelligence research has been provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-80-C-0505. Partial support for the author&apos;s</rawString>
</citation>
<citation valid="true">
<authors>
<author>See Barton</author>
</authors>
<title>for discussion. 3. This estimate is from an anonymous referee. 4. See section 5; it is in fact the best case.</title>
<date>1984</date>
<marker>Barton, 1984</marker>
<rawString>2. See Barton (1984) for discussion. 3. This estimate is from an anonymous referee. 4. See section 5; it is in fact the best case.</rawString>
</citation>
<citation valid="false">
<title>Shieber&apos;s representation differs in some ways from the representation used here, which was developed independently by the author. The differences are generally inessential, but see note 7.</title>
<marker></marker>
<rawString>5. Shieber&apos;s representation differs in some ways from the representation used here, which was developed independently by the author. The differences are generally inessential, but see note 7.</rawString>
</citation>
<citation valid="false">
<title>The states related to the auxiliary start symbol and endmarker that are added by some versions of the Earley parser have been omitted for simplicity.</title>
<marker></marker>
<rawString>6. The states related to the auxiliary start symbol and endmarker that are added by some versions of the Earley parser have been omitted for simplicity.</rawString>
</citation>
<citation valid="false">
<title>7. In contrast to the representation illustrated here, Shieber&apos;s representation actually suffers to some extent from the same problem. Shieber (1983:10) uses an ordered sequence instead of a multiset before the dot; consequently,</title>
<booktitle>in place of the state involving S {A,B,C1 • {D,E}, Shieber would have the 3! = 6 states involving S</booktitle>
<marker></marker>
<rawString>7. In contrast to the representation illustrated here, Shieber&apos;s representation actually suffers to some extent from the same problem. Shieber (1983:10) uses an ordered sequence instead of a multiset before the dot; consequently, in place of the state involving S {A,B,C1 • {D,E}, Shieber would have the 3! = 6 states involving S a • {D,E}, where a ranges over the six permutations of ABC.</rawString>
</citation>
<citation valid="true">
<title>Recognition is simpler than parsing because a recognizer is not required to recover the structure of an input string, but only to decide whether the string is</title>
<date>1985</date>
<booktitle>in the language Computational Linguistics, Volume 11, Number 4, October-December</booktitle>
<pages>217</pages>
<contexts>
<context position="1485" citStr="(1985)" startWordPosition="206" endWordPosition="206">icular, it is important to note that, despite its potential for combinatorial explosion, Shieber&apos;s algorithm remains better than the alternative of parsing an expanded object grammar. 1 INTRODUCTION It is common in recent linguistic theories for various surface characteristics of a language to be described in terms of several different kinds of underlying constraints. ID/LP grammars involve immediate-dominance rules and linear-order constraints; more broadly, GPSG systems can also involve feature relationships and metarules (Gazdar et al. 1985). The tree adjunction grammars of Kroch and Joshi (1985) separate the statement of local constraints from the projection of those constraints to larger structures. The GB-framework of Chomsky (1981:5) identifies the subtheories of bounding, government, 0-marking, binding, Case, and control. When several independent constraints are involved, a system that explicitly multiplies out their effects is large, cumbersome, and uninformative.2 If done properly, the disentanglement of different kinds of constraints can result in shorter and more illuminating language descriptions. With any such modular framework, two questions immediately arise: how can the </context>
<context position="38732" citStr="(1985)" startWordPosition="6698" endWordPosition="6698">echnically generates a context-free language, but the potentially large size of the corresponding CFG means that we can&apos;t count on that fact to give us efficient parsing. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as (early) GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1982, 1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber&apos;s direct parsing algorithm wins out over the use of the object grammar However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself does not tell the whole story. That is, they point out that the range of possible languages has been incorrectly characterized: the additional constraints that guarantee efficient parsability remain unstated. Since the genera</context>
</contexts>
<marker>1985</marker>
<rawString>8. Recognition is simpler than parsing because a recognizer is not required to recover the structure of an input string, but only to decide whether the string is in the language Computational Linguistics, Volume 11, Number 4, October-December 1985 217</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Edward Barton</author>
</authors>
<title>On the Complexity of ID/LP Parsing generated by the grammar: that is, whether or not there exists a parse.</title>
<marker>Barton, </marker>
<rawString>G. Edward Barton, Jr. On the Complexity of ID/LP Parsing generated by the grammar: that is, whether or not there exists a parse.</rawString>
</citation>
<citation valid="false">
<title>If the vertex cover is smaller than expected, the D-symbols will soak up the extra contiguous runs that could have been matched by more U-symbols.</title>
<marker></marker>
<rawString>9. If the vertex cover is smaller than expected, the D-symbols will soak up the extra contiguous runs that could have been matched by more U-symbols.</rawString>
</citation>
<citation valid="true">
<title>Even assuming 8&apos; it does not follow that the time complexity must be exponential, though it seems likely to be. There are functions such as ni&amp;quot;&amp;quot; that fall between polynomials and exponentials. See Hoperoft and Ullman</title>
<date>1979</date>
<marker>1979</marker>
<rawString>10. Even assuming 8&apos; it does not follow that the time complexity must be exponential, though it seems likely to be. There are functions such as ni&amp;quot;&amp;quot; that fall between polynomials and exponentials. See Hoperoft and Ullman (1979:341).</rawString>
</citation>
<citation valid="false">
<title>Shieber (1983:15 n. 6) mentions a possible precompilation step, but it is concerned with the LP relation rather than the ID rules.</title>
<marker></marker>
<rawString>11. Shieber (1983:15 n. 6) mentions a possible precompilation step, but it is concerned with the LP relation rather than the ID rules.</rawString>
</citation>
<citation valid="false">
<title>It is not known whether the worst-case complexity of ID/LP parsing is exponential, since more generally it is not known for sure that .9 #</title>
<marker></marker>
<rawString>12. It is not known whether the worst-case complexity of ID/LP parsing is exponential, since more generally it is not known for sure that .9 #</rawString>
</citation>
<citation valid="false">
<title>The complexity of ID/LP parsing drops as maximum rule length drops, but so does the succinctness advantage of an ID/LP grammar over a standard CFG.</title>
<marker></marker>
<rawString>13. The complexity of ID/LP parsing drops as maximum rule length drops, but so does the succinctness advantage of an ID/LP grammar over a standard CFG.</rawString>
</citation>
<citation valid="false">
<title>In the GB-framework of Chomsky</title>
<date>1981</date>
<marker>1981</marker>
<rawString>14. In the GB-framework of Chomsky (1981), for instance, the syntactic expression of unordered 0-grids at the X level is constrained by the principles of Case theory. In a related framework, the limited possibilities for projection from &amp;quot;lexical-conceptual structure&amp;quot; to syntactic argument structure combine with Case-assignment rules to severely restrict the possible configurations (Guerssel et al. 1985,</rawString>
</citation>
<citation valid="true">
<title>See also Berwick&apos;s</title>
<date>1982</date>
<contexts>
<context position="3414" citStr="(1982)" startWordPosition="492" endWordPosition="492">ndard algorithms operate on CFGs, not on extended variants of them. Unfortunately, the object grammar may be huge after the effects of all nonstandard devices have been expanded out. Estimates of the object-grammar size for typical systems vary from hundreds or thousands3 up to trillions of rules (Shieber 1983:4). With some formalisms, the context-free object-grammar approach is not even possible because the object grammar would be infinite (Shieber 1985:145). Grammar size matters beyond questions of elegance and clumsiness, for it typically affects processing complexity. Berwick and Weinberg (1982) argue that the effects of grammar size can actually dominate complexity for a relevant range of input lengths. Given the disadvantages of multiplying out the effects of separate systems of constraints, Shieber&apos;s (1983) work on direct parsing leads in a welcome direction. Shieber considers how one might do parsing with ID/LP grammars, which involve two orthogonal kinds of rules. ID rules constrain immediate dominance irrespective of constituent order (&amp;quot;a sentence can be composed of V with NP and SBAR complements&amp;quot;), while LP rules Copyright 1 985 by the Association for Computational Linguistics</context>
<context position="38678" citStr="(1982)" startWordPosition="6690" endWordPosition="6690">guarantee efficient parsability. Every ID/LP grammar technically generates a context-free language, but the potentially large size of the corresponding CFG means that we can&apos;t count on that fact to give us efficient parsing. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as (early) GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1982, 1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber&apos;s direct parsing algorithm wins out over the use of the object grammar However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself does not tell the whole story. That is, they point out that the range of possible languages has been incorrectly characterized: the additional constraints that guarantee e</context>
</contexts>
<marker>1982</marker>
<rawString>Levin 1985). See also Berwick&apos;s (1982) discussion of constraints that could be placed on another grammatical formalism — lexical-functional grammar — to avoid a similar intractability result.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Indeed</author>
</authors>
<title>one may not conclude a priori that all the sentences of every language permitted by linguistic theory are algorithmically parsable at all (Chomsky 1980). This is true for a variety of reasons. Imagine, for instance, that linguistic theory allowed the strings ruled out by filters to be specified by complex enumerators. Then the strings of a language would be defined in part by subtracting off an r.e. set, which could lead to nonrecursiveness because the complement of an r.e. set is not always r.e. But even if nonrecursive, the set of strings would be perfectly well-defined.</title>
<marker>Indeed, </marker>
<rawString>15. Indeed, one may not conclude a priori that all the sentences of every language permitted by linguistic theory are algorithmically parsable at all (Chomsky 1980). This is true for a variety of reasons. Imagine, for instance, that linguistic theory allowed the strings ruled out by filters to be specified by complex enumerators. Then the strings of a language would be defined in part by subtracting off an r.e. set, which could lead to nonrecursiveness because the complement of an r.e. set is not always r.e. But even if nonrecursive, the set of strings would be perfectly well-defined.</rawString>
</citation>
<citation valid="false">
<title>It is often anecdotally remarked that languages that allow relatively free word order tend to make heavy use of inflections. A rich inflectional system can supply parsing constraints that make up for the lack of ordering constraints; thus the situation we do not find is the computationally difficult case of weak constraint.</title>
<marker></marker>
<rawString>16. It is often anecdotally remarked that languages that allow relatively free word order tend to make heavy use of inflections. A rich inflectional system can supply parsing constraints that make up for the lack of ordering constraints; thus the situation we do not find is the computationally difficult case of weak constraint.</rawString>
</citation>
<citation valid="false">
<title>This formulation differs trivially from the one cited by Garey and Johnson.</title>
<marker></marker>
<rawString>17. This formulation differs trivially from the one cited by Garey and Johnson.</rawString>
</citation>
<citation valid="false">
<title>The grammar would allow the substring (a), ...</title>
<note>(a)1,1 to appear in any permutation, but in a(V,E,k) it appears only in the indicated order.</note>
<marker></marker>
<rawString>18. The grammar would allow the substring (a), ... (a)1,1 to appear in any permutation, but in a(V,E,k) it appears only in the indicated order.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>