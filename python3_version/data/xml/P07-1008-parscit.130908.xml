<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.991671">
Making Lexical Ontologies Functional and Context-Sensitive
</title>
<author confidence="0.998234">
Tony Veale
</author>
<affiliation confidence="0.828089333333333">
Computer Science and Informatics
University College Dublin
Ireland
</affiliation>
<email confidence="0.997274">
tony.veale@ucd.ie
</email>
<author confidence="0.996387">
Yanfen Hao
</author>
<affiliation confidence="0.827836">
Computer Science and Informatics
University College Dublin
Ireland
</affiliation>
<email confidence="0.997361">
yanfen.hao@ucd.ie
</email>
<sectionHeader confidence="0.998586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977536">
Human categorization is neither a binary nor
a context-free process. Rather, some con-
cepts are better examples of a category than
others, while the criteria for category mem-
bership may be satisfied to different degrees
by different concepts in different contexts.
In light of these empirical facts, WordNet’s
static category structure appears both exces-
sively rigid and unduly fragile for process-
ing real texts. In this paper we describe a
syntagmatic, corpus-based approach to re-
defining WordNet’s categories in a func-
tional, gradable and context-sensitive fash-
ion. We describe how the diagnostic prop-
erties for these definitions are automati-
cally acquired from the web, and how the
increased flexibility in categorization that
arises from these redefinitions offers a ro-
bust account of metaphor comprehension
in the mold of Glucksberg’s (2001) the-
ory of category-inclusion. Furthermore, we
demonstrate how this competence with figu-
rative categorization can effectively be gov-
erned by automatically-generated ontologi-
cal constraints, also acquired from the web.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992116">
Linguistic variation across contexts is often symp-
tomatic of ontological differences between contexts.
These observable variations can serve as valuable
clues not just to the specific senses of words in con-
text (e.g., see Pustejovsky, Hanks and Rumshisky,
</bodyText>
<page confidence="0.986886">
57
</page>
<bodyText confidence="0.999899117647059">
2004) but to the underlying ontological structure it-
self (see Cimiano, Hotho and Staab, 2005). The
most revealing variations are syntagmatic in nature,
which is to say, they look beyond individual word
forms to larger patterns of contiguous usage (Hanks,
2004). In most contexts, the similarity between
chocolate, say, and a narcotic like heroin will mea-
gerly reflect the simple ontological fact that both are
kinds of substances; certainly, taxonomic measures
of similarity as discussed in Budanitsky and Hirst
(2006) will capture little more than this common-
ality. However, in a context in which the addictive
properties of chocolate are very salient (e.g., an on-
line dieting forum), chocolate is more likely to be
categorized as a drug and thus be considered more
similar to heroin. Look, for instance, at the simi-
lar ways in which these words can be used: one can
be ”chocolate-crazed” or ”chocolate-addicted” and
suffer ”chocolate-induced” symptoms (e.g., each of
these uses can be found in the pages of Wikipedia).
In a context that gives rise to these expressions, it is
unsurprising that chocolate should appear altogether
more similar to a harmful narcotic.
In this paper we computationally model this idea
that language use reflects category structure. As
noted by De Leenheer and de Moor (2005), ontolo-
gies are lexical representations of concepts, so we
can expect the effects of context on language use
to closely reflect the effects of context on ontolog-
ical structure. An understanding of the linguistic ef-
fects of context, as expressed through syntagmatic
patterns of word usage, should lead therefore to the
design of more flexible lexical ontologies that natu-
rally adapt to their contexts of use. WordNet (Fell-
</bodyText>
<subsectionHeader confidence="0.3255">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 57–64,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.990554893617021">
baum, 1998) is just one such lexical ontology that a kind of Prey, while ”X-covered”, ”X-dipped” and
can benefit greatly from the added flexibility that ”X-frosted” all indicate that X is a kind of Covering.
context-sensitivity can bring. Though comprehen- Likewise, ”army of X” suggests that a context views
sive in scale and widely used, WordNet suffers from X as a kind of Soldier, while ”barrage of X” suggests
an obvious structural rigidity in which concepts are that X should be seen as a kind of Projectile.
either entirely within a category or entirely outside We operationalize the collocation-type of adjec-
a category: no gradation of category membership tive and noun via the function (attr ADJ NOUN),
is allowed, and no contextual factors are brought to which returns a number in the range 0...1; this
bear on criteria for membership. Thus, a gun is al- represents the extent to which ADJ is used to
ways a weapon in WordNet while an axe is never so, modify NOUN in the context-defining corpus.
despite the uses (sporting or murderous) to which Dice’s coefficient (e.g., see Cimiano et al., 2005) is
each can be put. used to implement this measure. A context-sensitive
In section two we describe a computational category membership function can be defined, as in
framework for giving WordNet senses a functional, that for Fundamentalist in Figure 1:
context-sensitive form. These functional forms si- (define Fundamentalist.0 (arg0)
multaneously represent i) an intensional definition (* (max
for each word sense; ii) a structured query capable (%isa arg0 Person.0)
of retrieving instances of the corresponding category (%isa arg0 Group.0))
from a context-specific corpus; and iii) a member- (min
ship function that assigns gradated scores to these (max
instances based on available syntagmatic evidence. (attr political arg0)
In section three we describe how the knowledge re- (attr religious arg0))
quired to automate this functional re-definition is ac- (max
quired from the web and linked to WordNet. In sec- (attr extreme arg0)
tion four we describe how these re-definitions can (attr violent arg0)
produce a robust model of metaphor, before we eval- (attr radical arg0)))))
uate the descriptive sufficiency of this approach in Figure 1. A functional re-definition of the cat-
section five, comparing it to the knowledge already egory Fundamentalist.
available within WordNet. We conclude with some The function of Figure 1 takes, as a single ar-
final remarks in section six. gument arg0, a putative member of the category
2 Functional Context-Sensitive Categories Fundamentalist.0 (note how the sense tag, 0, is
We take a wholly textual view of context and as- used to identify a specific WordNet sense of ”fun-
sume that a given context can be implicitly charac- damentalist”), and returns a membership score in
terized by a representative text corpus. This corpus the range 0...1 for this term. This score reflects the
can be as large as a text archive or an encyclopedia syntagmatic evidence for considering arg0 to be
(e.g., the complete text of Wikipedia), or as small political or religious, as well as extreme or violent
as a single document, a sentence or even a single or radical. The function (%isa arg0 CAT) returns a
noun-phrase. For instance, the micro-context ”alco- value of 1.0 if some sense of arg0 is a descendent
holic apple-juice” is enough to implicate the cate- of CAT (here Person.0 or Group.0), otherwise 0.
gory Liquor, rather than Juice, as a semantic head, This safeguards ontological coherence and ensures
while ”lovable snake” can be enough of a context to that only kinds of people or groups can ever be
locally categorize Snake as a kind of Pet. There is a considered as fundamentalists.
range of syntagmatic patterns that one can exploit to The example of Figure 1 is hand-crafted, but a
glean category insights from a text. For instance, the functional form can be assigned automatically to
”X kills” pattern is enough to categorize X as a kind many of the synsets in WordNet by heuristic means.
of Killer, ”hunts X” is enough to categorize X as
58
</bodyText>
<figureCaption confidence="0.961376230769231">
For instance, those of Figure 2 are automatically non-zero taxonomic similarity to coffee can be
derived from WordNet’s morpho-semantic links: considered a kind of Espresso.
(define Fraternity.0 (arg0) Combining the contents of WordNet 1.6 and
(* (%sim arg0 Fraternity.0) WordNet 2.1, 27,732 different glosses (shared by
(max 51,035 unique word senses) can be shallow parsed to
(attr fraternal arg0) yield a definition of the kind shown in Figure 3. Of
(attr brotherly arg0)))) these, 4525 glosses yield two or more properties that
(define Orgasm.0 (arg0) can be given functional form via attr. However, one
(* (%sim arg0 Orgasm.0) can question whether these features are sufficient,
(max and more importantly, whether they are truly diag-
(attr climactic arg0) nostic of the categories they are used to define. In
(attr orgasmic arg0)))) the next section we consider another source of diag-
Figure 2. Exploiting the WordNet links be- nostic properties, explicit similes on the web, before,
</figureCaption>
<bodyText confidence="0.880478666666667">
tween nouns and their adjectival forms. in section 5, comparing the quality of these proper-
The function (%sim arg0 CAT) reflects the ties to those available from WordNet.
perceived similarity between the putative member 3 Diagnostic Properties on the Web
arg0 and a synset CAT in WordNet, using one of We employ the Google search engine as a retrieval
the standard formulations described in Budanitsky mechanism for acquiring the diagnostic properties
and Hirst (2006). Thus, any kind of group (e.g., a of categories from the web, since the Google API
glee club, a Masonic lodge, or a barbershop quartet) and its support for the wildcard term * allows this
described in a text as ”fraternal” or ”brotherly” process to be fully automated. The guiding intu-
(both occupy the same WordNet synset) can be ition here is that looking for explicit similes of the
considered a Fraternity to the corresponding degree, form ”X is as P as Y” is the surest way of finding
tempered by its a priori similarity to a Fraternity; the most salient properties of a term Y; with other
likewise, any climactic event can be categorized as syntagmatic patterns, such as adjective:noun collo-
an Orgasm to a more or less degree. cations, one cannot be sure that the adjective is cen-
Alternately, the function of Figure 3 is automat- tral to the noun.
ically obtained for the lexical concept Espresso by Since we expect that explicit similes will tend to
shallow parsing its WordNet gloss: ”strong black exploit properties that occupy an exemplary point on
coffee brewed by forcing steam under pressure a scale, we first extract a list of antonymous adjec-
through powdered coffee beans”. tives, such as ”hot” or ”cold”, from WordNet. For
(define Espresso.0 (arg0) every adjective ADJ on this list, we send the query
(* (%sim arg0 Espresso.0) ”as ADJ as *” to Google and scan the first 200 snip-
(min pets returned to extract different noun values for the
(attr strong arg0) wildcard *. From each set of snippets we can also
(attr black arg0)))) ascertain the relative frequencies of different noun
Figure 3. A functional re-definition of the cat- values for ADJ. The complete set of nouns extracted
egory Espresso based on its WordNet gloss. in this way is then used to drive a second phase of
It follows that any substance (e.g., oil or tea) the search, in which the query template ”as * as a
described locally as ”black” and ”strong” with a NOUN” is used to acquire similes that may have
59 lain beyond the 200-snippet horizon of the original
search, or that may hinge on adjectives not included
on the original list. Together, both phases collect
a wide-ranging series of core samples (of 200 hits
each) from across the web, yielding a set of 74,704
simile instances (of 42,618 unique types) relating
3769 different adjectives to 9286 different nouns too). Disambiguation is trivial for nouns with just
3.1 Property Filtering a single sense in WordNet. For nouns with two or
Unfortunately, many of these similes are not suffi- more fine-grained senses that are all taxonomically
ciently well-formed to identify salient properties. In close, such as ”gladiator” (two senses: a boxer and a
many cases, the noun value forms part of a larger combatant), we consider each sense to be a suitable
noun phrase: it may be the modifier of a compound target. In some cases, the WordNet gloss for as par-
noun (as in ”bread lover”), or the head of complex ticular sense will literally mention the adjective of
noun phrase (such as ”gang of thieves” or ”wound the simile, and so this sense is chosen. In all other
that refuses to heal”). In the former case, the com- cases, we employ a strategy of mutual disambigua-
pound is used if it corresponds to a compound term tion to relate the noun vehicle in each simile to a spe-
in WordNet and thus constitutes a single lexical unit; cific sense in WordNet. Two similes ”as A as N1”
if not, or if the latter case, the simile is rejected. and ”as A as N2” are mutually disambiguating if N1
Other similes are simply too contextual or under- and N2 are synonyms in WordNet, or if some sense
specified to function well in a null context, so if one of N1 is a hypernym or hyponym of some sense of
must read the original document to make sense of N2 in WordNet. For instance, the adjective ”scary”
the simile, it is rejected. More surprisingly, per- is used to describe both the noun ”rattler” and the
haps, a substantial number of the retrieved simi- noun ”rattlesnake” in bona-fide (non-ironic) similes;
les are ironic, in which the literal meaning of the since these nouns share a sense, we can assume that
simile is contrary to the meaning dictated by com- the intended sense of ”rattler” is that of a danger-
mon sense. For instance, ”as hairy as a bowling ous snake rather than a child’s toy. Similarly, the
ball” (found once) is an ironic way of saying ”as adjective ”brittle” is used to describe both saltines
hairless as a bowling ball” (also found just once). and crackers, suggesting that it is the bread sense of
Many ironies can only be recognized using world ”cracker” rather than the hacker, firework or hillbilly
knowledge, such as ”as sober as a Kennedy” and ”as senses (all in WordNet) that is intended.
tanned as an Irishman”. These heuristics allow us to automatically disam-
Given the creativity involved in these construc- biguate 10,378 bona-fide simile types (85%), yield-
tions, one cannot imagine a reliable automatic fil- ing a mapping of 2124 adjectives to 3778 different
ter to safely identify bona-fide similes. For this WordNet senses. Likewise, 77% (or 2164) of the
reason, the filtering task is performed by a human simile types annotated as ironic are disambiguated
judge, who annotated 30,991 of these simile in- automatically. A remarkable stability is observed in
stances (for 12,259 unique adjective/noun pairings) the alignment of noun vehicles to WordNet senses:
as non-ironic and meaningful in a null context; these 100% of the ironic vehicles always denote the same
similes relate a set of 2635 adjectives to a set of sense, no matter the adjective involved, while 96%
4061 different nouns. In addition, the judge also of bona-fide vehicles always denote the same sense.
annotated 4685 simile instances (of 2798 types) as This stability suggests two conclusions: the dis-
ironic; these similes relate a set of 936 adjectives ambiguation process is consistent and accurate; but
to a set of 1417 nouns. Perhaps surprisingly, ironic more intriguingly, only one coarse-grained sense of
pairings account for over 13% of all annotated sim- any word is likely to be sufficiently exemplary of
ile instances and over 20% of all annotated types. some property to be useful in a simile.
3.2 Linking to WordNet Senses 4 From Similes to Category Functions
To create functional WordNet definitions from these As noted in section 3, the filtered web data yields
adjective:noun pairings, we first need to identify the 12,259 bona-fide similes describing 4061 target
WordNet sense of each noun. For instance, ”as stiff nouns in terms of 2635 different adjectival prop-
as a zombie” might refer either to a re-animated erties. Word-sense disambiguation allows 3778
corpse or to an alcoholic cocktail (both are senses synsets in WordNet to be given a functional re-
of ”zombie” in WordNet, and drinks can be ”stiff” definition in terms of 2124 diagnostic properties, as
60
in the definition of Gladiator in Figure 4: be called ”as black as espresso” or ”as strong
(define Gladiator.0 (arg0) as espresso”, yet few such things can meaning-
(* (%isa arg0 Person.0) fully be called just ”espresso”. While simile is a
(* (%sim arg0 Gladiator.0) mechanism for highlighting inter-concept similarity,
(combine metaphor is at heart a mechanism of category inclu-
(attr strong arg0) sion (see Glucksberg, 2001). As the espresso exam-
(attr violent arg0) ple demonstrates, category inclusion is more than a
(attr manly arg0))))) matter of shared properties: humans have strong in-
Figure 4. A web-based definition of Gladiator. tuitions about the structure of categories and the ex-
Since we cannot ascertain from the web data tent to which they can be stretched to include new
which properties are necessary and which are members. So while it is sensible to apply the cat-
collectively sufficient, we use the function combine egory Espresso to other substances, preferably liq-
to aggregate the available evidence. This function uids, it seems nonsensical to apply the category to
implements a naive probabilistic or, in which each animals, artifacts, places and so on.
piece of syntagmatic evidence is naively assumed to Much as the salient properties of categories can
be independent, as follows: be acquired form the web (see section 3), so too
(combine e0 e1) = e0 + e1(1 − e0) can the intuitions governing inclusion amongst cat-
(combine e0 e1...e�) = (combine e0 (combine e1...e�)) egories. For instance, an attested web-usage of the
Thus, any combatant or competitor (such as a phrase ”Espresso-like CAT” tells us that sub-types
sportsman) that is described as strong, violent or of CAT are allowable targets of categorization by the
manly in a corpus can be categorized as a Gladiator category Espresso. Thus, since the query ”espresso-
in that context; the more properties that hold, and like substance” returns 3 hits via Google, types of
the greater the degree to which they hold, the greater substance (oil, etc.) can be described as Espresso if
the membership score that is assigned. they are contextually strong and black. In contrast,
The source of the hard taxonomic constraint the query ”espresso-like person” returns 0 hits, so
(%isa arg0 Person.0) is explained in the next sec- no instance of person can be described as Espresso,
tion. For now, note how the use of %sim in the no matter how black or how strong. While this is
functions of Figures 2, 3 and 4 means that these clearly a heuristic approach to a complex cognitive
membership functions readily admit both literal and problem, it does allow us to tap into the tacit knowl-
metaphoric members. Since the line between lit- edge that humans employ in categorization. More
eral and metaphoric uses of a category is often im- generally, a concept X can be included in a category
possible to draw, the best one can do is to accept C if X exhibits salient properties of C and, for some
metaphor as a gradable phenomenon (see Hanks, hypernym H of X in WordNet, we can find an at-
2006). The incorporation of taxonomic similarity tested use of ”C-like H” on the web.
via %sim ensures that literal members will tend to If we can pre-fetch all possible ”C-like H”
receive higher membership scores, and that the most from the web, this will allow comprehension to
tenuous metaphors will receive the lowest member- proceed without having to resort to web analysis
ship scores (close to 0.0). in mid-categorization. While there are too many
4.1 Constrained Category Inclusion possible values of H to make full pre-fetching a
Simile and metaphor involve quite different con- practical reality, we can generalize the problem
ceptual mechanisms. For instance, anything that somewhat, by selecting a range of values for H
is particularly strong or black might meaningfully from the middle-layer of WordNet, such as Person,
61 Substance, Animal, Tool, Plant, Structure, Event,
Vehicle, Idea and Place, and by pre-fetching the
query ”C-like H” for all 4061 nouns collected in
section 3, combined with this limited set of H
values. For every noun in our database then, we pre-
compile a vector of possible category inclusions.
For instance, ”lattice” yields the following vector:
</bodyText>
<equation confidence="0.9523475">
{structure(1620), substance(8), container(1),
vehicle(1)}
</equation>
<bodyText confidence="0.999989333333333">
where numbers in parentheses indicate the web-
frequency of the corresponding ”Lattice-like H”
query. Thus, the category Lattice can be used to
describe (and metaphorically include) other kinds
of structure (like crystals), types of substance (e.g.,
crystalline substances), containers (like honey-
combs) and even vehicles (e.g., those with many
compartments). Likewise, the noun ”snake” yields
the following vector of possibilities:
</bodyText>
<equation confidence="0.6380425">
{structure(125), animal(122), person(56), ve-
hicle(17), tool(9)}
</equation>
<bodyText confidence="0.9967591">
(note, the frequency for ”person” includes the
frequency for ”man” and ”woman”). The category
Snake can also be used to describe and include
structures (like tunnels), other animals (like eels),
people (e.g., the dishonest variety), vehicles (e.g.,
articulated trucks, trains) and tools (e.g., hoses). The
noun ”gladiator” yields a vector of just one element,
{person(1)}, from which the simple constraint
(%isa arg0 Person.0) in Figure 4 is derived. In con-
trast, ”snake” is now given the definition of Figure 5:
</bodyText>
<equation confidence="0.985834625">
(define Snake.0 (arg0)
(* (max
(%isa arg0 Structure.0)
(%isa arg0 Animal.0)
(%isa arg0 Person.0)
(%isa arg0 Vehicle.0))
(* (%sim arg0 Snake.0)
(combine
</equation>
<construct confidence="0.572053">
(attr cunning arg0)
(attr slippery arg0)
(attr flexible arg0)
(attr slim arg0)
(attr sinuous arg0)
(attr crooked arg0)
(attr deadly arg0)
(attr poised arg0)))))
</construct>
<figureCaption confidence="0.9975035">
Figure 5. A membership function for Snake
using web-derived category-inclusion constraints.
</figureCaption>
<bodyText confidence="0.99951005882353">
Glucksberg (2001) notes that the same category,
used figuratively, can exhibit different qualities in
different metaphors. For instance, Snake might
describe a kind of crooked person in one metaphor,
a poised killer in another metaphor, and a kind of
flexible tool in yet another. The use of combine
in Figure 5 means that a single category definition
can give rise to each of these perspectives in the
appropriate contexts. We therefore do not need a
different category definition for each metaphoric
use of Snake.
To illustrate the high-level workings of category-
inclusion, Table 1 generalizes over the set of 3778
disambiguated nouns from section 3 to estimate the
propensity for one semantic category, like Person, to
include members of another category, like Animal,
in X-like Y constructs.
</bodyText>
<table confidence="0.9944625">
X-like Y P A Sub T Str
(P)erson .66 .05 .03 .04 .09
(A)nimal .36 .27 .04 .05 .15
(Sub)stance .14 .03 .37 .05 .32
(T)ool .08 .03 .07 .22 .34
(Str)ucture .04 .03 .03 .03 .43
</table>
<tableCaption confidence="0.984274333333333">
Table 1. The Likelihood of a category X accommo-
dating a category Y.
Table 1 reveals that 36% of ”ANIMAL-like”
</tableCaption>
<bodyText confidence="0.9929820625">
patterns on the web describe a kind of Person,
while only 5% of ”PERSON-like” patterns on the
web describe a kind of Animal. Category inclusion
appears here to be a conservative mechanism, with
like describing like in most cases; thus, types of
Person are most often used to describe other kinds
of Person (comprising 66% of ”PERSON-like”
patterns), types of substance to describe other sub-
stances, and so on. The clear exception is Animal,
with ”ANIMAL-like” phrases more often used to
describe people (36%) than other kinds of animal
(27%). The anthropomorphic uses of this category
demonstrate the importance of folk-knowledge in
figurative categorization, of the kind one is more
likely to find in real text, and on the web (as in
section 3), rather than in resources like WordNet.
</bodyText>
<page confidence="0.991406">
62
</page>
<figure confidence="0.604549">
5 Empirical Evaluation A. Adjectives derived from annotated bona-fide
(non-ironic) similes only.
</figure>
<bodyText confidence="0.9636093625">
The simile gathering process of section 3, abetted
by Google’s practice of ranking pages according to
popularity, should reveal the most frequently-used
comparative nouns, and thus, the most useful cat-
egories to capture in a general-purpose ontology
like WordNet. But the descriptive sufficiency of
these categories is not guaranteed unless the defin-
ing properties ascribed to each can be shown to
be collectively rich enough, and individually salient
enough, to predict how each category is perceived
and applied by a language user.
If similes are indeed a good basis for mining
the most salient and diagnostic properties of cate-
gories, we should expect the set of properties for
each category to accurately predict how the cate-
gory is perceived as a whole. For instance, humans
– unlike computers – do not generally adopt a dis-
passionate view of ideas, but rather tend to asso-
ciate certain positive or negative feelings, or affec-
tive values, with particular ideas. Unsavoury activi-
ties, people and substances generally possess a nega-
tive affect, while pleasant activities and people pos-
sess a positive affect. Whissell (1989) reduces the
notion of affect to a single numeric dimension, to
produce a dictionary of affect that associates a nu-
meric value in the range 1.0 (most unpleasant) to 3.0
(most pleasant) with over 8000 words in a range of
syntactic categories (including adjectives, verbs and
nouns). So to the extent that the adjectival proper-
ties yielded by processing similes paint an accurate
picture of each category / noun-sense, we should be
able to predict the affective rating of each vehicle
via a weighted average of the affective ratings of
the adjectival properties ascribed to these nouns (i.e.,
where the affect rating of each adjective contributes
to the estimated rating of a noun in proportion to
its frequency of co-occurrence with that noun in our
simile data). More specifically, we should expect
that ratings estimated via these simile-derived prop-
erties should correlate well with the independent rat-
ings contained in Whissell’s dictionary.
To determine whether similes do offer the clearest
perspective on a category’s most salient properties,
we calculate and compare this correlation using the
following data sets:
B. Adjectives derived from all annotated similes
(both ironic and non-ironic).
C. Adjectives derived from ironic similes only.
D. All adjectives used to modify a given noun in
a large corpus. We use over 2-gigabytes of
text from the online encyclopaedia Wikipedia
as our corpus.
E. The set of 63,935 unique property-of-noun
pairings extracted via shallow-parsing from
WordNet glosses in section 2, e.g., strong and
black for Espresso.
Predictions of affective rating were made from each
of these data sources and then correlated with the
ratings reported in Whissell’s dictionary of affect
using a two-tailed Pearson test (p &lt; 0.01). As ex-
pected, property sets derived from bona-fide simi-
les only (A) yielded the best correlation (+0.514)
while properties derived from ironic similes only
(C) yielded the worst (-0.243); a middling corre-
lation coefficient of 0.347 was found for all simi-
les together, demonstrating the fact that bona-fide
similes outnumber ironic similes by a ratio of 4
to 1. A weaker correlation of 0.15 was found us-
ing the corpus-derived adjectival modifiers for each
noun (D); while this data provides quite large prop-
erty sets for each noun, these properties merely re-
flect potential rather than intrinsic properties of each
noun and so do not reveal what is most diagnostic
about a category. More surprisingly, property sets
derived from WordNet glosses (E) are also poorly
predictive, yielding a correlation with Whissell’s af-
fect ratings of just 0.278. This suggests that the
properties used to define categories in hand-crafted
resources like WordNet are not always those that ac-
tually reflect how humans think of these categories.
</bodyText>
<sectionHeader confidence="0.991648" genericHeader="introduction">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9994148">
Much of what we understand about different cate-
gories is based on tacit and defeasible knowledge of
the outside world, knowledge that cannot easily be
shoe-horned into the rigid is-a structure of an on-
tology like WordNet. This already-complex picture
</bodyText>
<page confidence="0.998156">
63
</page>
<bodyText confidence="0.999994368421053">
is complicated even further by the often metaphoric
relationship between words and the categories they
denote, and by the fact that the metaphor/literal dis-
tinction is not binary but gradable. Furthermore, the
gradability of category membership is clearly influ-
enced by context: in a corpus describing the exploits
of Vikings, an axe will most likely be seen as a kind
of weapon, but in a corpus dedicated to forestry, it
will likely describe a tool. A resource like WordNet,
in which is-a links are reserved for category relation-
ships that are always true, in any context, is going to
be inherently limited when dealing with real text.
We have described an approach that can be seen as
a functional equivalent to the CPA (Corpus Pattern
Analysis) approach of Pustejovsky et al. (2004), in
which our goal is not that of automated induction of
word senses in context (as it is in CPA) but the au-
tomated induction of flexible, context-sensitive cat-
egory structures. As such, our goal is primarily on-
tological rather than lexicographic, though both ap-
proaches are complementary since each views syn-
tagmatic evidence as the key to understanding the
use of lexical concepts in context. By defining cat-
egory membership in terms of syntagmatic expec-
tations, we establish a functional and gradable ba-
sis for determining whether one lexical concept (or
synset) in WordNet deserves to be seen as a de-
scendant of another in a particular corpus and con-
text. Augmented with ontological constraints de-
rived from the usage of ”X-like Y” patterns on the
web, we also show how these membership functions
can implement Glucksberg’s (2001) theory of cate-
gory inclusion.
We have focused on just one syntagmatic pattern
here – adjectival modification of nouns – but cate-
gorization can be inferred from a wide range of pro-
ductive patterns in text, particularly those concern-
ing verbs and their case-fillers. For instance, verb-
centred similes of the form ”to V+inf like alan N”
and ”to be V+past like alan N” reveal insights into
the diagnostic behaviour of entities (e.g., that preda-
tors hunt, that prey is hunted, that eagles soar and
bombs explode). Taken together, adjective-based
properties and verb-based behaviours can paint an
even more comprehensive picture of each lexical
concept, so that e.g., political agents that kill can
be categorized as assassins, loyal entities that fight
can be categorized as soldiers, and so on. An im-
portant next step, then, is to mine these behaviours
from the web and incorporate the corresponding
syntagmatic expectations into our category defini-
tions. The symbolic nature of the resulting defini-
tions means these can serve not just as mathematical
membership functions, but as ”active glosses”, capa-
ble of recruiting their own members in a particular
context while demonstrating a flexibility with cate-
gorization and a genuine competence with metaphor.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981403125">
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Semantic
Relatedness. Computational Linguistics, 32(1), pp 13-
47.
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA.
Cynthia Whissell. 1989. The dictionary of affect in lan-
guage. In R. Plutchnik &amp; H. Kellerman (Eds.). Emo-
tion: Theory and research. New York, Harcourt
Brace, 113-131.
James Pustejovsky, Patrick Hanks and Anna Rumshisky.
2004. Automated Induction of Sense in Context. In
Proceedings of COLING 2004, Geneva, pp 924-931.
Patrick Hanks. 2006. Metaphoricity is a Gradable. In A.
Stefanowitsch and S. Gries (eds.). Corpora in Cog-
nitive Linguistics. Vol. 1: Metaphor and Metonymy.
Berlin: Mouton.
Patrick Hanks. 2004. The syntagmatics of metaphor and
idiom. International Journal ofLexicography, 17(3).
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning Concept Hierarchies from Text Cor-
pora using Formal Concept Analysis. Journal of AI
Research, 24: 305-339.
Pieter De Leenheer and Aldo de Moor. 2005. Context-
driven Disambiguation in Ontology Elicitation. In
Shvaiko P. &amp; Euzenat J. (eds.), Context and Ontolo-
gies: Theory, Practice and Applications, AAAI Tech
Report WS-05-01. AAAI Press, pp 17-24.
Sam Glucksberg. 2001. Understanding figurative lan-
guage: From metaphors to idioms. Oxford: Oxford
University Press.
</reference>
<page confidence="0.999415">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.208298">
<title confidence="0.999749">Making Lexical Ontologies Functional and Context-Sensitive</title>
<author confidence="0.999993">Tony Veale</author>
<affiliation confidence="0.9991925">Computer Science and Informatics University College Dublin</affiliation>
<address confidence="0.521556">Ireland</address>
<email confidence="0.947249">tony.veale@ucd.ie</email>
<author confidence="0.99871">Yanfen Hao</author>
<affiliation confidence="0.851176333333333">Computer Science and Informatics University College Dublin Ireland</affiliation>
<email confidence="0.98355">yanfen.hao@ucd.ie</email>
<abstract confidence="0.986210807692308">Human categorization is neither a binary nor a context-free process. Rather, some concepts are better examples of a category than others, while the criteria for category membership may be satisfied to different degrees by different concepts in different contexts. In light of these empirical facts, WordNet’s static category structure appears both excessively rigid and unduly fragile for processing real texts. In this paper we describe a syntagmatic, corpus-based approach to redefining WordNet’s categories in a functional, gradable and context-sensitive fashion. We describe how the diagnostic properties for these definitions are automatically acquired from the web, and how the increased flexibility in categorization that arises from these redefinitions offers a robust account of metaphor comprehension in the mold of Glucksberg’s (2001) theory of category-inclusion. Furthermore, we demonstrate how this competence with figurative categorization can effectively be governed by automatically-generated ontological constraints, also acquired from the web.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based Measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>13--47</pages>
<contexts>
<context position="2115" citStr="Budanitsky and Hirst (2006)" startWordPosition="305" endWordPosition="308">le clues not just to the specific senses of words in context (e.g., see Pustejovsky, Hanks and Rumshisky, 57 2004) but to the underlying ontological structure itself (see Cimiano, Hotho and Staab, 2005). The most revealing variations are syntagmatic in nature, which is to say, they look beyond individual word forms to larger patterns of contiguous usage (Hanks, 2004). In most contexts, the similarity between chocolate, say, and a narcotic like heroin will meagerly reflect the simple ontological fact that both are kinds of substances; certainly, taxonomic measures of similarity as discussed in Budanitsky and Hirst (2006) will capture little more than this commonality. However, in a context in which the addictive properties of chocolate are very salient (e.g., an online dieting forum), chocolate is more likely to be categorized as a drug and thus be considered more similar to heroin. Look, for instance, at the similar ways in which these words can be used: one can be ”chocolate-crazed” or ”chocolate-addicted” and suffer ”chocolate-induced” symptoms (e.g., each of these uses can be found in the pages of Wikipedia). In a context that gives rise to these expressions, it is unsurprising that chocolate should appea</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1), pp 13-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum (ed.). 1998. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Whissell</author>
</authors>
<title>The dictionary of affect in language. In</title>
<date>1989</date>
<pages>113--131</pages>
<location>New York, Harcourt Brace,</location>
<contexts>
<context position="24759" citStr="Whissell (1989)" startWordPosition="4028" endWordPosition="4029">d by a language user. If similes are indeed a good basis for mining the most salient and diagnostic properties of categories, we should expect the set of properties for each category to accurately predict how the category is perceived as a whole. For instance, humans – unlike computers – do not generally adopt a dispassionate view of ideas, but rather tend to associate certain positive or negative feelings, or affective values, with particular ideas. Unsavoury activities, people and substances generally possess a negative affect, while pleasant activities and people possess a positive affect. Whissell (1989) reduces the notion of affect to a single numeric dimension, to produce a dictionary of affect that associates a numeric value in the range 1.0 (most unpleasant) to 3.0 (most pleasant) with over 8000 words in a range of syntactic categories (including adjectives, verbs and nouns). So to the extent that the adjectival properties yielded by processing similes paint an accurate picture of each category / noun-sense, we should be able to predict the affective rating of each vehicle via a weighted average of the affective ratings of the adjectival properties ascribed to these nouns (i.e., where the</context>
</contexts>
<marker>Whissell, 1989</marker>
<rawString>Cynthia Whissell. 1989. The dictionary of affect in language. In R. Plutchnik &amp; H. Kellerman (Eds.). Emotion: Theory and research. New York, Harcourt Brace, 113-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Anna Rumshisky</author>
</authors>
<title>Automated Induction of Sense in Context.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004, Geneva,</booktitle>
<pages>924--931</pages>
<contexts>
<context position="28599" citStr="Pustejovsky et al. (2004)" startWordPosition="4645" endWordPosition="4648">tion is not binary but gradable. Furthermore, the gradability of category membership is clearly influenced by context: in a corpus describing the exploits of Vikings, an axe will most likely be seen as a kind of weapon, but in a corpus dedicated to forestry, it will likely describe a tool. A resource like WordNet, in which is-a links are reserved for category relationships that are always true, in any context, is going to be inherently limited when dealing with real text. We have described an approach that can be seen as a functional equivalent to the CPA (Corpus Pattern Analysis) approach of Pustejovsky et al. (2004), in which our goal is not that of automated induction of word senses in context (as it is in CPA) but the automated induction of flexible, context-sensitive category structures. As such, our goal is primarily ontological rather than lexicographic, though both approaches are complementary since each views syntagmatic evidence as the key to understanding the use of lexical concepts in context. By defining category membership in terms of syntagmatic expectations, we establish a functional and gradable basis for determining whether one lexical concept (or synset) in WordNet deserves to be seen as</context>
</contexts>
<marker>Pustejovsky, Hanks, Rumshisky, 2004</marker>
<rawString>James Pustejovsky, Patrick Hanks and Anna Rumshisky. 2004. Automated Induction of Sense in Context. In Proceedings of COLING 2004, Geneva, pp 924-931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>Metaphoricity is a Gradable.</title>
<date>2006</date>
<booktitle>Corpora in Cognitive Linguistics. Vol. 1: Metaphor and Metonymy.</booktitle>
<editor>In A. Stefanowitsch and S. Gries (eds.).</editor>
<location>Berlin: Mouton.</location>
<marker>Hanks, 2006</marker>
<rawString>Patrick Hanks. 2006. Metaphoricity is a Gradable. In A. Stefanowitsch and S. Gries (eds.). Corpora in Cognitive Linguistics. Vol. 1: Metaphor and Metonymy. Berlin: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>The syntagmatics of metaphor and idiom.</title>
<date>2004</date>
<journal>International Journal ofLexicography,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="1857" citStr="Hanks, 2004" startWordPosition="268" endWordPosition="269"> automatically-generated ontological constraints, also acquired from the web. 1 Introduction Linguistic variation across contexts is often symptomatic of ontological differences between contexts. These observable variations can serve as valuable clues not just to the specific senses of words in context (e.g., see Pustejovsky, Hanks and Rumshisky, 57 2004) but to the underlying ontological structure itself (see Cimiano, Hotho and Staab, 2005). The most revealing variations are syntagmatic in nature, which is to say, they look beyond individual word forms to larger patterns of contiguous usage (Hanks, 2004). In most contexts, the similarity between chocolate, say, and a narcotic like heroin will meagerly reflect the simple ontological fact that both are kinds of substances; certainly, taxonomic measures of similarity as discussed in Budanitsky and Hirst (2006) will capture little more than this commonality. However, in a context in which the addictive properties of chocolate are very salient (e.g., an online dieting forum), chocolate is more likely to be categorized as a drug and thus be considered more similar to heroin. Look, for instance, at the similar ways in which these words can be used: </context>
</contexts>
<marker>Hanks, 2004</marker>
<rawString>Patrick Hanks. 2004. The syntagmatics of metaphor and idiom. International Journal ofLexicography, 17(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Andreas Hotho</author>
<author>Steffen Staab</author>
</authors>
<title>Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis.</title>
<date>2005</date>
<journal>Journal of AI Research,</journal>
<volume>24</volume>
<pages>305--339</pages>
<contexts>
<context position="4616" citStr="Cimiano et al., 2005" startWordPosition="714" endWordPosition="717">een as a kind of Projectile. either entirely within a category or entirely outside We operationalize the collocation-type of adjeca category: no gradation of category membership tive and noun via the function (attr ADJ NOUN), is allowed, and no contextual factors are brought to which returns a number in the range 0...1; this bear on criteria for membership. Thus, a gun is al- represents the extent to which ADJ is used to ways a weapon in WordNet while an axe is never so, modify NOUN in the context-defining corpus. despite the uses (sporting or murderous) to which Dice’s coefficient (e.g., see Cimiano et al., 2005) is each can be put. used to implement this measure. A context-sensitive In section two we describe a computational category membership function can be defined, as in framework for giving WordNet senses a functional, that for Fundamentalist in Figure 1: context-sensitive form. These functional forms si- (define Fundamentalist.0 (arg0) multaneously represent i) an intensional definition (* (max for each word sense; ii) a structured query capable (%isa arg0 Person.0) of retrieving instances of the corresponding category (%isa arg0 Group.0)) from a context-specific corpus; and iii) a member- (min</context>
</contexts>
<marker>Cimiano, Hotho, Staab, 2005</marker>
<rawString>Philipp Cimiano, Andreas Hotho, and Steffen Staab. 2005. Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis. Journal of AI Research, 24: 305-339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pieter De Leenheer</author>
<author>Aldo de Moor</author>
</authors>
<title>Contextdriven Disambiguation in Ontology Elicitation.</title>
<date>2005</date>
<booktitle>Context and Ontologies: Theory, Practice and Applications, AAAI Tech Report WS-05-01.</booktitle>
<pages>17--24</pages>
<editor>In Shvaiko P. &amp; Euzenat J. (eds.),</editor>
<publisher>AAAI Press,</publisher>
<marker>De Leenheer, de Moor, 2005</marker>
<rawString>Pieter De Leenheer and Aldo de Moor. 2005. Contextdriven Disambiguation in Ontology Elicitation. In Shvaiko P. &amp; Euzenat J. (eds.), Context and Ontologies: Theory, Practice and Applications, AAAI Tech Report WS-05-01. AAAI Press, pp 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Glucksberg</author>
</authors>
<title>Understanding figurative language: From metaphors to idioms. Oxford:</title>
<date>2001</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="16388" citStr="Glucksberg, 2001" startWordPosition="2678" endWordPosition="2679">o an alcoholic cocktail (both are senses synsets in WordNet to be given a functional reof ”zombie” in WordNet, and drinks can be ”stiff” definition in terms of 2124 diagnostic properties, as 60 in the definition of Gladiator in Figure 4: be called ”as black as espresso” or ”as strong (define Gladiator.0 (arg0) as espresso”, yet few such things can meaning(* (%isa arg0 Person.0) fully be called just ”espresso”. While simile is a (* (%sim arg0 Gladiator.0) mechanism for highlighting inter-concept similarity, (combine metaphor is at heart a mechanism of category inclu(attr strong arg0) sion (see Glucksberg, 2001). As the espresso exam(attr violent arg0) ple demonstrates, category inclusion is more than a (attr manly arg0))))) matter of shared properties: humans have strong inFigure 4. A web-based definition of Gladiator. tuitions about the structure of categories and the exSince we cannot ascertain from the web data tent to which they can be stretched to include new which properties are necessary and which are members. So while it is sensible to apply the catcollectively sufficient, we use the function combine egory Espresso to other substances, preferably liqto aggregate the available evidence. This </context>
<context position="21685" citStr="Glucksberg (2001)" startWordPosition="3522" endWordPosition="3523">tor” yields a vector of just one element, {person(1)}, from which the simple constraint (%isa arg0 Person.0) in Figure 4 is derived. In contrast, ”snake” is now given the definition of Figure 5: (define Snake.0 (arg0) (* (max (%isa arg0 Structure.0) (%isa arg0 Animal.0) (%isa arg0 Person.0) (%isa arg0 Vehicle.0)) (* (%sim arg0 Snake.0) (combine (attr cunning arg0) (attr slippery arg0) (attr flexible arg0) (attr slim arg0) (attr sinuous arg0) (attr crooked arg0) (attr deadly arg0) (attr poised arg0))))) Figure 5. A membership function for Snake using web-derived category-inclusion constraints. Glucksberg (2001) notes that the same category, used figuratively, can exhibit different qualities in different metaphors. For instance, Snake might describe a kind of crooked person in one metaphor, a poised killer in another metaphor, and a kind of flexible tool in yet another. The use of combine in Figure 5 means that a single category definition can give rise to each of these perspectives in the appropriate contexts. We therefore do not need a different category definition for each metaphoric use of Snake. To illustrate the high-level workings of categoryinclusion, Table 1 generalizes over the set of 3778 </context>
</contexts>
<marker>Glucksberg, 2001</marker>
<rawString>Sam Glucksberg. 2001. Understanding figurative language: From metaphors to idioms. Oxford: Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>