<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025061">
<title confidence="0.998464">
Automatic Acquisition of English Topic Signatures Based on
a Second Language
</title>
<author confidence="0.999134">
Xinglong Wang
</author>
<affiliation confidence="0.995077">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.994084">
Brighton, BN1 9QH, UK
</address>
<email confidence="0.998735">
xw20@sussex.ac.uk
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963157894737">
We present a novel approach for auto-
matically acquiring English topic sig-
natures. Given a particular concept,
or word sense, a topic signature is a
set of words that tend to co-occur with
it. Topic signatures can be useful in a
number of Natural Language Process-
ing (NLP) applications, such as Word
Sense Disambiguation (WSD) and Text
Summarisation. Our method takes ad-
vantage of the different way in which
word senses are lexicalised in English
and Chinese, and also exploits the large
amount of Chinese text available in cor-
pora and on the Web. We evaluated the
topic signatures on a WSD task, where
we trained a second-order vector co-
occurrence algorithm on standard WSD
datasets, with promising results.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986944444444">
Lexical knowledge is crucial for many NLP tasks.
Huge efforts and investments have been made to
build repositories with different types of knowl-
edge. Many of them have proved useful, such as
WordNet (Miller et al., 1990). However, in some
areas, such as WSD, manually created knowledge
bases seem never to satisfy the huge requirement
by supervised machine learning systems. This
is the so-called knowledge acquisition bottleneck.
As an alternative, automatic or semi-automatic ac-
quisition methods have been proposed to tackle
the bottleneck. For example, Agirre et al. (2001)
tried to automatically extract topic signatures by
querying a search engine using monosemous syn-
onyms or other knowledge associated with a con-
cept defined in WordNet.
The Web provides further ways of overcoming
the bottleneck. Mihalcea et al. (1999) presented
a method enabling automatic acquisition of sense-
tagged corpora, based on WordNet and an Inter-
net search engine. Chklovski and Mihalcea (2002)
presented another interesting proposal which turns
to Web users to produce sense-tagged corpora.
Another type of method, which exploits dif-
ferences between languages, has shown great
promise. For example, some work has been done
based on the assumption that mappings of words
and meanings are different in different languages.
Gale et al. (1992) proposed a method which au-
tomatically produces sense-tagged data using par-
allel bilingual corpora. Diab and Resnik (2002)
presented an unsupervised method for WSD us-
ing the same type of resource. One problem with
relying on bilingual corpora for data collection is
that bilingual corpora are rare, and aligned bilin-
gual corpora are even rarer. Mining the Web for
bilingual text (Resnik, 1999) is not likely to pro-
vide sufficient quantities of high quality data. An-
other problem is that if two languages are closely
related, data for some words cannot be collected
because different senses of polysemous words in
one language often translate to the same word in
the other.
In this paper, we present a novel approach for
automatically acquiring topic signatures (see Ta-
ble 1 for an example of topic signatures), which
also adopts the cross-lingual paradigm. To solve
the problem of different senses not being distin-
guishable mentioned in the previous paragraph,
we chose a language very distant to English –
Chinese, since the more distant two languages
are, the more likely that senses are lexicalised
differently (Resnik and Yarowsky, 1999). Be-
cause our approach only uses Chinese monolin-
gual text, we also avoid the problem of shortage
of aligned bilingual corpora. We build the topic
signatures by using Chinese-English and English-
Chinese bilingual lexicons and a large amount of
Chinese text, which can be collected either from
the Web or from Chinese corpora. Since topic sig-
natures are potentially good training data for WSD
algorithms, we set up a task to disambiguate 6
words using a WSD algorithm similar to Sch¨utze’s
(1998) context-group discrimination. The results
show that our topic signatures are useful for WSD.
The remainder of the paper is organised as fol-
lows. Section 2 describes the process of acqui-
sition of the topic signatures. Section 3 demon-
strates the application of this resource on WSD,
and presents the results of our experiments. Sec-
tion 4 discusses factors that could affect the acqui-
sition process and then we conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.603866" genericHeader="method">
2 Acquisition of Topic Signatures
</sectionHeader>
<bodyText confidence="0.9997805">
A topic signature is defined as: TS =
{(t1, w1), ..., (ti, wi), ...}, where ti is a term
highly correlated to a target topic (or concept) with
association weight wi, which can be omitted. The
steps we perform to produce the topic signatures
are described below, and illustrated in Figure 1.
</bodyText>
<listItem confidence="0.959352625">
1. Translate an English ambiguous word w to Chinese,
using an English-Chinese lexicon. Given the assump-
tion we mentioned, each sense si of w maps to a dis-
tinct Chinese word1. At the end of this step, we have
produced a set C, which consists of Chinese words
{c1, c2, ..., cn}, where ci is the translation correspond-
ing to sense si of w, and n is the number of senses that
w has.
</listItem>
<footnote confidence="0.982781166666667">
2. Query large Chinese corpora or/and a search engine
that supports Chinese using each element in C. Then,
for each ci in C, we collect the text snippets retrieved
and construct a Chinese corpus.
1It is also possible that the English sense maps to a set of
Chinese synonyms that realise the same concept.
</footnote>
<figure confidence="0.972936705882353">
English ambiguous word ❑wEI
Sense 1 of ❑w11 English-Chinese❑ Sense 2 of ❑w1
Lexicon❑
Chinese translation of❑ Chinese translation of❑
sense 1❑ Chinese❑ sense 2❑
Search❑
Engine❑
1. Chinese document 1❑ Chinese❑ 1. Chinese document 1❑
2. Chinese document 2❑ segmentation❑ 2. Chinese document 2❑
... ...❑ and POS❑ ... ...❑
tagging;❑
Chinese-❑
English❑
Lexicon❑
1. {English topic signature 1}❑ 1. {English topic signature 1}❑
2. {English topic signature 2}❑ 2. {English topic signature 2}❑
... ...❑ ... ...❑
</figure>
<figureCaption confidence="0.973573">
Figure 1:Process of automatic acquisition of topic signatures.
For simplicity, we assume here that w has two senses.
</figureCaption>
<listItem confidence="0.807276">
3. Shallow process these Chinese corpora. Text segmen-
tation and POS tagging are done in this step.
4. Either use an electronic Chinese-English lexicon to
translate the Chinese corpora word by word to En-
glish, or use machine translation software to translate
the whole text. In our experiments, we did the former.
</listItem>
<bodyText confidence="0.99977872">
The complete process is automatic, and unsu-
pervised. At the end of this process, for each sense
si of an ambiguous word w, we have a large set
of English contexts. Each context is a topic sig-
nature, which represents topical information that
tends to co-occur with sense si. Note that an el-
ement in our topic signatures is not necessarily a
single English word. It can be a set of English
words which are translations of a Chinese word c.
For example, the component of a topic signature,
{vesture, clothing, clothes}, is translated from the
Chinese word AW Under the assumption that the
majority of c’s are unambiguous, which we dis-
cuss later, we refer to elements in a topic signature
as concepts in this paper.
Choosing an appropriate English-Chinese dic-
tionary is the first problem we faced. The one
we decided to use is the Yahoo! Student English-
Chinese On-line Dictionary2. As this dictionary
is designed for English learners, its sense gran-
ularity is far coarser-grained than that of Word-
Net. However, researchers argue that the granular-
ity of WordNet is too fine for many applications,
and some also proposed new evaluation standards.
For example, Resnik and Yarowsky (1999) sug-
</bodyText>
<footnote confidence="0.555849">
2See: http://cn.yahoo.com/dictionary/
</footnote>
<bodyText confidence="0.999962255813954">
gested that for the purpose of WSD, the different
senses of a word could be determined by consid-
ering only sense distinctions that are lexicalised
cross-linguistically. Our approach is in accord
with their proposal, since bilingual dictionaries in-
terpret sense distinctions crossing two languages.
For efficiency purposes, we extract our topic
signatures mainly from the Mandarin portion of
the Chinese Gigaword Corpus (CGC), produced
by the LDC3, which contains 1.3GB of newswire
text drawn from Xinhua newspaper. Some Chi-
nese translations of English word senses could be
sparse, making it impossible to extract sufficient
training data simply relying on CGC. In this sit-
uation, we can turn to the large amount of Chi-
nese text on the Web. There are many good search
engines and on-line databases supporting the Chi-
nese language. After investigation, we chose Peo-
ple’s Daily On-line4, which is the website for Peo-
ple’s Daily, one of the most influential newspaper
in mainland China. It maintains a vast database
of news stories, available to search by the public.
Among other reasons, we chose this website be-
cause its articles have similar quality and cover-
age to those in the CGC, so that we could com-
bine texts from these two resources to get a larger
amount of topic signatures. Note that we can al-
ways turn to other sources on the Web to retrieve
even more data, if needed.
For Chinese text segmentation and POS tag-
ging5 we adopted the freely-available software
package — ICTCLAS6. This system includes a
word segmenter, a POS tagger and an unknown-
word recogniser. The claimed precision of seg-
mentation is 97.58%, evaluated on a 1.2M word
portion of the People’s Daily Corpus.
To automatically translate the Chinese text back
to English, we used the electronic LDC Chinese-
English Translation Lexicon Version 3.0. An al-
ternative was to use machine translation software,
which would yield a rather different type of re-
source, but this is beyond the scope of this pa-
per. Then, we filtered the topic signatures with
</bodyText>
<footnote confidence="0.9999438">
3Available at: http://www.ldc.upenn.edu/Catalog/
4See: http://www.people.com.cn
5POS tagging can be omitted. We did it in our experi-
ments purely for convenience for error analysis in the future.
6See: http://mtgroup.ict.ac.cn/∼zhp/ICTCLAS/index.html
</footnote>
<bodyText confidence="0.999738466666667">
a stop-word list, to ensure only content words are
included in our final results.
One might argue that, since many Chinese
words are also ambiguous, a Chinese word may
have more than one English translation and thus
translated concepts in topic signatures would still
be ambiguous. This happens for some Chinese
words, and will inevitably affect the performance
of our system to some extent. A practical solu-
tion is to expand the queries with different descrip-
tions associated with each sense of w, normally
provided in a bilingual dictionary, when retriev-
ing the Chinese text. To get an idea of the baseline
performance, we did not follow this solution in our
experiments.
</bodyText>
<table confidence="0.991771454545454">
Topic signatures for the &amp;quot;flnancialM sense of &amp;quot;WerestM11
M 1. fate�;� �2.bond; 3.#�yment; 4. market ;N ftbtM dollar;❑
7. �bank�;��8. year; 9. loan; 10. �income�;��11.�company�;�
12. inflation; 13. reserve; 14. government; 15. ktonomyP
16.AtockM.IfundM. week; 19. security; 20. level;El
A 1. {bankD 2. {1danD 3. {0mpany�, firm, corporation};❑
4. {i*te}l 5. {deposit}; 6. {iHcomeptevenue}; 7. {fdndDE1
8. {bonus, divident}; 9. {investment}; 10. {market};
11. {tax, duty}; 12. {dtonomyD 13. {ftbtD 14. {money};❑
15. {saving}; 16. {profit}; 17. {bondD 18. {ihcome�earning};❑
19. {share, §tock}; 20. {finance, banking};❑
</table>
<tableCaption confidence="0.995942">
Table 1:A sample of our topic signatures. Signature M was
extracted from a manually-sense-tagged corpus and A was
produced by our algorithm. Words occurring in both A and
M are marked in bold.
</tableCaption>
<bodyText confidence="0.999757357142857">
The topic signatures we acquired contain rich
topical information. But they do not provide any
other types of linguistic knowledge. Since they
were created by word to word translation, syntac-
tic analysis of them is not possible. Even the dis-
tances between the target ambiguous word and its
context words are not reliable because of differ-
ences in word order between Chinese and English.
Table 1 lists two sets of topic signatures, each con-
taining the 20 most frequent nouns, ranked by oc-
currence count, that surround instances of the fi-
nancial sense of interest. One set was extracted
from a hand-tagged corpus (Bruce and Wiebe,
1994) and the other by our algorithm.
</bodyText>
<sectionHeader confidence="0.978116" genericHeader="method">
3 Application on WSD
</sectionHeader>
<bodyText confidence="0.9770708">
To evaluate the usefulness of the topic signatures
acquired, we applied them in a WSD task. We
adopted an algorithm similar to Sch¨utze’s (1998)
context-group discrimination, which determines a
word sense according to the semantic similarity
of contexts, computed using a second-order co-
occurrence vector model. In this section, we firstly
introduce our adaptation of this algorithm, and
then describe the disambiguation experiments on
6 words for which a gold standard is available.
</bodyText>
<subsectionHeader confidence="0.985869">
3.1 Context-Group Discrimination
</subsectionHeader>
<bodyText confidence="0.99158924137931">
We chose the so-called context-group discrimina-
tion algorithm because it disambiguates instances
only relying on topical information, which hap-
pens to be what our topic signatures specialise
in7. The original context-group discrimination
is a disambiguation algorithm based on cluster-
ing. Words, contexts and senses are represented
in Word Space, a high-dimensional, real-valued
space in which closeness corresponds to semantic
similarity. Similarity in Word Space is based on
second-order co-occurrence: two tokens (or con-
texts) of the ambiguous word are assigned to the
same sense cluster if the words they co-occur with
themselves occur with similar words in a training
corpus. The number of sense clusters determines
sense granularity.
In our adaptation of this algorithm, we omitted
the clustering step, because our data has already
been sense classified according to the senses de-
fined in the English-Chinese dictionary. In other
words, our algorithm performs sense classifica-
tion by using a bilingual lexicon and the level
of sense granularity of the lexicon determines the
sense distinctions that our system can handle: a
finer-grained lexicon would enable our system to
identify finer-grained senses. Also, our adapta-
tion represents senses in Concept Space, in con-
trast to Word Space in the original algorithm. This
is because our topic signatures are not realised in
the form of words, but concepts. For example, a
topic signature may consist of {duty, tariff, cus-
toms duty}, which represents a concept of “a gov-
ernment tax on imports or exports”.
A vector for concept c is derived from all the
close neighbours of c, where close neighbours re-
fer to all concepts that co-occur with c in a context
window. The size of the window is around 100
7Using our topic signatures as training data, other classi-
fication algorithms would also work on this WSD task.
words. The entry for concept c&apos; in the vector for
c records the number of times that c&apos; occurs close
to c in the corpus. It is this representational vector
space that we refer to as Concept Space.
In our experiments, we chose concepts that
serve as dimensions of Concept Space using a
frequency cut-off. We count the number of oc-
currences of any concepts that co-occur with the
ambiguous word within a context window. The
2, 500 most frequent concepts are chosen as the
dimensions of the space. Thus, the Concept Space
was formed by collecting a n-by-2, 500 matrix M,
such that element mid records the number of times
that concept i and j co-occur in a window, where
n is the number of concept vectors that occur in
the corpus. Row l of matrix M represents concept
vector l.
We measure the similarity of two vectors by the
cosine score:
</bodyText>
<equation confidence="0.9792055">
N 2 N 2
�i=1 vZ �i=1 wZ
</equation>
<bodyText confidence="0.99951488">
where v~ and w~ are vectors and N is the dimen-
sion of the vector space. The more overlap there
is between the neighbours of the two words whose
vectors are compared, the higher the score.
Contexts are represented as context vectors in
Concept Space. A context vector is the sum of the
vectors of concepts that occur in a context win-
dow. If many of the concepts in a window have a
strong component for one of the topics, then the
sum of the vectors, the context vector, will also
have a strong component for the topic. Hence, the
context vector indicates the strength of different
topical or semantic components in a context.
Senses are represented as sense vectors in Con-
cept Space. A vector of sense si is the sum of the
vectors of contexts in which the ambiguous word
realises si. Since our topic signatures are classi-
fied naturally according to definitions in a bilin-
gual dictionary, calculation of the vector for sense
si is fairly straightforward: simply sum all the vec-
tors of the contexts associated with sense si.
After the training phase, we have obtained a
sense vector ~vi for each sense si of an ambiguous
word w. Then, we perform the following steps to
tag an occurrence t of w:
</bodyText>
<equation confidence="0.827466">
N
corr (~v, ~w) = Ei=1 vi~wi
</equation>
<listItem confidence="0.937826444444445">
1. Compute the context vector c� for t in Concept Space
by summing the vectors of the concepts in t’s context.
Since the basic units of the test data are words rather
than concepts, we have to convert all words in the test
data into concepts. A simple way to achieve this is to
replace a word v with all the concepts that contain v.
2. Compute the cosine scores between all sense vectors of
w and c, and then assign t to the sense si whose sense
vector s,; is closest to c.
</listItem>
<subsectionHeader confidence="0.998572">
3.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999482076923077">
We tested our system on 6 nouns, as shown in Ta-
ble 2, which also shows information on the train-
ing and test data we used in the experiments. The
training sets for motion, plant and tank are topic
signatures extracted from the CGC; whereas those
for bass, crane and palm are obtained from both
CGC and the People’s Daily On-line. This is be-
cause the Chinese translation equivalents of senses
of the latter 3 words don’t occur frequently in
CGC, and we had to seek more data from the Web.
Where applicable, we also limited the training data
of each sense to a maximum of 6, 000 instances for
efficiency purposes.
</bodyText>
<table confidence="0.999679722222222">
Word� Sense� Training� Test� Precision[
&apos;Supervised&apos;�
Baseline[
bass� 1. fish� 418� 1203� 10� 107[ 90.7%11 93.5%11
2. music[ 825[1 97[
crane� 1. bird� 829� 2301� 24� 95El 74.7%11 76.6%11
2. machine[ 1472[1 71[
motion� 6000� 9265 141 201El 70.1%11 69.7%11
1. physical�
2. legal[ 3265El 60[
palm� 852� 12481 143 201El 71.1%11 76.1%11
1. hand�
2. tree[ 396[1 58[
plant� 1. living� 6000� 12000� 86� 188� 54.3%� 70.2%[]
2. factory[ 6000[1 102[
tank� 6000� 93461 126[ 201 E 62.7%11 70.1%11
1. container�
2. vehicle[ 3346[1 75[
</table>
<tableCaption confidence="0.970963">
Table 2:Sizes of the training data and the test data, baseline
performance, and the results.
</tableCaption>
<bodyText confidence="0.999995953488372">
The test data is a binary sense-tagged corpus,
the TWA Sense Tagged Data Set, manually pro-
duced by Rada Mihalcea and Li Yang (Mihalcea,
2003), from text drawn from the British National
Corpus. We calculated a ‘supervised’ baseline
from the annotated data by assigning the most fre-
quent sense in the test data to all instances, al-
though it could be argued that the baseline for un-
supervised disambiguation should be computed by
randomly assigning one of the senses to instances
(e.g. it would be 50% for words with two senses).
According to our previous description, the
2, 500 most frequent concepts were selected as di-
mensions. The number of features in a Concept
Space depends on how many unique concepts ac-
tually occur in the training sets. Larger amounts
of training data tend to yield a larger set of fea-
tures. At the end of the training stage, for each
sense, a sense vector was produced. Then we lem-
matised the test data and extracted a set of context
vectors for all instances in the same way. For each
instance in the test data, the cosine scores between
its context vector and all possible sense vectors ac-
quired through training were calculated and com-
pared, and then the sense scoring the highest was
allocated to the instance.
The results of the experiments are also given
in Table 2 (last column). Using our topic sig-
natures, we obtained good results: the accuracy
for all words exceeds the supervised baseline, ex-
cept for motion which approaches it. The Chi-
nese translations for motion are also ambiguous,
which might be the reason that our WSD system
performed less well on this word. However, as
we mentioned, to avoid this problem, we could
have expanded motion’s Chinese translations, us-
ing their Chinese monosemous synonyms, when
we query the Chinese corpus or the Web. Consid-
ering our system is unsupervised, the results are
very promising. An indicative comparison might
be with the work of Mihalcea (2003), who with
a very different approach achieved similar perfor-
mance on the same test data.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9999858">
Although these results are promising, higher qual-
ity topic signatures would probably yield better re-
sults in our WSD experiments. There are a num-
ber of factors that could affect the acquisition pro-
cess, which determines the quality of this resource.
Firstly, since the translation was achieved by look-
ing up in a bilingual dictionary, the deficiencies
of the dictionary could cause problems. For ex-
ample, the LDC Chinese-English Lexicon we used
is not up to date, for example, lacking entries for
words such as T-#L (mobile phone), 79-IKM (the
Internet), etc. This defect makes our WSD algo-
rithm unable to use the possibly strong topical in-
formation contained in those words. Secondly, er-
rors generated during Chinese segmentation could
affect the distributions of words. For example, a
Chinese string ABC may be segmented as either
A + BC or AB + C; assuming the former is cor-
rect whereas AB + C was produced by the seg-
menter, distributions of words A, AB, BC, and C
are all affected accordingly. Other factors such as
cultural differences reflected in the different lan-
guages could also affect the results of this knowl-
edge acquisition process.
In our experiments, we adopted Chinese as a
source language to retrieve English topic signa-
tures. Nevertheless, our technique should also
work on other distant language pairs, as long
as there are existing bilingual lexicons and large
monolingual corpora for the languages used. For
example, one should be able to build French topic
signatures using Chinese text, or Spanish topic
signatures from Japanese text. In particular cases,
where one only cares about translation ambiguity,
this technique can work on any language pair.
</bodyText>
<sectionHeader confidence="0.99104" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989666666667">
We presented a novel method for acquiring En-
glish topic signatures from large quantities of
Chinese text and English-Chinese and Chinese-
English bilingual dictionaries. The topic signa-
tures we acquired are a new type of resource,
which can be useful in a number of NLP applica-
tions. Experimental results have shown its appli-
cation to WSD is promising and the performance
is competitive with other unsupervised algorithms.
We intend to carry out more extensive evaluation
to further explore this new resource’s properties
and potential.
</bodyText>
<sectionHeader confidence="0.99655" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999470714285714">
This research is funded by EU IST-2001-
34460 project MEANING: Developing Multilin-
gual Web-Scale Language Technologies, and by
the Department of Informatics at Sussex Univer-
sity. I am very grateful to Dr John Carroll, my
supervisor, for his continual help and encourage-
ment.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984924">
Eneko Agirre, Olatz Ansa, David Martinez, and Ed-
uard Hovy. 2001. Enriching WordNet concepts
with topic signatures. In Proceedings of the NAACL
workshop on WordNet and Other Lexical Resources:
Applications, Extensions and Customizations. Pitts-
burgh, USA.
Rebecca Bruce and Janyce Wiebe. 1994. Word-sense
disambiguation using decomposable models. In
Proceedings of the 32nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 139–
146.
Timothy Chklovski and Rada Mihalcea. 2002. Build-
ing a sense tagged corpus with open mind word ex-
pert. In Proceedings of the ACL 2002 Workshop on
“Word Sense Disambiguation Recent Successes and
Future Directions”. Philadelphia, USA.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Anniversary Meet-
ing of the Association for Computational Linguistics
(ACL-02). Philadelphia, USA.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Using bilingual materials to
develop word sense disambiguation methods. In
Proceedings of the International Conference on
Theoretical and Methodological Issues in Machine
Translation, pages 101–112.
Rada Mihalcea and Dan I. Moldovan. 1999. An auto-
matic method for generating sense tagged corpora.
In Proceedings of the 16th Conference of the Amer-
ican Association ofArtificial Intelligence.
Rada Mihalcea. 2003. The role of non-ambiguous
words in natural language disambiguation. In Pro-
ceedings of the Conference on Recent Advances
in Natural Language Processing, RANLP 2003.
Borovetz, Bulgaria.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller.
1990. Introduction to WordNet: An on-line lexical
database. Journal ofLexicography, 3(4):235–244.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New evalua-
tion methods for word sense disambiguation. Natu-
ral Language Engineering, 5(2):113–133.
Philip Resnik. 1999. Mining the Web for bilingual
text. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883004">
<title confidence="0.9724085">Automatic Acquisition of English Topic Signatures Based on a Second Language</title>
<author confidence="0.999142">Xinglong Wang</author>
<affiliation confidence="0.9999055">Department of Informatics University of Sussex</affiliation>
<address confidence="0.999645">Brighton, BN1 9QH, UK</address>
<email confidence="0.98648">xw20@sussex.ac.uk</email>
<abstract confidence="0.99610875">We present a novel approach for automatically acquiring English topic signatures. Given a particular concept, or word sense, a topic signature is a set of words that tend to co-occur with it. Topic signatures can be useful in a number of Natural Language Processing (NLP) applications, such as Word Sense Disambiguation (WSD) and Text Summarisation. Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web. We evaluated the topic signatures on a WSD task, where we trained a second-order vector cooccurrence algorithm on standard WSD datasets, with promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>David Martinez</author>
<author>Eduard Hovy</author>
</authors>
<title>Enriching WordNet concepts with topic signatures.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations.</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="1486" citStr="Agirre et al. (2001)" startWordPosition="231" endWordPosition="234">ets, with promising results. 1 Introduction Lexical knowledge is crucial for many NLP tasks. Huge efforts and investments have been made to build repositories with different types of knowledge. Many of them have proved useful, such as WordNet (Miller et al., 1990). However, in some areas, such as WSD, manually created knowledge bases seem never to satisfy the huge requirement by supervised machine learning systems. This is the so-called knowledge acquisition bottleneck. As an alternative, automatic or semi-automatic acquisition methods have been proposed to tackle the bottleneck. For example, Agirre et al. (2001) tried to automatically extract topic signatures by querying a search engine using monosemous synonyms or other knowledge associated with a concept defined in WordNet. The Web provides further ways of overcoming the bottleneck. Mihalcea et al. (1999) presented a method enabling automatic acquisition of sensetagged corpora, based on WordNet and an Internet search engine. Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. Another type of method, which exploits differences between languages, has shown great promise. For e</context>
</contexts>
<marker>Agirre, Ansa, Martinez, Hovy, 2001</marker>
<rawString>Eneko Agirre, Olatz Ansa, David Martinez, and Eduard Hovy. 2001. Enriching WordNet concepts with topic signatures. In Proceedings of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations. Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>Word-sense disambiguation using decomposable models.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="11847" citStr="Bruce and Wiebe, 1994" startWordPosition="1926" endWordPosition="1929">gnatures we acquired contain rich topical information. But they do not provide any other types of linguistic knowledge. Since they were created by word to word translation, syntactic analysis of them is not possible. Even the distances between the target ambiguous word and its context words are not reliable because of differences in word order between Chinese and English. Table 1 lists two sets of topic signatures, each containing the 20 most frequent nouns, ranked by occurrence count, that surround instances of the financial sense of interest. One set was extracted from a hand-tagged corpus (Bruce and Wiebe, 1994) and the other by our algorithm. 3 Application on WSD To evaluate the usefulness of the topic signatures acquired, we applied them in a WSD task. We adopted an algorithm similar to Sch¨utze’s (1998) context-group discrimination, which determines a word sense according to the semantic similarity of contexts, computed using a second-order cooccurrence vector model. In this section, we firstly introduce our adaptation of this algorithm, and then describe the disambiguation experiments on 6 words for which a gold standard is available. 3.1 Context-Group Discrimination We chose the so-called contex</context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>Rebecca Bruce and Janyce Wiebe. 1994. Word-sense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 139– 146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with open mind word expert.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL 2002 Workshop on “Word Sense Disambiguation Recent Successes and Future Directions”.</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1888" citStr="Chklovski and Mihalcea (2002)" startWordPosition="293" endWordPosition="296">machine learning systems. This is the so-called knowledge acquisition bottleneck. As an alternative, automatic or semi-automatic acquisition methods have been proposed to tackle the bottleneck. For example, Agirre et al. (2001) tried to automatically extract topic signatures by querying a search engine using monosemous synonyms or other knowledge associated with a concept defined in WordNet. The Web provides further ways of overcoming the bottleneck. Mihalcea et al. (1999) presented a method enabling automatic acquisition of sensetagged corpora, based on WordNet and an Internet search engine. Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. Another type of method, which exploits differences between languages, has shown great promise. For example, some work has been done based on the assumption that mappings of words and meanings are different in different languages. Gale et al. (1992) proposed a method which automatically produces sense-tagged data using parallel bilingual corpora. Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. One problem with relying on bilingual corpora for data co</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Timothy Chklovski and Rada Mihalcea. 2002. Building a sense tagged corpus with open mind word expert. In Proceedings of the ACL 2002 Workshop on “Word Sense Disambiguation Recent Successes and Future Directions”. Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL-02).</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="2356" citStr="Diab and Resnik (2002)" startWordPosition="364" endWordPosition="367">9) presented a method enabling automatic acquisition of sensetagged corpora, based on WordNet and an Internet search engine. Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. Another type of method, which exploits differences between languages, has shown great promise. For example, some work has been done based on the assumption that mappings of words and meanings are different in different languages. Gale et al. (1992) proposed a method which automatically produces sense-tagged data using parallel bilingual corpora. Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. One problem with relying on bilingual corpora for data collection is that bilingual corpora are rare, and aligned bilingual corpora are even rarer. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data. Another problem is that if two languages are closely related, data for some words cannot be collected because different senses of polysemous words in one language often translate to the same word in the other. In this paper, we present a novel approach for au</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL-02). Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proceedings of the International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>101--112</pages>
<contexts>
<context position="2234" citStr="Gale et al. (1992)" startWordPosition="346" endWordPosition="349">ed with a concept defined in WordNet. The Web provides further ways of overcoming the bottleneck. Mihalcea et al. (1999) presented a method enabling automatic acquisition of sensetagged corpora, based on WordNet and an Internet search engine. Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. Another type of method, which exploits differences between languages, has shown great promise. For example, some work has been done based on the assumption that mappings of words and meanings are different in different languages. Gale et al. (1992) proposed a method which automatically produces sense-tagged data using parallel bilingual corpora. Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. One problem with relying on bilingual corpora for data collection is that bilingual corpora are rare, and aligned bilingual corpora are even rarer. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data. Another problem is that if two languages are closely related, data for some words cannot be collected because different senses of polysem</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Proceedings of the International Conference on Theoretical and Methodological Issues in Machine Translation, pages 101–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>An automatic method for generating sense tagged corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th Conference of the American Association ofArtificial Intelligence.</booktitle>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan I. Moldovan. 1999. An automatic method for generating sense tagged corpora. In Proceedings of the 16th Conference of the American Association ofArtificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>The role of non-ambiguous words in natural language disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Recent Advances in Natural Language Processing, RANLP</booktitle>
<location>Borovetz, Bulgaria.</location>
<contexts>
<context position="18232" citStr="Mihalcea, 2003" startWordPosition="3031" endWordPosition="3032">� 1. bird� 829� 2301� 24� 95El 74.7%11 76.6%11 2. machine[ 1472[1 71[ motion� 6000� 9265 141 201El 70.1%11 69.7%11 1. physical� 2. legal[ 3265El 60[ palm� 852� 12481 143 201El 71.1%11 76.1%11 1. hand� 2. tree[ 396[1 58[ plant� 1. living� 6000� 12000� 86� 188� 54.3%� 70.2%[] 2. factory[ 6000[1 102[ tank� 6000� 93461 126[ 201 E 62.7%11 70.1%11 1. container� 2. vehicle[ 3346[1 75[ Table 2:Sizes of the training data and the test data, baseline performance, and the results. The test data is a binary sense-tagged corpus, the TWA Sense Tagged Data Set, manually produced by Rada Mihalcea and Li Yang (Mihalcea, 2003), from text drawn from the British National Corpus. We calculated a ‘supervised’ baseline from the annotated data by assigning the most frequent sense in the test data to all instances, although it could be argued that the baseline for unsupervised disambiguation should be computed by randomly assigning one of the senses to instances (e.g. it would be 50% for words with two senses). According to our previous description, the 2, 500 most frequent concepts were selected as dimensions. The number of features in a Concept Space depends on how many unique concepts actually occur in the training set</context>
<context position="20019" citStr="Mihalcea (2003)" startWordPosition="3338" endWordPosition="3339">sing our topic signatures, we obtained good results: the accuracy for all words exceeds the supervised baseline, except for motion which approaches it. The Chinese translations for motion are also ambiguous, which might be the reason that our WSD system performed less well on this word. However, as we mentioned, to avoid this problem, we could have expanded motion’s Chinese translations, using their Chinese monosemous synonyms, when we query the Chinese corpus or the Web. Considering our system is unsupervised, the results are very promising. An indicative comparison might be with the work of Mihalcea (2003), who with a very different approach achieved similar performance on the same test data. 4 Discussion Although these results are promising, higher quality topic signatures would probably yield better results in our WSD experiments. There are a number of factors that could affect the acquisition process, which determines the quality of this resource. Firstly, since the translation was achieved by looking up in a bilingual dictionary, the deficiencies of the dictionary could cause problems. For example, the LDC Chinese-English Lexicon we used is not up to date, for example, lacking entries for w</context>
</contexts>
<marker>Mihalcea, 2003</marker>
<rawString>Rada Mihalcea. 2003. The role of non-ambiguous words in natural language disambiguation. In Proceedings of the Conference on Recent Advances in Natural Language Processing, RANLP 2003. Borovetz, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>Journal ofLexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="1130" citStr="Miller et al., 1990" startWordPosition="179" endWordPosition="182">D) and Text Summarisation. Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web. We evaluated the topic signatures on a WSD task, where we trained a second-order vector cooccurrence algorithm on standard WSD datasets, with promising results. 1 Introduction Lexical knowledge is crucial for many NLP tasks. Huge efforts and investments have been made to build repositories with different types of knowledge. Many of them have proved useful, such as WordNet (Miller et al., 1990). However, in some areas, such as WSD, manually created knowledge bases seem never to satisfy the huge requirement by supervised machine learning systems. This is the so-called knowledge acquisition bottleneck. As an alternative, automatic or semi-automatic acquisition methods have been proposed to tackle the bottleneck. For example, Agirre et al. (2001) tried to automatically extract topic signatures by querying a search engine using monosemous synonyms or other knowledge associated with a concept defined in WordNet. The Web provides further ways of overcoming the bottleneck. Mihalcea et al. </context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An on-line lexical database. Journal ofLexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="3375" citStr="Resnik and Yarowsky, 1999" startWordPosition="531" endWordPosition="534"> related, data for some words cannot be collected because different senses of polysemous words in one language often translate to the same word in the other. In this paper, we present a novel approach for automatically acquiring topic signatures (see Table 1 for an example of topic signatures), which also adopts the cross-lingual paradigm. To solve the problem of different senses not being distinguishable mentioned in the previous paragraph, we chose a language very distant to English – Chinese, since the more distant two languages are, the more likely that senses are lexicalised differently (Resnik and Yarowsky, 1999). Because our approach only uses Chinese monolingual text, we also avoid the problem of shortage of aligned bilingual corpora. We build the topic signatures by using Chinese-English and EnglishChinese bilingual lexicons and a large amount of Chinese text, which can be collected either from the Web or from Chinese corpora. Since topic signatures are potentially good training data for WSD algorithms, we set up a task to disambiguate 6 words using a WSD algorithm similar to Sch¨utze’s (1998) context-group discrimination. The results show that our topic signatures are useful for WSD. The remainder</context>
<context position="7421" citStr="Resnik and Yarowsky (1999)" startWordPosition="1220" endWordPosition="1223">W Under the assumption that the majority of c’s are unambiguous, which we discuss later, we refer to elements in a topic signature as concepts in this paper. Choosing an appropriate English-Chinese dictionary is the first problem we faced. The one we decided to use is the Yahoo! Student EnglishChinese On-line Dictionary2. As this dictionary is designed for English learners, its sense granularity is far coarser-grained than that of WordNet. However, researchers argue that the granularity of WordNet is too fine for many applications, and some also proposed new evaluation standards. For example, Resnik and Yarowsky (1999) sug2See: http://cn.yahoo.com/dictionary/ gested that for the purpose of WSD, the different senses of a word could be determined by considering only sense distinctions that are lexicalised cross-linguistically. Our approach is in accord with their proposal, since bilingual dictionaries interpret sense distinctions crossing two languages. For efficiency purposes, we extract our topic signatures mainly from the Mandarin portion of the Chinese Gigaword Corpus (CGC), produced by the LDC3, which contains 1.3GB of newswire text drawn from Xinhua newspaper. Some Chinese translations of English word s</context>
</contexts>
<marker>Resnik, Yarowsky, 1999</marker>
<rawString>Philip Resnik and David Yarowsky. 1999. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation. Natural Language Engineering, 5(2):113–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Mining the Web for bilingual text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2627" citStr="Resnik, 1999" startWordPosition="411" endWordPosition="412">h exploits differences between languages, has shown great promise. For example, some work has been done based on the assumption that mappings of words and meanings are different in different languages. Gale et al. (1992) proposed a method which automatically produces sense-tagged data using parallel bilingual corpora. Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. One problem with relying on bilingual corpora for data collection is that bilingual corpora are rare, and aligned bilingual corpora are even rarer. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data. Another problem is that if two languages are closely related, data for some words cannot be collected because different senses of polysemous words in one language often translate to the same word in the other. In this paper, we present a novel approach for automatically acquiring topic signatures (see Table 1 for an example of topic signatures), which also adopts the cross-lingual paradigm. To solve the problem of different senses not being distinguishable mentioned in the previous paragraph, we chose a language very distant</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Mining the Web for bilingual text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>