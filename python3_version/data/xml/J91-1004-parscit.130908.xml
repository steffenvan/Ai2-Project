<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.892820333333333">
Technical Correspondence
Techniques for Automatic Memoization with
Applications to Context-Free Parsing
</title>
<author confidence="0.997072">
Peter Norvig.
</author>
<affiliation confidence="0.987247">
University of California
</affiliation>
<bodyText confidence="0.9938572">
It is shown that a process similar to Earley&apos;s algorithm can be generated by a simple top-down
backtracking parser, when augmented by automatic memoization. The memoized parser has the
same complexity as Fancy&apos;s algorithm, but parses constituents in a different order. Techniques
for deriving memo functions are described, with a complete implementation in Common Lisp,
and an outline of a macro-based approach for other languages.
</bodyText>
<sectionHeader confidence="0.981114" genericHeader="abstract">
1. Memoization
</sectionHeader>
<bodyText confidence="0.993454333333333">
The term memoization was coined by Donald Michie (1968) to refer to the process by
which a function is made to automatically remember the results of previous compu-
tations. The idea has become more popular in recent years with the rise of functional
languages; Field and Harrison (1988) devote a whole chapter to it. The basic idea is
just to keep a table of previously computed input/result pairs. In Common Lisp one
could write:1
</bodyText>
<figure confidence="0.573561888888889">
(defun memo (fn)
&amp;quot;Return a memo—function of fn.&amp;quot;
(let ((table (make—hash—table)))
#&apos;(lambda (x)
(multiple—value—bind (val found)
(gethash x table)
(if found
val
(sett (gethash x table) (funcall fn
</figure>
<bodyText confidence="0.997303555555556">
(For those familiar with Lisp but not Common Lisp, gethash returns two values,
the stored entry in the table, and a boolean flag indicating if there is in fact an entry.
The special form multiple-value-bind binds these two values to the symbols val
and found. The special form set f is used here to update the table entry for x.)
In this simple implementation In is required to take one argument and return
one value, and arguments that are eql produce the same value. Below we will relax
some of these restrictions. The problem with this approach is that we also want to
memoize any recursive calls that fn may make. To use the canonical example, if we
have:
</bodyText>
<footnote confidence="0.8304846">
* Computer Science Division, University of California, Berkeley, CA 94720
1 All examples are in Common Lisp, rather than in generic pseudo-code for two reasons. First, I want to
stress that these techniques can actually be used in existing languages. Second, Common Lisp provides
a rich set of primitives (such as make-hash-table) that would otherwise require lengthy explanations.
C) 1991 Association for Computational Linguistics
</footnote>
<note confidence="0.7032">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.933573464285714">
(defun fib (n)
&amp;quot;Compute the Nth Fibonacci number.&amp;quot;
(if (&lt;= n 1) n
(+ (fib (- n 1)) (fib (- n 2))))),
then the function (memo (function fib)) will not have linear-order complexity, be-
cause recursive calls will go to the original function fib, not to the memoized version.
One way to fix this problem is to assign the new memoized function to the name fib.
Common Lisp is a good host language for this approach, because there are primitives
for accessing and altering the global function name space. Consider the following:
(defun memoize (fn-name)
&amp;quot;Replace fn-name&apos;s global definition with a memoized version.&amp;quot;
(setf (symbol-function fn-name) (memo (symbol-function fn-name)))).
When passed a symbol that names a function, memoize changes the global defini-
tion of the function to a memo-function. Thus, any recursive calls will go first to the
memo-function, rather than to the original function. This is just what we want; all we
have to say is (memoize &apos; f ib) to transform fib from an exponential- to a linear-order
algorithm.
To make sure that the memoization step isn&apos;t left out during program develop-
ment, some programmers may prefer to write code like this:
(memoize
(defun f (x) ...)).
Or even like this:
(defmacro defun-memo (fn args &amp;body body)
&amp;quot;Define a memoized function.&amp;quot;
&apos;(memoize (defun ,fn ,args . ,body)))
(defun-memo f (x) ...).
Both of these approaches rely on the fact that defun returns the name of the
function defined. Note that the following will not work
</bodyText>
<equation confidence="0.9650282">
(labels ((fib (n)
(if (&lt;= n 1) n
(4- (fib (- n 1) (fib (- n 2)))))))
(memoize &apos;fib)
(fib 100))
</equation>
<bodyText confidence="0.9979544">
because memo ize affects only the global function binding, not the lexical (local) defi-
nition.
The version of memo presented above suffers from a serious limitation — the func-
tion to be memoized can only have one argument. In the revised definition given
below, the function can take any number of arguments, and the indexing can be on
</bodyText>
<page confidence="0.983913">
92
</page>
<note confidence="0.378477">
Norvig Memoization and Context-Free Parsing
</note>
<bodyText confidence="0.984013733333333">
any function of the arguments. When there is only argument, the default key function,
first, is appropriate. To hash on all the arguments, use identity as the key.
(defun memo (fn &amp;key (key #&apos;first) (test #&apos;eql) name)
&amp;quot;Return a memo-function of fn.&amp;quot;
(let ((table (make-hash-table :test test)))
(setf (get name &apos;memo) table)
#&apos;(lambda (&amp;rest args)
(let ((k (funcall key args)))
(multiple-value-bind (val found)
(gethash k table)
(if found val
(setf (gethash k table) (apply fn args))))))))
(defun memoize (fn-name &amp;key (key #&apos;first) (test #&apos;eq1))
&amp;quot;Replace fn-name&apos;s global definition with a memoized version.&amp;quot;
(setf (symbol-function fn-name)
(memo (symbol-function fn-name)
:name fn-name :key key :test test)))
(defun clear-memoize (fn-name)
(clrhash (get fn-name &apos;memo)))
Also note that the hash table is stored away on the function name&apos;s property list, so
that it can be inspected or cleared at will. The intent is that the user&apos;s program should
clear the table when the results are likely to be out of the working set. We might also
want a hash mechanism that caches only recent entries, and discards earlier ones, as
originally suggested by Michie (1968).
The user also has the responsibility of choosing the appropriate hashing function.
By choosing eql instead of equal hashing, for example, hashing overhead will be
reduced, but computation will be duplicated for equal lists. A compromise is to use
eql hashing in conjunction with unique or canonical lists (Szolovits and Martin, 1981).
eql hashing has the additional advantage of allowing infinite circular lists, as discussed
by Hughes (1985).
</bodyText>
<sectionHeader confidence="0.983026" genericHeader="categories and subject descriptors">
2. Context-Free Parsing
</sectionHeader>
<bodyText confidence="0.999869615384615">
All efficient algorithms for parsing context-free grammars make use of some kind
of well-formed substring table. Earley&apos;s algorithm (1970) is perhaps the best-known
example. The algorithm builds up a vector of parse lists, where each entry in a parse
list is an item — a production rule with one indicator showing how much of the right-
hand side has been parsed, and another saying where the parse started. Kay (1980)
introduced a similar data structure called a chart, along with a family of parsing
algorithms that operate on the chart. Chart parsing is described in virtually all recent
texts on natural language processing; for example, Winograd (1983) devotes 19 pages to
the topic. It is part of the &amp;quot;folklore&amp;quot; on parsing that these algorithms can be thought of
as tabular (i.e., memoized) versions of corresponding simpler algorithms. Earley (1970)
himself mentions it, and Shieber (1989) gives a general abstract parsing strategy, and
then proves that Earley&apos;s algorithm can be derived by suitable constraints on control
of the strategy.
</bodyText>
<page confidence="0.979803">
93
</page>
<note confidence="0.277053">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.9085602">
This paper&apos;s contribution is a concrete demonstration of just how direct the cor-
respondence is between the simple and the efficient algorithm. We present a simple
parser which, when memoized, performs the same calculations as Earley&apos;s algorithm.
The core of the parser is only 15 lines of code:
(defun parse (tokens start-symbol)
&amp;quot;Parse a list of tokens, return parse trees and remainders.&amp;quot;
(if (eq (first tokens) start-symbol)
(list (make-parse :tree (first tokens) :rem (rest tokens)))
(mapcan #&apos;(lambda (rule)
(extend-parse (lhs rule) nil tokens (rhs rule)))
(rules-for start-symbol))))
(defun extend-parse (lhs rhs rem needed)
&amp;quot;Parse the remaining needed symbols.&amp;quot;
(if (null needed)
(list (make-parse :tree (cons lhs rhs) :rem rem))
</bodyText>
<equation confidence="0.7613742">
(mapcan
#&apos;(lambda (p)
(extend-parse lhs (append rhs (list (parse-tree p)))
(parse-rem p) (rest needed)))
(parse rem (first needed))))).
</equation>
<bodyText confidence="0.935873272727273">
This assumes that there are no left-recursive rules. The parser requires the following
definitions:
(defstruct (parse) &amp;quot;A parse tree and a remainder.&amp;quot; tree rem)
;; Trees (and rules) are of the form: (lhs . rhs)
(defun lhs (tree) (first tree))
(defun rhs (tree) (rest tree))
(defun rules-for (symbol)
&amp;quot;Return a list of the rules with symbol on the left hand side.&amp;quot;
(remove symbol *grammar* :key #&apos;1hs :test-not #&apos;eq1)).
We now need to specify the memoization. In addition, since the function parse returns
all valid parses of all prefixes of the input, we add the function parser which returns
only parses of the complete input.
(memoize &apos;rules-for)
(memoize &apos;parse :test #&apos;equal :key #&apos;identity)
(defun parser (tokens start-symbol)
&amp;quot;Return all complete parses of a list of tokens.&amp;quot;
(clear-memoize &apos;parse)
(mapcar #&apos;parse-tree
(remove-if-not #&apos;null (parse tokens start-symbol)
:key #&apos;parse-rem)))
As an example, consider the following grammar, taken from page 321 of Aho and
Ullman (1972). Here the Lisp form (E T + E) corresponds to the grammar rule
</bodyText>
<page confidence="0.992977">
94
</page>
<note confidence="0.653784">
Norvig Memoization and Context-Free Parsing
</note>
<equation confidence="0.991103857142857">
E -- T + E, where +, *, [ and ] are all terminal symbols in the grammar, and con-
catenation is implicit.
(defparameter *grammar*
&apos;((E T + E)
(E T)
(T F * T)
(T F)
(F [ E ])
(F a)))
We can use this to obtain parses like the following:
&gt; (parser &apos;( [ a + a ] * a) &apos;E)
((E (T (F [ (E (T (F A)) + (E (T (F A)))) ])
*
(T (F A))))).
</equation>
<bodyText confidence="0.969360178571429">
We do not include here a proof that the memoized parse does the same work as
Earley&apos;s algorithm, but it is not too difficult to show. Interested readers can insert
the following format statement as the first expression in extend-parse, and then run
the example again. The resulting output, when sorted, matches exactly the parse lists
shown on page 322 of Aho and Ullman (1972).
(format t &amp;quot;-&amp;-d -a -&gt;-{ -a-} . -{ -a-}&amp;quot;
(- 7 (length rem)) lhs
(mapcar #&apos;(lambda (x) (if (consp x) (first x) x)) rhs)
needed)
The items come out in a different order because the Earley algorithm is strict left-to-
right, while the memoized version is doing backtracking, but with the memoization
eliminating all duplication of effort.
Unfortunately, the memoized parser as presented does not have the same asymp-
totic complexity as Earley&apos;s algorithm. The problem is that hashing is done on the
complete argument list to parse: the start symbol and the list of remaining tokens.
Hashing on this takes time proportional to the number of tokens, so the whole algo-
rithm is 0(n4) instead of 0(n3). What we really want is a hash table that is a compro-
mise between equal and eql hashing, one that takes keys that are lists, where each
element of the list should be compared by eql. Such a table can be easily managed:
(defun put-multi-hash (keylist value hash-table)
&amp;quot;Store a value in a multi-level hash table:
one level for each element of keylist&amp;quot;
(if (= (length keylist) 1)
(setf (gethash (first keylist) hash-table) value)
(let ((table1 (or (gethash (first keylist) hash-table)
(setf (gethash (first keylist) hash-table)
(make-hash-table)))))
(put-multi-hash (rest keylist) value table1))))
</bodyText>
<page confidence="0.986474">
95
</page>
<figure confidence="0.783701185185185">
Computational Linguistics Volume 17, Number 1
(defun get-multi-hash (keylist hash-table)
&amp;quot;Fetch a value from a multi-level hash table:
one level for each element of keylist&amp;quot;
(if (= (length keylist) 1)
(gethash (first keylist) hash-table)
(let ((tablel (or (gethash (first keylist) hash-table)
(setf (gethash (first keylist) hash-table)
(make-hash-table)))))
(get-multi-hash (rest keylist) table1)))).
Now to use these multi-level hash tables, we need another version of memo, one that
gives us the flexibility to specify the get and put functions:
(defun memo (fn &amp;key name (maker #&apos;make-hash-table)
(getter #&apos;gethash) (putter #&apos;puthash))
&amp;quot;Return a memo-function of fn.&amp;quot;
(let ((table (funcall maker)))
(setf (get name &apos;memo) table)
#&apos;(lambda (&amp;rest args)
(multiple-value-bind (val found)
(funcall getter args table)
(if found val
(funcall putter args (apply fn args) table))))))
(defun memoize (fn-name &amp;rest memo-keys)
&amp;quot;Replace fn-name&apos;s global definition with a memoized version.&amp;quot;
(setf (symbol-function fn-name)
(apply #&apos;memo (symbol-function fn-name)
:name fn-name memo-keys))).
</figure>
<bodyText confidence="0.738125">
Finally, we can get an 0(n3) parser by saying:
(memoize &apos;parse :getter #&apos;get-multi-hash :putter #&apos;put-multi-hash).
</bodyText>
<sectionHeader confidence="0.599253" genericHeader="general terms">
3. Memoizing in Other Languages
</sectionHeader>
<bodyText confidence="0.870129583333333">
In the Scheme dialect of Lisp, the programmer would have to write (set! fib (memo
f ib) ) instead of (memoize &apos;fib). This is because Scheme lacks symbol-value and
set-symbol-value! functions, and has nothing to do with the fact that Scheme has a
single name space for functions and variables. It is also possible to write the memoized
function all in one step:
(define fib
(memo (lambda (n)
(if (&lt;= n 1) n
(+ (fib (- n 1) (fib (- n 2)))))))).
This is the approach taken by Abelson and Sussman (1985; see exercise 3.27).
In a Scheme implementation with macros, we could of course define memo ize or
define-memo as macros, thereby achieving the same result as in Common Lisp. In
</bodyText>
<page confidence="0.973346">
96
</page>
<note confidence="0.68749">
Norvig Memoization and Context-Free Parsing
</note>
<bodyText confidence="0.878911266666667">
fact, the macro solution even works in languages without first class functional objects,
if a sufficiently powerful macro facility is available, or if the language&apos;s parser can
be modified. For example, one could add the keyword memo to Pascal, and write the
following:
memo function fib(n:integer) : integer;
begin
if n &lt;= 1 then
fib := n
else fib := fib(n-1) + fib(n-2);
end;
and have it parsed as if it were:
function fib(n:integer) : integer;
begin
if not inTable(n) then
if n &lt;= 1 then
</bodyText>
<equation confidence="0.96773375">
addToTable(n,n)
else addToTable(n,fib(n-1) + fib(n-2));
fib := tableValue(n);
end;
</equation>
<bodyText confidence="0.999765">
Texts such as Field and Harrison (1988) assume that memoization is done by a built-in
feature of the language that in effect implements this kind of source-to-source trans-
formation. This paper shows that memoization can also be done completely within
the language — provided the language has either (a) a macro facility or (b) both
higher-order functions and a way to set function bindings.
</bodyText>
<sectionHeader confidence="0.991676" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.998896105263158">
What has been gained by memoization? The memoized algorithms presented here are
of the same order of complexity as the existing Earley algorithm and linear-recursive
Fibonacci function. In fact, unless tables are implemented very carefully, the memoized
algorithms perform worse by a constant factor.
However, note that Earley&apos;s algorithm is not the best for all parsing applications.
One might prefer a bottom-up parser or a deterministic parser. The point is that ex-
perimenting with alternate parsing strategies is easier when starting from a simple
15-line program, rather than from a multi-page parser that maintains complex explicit
data structures.
Memoization supports an incremental, lazy-evaluation-like style that can make
the overall design simpler. For example, note that besides parse, we also memoized
rules-for. This has the effect of building an inverted index of the grammar rules,
implicitly and incrementally. The programmer is freed from having to remember to
declare, allocate, and initialize a rule table, but still gets the benefit of efficient access
to the rules. This is the ultimate data abstraction: the programmer is shielded not only
from the details of the implementation of the table, but even from the existence of a
table at all.
For twenty years, algorithms like Earley&apos;s (and more recently chart parsing) have
been treated as special techniques, worthy of special attention. This paper has shown
</bodyText>
<page confidence="0.996816">
97
</page>
<note confidence="0.559627">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.995760285714286">
that the maintenance of well-formed substring tables or charts can be seen as a spe-
cial case of a more general technique: memoization. Furthermore, we have shown
that Common Lisp, with its mutable function name space, is an especially congenial
host language for memoization, and that other languages can be hosts with other
approaches. It is our hope that programmers will adopt automatic techniques like
memoization where appropriate, and concentrate their data-creation and algorithm-
optimization efforts on truly special cases.
</bodyText>
<sectionHeader confidence="0.999162" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.982732333333333">
This work has been supported by the
Defense Advanced Research Projects
Agency (DoD), Arpa Order No. 4871,
monitored by Space and Naval Warfare
Systems Command under Contract
N00039-84-C-0089.
</reference>
<sectionHeader confidence="0.659353" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999595771428571">
Abelson, H. and Sussman, G.J. (1985).
Structure and Interpretation of Computer
Programs. MIT Press.
Aho, A.V. and Ullman, J.D. (1972). The
Theory of Parsing, Translation, and
Compiling, Vol 1: Parsing. Prentice-Hall.
Earley, J. (1970). &amp;quot;An efficient context-free
parsing algorithm.&amp;quot; Communications of the
Association for Computing Machinery 6(2),
451-455.
Field, A.J. and Harrison, P.C. (1988).
Functional Programming. Addison-Wesley.
Grosz, B.J., Sparck-Jones, K., and Webber,
B.L. (1986). Readings in Natural Language
Processing. Morgan Kaufman.
Hughes, R.J.M. (1985). &amp;quot;Lazy memo
functions.&amp;quot; In Proceedings, Conference on
Functional Programming and Computer
Architecture, 129-146. Springer-Verlag.
Kay, M. (1980). &amp;quot;Algorithm schemata and
data structures in syntactic processing.&amp;quot;
In Proceedings, Symposium on Text
Processing, Nobel Academy.
Michie, D. (1968). &amp;quot;Memo functions and
machine learning.&amp;quot; Nature, 218, 19-22.
Sheiber, Stuart M. (1989). &amp;quot;Parsing and type
inference for natural and computer
languages.&amp;quot; SRI International Technical
Note 460.
Szolovits, P. and Martin, W.A. (1981).
&amp;quot;Brand X: Lisp support for semantic
networks.&amp;quot; In Proceedings, 7th IJCAI,
Vancouver.
Winograd, T. (1983). Language as a Cognitive
Process. Addison-Wesley.
</reference>
<page confidence="0.996242">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.040145">
<title confidence="0.982630666666667">Technical Correspondence Techniques for Automatic Memoization with Applications to Context-Free Parsing</title>
<author confidence="0.99902">Peter Norvig</author>
<affiliation confidence="0.998824">University of California</affiliation>
<abstract confidence="0.981334674418604">It is shown that a process similar to Earley&apos;s algorithm can be generated by a simple top-down backtracking parser, when augmented by automatic memoization. The memoized parser has the same complexity as Fancy&apos;s algorithm, but parses constituents in a different order. Techniques for deriving memo functions are described, with a complete implementation in Common Lisp, and an outline of a macro-based approach for other languages. 1. Memoization term coined by Donald Michie (1968) to refer to the process by which a function is made to automatically remember the results of previous computations. The idea has become more popular in recent years with the rise of functional languages; Field and Harrison (1988) devote a whole chapter to it. The basic idea is just to keep a table of previously computed input/result pairs. In Common Lisp one (defun memo (fn) &amp;quot;Return a memo—function of fn.&amp;quot; (let ((table (make—hash—table))) (multiple—value—bind (val found) (gethash x table) (if found val (gethash x table) (funcall (For those familiar with Lisp but not Common Lisp, gethash returns two values, the stored entry in the table, and a boolean flag indicating if there is in fact an entry. The special form multiple-value-bind binds these two values to the symbols val and found. The special form set f is used here to update the table entry for x.) In this simple implementation In is required to take one argument and return one value, and arguments that are eql produce the same value. Below we will relax some of these restrictions. The problem with this approach is that we also want to any recursive calls that make. To use the canonical example, if we have: * Computer Science Division, University of California, Berkeley, CA 94720 1 All examples are in Common Lisp, rather than in generic pseudo-code for two reasons. First, I want to stress that these techniques can actually be used in existing languages. Second, Common Lisp provides rich set of primitives (such as would otherwise require lengthy explanations. C) 1991 Association for Computational Linguistics Computational Linguistics Volume 17, Number 1 (defun fib (n) &amp;quot;Compute the Nth Fibonacci number.&amp;quot; (if (&lt;= n 1) n (+ (fib (n 1)) (fib (n 2))))), then the function (memo (function fib)) will not have linear-order complexity, berecursive calls will go to the original function not the memoized version. way to fix this problem is to assign the new memoized function to the name Common Lisp is a good host language for this approach, because there are primitives for accessing and altering the global function name space. Consider the following: (defun memoize (fn-name) &amp;quot;Replace fn-name&apos;s global definition with a memoized version.&amp;quot; (setf (symbol-function fn-name) (memo (symbol-function fn-name)))). When passed a symbol that names a function, memoize changes the global definition of the function to a memo-function. Thus, any recursive calls will go first to the memo-function, rather than to the original function. This is just what we want; all we to say is (memoize &apos; ib) transform an exponentialto a linear-order algorithm. To make sure that the memoization step isn&apos;t left out during program development, some programmers may prefer to write code like this: (memoize (defun f (x) ...)). Or even like this: (defmacro defun-memo (fn args &amp;body body) &amp;quot;Define a memoized function.&amp;quot; &apos;(memoize (defun ,fn ,args . ,body))) (defun-memo f (x) ...). of these approaches rely on the fact that the name of the defined. Note that the following will (labels ((fib (n) (if (&lt;= n 1) n (fib (n 1) (fib (n 2))))))) (memoize &apos;fib) (fib 100)) because memo ize affects only the global function binding, not the lexical (local) definition. The version of memo presented above suffers from a serious limitation — the function to be memoized can only have one argument. In the revised definition given below, the function can take any number of arguments, and the indexing can be on 92 Norvig Memoization and Context-Free Parsing any function of the arguments. When there is only argument, the default key function, appropriate. To hash on all the arguments, use the key. (defun memo (fn &amp;key (key #&apos;first) (test #&apos;eql) name) &amp;quot;Return a memo-function of fn.&amp;quot; (let ((table (make-hash-table :test test))) (setf (get name &apos;memo) table) (let ((k (funcall key args))) (multiple-value-bind (val found) (gethash k table) (if found val (setf (gethash k table) (apply fn args)))))))) (defun memoize (fn-name &amp;key (key #&apos;first) (test #&apos;eq1)) &amp;quot;Replace fn-name&apos;s global definition with a memoized version.&amp;quot; (setf (symbol-function fn-name) (memo (symbol-function fn-name) :name fn-name :key key :test test))) (defun clear-memoize (fn-name) (clrhash (get fn-name &apos;memo))) Also note that the hash table is stored away on the function name&apos;s property list, so that it can be inspected or cleared at will. The intent is that the user&apos;s program should clear the table when the results are likely to be out of the working set. We might also want a hash mechanism that caches only recent entries, and discards earlier ones, as originally suggested by Michie (1968). The user also has the responsibility of choosing the appropriate hashing function. By choosing eql instead of equal hashing, for example, hashing overhead will be reduced, but computation will be duplicated for equal lists. A compromise is to use eql hashing in conjunction with unique or canonical lists (Szolovits and Martin, 1981). eql hashing has the additional advantage of allowing infinite circular lists, as discussed by Hughes (1985). Parsing All efficient algorithms for parsing context-free grammars make use of some kind of well-formed substring table. Earley&apos;s algorithm (1970) is perhaps the best-known example. The algorithm builds up a vector of parse lists, where each entry in a parse list is an item — a production rule with one indicator showing how much of the righthand side has been parsed, and another saying where the parse started. Kay (1980) introduced a similar data structure called a chart, along with a family of parsing algorithms that operate on the chart. Chart parsing is described in virtually all recent texts on natural language processing; for example, Winograd (1983) devotes 19 pages to the topic. It is part of the &amp;quot;folklore&amp;quot; on parsing that these algorithms can be thought of as tabular (i.e., memoized) versions of corresponding simpler algorithms. Earley (1970) himself mentions it, and Shieber (1989) gives a general abstract parsing strategy, and then proves that Earley&apos;s algorithm can be derived by suitable constraints on control of the strategy. 93 Computational Linguistics Volume 17, Number 1 This paper&apos;s contribution is a concrete demonstration of just how direct the correspondence is between the simple and the efficient algorithm. We present a simple parser which, when memoized, performs the same calculations as Earley&apos;s algorithm. The core of the parser is only 15 lines of code: (defun parse (tokens start-symbol) &amp;quot;Parse a list of tokens, return parse trees and remainders.&amp;quot; (if (eq (first tokens) start-symbol) (list (make-parse :tree (first tokens) :rem (rest tokens))) (mapcan #&apos;(lambda (rule) (extend-parse (lhs rule) nil tokens (rhs rule))) (rules-for start-symbol)))) (defun extend-parse (lhs rhs rem needed) &amp;quot;Parse the remaining needed symbols.&amp;quot; (if (null needed) (list (make-parse :tree (cons lhs rhs) :rem rem)) (mapcan (extend-parse lhs (append rhs (list (parse-tree p))) (parse-rem p) (rest needed))) (parse rem (first needed))))). This assumes that there are no left-recursive rules. The parser requires the following definitions: (defstruct (parse) &amp;quot;A parse tree and a remainder.&amp;quot; tree rem) ;; Trees (and rules) are of the form: (lhs . rhs) (defun lhs (tree) (first tree)) (defun rhs (tree) (rest tree)) (defun rules-for (symbol) &amp;quot;Return a list of the rules with symbol on the left hand side.&amp;quot; (remove symbol *grammar* :key #&apos;1hs :test-not #&apos;eq1)). We now need to specify the memoization. In addition, since the function parse returns all valid parses of all prefixes of the input, we add the function parser which returns only parses of the complete input. (memoize &apos;rules-for) (memoize &apos;parse :test #&apos;equal :key #&apos;identity) (defun parser (tokens start-symbol) &amp;quot;Return all complete parses of a list of tokens.&amp;quot; (clear-memoize &apos;parse) (mapcar #&apos;parse-tree (remove-if-not #&apos;null (parse tokens start-symbol) :key #&apos;parse-rem))) As an example, consider the following grammar, taken from page 321 of Aho and (1972). Here Lisp form (E T + E) corresponds to the grammar rule 94 Norvig Memoization and Context-Free Parsing -- T + E, +, *, [ and ] are all terminal symbols in the grammar, and concatenation is implicit. (defparameter *grammar* &apos;((E T + E) (E T) (T F * T) (T F) (F [ E ]) (F a))) We can use this to obtain parses like the following: &gt; (parser &apos;( [ a + a ] * a) &apos;E) ((E (T (F [ (E (T (F A)) + (E (T (F A)))) ]) * (T (F A))))). We do not include here a proof that the memoized parse does the same work as Earley&apos;s algorithm, but it is not too difficult to show. Interested readers can insert following statement the first expression in then run the example again. The resulting output, when sorted, matches exactly the parse lists shown on page 322 of Aho and Ullman (1972). t -&gt;-{ . -{ (- 7 (length rem)) lhs (mapcar #&apos;(lambda (x) (if (consp x) (first x) x)) rhs) needed) items come out in different order because the Earley algorithm is strict left-toright, while the memoized version is doing backtracking, but with the memoization eliminating all duplication of effort. Unfortunately, the memoized parser as presented does not have the same asymptotic complexity as Earley&apos;s algorithm. The problem is that hashing is done on the argument list to the symbol and the list of remaining tokens. Hashing on this takes time proportional to the number of tokens, so the whole algois of we really want is a hash table that is a comprobetween equal and eql one that takes keys that are lists, where each element of the list should be compared by eql. Such a table can be easily managed: (defun put-multi-hash (keylist value hash-table) &amp;quot;Store a value in a multi-level hash table: one level for each element of keylist&amp;quot; (if (= (length keylist) 1) (setf (gethash (first keylist) hash-table) value) (let ((table1 (or (gethash (first keylist) hash-table) (setf (gethash (first keylist) hash-table) (make-hash-table))))) (put-multi-hash (rest keylist) value table1)))) 95 Computational Linguistics Volume 17, Number 1 (defun get-multi-hash (keylist hash-table) &amp;quot;Fetch a value from a multi-level hash table: one level for each element of keylist&amp;quot; (if (= (length keylist) 1) (gethash (first keylist) hash-table) (let ((tablel (or (gethash (first keylist) hash-table) (setf (gethash (first keylist) hash-table) (make-hash-table))))) (get-multi-hash (rest keylist) table1)))). Now to use these multi-level hash tables, we need another version of memo, one that gives us the flexibility to specify the get and put functions: (defun memo (fn &amp;key name (maker #&apos;make-hash-table) (getter #&apos;gethash) (putter #&apos;puthash)) &amp;quot;Return a memo-function of fn.&amp;quot; (let ((table (funcall maker))) (setf (get name &apos;memo) table) (multiple-value-bind (val found) (funcall getter args table) (if found val (funcall putter args (apply fn args) table)))))) (defun memoize (fn-name &amp;rest memo-keys) &amp;quot;Replace fn-name&apos;s global definition with a memoized version.&amp;quot; (setf (symbol-function fn-name) (apply #&apos;memo (symbol-function fn-name) :name fn-name memo-keys))). we can get an parser by saying: (memoize &apos;parse :getter #&apos;get-multi-hash :putter #&apos;put-multi-hash). 3. Memoizing in Other Languages Scheme dialect of Lisp, the programmer would have to write (set! (memo ib) ) instead of (memoize &apos;fib). This is because Scheme lacks and has nothing to do with the fact that Scheme has a single name space for functions and variables. It is also possible to write the memoized function all in one step: (define fib (memo (lambda (n) (if (&lt;= n 1) n (+ (fib (n 1) (fib (n 2)))))))). This is the approach taken by Abelson and Sussman (1985; see exercise 3.27). In a Scheme implementation with macros, we could of course define memo ize or macros, thereby achieving the same result as in Common Lisp. In 96 Norvig Memoization and Context-Free Parsing fact, the macro solution even works in languages without first class functional objects, if a sufficiently powerful macro facility is available, or if the language&apos;s parser can be modified. For example, one could add the keyword memo to Pascal, and write the following: memo function fib(n:integer) : integer; begin n &lt;= 1 fib := n fib := fib(n-1) + end; and have it parsed as if it were: function fib(n:integer) : integer; begin if not inTable(n) then if n &lt;= 1 then addToTable(n,n) else addToTable(n,fib(n-1) + fib(n-2)); fib := tableValue(n); end; Texts such as Field and Harrison (1988) assume that memoization is done by a built-in feature of the language that in effect implements this kind of source-to-source transformation. This paper shows that memoization can also be done completely within the language — provided the language has either (a) a macro facility or (b) both higher-order functions and a way to set function bindings. 4. Conclusion What has been gained by memoization? The memoized algorithms presented here are of the same order of complexity as the existing Earley algorithm and linear-recursive Fibonacci function. In fact, unless tables are implemented very carefully, the memoized algorithms perform worse by a constant factor. However, note that Earley&apos;s algorithm is not the best for all parsing applications. One might prefer a bottom-up parser or a deterministic parser. The point is that experimenting with alternate parsing strategies is easier when starting from a simple 15-line program, rather than from a multi-page parser that maintains complex explicit data structures. Memoization supports an incremental, lazy-evaluation-like style that can make the overall design simpler. For example, note that besides parse, we also memoized has the effect of building an inverted index of the grammar rules, implicitly and incrementally. The programmer is freed from having to remember to declare, allocate, and initialize a rule table, but still gets the benefit of efficient access to the rules. This is the ultimate data abstraction: the programmer is shielded not only from the details of the implementation of the table, but even from the existence of a table at all. For twenty years, algorithms like Earley&apos;s (and more recently chart parsing) have been treated as special techniques, worthy of special attention. This paper has shown 97 Computational Linguistics Volume 17, Number 1 that the maintenance of well-formed substring tables or charts can be seen as a special case of a more general technique: memoization. Furthermore, we have shown that Common Lisp, with its mutable function name space, is an especially congenial host language for memoization, and that other languages can be hosts with other approaches. It is our hope that programmers will adopt automatic techniques like memoization where appropriate, and concentrate their data-creation and algorithmoptimization efforts on truly special cases.</abstract>
<note confidence="0.956349441860466">Acknowledgments This work has been supported by the Defense Advanced Research Projects Agency (DoD), Arpa Order No. 4871, monitored by Space and Naval Warfare Systems Command under Contract N00039-84-C-0089. References Abelson, H. and Sussman, G.J. (1985). Structure and Interpretation of Computer Press. A.V. and Ullman, J.D. (1972). Theory of Parsing, Translation, and Vol 1: Parsing. Earley, J. (1970). &amp;quot;An efficient context-free algorithm.&amp;quot; of the for Computing Machinery 451-455. Field, A.J. and Harrison, P.C. (1988). Programming. Grosz, B.J., Sparck-Jones, K., and Webber, in Natural Language Kaufman. Hughes, R.J.M. (1985). &amp;quot;Lazy memo In Conference on Functional Programming and Computer Springer-Verlag. Kay, M. (1980). &amp;quot;Algorithm schemata and data structures in syntactic processing.&amp;quot; Symposium on Text Academy. Michie, D. (1968). &amp;quot;Memo functions and learning.&amp;quot; 19-22. Sheiber, Stuart M. (1989). &amp;quot;Parsing and type inference for natural and computer languages.&amp;quot; SRI International Technical Note 460. Szolovits, P. and Martin, W.A. (1981). &amp;quot;Brand X: Lisp support for semantic In 7th IJCAI, Vancouver. T. (1983). as a Cognitive 98</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work has been supported by the Defense Advanced Research Projects Agency (DoD),</title>
<booktitle>Arpa Order No. 4871, monitored by Space and Naval Warfare Systems Command under Contract</booktitle>
<pages>00039--84</pages>
<marker></marker>
<rawString>This work has been supported by the Defense Advanced Research Projects Agency (DoD), Arpa Order No. 4871, monitored by Space and Naval Warfare Systems Command under Contract N00039-84-C-0089.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Abelson</author>
<author>G J Sussman</author>
</authors>
<title>Structure and Interpretation of Computer Programs.</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12727" citStr="Abelson and Sussman (1985" startWordPosition="2042" endWordPosition="2045">an 0(n3) parser by saying: (memoize &apos;parse :getter #&apos;get-multi-hash :putter #&apos;put-multi-hash). 3. Memoizing in Other Languages In the Scheme dialect of Lisp, the programmer would have to write (set! fib (memo f ib) ) instead of (memoize &apos;fib). This is because Scheme lacks symbol-value and set-symbol-value! functions, and has nothing to do with the fact that Scheme has a single name space for functions and variables. It is also possible to write the memoized function all in one step: (define fib (memo (lambda (n) (if (&lt;= n 1) n (+ (fib (- n 1) (fib (- n 2)))))))). This is the approach taken by Abelson and Sussman (1985; see exercise 3.27). In a Scheme implementation with macros, we could of course define memo ize or define-memo as macros, thereby achieving the same result as in Common Lisp. In 96 Norvig Memoization and Context-Free Parsing fact, the macro solution even works in languages without first class functional objects, if a sufficiently powerful macro facility is available, or if the language&apos;s parser can be modified. For example, one could add the keyword memo to Pascal, and write the following: memo function fib(n:integer) : integer; begin if n &lt;= 1 then fib := n else fib := fib(n-1) + fib(n-2); e</context>
</contexts>
<marker>Abelson, Sussman, 1985</marker>
<rawString>Abelson, H. and Sussman, G.J. (1985). Structure and Interpretation of Computer Programs. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling, Vol 1: Parsing.</booktitle>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="8895" citStr="Aho and Ullman (1972)" startWordPosition="1408" endWordPosition="1411">emove symbol *grammar* :key #&apos;1hs :test-not #&apos;eq1)). We now need to specify the memoization. In addition, since the function parse returns all valid parses of all prefixes of the input, we add the function parser which returns only parses of the complete input. (memoize &apos;rules-for) (memoize &apos;parse :test #&apos;equal :key #&apos;identity) (defun parser (tokens start-symbol) &amp;quot;Return all complete parses of a list of tokens.&amp;quot; (clear-memoize &apos;parse) (mapcar #&apos;parse-tree (remove-if-not #&apos;null (parse tokens start-symbol) :key #&apos;parse-rem))) As an example, consider the following grammar, taken from page 321 of Aho and Ullman (1972). Here the Lisp form (E T + E) corresponds to the grammar rule 94 Norvig Memoization and Context-Free Parsing E -- T + E, where +, *, [ and ] are all terminal symbols in the grammar, and concatenation is implicit. (defparameter *grammar* &apos;((E T + E) (E T) (T F * T) (T F) (F [ E ]) (F a))) We can use this to obtain parses like the following: &gt; (parser &apos;( [ a + a ] * a) &apos;E) ((E (T (F [ (E (T (F A)) + (E (T (F A)))) ]) * (T (F A))))). We do not include here a proof that the memoized parse does the same work as Earley&apos;s algorithm, but it is not too difficult to show. Interested readers can insert </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, A.V. and Ullman, J.D. (1972). The Theory of Parsing, Translation, and Compiling, Vol 1: Parsing. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.&amp;quot;</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery</journal>
<volume>6</volume>
<issue>2</issue>
<pages>451--455</pages>
<contexts>
<context position="6795" citStr="Earley (1970)" startWordPosition="1095" endWordPosition="1096">n a parse list is an item — a production rule with one indicator showing how much of the righthand side has been parsed, and another saying where the parse started. Kay (1980) introduced a similar data structure called a chart, along with a family of parsing algorithms that operate on the chart. Chart parsing is described in virtually all recent texts on natural language processing; for example, Winograd (1983) devotes 19 pages to the topic. It is part of the &amp;quot;folklore&amp;quot; on parsing that these algorithms can be thought of as tabular (i.e., memoized) versions of corresponding simpler algorithms. Earley (1970) himself mentions it, and Shieber (1989) gives a general abstract parsing strategy, and then proves that Earley&apos;s algorithm can be derived by suitable constraints on control of the strategy. 93 Computational Linguistics Volume 17, Number 1 This paper&apos;s contribution is a concrete demonstration of just how direct the correspondence is between the simple and the efficient algorithm. We present a simple parser which, when memoized, performs the same calculations as Earley&apos;s algorithm. The core of the parser is only 15 lines of code: (defun parse (tokens start-symbol) &amp;quot;Parse a list of tokens, retur</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. (1970). &amp;quot;An efficient context-free parsing algorithm.&amp;quot; Communications of the Association for Computing Machinery 6(2), 451-455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Field</author>
<author>P C Harrison</author>
</authors>
<title>Functional Programming.</title>
<date>1988</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="875" citStr="Field and Harrison (1988)" startWordPosition="127" endWordPosition="130"> parser, when augmented by automatic memoization. The memoized parser has the same complexity as Fancy&apos;s algorithm, but parses constituents in a different order. Techniques for deriving memo functions are described, with a complete implementation in Common Lisp, and an outline of a macro-based approach for other languages. 1. Memoization The term memoization was coined by Donald Michie (1968) to refer to the process by which a function is made to automatically remember the results of previous computations. The idea has become more popular in recent years with the rise of functional languages; Field and Harrison (1988) devote a whole chapter to it. The basic idea is just to keep a table of previously computed input/result pairs. In Common Lisp one could write:1 (defun memo (fn) &amp;quot;Return a memo—function of fn.&amp;quot; (let ((table (make—hash—table))) #&apos;(lambda (x) (multiple—value—bind (val found) (gethash x table) (if found val (sett (gethash x table) (funcall fn (For those familiar with Lisp but not Common Lisp, gethash returns two values, the stored entry in the table, and a boolean flag indicating if there is in fact an entry. The special form multiple-value-bind binds these two values to the symbols val and foun</context>
<context position="13566" citStr="Field and Harrison (1988)" startWordPosition="2180" endWordPosition="2183">ree Parsing fact, the macro solution even works in languages without first class functional objects, if a sufficiently powerful macro facility is available, or if the language&apos;s parser can be modified. For example, one could add the keyword memo to Pascal, and write the following: memo function fib(n:integer) : integer; begin if n &lt;= 1 then fib := n else fib := fib(n-1) + fib(n-2); end; and have it parsed as if it were: function fib(n:integer) : integer; begin if not inTable(n) then if n &lt;= 1 then addToTable(n,n) else addToTable(n,fib(n-1) + fib(n-2)); fib := tableValue(n); end; Texts such as Field and Harrison (1988) assume that memoization is done by a built-in feature of the language that in effect implements this kind of source-to-source transformation. This paper shows that memoization can also be done completely within the language — provided the language has either (a) a macro facility or (b) both higher-order functions and a way to set function bindings. 4. Conclusion What has been gained by memoization? The memoized algorithms presented here are of the same order of complexity as the existing Earley algorithm and linear-recursive Fibonacci function. In fact, unless tables are implemented very care</context>
</contexts>
<marker>Field, Harrison, 1988</marker>
<rawString>Field, A.J. and Harrison, P.C. (1988). Functional Programming. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>K Sparck-Jones</author>
<author>B L Webber</author>
</authors>
<date>1986</date>
<booktitle>Readings in Natural Language Processing.</booktitle>
<publisher>Morgan Kaufman.</publisher>
<marker>Grosz, Sparck-Jones, Webber, 1986</marker>
<rawString>Grosz, B.J., Sparck-Jones, K., and Webber, B.L. (1986). Readings in Natural Language Processing. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J M Hughes</author>
</authors>
<title>Lazy memo functions.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, Conference on Functional Programming and Computer Architecture,</booktitle>
<pages>129--146</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="5915" citStr="Hughes (1985)" startWordPosition="953" endWordPosition="954">ly to be out of the working set. We might also want a hash mechanism that caches only recent entries, and discards earlier ones, as originally suggested by Michie (1968). The user also has the responsibility of choosing the appropriate hashing function. By choosing eql instead of equal hashing, for example, hashing overhead will be reduced, but computation will be duplicated for equal lists. A compromise is to use eql hashing in conjunction with unique or canonical lists (Szolovits and Martin, 1981). eql hashing has the additional advantage of allowing infinite circular lists, as discussed by Hughes (1985). 2. Context-Free Parsing All efficient algorithms for parsing context-free grammars make use of some kind of well-formed substring table. Earley&apos;s algorithm (1970) is perhaps the best-known example. The algorithm builds up a vector of parse lists, where each entry in a parse list is an item — a production rule with one indicator showing how much of the righthand side has been parsed, and another saying where the parse started. Kay (1980) introduced a similar data structure called a chart, along with a family of parsing algorithms that operate on the chart. Chart parsing is described in virtua</context>
</contexts>
<marker>Hughes, 1985</marker>
<rawString>Hughes, R.J.M. (1985). &amp;quot;Lazy memo functions.&amp;quot; In Proceedings, Conference on Functional Programming and Computer Architecture, 129-146. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm schemata and data structures in syntactic processing.&amp;quot;</title>
<date>1980</date>
<booktitle>In Proceedings, Symposium on Text Processing, Nobel Academy.</booktitle>
<contexts>
<context position="6357" citStr="Kay (1980)" startWordPosition="1026" endWordPosition="1027">with unique or canonical lists (Szolovits and Martin, 1981). eql hashing has the additional advantage of allowing infinite circular lists, as discussed by Hughes (1985). 2. Context-Free Parsing All efficient algorithms for parsing context-free grammars make use of some kind of well-formed substring table. Earley&apos;s algorithm (1970) is perhaps the best-known example. The algorithm builds up a vector of parse lists, where each entry in a parse list is an item — a production rule with one indicator showing how much of the righthand side has been parsed, and another saying where the parse started. Kay (1980) introduced a similar data structure called a chart, along with a family of parsing algorithms that operate on the chart. Chart parsing is described in virtually all recent texts on natural language processing; for example, Winograd (1983) devotes 19 pages to the topic. It is part of the &amp;quot;folklore&amp;quot; on parsing that these algorithms can be thought of as tabular (i.e., memoized) versions of corresponding simpler algorithms. Earley (1970) himself mentions it, and Shieber (1989) gives a general abstract parsing strategy, and then proves that Earley&apos;s algorithm can be derived by suitable constraints</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Kay, M. (1980). &amp;quot;Algorithm schemata and data structures in syntactic processing.&amp;quot; In Proceedings, Symposium on Text Processing, Nobel Academy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Michie</author>
</authors>
<title>Memo functions and machine learning.&amp;quot;</title>
<date>1968</date>
<journal>Nature,</journal>
<volume>218</volume>
<contexts>
<context position="645" citStr="Michie (1968)" startWordPosition="90" endWordPosition="91">ues for Automatic Memoization with Applications to Context-Free Parsing Peter Norvig. University of California It is shown that a process similar to Earley&apos;s algorithm can be generated by a simple top-down backtracking parser, when augmented by automatic memoization. The memoized parser has the same complexity as Fancy&apos;s algorithm, but parses constituents in a different order. Techniques for deriving memo functions are described, with a complete implementation in Common Lisp, and an outline of a macro-based approach for other languages. 1. Memoization The term memoization was coined by Donald Michie (1968) to refer to the process by which a function is made to automatically remember the results of previous computations. The idea has become more popular in recent years with the rise of functional languages; Field and Harrison (1988) devote a whole chapter to it. The basic idea is just to keep a table of previously computed input/result pairs. In Common Lisp one could write:1 (defun memo (fn) &amp;quot;Return a memo—function of fn.&amp;quot; (let ((table (make—hash—table))) #&apos;(lambda (x) (multiple—value—bind (val found) (gethash x table) (if found val (sett (gethash x table) (funcall fn (For those familiar with Li</context>
<context position="5471" citStr="Michie (1968)" startWordPosition="885" endWordPosition="886"> &amp;quot;Replace fn-name&apos;s global definition with a memoized version.&amp;quot; (setf (symbol-function fn-name) (memo (symbol-function fn-name) :name fn-name :key key :test test))) (defun clear-memoize (fn-name) (clrhash (get fn-name &apos;memo))) Also note that the hash table is stored away on the function name&apos;s property list, so that it can be inspected or cleared at will. The intent is that the user&apos;s program should clear the table when the results are likely to be out of the working set. We might also want a hash mechanism that caches only recent entries, and discards earlier ones, as originally suggested by Michie (1968). The user also has the responsibility of choosing the appropriate hashing function. By choosing eql instead of equal hashing, for example, hashing overhead will be reduced, but computation will be duplicated for equal lists. A compromise is to use eql hashing in conjunction with unique or canonical lists (Szolovits and Martin, 1981). eql hashing has the additional advantage of allowing infinite circular lists, as discussed by Hughes (1985). 2. Context-Free Parsing All efficient algorithms for parsing context-free grammars make use of some kind of well-formed substring table. Earley&apos;s algorith</context>
</contexts>
<marker>Michie, 1968</marker>
<rawString>Michie, D. (1968). &amp;quot;Memo functions and machine learning.&amp;quot; Nature, 218, 19-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Sheiber</author>
</authors>
<title>Parsing and type inference for natural and computer languages.&amp;quot;</title>
<date>1989</date>
<journal>SRI International Technical Note</journal>
<volume>460</volume>
<marker>Sheiber, 1989</marker>
<rawString>Sheiber, Stuart M. (1989). &amp;quot;Parsing and type inference for natural and computer languages.&amp;quot; SRI International Technical Note 460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Szolovits</author>
<author>W A Martin</author>
</authors>
<title>Brand X: Lisp support for semantic networks.&amp;quot;</title>
<date>1981</date>
<booktitle>In Proceedings, 7th IJCAI,</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="5806" citStr="Szolovits and Martin, 1981" startWordPosition="935" endWordPosition="938">can be inspected or cleared at will. The intent is that the user&apos;s program should clear the table when the results are likely to be out of the working set. We might also want a hash mechanism that caches only recent entries, and discards earlier ones, as originally suggested by Michie (1968). The user also has the responsibility of choosing the appropriate hashing function. By choosing eql instead of equal hashing, for example, hashing overhead will be reduced, but computation will be duplicated for equal lists. A compromise is to use eql hashing in conjunction with unique or canonical lists (Szolovits and Martin, 1981). eql hashing has the additional advantage of allowing infinite circular lists, as discussed by Hughes (1985). 2. Context-Free Parsing All efficient algorithms for parsing context-free grammars make use of some kind of well-formed substring table. Earley&apos;s algorithm (1970) is perhaps the best-known example. The algorithm builds up a vector of parse lists, where each entry in a parse list is an item — a production rule with one indicator showing how much of the righthand side has been parsed, and another saying where the parse started. Kay (1980) introduced a similar data structure called a cha</context>
</contexts>
<marker>Szolovits, Martin, 1981</marker>
<rawString>Szolovits, P. and Martin, W.A. (1981). &amp;quot;Brand X: Lisp support for semantic networks.&amp;quot; In Proceedings, 7th IJCAI, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Language as a Cognitive Process.</title>
<date>1983</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="6596" citStr="Winograd (1983)" startWordPosition="1063" endWordPosition="1064">text-free grammars make use of some kind of well-formed substring table. Earley&apos;s algorithm (1970) is perhaps the best-known example. The algorithm builds up a vector of parse lists, where each entry in a parse list is an item — a production rule with one indicator showing how much of the righthand side has been parsed, and another saying where the parse started. Kay (1980) introduced a similar data structure called a chart, along with a family of parsing algorithms that operate on the chart. Chart parsing is described in virtually all recent texts on natural language processing; for example, Winograd (1983) devotes 19 pages to the topic. It is part of the &amp;quot;folklore&amp;quot; on parsing that these algorithms can be thought of as tabular (i.e., memoized) versions of corresponding simpler algorithms. Earley (1970) himself mentions it, and Shieber (1989) gives a general abstract parsing strategy, and then proves that Earley&apos;s algorithm can be derived by suitable constraints on control of the strategy. 93 Computational Linguistics Volume 17, Number 1 This paper&apos;s contribution is a concrete demonstration of just how direct the correspondence is between the simple and the efficient algorithm. We present a simpl</context>
</contexts>
<marker>Winograd, 1983</marker>
<rawString>Winograd, T. (1983). Language as a Cognitive Process. Addison-Wesley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>