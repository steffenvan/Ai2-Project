<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000633">
<title confidence="0.9042065">
A Divide-and-Conquer Strategy for Shallow Parsing of German
Free Texts
</title>
<author confidence="0.871755">
Gunter Neumann* Christian Braunt Jakub PiskorskiI
</author>
<sectionHeader confidence="0.959767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942133333333">
We present a divide-and-conquer strategy based on
finite state technology for shallow parsing of real-
world German texts. In a first phase only the topo-
logical structure of a sentence (i.e., verb groups,
subclauses) are determined. In a second phase the
phrasal grammars are applied to the contents of the
different fields of the main and sub-clauses. Shallow
parsing is supported by suitably configured prepro-
cessing, including: morphological and on-line com-
pound analysis, efficient POS-filtering, and named
entity recognition. The whole approach proved to
be very useful for processing of free word order lan-
guages like German. Especially for the divide-and-
conquer parsing strategy we obtained an f-measure
of 87.14% on unseen data.
</bodyText>
<sectionHeader confidence="0.998519" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.950830275362319">
Current information extraction (IE) systems are
quite successful in efficient processing of large free
text collections due to the fact that they can provide
a partial understanding of specific types of text with
a certain degree of partial accuracy using fast and ro-
bust language processing strategies (basically finite
state technology). They have been &amp;quot;made sensitive&amp;quot;
to certain key pieces of information and thereby pro-
vide an easy means to skip text without deep anal-
ysis. The majority of existing IE systems are ap-
plied to English text, but there are now a number of
systems which process other languages as well (e.g.,
German (Neumann et al., 1997), Italian (Ciravegna
et al., 1999) or Japanese (Sekine and Nobata, 1998)).
The majority of current systems perform a partial
parsing approach using only very few general syntac-
tic knowledge for the identification of nominal and
prepositional phrases and verb groups. The combi-
nation of such units is then performed by means of
domain-specific templates. Usually, these templates
* DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbrticken,
Germany, neumann0df ki . de
t DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken,
Germany, cbraun@dfki . de
DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken,
Germany, pi skorsk@df ki . de
are triggered by domain-specific predicates attached
only to a relevant subset of verbs which express
domain-specific selectional restrictions for possible
argument fillers.
In most of the well-known shallow text process-
ing systems (cf. (Sundheim, 1995) and (SAIC,
1998)) cascaded chunk parsers are used which per-
form clause recognition after fragment recognition
following a bottom-up style as described in (Abney,
1996). We have also developed a similar bottom-
up strategy for the processing of German texts, cf.
(Neumann et al., 1997). However, the main prob-
lem we experienced using the bottom-up strategy
was insufficient robustness: because the parser de-
pends on the lower phrasal recognizers, its perfor-
mance is heavily influenced by their respective per-
formance. As a consequence, the parser frequently
wasn&apos;t able to process structurally simple sentences,
because they contained, for example, highly complex
nominal phrases, as in the following example:
&amp;quot;[Die vom Bundesgerichtshof und den
Wettbewerbshiitern als VerstoB gegen
das Kartellverbot gegeiBelte zentrale TV-
Vermarktung] ist gangige Praxis.&amp;quot;
Central television marketing, censured by the
German Federal High Court and the guards
against unfair competition as an infringement
of anti-cartel legislation, is common practice.
During free text processing it might be not possible
(or even desirable) to recognize such a phrase com-
pletely. However, if we assume that domain-specific
templates are associated with certain verbs or verb
groups which trigger template filling, then it will be
very difficult to find the appropriate fillers without
knowing the correct clause structure. Furthermore
in a sole bottom-up approach some ambiguities — for
example relative pronouns — can&apos;t be resolved with-
out introducing much underspecification into the in-
termediate structures.
Therefore we propose the following divide-and-
conquer parsing strategy: In a first phase only
the verb groups and the topological structure of
a sentence according to the linguistic field the-
</bodyText>
<page confidence="0.995748">
239
</page>
<bodyText confidence="0.4518225">
tcoords Went Diese Angaben konnte der Bundes-
grenzschutz aber nicht best atigen],
</bodyText>
<figure confidence="0.833100375">
[SSent Kinkel
sprach von Horrorzahlen,
Lretcl
denen er keinen
Glauben schenkell].&amp;quot;
This information couldn&apos;t be verified by the Border
Police, Kinkel spoke of horrible figures that he didn&apos;t
believe.
</figure>
<figureCaption confidence="0.995251">
Figure 1: An example of a topological structure.
</figureCaption>
<figure confidence="0.979481214285714">
Text
PREPROCESSOR
TokenizeIon
Morphological Anatolia
POS-Fillesing
IlanwoFErally Finder
DC-PARSER
TOPOLOGICAL PARSER
FahlINPP
Boa Chums
t
Clauso-Canhinadon
FRAGMENT RECOGPSZER
Underspecifled dependency trees
</figure>
<figureCaption confidence="0.999681">
Figure 2: Overview of the system&apos;s architecture.
</figureCaption>
<bodyText confidence="0.998554">
ory (cf. (Engel, 1988)) are determined domain-
independently. In a second phase, general (as well
as domain-specific) phrasal grammars (nominal and
prepositional phrases) are applied to the contents of
the different fields of the main and sub-clauses (see
fig. 1)
This approach offers several advantages:
</bodyText>
<listItem confidence="0.9994049">
• improved robustness, because parsing of the
sentence topology is based only on simple in-
dicators like verbgroups and conjunctions and
their interplay,
• the resolution of some ambiguities, including
relative pronouns vs. determiner, subjunction
vs. preposition and sentence coordination vs.
NP coordination, and
• a high degree of modularity (easy integration of
domain-dependent subcomponents).
</listItem>
<bodyText confidence="0.998820625">
The shallow divide-and-conquer parser (DC-
PARSER) is supported by means of powerful mor-
phological processing (including on-line compound
analysis), efficient POS-filtering and named entity
recognition. Thus the architecture of the complete
shallow text processing approach consists basically
of two main components: the preprocessor and the
DC-PARSER itself (see fig. 2).
</bodyText>
<sectionHeader confidence="0.883692" genericHeader="introduction">
2 Preprocessor
</sectionHeader>
<bodyText confidence="0.999964431818182">
The DC-PARSER relies on a suitably configured pre-
processing strategy in order to achieve the desired
simplicity and performance. It consists of the fol-
lowing main steps:
Tokenization The tokenizer maps sequences of
consecutive characters into larger units called tokens
and identifies their types. Currently we use more
than 50 domain-independent token classes including
generic classes for semantically ambiguous tokens
(e.g., &amp;quot;10:15&amp;quot; could be a time expression or volley-
ball result, hence we classify this token as number-
dot compound) and complex classes like abbrevia-
tions or complex compounds (e.g., &amp;quot;AT&amp;T-Chief&amp;quot;).
It proved that such variety of token classes simpli-
fies the processing of subsequent submodules signif-
icantly.
Morphology Each token identified as a potential
wordform is submitted to the morphological analysis
including on-line recognition of compounds (which is
crucial since compounding is a very productive pro-
cess of the German language) and hyphen coordina-
tion (e.g., in &amp;quot;An- und Verkauf&amp;quot; (purchase and sale)
&amp;quot;An-&amp;quot; is resolved to &amp;quot;Ankauf&amp;quot; (purchase)). Each
token recognized as a valid word form is associated
with the list of its possible readings, characterized
by stem, inflection information and part of speech
category.
POS-filtering Since a high amount of German
word forms is ambiguous, especially word forms with
a verb readingl and due to the fact that the qual-
ity of the results of the DC-PARSER relies essen-
tially on the proper recognition of verb groups, ef-
ficient disambiguation strategies are needed. Using
case-sensitive rules is straightforward since generally
only nouns (and proper names) are written in stan-
dard German with a capitalized initial letter (e.g.,
&amp;quot;das Unternehmen&amp;quot; - the enterprise vs. &amp;quot;wir un-
ternehmen&amp;quot; - we undertake). However for disam-
biguation of word forms appearing at the beginning
of the sentence local contextual filtering rules are
applied. For instance, the rule which forbids the
verb written with a capitalized initial letter to be
followed by a finite verb would filter out the verb
reading of the word &amp;quot;unternehmen&amp;quot; in the sentence
</bodyText>
<footnote confidence="0.995529">
130% of the wordforms in the test corpus
&amp;quot;Wirtschaftswoche&amp;quot; (business news journal), which have a
verb reading, turned to have at least one other non-verb
reading.
</footnote>
<page confidence="0.996816">
240
</page>
<bodyText confidence="0.999898425531915">
&amp;quot;Unternehmen sind an Gewinnmaxirnierung intere-
siert.&amp;quot; (Enterprises are interested in maximizing
their profits). A major subclass of ambiguous word-
forms are those which have an adjective or attribu-
tivly used participle reading beside the verb reading.
For instance, in the sentence &amp;quot;Sie bekannten, die
bekannten Bilder gestohlen zu haben.&amp;quot; (They con-
fessed they have stolen the famous paintings.) the
wordform &amp;quot;bekannten&amp;quot; is firstly used as a verb (con-
fessed) and secondly as an adjective (famous). Since
adjectives and attributively used participles are in
most cases part of a nominal phrase a convenient
rule would reject the verb reading if the previous
word form is a determiner or the next word form is
a noun. It is important to notice that such rules
are based on some regularities, but they may yield
false results, like for instance the rule for filtering
out the verb reading of some word forms extremely
rarely used as verbs (e.g., &amp;quot;recht&amp;quot; - right, to rake
(3rd person,sg)). All rules are compiled into a sin-
gle finite-state transducer according to the approach
described in (Roche and Schabes, 1995).2
Named entity finder Named entities such as or-
ganizations, persons, locations and time expressions
are identified using finite-state grammars. Since
some named entities (e.g. company names) may ap-
pear in the text either with or without a designator,
we use a dynamic lexicon to store recognized named
entities without their designators (e.g., &amp;quot;Braun AG&amp;quot;
vs. &amp;quot;Braun&amp;quot;) in order to identify subsequent occur-
rences correctly. However a named entity, consisting
solely of one word, may be also a valid word form
(e.g., &amp;quot;Braun&amp;quot; – brown). Hence we classify such
words as candidates for named entities since gen-
erally such ambiguities cannot be resolved at this
level. Recognition of named entities could be post-
poned and integrated into the fragment recognizer,
but performing this task at this stage of processing
seems to be more appropriate. Firstly because the
results of POS-filtering could be partially verified
and improved and secondly the amount of the word
forms to be processed by subsequent modules could
be considerably reduced. For instance the verb read-
ing of the word form &amp;quot;achten&amp;quot; (watch vs. eight) in
the time expression &amp;quot;am achten Oktober 1995&amp;quot; (at
the eight of the October 1995) could be filtered out
if not done yet.
</bodyText>
<sectionHeader confidence="0.9527715" genericHeader="method">
3 A Shallow Divide-and-Conquer
Strategy
</sectionHeader>
<bodyText confidence="0.9977735">
The DC-PARSER consists of two major domain-
independent modules based on finite state technol-
</bodyText>
<footnote confidence="0.9050716">
2The manually constructed rules proved to be a useful
means for disambiguation, however not sufficient enough to
filter out all unplausible readings. Hence supplementary rules
determined by Brill&apos;s tagger were used in order to achieve
broader coverage.
</footnote>
<bodyText confidence="0.997077">
ogy: 1) construction of the topological sentence
structure, and 2) application of phrasal grammars
on each determined subclause (see also fig. 3). In
this paper we will concentrate on the first step, be-
cause it is the more novel part of the DC-PARSER,
and will only briefly describe the second step in sec-
tion 3.2.
</bodyText>
<subsectionHeader confidence="0.999829">
3.1 Topological structure
</subsectionHeader>
<bodyText confidence="0.983935261904762">
The DC-PARSER applies cascades of finite-state
grammars to the stream of tokens and named en-
tities delivered by the preprocessor in order to de-
termine the topological structure of the sentence ac-
cording to the linguistic field theory (Engel, 1988).3
Based on the fact that in German a verb group
(like &amp;quot;hatte fiberredet werden miissen&amp;quot; — *have con-
vinced been should meaning should have been con-
vinced) can be split into a left and a right verb part
(&amp;quot;hatte&amp;quot; and &amp;quot;ffberredet werden miissen&amp;quot;) these
parts (abbreviated as LVP and RvP) are used for the
segmentation of a main sentence into several parts:
the front field (vF), the left verb part, middle field
(MF), right verb part, and rest field (RF). Subclauses
can also be expressed in that way such that the left.
verb part is either empty or occupied by a relative
pronoun or a subjunction element, and the complete
verb group is placed in the right verb part, cf. figure
3. Note that each separated field can be arbitrarily
complex with very few restrictions on the ordering
of the phrases inside a field.
Recognition of the topological structure of a sen-
tence can be described by the following four phases
realized as cascade of finite state grammars (see also
fig. 2; fig. 4 shows the different steps in action).4
Initially, the stream of tokens and named entities is
separated into a list of sentences based on punctua-
tion signs.&apos;
Verb groups A verb grammar recognizes all sin-
gle occurrences of verbforms (in most cases corre-
sponding to LVP) and all closed verbgroups (i.e., se-
quences of verbforms, corresponding to RvP). The
parts of discontinuous verb groups (e.g., separated
LVP and RVP or separated verbs and verb-prefixes)
cannot be put together at that step of processing
because one needs contextual information which will
only be available in the next steps. The major prob-
lem at this phase is not a structural one but the
3Details concerning the implementation of the topological
parsing strategy can be found in (Braun, 1999). Details con-
cerning the representation and compilation of the used finite
state machinery can be found in (Neumann et al., 1997)
</bodyText>
<footnote confidence="0.89756475">
41n this paper we can give only a brief overview of the
current coverage of the individual steps. An exhaustive de-
scription of the covered phenomena can be found in (Braun,
1999).
5 Performing this step after preprocessing has the advan-
tage that the tokenizer and named entity finder already have
determined abbreviation signs, so that this sort of disam-
biguation is resolved.
</footnote>
<page confidence="0.966742">
241
</page>
<table confidence="0.998595672727273">
OF:
WERNSAT2i
MF: / r$EM: [NERD, ( &amp;quot;sie&amp;quot;) 3 1 r:Emrpl: [HEAD: ( &amp;quot;ektie&amp;quot;) ]])
\ LACR: *Morphix-FVektorir LACR: inorphox-FVektorf
SENT-MARKER: &amp;quot;.&amp;quot;
VERB: OMODVERBi
FORM: &amp;quot;owlets werkayfen&amp;quot;
MORPNO-INFOI ACR: OtkoiNlix-FVektork
FINIT: T
GENUS: :AK
[ TENSE: :PRES
SCOPE: *VERB*
FORM: &apos;werkaufen&amp;quot;
MCRPHO-INFO: ACR: enorphix-FVektori
{ FINIT: :INF
GENUS: 011(
TENSE: :PRES
STEM: &amp;quot;verkauf&amp;quot;
STEM: &apos;INAMS&amp;quot;
iSUBJ-CLS
CONTENT: OSPANNSATBK
MF: -OREL-CI.*
CONTENT:
[sEn. rCOMP-FORM: &amp;quot;GmbH&amp;quot;11
LNAME: &amp;quot;S.esens &apos; j
*NAME -hre
ACR: *Morphoc-FVektore
ITHN1:gt
[COMP: (HEAD: ( .e.pork&amp;quot;)
OPP*
*etk*%il-FEZ7leKR0rt&amp;quot;)
*AP*
AGR: Knorphix-FVektore
VERB
•OVERBY
FORM: INK!&amp;quot;
MORPHO-INFO:
[A.C.R1 Knorphix-FVektore]
FINIT: 7
GENUS: :AK
TENSE: ,PRE
STEM: &amp;quot;leb&apos;
REL-PRON: (&apos; die&amp;quot;)
[
VERB: OVERBY
FORM: &apos;orlitten Matte
MORPHO-INFO: [ACR: iMorphix-FVektorei
Finn&apos;: &apos;I
GENUS: tAK
TENSE: ,PERF
STEM; &amp;quot;erle:d&amp;quot;
SUBCONJ:
.&amp;quot;. {SEM: [HEAD: ( &amp;quot;war lust&amp;quot;) 1
SNP,
MR: *Morph i x-FVektore
</table>
<figureCaption confidence="0.980462">
Figure 3: The result of the DC-PARSER for the sentence &amp;quot;Weil die Siemens GmbH, die vom Export lebt,
</figureCaption>
<bodyText confidence="0.957631166666667">
Verluste erlitten hat, musste sie Aktien verkaufen.&amp;quot; (Because the Siemens GmbH which strongly depends on
exports suffered from losses they had to sell some of the shares.) abbreviated where convenient. It shows
the separation of a sentence into the front field (vF), the verb group (VERB), and the middle field (AF). The
elements of different fields have been computed by means of fragment recognition which takes place after
the (possibly recursive) topological structure has been computed. Note that the front field consists only of
one but complex subclause which itself has an internal field structure.
</bodyText>
<figure confidence="0.8037955">
Well die Siemens GmbH, die von Export lebt, Verluste erlitt, musste sie Aktien verkauf en.
41
Weil die Siemens GmbH, die . . . [Verb-Fin] , Verl . [Verb-Fin] , [Modv-Fin] sie Akt . [FV-Inf] .
Weil die Siemens GmbH [Rel-Ci], Verluste [Verb-Fin] , [Modv-Fin] sie Aktien [FV-Inf] .
J.
[Subconj -CL] , [Modv-F in] sie Akt i en [FV-Inf] .
[Subconj -CL] , [Modv-Fin] sie Aktien [FV-Inf ] .
[clause]
</figure>
<figureCaption confidence="0.999977">
Figure 4: The different steps of the DC-PARSER for the sentence of figure 3.
</figureCaption>
<bodyText confidence="0.999763055555556">
massive rnorphosyntactic ambiguity of verbs (for ex-
ample, most plural verb forms can also be non-finite
or imperative forms). This kind of ambiguity can-
not be resolved without taking into account a wider
context. Therefore these verb forms are assigned dis-
junctive types, similar to the underspecified chunk
categories proposed by (Federici et al., 1996). These
types, like for example Fin-Inf -PP or Fin-PP, re-
flect the different readings of the verbform and en-
able following modules to use these verb forms ac-
cording to the wider context, thereby removing the
ambiguity. In addition to a type each recognized
verb form is assigned a set of features which rep-
resent various properties of the form like tense and
mode information. (cf. figure 5).
Base clauses (BC) are subclauses of type sub-
junctive and subordinate. Although they are embed-
ded into a larger structure they can independently
</bodyText>
<page confidence="0.992888">
242
</page>
<figureCaption confidence="0.949406666666667">
Figure 5: The structure of the verb fragment &amp;quot;nicht
gelobt haben kann&amp;quot; — *not praised have could-been
meaning could not have been praised
</figureCaption>
<bodyText confidence="0.98034625">
and simply be recognized on the basis of commas,
initial elements (like complementizer, interrogative
or relative item — see also fig. 4, where SUBCONJ-
CL and REL-CL are tags for subclauses) and verb
fragments. The different types of subclauses are de-
scribed very compactly as finite state expressions.
Figure 6 shows a (simplified) BC-structure in fea-
ture matrix notation.
</bodyText>
<figure confidence="0.996106375">
&apos;Type Subj-Cl
Subj wenn
&apos;Type Spannsatz
[Type Verb I
Form stellten
Cont
NF
■
</figure>
<figureCaption confidence="0.7756182">
Figure 6: Simplified feature matrix of the base clause
‘`..., wenn die Arbeitgeber Forderungen stellten,
ohne als Gegenleistung neue Stellen zu schaffen.&amp;quot;
if the employers make new demands, without compensat-
ing by creating new jobs.
</figureCaption>
<bodyText confidence="0.999658125">
Clause combination It is very often the case that
base clauses are recursively embedded as in the fol-
lowing example:
weil der Hund den Braten gefressen
hatte, den die Frau, nachdem sie ihn zu-
bereitet hatte, auf die Fensterbank gestellt
hatte.
Because the dog ate the beef which was put on
the window sill after it had been prepared by
the woman.
Two sorts of recursion can be distinguished: 1)
middle field (MF) recursion, where the embedded
base clause is framed by the left and right verb parts
of the embedding sentence, and 2) the rest field (RF)
recursion, where the embedded clause follows the
right verb part of the embedding sentence. In order
to express and handle this sort of recursion using
a finite state approach, both recursions are treated
as iterations such that they destructively substitute
recognized embedded base clauses with their type.
Hence, the complexity of the recognized structure
of the sentence is reduced successively. However,
because subclauses of MF-recursion may have their
own embedded RF-recursion the CLAUSE COMBINA-
TION (CC) is used for bundling subsequent base
clauses before they would be combined with sub-
clauses identified by the outer MF-recursion. The
BC and CC module are called until no more base
clauses can be reduced. If the CC module would not
be used, then the following incorrect segmentation
could not be avoided:
*[dal3 das Gliick [, das Jochen
Kroehne empfunden haben sollte Rel-C11
r, als ihm jiingst sein GroBaktionar die
Obertragungsrechte bescherte Subj -C11,
nicht mehr so recht erwarmt Subj -Cl]
In the correct reading the second subclause &amp;quot;... als
ihm jiingst sein ...&amp;quot; is embedded into the first one
&amp;quot;... das Jochen Kroehne ...&amp;quot;.
Main clauses (MC) Finally the MC module
builds the complete topological structure of the in-
put sentence on the basis of the recognized (remain-
ing) verb groups and base clauses, as well as on the
word form information not yet consumed. The latter
includes basically punctuations and coordinations.
The following figure schematically describes the cur-
rent coverage of the implemented MC-module (see
figure 1 for an example structure):
</bodyText>
<equation confidence="0.507877">
CSent ::= . . . LVP [RVP]
SSent LVP [RVP]
CoordS ::= CSent ( , CSent)* Coord CSent I
</equation>
<bodyText confidence="0.98525425">
::= CSent (,SSent)* Coord SSent
AsyndSent ::= CSent , CSent
CmpCSent CSent , SSent CSent , CSent
AsyndCond ::= SSent , SSent
</bodyText>
<subsectionHeader confidence="0.999677">
3.2 Phrase recognition
</subsectionHeader>
<bodyText confidence="0.999740846153846">
After the topological structure of a sentence has been
identified, each substring is passed to the FRAG-
MENT RECOGNIZER in order to determine the in-
ternal phrasal structure. Note that processing of
a substring might still be partial in the sense that
no complete structure need be found (e.g., if we
cannot combine sequences of phrases to one larger
unit). The FRAGMENT RECOGNIZER uses finite state
grammars in order to extract nominal and preposi-
tional phrases, where the named entities recognized
by the preprocessor are integrated into appropriate
places (unplausible phrases are rejected by agree-
ment checking; see (Neumann et al., 1997) for more
</bodyText>
<figure confidence="0.998497904761905">
&apos;Type VG-final
Subtype Mod-Pert -Ak
Modal-stem konn
Stem lob
Form nicht gelobt haben kann
Neg
Agr
Verb
MF (die Arbeitgeber Forderungen)
/ -Type
Subj
Cont
Inf-C1
ohne
Verb
MF
(als Gegenleistung
neue Stellen)
[Type Simple-mi
[Type Verb
Form zu schaffen
</figure>
<page confidence="0.998307">
243
</page>
<bodyText confidence="0.99996328">
details)). The phrasal recognizer currently only con-
siders processing of simple, non-recursive structures
(see fig. 3; here, *NP* and *PP* are used for de-
noting phrasal types). Note that because of the
high degree of modularity of our shallow parsing
architecture, it is very easy to exchange the cur-
rently domain-independent fragment recognizer with
a domain-specific one, without effecting the domain-
independent DC-PARSER.
The final output of the parser for a sentence is an
underspecified dependence structure UDS. An UDS
is a flat dependency-based structure of a sentence,
where only upper bounds for attachment and scop-
ing of modifiers are expressed. This is achieved by
collecting all NPs and PPs of a clause into sepa-
rate sets as long as they are not part of some sub-
clauses. This means that although the exact attach-
ment point of each individual PP is not known it
is guaranteed that a PP can only be attached to
phrases which are dominated by the main verb of the
sentence (which is the root node of the clause&apos;s tree).
However, the exact point of attachment is a matter
of domain-specific knowledge and hence should be
defined as part of the domain knowledge of an ap-
plication.
</bodyText>
<sectionHeader confidence="0.993134" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99958175">
Due to the limited space, we concentrate on the
evaluation of the topological structure. An eval-
uation of the other components (based on a sub-
set of 20.000 tokens of the mentioned corpus from
the &amp;quot;Wirtschaftswoche&amp;quot;, see below) yields: From
the 93,89% of the tokens which were identified by
the morphological component as valid word forms,
95,23% got a unique POS-assignment with an ac-
curacy of 97,9%. An initial evaluation on the same
subset yielded a precision of 95.77% and a recall of
85% (90.1% F-measure) for our current named en-
tity finder. Evaluation of the compound analysis
of nouns, i.e. how often a morphosyntactical cor-
rect segmentation was found yield: Based on the
20.000 tokens, 1427 compounds are found, where
1417 have the correct segmentation (0.9929% preci-
sion). On a smaller subset of 1000 tokens containing
102 compounds, 101 correct segmentations where
found (0.9901% recall), which is a quite promising
result. An evaluation of simple NPs yielded a recall
of 0.7611% and precision of 0.9194%. The low recall
was mainly because of unknown words.
During the 2nd and 5th of July 1999 a test cor-
pus of 43 messages from different press releases (viz.
DEUTSCHE PREESSEAGENTUR (dpa), ASSOCIATED
PRESS (ap) and REUTERS) and different domains
(equal distribution of politics, business, sensations)
was collected.6 The corpus contains 400 sentences
6This data collection and evaluation was carried out by
(Braun, 1999).
with a total of 6306 words. Note that it also was
created after the DC-PARSER and all grammars were
finally implemented. Table 1 shows the result of
the evaluations (the F-measure was computed with
0=1). We used the correctness criteria as defined in
figure 7.
The evaluation of each component was measured
on the basis of the result of all previous components.
For the BC and MC module we also measured the
performance by manually correcting the errors of the
previous components (denoted as &amp;quot;isolated evalua-
tion&amp;quot;). In most cases the difference between the pre-
cision and recall values is quite small, meaning that
the modules keep a good balance between coverage
and correctness. Only in the case of the MC-module
the difference is about 5%. However, the result for
the isolated evaluation of the MC-module suggests
that this is mainly due to errors caused by previous
components.
A more detailed analysis showed that the major-
ity of errors were caused by mistakes in the prepro-
cessing phase. For example ten errors were caused
by an ambiguity between different verb stems (only
the first reading is chosen) and ten errors because
of wrong POS-filtering. Seven errors were caused by
unknown verb forms, and in eight cases the parser
failed because it could not properly handle the ambi-
guities of some word forms being either a separated
verb prefix or adverb.
The evaluation has been performed with the
Lisp-based version of SMES (cf. (Neumann et al.,
1997)) by replacing the original bidirectional shal-
low buttom-up parsing module with the DC-PARSER.
The average run-time per sentence (average length
26 words) is 0.57 sec. A C++-version is nearly
finished missing only the re-implementation of the
base and main clause recognition phases, cf. (Pisko-
rski and Neumann, 2000). The run-time behavior
is already encouraging: processing of a German text
document (a collection of business news articles from
the &amp;quot;Wirtschaftswoche&amp;quot; ) of 197118 tokens (1.26 MB)
needs 45 seconds on a PentiumIL 266 MHz, 128
RAM, which corresponds to 4380 tokens per second.
Since this is an increase in speed-up by a factor &gt; 20
compared to the Lisp-version, we expect to be able
to process 75-100 sentences per second.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999950888888889">
To our knowledge, there are only very few other
systems described which process free German texts.
The new shallow text processor is a direct succes-
sor of the one used in the SMES—System, an IE-core
system for real world German text processing (Neu-
mann et al., 1997). Here, a bidirectional verb-driven
bottom-up parser was used, where the problems de-
scribed in this paper concerning parsing of longer
sentences were encountered. Another similar divide-
</bodyText>
<page confidence="0.995491">
244
</page>
<note confidence="0.7944202">
Criterium Matching of annotated data and results Used by module
Borders start and end points verbforms, BC
Type start and end points, type verbforms, BC, MC
Partial start or end point, type BC
Top start and end points, type MC
for the largest tag
Struct 1 see Top, plus test of substructures MC
using Partial
Struct2 see Top, plus test of substructures MC
using Type
</note>
<figureCaption confidence="0.839538">
Figure 7: Correctness criteria used during evaluation.
</figureCaption>
<table confidence="0.999924135135135">
Verb-Module
correctness Verbfragments Recall Precision F-measure
criterium in % in % in %
total found correct
Borders 897 894 883 98.43 98.77 98.59
Type 897 894 880 98.10 98.43 98.26
Base-Clause-Module
correctness BC-Fragments Recall Precision F-measure
criterium in % in % in %
total found correct
Type 130 129 121 93.08 93.80 93.43
Partial 130 129 125 96.15 96.89 96.51
Base-Clause-Module (isolated evaluation)
correctness Base-Clauses Recall Precision F-measure
criterium in % in % in %
total found correct
Type 130 131 123 94.61 93.89 94.24
Partial 130 131 127 97.69 96.94 97.31
Main-Clause-Module
correctness Main-Clauses Recall Precision F-measure
criterium in % in % in %
total found correct
Top 400 377 361 90.25 95.75 92.91
Struct 1 400 377 361 90.25 95.75 92.91
Struct2 400 377 356 89.00 94.42 91.62
Main-Clause-Module (isolated evaluation)
correctness Main-Clauses Recall Precision F-measure
criterium in % in % in %
total found correct
Top 400 389 376 94.00 96.65 95.30
Struct 1 400 389 376 94.00 96.65 95.30
Struct2 400 389 372 93.00 95.62 94.29
complete analysis
correctness all components Recall Precision F-measure
criterium in % in % in %
total found correct
Struct2 400 377 339 84.75 89.68 87.14
</table>
<tableCaption confidence="0.999935">
Table 1: Results of the evaluation of the topological structure
</tableCaption>
<bodyText confidence="0.999855666666667">
and-conquer approach using a chart-based parser
for analysis of German text documents was pre-
sented by (Wauschkuhn, 1996). Nevertheless, com-
paring its performance with our approach seems to
be rather difficult since he only measures for an un-
annotated test corpus how often his parser finds at
least one result (where he reports 85.7% &amp;quot;coverage&amp;quot;
of a test corpus of 72.000 sentences) disregarding to
measure the accuracy of the parser. In this sense,
our parser achieved a &amp;quot;coverage&amp;quot; of 94.25% (com-
puting found/total), almost certainly because we
use more advanced lexical and phrasal components,
</bodyText>
<page confidence="0.994274">
245
</page>
<bodyText confidence="0.999947727272727">
e.g., pos-filter, compound and named entity process-
ing. (Peh and Ting, 1996) also describe a divide-
and-conquer approach based on statistical methods,
where the segmentation of the sentence is done by
identifying so called link words (solely punctuations,
conjunctions and prepositions) and disambiguating
their specific role in the sentence. On an annotated
test corpus of 600 English sentences they report an
accuracy of 85.1% based on the correct recognition of
part-of-speech, comma and conjunction disambigua-
tion, and exact noun phrase recognition.
</bodyText>
<sectionHeader confidence="0.987224" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999966903225806">
We have presented a divide-and-conquer strategy
for shallow analysis of German texts which is sup-
ported by means of powerful morphological process-
ing, efficient POS-filtering and named entity recog-
nition. Especially for the divide-and-conquer pars-
ing strategy we obtained an F-measure of 87.14%
on unseen data. Our shallow parsing strategy has
a high degree of modularity which allows the inte-
gration of the domain-independent sentence recog-
nition part with arbitrary domain-dependent sub-
components (e.g., specific named entity finders and
fragment recognizers).
Considered from an application-oriented point of
view, our main experience is that even if we are only
interested in some parts of a text (e.g., only in those
linguistic entities which verbalize certain aspects of
a domain-concept) we have to unfold the structural
relationship between all elements of a large enough
area (a paragraph or more) up to a certain level
of depth in which the relevant information is em-
bedded. Beside continuing the improvement of the
whole approach we also started investigations to-
wards the integration of deep processing into the
DC-PARSER. The core idea is to call a deep parser
only to the separated field elements which contain
sequences of simple NPs and PPs (already deter-
mined by the shallow parser). Thus seen the shallow
parser is used as an efficient preprocessor for divid-
ing a sentence into syntactically valid smaller units,
where the deep parser&apos;s task would be to identify the
exact constituent structure only on demand.
</bodyText>
<sectionHeader confidence="0.999315" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999988857142857">
The research underlying this paper was supported
by a research grant from the German Bundesmin-
isterium fiir Bildung, Wissenschaft, Forschung
und Technologie (BMBF) to the DFKI project
PARADIME, FKZ ITW 9704. Many thanks to
Thierry Declerck and Milena Valkova for their sup-
port during the evaluation of the system.
</bodyText>
<sectionHeader confidence="0.998964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999708925925926">
S. Abney. 1996. Partial parsing via finite-state cas-
cades. Proceedings of the ESSLLI 96 Robust Pars-
ing Workshop.
C. Braun. 1999. Flaches und robustes Parsen
Deutscher Satzgefiige. Master&apos;s thesis, University
of the Saarland.
F. Ciravegna, A. Lavelli, N. Mana, L. Gilardoni,
S. Mazza, M. Ferraro, J. Matiasek, W. Black,
F. Rinaldi, and D. Mowatt. 1999. Facile: Clas-
sifying texts integrating pattern matching and in-
formation extraction. In Proceedings of LICAI-99,
Stockholm.
Ulrich Engel. 1988. Deutsche Grammatik. Julius
Groos Verlag, Heidelberg, 2., improved edition.
S. Federici, S. Monyemagni, and V. Pirrelli. 1996.
Shallow parsing and text chunking: A view on un-
derspecification in syntax. In Workshop on Robust
Parsing, 8th ESSLLI, pages 35-44.
G. Neumann, R. Backofen, J. Baur, M. Becker,
and C. Braun. 1997. An information extraction
core system for real world german text processing.
In 5th International Conference of Applied Natu-
ral Language, pages 208-215, Washington, USA,
March.
L. Peh and Christopher H. Ting. 1996. A divide-
and-conquer strategy for parsing. In Proceedings
of the ACL/SIGPARSE 5th International Work-
shop on Parsing Technologies, pages 57-66.
J. Piskorski and G. Neumann. 2000. An intelligent
text extraction and navigation system. In 6th In-
ternational Conference on Computer-Assisted In-
formation Retrieval (RIAO-2000). Paris, April.
18 pages.
E. Roche and Y. Schabes. 1995. Deterministic part-
of-speech tagging with finite state transducers.
Computational Linguistics, 21(2):227-253.
SAIC, editor. 1998. Seventh Message
Understanding Conference (MUC-7),
http://www.muc.saic.com/. SAIC Information
Extraction.
S. Sekine and C. Nobata. 1998. An infor-
mation extraction system and a customization
tool. In Proceedings of Hitachi workshop-98,
http://cs.nyu.edu/cs/projects/proteus/sekine/.
B. Sundheim, editor. 1995. Sixth Message Un-
derstanding Conference (MUC-6), Washington.
Distributed by Morgan Kaufmann Publishers,
Inc.,San Mateo, California.
0. Wauschkuhn. 1996. Ein Werkzeug zur par-
tiellen syntaktischen Analyse deutscher Textko-
rpora. In Dafydd Gibbon, editor, Natural Lan-
guage Processing and Speech Technology. Results
of the Third KONVENS Conference, pages 356-
368. Mouton de Gruyter, Berlin.
</reference>
<page confidence="0.998719">
246
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897407">
<title confidence="0.95883">A Divide-and-Conquer Strategy for Shallow Parsing of German Free Texts</title>
<author confidence="0.987036">Gunter Neumann Christian Braunt Jakub PiskorskiI</author>
<abstract confidence="0.9990215">We present a divide-and-conquer strategy based on finite state technology for shallow parsing of realworld German texts. In a first phase only the topological structure of a sentence (i.e., verb groups, subclauses) are determined. In a second phase the phrasal grammars are applied to the contents of the different fields of the main and sub-clauses. Shallow parsing is supported by suitably configured preprocessing, including: morphological and on-line compound analysis, efficient POS-filtering, and named entity recognition. The whole approach proved to be very useful for processing of free word order languages like German. Especially for the divide-andconquer parsing strategy we obtained an f-measure of 87.14% on unseen data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<booktitle>Proceedings of the ESSLLI 96 Robust Parsing Workshop.</booktitle>
<contexts>
<context position="2583" citStr="Abney, 1996" startWordPosition="390" endWordPosition="391">ermany, neumann0df ki . de t DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, cbraun@dfki . de DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, pi skorsk@df ki . de are triggered by domain-specific predicates attached only to a relevant subset of verbs which express domain-specific selectional restrictions for possible argument fillers. In most of the well-known shallow text processing systems (cf. (Sundheim, 1995) and (SAIC, 1998)) cascaded chunk parsers are used which perform clause recognition after fragment recognition following a bottom-up style as described in (Abney, 1996). We have also developed a similar bottomup strategy for the processing of German texts, cf. (Neumann et al., 1997). However, the main problem we experienced using the bottom-up strategy was insufficient robustness: because the parser depends on the lower phrasal recognizers, its performance is heavily influenced by their respective performance. As a consequence, the parser frequently wasn&apos;t able to process structurally simple sentences, because they contained, for example, highly complex nominal phrases, as in the following example: &amp;quot;[Die vom Bundesgerichtshof und den Wettbewerbshiitern als V</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>S. Abney. 1996. Partial parsing via finite-state cascades. Proceedings of the ESSLLI 96 Robust Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Braun</author>
</authors>
<title>Flaches und robustes Parsen Deutscher Satzgefiige. Master&apos;s thesis,</title>
<date>1999</date>
<institution>University of the Saarland.</institution>
<contexts>
<context position="13192" citStr="Braun, 1999" startWordPosition="2044" endWordPosition="2045">signs.&apos; Verb groups A verb grammar recognizes all single occurrences of verbforms (in most cases corresponding to LVP) and all closed verbgroups (i.e., sequences of verbforms, corresponding to RvP). The parts of discontinuous verb groups (e.g., separated LVP and RVP or separated verbs and verb-prefixes) cannot be put together at that step of processing because one needs contextual information which will only be available in the next steps. The major problem at this phase is not a structural one but the 3Details concerning the implementation of the topological parsing strategy can be found in (Braun, 1999). Details concerning the representation and compilation of the used finite state machinery can be found in (Neumann et al., 1997) 41n this paper we can give only a brief overview of the current coverage of the individual steps. An exhaustive description of the covered phenomena can be found in (Braun, 1999). 5 Performing this step after preprocessing has the advantage that the tokenizer and named entity finder already have determined abbreviation signs, so that this sort of disambiguation is resolved. 241 OF: WERNSAT2i MF: / r$EM: [NERD, ( &amp;quot;sie&amp;quot;) 3 1 r:Emrpl: [HEAD: ( &amp;quot;ektie&amp;quot;) ]]) \ LACR: *Mor</context>
<context position="23438" citStr="Braun, 1999" startWordPosition="3705" endWordPosition="3706">ntaining 102 compounds, 101 correct segmentations where found (0.9901% recall), which is a quite promising result. An evaluation of simple NPs yielded a recall of 0.7611% and precision of 0.9194%. The low recall was mainly because of unknown words. During the 2nd and 5th of July 1999 a test corpus of 43 messages from different press releases (viz. DEUTSCHE PREESSEAGENTUR (dpa), ASSOCIATED PRESS (ap) and REUTERS) and different domains (equal distribution of politics, business, sensations) was collected.6 The corpus contains 400 sentences 6This data collection and evaluation was carried out by (Braun, 1999). with a total of 6306 words. Note that it also was created after the DC-PARSER and all grammars were finally implemented. Table 1 shows the result of the evaluations (the F-measure was computed with 0=1). We used the correctness criteria as defined in figure 7. The evaluation of each component was measured on the basis of the result of all previous components. For the BC and MC module we also measured the performance by manually correcting the errors of the previous components (denoted as &amp;quot;isolated evaluation&amp;quot;). In most cases the difference between the precision and recall values is quite sma</context>
</contexts>
<marker>Braun, 1999</marker>
<rawString>C. Braun. 1999. Flaches und robustes Parsen Deutscher Satzgefiige. Master&apos;s thesis, University of the Saarland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ciravegna</author>
<author>A Lavelli</author>
<author>N Mana</author>
<author>L Gilardoni</author>
<author>S Mazza</author>
<author>M Ferraro</author>
<author>J Matiasek</author>
<author>W Black</author>
<author>F Rinaldi</author>
<author>D Mowatt</author>
</authors>
<title>Facile: Classifying texts integrating pattern matching and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of LICAI-99,</booktitle>
<location>Stockholm.</location>
<contexts>
<context position="1570" citStr="Ciravegna et al., 1999" startWordPosition="241" endWordPosition="244"> efficient processing of large free text collections due to the fact that they can provide a partial understanding of specific types of text with a certain degree of partial accuracy using fast and robust language processing strategies (basically finite state technology). They have been &amp;quot;made sensitive&amp;quot; to certain key pieces of information and thereby provide an easy means to skip text without deep analysis. The majority of existing IE systems are applied to English text, but there are now a number of systems which process other languages as well (e.g., German (Neumann et al., 1997), Italian (Ciravegna et al., 1999) or Japanese (Sekine and Nobata, 1998)). The majority of current systems perform a partial parsing approach using only very few general syntactic knowledge for the identification of nominal and prepositional phrases and verb groups. The combination of such units is then performed by means of domain-specific templates. Usually, these templates * DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbrticken, Germany, neumann0df ki . de t DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, cbraun@dfki . de DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, pi skorsk@df ki . de are trig</context>
</contexts>
<marker>Ciravegna, Lavelli, Mana, Gilardoni, Mazza, Ferraro, Matiasek, Black, Rinaldi, Mowatt, 1999</marker>
<rawString>F. Ciravegna, A. Lavelli, N. Mana, L. Gilardoni, S. Mazza, M. Ferraro, J. Matiasek, W. Black, F. Rinaldi, and D. Mowatt. 1999. Facile: Classifying texts integrating pattern matching and information extraction. In Proceedings of LICAI-99, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Engel</author>
</authors>
<title>Deutsche Grammatik. Julius Groos Verlag,</title>
<date>1988</date>
<location>Heidelberg,</location>
<note>2., improved edition.</note>
<contexts>
<context position="4797" citStr="Engel, 1988" startWordPosition="709" endWordPosition="710">39 tcoords Went Diese Angaben konnte der Bundesgrenzschutz aber nicht best atigen], [SSent Kinkel sprach von Horrorzahlen, Lretcl denen er keinen Glauben schenkell].&amp;quot; This information couldn&apos;t be verified by the Border Police, Kinkel spoke of horrible figures that he didn&apos;t believe. Figure 1: An example of a topological structure. Text PREPROCESSOR TokenizeIon Morphological Anatolia POS-Fillesing IlanwoFErally Finder DC-PARSER TOPOLOGICAL PARSER FahlINPP Boa Chums t Clauso-Canhinadon FRAGMENT RECOGPSZER Underspecifled dependency trees Figure 2: Overview of the system&apos;s architecture. ory (cf. (Engel, 1988)) are determined domainindependently. In a second phase, general (as well as domain-specific) phrasal grammars (nominal and prepositional phrases) are applied to the contents of the different fields of the main and sub-clauses (see fig. 1) This approach offers several advantages: • improved robustness, because parsing of the sentence topology is based only on simple indicators like verbgroups and conjunctions and their interplay, • the resolution of some ambiguities, including relative pronouns vs. determiner, subjunction vs. preposition and sentence coordination vs. NP coordination, and • a h</context>
<context position="11448" citStr="Engel, 1988" startWordPosition="1745" endWordPosition="1746">hieve broader coverage. ogy: 1) construction of the topological sentence structure, and 2) application of phrasal grammars on each determined subclause (see also fig. 3). In this paper we will concentrate on the first step, because it is the more novel part of the DC-PARSER, and will only briefly describe the second step in section 3.2. 3.1 Topological structure The DC-PARSER applies cascades of finite-state grammars to the stream of tokens and named entities delivered by the preprocessor in order to determine the topological structure of the sentence according to the linguistic field theory (Engel, 1988).3 Based on the fact that in German a verb group (like &amp;quot;hatte fiberredet werden miissen&amp;quot; — *have convinced been should meaning should have been convinced) can be split into a left and a right verb part (&amp;quot;hatte&amp;quot; and &amp;quot;ffberredet werden miissen&amp;quot;) these parts (abbreviated as LVP and RvP) are used for the segmentation of a main sentence into several parts: the front field (vF), the left verb part, middle field (MF), right verb part, and rest field (RF). Subclauses can also be expressed in that way such that the left. verb part is either empty or occupied by a relative pronoun or a subjunction eleme</context>
</contexts>
<marker>Engel, 1988</marker>
<rawString>Ulrich Engel. 1988. Deutsche Grammatik. Julius Groos Verlag, Heidelberg, 2., improved edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Federici</author>
<author>S Monyemagni</author>
<author>V Pirrelli</author>
</authors>
<title>Shallow parsing and text chunking: A view on underspecification in syntax.</title>
<date>1996</date>
<booktitle>In Workshop on Robust Parsing, 8th ESSLLI,</booktitle>
<pages>35--44</pages>
<contexts>
<context position="16179" citStr="Federici et al., 1996" startWordPosition="2510" endWordPosition="2513">iemens GmbH [Rel-Ci], Verluste [Verb-Fin] , [Modv-Fin] sie Aktien [FV-Inf] . J. [Subconj -CL] , [Modv-F in] sie Akt i en [FV-Inf] . [Subconj -CL] , [Modv-Fin] sie Aktien [FV-Inf ] . [clause] Figure 4: The different steps of the DC-PARSER for the sentence of figure 3. massive rnorphosyntactic ambiguity of verbs (for example, most plural verb forms can also be non-finite or imperative forms). This kind of ambiguity cannot be resolved without taking into account a wider context. Therefore these verb forms are assigned disjunctive types, similar to the underspecified chunk categories proposed by (Federici et al., 1996). These types, like for example Fin-Inf -PP or Fin-PP, reflect the different readings of the verbform and enable following modules to use these verb forms according to the wider context, thereby removing the ambiguity. In addition to a type each recognized verb form is assigned a set of features which represent various properties of the form like tense and mode information. (cf. figure 5). Base clauses (BC) are subclauses of type subjunctive and subordinate. Although they are embedded into a larger structure they can independently 242 Figure 5: The structure of the verb fragment &amp;quot;nicht gelobt </context>
</contexts>
<marker>Federici, Monyemagni, Pirrelli, 1996</marker>
<rawString>S. Federici, S. Monyemagni, and V. Pirrelli. 1996. Shallow parsing and text chunking: A view on underspecification in syntax. In Workshop on Robust Parsing, 8th ESSLLI, pages 35-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
<author>R Backofen</author>
<author>J Baur</author>
<author>M Becker</author>
<author>C Braun</author>
</authors>
<title>An information extraction core system for real world german text processing.</title>
<date>1997</date>
<booktitle>In 5th International Conference of Applied Natural Language,</booktitle>
<pages>208--215</pages>
<location>Washington, USA,</location>
<contexts>
<context position="1536" citStr="Neumann et al., 1997" startWordPosition="236" endWordPosition="239"> systems are quite successful in efficient processing of large free text collections due to the fact that they can provide a partial understanding of specific types of text with a certain degree of partial accuracy using fast and robust language processing strategies (basically finite state technology). They have been &amp;quot;made sensitive&amp;quot; to certain key pieces of information and thereby provide an easy means to skip text without deep analysis. The majority of existing IE systems are applied to English text, but there are now a number of systems which process other languages as well (e.g., German (Neumann et al., 1997), Italian (Ciravegna et al., 1999) or Japanese (Sekine and Nobata, 1998)). The majority of current systems perform a partial parsing approach using only very few general syntactic knowledge for the identification of nominal and prepositional phrases and verb groups. The combination of such units is then performed by means of domain-specific templates. Usually, these templates * DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbrticken, Germany, neumann0df ki . de t DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, cbraun@dfki . de DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germ</context>
<context position="13321" citStr="Neumann et al., 1997" startWordPosition="2063" endWordPosition="2066">nd all closed verbgroups (i.e., sequences of verbforms, corresponding to RvP). The parts of discontinuous verb groups (e.g., separated LVP and RVP or separated verbs and verb-prefixes) cannot be put together at that step of processing because one needs contextual information which will only be available in the next steps. The major problem at this phase is not a structural one but the 3Details concerning the implementation of the topological parsing strategy can be found in (Braun, 1999). Details concerning the representation and compilation of the used finite state machinery can be found in (Neumann et al., 1997) 41n this paper we can give only a brief overview of the current coverage of the individual steps. An exhaustive description of the covered phenomena can be found in (Braun, 1999). 5 Performing this step after preprocessing has the advantage that the tokenizer and named entity finder already have determined abbreviation signs, so that this sort of disambiguation is resolved. 241 OF: WERNSAT2i MF: / r$EM: [NERD, ( &amp;quot;sie&amp;quot;) 3 1 r:Emrpl: [HEAD: ( &amp;quot;ektie&amp;quot;) ]]) \ LACR: *Morphix-FVektorir LACR: inorphox-FVektorf SENT-MARKER: &amp;quot;.&amp;quot; VERB: OMODVERBi FORM: &amp;quot;owlets werkayfen&amp;quot; MORPNO-INFOI ACR: OtkoiNlix-FVek</context>
<context position="20541" citStr="Neumann et al., 1997" startWordPosition="3226" endWordPosition="3229">al structure of a sentence has been identified, each substring is passed to the FRAGMENT RECOGNIZER in order to determine the internal phrasal structure. Note that processing of a substring might still be partial in the sense that no complete structure need be found (e.g., if we cannot combine sequences of phrases to one larger unit). The FRAGMENT RECOGNIZER uses finite state grammars in order to extract nominal and prepositional phrases, where the named entities recognized by the preprocessor are integrated into appropriate places (unplausible phrases are rejected by agreement checking; see (Neumann et al., 1997) for more &apos;Type VG-final Subtype Mod-Pert -Ak Modal-stem konn Stem lob Form nicht gelobt haben kann Neg Agr Verb MF (die Arbeitgeber Forderungen) / -Type Subj Cont Inf-C1 ohne Verb MF (als Gegenleistung neue Stellen) [Type Simple-mi [Type Verb Form zu schaffen 243 details)). The phrasal recognizer currently only considers processing of simple, non-recursive structures (see fig. 3; here, *NP* and *PP* are used for denoting phrasal types). Note that because of the high degree of modularity of our shallow parsing architecture, it is very easy to exchange the currently domain-independent fragment </context>
<context position="24898" citStr="Neumann et al., 1997" startWordPosition="3949" endWordPosition="3952">due to errors caused by previous components. A more detailed analysis showed that the majority of errors were caused by mistakes in the preprocessing phase. For example ten errors were caused by an ambiguity between different verb stems (only the first reading is chosen) and ten errors because of wrong POS-filtering. Seven errors were caused by unknown verb forms, and in eight cases the parser failed because it could not properly handle the ambiguities of some word forms being either a separated verb prefix or adverb. The evaluation has been performed with the Lisp-based version of SMES (cf. (Neumann et al., 1997)) by replacing the original bidirectional shallow buttom-up parsing module with the DC-PARSER. The average run-time per sentence (average length 26 words) is 0.57 sec. A C++-version is nearly finished missing only the re-implementation of the base and main clause recognition phases, cf. (Piskorski and Neumann, 2000). The run-time behavior is already encouraging: processing of a German text document (a collection of business news articles from the &amp;quot;Wirtschaftswoche&amp;quot; ) of 197118 tokens (1.26 MB) needs 45 seconds on a PentiumIL 266 MHz, 128 RAM, which corresponds to 4380 tokens per second. Since </context>
</contexts>
<marker>Neumann, Backofen, Baur, Becker, Braun, 1997</marker>
<rawString>G. Neumann, R. Backofen, J. Baur, M. Becker, and C. Braun. 1997. An information extraction core system for real world german text processing. In 5th International Conference of Applied Natural Language, pages 208-215, Washington, USA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Peh</author>
<author>Christopher H Ting</author>
</authors>
<title>A divideand-conquer strategy for parsing.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL/SIGPARSE 5th International Workshop on Parsing Technologies,</booktitle>
<pages>57--66</pages>
<contexts>
<context position="28485" citStr="Peh and Ting, 1996" startWordPosition="4529" endWordPosition="4532">is of German text documents was presented by (Wauschkuhn, 1996). Nevertheless, comparing its performance with our approach seems to be rather difficult since he only measures for an unannotated test corpus how often his parser finds at least one result (where he reports 85.7% &amp;quot;coverage&amp;quot; of a test corpus of 72.000 sentences) disregarding to measure the accuracy of the parser. In this sense, our parser achieved a &amp;quot;coverage&amp;quot; of 94.25% (computing found/total), almost certainly because we use more advanced lexical and phrasal components, 245 e.g., pos-filter, compound and named entity processing. (Peh and Ting, 1996) also describe a divideand-conquer approach based on statistical methods, where the segmentation of the sentence is done by identifying so called link words (solely punctuations, conjunctions and prepositions) and disambiguating their specific role in the sentence. On an annotated test corpus of 600 English sentences they report an accuracy of 85.1% based on the correct recognition of part-of-speech, comma and conjunction disambiguation, and exact noun phrase recognition. 6 Conclusion and future work We have presented a divide-and-conquer strategy for shallow analysis of German texts which is </context>
</contexts>
<marker>Peh, Ting, 1996</marker>
<rawString>L. Peh and Christopher H. Ting. 1996. A divideand-conquer strategy for parsing. In Proceedings of the ACL/SIGPARSE 5th International Workshop on Parsing Technologies, pages 57-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Piskorski</author>
<author>G Neumann</author>
</authors>
<title>An intelligent text extraction and navigation system.</title>
<date>2000</date>
<booktitle>In 6th International Conference on Computer-Assisted Information Retrieval (RIAO-2000).</booktitle>
<volume>18</volume>
<pages>pages.</pages>
<location>Paris,</location>
<contexts>
<context position="25215" citStr="Piskorski and Neumann, 2000" startWordPosition="3996" endWordPosition="4000">ltering. Seven errors were caused by unknown verb forms, and in eight cases the parser failed because it could not properly handle the ambiguities of some word forms being either a separated verb prefix or adverb. The evaluation has been performed with the Lisp-based version of SMES (cf. (Neumann et al., 1997)) by replacing the original bidirectional shallow buttom-up parsing module with the DC-PARSER. The average run-time per sentence (average length 26 words) is 0.57 sec. A C++-version is nearly finished missing only the re-implementation of the base and main clause recognition phases, cf. (Piskorski and Neumann, 2000). The run-time behavior is already encouraging: processing of a German text document (a collection of business news articles from the &amp;quot;Wirtschaftswoche&amp;quot; ) of 197118 tokens (1.26 MB) needs 45 seconds on a PentiumIL 266 MHz, 128 RAM, which corresponds to 4380 tokens per second. Since this is an increase in speed-up by a factor &gt; 20 compared to the Lisp-version, we expect to be able to process 75-100 sentences per second. 5 Related Work To our knowledge, there are only very few other systems described which process free German texts. The new shallow text processor is a direct successor of the one</context>
</contexts>
<marker>Piskorski, Neumann, 2000</marker>
<rawString>J. Piskorski and G. Neumann. 2000. An intelligent text extraction and navigation system. In 6th International Conference on Computer-Assisted Information Retrieval (RIAO-2000). Paris, April. 18 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Roche</author>
<author>Y Schabes</author>
</authors>
<title>Deterministic partof-speech tagging with finite state transducers.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="9243" citStr="Roche and Schabes, 1995" startWordPosition="1389" endWordPosition="1392"> (famous). Since adjectives and attributively used participles are in most cases part of a nominal phrase a convenient rule would reject the verb reading if the previous word form is a determiner or the next word form is a noun. It is important to notice that such rules are based on some regularities, but they may yield false results, like for instance the rule for filtering out the verb reading of some word forms extremely rarely used as verbs (e.g., &amp;quot;recht&amp;quot; - right, to rake (3rd person,sg)). All rules are compiled into a single finite-state transducer according to the approach described in (Roche and Schabes, 1995).2 Named entity finder Named entities such as organizations, persons, locations and time expressions are identified using finite-state grammars. Since some named entities (e.g. company names) may appear in the text either with or without a designator, we use a dynamic lexicon to store recognized named entities without their designators (e.g., &amp;quot;Braun AG&amp;quot; vs. &amp;quot;Braun&amp;quot;) in order to identify subsequent occurrences correctly. However a named entity, consisting solely of one word, may be also a valid word form (e.g., &amp;quot;Braun&amp;quot; – brown). Hence we classify such words as candidates for named entities sinc</context>
</contexts>
<marker>Roche, Schabes, 1995</marker>
<rawString>E. Roche and Y. Schabes. 1995. Deterministic partof-speech tagging with finite state transducers. Computational Linguistics, 21(2):227-253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>editor SAIC</author>
</authors>
<date>1998</date>
<booktitle>Seventh Message Understanding Conference (MUC-7), http://www.muc.saic.com/. SAIC Information Extraction.</booktitle>
<contexts>
<context position="2432" citStr="SAIC, 1998" startWordPosition="368" endWordPosition="369"> such units is then performed by means of domain-specific templates. Usually, these templates * DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbrticken, Germany, neumann0df ki . de t DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, cbraun@dfki . de DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, pi skorsk@df ki . de are triggered by domain-specific predicates attached only to a relevant subset of verbs which express domain-specific selectional restrictions for possible argument fillers. In most of the well-known shallow text processing systems (cf. (Sundheim, 1995) and (SAIC, 1998)) cascaded chunk parsers are used which perform clause recognition after fragment recognition following a bottom-up style as described in (Abney, 1996). We have also developed a similar bottomup strategy for the processing of German texts, cf. (Neumann et al., 1997). However, the main problem we experienced using the bottom-up strategy was insufficient robustness: because the parser depends on the lower phrasal recognizers, its performance is heavily influenced by their respective performance. As a consequence, the parser frequently wasn&apos;t able to process structurally simple sentences, because</context>
</contexts>
<marker>SAIC, 1998</marker>
<rawString>SAIC, editor. 1998. Seventh Message Understanding Conference (MUC-7), http://www.muc.saic.com/. SAIC Information Extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>C Nobata</author>
</authors>
<title>An information extraction system and a customization tool.</title>
<date>1998</date>
<booktitle>In Proceedings of Hitachi workshop-98,</booktitle>
<pages>http://cs.nyu.edu/cs/projects/proteus/sekine/.</pages>
<contexts>
<context position="1608" citStr="Sekine and Nobata, 1998" startWordPosition="247" endWordPosition="250">ext collections due to the fact that they can provide a partial understanding of specific types of text with a certain degree of partial accuracy using fast and robust language processing strategies (basically finite state technology). They have been &amp;quot;made sensitive&amp;quot; to certain key pieces of information and thereby provide an easy means to skip text without deep analysis. The majority of existing IE systems are applied to English text, but there are now a number of systems which process other languages as well (e.g., German (Neumann et al., 1997), Italian (Ciravegna et al., 1999) or Japanese (Sekine and Nobata, 1998)). The majority of current systems perform a partial parsing approach using only very few general syntactic knowledge for the identification of nominal and prepositional phrases and verb groups. The combination of such units is then performed by means of domain-specific templates. Usually, these templates * DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbrticken, Germany, neumann0df ki . de t DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, cbraun@dfki . de DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany, pi skorsk@df ki . de are triggered by domain-specific predicates at</context>
</contexts>
<marker>Sekine, Nobata, 1998</marker>
<rawString>S. Sekine and C. Nobata. 1998. An information extraction system and a customization tool. In Proceedings of Hitachi workshop-98, http://cs.nyu.edu/cs/projects/proteus/sekine/.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Sixth Message Understanding Conference (MUC-6), Washington. Distributed by</booktitle>
<editor>B. Sundheim, editor.</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Inc.,San Mateo, California.</location>
<marker>1995</marker>
<rawString>B. Sundheim, editor. 1995. Sixth Message Understanding Conference (MUC-6), Washington. Distributed by Morgan Kaufmann Publishers, Inc.,San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wauschkuhn</author>
</authors>
<title>Ein Werkzeug zur partiellen syntaktischen Analyse deutscher Textkorpora.</title>
<date>1996</date>
<booktitle>Natural Language Processing and Speech Technology. Results of the Third KONVENS Conference,</booktitle>
<pages>356--368</pages>
<editor>In Dafydd Gibbon, editor,</editor>
<location>Berlin.</location>
<contexts>
<context position="27929" citStr="Wauschkuhn, 1996" startWordPosition="4442" endWordPosition="4443">0 377 356 89.00 94.42 91.62 Main-Clause-Module (isolated evaluation) correctness Main-Clauses Recall Precision F-measure criterium in % in % in % total found correct Top 400 389 376 94.00 96.65 95.30 Struct 1 400 389 376 94.00 96.65 95.30 Struct2 400 389 372 93.00 95.62 94.29 complete analysis correctness all components Recall Precision F-measure criterium in % in % in % total found correct Struct2 400 377 339 84.75 89.68 87.14 Table 1: Results of the evaluation of the topological structure and-conquer approach using a chart-based parser for analysis of German text documents was presented by (Wauschkuhn, 1996). Nevertheless, comparing its performance with our approach seems to be rather difficult since he only measures for an unannotated test corpus how often his parser finds at least one result (where he reports 85.7% &amp;quot;coverage&amp;quot; of a test corpus of 72.000 sentences) disregarding to measure the accuracy of the parser. In this sense, our parser achieved a &amp;quot;coverage&amp;quot; of 94.25% (computing found/total), almost certainly because we use more advanced lexical and phrasal components, 245 e.g., pos-filter, compound and named entity processing. (Peh and Ting, 1996) also describe a divideand-conquer approach </context>
</contexts>
<marker>Wauschkuhn, 1996</marker>
<rawString>0. Wauschkuhn. 1996. Ein Werkzeug zur partiellen syntaktischen Analyse deutscher Textkorpora. In Dafydd Gibbon, editor, Natural Language Processing and Speech Technology. Results of the Third KONVENS Conference, pages 356-368. Mouton de Gruyter, Berlin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>