<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<note confidence="0.793941428571428">
A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text
Kenneth Ward Church
Bell Laboratories
600 Mountain Ave.
Murray Hill, N.J., USA
201-582-5325
alice!kwc
</note>
<bodyText confidence="0.998540222222222">
It is well-known that part of speech depends on
context. The word &amp;quot;table,&amp;quot; for example, can be
a verb in some contexts (e.g., &amp;quot;He will table the
motion&amp;quot;) and a noun in others (e.g., &amp;quot;The table
is ready&amp;quot;). A program has been written which
tags each word in an input sentence with the
most likely part of speech. The program
produces the following output for the two
&amp;quot;table&amp;quot; sentences just mentioned:
</bodyText>
<listItem confidence="0.997614333333333">
• He/PPS will/MD table/VB the/AT
motion/NN .1.
• The/AT table/NN is/BEZ ready/JJ .1.
</listItem>
<bodyText confidence="0.999906960526316">
(PPS = subject pronoun; MD = modal; VB =
verb (no inflection); AT = article; NN = noun;
BEZ = present 3rd sg form of &amp;quot;to be&amp;quot;; JJ =
adjective; notation is borrowed from [Francis and
Kucera, pp. 6-8])
Part of speech tagging is an important practical
problem with potential applications in many
areas including speech synthesis, speech
recognition, spelling correction, proof-reading,
query answering, machine translation and
searching large text data bases (e.g., patents,
newspapers). The author is particularly
interested in speech synthesis applications, where
it is clear that pronunciation sometimes depends
on part of speech. Consider the following three
examples where pronunciation depends on part
of speech. First, there are words like &amp;quot;wind&amp;quot;
where the noun has a different vowel than the
verb. That is, the noun &amp;quot;wind&amp;quot; has a short
vowel as in &amp;quot;the wind is strong,&amp;quot; whereas the
verb &amp;quot;wind&amp;quot; has a long vowel as in &amp;quot;Don&apos;t
forget to wind your watch.&amp;quot; Secondly, the
pronoun &amp;quot;that&amp;quot; is stressed as in &amp;quot;Did you see
THAT?&amp;quot; unlike the complementizer &amp;quot;that,&amp;quot; as
in &amp;quot;It is a shame that he&apos;s leaving.&amp;quot; Thirdly,
note the difference between &amp;quot;oily FLUID&amp;quot; and
&amp;quot;TRANSMISSION fluid&amp;quot;; as a general rule, an
adjective-noun sequence such as &amp;quot;oily FLUID&amp;quot;
is typically stressed on the right whereas a
noun-noun sequence such as &amp;quot;TRANSMISSION
fluid&amp;quot; is typically stressed on the left. These are
but three of the many constructions which would
sound more natural if the synthesizer had access
to accurate part of speech information.
Perhaps the most important application of
tagging programs is as a tool for future research.
A number of large projects such as [Cobuild]
have recently been collecting large corpora (10-
1000 million words) in order to better describe
how language is actually used in practice:
&amp;quot;For the first time, a dictionary has been
compiled by the thorough examination of
representative group of English texts, spoken
and written, running to many millions of
words. This means that in addition to all the
tools of the conventional dictionary makers...
the dictionary is based on hard, measureable
evidence.&amp;quot; [Cobuild, p. xv]
It is likely that there will be more and more
research projects collecting larger and larger
corpora. A reliable parts program might greatly
enhance the value of these corpora to many of
these researchers.
The program uses a linear time dynamic
programming algorithm to find an assignment of
parts of speech to words that optimizes the
product of (a) lexical probabilities (probability of
observing part of speech i given word j), and (b)
contextual probabilities (probability of observing
part of speech i given k previous parts of
speech). Probability estimates were obtained by
training on the Tagged Brown Corpus [Francis
and Kucera], a corpus of approximately
1,000,000 words with part of speech tags
assigned laboriously by hand over many years.
Program performance is encouraging (95-99%
&amp;quot;correct&amp;quot;, depending on the definition of
&amp;quot;correct&amp;quot;). A small 400 word sample is
presented in the Appendix, and is judged to be
99.5% correct. It is surprising that a local
&amp;quot;bottom-up&amp;quot; approach can perform so well.
Most errors are attributable to defects in the
lexicon; remarkably few errors are related to the
inadequacies of the extremely over-simplified
grammar (a trigram model). Apparently, &amp;quot;long
distance&amp;quot; dependences are not very important, at
</bodyText>
<page confidence="0.998144">
136
</page>
<bodyText confidence="0.9967581875">
least most of the time.
One might have thought that ngram models
weren&apos;t adequate for the task since it is well-
known that they are inadequate for determining
grammaticality:
&amp;quot;We find that no finite-state Markov process
that produces symbols with transition from
state to state can serve as an English
grammar. Furthermore, the particular
subclass of such processes that produce n-
order statistical approximations to English do
not come closer, with increasing n, to
matching the output of an English grammar.&amp;quot;
[Chomsky, p. 113]
Chomslcy&apos;s conclusion was based on the
observation that constructions such as:
</bodyText>
<listItem confidence="0.999788333333333">
• If S I then 52.
• Either S3, or S4.
• The man who said that S5, is arriving today.
</listItem>
<bodyText confidence="0.999849583333333">
have long distance dependencies that span across
any fixed length window n. Thus, ngram
models are clearly inadequate for many natural
language applications. However, for the tagging
application, the ngram approximation may be ac-
ceptable since long distance dependencies do not
seem to be very important.
Statistical ngram models were quite popular in
the 1950s, and have been regaining popularity
over the past few years. The IBM speech group
is perhaps the strongest advocate of ngram
methods, especially in other applications such as
speech recognition. Robert Mercer (private
communication, 1982) has experimented with the
tagging application, using a restricted corpus
(laser patents) and small vocabulary (1000
words). Another group of researchers working
in Lancaster around the same time, Leech,
Garside and Atwell, also found ngram models
highly effective; they report 96.7% success in
automatically tagging the LOB Corpus, using a
bigram model modified with heuristics to cope
with more important trigrams. The present work
developed independently from the LOB project.
</bodyText>
<listItem confidence="0.494178">
1. How Hard is Lexical Ambiguity?
</listItem>
<bodyText confidence="0.995265">
Many people who have not worked in
computational linguistics have a strong intuition
that lexical ambiguity is usually not much of a
problem. It is commonly believed that most
words have just one part of speech, and that the
few exceptions such as &amp;quot;table&amp;quot; are easily
disambiguated by context in most cases. In
contrast, most experts in computational linguists
have found lexical ambiguity to be a major
issue; it is said that practically any content word
can be used as a noun, verb or adjective,I and
that local context is not always adequate to
disambiguate. Introductory texts are full of
ambiguous sentences such as
</bodyText>
<listItem confidence="0.99792">
• Time flies like an arrow.
• Flying planes can be dangerous.
</listItem>
<bodyText confidence="0.981782085714286">
where no amount of syntactic parsing will help.
These examples are generally taken to indicate
that the parser must allow for multiple
possibilities and that grammar formalisms such
as LR(k) are inadequate for natural language
since these formalisms cannot cope with
ambiguity. This argument was behind a large set
of objections to Marcus&apos; &amp;quot;LR(k)-like&amp;quot; Deter-
ministic Parser.
Although it is clear that an expert in compu-
tational linguistics can dream up arbitrarily hard
sentences, it may be, as Marcus suggested, that
most texts are not very hard in practice. Recall
that Marcus hypothesized most decisions can be
resolved by the parser within a small window
(i.e., three buffer cells), and there are only a few
problematic cases where the parser becomes
confused. He called these confusing cases
&amp;quot;garden paths,&amp;quot; by analogy with the famous
example:
• The horse raced past the barn fell.
With just a few exceptions such as these
&amp;quot;garden paths,&amp;quot; Marcus assumes, there is almost
always a unique &amp;quot;best&amp;quot; interpretation which Can
be found with very limited resources. The
proposed stochastic approach is largely
compatible with this; the proposed approach
1. From an information theory point of view, one can
quantity ambiguity in bits. In the case of the Brown
Tagged Corpus, the lexical entropy, the conditional
entropy of the part of speech given the word is about 0.25
bits per part of speech. This is considerably smaller than
the contextual entropy, the conditional entropy of the part
of speech given the next two parts of speech. This
entropy is estimated to be about 2 bits per part of speech.
</bodyText>
<page confidence="0.989196">
137
</page>
<bodyText confidence="0.999944545454546">
assumes that it is almost always sufficient to
assign each word a unique &amp;quot;best&amp;quot; part of speech
(and this can be accomplished with a very
efficient linear time dynamic programming
algorithm). After reading introductory
discussions of &amp;quot;Flying planes can be
dangerous,&amp;quot; one might have expected that
lexical ambiguity was so pervasive that it would
be hopeless to try to assign just one part of
speech to each word and in just one linear time
pass over the input words.
</bodyText>
<sectionHeader confidence="0.651255" genericHeader="method">
2. Lexical Disambiguation Rules
</sectionHeader>
<bodyText confidence="0.999442333333333">
However, the proposed stochastic method is
considerably simpler than what Marcus had in
mind. His thesis parser used considerably more
syntax than the proposed stochastic method.
Consider the following pair described in
[Marcus]:
</bodyText>
<listItem confidence="0.986705">
• Have/VB [the students who missed the exam]
TAKE the exam today. (imperative)
• Have/AUX [the students who missed the
exam] TAKEN the exam today? (question)
</listItem>
<bodyText confidence="0.999419333333333">
where it appears that the parser needs to look
past an arbitrarily long noun phrase in order to
correctly analyze &amp;quot;have,&amp;quot; which could be either
a tenseless main verb (imperative) or a tensed
auxiliary verb (question). Marcus&apos; rather
unusual example can no longer be handled by
Fidditch, a more recent Marcus-style parser with
very large coverage. In order to obtain such
large coverage, Fidditch has had to take a more
robust/modest view of lexical disambiguation.
Whereas Marcus&apos; Parsifal program distinguished
patterns such as &amp;quot;have NP tenseless&amp;quot; and &amp;quot;have
NP past-participle,&amp;quot; most of Fidditch&apos;s
diagnostic rules are less ambitious and look only
for the start of a noun phrase and do not attempt
to look past an arbitrarily long noun phrase. For
example, Fidditch has the following lexical
disambiguation rule:
</bodyText>
<listItem confidence="0.827075">
• (defrule n+prep!
&amp;quot; &gt; [**n+prep] != n [npstarters]&amp;quot;)
</listItem>
<bodyText confidence="0.999510745454546">
which says that a preposition is more likely than
a noun before a noun phrase. More precisely,
the rule says that if a noun/preposition
ambiguous word (e.g., &amp;quot;out&amp;quot;) is followed by
something that starts a noun phrase (e.g., a
determiner), then rule out the noun possibility.
This type of lexical diagnostic rule can be
captured with bigram and trigram statistics; it
turns out that the sequence ...preposition
determiner.., is much more common in the
Brown Corpus (43924 observations) than the
sequence ...noun determiner... (1135
observations). Most lexical disambiguation rules
in Fidditch can be reformulated in terms of
bigram and trigram statistics in this way.
Moreover, it is worth doing so, because bigram
and trigram statistics are much easier to obtain
than Fidditch-type disambiguation rules, which
are extremely tedious to program, test and
debug.
In addition, the proposed stochastic approach can
naturally take advantage of lexical probabilities
in a way that is not easy to capture with parsers
that do not make use of frequency information.
Consider, for example, the word &amp;quot;see,&amp;quot; which is
almost always a verb, but does have an archaic
nominal usage as in &amp;quot;the Holy See.&amp;quot; For
practical purposes, &amp;quot;see&amp;quot; should not be
considered noun/verb ambiguous in the same
sense as truly ambiguous words like &amp;quot;program,&amp;quot;
&amp;quot;house&amp;quot; and &amp;quot;wind&amp;quot;; the nominal usage of
&amp;quot;see&amp;quot; is possible, but not likely.
If every possibility in the dictionary must be
given equal weight, parsing is very difficult.
Dictionaries tend to focus on what is possible,
not on what is likely. Consider the trivial
sentence, &amp;quot;I see a bird.&amp;quot; For all practical
purposes, every word in the sentence is
unambiguous. According to [Francis and
Kucera], the word &amp;quot;I&amp;quot; appears as a pronoun
(PPLS) in 5837 out of 5838 observations
(-100%), &amp;quot;see&amp;quot; appears as a verb in 771 out of
772 observations (-100%), &amp;quot;a&amp;quot; appears as an
article in 23013 out of 23019 observations
C100%) and &amp;quot;bird&amp;quot; appears as a noun in 26 out
of 26 observations (-100%). However, according
to Webster&apos;s Seventh New Collegiate Dictionary,
every word is ambiguous. In addition to the
desired assignments of tags, the first three words
are listed as nouns and the last as an intransitive
verb. One might hope that these spurious
assignments could be ruled out by the parser as
syntactically ill-formed. Unfortunately, this is
unlikely to work. If the parser is going to accept
noun phrases of the form:
</bodyText>
<listItem confidence="0.919316333333333">
• [NP [N city] [N school] [N committee] [N
meeting]]
then it can&apos;t rule out
</listItem>
<page confidence="0.883194">
138
</page>
<listItem confidence="0.976551">
• [NP [N I] [N see] [N a] [N bird]]
</listItem>
<bodyText confidence="0.993302666666667">
Similarly, the parser probably also has to accept
&amp;quot;bird&amp;quot; as an intransitive verb, since there is
nothing syntactically wrong with:
</bodyText>
<listItem confidence="0.983409">
• [S [NP [N I] [N see] [N a]] [VP [V bird]]]
</listItem>
<bodyText confidence="0.880799">
These part of speech assignments aren&apos;t wrong;
they are just extremely improbable.
</bodyText>
<sectionHeader confidence="0.795152" genericHeader="method">
3. The Proposed Method
</sectionHeader>
<bodyText confidence="0.999749571428572">
Consider once again the sentence, &amp;quot;I see a
bird.&amp;quot; The problem is to find an assignment of
parts of speech to words that optimizes both
lexical and contextual probabilities, both of
which are estimated from the Tagged Brown
Corpus. The lexical probabilities are estimated
from the following frequencies:
</bodyText>
<table confidence="0.9660382">
Word Parts of Speech
PPSS 5837 NP 1
see VB 771 UH 1
a AT 23013 IN (French) 6
bird NN 26
</table>
<bodyText confidence="0.968215787878788">
(PPSS = pronoun; NP = proper noun; VB =
verb; U11 = intellection; IN = preposition; AT =
article; NN = noun)
The lexical probabilities are estimated in the
obvious way. For example, the probability that
&amp;quot;I&amp;quot; is a pronoun, Prob(PPSS I &amp;quot;I&amp;quot;), is estimated
as the freq(PPSS I &amp;quot;I&amp;quot;)/freq(&amp;quot;I&amp;quot;) or 5837/5838.
The probability that &amp;quot;see&amp;quot; is a verb is estimated
to be 771/772. The other lexical probability
estimates follow the same pattern.
The contextual probability, the probability of
observing part of speech X given the following
two parts of speech Y and Z, is estimated by
dividing the trigram frequency XYZ by the
bigram frequency YZ. Thus, for example, the
probability of observing a verb before an article
and a noun is estimated to be the ratio of the
freq(VB, AT, NN) over the freq(AT, NN) or
3412/53091 = 0.064. The probability of
observing a noun in the same context is
estimated as the ratio of freq(NN, AT, NN) over
53091 or 629/53091 = 0.01. The other
contextual probability estimates follow the same
pattern.
A search is performed in order to find the
assignment of part of speech tags to words that
optimizes the product of the lexical and
contextual probabilities. Conceptually, the
search enumerates all possible assignments of
parts of speech to input words. In this case,
there are four input words, three of which are
two ways ambiguous, producing a set of
2*2*2*1=8 possible assignments of parts of
</bodyText>
<table confidence="0.9984932">
speech to input words: a bird
see
PPSS VB AT NN
PPSS VB IN NN
PPSS UH AT NN
PPSS UH IN NN
NP VB AT NN
NP VB IN NN
NP UH AT NN
NP UH IN NN
</table>
<bodyText confidence="0.998732894736842">
Each of the eight sequences are then scored by
the product of the lexical probabilities and the
contextual probabilities, and the best sequence is
selected. In this case, the first sequence is by far
the best.
In fact, it is not necessary to enumerate all
possible assignments because the scoring
function cannot see more than two words away.
In other words, in the process of enumerating
part of speech sequences, it is possible in some
cases to know that some sequence cannot
possibly compete with another and can therefore
be abandoned. Because of this fact, only 0(n)
paths will be enumerated. Let us illustrate this
optimization with an example:
Find all assignments of parts of speech to
&amp;quot;bird&amp;quot; and score the partial sequence.
Henceforth, all scores are to be interpreted as log
probabilities.
</bodyText>
<sectionHeader confidence="0.364906" genericHeader="method">
(-4.848072 &amp;quot;NN&amp;quot;)
</sectionHeader>
<bodyText confidence="0.792711857142857">
Find all assignments of parts of speech to &amp;quot;a&amp;quot;
and score. At this point, there are two paths:
(-7.4453945 &amp;quot;AT&amp;quot; &amp;quot;NN&amp;quot;)
(-15.01957 &amp;quot;IN&amp;quot; &amp;quot;NN&amp;quot;)
Now, find assignments of &amp;quot;see&amp;quot; and score. At
this point, the number of paths seem to be
growing exponentially.
</bodyText>
<page confidence="0.997887">
139
</page>
<table confidence="0.8916015">
(-10.1914 &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NW)
(-18.54318 &amp;quot;VB&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;NN&amp;quot;)
(-29.974142 &amp;quot;UH&amp;quot; &amp;quot;AT&amp;quot; &amp;quot;NW)
(-36.53299 &amp;quot;UH&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;NN&amp;quot;)
</table>
<bodyText confidence="0.931291571428571">
Now, find assignments of &amp;quot;I&amp;quot; and score. Note,
however, that it is no longer necessary to
hypothesize that &amp;quot;a&amp;quot; might be a French
preposition IN because all four paths, PPSS VB
IN NN, NN VB IN NN, PPSS UH IN NN and
NP UH AT NN score less well than some other
path and there is no way that any additional
input could make any difference. In particular,
the path, PPSS VB IN NN scores less well than
the path PPSS VB AT NN, and additional input
will not help PPSS VB IN NN because the
contextual scoring function has a limited window
of three parts of speech, and that is not enough
to see past the existing PPSS and VB.
</bodyText>
<table confidence="0.961401085714286">
(-12.927581 &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;)
(-24.177242 &amp;quot;NP&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;)
(-35.667458 &amp;quot;PPSS&amp;quot; &amp;quot;UH&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;)
(-44.33943 &amp;quot;NP&amp;quot; &amp;quot;UH&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;)
The search continues two more iterations,
assuming blank parts of speech for words out of
range.
(-13.262333 &amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;)
(-26.5196 &amp;quot;&amp;quot; &amp;quot;NP&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;)
Fmally, the result is: PPSS VB AT NN.
(-12.262333 &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&amp;quot;
The final result is: I/PPSS see/VB a/AT bird/NN.
A slightly more interesting example is: &amp;quot;Can
they can cans.&amp;quot;
cans
(-5.456845 &amp;quot;NNS&amp;quot;)
can
(-12.603266 &amp;quot;NN&amp;quot; &amp;quot;NNS&amp;quot;)
(-15.935471 &amp;quot;VB&amp;quot; &amp;quot;NNS&amp;quot;)
(-15.946739 &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;)
they
(-18.02618 &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;)
(-18.779934 &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;NNS&amp;quot;)
(-21.411636 &amp;quot;PPSS&amp;quot; &amp;quot;NN&amp;quot; &amp;quot;NNS&amp;quot;)
can
(-21.766554 &amp;quot;MD&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;&apos;VB&amp;quot; &amp;quot;NNS&amp;quot;)
(-26.45485 &amp;quot;NN&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;)
(-28.306572 &amp;quot;VB&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;)
(-21.932137 &amp;quot; &amp;quot;MD&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;NNS&amp;quot;)
(-30.170452 &amp;quot;&amp;quot;VB&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;)
(-31.453785 &amp;quot; &amp;quot;NN&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;)
And the result is: Can/MD they/PPSS can/VB
cans/NNS
(-20.932137 &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;PPSS&amp;quot;
4. Parsing Simple Non-Recursive Noun Phrases
</table>
<subsubsectionHeader confidence="0.390777">
Stochastically
</subsubsectionHeader>
<bodyText confidence="0.8032722">
Similar stochastic methods have been applied to
locate simple noun phrases with very high
accuracy. The program inserts brackets into a
sequence of parts of speech, producing output
such as:
</bodyText>
<construct confidence="0.838529125">
[A/AT former/AP top/NN aide/NN] to/IN
[Attorney/NP General/NP Edwin/NP Meese/NP]
interceded/VBD to/TO extend/VB [an/AT
aircraft/NN company/NN [business/NM with/IN
[a,/AT lobbyist/NN] [who/WPS] worked/VBD
for/IN [the/AT defense/NN contractor/NN] ,/,
according/1N to/IN [a/AT published/VBN re-
port/NN] .1.
</construct>
<bodyText confidence="0.994834125">
The proposed method is a stochastic analog of
precedence parsing. Recall that precedence
parsing makes use of a table that says whether to
insert an open or close bracket between any two
categories (terminal or nonterminal). The
proposed method makes use of a table that givvs
the probabilities of an open and close bracket
between all pairs of parts of speech. A sample
is shown below for the five parts of speech: AT
(article), NN (singular noun), NNS (non-singular
noun), VB (uninflected verb), IN (preposition).
The table says, for example, that there is no
chance of starting a noun phrases after an article
(all five entries on the AT row are 0) and that
there is a large probability of starting a noun
phrase between a verb and an noun (the entry in
</bodyText>
<page confidence="0.992949">
140
</page>
<table confidence="0.984034333333333">
(VB, AT) is 1.0).
Probability of Starting a Noun Phrase
AT NN NNS VB IN
AT 0 0 0
NN .99 .01 0
NNS 1.0 .02 .11
VB 1.0 1.0 1.0
IN 1.0 1.0 1.0
Probability of Ending a Noun Phrase
AT NN NNS VB IN
AT 0 0 0 0 0
NN 1.0 .01 0 0 1.0
NNS 1.0 .02 .11 1.0 1.0
VB 0 0 0 0 0
IN 0 0 0 0 .02
</table>
<bodyText confidence="0.999856117647059">
These probabilities were estimated from about
40,000 words (11,000 noun phrases) of training
material selected from the Brown Corpus. The
training material was parsed into noun phrases
by laborious semi-automatic means (with
considerable help from Eva Ejerhed). It took
about a man-week to prepare the training
material.
The stochastic parser is given a sequence of parts
of speech as input and is asked to insert brackets
corresponding to the beginning and end of noun
phrases. Conceptually, the parser enumerates all
possible parsings of the input and scores each of
them by the precedence probabilities. Consider,
for example, the input sequence: NN VB. There
are 5 possible ways to bracket this sequence
(assuming no recursion):
</bodyText>
<listItem confidence="0.9998394">
• NN VB
• [NN] VB
• NN VB]
• [NN] [NM]
• NN [V13]
</listItem>
<bodyText confidence="0.999764071428571">
Each of these parsings is scored by multiplying 6
precedence probabilities, the probability of an
open/close bracket appearing (or not appearing)
in any one of the three positions (before the NN,
after the NN or after the VB). The parsing with
the highest score is returned as output.
A small sample of the output is given in the
appendix. The method works remarkably well
considering how simple it is. There is some
tendency to underestimate the number of
brackets and run two noun phrases together as in
[NP the time Fairchild]. The proposed method
omitted only 5 of 243 noun phrase brackets in
the appendix.
</bodyText>
<sectionHeader confidence="0.587914" genericHeader="method">
5. Smoothing Issues
</sectionHeader>
<bodyText confidence="0.999978048780488">
Some of the probabilities are very hard to
estimate by direct counting because of Zipf s
Law (frequency is roughly proportional to
inverse rank). Consider, for example, the lexical
probabilities. We need to estimate how often
each word appears with each part of speech.
Unfortunately, because of Zipfs Law, no matter
how much text we look at, there will always be
a large tail of words that appear only a few
times. In the Brown Corpus, for example,
40,000 words appear five times or less. If a
word such as &amp;quot;yawn&amp;quot; appears once as a noun
and once as a verb, what is the probability that it
can be an adjective? It is impossible to say
without more information. Fortunately,
conventional dictionaries can help alleviate this
problem to some extent. We add one to the
frequency count of possibilities in the dictionary.
For example, &amp;quot;yawn&amp;quot; happens to be listed in
our dictionary as noun/verb ambiguous. Thus,
we smooth the frequency counts obtained from
the Brown Corpus by adding one to both
possibilities. In this case, the probabilities
remain unchanged. Both before and after
smoothing, we estimate &amp;quot;yawn&amp;quot; to be a noun
50% of the time, and a verb the rest. There is
no chance that &amp;quot;yawn&amp;quot; is an adjective.
In some other cases, smoothing makes a big
difference. Consider the word &amp;quot;cans.&amp;quot; This
word appears 5 times as a plural noun and never
as a verb in the Brown Corpus. The lexicon
(and its morphological routines), fortunately,
give both possibilities. Thus, the revised
estimate is that &amp;quot;cans&amp;quot; appears 6/7 times as a
plural noun and 1/7 times as a verb.
Proper nouns and capitalized words are
particularly problematic; some capitalized words
are proper nouns and some are not. Estimates
from the Brown Corpus can be misleading. For
example, the capitalized word &amp;quot;Acts&amp;quot; is found
twice in the Brown Corpus, both times as a
</bodyText>
<page confidence="0.957384">
141
</page>
<bodyText confidence="0.997128314285714">
proper noun (in a title). It would be a mistake to Ejerhed, E., &amp;quot;Finding Clauses in Unrestricted
infer from this evidence that the word &amp;quot;Acts&amp;quot; is Text by Stochastic and Funtary Methods,&amp;quot;
always a proper noun. For this reason, abstracted submitted to this conference.
capitalized words with small frequency counts (&lt; Francis, W., and Kucera, H., &amp;quot;Frequency
20) were thrown out of the lexicon. Analysis of English Usage,&amp;quot; Houghton Mifflin
There are other problems with capitalized words. Company, Boston, 1982.
Consider, for example, a sentence beginning with Leech, G., Garside, R., Atwell, E., &amp;quot;The
the capitalized word &amp;quot;Fall&amp;quot;; what is the Automatic Grammatical Tagging of the LOB
probability that it is a proper noun (i.e., a Corpus,&amp;quot; ICAME News 7, 13-33, 1983.
surname)? Estimates from the Brown Corpus Marcus, M., &amp;quot;A Theory of Syntactic Recognition
are of little help here since &amp;quot;Fall&amp;quot; never appears for Natural Language,&amp;quot; MIT Press, Cambridge,
as a capitalized word and it never appears as a Massachusetts, 1980.
proper noun. Two steps were taken to alleviate &amp;quot;Webster&apos;s Seventh New Collegiate
this problem. First, the frequency estimates for Dictionary,&amp;quot; Merriam Company, Springfield,
&amp;quot;Fall&amp;quot; are computed from the estimates for Massachusetts, 1972.
&amp;quot;fall&amp;quot; plus 1 for the proper noun possibility. Appendix: Sample Results
Thus, &amp;quot;Fall&amp;quot; has frequency estimates of: ((I . The following story was distributed over the AP
&amp;quot;NP&amp;quot;) (1 . &amp;quot;JJ&amp;quot;) (65 . &amp;quot;VB&amp;quot;) (72 . &amp;quot;NN&amp;quot;)) during the week of May 26, 1987. There are
because &amp;quot;fall&amp;quot; has the estimates of: ((1 . &amp;quot;JJ&amp;quot;) just two tagging errors which are indicated with
(65 . &amp;quot;VB&amp;quot;) (72 . &amp;quot;NN&amp;quot;)). Secondly, a prepass There are five missing brackets which
was introduced which labels words as proper are indicated as &amp;quot;*[&amp;quot; or &amp;quot;1&amp;quot;. Words with a
nouns if they are &amp;quot;adjacent to&amp;quot; other capitalized second NP tag were identified as proper nouns in
words (e.g., &amp;quot;White House,&amp;quot; &amp;quot;State of the a prepass.
Union&amp;quot;) or if they appear several times in a [A/AT former/AP top/NN aide/NN] to/IN [At-
discourse and are always capitalized. torney/NP/NP General/NP/NP Edwin/NP/NP
The lexical probabilities are not the only Meese/NP/NP] interceded/VBD to/TO extend/VB
probabilities that require smoothing. Contextual [an/AT aircraftNN company/NN &apos;s/$ govern-
frequencies also seem to follow Zipf s Law. ment/NN contract/NN] ,/, then/RB went/VBD
That is, for the set of all sequences of three parts into/IN [business/NM] with/IN [a/AT lobby-
of speech, we have plotted the frequency of the ist/NN1 [who/WPS] worked/VBD for/IN [the/AT
sequence against its rank on log log paper and defense/NN contractor/NN] ,/, according/IN to/IN
observed the classic (approximately) linear [a/AT published/VBN report/NN] ./.
relationship and slope of (almost) —1. It is clear [James/NP/NP E/NP./NP Jenkins/NP/NP] ,/,
that the contextual frequencies require [a/AT one-time/JJ senior/JJ deputy/NN] to/IN
smoothing. Zeros should be avoided. [Meese/NP/NP] joined/VBD [the/AT
</bodyText>
<table confidence="0.84226455">
6. Conclusion board/NN] of/IN [directors/NNS] of/IN [Trans-
A stochastic part of speech program and noun world/NP/NP Group/NP/NP Ltd/NP./NP] on/IN
phrase parser has been presented. Performance [April/NP/NP 28/CD] ,/, [1984/CD] ,/, [the/AT
is very encouraging as can be seen from the Chicago/NP/NP Tribune/NP/NP] reporteci/VBD
Appendix. in/IN [its/PP$ Tuesday/NR editions/NNS] ./.
References [The/AT principal/JJ figure/NN] in/IN [Trans-
Chomsky, N., &amp;quot;Three Models for the Description world/NP/NP] was/BEDZ [Richard/NP/NP Mill-
of Language,&amp;quot; IRE Transactions on Information man/NP/NP] ,/, [a/AT lobbyist/NN] for/IN [Fair-
Theory, vol. IT-2, Proceedings of the child/NP/NP Industries/NP/NP Inc/NP./NP] ,/,
Symposium on Information Theory, 1956. [a/AT Virginia/NP/NP de fense/NN con-
Cobuild English Language Dictionary,&amp;quot; 142
William Collins Sons &amp; Co Ltd, 1987.
tractor/NN] I, [the/AT Tribune/NP/NP]
said/VBD .1.
[A/AT federal/JJ grand/JJ jury/NN] is/BEZ in-
vestigating/VBG [the/AT Fairchild/NP/NP trans-
action/NN] and/CC [other/AP actions/NNS]
of/IN [Meese/NP/NP] and/CC [former/AP
White/NP/NP House/NP/NP aide/NN
Lyn/NP/NP Nofziger/NP/NP] in/IN [connec-
</table>
<reference confidence="0.999763552941177">
tion/NN] with/IN [Wedtech/NP/NP
Corp/NP./NP] j, [a/AT New/NP/NP
York/NP/NP defense/NN company/NN]
[that/WPS] received/VBD [$250/CD million/CD]
in/IN [govemment/NN contracts/NNS] is-
sued/VBN without/EN [competitive/JJ bid-
ding/NN] during/IN [the/AT Reagan/NP/NP ad-
ministration/NN] ./.
[Jenkins/NP/NP] left/VBD [the/AT White/NP/NP
House/NP/NP] in/IN [1984/CD] 1, and/CC
joined/VBD [Wedtech/NP/NP] as/CS [its/PP$
director/NN] of/IN [marketing/NN *]*[ two/CD
years/NNS] later/RBR .1.
[Deborah/NP/NP Tucker/NP/NP] ,/, [a/AT
spokeswoman/NN] for/IN [Fairchild/NP/NP] ,/,
said/VBD [Friday/NR] that/CS [the/AT corn-
pany/NN] had/HVD been/BEN contacted/VBN
by/IN [the/AT office/NN] of/IN [independent/JJ
counsel/NN James/NP/NP McKay/NP/NP]
and/CC [subpoenas/NNS] had/HVD been/BEN
served/VBN on/IN [Fairchild/NP/NP] ./.
[Tucker/NP/NP] said/VBD [the/AT in-
vestigation/NN] involving/IN [Fairchild/NP/NP]
had/HVD been/BEN going/VBG on/IN [a/AT
number/NN] of/IN [weeks/NNS] and/CC
predates/VBZ [last/AP week/NN &apos;s/$ ex-
pansion/NN] of/1N [McKay/NP/NP &apos;s/$ in-
vestigation/NN] to/TO include/VB
[Meese/NP/NP] ./.
[The/AT company/NN] is/BEZ cooperating/VBG
in/1N [the/AT investigation/NN] ,/,
[Tucker/NP/NP] said/VBD ./.
[A/AT source/NN *] close/NN***] to/IN
[McKay/NP/NP] said/VBD [last/AP week/NN&amp;quot;
that/CS [Meese/NP/NP] isn&apos;t/BEZ* under/IN
[cruninalaJ investigation/NN] in/IN [the/AT
Fairchild/NP/NP matter/NN] ,/, but/RB is/BEZ
[a/AT witness/NN] ./.
[The/NP Tribune/NP/NP] said/VBD [Mill-
man/NP/NP] ,/, acting/VBG as/CS [a/AT lobby-
ist/NN] for/1N [the/AT Chantilly/NP/NP] ,/,
[Va/NP.-based/NP company/NN] ,/, went/VBD
to/TO see/VB [Jenkins/NP/NP] in/IN [1982/CD]
and/CC urged/VBD [him/PPO] and/CC
[Meese/NP/NPI to/TO encourage/VB [the/AT
Air/NP/NP Force/NP/NP] to/E0 extend/VB
[the/AT production/NN] of/1/•1 [Fairchild/NP/NP
&apos;s/$ A-10/NP bomber/NN] for/IN [a/AT
year/NN] ./.
[Millman/NP/NP] said/VBD there/RB was/BEDZ
[a/AT lucrative/JJ market/NN] in/IN
[Third/NP/NP World/NP/NP countries/NNS] ,/,
but/CC that/CS [Fairchild/NP/NP &apos;s/$
chances/NNS] would/MD be/BE limited/VBN
if/CS [the/AT Air/NP/NP Force/NP/NP]
was/BEDZ not/* producing/VBG [the/AT
plane/NN] ./.
[The/AT Air/NP/NP Force/NP/NP] had/HVD de-
cided/VBN to/TO discontinue/VB [pro-
duction/NN] of/IN [the/AT A-10/NP] ,/, [a/AT
1960s-era/CD ground-support/NN attack/NN
bomber/NN] at/IN [the/AT time/NN *]*[ Fair-
child/NP/NP] was/BEDZ hoping/VBG to/TO
sell/VB [A-10s/NP] abroad/RB j, [the/AT
Tribune/NP/NP] said/VBD ./.
[The/AT newspaper/NN] said/VBD [one/CD
source/NN] reported/VBD that/CS after/CS
[Millman/NP/NP] made/VBD [his/PPS pitch/NN]
J, [Meese/NP/NP] ordered/VBD [Jen-
lcins/NP/NP] to/TO prepare/VB [a/AT
memo/NN] on/IN [behalf/NN] of/IN [Fair-
child/NP/NP] ./.
[Memos/NP***] signed/VBD by/IN
[Meese/NP/NP] ,/, stressing/VBG [the/AT impor-
tance/NN] of/IN [Fairchild/NP/NP &apos;s/$ ar-
ranging/VBG sales/NNS] in/IN [Third/NP/NP
World/NP/NP countries/NNS] j, were/BED
sent/VBN to/IN [the/AT State/NP/NP Depart-
ment/NP/NP] and/CC [the/AT Air/NP/NP
Force/NP/NP] ./.
[Millman/NP/NP] did/DOD not/* return/VB
[telephone/NN calls/NNS] to/EN [his/PP$ of-
fice/NN1 and/CC [referral/NN numbers/NNS]
[Monday/NR] ,I, [the/AT Tribune/NP/NP]
said/VBD ./.
</reference>
<page confidence="0.999132">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002187">
<title confidence="0.999925">A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text</title>
<author confidence="0.999545">Kenneth Ward Church</author>
<affiliation confidence="0.999476">Bell Laboratories</affiliation>
<address confidence="0.9997915">600 Mountain Ave. Murray Hill, N.J., USA</address>
<phone confidence="0.968754">201-582-5325</phone>
<email confidence="0.779975">alice!kwc</email>
<abstract confidence="0.998588032258065">It is well-known that part of speech depends on context. The word &amp;quot;table,&amp;quot; for example, can be a verb in some contexts (e.g., &amp;quot;He will table the motion&amp;quot;) and a noun in others (e.g., &amp;quot;The table is ready&amp;quot;). A program has been written which tags each word in an input sentence with the most likely part of speech. The program produces the following output for the two &amp;quot;table&amp;quot; sentences just mentioned: • He/PPS will/MD table/VB the/AT motion/NN .1. • The/AT table/NN is/BEZ ready/JJ .1. (PPS = subject pronoun; MD = modal; VB = verb (no inflection); AT = article; NN = noun; BEZ = present 3rd sg form of &amp;quot;to be&amp;quot;; JJ = adjective; notation is borrowed from [Francis and Kucera, pp. 6-8]) Part of speech tagging is an important practical problem with potential applications in many areas including speech synthesis, speech recognition, spelling correction, proof-reading, query answering, machine translation and searching large text data bases (e.g., patents, newspapers). The author is particularly interested in speech synthesis applications, where it is clear that pronunciation sometimes depends on part of speech. Consider the following three examples where pronunciation depends on part of speech. First, there are words like &amp;quot;wind&amp;quot; where the noun has a different vowel than the verb. That is, the noun &amp;quot;wind&amp;quot; has a short vowel as in &amp;quot;the wind is strong,&amp;quot; whereas the verb &amp;quot;wind&amp;quot; has a long vowel as in &amp;quot;Don&apos;t forget to wind your watch.&amp;quot; Secondly, the &amp;quot;that&amp;quot; as in &amp;quot;Did you see THAT?&amp;quot; unlike the complementizer &amp;quot;that,&amp;quot; as in &amp;quot;It is a shame that he&apos;s leaving.&amp;quot; Thirdly, note the difference between &amp;quot;oily FLUID&amp;quot; and &amp;quot;TRANSMISSION fluid&amp;quot;; as a general rule, an adjective-noun sequence such as &amp;quot;oily FLUID&amp;quot; is typically stressed on the right whereas a noun-noun sequence such as &amp;quot;TRANSMISSION fluid&amp;quot; is typically stressed on the left. These are but three of the many constructions which would sound more natural if the synthesizer had access to accurate part of speech information. Perhaps the most important application of tagging programs is as a tool for future research. A number of large projects such as [Cobuild] have recently been collecting large corpora (10- 1000 million words) in order to better describe how language is actually used in practice: &amp;quot;For the first time, a dictionary has been compiled by the thorough examination of representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers... the dictionary is based on hard, measureable evidence.&amp;quot; [Cobuild, p. xv] It is likely that there will be more and more research projects collecting larger and larger corpora. A reliable parts program might greatly enhance the value of these corpora to many of these researchers. The program uses a linear time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of part of speech i given word (b) contextual probabilities (probability of observing part of speech i given k previous parts of speech). Probability estimates were obtained by training on the Tagged Brown Corpus [Francis and Kucera], a corpus of approximately words part of speech assigned laboriously by hand over many years. Program performance is encouraging (95-99% &amp;quot;correct&amp;quot;, depending on the definition of &amp;quot;correct&amp;quot;). A small 400 word sample is presented in the Appendix, and is judged to be 99.5% correct. It is surprising that a local &amp;quot;bottom-up&amp;quot; approach can perform so well. Most errors are attributable to defects in the lexicon; remarkably few errors are related to the inadequacies of the extremely over-simplified grammar (a trigram model). Apparently, &amp;quot;long dependences are very important, at 136 least most of the time. One might have thought that ngram models weren&apos;t adequate for the task since it is wellknown that they are inadequate for determining grammaticality: &amp;quot;We find that no finite-state Markov process that produces symbols with transition from state to state can serve as an English grammar. Furthermore, the particular subclass of such processes that produce norder statistical approximations to English do not come closer, with increasing n, to matching the output of an English grammar.&amp;quot; [Chomsky, p. 113] Chomslcy&apos;s conclusion was based on the observation that constructions such as: If then Either The man who said that arriving today. have long distance dependencies that span across fixed length window ngram models are clearly inadequate for many natural language applications. However, for the tagging application, the ngram approximation may be acceptable since long distance dependencies do not seem to be very important. Statistical ngram models were quite popular in the 1950s, and have been regaining popularity the years. The IBM speech group is perhaps the strongest advocate of ngram methods, especially in other applications such as speech recognition. Robert Mercer (private communication, 1982) has experimented with the tagging application, using a restricted corpus (laser patents) and small vocabulary (1000 words). Another group of researchers working in Lancaster around the same time, Leech, Garside and Atwell, also found ngram models highly effective; they report 96.7% success in automatically tagging the LOB Corpus, using a bigram model modified with heuristics to cope with more important trigrams. The present work developed independently from the LOB project. 1. How Hard is Lexical Ambiguity? Many people who have not worked in computational linguistics have a strong intuition that lexical ambiguity is usually not much of a problem. It is commonly believed that most words have just one part of speech, and that the few exceptions such as &amp;quot;table&amp;quot; are easily disambiguated by context in most cases. In contrast, most experts in computational linguists have found lexical ambiguity to be a major issue; it is said that practically any content word as a noun, verb or and that local context is not always adequate to disambiguate. Introductory texts are full of ambiguous sentences such as • Time flies like an arrow. • Flying planes can be dangerous. where no amount of syntactic parsing will help. These examples are generally taken to indicate that the parser must allow for multiple possibilities and that grammar formalisms such as LR(k) are inadequate for natural language since these formalisms cannot cope with ambiguity. This argument was behind a large set of objections to Marcus&apos; &amp;quot;LR(k)-like&amp;quot; Deterministic Parser. Although it is clear that an expert in computational linguistics can dream up arbitrarily hard sentences, it may be, as Marcus suggested, that most texts are not very hard in practice. Recall that Marcus hypothesized most decisions can be resolved by the parser within a small window (i.e., three buffer cells), and there are only a few problematic cases where the parser becomes He called these confusing &amp;quot;garden paths,&amp;quot; by analogy with the famous example: • The horse raced past the barn fell. With just a few exceptions such as these &amp;quot;garden paths,&amp;quot; Marcus assumes, there is almost always a unique &amp;quot;best&amp;quot; interpretation which Can be found with very limited resources. The proposed stochastic approach is largely compatible with this; the proposed approach 1. From an information theory point of view, one can quantity ambiguity in bits. In the case of the Brown Tagged Corpus, the lexical entropy, the conditional entropy of the part of speech given the word is about 0.25 bits per part of speech. This is considerably smaller than the contextual entropy, the conditional entropy of the part of speech given the next two parts of speech. This entropy is estimated to be about 2 bits per part of speech. 137 assumes that it is almost always sufficient to assign each word a unique &amp;quot;best&amp;quot; part of speech (and this can be accomplished with a very efficient linear time dynamic programming algorithm). After reading introductory discussions of &amp;quot;Flying planes can be dangerous,&amp;quot; one might have expected that lexical ambiguity was so pervasive that it would be hopeless to try to assign just one part of speech to each word and in just one linear time pass over the input words. 2. Lexical Disambiguation Rules However, the proposed stochastic method is considerably simpler than what Marcus had in mind. His thesis parser used considerably more syntax than the proposed stochastic method. Consider the following pair described in [Marcus]: • Have/VB [the students who missed the exam] TAKE the exam today. (imperative) • Have/AUX [the students who missed the exam] TAKEN the exam today? (question) where it appears that the parser needs to look past an arbitrarily long noun phrase in order to correctly analyze &amp;quot;have,&amp;quot; which could be either a tenseless main verb (imperative) or a tensed auxiliary verb (question). Marcus&apos; rather unusual example can no longer be handled by Fidditch, a more recent Marcus-style parser with very large coverage. In order to obtain such large coverage, Fidditch has had to take a more robust/modest view of lexical disambiguation. Whereas Marcus&apos; Parsifal program distinguished patterns such as &amp;quot;have NP tenseless&amp;quot; and &amp;quot;have NP past-participle,&amp;quot; most of Fidditch&apos;s diagnostic rules are less ambitious and look only for the start of a noun phrase and do not attempt to look past an arbitrarily long noun phrase. For example, Fidditch has the following lexical disambiguation rule: • (defrule n+prep! &amp;quot; &gt; [**n+prep] != n [npstarters]&amp;quot;) which says that a preposition is more likely than a noun before a noun phrase. More precisely, the rule says that if a noun/preposition ambiguous word (e.g., &amp;quot;out&amp;quot;) is followed by something that starts a noun phrase (e.g., a determiner), then rule out the noun possibility. This type of lexical diagnostic rule can be captured with bigram and trigram statistics; it turns out that the sequence ...preposition determiner.., is much more common in the Brown Corpus (43924 observations) than the sequence ...noun determiner... (1135 observations). Most lexical disambiguation rules in Fidditch can be reformulated in terms of bigram and trigram statistics in this way. Moreover, it is worth doing so, because bigram and trigram statistics are much easier to obtain than Fidditch-type disambiguation rules, which are extremely tedious to program, test and debug. In addition, the proposed stochastic approach can naturally take advantage of lexical probabilities in a way that is not easy to capture with parsers that do not make use of frequency information. Consider, for example, the word &amp;quot;see,&amp;quot; which is almost always a verb, but does have an archaic nominal usage as in &amp;quot;the Holy See.&amp;quot; For practical purposes, &amp;quot;see&amp;quot; should not be considered noun/verb ambiguous in the same sense as truly ambiguous words like &amp;quot;program,&amp;quot; &amp;quot;house&amp;quot; and &amp;quot;wind&amp;quot;; the nominal usage of &amp;quot;see&amp;quot; is possible, but not likely. If every possibility in the dictionary must be given equal weight, parsing is very difficult. Dictionaries tend to focus on what is possible, not on what is likely. Consider the trivial sentence, &amp;quot;I see a bird.&amp;quot; For all practical purposes, every word in the sentence is unambiguous. According to [Francis and Kucera], the word &amp;quot;I&amp;quot; appears as a pronoun (PPLS) in 5837 out of 5838 observations &amp;quot;see&amp;quot; appears as a verb in 771 out of observations &amp;quot;a&amp;quot; appears as an article in 23013 out of 23019 observations C100%) and &amp;quot;bird&amp;quot; appears as a noun in 26 out 26 observations However, according to Webster&apos;s Seventh New Collegiate Dictionary, every word is ambiguous. In addition to the desired assignments of tags, the first three words are listed as nouns and the last as an intransitive verb. One might hope that these spurious assignments could be ruled out by the parser as syntactically ill-formed. Unfortunately, this is unlikely to work. If the parser is going to accept noun phrases of the form: • [NP [N city] [N school] [N committee] [N meeting]] then it can&apos;t rule out 138 • [NP [N I] [N see] [N a] [N bird]] Similarly, the parser probably also has to accept &amp;quot;bird&amp;quot; as an intransitive verb, since there is nothing syntactically wrong with: • [S [NP [N I] [N see] [N a]] [VP [V bird]]] These part of speech assignments aren&apos;t wrong; they are just extremely improbable. 3. The Proposed Method Consider once again the sentence, &amp;quot;I see a bird.&amp;quot; The problem is to find an assignment of parts of speech to words that optimizes both lexical and contextual probabilities, both of which are estimated from the Tagged Brown Corpus. The lexical probabilities are estimated from the following frequencies: Word Parts of Speech PPSS 5837 NP 1 see VB 771 UH 1 a AT 23013 IN (French) 6 bird NN 26 (PPSS = pronoun; NP = proper noun; VB = verb; U11 = intellection; IN = preposition; AT = article; NN = noun) The lexical probabilities are estimated in the obvious way. For example, the probability that is a pronoun, Prob(PPSS I&amp;quot;I&amp;quot;), is estimated as the freq(PPSS I &amp;quot;I&amp;quot;)/freq(&amp;quot;I&amp;quot;) or 5837/5838. The probability that &amp;quot;see&amp;quot; is a verb is estimated to be 771/772. The other lexical probability estimates follow the same pattern. The contextual probability, the probability of observing part of speech X given the following two parts of speech Y and Z, is estimated by dividing the trigram frequency XYZ by the bigram frequency YZ. Thus, for example, the probability of observing a verb before an article and a noun is estimated to be the ratio of the freq(VB, AT, NN) over the freq(AT, NN) or 3412/53091 = 0.064. The probability of observing a noun in the same context is estimated as the ratio of freq(NN, AT, NN) over 53091 or 629/53091 = 0.01. The other contextual probability estimates follow the same pattern. A search is performed in order to find the assignment of part of speech tags to words that optimizes the product of the lexical and contextual probabilities. Conceptually, the search enumerates all possible assignments of of speech to input words. In this there are four input words, three of which are two ways ambiguous, producing a set of 2*2*2*1=8 possible assignments of parts of speech to input words: a bird see</abstract>
<title confidence="0.953436125">PPSS VB AT NN PPSS VB IN NN PPSS UH AT NN PPSS UH IN NN NP VB AT NN NP VB IN NN NP UH AT NN NP UH IN NN</title>
<abstract confidence="0.969314348214286">Each of the eight sequences are then scored by the product of the lexical probabilities and the contextual probabilities, and the best sequence is selected. In this case, the first sequence is by far the best. In fact, it is not necessary to enumerate all possible assignments because the scoring function cannot see more than two words away. In other words, in the process of enumerating part of speech sequences, it is possible in some cases to know that some sequence cannot possibly compete with another and can therefore be abandoned. Because of this fact, only 0(n) paths will be enumerated. Let us illustrate this optimization with an example: Find all assignments of parts of speech to &amp;quot;bird&amp;quot; and score the partial sequence. Henceforth, all scores are to be interpreted as log probabilities. (-4.848072 &amp;quot;NN&amp;quot;) Find all assignments of parts of speech to &amp;quot;a&amp;quot; and score. At this point, there are two paths: (-7.4453945 &amp;quot;AT&amp;quot; &amp;quot;NN&amp;quot;) (-15.01957 &amp;quot;IN&amp;quot; &amp;quot;NN&amp;quot;) Now, find assignments of &amp;quot;see&amp;quot; and score. At this point, the number of paths seem to be growing exponentially. 139 (-10.1914 &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NW) (-18.54318 &amp;quot;VB&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;NN&amp;quot;) (-29.974142 &amp;quot;UH&amp;quot; &amp;quot;AT&amp;quot; &amp;quot;NW) (-36.53299 &amp;quot;UH&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;NN&amp;quot;) Now, find assignments of &amp;quot;I&amp;quot; and score. Note, however, that it is no longer necessary to hypothesize that &amp;quot;a&amp;quot; might be a French preposition IN because all four paths, PPSS VB IN NN, NN VB IN NN, PPSS UH IN NN and NP UH AT NN score less well than some other path and there is no way that any additional input could make any difference. In particular, the path, PPSS VB IN NN scores less well than the path PPSS VB AT NN, and additional input will not help PPSS VB IN NN because the contextual scoring function has a limited window of three parts of speech, and that is not enough to see past the existing PPSS and VB. (-12.927581 &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;) (-24.177242 &amp;quot;NP&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;) (-35.667458 &amp;quot;PPSS&amp;quot; &amp;quot;UH&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;) (-44.33943 &amp;quot;NP&amp;quot; &amp;quot;UH&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;) The search continues two more iterations, assuming blank parts of speech for words out of range. (-13.262333 &amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;) (-26.5196 &amp;quot;&amp;quot; &amp;quot;NP&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&apos; &amp;quot;NN&amp;quot;) Fmally, the result is: PPSS VB AT NN. (-12.262333 &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;AT&amp;quot; The final result is: I/PPSS see/VB a/AT bird/NN. A slightly more interesting example is: &amp;quot;Can they can cans.&amp;quot; cans (-5.456845 &amp;quot;NNS&amp;quot;) can (-12.603266 &amp;quot;NN&amp;quot; &amp;quot;NNS&amp;quot;) (-15.935471 &amp;quot;VB&amp;quot; &amp;quot;NNS&amp;quot;) (-15.946739 &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;) they (-18.02618 &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;) (-18.779934 &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;NNS&amp;quot;) (-21.411636 &amp;quot;PPSS&amp;quot; &amp;quot;NN&amp;quot; &amp;quot;NNS&amp;quot;) can (-21.766554 &amp;quot;MD&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;&apos;VB&amp;quot; &amp;quot;NNS&amp;quot;) (-26.45485 &amp;quot;NN&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;) (-28.306572 &amp;quot;VB&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;) (-21.932137 &amp;quot; &amp;quot;MD&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;VB&amp;quot; &amp;quot;NNS&amp;quot;) (-30.170452 &amp;quot;&amp;quot;VB&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;) (-31.453785 &amp;quot; &amp;quot;NN&amp;quot; &amp;quot;PPSS&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;NNS&amp;quot;) And the result is: Can/MD they/PPSS can/VB cans/NNS (-20.932137 &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;PPSS&amp;quot; 4. Parsing Simple Non-Recursive Noun Phrases Stochastically Similar stochastic methods have been applied to locate simple noun phrases with very high accuracy. The program inserts brackets into a sequence of parts of speech, producing output [A/AT former/AP top/NN aide/NN] to/IN [Attorney/NP General/NP Edwin/NP Meese/NP] interceded/VBD to/TO extend/VB [an/AT aircraft/NN company/NN [business/NM with/IN [a,/AT lobbyist/NN] [who/WPS] worked/VBD for/IN [the/AT defense/NN contractor/NN] ,/, according/1N to/IN [a/AT published/VBN report/NN] .1. The proposed method is a stochastic analog of precedence parsing. Recall that precedence parsing makes use of a table that says whether to insert an open or close bracket between any two categories (terminal or nonterminal). The proposed method makes use of a table that givvs the probabilities of an open and close bracket between all pairs of parts of speech. A sample is shown below for the five parts of speech: AT (article), NN (singular noun), NNS (non-singular noun), VB (uninflected verb), IN (preposition). The table says, for example, that there is no chance of starting a noun phrases after an article (all five entries on the AT row are 0) and that there is a large probability of starting a noun phrase between a verb and an noun (the entry in 140 AT) 1.0).</abstract>
<title confidence="0.379377">Probability of Starting a Noun Phrase AT NN NNS VB IN</title>
<note confidence="0.810758833333333">AT 0 0 0 NN .99 .01 0 NNS 1.0 .02 .11 VB 1.0 1.0 1.0 IN 1.0 1.0 1.0 Probability of Ending a Noun Phrase AT NN NNS VB IN AT 0 0 0 0 0 NN 1.0 .01 0 0 1.0 NNS 1.0 .02 .11 1.0 1.0 VB 0 0 0 0 0 IN 0 0 0 0 .02</note>
<abstract confidence="0.9956935375">These probabilities were estimated from about 40,000 words (11,000 noun phrases) of training material selected from the Brown Corpus. The training material was parsed into noun phrases by laborious semi-automatic means (with considerable help from Eva Ejerhed). It took about a man-week to prepare the training material. The stochastic parser is given a sequence of parts of speech as input and is asked to insert brackets corresponding to the beginning and end of noun phrases. Conceptually, the parser enumerates all possible parsings of the input and scores each of them by the precedence probabilities. Consider, for example, the input sequence: NN VB. There are 5 possible ways to bracket this sequence (assuming no recursion): • NN VB • [NN] VB • NN VB] • [NN] [NM] • NN [V13] Each of these parsings is scored by multiplying 6 precedence probabilities, the probability of an open/close bracket appearing (or not appearing) in any one of the three positions (before the NN, after the NN or after the VB). The parsing with the highest score is returned as output. A small sample of the output is given in the appendix. The method works remarkably well considering how simple it is. There is some tendency to underestimate the number of brackets and run two noun phrases together as in time Fairchild]. proposed method omitted only 5 of 243 noun phrase brackets in the appendix. 5. Smoothing Issues Some of the probabilities are very hard to estimate by direct counting because of Zipf s Law (frequency is roughly proportional to inverse rank). Consider, for example, the lexical probabilities. We need to estimate how often each word appears with each part of speech. Unfortunately, because of Zipfs Law, no matter how much text we look at, there will always be a large tail of words that appear only a few times. In the Brown Corpus, for example, 40,000 words appear five times or less. If a word such as &amp;quot;yawn&amp;quot; appears once as a noun and once as a verb, what is the probability that it can be an adjective? It is impossible to say without more information. Fortunately, conventional dictionaries can help alleviate this problem to some extent. We add one to the frequency count of possibilities in the dictionary. For example, &amp;quot;yawn&amp;quot; happens to be listed in our dictionary as noun/verb ambiguous. Thus, we smooth the frequency counts obtained from the Brown Corpus by adding one to both possibilities. In this case, the probabilities remain unchanged. Both before and after smoothing, we estimate &amp;quot;yawn&amp;quot; to be a noun 50% of the time, and a verb the rest. There is no chance that &amp;quot;yawn&amp;quot; is an adjective. In some other cases, smoothing makes a big difference. Consider the word &amp;quot;cans.&amp;quot; This word appears 5 times as a plural noun and never as a verb in the Brown Corpus. The lexicon (and its morphological routines), fortunately, give both possibilities. Thus, the revised estimate is that &amp;quot;cans&amp;quot; appears 6/7 times as a plural noun and 1/7 times as a verb. Proper nouns and capitalized words are particularly problematic; some capitalized words are proper nouns and some are not. Estimates from the Brown Corpus can be misleading. For example, the capitalized word &amp;quot;Acts&amp;quot; is found twice in the Brown Corpus, both times as a 141 proper noun (in a title). It would be a mistake to infer from this evidence that the word &amp;quot;Acts&amp;quot; is always a proper noun. For this reason, capitalized words with small frequency counts (&lt; 20) were thrown out of the lexicon. Ejerhed, E., &amp;quot;Finding Clauses in Unrestricted Text by Stochastic and Funtary Methods,&amp;quot; abstracted submitted to this conference.</abstract>
<note confidence="0.957981285714286">There are other problems with capitalized words. Consider, for example, a sentence beginning with the capitalized word &amp;quot;Fall&amp;quot;; what is the probability that it is a proper noun (i.e., a surname)? Estimates from the Brown Corpus are of little help here since &amp;quot;Fall&amp;quot; never appears as a capitalized word and it never appears as a proper noun. Two steps were taken to alleviate this problem. First, the frequency estimates for &amp;quot;Fall&amp;quot; are computed from the estimates for &amp;quot;fall&amp;quot; plus 1 for the proper noun possibility. Thus, &amp;quot;Fall&amp;quot; has frequency estimates of: ((I . &amp;quot;NP&amp;quot;) (1 . &amp;quot;JJ&amp;quot;) (65 . &amp;quot;VB&amp;quot;) (72 . &amp;quot;NN&amp;quot;)) because &amp;quot;fall&amp;quot; has the estimates of: ((1 . &amp;quot;JJ&amp;quot;) (65 . &amp;quot;VB&amp;quot;) (72 . &amp;quot;NN&amp;quot;)). Secondly, a prepass was introduced which labels words as proper nouns if they are &amp;quot;adjacent to&amp;quot; other capitalized words (e.g., &amp;quot;White House,&amp;quot; &amp;quot;State of the Union&amp;quot;) or if they appear several times in a discourse and are always capitalized. Francis, W., and Kucera, H., &amp;quot;Frequency Analysis of English Usage,&amp;quot; Houghton Mifflin Company, Boston, 1982. The lexical probabilities are not the only probabilities that require smoothing. Contextual frequencies also seem to follow Zipf s Law. That is, for the set of all sequences of three parts of speech, we have plotted the frequency of the sequence against its rank on log log paper and observed the classic (approximately) linear relationship and slope of (almost) —1. It is clear that the contextual frequencies require smoothing. Zeros should be avoided. Leech, G., Garside, R., Atwell, E., &amp;quot;The Automatic Grammatical Tagging of the LOB Corpus,&amp;quot; ICAME News 7, 13-33, 1983. 6. Conclusion Marcus, M., &amp;quot;A Theory of Syntactic Recognition for Natural Language,&amp;quot; MIT Press, Cambridge, Massachusetts, 1980. A stochastic part of speech program and noun phrase parser has been presented. Performance is very encouraging as can be seen from the Appendix. &amp;quot;Webster&apos;s Seventh New Collegiate References Dictionary,&amp;quot; Merriam Company, Springfield, Massachusetts, 1972. Chomsky, N., &amp;quot;Three Models for the Description of Language,&amp;quot; IRE Transactions on Information Theory, vol. IT-2, Proceedings of the Symposium on Information Theory, 1956. Appendix: Sample Results Cobuild English Language Dictionary,&amp;quot; William Collins Sons &amp; Co Ltd, 1987. The following story was distributed over the AP during the week of May 26, 1987. There are just two tagging errors which are indicated with</note>
<abstract confidence="0.984225884615385">There are five missing brackets which are indicated as &amp;quot;*[&amp;quot; or &amp;quot;1&amp;quot;. Words with a second NP tag were identified as proper nouns in a prepass. [A/AT former/AP top/NN aide/NN] to/IN [At-torney/NP/NP General/NP/NP Edwin/NP/NP Meese/NP/NP] interceded/VBD to/TO extend/VB [an/AT aircraftNN company/NN &apos;s/$ govern-ment/NN contract/NN] ,/, then/RB went/VBD into/IN [business/NM] with/IN [a/AT lobby-ist/NN1 [who/WPS] worked/VBD for/IN [the/AT defense/NN contractor/NN] ,/, according/IN to/IN [a/AT published/VBN report/NN] ./. [James/NP/NP E/NP./NP Jenkins/NP/NP] ,/, [a/AT one-time/JJ senior/JJ deputy/NN] to/IN [Meese/NP/NP] joined/VBD [the/AT board/NN] of/IN [directors/NNS] of/IN [Transworld/NP/NP Group/NP/NP Ltd/NP./NP] on/IN [April/NP/NP 28/CD] ,/, [1984/CD] ,/, [the/AT Chicago/NP/NP Tribune/NP/NP] reporteci/VBD in/IN [its/PP$ Tuesday/NR editions/NNS] ./. [The/AT principal/JJ figure/NN] in/IN [Trans-world/NP/NP] was/BEDZ [Richard/NP/NP Mill-man/NP/NP] ,/, [a/AT lobbyist/NN] for/IN [Fair-child/NP/NP Industries/NP/NP Inc/NP./NP] ,/, Virginia/NP/NP de fense/NN con- 142 tractor/NN] I, [the/AT Tribune/NP/NP] said/VBD .1. [A/AT federal/JJ grand/JJ jury/NN] is/BEZ in- [the/AT Fairchild/NP/NP transaction/NN] and/CC [other/AP actions/NNS] of/IN [Meese/NP/NP] and/CC [former/AP White/NP/NP House/NP/NP aide/NN Nofziger/NP/NP] in/IN [connection/NN] with/IN [Wedtech/NP/NP New/NP/NP York/NP/NP defense/NN company/NN] [that/WPS] received/VBD [$250/CD million/CD] in/IN [govemment/NN contracts/NNS] issued/VBN without/EN [competitive/JJ bidding/NN] during/IN [the/AT Reagan/NP/NP administration/NN] ./. [Jenkins/NP/NP] left/VBD [the/AT White/NP/NP House/NP/NP] in/IN [1984/CD] 1, and/CC joined/VBD [Wedtech/NP/NP] as/CS [its/PP$ director/NN] of/IN [marketing/NN *]*[ two/CD years/NNS] later/RBR .1. [Deborah/NP/NP Tucker/NP/NP] ,/, [a/AT spokeswoman/NN] for/IN [Fairchild/NP/NP] ,/, said/VBD [Friday/NR] that/CS [the/AT cornpany/NN] had/HVD been/BEN contacted/VBN by/IN [the/AT office/NN] of/IN [independent/JJ counsel/NN James/NP/NP McKay/NP/NP] and/CC [subpoenas/NNS] had/HVD been/BEN served/VBN on/IN [Fairchild/NP/NP] ./. [Tucker/NP/NP] said/VBD [the/AT investigation/NN] involving/IN [Fairchild/NP/NP] had/HVD been/BEN going/VBG on/IN [a/AT number/NN] of/IN [weeks/NNS] and/CC predates/VBZ [last/AP week/NN &apos;s/$ exof/1N [McKay/NP/NP &apos;s/$ investigation/NN] to/TO include/VB [Meese/NP/NP] ./. [The/AT company/NN] is/BEZ cooperating/VBG in/1N [the/AT investigation/NN] ,/, [Tucker/NP/NP] said/VBD ./. [A/AT source/NN *] close/NN***] to/IN [McKay/NP/NP] said/VBD [last/AP week/NN&amp;quot; that/CS [Meese/NP/NP] isn&apos;t/BEZ* under/IN [cruninalaJ investigation/NN] in/IN [the/AT Fairchild/NP/NP matter/NN] ,/, but/RB is/BEZ [a/AT witness/NN] ./. [The/NP Tribune/NP/NP] said/VBD [Mill- ,/, acting/VBG as/CS [a/AT lobbyist/NN] for/1N [the/AT Chantilly/NP/NP] ,/, [Va/NP.-based/NP company/NN] ,/, went/VBD to/TO see/VB [Jenkins/NP/NP] in/IN [1982/CD] and/CC urged/VBD [him/PPO] and/CC [Meese/NP/NPI to/TO encourage/VB [the/AT Air/NP/NP Force/NP/NP] to/E0 extend/VB [the/AT production/NN] of/1/•1 [Fairchild/NP/NP &apos;s/$ A-10/NP bomber/NN] for/IN [a/AT year/NN] ./. [Millman/NP/NP] said/VBD there/RB was/BEDZ [a/AT lucrative/JJ market/NN] in/IN [Third/NP/NP World/NP/NP countries/NNS] ,/, but/CC that/CS [Fairchild/NP/NP &apos;s/$ chances/NNS] would/MD be/BE limited/VBN if/CS [the/AT Air/NP/NP Force/NP/NP] was/BEDZ not/* producing/VBG [the/AT plane/NN] ./. [The/AT Air/NP/NP Force/NP/NP] had/HVD decided/VBN to/TO discontinue/VB [production/NN] of/IN [the/AT A-10/NP] ,/, [a/AT 1960s-era/CD ground-support/NN attack/NN bomber/NN] at/IN [the/AT time/NN *]*[ Fairchild/NP/NP] was/BEDZ hoping/VBG to/TO sell/VB [A-10s/NP] abroad/RB j, [the/AT Tribune/NP/NP] said/VBD ./. [The/AT newspaper/NN] said/VBD [one/CD source/NN] reported/VBD that/CS after/CS [Millman/NP/NP] made/VBD [his/PPS pitch/NN] J, [Meese/NP/NP] ordered/VBD [Jenlcins/NP/NP] to/TO prepare/VB [a/AT memo/NN] on/IN [behalf/NN] of/IN [Fairchild/NP/NP] ./. [Memos/NP***] signed/VBD by/IN [Meese/NP/NP] ,/, stressing/VBG [the/AT importance/NN] of/IN [Fairchild/NP/NP &apos;s/$ arranging/VBG sales/NNS] in/IN [Third/NP/NP World/NP/NP countries/NNS] j, were/BED sent/VBN to/IN [the/AT State/NP/NP Department/NP/NP] and/CC [the/AT Air/NP/NP Force/NP/NP] ./. [Millman/NP/NP] did/DOD not/* return/VB [telephone/NN calls/NNS] to/EN [his/PP$ office/NN1 and/CC [referral/NN numbers/NNS] [Monday/NR] ,I, [the/AT Tribune/NP/NP] said/VBD ./.</abstract>
<intro confidence="0.71039">143</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<journal>tion/NN] with/IN [Wedtech/NP/NP Corp/NP./NP] j, [a/AT New/NP/NP York/NP/NP</journal>
<note>defense/NN company/NN</note>
<marker></marker>
<rawString> tion/NN] with/IN [Wedtech/NP/NP Corp/NP./NP] j, [a/AT New/NP/NP York/NP/NP defense/NN company/NN]</rawString>
</citation>
<citation valid="false">
<note>received/VBD [$250/CD million/CD] in/IN [govemment/NN contracts/NNS] issued/VBN without/EN [competitive/JJ bidding/NN] during/IN [the/AT Reagan/NP/NP administration/NN] ./.</note>
<marker>[that/WPS]</marker>
<rawString>received/VBD [$250/CD million/CD] in/IN [govemment/NN contracts/NNS] issued/VBN without/EN [competitive/JJ bidding/NN] during/IN [the/AT Reagan/NP/NP administration/NN] ./.</rawString>
</citation>
<citation valid="false">
<title>left/VBD [the/AT White/NP/NP House/NP/NP] in/IN [1984/CD] 1, and/CC joined/VBD [Wedtech/NP/NP]</title>
<note>as/CS [its/PP$ director/NN] of/IN [marketing/NN *]*[ two/CD years/NNS] later/RBR .1.</note>
<marker>[Jenkins/NP/NP]</marker>
<rawString>left/VBD [the/AT White/NP/NP House/NP/NP] in/IN [1984/CD] 1, and/CC joined/VBD [Wedtech/NP/NP] as/CS [its/PP$ director/NN] of/IN [marketing/NN *]*[ two/CD years/NNS] later/RBR .1.</rawString>
</citation>
<citation valid="false">
<title>spokeswoman/NN] for/IN [Fairchild/NP/NP] ,/, said/VBD [Friday/NR]</title>
<note>that/CS [the/AT cornpany/NN] had/HVD been/BEN contacted/VBN by/IN [the/AT office/NN] of/IN [independent/JJ counsel/NN James/NP/NP McKay/NP/NP] and/CC [subpoenas/NNS] had/HVD been/BEN served/VBN on/IN [Fairchild/NP/NP] ./.</note>
<marker>[Deborah/NP/NP Tucker/NP/NP]</marker>
<rawString>,/, [a/AT spokeswoman/NN] for/IN [Fairchild/NP/NP] ,/, said/VBD [Friday/NR] that/CS [the/AT cornpany/NN] had/HVD been/BEN contacted/VBN by/IN [the/AT office/NN] of/IN [independent/JJ counsel/NN James/NP/NP McKay/NP/NP] and/CC [subpoenas/NNS] had/HVD been/BEN served/VBN on/IN [Fairchild/NP/NP] ./.</rawString>
</citation>
<citation valid="false">
<note>said/VBD [the/AT investigation/NN] involving/IN [Fairchild/NP/NP] had/HVD been/BEN going/VBG on/IN [a/AT number/NN] of/IN [weeks/NNS] and/CC predates/VBZ [last/AP week/NN &apos;s/$ expansion/NN] of/1N [McKay/NP/NP &apos;s/$ investigation/NN] to/TO include/VB</note>
<marker>[Tucker/NP/NP]</marker>
<rawString>said/VBD [the/AT investigation/NN] involving/IN [Fairchild/NP/NP] had/HVD been/BEN going/VBG on/IN [a/AT number/NN] of/IN [weeks/NNS] and/CC predates/VBZ [last/AP week/NN &apos;s/$ expansion/NN] of/1N [McKay/NP/NP &apos;s/$ investigation/NN] to/TO include/VB</rawString>
</citation>
<citation valid="false">
<marker>[Meese/NP/NP]</marker>
<rawString>./.</rawString>
</citation>
<citation valid="false">
<note>is/BEZ cooperating/VBG in/1N [the/AT investigation/NN] ,/,</note>
<marker>[The/AT company/NN]</marker>
<rawString>is/BEZ cooperating/VBG in/1N [the/AT investigation/NN] ,/,</rawString>
</citation>
<citation valid="false">
<authors>
<author>saidVBD</author>
</authors>
<marker>[Tucker/NP/NP]</marker>
<rawString>said/VBD ./.</rawString>
</citation>
<citation valid="false">
<note>close/NN***] to/IN</note>
<marker>[A/AT source/NN *]</marker>
<rawString>close/NN***] to/IN</rawString>
</citation>
<citation valid="false">
<title>said/VBD [last/AP week/NN&amp;quot; that/CS [Meese/NP/NP]</title>
<note>isn&apos;t/BEZ* under/IN</note>
<marker>[McKay/NP/NP]</marker>
<rawString>said/VBD [last/AP week/NN&amp;quot; that/CS [Meese/NP/NP] isn&apos;t/BEZ* under/IN</rawString>
</citation>
<citation valid="false">
<note>in/IN [the/AT Fairchild/NP/NP matter/NN] ,/, but/RB is/BEZ</note>
<marker>[cruninalaJ investigation/NN]</marker>
<rawString>in/IN [the/AT Fairchild/NP/NP matter/NN] ,/, but/RB is/BEZ</rawString>
</citation>
<citation valid="false">
<marker>[a/AT witness/NN]</marker>
<rawString>./.</rawString>
</citation>
<citation valid="false">
<authors>
<author>saidVBD</author>
</authors>
<note>acting/VBG as/CS [a/AT lobbyist/NN] for/1N [the/AT Chantilly/NP/NP] ,/,</note>
<marker>[The/NP Tribune/NP/NP]</marker>
<rawString>said/VBD [Millman/NP/NP] ,/, acting/VBG as/CS [a/AT lobbyist/NN] for/1N [the/AT Chantilly/NP/NP] ,/,</rawString>
</citation>
<citation valid="false">
<title>to/TO see/VB [Jenkins/NP/NP] in/IN [1982/CD] and/CC urged/VBD [him/PPO]</title>
<journal>and/CC [Meese/NP/NPI to/TO encourage/VB [the/AT Air/NP/NP Force/NP/NP] to/E0 extend/VB</journal>
<marker>[Va/NP.-based/NP company/NN]</marker>
<rawString>,/, went/VBD to/TO see/VB [Jenkins/NP/NP] in/IN [1982/CD] and/CC urged/VBD [him/PPO] and/CC [Meese/NP/NPI to/TO encourage/VB [the/AT Air/NP/NP Force/NP/NP] to/E0 extend/VB</rawString>
</citation>
<citation valid="false">
<booktitle>of/1/•1 [Fairchild/NP/NP &apos;s/$ A-10/NP</booktitle>
<note>bomber/NN] for/IN [a/AT year/NN] ./.</note>
<marker>[the/AT production/NN]</marker>
<rawString>of/1/•1 [Fairchild/NP/NP &apos;s/$ A-10/NP bomber/NN] for/IN [a/AT year/NN] ./.</rawString>
</citation>
<citation valid="false">
<note>said/VBD there/RB was/BEDZ</note>
<marker>[Millman/NP/NP]</marker>
<rawString>said/VBD there/RB was/BEDZ</rawString>
</citation>
<citation valid="false">
<pages>in/IN</pages>
<marker>[a/AT lucrative/JJ market/NN]</marker>
<rawString>in/IN</rawString>
</citation>
<citation valid="false">
<note>but/CC that/CS [Fairchild/NP/NP &apos;s/$ chances/NNS] would/MD be/BE limited/VBN if/CS [the/AT Air/NP/NP Force/NP/NP] was/BEDZ not/* producing/VBG [the/AT plane/NN] ./.</note>
<marker>[Third/NP/NP World/NP/NP countries/NNS]</marker>
<rawString>,/, but/CC that/CS [Fairchild/NP/NP &apos;s/$ chances/NNS] would/MD be/BE limited/VBN if/CS [the/AT Air/NP/NP Force/NP/NP] was/BEDZ not/* producing/VBG [the/AT plane/NN] ./.</rawString>
</citation>
<citation valid="false">
<note>had/HVD decided/VBN to/TO discontinue/VB [production/NN] of/IN [the/AT A-10/NP] ,/, [a/AT 1960s-era/CD ground-support/NN attack/NN bomber/NN] at/IN [the/AT time/NN *]*[ Fairchild/NP/NP] was/BEDZ hoping/VBG to/TO sell/VB [A-10s/NP] abroad/RB j, [the/AT Tribune/NP/NP] said/VBD ./.</note>
<marker>[The/AT Air/NP/NP Force/NP/NP]</marker>
<rawString>had/HVD decided/VBN to/TO discontinue/VB [production/NN] of/IN [the/AT A-10/NP] ,/, [a/AT 1960s-era/CD ground-support/NN attack/NN bomber/NN] at/IN [the/AT time/NN *]*[ Fairchild/NP/NP] was/BEDZ hoping/VBG to/TO sell/VB [A-10s/NP] abroad/RB j, [the/AT Tribune/NP/NP] said/VBD ./.</rawString>
</citation>
<citation valid="false">
<note>said/VBD [one/CD source/NN] reported/VBD that/CS after/CS</note>
<marker>[The/AT newspaper/NN]</marker>
<rawString>said/VBD [one/CD source/NN] reported/VBD that/CS after/CS</rawString>
</citation>
<citation valid="false">
<note>made/VBD [his/PPS pitch/NN] J, [Meese/NP/NP] ordered/VBD [Jenlcins/NP/NP] to/TO prepare/VB [a/AT memo/NN] on/IN [behalf/NN] of/IN [Fairchild/NP/NP] ./.</note>
<marker>[Millman/NP/NP]</marker>
<rawString>made/VBD [his/PPS pitch/NN] J, [Meese/NP/NP] ordered/VBD [Jenlcins/NP/NP] to/TO prepare/VB [a/AT memo/NN] on/IN [behalf/NN] of/IN [Fairchild/NP/NP] ./.</rawString>
</citation>
<citation valid="false">
<note>signed/VBD by/IN</note>
<marker>[Memos/NP***]</marker>
<rawString>signed/VBD by/IN</rawString>
</citation>
<citation valid="false">
<title>[the/AT importance/NN] of/IN [Fairchild/NP/NP &apos;s/$ arranging/VBG sales/NNS]</title>
<journal>in/IN [Third/NP/NP World/NP/NP countries/NNS] j, were/BED sent/VBN to/IN [the/AT State/NP/NP Department/NP/NP] and/CC [the/AT Air/NP/NP Force/NP/NP] ./.</journal>
<marker>[Meese/NP/NP]</marker>
<rawString>,/, stressing/VBG [the/AT importance/NN] of/IN [Fairchild/NP/NP &apos;s/$ arranging/VBG sales/NNS] in/IN [Third/NP/NP World/NP/NP countries/NNS] j, were/BED sent/VBN to/IN [the/AT State/NP/NP Department/NP/NP] and/CC [the/AT Air/NP/NP Force/NP/NP] ./.</rawString>
</citation>
<citation valid="false">
<note>did/DOD not/* return/VB</note>
<marker>[Millman/NP/NP]</marker>
<rawString>did/DOD not/* return/VB</rawString>
</citation>
<citation valid="false">
<note>to/EN [his/PP$ office/NN1 and/CC [referral/NN numbers/NNS]</note>
<marker>[telephone/NN calls/NNS]</marker>
<rawString>to/EN [his/PP$ office/NN1 and/CC [referral/NN numbers/NNS]</rawString>
</citation>
<citation valid="false">
<authors>
<author>I</author>
</authors>
<tech>[the/AT Tribune/NP/NP] said/VBD ./.</tech>
<marker>[Monday/NR]</marker>
<rawString>,I, [the/AT Tribune/NP/NP] said/VBD ./.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>