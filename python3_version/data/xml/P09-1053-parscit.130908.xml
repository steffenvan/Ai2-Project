<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.842048">
Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition
</title>
<author confidence="0.954196">
Dipanjan Das and Noah A. Smith
</author>
<affiliation confidence="0.867894333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.991728">
{dipanjan,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964578947368">
We present a novel approach to decid-
ing whether two sentences hold a para-
phrase relationship. We employ a gen-
erative model that generates a paraphrase
of a given sentence, and we use proba-
bilistic inference to reason about whether
two sentences share the paraphrase rela-
tionship. The model cleanly incorporates
both syntax and lexical semantics using
quasi-synchronous dependency grammars
(Smith and Eisner, 2006). Furthermore,
using a product of experts (Hinton, 2002),
we combine the model with a comple-
mentary logistic regression model based
on state-of-the-art lexical overlap features.
We evaluate our models on the task of
distinguishing true paraphrase pairs from
false ones on a standard corpus, giving
competitive state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990263032258065">
The problem of modeling paraphrase relation-
ships between natural language utterances (McK-
eown, 1979) has recently attracted interest. For
computational linguists, solving this problem may
shed light on how best to model the semantics
of sentences. For natural language engineers, the
problem bears on information management sys-
tems like abstractive summarizers that must mea-
sure semantic overlap between sentences (Barzi-
lay and Lee, 2003), question answering modules
(Marsi and Krahmer, 2005) and machine transla-
tion (Callison-Burch et al., 2006).
The paraphrase identification problem asks
whether two sentences have essentially the same
meaning. Although paraphrase identification is
defined in semantic terms, it is usually solved us-
ing statistical classifiers based on shallow lexical,
n-gram, and syntactic “overlap” features. Such
overlap features give the best-published classifi-
cation accuracy for the paraphrase identification
task (Zhang and Patrick, 2005; Finch et al., 2005;
Wan et al., 2006; Corley and Mihalcea, 2005, in-
ter alia), but do not explicitly model correspon-
dence structure (or “alignment”) between the parts
of two sentences. In this paper, we adopt a model
that posits correspondence between the words in
the two sentences, defining it in loose syntactic
terms: if two sentences are paraphrases, we expect
their dependency trees to align closely, though
some divergences are also expected, with some
more likely than others. Following Smith and Eis-
ner (2006), we adopt the view that the syntactic
structure of sentences paraphrasing some sentence
s should be “inspired” by the structure of s.
Because dependency syntax is still only a crude
approximation to semantic structure, we augment
the model with a lexical semantics component,
based on WordNet (Miller, 1995), that models how
words are probabilistically altered in generating
a paraphrase. This combination of loose syntax
and lexical semantics is similar to the “Jeopardy”
model of Wang et al. (2007).
This syntactic framework represents a major de-
parture from useful and popular surface similarity
features, and the latter are difficult to incorporate
into our probabilistic model. We use a product of
experts (Hinton, 2002) to bring together a logis-
tic regression classifier built from n-gram overlap
features and our syntactic model. This combined
model leverages complementary strengths of the
two approaches, outperforming a strong state-of-
the-art baseline (Wan et al., 2006).
This paper is organized as follows. We intro-
duce our probabilistic model in §2. The model
makes use of three quasi-synchronous grammar
models (Smith and Eisner, 2006, QG, hereafter) as
components (one modeling paraphrase, one mod-
eling not-paraphrase, and one a base grammar);
these are detailed, along with latent-variable in-
ference and discriminative training algorithms, in
§3. We discuss the Microsoft Research Paraphrase
Corpus, upon which we conduct experiments, in
</bodyText>
<listItem confidence="0.495889">
§4. In §5, we present experiments on paraphrase
</listItem>
<page confidence="0.97912">
468
</page>
<note confidence="0.999613">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468–476,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999783166666667">
identification with our model and make compar-
isons with the existing state-of-the-art. We de-
scribe the product of experts and our lexical over-
lap model, and discuss the results achieved in §6.
We relate our approach to prior work (§7) and con-
clude (§8).
</bodyText>
<sectionHeader confidence="0.996307" genericHeader="introduction">
2 Probabilistic Model
</sectionHeader>
<bodyText confidence="0.999980125">
Since our task is a classification problem, we re-
quire our model to provide an estimate of the pos-
terior probability of the relationship (i.e., “para-
phrase,” denoted p, or “not paraphrase,” denoted
n), given the pair of sentences.1 Here, pQ denotes
model probabilities, c is a relationship class (p or
n), and s1 and s2 are the two sentences. We choose
the class according to:
</bodyText>
<equation confidence="0.995789">
c� = argmax pQ(c  |s1, s2)
cE{p,n}
= argmax pQ(c) x pQ(s1,s2  |c) (1)
cE{p,n}
</equation>
<bodyText confidence="0.9999865">
We define the class-conditional probabilities of
the two sentences using the following generative
story. First, grammar G0 generates a sentence s.
Then a class c is chosen, corresponding to a class-
specific probabilistic quasi-synchronous grammar
Gc. (We will discuss QG in detail in §3. For the
present, consider it a specially-defined probabilis-
tic model that generates sentences with a specific
property, like “paraphrases s,” when c = p.) Given
s, Gc generates the other sentence in the pair, s&apos;.
When we observe a pair of sentences s1 and s2
we do not presume to know which came first (i.e.,
which was s and which was s&apos;). Both orderings
are assumed to be equally probable. For class c,
</bodyText>
<equation confidence="0.999376666666667">
pQ(s1,s2  |c) =
0.5 x pQ(s1  |G0) x pQ(s2  |Gc(s1))
+ 0.5 x pQ(s2  |G0) x pQ(s1  |Gc(s2))(2)
</equation>
<bodyText confidence="0.999698222222222">
where c can be p or n; Gp(s) is the QG that gen-
erates paraphrases for sentence s, while Gn(s) is
the QG that generates sentences that are not para-
phrases of sentence s. This latter model may seem
counter-intuitive: since the vast majority of pos-
sible sentences are not paraphrases of s, why is a
special grammar required? Our use of a Gn fol-
lows from the properties of the corpus currently
used for learning, in which the negative examples
</bodyText>
<footnote confidence="0.832329333333333">
1Although we do not explore the idea here, the model
could be adapted for other sentence-pair relationships like en-
tailment or contradiction.
</footnote>
<bodyText confidence="0.971727">
were selected to have high lexical overlap. We re-
turn to this point in §4.
</bodyText>
<sectionHeader confidence="0.997855" genericHeader="method">
3 QG for Paraphrase Modeling
</sectionHeader>
<bodyText confidence="0.995684">
Here, we turn to the models Gp and Gn in detail.
</bodyText>
<subsectionHeader confidence="0.994146">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.999931090909091">
Smith and Eisner (2006) introduced the quasi-
synchronous grammar formalism. Here, we de-
scribe some of its salient aspects. The model
arose out of the empirical observation that trans-
lated sentences have some isomorphic syntactic
structure, but divergences are possible. Therefore,
rather than an isomorphic structure over a pair of
source and target sentences, the syntactic tree over
a target sentence is modeled by a source sentence-
specific grammar “inspired” by the source sen-
tence’s tree. This is implemented by associating
with each node in the target tree a subset of the
nodes in the source tree. Since it loosely links
the two sentences’ syntactic structures, QG is well
suited for problems like word alignment for MT
(Smith and Eisner, 2006) and question answering
(Wang et al., 2007).
Consider a very simple quasi-synchronous
context-free dependency grammar that generates
one dependent per production rule.2 Let s =
(s1,..., sm) be the source sentence. The grammar
rules will take one of the two forms:
</bodyText>
<equation confidence="0.820885">
(t, l) —* (t, l)(t&apos;, k) or (t, l) —* (t&apos;, k)(t, l)
</equation>
<bodyText confidence="0.9995940625">
where t and t&apos; range over the vocabulary of the
target language, and l and k E 10,..., m} are in-
dices in the source sentence, with 0 denoting null.3
Hard or soft constraints can be applied between l
and k in a rule. These constraints imply permissi-
ble “configurations.” For example, requiring l =� 0
and, if k =� 0 then sk must be a child of sl in the
source tree, we can implement a synchronous de-
pendency grammar similar to (Melamed, 2004).
Smith and Eisner (2006) used a quasi-
synchronous grammar to discover the correspon-
dence between words implied by the correspon-
dence between the trees. We follow Wang et al.
(2007) in treating the correspondences as latent
variables, and in using a WordNet-based lexical
semantics model to generate the target words.
</bodyText>
<footnote confidence="0.971002666666667">
2Our actual model is more complicated; see §3.2.
3A more general QG could allow one-to-many align-
ments, replacing l and k with sets of indices.
</footnote>
<page confidence="0.998869">
469
</page>
<subsectionHeader confidence="0.951834">
3.2 Detailed Model
</subsectionHeader>
<bodyText confidence="0.997986371428571">
We describe how we model pQ(t  |Gp(s)) and
pQ(t  |G„(s)) for source and target sentences s
and t (appearing in Eq. 2 alternately as s1 and s2).
A dependency tree on a sequence w =
(w1, ..., wk) is a mapping of indices of words to
indices of syntactic parents, τp : {1, ..., k} —*
{0,..., k}, and a mapping of indices of words to
dependency relation types in L, τ` : {1, ..., k} —*
L. The set of indices children of wi to its left,
{j : τw(j) = i, j &lt; i}, is denoted λw(i), and
ρw(i) is used for right children. wi has a single
parent, denoted by wτp(i). Cycles are not allowed,
and w0 is taken to be the dummy “wall” symbol,
$, whose only child is the root word of the sen-
tence (normally the main verb). The label for wi
is denoted by τ`(i). We denote the whole tree of
a sentence w by τw, the subtree rooted at the ith
word by τw,i.
Consider two sentences: let the source sen-
tence s contain m words and the target sentence
t contain n words. Let the correspondence x :
{1, ..., n} —* {0, ..., m} be a mapping from in-
dices of words in t to indices of words in s. (We
require each target word to map to at most one
source word, though multiple target words can
map to the same source word, i.e., x(i) = x(j)
while i =� j.) When x(i) = 0, the ith target word
maps to the wall symbol, equivalently a “null”
word. Each of our QGs Gp and G„ generates the
alignments x, the target tree τt, and the sentence
t. Both Gp and G„ are structured in the same way,
differing only in their parameters; henceforth we
discuss Gp; G„ is similar.
We assume that the parse trees of s and t are
known.4 Therefore our model defines:
</bodyText>
<equation confidence="0.998517">
pQ(t  |Gp(s)) = p(τt  |Gp(τs))
= Ex p(τt,x  |Gp(τs)) (3)
</equation>
<bodyText confidence="0.99668325">
Because the QG is essentially a context-free de-
pendency grammar, we can factor it into recur-
sive steps as follows (let i be an arbitrary index
in {1, ..., n}):
</bodyText>
<equation confidence="0.826541">
P(τt,i  |ti, x(i), τs) = pval(|λt(i)|,|ρt(i)  ||ti)
</equation>
<bodyText confidence="0.990666666666667">
4In our experiments, we use the parser described by Mc-
Donald et al. (2005), trained on sections 2–21 of the WSJ
Penn Treebank, transformed to dependency trees following
Yamada and Matsumoto (2003). (The same treebank data
were also to estimate many of the parameters of our model, as
discussed in the text.) Though it leads to a partial “pipeline”
approximation of the posterior probability p(c I s, t), we be-
lieve that the relatively high quality of English dependency
parsing makes this approximation reasonable.
</bodyText>
<equation confidence="0.999644">
11 X
jEλt(i)Uρt(i)
Xpkid(tj,τt` (j),x(j)  |ti, x(i),τs) (4)
</equation>
<bodyText confidence="0.999965857142857">
where pval and pkid are valence and child-
production probabilities parameterized as dis-
cussed in §3.4. Note the recursion in the second-
to-last line.
We next describe a dynamic programming so-
lution for calculating p(τt  |Gp(τs)). In §3.4 we
discuss the parameterization of the model.
</bodyText>
<subsectionHeader confidence="0.997356">
3.3 Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.999478666666667">
Let C(i, l) refer to the probability of τt,i, assum-
ing that the parent of ti, tτt p(i), is aligned to sl. For
leaves of τt, the base case is:
</bodyText>
<equation confidence="0.9893565">
C(i, l) = pval(0, 0  |ti) X (5)
Emk=0 pkid(ti, τt` (i), k  |tτtp(i), l, τs)
</equation>
<bodyText confidence="0.999867666666667">
where k ranges over possible values of x(i), the
source-tree node to which ti is aligned. The recur-
sive case is:
</bodyText>
<equation confidence="0.99976075">
C(i,l) = pval(|λt(i)|,|ρt(i)  ||ti) (6)
X Emk=0 pkid(ti, τt` (i), k  |tτtp(i),l, τs)
I
X IjEλt(i)Uρt(i) C(j, k)
</equation>
<bodyText confidence="0.9698388">
We assume that the wall symbols t0 and s0 are
aligned, so p(τt  |Gp(τs)) = C(r, 0), where r is
the index of the root word of the target tree τt. It
is straightforward to show that this algorithm re-
quires O(m2n) runtime and O(mn) space.
</bodyText>
<subsectionHeader confidence="0.977481">
3.4 Parameterization
</subsectionHeader>
<bodyText confidence="0.9999799375">
The valency distribution pval in Eq. 4 is estimated
in our model using the transformed treebank (see
footnote 4). For unobserved cases, the conditional
probability is estimated by backing off to the par-
ent POS tag and child direction.
We discuss next how to parameterize the prob-
ability pkid that appears in Equations 4, 5, and 6.
This conditional distribution forms the core of our
QGs, and we deviate from earlier research using
QGs in defining pkid in a fully generative way.
In addition to assuming that dependency parse
trees for s and t are observable, we also assume
each word wi comes with POS and named entity
tags. In our experiments these were obtained au-
tomatically using MXPOST (Ratnaparkhi, 1996)
and BBN’s Identifinder (Bikel et al., 1999).
</bodyText>
<equation confidence="0.951454333333333">
m
E P(τt,j  |tj,x(j),τs)
x(j)=0
</equation>
<page confidence="0.854353">
470
</page>
<bodyText confidence="0.857704">
For clarity, let j = τtp(i) and let l = x(j).
</bodyText>
<equation confidence="0.990155625">
pkid(ti, τt` (i), x(i)  |tj, l, τs) =
pconfig(config(ti, tj, sx(i), sl)  |tj, l, τs) (7)
Xpunif (x(i)  |config(ti, tj, sx(i), sl)) (8)
Xplab(τt` (i)  |config(ti, tj, sx(i), sl)) (9)
Xppos(Pos(ti)  |Pos(sx(i))) (10)
Xpne(ne(ti)  |ne(sx(i))) (11)
Xplsrel(lsrel(ti)  |sx(i)) (12)
Xpword(ti  |lsrel(ti),sx(i)) (13)
</equation>
<bodyText confidence="0.999323111111111">
We consider each of the factors above in turn.
Configuration In QG, “configurations” refer to
the tree relationship among source-tree nodes
(above, sl and sx(i)) aligned to a pair of parent-
child target-tree nodes (above, tj and ti). In deriv-
ing τt,j, the model first chooses the configuration
that will hold among ti, tj, sx(i) (which has yet
to be chosen), and sl (line 7). This is defined for
configuration c log-linearly by:5
</bodyText>
<equation confidence="0.91755">
pconfig(c  |tj, l, τs) =
αc0
c0:�1sk,config(ti,tj,sk,sl)=c0
(14)
</equation>
<bodyText confidence="0.939706592592592">
Permissible configurations in our model are shown
in Table 1. These are identical to prior work
(Smith and Eisner, 2006; Wang et al., 2007),
except that we add a “root” configuration that
aligns the target parent-child pair to null and the
head word of the source sentence, respectively.
Using many permissible configurations helps re-
move negative effects from noisy parses, which
our learner treats as evidence. Fig. 1 shows some
examples of major configurations that Gp discov-
ers in the data.
Source tree alignment After choosing the config-
uration, the specific node in τs that ti will align
to, sx(i) is drawn uniformly (line 8) from among
those in the configuration selected.
Dependency label, POS, and named entity class
The newly generated target word’s dependency
label, POS, and named entity class drawn from
multinomial distributions plab, ppos, and pne that
condition, respectively, on the configuration and
the POS and named entity class of the aligned
source-tree word sx(i) (lines 9–11).
5We use log-linear models three times: for the configura-
tion, the lexical semantics class, and the word. Each time,
we are essentially assigning one weight per outcome and
renormalizing among the subset of outcomes that are possible
given what has been derived so far.
</bodyText>
<table confidence="0.996089866666667">
Configuration Description
parent-child TP(x(i)) = x(j), appended with Te(x(i))
child-parent x(i) = TP(x(j)), appended with Te(x(j))
grandparent- TP(TP(x(i))) = x(j), appended with
grandchild TS (x(i))
siblings TP(x(i)) = TP(x(j)), x(i) =� x(j)
same-node x(i) = x(j)
c-command the parent of one source-side word is an
ancestor of the other source-side word
root x(j) = 0, x(i) is the root of s
child-null x(i) = 0
parent-null x(j) = 0, x(i) is something other than
root of s
other catch-all for all other types of configura-
tions, which are permitted
</table>
<tableCaption confidence="0.9954685">
Table 1: Permissible configurations. i is an index in t whose
configuration is to be chosen; j = TP(i) is i’s parent.
</tableCaption>
<bodyText confidence="0.999150461538462">
WordNet relation(s) The model next chooses a
lexical semantics relation between sx(i) and the
yet-to-be-chosen word ti (line 12). Following
Wang et al. (2007),6 we employ a 14-feature log-
linear model over all logically possible combina-
tions of the 14 WordNet relations (Miller, 1995).7
Similarly to Eq. 14, we normalize this log-linear
model based on the set of relations that are non-
empty in WordNet for the word sx(i).
Word Finally, the target word is randomly chosen
from among the set of words that bear the lexical
semantic relationship just chosen (line 13). This
distribution is, again, defined log-linearly:
</bodyText>
<equation confidence="0.7066035">
Ew0:s�(i)Rw0 αw0
(15)
</equation>
<bodyText confidence="0.989101333333333">
Here αw is the Good-Turing unigram probability
estimate of a word w from the Gigaword corpus
(Graff, 2003).
</bodyText>
<subsectionHeader confidence="0.981165">
3.5 Base Grammar G0
</subsectionHeader>
<bodyText confidence="0.999767571428571">
In addition to the QG that generates a second sen-
tence bearing the desired relationship (paraphrase
or not) to the first sentence s, our model in §2 also
requires a base grammar G0 over s.
We view this grammar as a trivial special case
of the same QG model already described. G0 as-
sumes the empty source sentence consists only of
</bodyText>
<footnote confidence="0.979170444444444">
6Note that Wang et al. (2007) designed pkid as an inter-
polation between a log-linear lexical semantics model and a
word model. Our approach is more fully generative.
7These are: identical-word, synonym, antonym (includ-
ing extended and indirect antonym), hypernym, hyponym,
derived form, morphological variation (e.g., plural form),
verb group, entailment, entailed-by, see-also, causal relation,
whether the two words are same and is a number, and no re-
lation.
</footnote>
<equation confidence="0.926035666666667">
αc
αti
pword(ti  |lsrel(ti) = R, sx(i)) =
</equation>
<page confidence="0.971294">
471
</page>
<figure confidence="0.999962195121951">
(b) child-parent (c) grandparent-grandchild
(a) parent-child
(d) c-command
quarter
first-quarter
(e) same-node
first
will
questionnaire
questionnaire
Liscouski
Secretary
collected
signatures
signatures
chief
will
fill
complete
dozens
wounded
dozens
injured
necessary
approaching
twice
treasury
needed
fell
dropped
refunding
(g) root
null
897,158
(f) siblings
treasury
null
massive
the
U.S
U.S
</figure>
<figureCaption confidence="0.6843725">
Figure 1: Some example configurations from Table 1 that Gp discovers in the dev. data. Directed arrows show head-modifier
relationships, while dotted arrows show alignments.
a single wall node. Thus every word generated un-
der G0 aligns to null, and we can simplify the dy-
namic programming algorithm that scores a tree
τs under G0:
</figureCaption>
<equation confidence="0.999963333333333">
C&apos;(i) = pval(|λt(i)|,|ρt(i)  ||si)
Xplab(τt� (i)) X ppos(pos(ti)) X p.e(ne(ti))
Xpword(ti) X 11j:Tt(j)=i C&apos;(j) (16)
</equation>
<bodyText confidence="0.999988">
where the final product is 1 when ti has no chil-
dren. It should be clear that p(s  |G0) = C&apos;(0).
We estimate the distributions over dependency
labels, POS tags, and named entity classes using
the transformed treebank (footnote 4). The dis-
tribution over words is taken from the Gigaword
corpus (as in §3.4).
It is important to note that G0 is designed to give
a smoothed estimate of the probability of a partic-
ular parsed, named entity-tagged sentence. It is
never used for parsing or for generation; it is only
used as a component in the generative probability
model presented in §2 (Eq. 2).
</bodyText>
<subsectionHeader confidence="0.987869">
3.6 Discriminative Training
</subsectionHeader>
<bodyText confidence="0.997885">
Given training data C (s(i) , s(i)2, c(i))&gt; N , we train
</bodyText>
<equation confidence="0.774048111111111">
i=1
the model discriminatively by maximizing regu-
larized conditional likelihood:
log pQ(c(i)  |s(i)
1 , s(i)
2 , O)
Y
Eq. 2 relates this to G{0,p,�1
(17)
</equation>
<bodyText confidence="0.999975809523809">
The parameters O to be learned include the class
priors, the conditional distributions of the depen-
dency labels given the various configurations, the
POS tags given POS tags, the NE tags given NE
tags appearing in expressions 9–11, the configura-
tion weights appearing in Eq. 14, and the weights
of the various features in the log-linear model for
the lexical-semantics model. As noted, the distri-
butions pval, the word unigram weights in Eq. 15,
and the parameters of the base grammar are fixed
using the treebank (see footnote 4) and the Giga-
word corpus.
Since there is a hidden variable (x), the objec-
tive function is non-convex. We locally optimize
using the L-BFGS quasi-Newton method (Liu and
Nocedal, 1989). Because many of our parameters
are multinomial probabilities that are constrained
to sum to one and L-BFGS is not designed to han-
dle constraints, we treat these parameters as un-
normalized weights that get renormalized (using a
softmax function) before calculating the objective.
</bodyText>
<sectionHeader confidence="0.977219" genericHeader="method">
4 Data and Task
</sectionHeader>
<bodyText confidence="0.999991">
In all our experiments, we have used the Mi-
crosoft Research Paraphrase Corpus (Dolan et al.,
2004; Quirk et al., 2004). The corpus contains
5,801 pairs of sentences that have been marked
as “equivalent” or “not equivalent.” It was con-
structed from thousands of news sources on the
web. Dolan and Brockett (2005) remark that
this corpus was created semi-automatically by first
training an SVM classifier on a disjoint annotated
10,000 sentence pair dataset and then applying
the SVM on an unseen 49,375 sentence pair cor-
pus, with its output probabilities skewed towards
over-identification, i.e., towards generating some
false paraphrases. 5,801 out of these 49,375 pairs
were randomly selected and presented to human
judges for refinement into true and false para-
phrases. 3,900 of the pairs were marked as having
</bodyText>
<equation confidence="0.95283975">
N
max
Θ i=1
−ClIOlI22
</equation>
<page confidence="0.983999">
472
</page>
<figure confidence="0.5039415">
About 120 potential jurors were being asked to complete a lengthy questionnaire .
The jurors were taken into the courtroom in groups of 40 and asked to fill out a questionnaire .
</figure>
<figureCaption confidence="0.994983">
Figure 2: Discovered alignment of Ex. 19 produced by GP. Observe that the model aligns identical words and also “complete”
and “fill” in this specific case. This kind of alignment provides an edge over a simple lexical overlap model.
</figureCaption>
<bodyText confidence="0.999634444444444">
“mostly bidirectional entailment,” a standard def-
inition of the paraphrase relation. Each sentence
was labeled first by two judges, who averaged 83%
agreement, and a third judge resolved conflicts.
We use the standard data split into 4,076 (2,753
paraphrase, 1,323 not) training and 1,725 (1147
paraphrase, 578 not) test pairs. We reserved a ran-
domly selected 1,075 training pairs for tuning.We
cite some examples from the training set here:
</bodyText>
<listItem confidence="0.716811">
(18) Revenue in the first quarter of the year dropped 15
percent from the same period a year earlier.
</listItem>
<bodyText confidence="0.979987707317073">
With the scandal hanging over Stewart’s company,
revenue in the first quarter of the year dropped 15
percent from the same period a year earlier.
(19) About 120 potential jurors were being asked to
complete a lengthy questionnaire.
The jurors were taken into the courtroom in groups of
40 and asked to fill out a questionnaire.
Ex. 18 is a true paraphrase pair. Notice the high
lexical overlap between the two sentences (uni-
gram overlap of 100% in one direction and 72%
in the other). Ex. 19 is another true paraphrase
pair with much lower lexical overlap (unigram
overlap of 50% in one direction and 30% in the
other). Notice the use of similar-meaning phrases
and irrelevant modifiers that retain the same mean-
ing in both sentences, which a lexical overlap
model cannot capture easily, but a model like a QG
might. Also, in both pairs, the relationship cannot
be called total bidirectional equivalence because
there is some extra information in one sentence
which cannot be inferred from the other.
Ex. 20 was labeled “not paraphrase”:
(20) “There were a number of bureaucratic and
administrative missed signals - there’s not one person
who’s responsible here,” Gehman said.
In turning down the NIMA offer, Gehman said, “there
were a number of bureaucratic and administrative
missed signals here.
There is significant content overlap, making a de-
cision difficult for a naive lexical overlap classifier.
(In fact, pQ labels this example n while the lexical
overlap models label it p.)
The fact that negative examples in this corpus
were selected because of their high lexical over-
lap is important. It means that any discrimina-
tive model is expected to learn to distinguish mere
overlap from paraphrase. This seems appropriate,
but it does mean that the “not paraphrase” relation
ought to be denoted “not paraphrase but decep-
tively similar on the surface.” It is for this reason
that we use a special QG for the n relation.
</bodyText>
<sectionHeader confidence="0.996202" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999984375">
Here we present our experimental evaluation using
pQ. We trained on the training set (3,001 pairs)
and tuned model metaparameters (C in Eq. 17)
and the effect of different feature sets on the de-
velopment set (1,075 pairs). We report accuracy
on the official MSRPC test dataset. If the poste-
rior probability pQ(p I s1, s2) is greater than 0.5,
the pair is labeled “paraphrase” (as in Eq. 1).
</bodyText>
<subsectionHeader confidence="0.952617">
5.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999973916666667">
We replicated a state-of-the-art baseline model for
comparison. Wan et al. (2006) report the best pub-
lished accuracy, to our knowledge, on this task,
using a support vector machine. Our baseline is
a reimplementation of Wan et al. (2006), using
features calculated directly from s1 and s2 with-
out recourse to any hidden structure: proportion
of word unigram matches, proportion of lemma-
tized unigram matches, BLEU score (Papineni et
al., 2001), BLEU score on lemmatized tokens, F
measure (Turian et al., 2003), difference of sen-
tence length, and proportion of dependency rela-
tion overlap. The SVM was trained to classify
positive and negative examples of paraphrase us-
ing SVM&amp;quot;ght (Joachims, 1999).8 Metaparameters,
tuned on the development data, were the regu-
larization constant and the degree of the polyno-
mial kernel (chosen in [10−5, 102] and 1–5 respec-
tively.).9
It is unsurprising that the SVM performs very
well on the MSRPC because of the corpus creation
process (see Sec. 4) where an SVM was applied
as well, with very similar features and a skewed
decision process (Dolan and Brockett, 2005).
</bodyText>
<footnote confidence="0.931257">
8http://svmlight.joachims.org
9Our replication of the Wan et al. model is approxi-
mate, because we used different preprocessing tools: MX-
POST for POS tagging (Ratnaparkhi, 1996), MSTParser
for parsing (McDonald et al., 2005), and Dan Bikel’s
interface (http://www.cis.upenn.edu/˜dbikel/
software.html#wn) to WordNet (Miller, 1995) for
lemmatization information. Tuning led to C = 17 and poly-
nomial degree 4.
</footnote>
<page confidence="0.997577">
473
</page>
<tableCaption confidence="0.723051">
Table 2: Accuracy,
</tableCaption>
<table confidence="0.969617666666667">
p-class precision, and
p-class recall on the test
set (N = 1,725). See
text for differences in
implementation
between Wan et al. and
our replication; their
reported score does not
include the full test set.
Model Accuracy Precision Recall
baselines all p 66.49 66.49 100.00
Wan et al. SVM (reported) 75.63 77.00 90.00
Wan et al. SVM (replication) 75.42 76.88 90.14
lexical semantics features removed 68.64 68.84 96.51
pQ all features 73.33 74.48 91.10
c-command disallowed (best; see text) 73.86 74.89 91.28
§6 pL 75.36 78.12 87.44
product of experts 76.06 79.57 86.05
</table>
<note confidence="0.894100333333333">
Wan et al. SVM and pL 80.17 100.00 92.07
oracles Wan et al. SVM and pQ 83.42 100.00 96.60
pQ and pL 83.19 100.00 95.29
</note>
<subsectionHeader confidence="0.928821">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999996903225806">
Tab. 2 shows performance achieved by the base-
line SVM and variations on pQ on the test set. We
performed a few feature ablation studies, evaluat-
ing on the development data. We removed the lex-
ical semantics component of the QG,10 and disal-
lowed the syntactic configurations one by one, to
investigate which components of pQ contributes to
system performance. The lexical semantics com-
ponent is critical, as seen by the drop in accu-
racy from the table (without this component, pQ
behaves almost like the “all p” baseline). We
found that the most important configurations are
“parent-child,” and “child-parent” while damage
from ablating other configurations is relatively
small. Most interestingly, disallowing the “c-
command” configuration resulted in the best ab-
solute accuracy, giving us the best version of pQ.
The c-command configuration allows more distant
nodes in a source sentence to align to parent-child
pairs in a target (see Fig. 1d). Allowing this con-
figuration guides the model in the wrong direction,
thus reducing test accuracy. We tried disallowing
more than one configuration at a time, without get-
ting improvements on development data. We also
tried ablating the WordNet relations, and observed
that the “identical-word” feature hurt the model
the most. Ablating the rest of the features did not
produce considerable changes in accuracy.
The development data-selected pQ achieves
higher recall by 1 point than Wan et al.’s SVM,
but has precision 2 points worse.
</bodyText>
<subsectionHeader confidence="0.989207">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9936258">
It is quite promising that a linguistically-motivated
probabilistic model comes so close to a string-
similarity baseline, without incorporating string-
local phrases. We see several reasons to prefer
10This is accomplished by eliminating lines 12 and 13 from
the definition of pkid and redefining p.,,d to be the unigram
word distribution estimated from the Gigaword corpus, as in
Go, without the help of WordNet.
the more intricate QG to the straightforward SVM.
First, the QG discovers hidden alignments be-
tween words. Alignments have been leveraged in
related tasks such as textual entailment (Giampic-
colo et al., 2007); they make the model more inter-
pretable in analyzing system output (e.g., Fig. 2).
Second, the paraphrases of a sentence can be con-
sidered to be monolingual translations. We model
the paraphrase problem using a direct machine
translation model, thus providing a translation in-
terpretation of the problem. This framework could
be extended to permit paraphrase generation, or to
exploit other linguistic annotations, such as repre-
sentations of semantics (see, e.g., Qiu et al., 2006).
Nonetheless, the usefulness of surface overlap
features is difficult to ignore. We next provide an
efficient way to combine a surface model with pQ.
</bodyText>
<sectionHeader confidence="0.952565" genericHeader="method">
6 Product of Experts
</sectionHeader>
<bodyText confidence="0.999900473684211">
Incorporating structural alignment and surface
overlap features inside a single model can make
exact inference infeasible. As an example, con-
sider features like n-gram overlap percentages that
provide cues of content overlap between two sen-
tences. One intuitive way of including these fea-
tures in a QG could be including these only at
the root of the target tree, i.e. while calculating
C(r, 0). These features have to be included in
estimating pkid, which has log-linear component
models (Eq. 7- 13). For these bigram or trigram
overlap features, a similar log-linear model has
to be normalized with a partition function, which
considers the (unnormalized) scores of all possible
target sentences, given the source sentence.
We therefore combine pQ with a lexical overlap
model that gives another posterior probability es-
timate pL(c  |s1, s2) through a product of experts
(PoE; Hinton, 2002), pJ(c  |s1, s2)
</bodyText>
<equation confidence="0.945770666666667">
pQ(c  |s1, s2) X pL(c  |s1, s2)(21)
EpQ(c&apos;  |s1, s2) X pL(c&apos;  |s1, s2)
c&apos;E{p,n}
</equation>
<page confidence="0.992519">
474
</page>
<bodyText confidence="0.992085203703704">
Eq. 21 takes the product of the two models’ poste-
rior probabilities, then normalizes it to sum to one.
PoE models are used to efficiently combine several
expert models that individually constrain different
dimensions in high-dimensional data, the product
therefore constraining all of the dimensions. Com-
bining models in this way grants to each expert
component model the ability to “veto” a class by
giving it low probability; the most probable class
is the one that is least objectionable to all experts.
Probabilistic Lexical Overlap Model We de-
vised a logistic regression (LR) model incorpo-
rating 18 simple features, computed directly from
s1 and s2, without modeling any hidden corre-
spondence. LR (like the QG) provides a proba-
bility distribution, but uses surface features (like
the SVM). The features are of the form precisionn
(number of n-gram matches divided by the num-
ber of n-grams in s1), recalln (number of n-gram
matches divided by the number of n-grams in s2)
and Fn (harmonic mean of the previous two fea-
tures), where 1 &lt; n &lt; 3. We also used lemma-
tized versions of these features. This model gives
the posterior probability pL(c  |s1, s2), where
c E {p, n}. We estimated the model parameters
analogously to Eq. 17. Performance is reported in
Tab. 2; this model is on par with the SVM, though
trading recall in favor of precision. We view it as a
probabilistic simulation of the SVM more suitable
for combination with the QG.
Training the PoE Various ways of training a PoE
exist. We first trained pQ and pL separately as
described, then initialized the PoE with those pa-
rameters. We then continued training, maximizing
(unregularized) conditional likelihood.
Experiment We used pQ with the “c-command”
configuration excluded, and the LR model in the
product of experts. Tab. 2 includes the final re-
sults achieved by the PoE. The PoE model outper-
forms all the other models, achieving an accuracy
of 76.06%.11 The PoE is conservative, labeling a
pair as p only if the LR and the QG give it strong
p probabilities. This leads to high precision, at the
expense of recall.
Oracle Ensembles Tab. 2 shows the results of
three different oracle ensemble systems that cor-
rectly classify a pair if either of the two individual
systems in the combination is correct. Note that
the combinations involving pQ achieve 83%, the
11This accuracy is significant over pQ under a paired t-test
(p &lt; 0.04), but is not significant over the SVM.
human agreement level for the MSRPC. The LR
and SVM are highly similar, and their oracle com-
bination does not perform as well.
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999983434782609">
There is a growing body of research that uses the
MSRPC (Dolan et al., 2004; Quirk et al., 2004)
to build models of paraphrase. As noted, the most
successful work has used edit distance (Zhang and
Patrick, 2005) or bag-of-words features to mea-
sure sentence similarity, along with shallow syn-
tactic features (Finch et al., 2005; Wan et al., 2006;
Corley and Mihalcea, 2005). Qiu et al. (2006)
used predicate-argument annotations.
Most related to our approach, Wu (2005) used
inversion transduction grammars—a synchronous
context-free formalism (Wu, 1997)—for this task.
Wu reported only positive-class (p) precision (not
accuracy) on the test set. He obtained 76.1%,
while our PoE model achieves 79.6% on that mea-
sure. Wu’s model can be understood as a strict
hierarchical maximum-alignment method. In con-
trast, our alignments are soft (we sum over them),
and we do not require strictly isomorphic syntac-
tic structures. Most importantly, our approach is
founded on a stochastic generating process and es-
timated discriminatively for this task, while Wu
did not estimate any parameters from data at all.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99996275">
In this paper, we have presented a probabilistic
model of paraphrase incorporating syntax, lexi-
cal semantics, and hidden loose alignments be-
tween two sentences’ trees. Though it fully de-
fines a generative process for both sentences and
their relationship, the model is discriminatively
trained to maximize conditional likelihood. We
have shown that this model is competitive for de-
termining whether there exists a semantic rela-
tionship between them, and can be improved by
principled combination with more standard lexical
overlap approaches.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997947666666667">
The authors thank the three anonymous review-
ers for helpful comments and Alan Black, Freder-
ick Crabbe, Jason Eisner, Kevin Gimpel, Rebecca
Hwa, David Smith, and Mengqiu Wang for helpful
discussions. This work was supported by DARPA
grant NBCH-1080004.
</bodyText>
<page confidence="0.998906">
475
</page>
<sectionHeader confidence="0.993888" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899913978494">
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proc. of NAACL.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what’s
in a name. Machine Learning, 34(1-3):211–231.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proc. of HLT-NAACL.
Courtney Corley and Rada Mihalcea. 2005. Mea-
suring the semantic similarity of texts. In Proc. of
ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment.
William B. Dolan and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proc. of IWP.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
Proc. of COLING.
Andrew Finch, Young Sook Hwang, and Eiichiro
Sumita. 2005. Using machine translation evalua-
tion techniques to determine sentence-level seman-
tic equivalence. In Proc. of IWP.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Computation, 14:1771–1800.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming (Ser. B), 45(3):503–528.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proc. of EWNLG.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Kathleen R. McKeown. 1979. Paraphrasing using
given and new information in a question-answer sys-
tem. In Proc. of ACL.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proc. of ACL.
George A. Miller. 1995. Wordnet: a lexical database
for English. Commun. ACM, 38(11):39–41.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. ofACL.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proc. of EMNLP.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proc. of EMNLP.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proc. of
EMNLP.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proc. of the HLT-
NAACL Workshop on Statistical Machine Transla-
tion.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003. Evaluation of machine translation and its
evaluation. In Proc. of Machine Translation Summit
IX.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. 2006. Using dependency-based features to
take the “para-farce” out of paraphrase. In Proc. of
ALTW.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proc. of EMNLP-
CoNLL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3).
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars.
In Proc. of the ACL Workshop on Empirical Model-
ing of Semantic Equivalence and Entailment.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT.
Yitao Zhang and Jon Patrick. 2005. Paraphrase identi-
fication by text canonicalization. In Proc. of ALTW.
</reference>
<page confidence="0.999105">
476
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985657">
<title confidence="0.999982">Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition</title>
<author confidence="0.99993">Das A Smith</author>
<affiliation confidence="0.999124">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.99935165">We present a novel approach to deciding whether two sentences hold a paraphrase relationship. We employ a generative model that generates a paraphrase of a given sentence, and we use probabilistic inference to reason about whether two sentences share the paraphrase relationship. The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006). Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1434" citStr="Barzilay and Lee, 2003" startWordPosition="197" endWordPosition="201">atures. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter </context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard L Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="12589" citStr="Bikel et al., 1999" startWordPosition="2121" endWordPosition="2124"> the conditional probability is estimated by backing off to the parent POS tag and child direction. We discuss next how to parameterize the probability pkid that appears in Equations 4, 5, and 6. This conditional distribution forms the core of our QGs, and we deviate from earlier research using QGs in defining pkid in a fully generative way. In addition to assuming that dependency parse trees for s and t are observable, we also assume each word wi comes with POS and named entity tags. In our experiments these were obtained automatically using MXPOST (Ratnaparkhi, 1996) and BBN’s Identifinder (Bikel et al., 1999). m E P(τt,j |tj,x(j),τs) x(j)=0 470 For clarity, let j = τtp(i) and let l = x(j). pkid(ti, τt` (i), x(i) |tj, l, τs) = pconfig(config(ti, tj, sx(i), sl) |tj, l, τs) (7) Xpunif (x(i) |config(ti, tj, sx(i), sl)) (8) Xplab(τt` (i) |config(ti, tj, sx(i), sl)) (9) Xppos(Pos(ti) |Pos(sx(i))) (10) Xpne(ne(ti) |ne(sx(i))) (11) Xplsrel(lsrel(ti) |sx(i)) (12) Xpword(ti |lsrel(ti),sx(i)) (13) We consider each of the factors above in turn. Configuration In QG, “configurations” refer to the tree relationship among source-tree nodes (above, sl and sx(i)) aligned to a pair of parentchild target-tree nodes (</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard L. Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="1542" citStr="Callison-Burch et al., 2006" startWordPosition="213" endWordPosition="216"> standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence structure (or “alignment”) between the parts of two senten</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proc. of ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</booktitle>
<contexts>
<context position="2026" citStr="Corley and Mihalcea, 2005" startWordPosition="281" endWordPosition="284">entences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence structure (or “alignment”) between the parts of two sentences. In this paper, we adopt a model that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the structure of s. Because dep</context>
<context position="32772" citStr="Corley and Mihalcea, 2005" startWordPosition="5425" endWordPosition="5428">e 11This accuracy is significant over pQ under a paired t-test (p &lt; 0.04), but is not significant over the SVM. human agreement level for the MSRPC. The LR and SVM are highly similar, and their oracle combination does not perform as well. 7 Related Work There is a growing body of research that uses the MSRPC (Dolan et al., 2004; Quirk et al., 2004) to build models of paraphrase. As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005). Qiu et al. (2006) used predicate-argument annotations. Most related to our approach, Wu (2005) used inversion transduction grammars—a synchronous context-free formalism (Wu, 1997)—for this task. Wu reported only positive-class (p) precision (not accuracy) on the test set. He obtained 76.1%, while our PoE model achieves 79.6% on that measure. Wu’s model can be understood as a strict hierarchical maximum-alignment method. In contrast, our alignments are soft (we sum over them), and we do not require strictly isomorphic syntactic structures. Most importantly, our approach is founded on a stocha</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proc. of ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proc. of IWP.</booktitle>
<contexts>
<context position="19995" citStr="Dolan and Brockett (2005)" startWordPosition="3324" endWordPosition="3327">nd Nocedal, 1989). Because many of our parameters are multinomial probabilities that are constrained to sum to one and L-BFGS is not designed to handle constraints, we treat these parameters as unnormalized weights that get renormalized (using a softmax function) before calculating the objective. 4 Data and Task In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and presented to human judges for refinement into true and false paraphrases. 3,900 of the pairs were marked as having N max Θ i=1 −ClIOlI22 472 About 120 potential jurors were being asked to complete a lengthy questi</context>
<context position="24934" citStr="Dolan and Brockett, 2005" startWordPosition="4142" endWordPosition="4145">s, F measure (Turian et al., 2003), difference of sentence length, and proportion of dependency relation overlap. The SVM was trained to classify positive and negative examples of paraphrase using SVM&amp;quot;ght (Joachims, 1999).8 Metaparameters, tuned on the development data, were the regularization constant and the degree of the polynomial kernel (chosen in [10−5, 102] and 1–5 respectively.).9 It is unsurprising that the SVM performs very well on the MSRPC because of the corpus creation process (see Sec. 4) where an SVM was applied as well, with very similar features and a skewed decision process (Dolan and Brockett, 2005). 8http://svmlight.joachims.org 9Our replication of the Wan et al. model is approximate, because we used different preprocessing tools: MXPOST for POS tagging (Ratnaparkhi, 1996), MSTParser for parsing (McDonald et al., 2005), and Dan Bikel’s interface (http://www.cis.upenn.edu/˜dbikel/ software.html#wn) to WordNet (Miller, 1995) for lemmatization information. Tuning led to C = 17 and polynomial degree 4. 473 Table 2: Accuracy, p-class precision, and p-class recall on the test set (N = 1,725). See text for differences in implementation between Wan et al. and our replication; their reported sco</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proc. of IWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="19781" citStr="Dolan et al., 2004" startWordPosition="3288" endWordPosition="3291"> fixed using the treebank (see footnote 4) and the Gigaword corpus. Since there is a hidden variable (x), the objective function is non-convex. We locally optimize using the L-BFGS quasi-Newton method (Liu and Nocedal, 1989). Because many of our parameters are multinomial probabilities that are constrained to sum to one and L-BFGS is not designed to handle constraints, we treat these parameters as unnormalized weights that get renormalized (using a softmax function) before calculating the objective. 4 Data and Task In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and</context>
<context position="32475" citStr="Dolan et al., 2004" startWordPosition="5376" endWordPosition="5379">s to high precision, at the expense of recall. Oracle Ensembles Tab. 2 shows the results of three different oracle ensemble systems that correctly classify a pair if either of the two individual systems in the combination is correct. Note that the combinations involving pQ achieve 83%, the 11This accuracy is significant over pQ under a paired t-test (p &lt; 0.04), but is not significant over the SVM. human agreement level for the MSRPC. The LR and SVM are highly similar, and their oracle combination does not perform as well. 7 Related Work There is a growing body of research that uses the MSRPC (Dolan et al., 2004; Quirk et al., 2004) to build models of paraphrase. As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005). Qiu et al. (2006) used predicate-argument annotations. Most related to our approach, Wu (2005) used inversion transduction grammars—a synchronous context-free formalism (Wu, 1997)—for this task. Wu reported only positive-class (p) precision (not accuracy) on the test set. He obtained 76.1%, while our</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
</authors>
<title>Young Sook Hwang, and Eiichiro Sumita.</title>
<date>2005</date>
<booktitle>In Proc. of IWP.</booktitle>
<marker>Finch, 2005</marker>
<rawString>Andrew Finch, Young Sook Hwang, and Eiichiro Sumita. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Proc. of IWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third PASCAL recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proc. of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="28179" citStr="Giampiccolo et al., 2007" startWordPosition="4656" endWordPosition="4660">ssion It is quite promising that a linguistically-motivated probabilistic model comes so close to a stringsimilarity baseline, without incorporating stringlocal phrases. We see several reasons to prefer 10This is accomplished by eliminating lines 12 and 13 from the definition of pkid and redefining p.,,d to be the unigram word distribution estimated from the Gigaword corpus, as in Go, without the help of WordNet. the more intricate QG to the straightforward SVM. First, the QG discovers hidden alignments between words. Alignments have been leveraged in related tasks such as textual entailment (Giampiccolo et al., 2007); they make the model more interpretable in analyzing system output (e.g., Fig. 2). Second, the paraphrases of a sentence can be considered to be monolingual translations. We model the paraphrase problem using a direct machine translation model, thus providing a translation interpretation of the problem. This framework could be extended to permit paraphrase generation, or to exploit other linguistic annotations, such as representations of semantics (see, e.g., Qiu et al., 2006). Nonetheless, the usefulness of surface overlap features is difficult to ignore. We next provide an efficient way to </context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proc. of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium.</title>
<date>2003</date>
<contexts>
<context position="16149" citStr="Graff, 2003" startWordPosition="2698" endWordPosition="2699">llowing Wang et al. (2007),6 we employ a 14-feature loglinear model over all logically possible combinations of the 14 WordNet relations (Miller, 1995).7 Similarly to Eq. 14, we normalize this log-linear model based on the set of relations that are nonempty in WordNet for the word sx(i). Word Finally, the target word is randomly chosen from among the set of words that bear the lexical semantic relationship just chosen (line 13). This distribution is, again, defined log-linearly: Ew0:s�(i)Rw0 αw0 (15) Here αw is the Good-Turing unigram probability estimate of a word w from the Gigaword corpus (Graff, 2003). 3.5 Base Grammar G0 In addition to the QG that generates a second sentence bearing the desired relationship (paraphrase or not) to the first sentence s, our model in §2 also requires a base grammar G0 over s. We view this grammar as a trivial special case of the same QG model already described. G0 assumes the empty source sentence consists only of 6Note that Wang et al. (2007) designed pkid as an interpolation between a log-linear lexical semantics model and a word model. Our approach is more fully generative. 7These are: identical-word, synonym, antonym (including extended and indirect anto</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English Gigaword. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<pages>14--1771</pages>
<contexts>
<context position="697" citStr="Hinton, 2002" startWordPosition="94" endWordPosition="95">s and Noah A. Smith Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {dipanjan,nasmith}@cs.cmu.edu Abstract We present a novel approach to deciding whether two sentences hold a paraphrase relationship. We employ a generative model that generates a paraphrase of a given sentence, and we use probabilistic inference to reason about whether two sentences share the paraphrase relationship. The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006). Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on </context>
<context position="3206" citStr="Hinton, 2002" startWordPosition="468" endWordPosition="469">y the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model. We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model. This combined model leverages complementary strengths of the two approaches, outperforming a strong state-ofthe-art baseline (Wan et al., 2006). This paper is organized as follows. We introduce our probabilistic model in §2. The model makes use of three quasi-synchronous grammar models (Smith and Eisner, 2006, QG, hereafter) as components (one modeling paraphrase, one modeling not-paraphrase, and one a base grammar); these are detailed, along with latent-variable inference and discri</context>
<context position="29724" citStr="Hinton, 2002" startWordPosition="4903" endWordPosition="4904"> these features in a QG could be including these only at the root of the target tree, i.e. while calculating C(r, 0). These features have to be included in estimating pkid, which has log-linear component models (Eq. 7- 13). For these bigram or trigram overlap features, a similar log-linear model has to be normalized with a partition function, which considers the (unnormalized) scores of all possible target sentences, given the source sentence. We therefore combine pQ with a lexical overlap model that gives another posterior probability estimate pL(c |s1, s2) through a product of experts (PoE; Hinton, 2002), pJ(c |s1, s2) pQ(c |s1, s2) X pL(c |s1, s2)(21) EpQ(c&apos; |s1, s2) X pL(c&apos; |s1, s2) c&apos;E{p,n} 474 Eq. 21 takes the product of the two models’ posterior probabilities, then normalizes it to sum to one. PoE models are used to efficiently combine several expert models that individually constrain different dimensions in high-dimensional data, the product therefore constraining all of the dimensions. Combining models in this way grants to each expert component model the ability to “veto” a class by giving it low probability; the most probable class is the one that is least objectionable to all expert</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods -Support Vector Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="24530" citStr="Joachims, 1999" startWordPosition="4076" endWordPosition="4077"> report the best published accuracy, to our knowledge, on this task, using a support vector machine. Our baseline is a reimplementation of Wan et al. (2006), using features calculated directly from s1 and s2 without recourse to any hidden structure: proportion of word unigram matches, proportion of lemmatized unigram matches, BLEU score (Papineni et al., 2001), BLEU score on lemmatized tokens, F measure (Turian et al., 2003), difference of sentence length, and proportion of dependency relation overlap. The SVM was trained to classify positive and negative examples of paraphrase using SVM&amp;quot;ght (Joachims, 1999).8 Metaparameters, tuned on the development data, were the regularization constant and the degree of the polynomial kernel (chosen in [10−5, 102] and 1–5 respectively.).9 It is unsurprising that the SVM performs very well on the MSRPC because of the corpus creation process (see Sec. 4) where an SVM was applied as well, with very similar features and a skewed decision process (Dolan and Brockett, 2005). 8http://svmlight.joachims.org 9Our replication of the Wan et al. model is approximate, because we used different preprocessing tools: MXPOST for POS tagging (Ratnaparkhi, 1996), MSTParser for pa</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods -Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Programming (Ser. B),</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="19387" citStr="Liu and Nocedal, 1989" startWordPosition="3224" endWordPosition="3227">the dependency labels given the various configurations, the POS tags given POS tags, the NE tags given NE tags appearing in expressions 9–11, the configuration weights appearing in Eq. 14, and the weights of the various features in the log-linear model for the lexical-semantics model. As noted, the distributions pval, the word unigram weights in Eq. 15, and the parameters of the base grammar are fixed using the treebank (see footnote 4) and the Gigaword corpus. Since there is a hidden variable (x), the objective function is non-convex. We locally optimize using the L-BFGS quasi-Newton method (Liu and Nocedal, 1989). Because many of our parameters are multinomial probabilities that are constrained to sum to one and L-BFGS is not designed to handle constraints, we treat these parameters as unnormalized weights that get renormalized (using a softmax function) before calculating the objective. 4 Data and Task In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brocket</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Programming (Ser. B), 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Emiel Krahmer</author>
</authors>
<title>Explorations in sentence fusion.</title>
<date>2005</date>
<booktitle>In Proc. of EWNLG.</booktitle>
<contexts>
<context position="1488" citStr="Marsi and Krahmer, 2005" startWordPosition="205" endWordPosition="208">uishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence stru</context>
</contexts>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>Erwin Marsi and Emiel Krahmer. 2005. Explorations in sentence fusion. In Proc. of EWNLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10335" citStr="McDonald et al. (2005)" startWordPosition="1733" endWordPosition="1737">p and G„ generates the alignments x, the target tree τt, and the sentence t. Both Gp and G„ are structured in the same way, differing only in their parameters; henceforth we discuss Gp; G„ is similar. We assume that the parse trees of s and t are known.4 Therefore our model defines: pQ(t |Gp(s)) = p(τt |Gp(τs)) = Ex p(τt,x |Gp(τs)) (3) Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index in {1, ..., n}): P(τt,i |ti, x(i), τs) = pval(|λt(i)|,|ρt(i) ||ti) 4In our experiments, we use the parser described by McDonald et al. (2005), trained on sections 2–21 of the WSJ Penn Treebank, transformed to dependency trees following Yamada and Matsumoto (2003). (The same treebank data were also to estimate many of the parameters of our model, as discussed in the text.) Though it leads to a partial “pipeline” approximation of the posterior probability p(c I s, t), we believe that the relatively high quality of English dependency parsing makes this approximation reasonable. 11 X jEλt(i)Uρt(i) Xpkid(tj,τt` (j),x(j) |ti, x(i),τs) (4) where pval and pkid are valence and childproduction probabilities parameterized as discussed in §3.4</context>
<context position="25159" citStr="McDonald et al., 2005" startWordPosition="4174" endWordPosition="4177">parameters, tuned on the development data, were the regularization constant and the degree of the polynomial kernel (chosen in [10−5, 102] and 1–5 respectively.).9 It is unsurprising that the SVM performs very well on the MSRPC because of the corpus creation process (see Sec. 4) where an SVM was applied as well, with very similar features and a skewed decision process (Dolan and Brockett, 2005). 8http://svmlight.joachims.org 9Our replication of the Wan et al. model is approximate, because we used different preprocessing tools: MXPOST for POS tagging (Ratnaparkhi, 1996), MSTParser for parsing (McDonald et al., 2005), and Dan Bikel’s interface (http://www.cis.upenn.edu/˜dbikel/ software.html#wn) to WordNet (Miller, 1995) for lemmatization information. Tuning led to C = 17 and polynomial degree 4. 473 Table 2: Accuracy, p-class precision, and p-class recall on the test set (N = 1,725). See text for differences in implementation between Wan et al. and our replication; their reported score does not include the full test set. Model Accuracy Precision Recall baselines all p 66.49 66.49 100.00 Wan et al. SVM (reported) 75.63 77.00 90.00 Wan et al. SVM (replication) 75.42 76.88 90.14 lexical semantics features r</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Paraphrasing using given and new information in a question-answer system.</title>
<date>1979</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1096" citStr="McKeown, 1979" startWordPosition="149" endWordPosition="151"> paraphrase relationship. The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006). Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in sem</context>
</contexts>
<marker>McKeown, 1979</marker>
<rawString>Kathleen R. McKeown. 1979. Paraphrasing using given and new information in a question-answer system. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7904" citStr="Melamed, 2004" startWordPosition="1270" endWordPosition="1271">dent per production rule.2 Let s = (s1,..., sm) be the source sentence. The grammar rules will take one of the two forms: (t, l) —* (t, l)(t&apos;, k) or (t, l) —* (t&apos;, k)(t, l) where t and t&apos; range over the vocabulary of the target language, and l and k E 10,..., m} are indices in the source sentence, with 0 denoting null.3 Hard or soft constraints can be applied between l and k in a rule. These constraints imply permissible “configurations.” For example, requiring l =� 0 and, if k =� 0 then sk must be a child of sl in the source tree, we can implement a synchronous dependency grammar similar to (Melamed, 2004). Smith and Eisner (2006) used a quasisynchronous grammar to discover the correspondence between words implied by the correspondence between the trees. We follow Wang et al. (2007) in treating the correspondences as latent variables, and in using a WordNet-based lexical semantics model to generate the target words. 2Our actual model is more complicated; see §3.2. 3A more general QG could allow one-to-many alignments, replacing l and k with sets of indices. 469 3.2 Detailed Model We describe how we model pQ(t |Gp(s)) and pQ(t |G„(s)) for source and target sentences s and t (appearing in Eq. 2 a</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. Dan Melamed. 2004. Statistical machine translation by parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for English.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2788" citStr="Miller, 1995" startWordPosition="404" endWordPosition="405">del that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model. We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model. This combined model leverages complementary strengths of the two appro</context>
<context position="15688" citStr="Miller, 1995" startWordPosition="2622" endWordPosition="2623">r source-side word root x(j) = 0, x(i) is the root of s child-null x(i) = 0 parent-null x(j) = 0, x(i) is something other than root of s other catch-all for all other types of configurations, which are permitted Table 1: Permissible configurations. i is an index in t whose configuration is to be chosen; j = TP(i) is i’s parent. WordNet relation(s) The model next chooses a lexical semantics relation between sx(i) and the yet-to-be-chosen word ti (line 12). Following Wang et al. (2007),6 we employ a 14-feature loglinear model over all logically possible combinations of the 14 WordNet relations (Miller, 1995).7 Similarly to Eq. 14, we normalize this log-linear model based on the set of relations that are nonempty in WordNet for the word sx(i). Word Finally, the target word is randomly chosen from among the set of words that bear the lexical semantic relationship just chosen (line 13). This distribution is, again, defined log-linearly: Ew0:s�(i)Rw0 αw0 (15) Here αw is the Good-Turing unigram probability estimate of a word w from the Gigaword corpus (Graff, 2003). 3.5 Base Grammar G0 In addition to the QG that generates a second sentence bearing the desired relationship (paraphrase or not) to the fi</context>
<context position="25265" citStr="Miller, 1995" startWordPosition="4186" endWordPosition="4187"> (chosen in [10−5, 102] and 1–5 respectively.).9 It is unsurprising that the SVM performs very well on the MSRPC because of the corpus creation process (see Sec. 4) where an SVM was applied as well, with very similar features and a skewed decision process (Dolan and Brockett, 2005). 8http://svmlight.joachims.org 9Our replication of the Wan et al. model is approximate, because we used different preprocessing tools: MXPOST for POS tagging (Ratnaparkhi, 1996), MSTParser for parsing (McDonald et al., 2005), and Dan Bikel’s interface (http://www.cis.upenn.edu/˜dbikel/ software.html#wn) to WordNet (Miller, 1995) for lemmatization information. Tuning led to C = 17 and polynomial degree 4. 473 Table 2: Accuracy, p-class precision, and p-class recall on the test set (N = 1,725). See text for differences in implementation between Wan et al. and our replication; their reported score does not include the full test set. Model Accuracy Precision Recall baselines all p 66.49 66.49 100.00 Wan et al. SVM (reported) 75.63 77.00 90.00 Wan et al. SVM (replication) 75.42 76.88 90.14 lexical semantics features removed 68.64 68.84 96.51 pQ all features 73.33 74.48 91.10 c-command disallowed (best; see text) 73.86 74.</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for English. Commun. ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="24277" citStr="Papineni et al., 2001" startWordPosition="4034" endWordPosition="4037"> report accuracy on the official MSRPC test dataset. If the posterior probability pQ(p I s1, s2) is greater than 0.5, the pair is labeled “paraphrase” (as in Eq. 1). 5.1 Baseline We replicated a state-of-the-art baseline model for comparison. Wan et al. (2006) report the best published accuracy, to our knowledge, on this task, using a support vector machine. Our baseline is a reimplementation of Wan et al. (2006), using features calculated directly from s1 and s2 without recourse to any hidden structure: proportion of word unigram matches, proportion of lemmatized unigram matches, BLEU score (Papineni et al., 2001), BLEU score on lemmatized tokens, F measure (Turian et al., 2003), difference of sentence length, and proportion of dependency relation overlap. The SVM was trained to classify positive and negative examples of paraphrase using SVM&amp;quot;ght (Joachims, 1999).8 Metaparameters, tuned on the development data, were the regularization constant and the degree of the polynomial kernel (chosen in [10−5, 102] and 1–5 respectively.).9 It is unsurprising that the SVM performs very well on the MSRPC because of the corpus creation process (see Sec. 4) where an SVM was applied as well, with very similar features</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="28661" citStr="Qiu et al., 2006" startWordPosition="4732" endWordPosition="4735"> hidden alignments between words. Alignments have been leveraged in related tasks such as textual entailment (Giampiccolo et al., 2007); they make the model more interpretable in analyzing system output (e.g., Fig. 2). Second, the paraphrases of a sentence can be considered to be monolingual translations. We model the paraphrase problem using a direct machine translation model, thus providing a translation interpretation of the problem. This framework could be extended to permit paraphrase generation, or to exploit other linguistic annotations, such as representations of semantics (see, e.g., Qiu et al., 2006). Nonetheless, the usefulness of surface overlap features is difficult to ignore. We next provide an efficient way to combine a surface model with pQ. 6 Product of Experts Incorporating structural alignment and surface overlap features inside a single model can make exact inference infeasible. As an example, consider features like n-gram overlap percentages that provide cues of content overlap between two sentences. One intuitive way of including these features in a QG could be including these only at the root of the target tree, i.e. while calculating C(r, 0). These features have to be includ</context>
<context position="32791" citStr="Qiu et al. (2006)" startWordPosition="5429" endWordPosition="5432">cant over pQ under a paired t-test (p &lt; 0.04), but is not significant over the SVM. human agreement level for the MSRPC. The LR and SVM are highly similar, and their oracle combination does not perform as well. 7 Related Work There is a growing body of research that uses the MSRPC (Dolan et al., 2004; Quirk et al., 2004) to build models of paraphrase. As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005). Qiu et al. (2006) used predicate-argument annotations. Most related to our approach, Wu (2005) used inversion transduction grammars—a synchronous context-free formalism (Wu, 1997)—for this task. Wu reported only positive-class (p) precision (not accuracy) on the test set. He obtained 76.1%, while our PoE model achieves 79.6% on that measure. Wu’s model can be understood as a strict hierarchical maximum-alignment method. In contrast, our alignments are soft (we sum over them), and we do not require strictly isomorphic syntactic structures. Most importantly, our approach is founded on a stochastic generating pro</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William B Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="19802" citStr="Quirk et al., 2004" startWordPosition="3292" endWordPosition="3295">ebank (see footnote 4) and the Gigaword corpus. Since there is a hidden variable (x), the objective function is non-convex. We locally optimize using the L-BFGS quasi-Newton method (Liu and Nocedal, 1989). Because many of our parameters are multinomial probabilities that are constrained to sum to one and L-BFGS is not designed to handle constraints, we treat these parameters as unnormalized weights that get renormalized (using a softmax function) before calculating the objective. 4 Data and Task In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and presented to human j</context>
<context position="32496" citStr="Quirk et al., 2004" startWordPosition="5380" endWordPosition="5383"> at the expense of recall. Oracle Ensembles Tab. 2 shows the results of three different oracle ensemble systems that correctly classify a pair if either of the two individual systems in the combination is correct. Note that the combinations involving pQ achieve 83%, the 11This accuracy is significant over pQ under a paired t-test (p &lt; 0.04), but is not significant over the SVM. human agreement level for the MSRPC. The LR and SVM are highly similar, and their oracle combination does not perform as well. 7 Related Work There is a growing body of research that uses the MSRPC (Dolan et al., 2004; Quirk et al., 2004) to build models of paraphrase. As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005). Qiu et al. (2006) used predicate-argument annotations. Most related to our approach, Wu (2005) used inversion transduction grammars—a synchronous context-free formalism (Wu, 1997)—for this task. Wu reported only positive-class (p) precision (not accuracy) on the test set. He obtained 76.1%, while our PoE model achieves 7</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William B. Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="12545" citStr="Ratnaparkhi, 1996" startWordPosition="2116" endWordPosition="2117">ank (see footnote 4). For unobserved cases, the conditional probability is estimated by backing off to the parent POS tag and child direction. We discuss next how to parameterize the probability pkid that appears in Equations 4, 5, and 6. This conditional distribution forms the core of our QGs, and we deviate from earlier research using QGs in defining pkid in a fully generative way. In addition to assuming that dependency parse trees for s and t are observable, we also assume each word wi comes with POS and named entity tags. In our experiments these were obtained automatically using MXPOST (Ratnaparkhi, 1996) and BBN’s Identifinder (Bikel et al., 1999). m E P(τt,j |tj,x(j),τs) x(j)=0 470 For clarity, let j = τtp(i) and let l = x(j). pkid(ti, τt` (i), x(i) |tj, l, τs) = pconfig(config(ti, tj, sx(i), sl) |tj, l, τs) (7) Xpunif (x(i) |config(ti, tj, sx(i), sl)) (8) Xplab(τt` (i) |config(ti, tj, sx(i), sl)) (9) Xppos(Pos(ti) |Pos(sx(i))) (10) Xpne(ne(ti) |ne(sx(i))) (11) Xplsrel(lsrel(ti) |sx(i)) (12) Xpword(ti |lsrel(ti),sx(i)) (13) We consider each of the factors above in turn. Configuration In QG, “configurations” refer to the tree relationship among source-tree nodes (above, sl and sx(i)) aligned </context>
<context position="25112" citStr="Ratnaparkhi, 1996" startWordPosition="4169" endWordPosition="4170">hrase using SVM&amp;quot;ght (Joachims, 1999).8 Metaparameters, tuned on the development data, were the regularization constant and the degree of the polynomial kernel (chosen in [10−5, 102] and 1–5 respectively.).9 It is unsurprising that the SVM performs very well on the MSRPC because of the corpus creation process (see Sec. 4) where an SVM was applied as well, with very similar features and a skewed decision process (Dolan and Brockett, 2005). 8http://svmlight.joachims.org 9Our replication of the Wan et al. model is approximate, because we used different preprocessing tools: MXPOST for POS tagging (Ratnaparkhi, 1996), MSTParser for parsing (McDonald et al., 2005), and Dan Bikel’s interface (http://www.cis.upenn.edu/˜dbikel/ software.html#wn) to WordNet (Miller, 1995) for lemmatization information. Tuning led to C = 17 and polynomial degree 4. 473 Table 2: Accuracy, p-class precision, and p-class recall on the test set (N = 1,725). See text for differences in implementation between Wan et al. and our replication; their reported score does not include the full test set. Model Accuracy Precision Recall baselines all p 66.49 66.49 100.00 Wan et al. SVM (reported) 75.63 77.00 90.00 Wan et al. SVM (replication)</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proc. of the HLTNAACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="641" citStr="Smith and Eisner, 2006" startWordPosition="84" endWordPosition="87">ication as Probabilistic Quasi-Synchronous Recognition Dipanjan Das and Noah A. Smith Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {dipanjan,nasmith}@cs.cmu.edu Abstract We present a novel approach to deciding whether two sentences hold a paraphrase relationship. We employ a generative model that generates a paraphrase of a given sentence, and we use probabilistic inference to reason about whether two sentences share the paraphrase relationship. The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006). Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentence</context>
<context position="2480" citStr="Smith and Eisner (2006)" startWordPosition="353" endWordPosition="357">e best-published classification accuracy for the paraphrase identification task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence structure (or “alignment”) between the parts of two sentences. In this paper, we adopt a model that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity </context>
<context position="6422" citStr="Smith and Eisner (2006)" startWordPosition="1012" endWordPosition="1015">ses of sentence s. This latter model may seem counter-intuitive: since the vast majority of possible sentences are not paraphrases of s, why is a special grammar required? Our use of a Gn follows from the properties of the corpus currently used for learning, in which the negative examples 1Although we do not explore the idea here, the model could be adapted for other sentence-pair relationships like entailment or contradiction. were selected to have high lexical overlap. We return to this point in §4. 3 QG for Paraphrase Modeling Here, we turn to the models Gp and Gn in detail. 3.1 Background Smith and Eisner (2006) introduced the quasisynchronous grammar formalism. Here, we describe some of its salient aspects. The model arose out of the empirical observation that translated sentences have some isomorphic syntactic structure, but divergences are possible. Therefore, rather than an isomorphic structure over a pair of source and target sentences, the syntactic tree over a target sentence is modeled by a source sentencespecific grammar “inspired” by the source sentence’s tree. This is implemented by associating with each node in the target tree a subset of the nodes in the source tree. Since it loosely lin</context>
<context position="7929" citStr="Smith and Eisner (2006)" startWordPosition="1272" endWordPosition="1275">ion rule.2 Let s = (s1,..., sm) be the source sentence. The grammar rules will take one of the two forms: (t, l) —* (t, l)(t&apos;, k) or (t, l) —* (t&apos;, k)(t, l) where t and t&apos; range over the vocabulary of the target language, and l and k E 10,..., m} are indices in the source sentence, with 0 denoting null.3 Hard or soft constraints can be applied between l and k in a rule. These constraints imply permissible “configurations.” For example, requiring l =� 0 and, if k =� 0 then sk must be a child of sl in the source tree, we can implement a synchronous dependency grammar similar to (Melamed, 2004). Smith and Eisner (2006) used a quasisynchronous grammar to discover the correspondence between words implied by the correspondence between the trees. We follow Wang et al. (2007) in treating the correspondences as latent variables, and in using a WordNet-based lexical semantics model to generate the target words. 2Our actual model is more complicated; see §3.2. 3A more general QG could allow one-to-many alignments, replacing l and k with sets of indices. 469 3.2 Detailed Model We describe how we model pQ(t |Gp(s)) and pQ(t |G„(s)) for source and target sentences s and t (appearing in Eq. 2 alternately as s1 and s2).</context>
<context position="13587" citStr="Smith and Eisner, 2006" startWordPosition="2281" endWordPosition="2284">(i)) (13) We consider each of the factors above in turn. Configuration In QG, “configurations” refer to the tree relationship among source-tree nodes (above, sl and sx(i)) aligned to a pair of parentchild target-tree nodes (above, tj and ti). In deriving τt,j, the model first chooses the configuration that will hold among ti, tj, sx(i) (which has yet to be chosen), and sl (line 7). This is defined for configuration c log-linearly by:5 pconfig(c |tj, l, τs) = αc0 c0:�1sk,config(ti,tj,sk,sl)=c0 (14) Permissible configurations in our model are shown in Table 1. These are identical to prior work (Smith and Eisner, 2006; Wang et al., 2007), except that we add a “root” configuration that aligns the target parent-child pair to null and the head word of the source sentence, respectively. Using many permissible configurations helps remove negative effects from noisy parses, which our learner treats as evidence. Fig. 1 shows some examples of major configurations that Gp discovers in the data. Source tree alignment After choosing the configuration, the specific node in τs that ti will align to, sx(i) is drawn uniformly (line 8) from among those in the configuration selected. Dependency label, POS, and named entity</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies. In Proc. of the HLTNAACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph P Turian</author>
<author>Luke Shen</author>
<author>I Dan Melamed</author>
</authors>
<title>Evaluation of machine translation and its evaluation.</title>
<date>2003</date>
<booktitle>In Proc. of Machine Translation Summit IX.</booktitle>
<contexts>
<context position="24343" citStr="Turian et al., 2003" startWordPosition="4045" endWordPosition="4048">r probability pQ(p I s1, s2) is greater than 0.5, the pair is labeled “paraphrase” (as in Eq. 1). 5.1 Baseline We replicated a state-of-the-art baseline model for comparison. Wan et al. (2006) report the best published accuracy, to our knowledge, on this task, using a support vector machine. Our baseline is a reimplementation of Wan et al. (2006), using features calculated directly from s1 and s2 without recourse to any hidden structure: proportion of word unigram matches, proportion of lemmatized unigram matches, BLEU score (Papineni et al., 2001), BLEU score on lemmatized tokens, F measure (Turian et al., 2003), difference of sentence length, and proportion of dependency relation overlap. The SVM was trained to classify positive and negative examples of paraphrase using SVM&amp;quot;ght (Joachims, 1999).8 Metaparameters, tuned on the development data, were the regularization constant and the degree of the polynomial kernel (chosen in [10−5, 102] and 1–5 respectively.).9 It is unsurprising that the SVM performs very well on the MSRPC because of the corpus creation process (see Sec. 4) where an SVM was applied as well, with very similar features and a skewed decision process (Dolan and Brockett, 2005). 8http:/</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Evaluation of machine translation and its evaluation. In Proc. of Machine Translation Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Using dependency-based features to take the “para-farce” out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proc. of ALTW.</booktitle>
<contexts>
<context position="1999" citStr="Wan et al., 2006" startWordPosition="277" endWordPosition="280"> overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence structure (or “alignment”) between the parts of two sentences. In this paper, we adopt a model that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the </context>
<context position="3461" citStr="Wan et al., 2006" startWordPosition="503" endWordPosition="506"> generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model. We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model. This combined model leverages complementary strengths of the two approaches, outperforming a strong state-ofthe-art baseline (Wan et al., 2006). This paper is organized as follows. We introduce our probabilistic model in §2. The model makes use of three quasi-synchronous grammar models (Smith and Eisner, 2006, QG, hereafter) as components (one modeling paraphrase, one modeling not-paraphrase, and one a base grammar); these are detailed, along with latent-variable inference and discriminative training algorithms, in §3. We discuss the Microsoft Research Paraphrase Corpus, upon which we conduct experiments, in §4. In §5, we present experiments on paraphrase 468 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the</context>
<context position="23915" citStr="Wan et al. (2006)" startWordPosition="3975" endWordPosition="3978">t deceptively similar on the surface.” It is for this reason that we use a special QG for the n relation. 5 Experimental Evaluation Here we present our experimental evaluation using pQ. We trained on the training set (3,001 pairs) and tuned model metaparameters (C in Eq. 17) and the effect of different feature sets on the development set (1,075 pairs). We report accuracy on the official MSRPC test dataset. If the posterior probability pQ(p I s1, s2) is greater than 0.5, the pair is labeled “paraphrase” (as in Eq. 1). 5.1 Baseline We replicated a state-of-the-art baseline model for comparison. Wan et al. (2006) report the best published accuracy, to our knowledge, on this task, using a support vector machine. Our baseline is a reimplementation of Wan et al. (2006), using features calculated directly from s1 and s2 without recourse to any hidden structure: proportion of word unigram matches, proportion of lemmatized unigram matches, BLEU score (Papineni et al., 2001), BLEU score on lemmatized tokens, F measure (Turian et al., 2003), difference of sentence length, and proportion of dependency relation overlap. The SVM was trained to classify positive and negative examples of paraphrase using SVM&amp;quot;ght (</context>
<context position="32744" citStr="Wan et al., 2006" startWordPosition="5421" endWordPosition="5424">pQ achieve 83%, the 11This accuracy is significant over pQ under a paired t-test (p &lt; 0.04), but is not significant over the SVM. human agreement level for the MSRPC. The LR and SVM are highly similar, and their oracle combination does not perform as well. 7 Related Work There is a growing body of research that uses the MSRPC (Dolan et al., 2004; Quirk et al., 2004) to build models of paraphrase. As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005). Qiu et al. (2006) used predicate-argument annotations. Most related to our approach, Wu (2005) used inversion transduction grammars—a synchronous context-free formalism (Wu, 1997)—for this task. Wu reported only positive-class (p) precision (not accuracy) on the test set. He obtained 76.1%, while our PoE model achieves 79.6% on that measure. Wu’s model can be understood as a strict hierarchical maximum-alignment method. In contrast, our alignments are soft (we sum over them), and we do not require strictly isomorphic syntactic structures. Most importantly, our app</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2006. Using dependency-based features to take the “para-farce” out of paraphrase. In Proc. of ALTW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasisynchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL.</booktitle>
<contexts>
<context position="2981" citStr="Wang et al. (2007)" startWordPosition="432" endWordPosition="435">sely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model. We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model. This combined model leverages complementary strengths of the two approaches, outperforming a strong state-ofthe-art baseline (Wan et al., 2006). This paper is organized as follows. We introduce our probabilistic model in §2. The model makes use of three quasi-syn</context>
<context position="7191" citStr="Wang et al., 2007" startWordPosition="1136" endWordPosition="1139">t translated sentences have some isomorphic syntactic structure, but divergences are possible. Therefore, rather than an isomorphic structure over a pair of source and target sentences, the syntactic tree over a target sentence is modeled by a source sentencespecific grammar “inspired” by the source sentence’s tree. This is implemented by associating with each node in the target tree a subset of the nodes in the source tree. Since it loosely links the two sentences’ syntactic structures, QG is well suited for problems like word alignment for MT (Smith and Eisner, 2006) and question answering (Wang et al., 2007). Consider a very simple quasi-synchronous context-free dependency grammar that generates one dependent per production rule.2 Let s = (s1,..., sm) be the source sentence. The grammar rules will take one of the two forms: (t, l) —* (t, l)(t&apos;, k) or (t, l) —* (t&apos;, k)(t, l) where t and t&apos; range over the vocabulary of the target language, and l and k E 10,..., m} are indices in the source sentence, with 0 denoting null.3 Hard or soft constraints can be applied between l and k in a rule. These constraints imply permissible “configurations.” For example, requiring l =� 0 and, if k =� 0 then sk must </context>
<context position="13607" citStr="Wang et al., 2007" startWordPosition="2285" endWordPosition="2288">ch of the factors above in turn. Configuration In QG, “configurations” refer to the tree relationship among source-tree nodes (above, sl and sx(i)) aligned to a pair of parentchild target-tree nodes (above, tj and ti). In deriving τt,j, the model first chooses the configuration that will hold among ti, tj, sx(i) (which has yet to be chosen), and sl (line 7). This is defined for configuration c log-linearly by:5 pconfig(c |tj, l, τs) = αc0 c0:�1sk,config(ti,tj,sk,sl)=c0 (14) Permissible configurations in our model are shown in Table 1. These are identical to prior work (Smith and Eisner, 2006; Wang et al., 2007), except that we add a “root” configuration that aligns the target parent-child pair to null and the head word of the source sentence, respectively. Using many permissible configurations helps remove negative effects from noisy parses, which our learner treats as evidence. Fig. 1 shows some examples of major configurations that Gp discovers in the data. Source tree alignment After choosing the configuration, the specific node in τs that ti will align to, sx(i) is drawn uniformly (line 8) from among those in the configuration selected. Dependency label, POS, and named entity class The newly gen</context>
<context position="15563" citStr="Wang et al. (2007)" startWordPosition="2600" endWordPosition="2603">gs TP(x(i)) = TP(x(j)), x(i) =� x(j) same-node x(i) = x(j) c-command the parent of one source-side word is an ancestor of the other source-side word root x(j) = 0, x(i) is the root of s child-null x(i) = 0 parent-null x(j) = 0, x(i) is something other than root of s other catch-all for all other types of configurations, which are permitted Table 1: Permissible configurations. i is an index in t whose configuration is to be chosen; j = TP(i) is i’s parent. WordNet relation(s) The model next chooses a lexical semantics relation between sx(i) and the yet-to-be-chosen word ti (line 12). Following Wang et al. (2007),6 we employ a 14-feature loglinear model over all logically possible combinations of the 14 WordNet relations (Miller, 1995).7 Similarly to Eq. 14, we normalize this log-linear model based on the set of relations that are nonempty in WordNet for the word sx(i). Word Finally, the target word is randomly chosen from among the set of words that bear the lexical semantic relationship just chosen (line 13). This distribution is, again, defined log-linearly: Ew0:s�(i)Rw0 αw0 (15) Here αw is the Good-Turing unigram probability estimate of a word w from the Gigaword corpus (Graff, 2003). 3.5 Base Gra</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? a quasisynchronous grammar for QA. In Proc. of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="32953" citStr="Wu, 1997" startWordPosition="5450" endWordPosition="5451">le combination does not perform as well. 7 Related Work There is a growing body of research that uses the MSRPC (Dolan et al., 2004; Quirk et al., 2004) to build models of paraphrase. As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005). Qiu et al. (2006) used predicate-argument annotations. Most related to our approach, Wu (2005) used inversion transduction grammars—a synchronous context-free formalism (Wu, 1997)—for this task. Wu reported only positive-class (p) precision (not accuracy) on the test set. He obtained 76.1%, while our PoE model achieves 79.6% on that measure. Wu’s model can be understood as a strict hierarchical maximum-alignment method. In contrast, our alignments are soft (we sum over them), and we do not require strictly isomorphic syntactic structures. Most importantly, our approach is founded on a stochastic generating process and estimated discriminatively for this task, while Wu did not estimate any parameters from data at all. 8 Conclusion In this paper, we have presented a prob</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comput. Linguist., 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Recognizing paraphrases and textual entailment using inversion transduction grammars.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</booktitle>
<contexts>
<context position="32868" citStr="Wu (2005)" startWordPosition="5441" endWordPosition="5442">uman agreement level for the MSRPC. The LR and SVM are highly similar, and their oracle combination does not perform as well. 7 Related Work There is a growing body of research that uses the MSRPC (Dolan et al., 2004; Quirk et al., 2004) to build models of paraphrase. As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005). Qiu et al. (2006) used predicate-argument annotations. Most related to our approach, Wu (2005) used inversion transduction grammars—a synchronous context-free formalism (Wu, 1997)—for this task. Wu reported only positive-class (p) precision (not accuracy) on the test set. He obtained 76.1%, while our PoE model achieves 79.6% on that measure. Wu’s model can be understood as a strict hierarchical maximum-alignment method. In contrast, our alignments are soft (we sum over them), and we do not require strictly isomorphic syntactic structures. Most importantly, our approach is founded on a stochastic generating process and estimated discriminatively for this task, while Wu did not estimate </context>
</contexts>
<marker>Wu, 2005</marker>
<rawString>Dekai Wu. 2005. Recognizing paraphrases and textual entailment using inversion transduction grammars. In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="10457" citStr="Yamada and Matsumoto (2003)" startWordPosition="1752" endWordPosition="1755">e way, differing only in their parameters; henceforth we discuss Gp; G„ is similar. We assume that the parse trees of s and t are known.4 Therefore our model defines: pQ(t |Gp(s)) = p(τt |Gp(τs)) = Ex p(τt,x |Gp(τs)) (3) Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index in {1, ..., n}): P(τt,i |ti, x(i), τs) = pval(|λt(i)|,|ρt(i) ||ti) 4In our experiments, we use the parser described by McDonald et al. (2005), trained on sections 2–21 of the WSJ Penn Treebank, transformed to dependency trees following Yamada and Matsumoto (2003). (The same treebank data were also to estimate many of the parameters of our model, as discussed in the text.) Though it leads to a partial “pipeline” approximation of the posterior probability p(c I s, t), we believe that the relatively high quality of English dependency parsing makes this approximation reasonable. 11 X jEλt(i)Uρt(i) Xpkid(tj,τt` (j),x(j) |ti, x(i),τs) (4) where pval and pkid are valence and childproduction probabilities parameterized as discussed in §3.4. Note the recursion in the secondto-last line. We next describe a dynamic programming solution for calculating p(τt |Gp(τ</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yitao Zhang</author>
<author>Jon Patrick</author>
</authors>
<title>Paraphrase identification by text canonicalization.</title>
<date>2005</date>
<booktitle>In Proc. of ALTW.</booktitle>
<contexts>
<context position="1961" citStr="Zhang and Patrick, 2005" startWordPosition="269" endWordPosition="272">active summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence structure (or “alignment”) between the parts of two sentences. In this paper, we adopt a model that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some s</context>
<context position="32611" citStr="Zhang and Patrick, 2005" startWordPosition="5399" endWordPosition="5402">s that correctly classify a pair if either of the two individual systems in the combination is correct. Note that the combinations involving pQ achieve 83%, the 11This accuracy is significant over pQ under a paired t-test (p &lt; 0.04), but is not significant over the SVM. human agreement level for the MSRPC. The LR and SVM are highly similar, and their oracle combination does not perform as well. 7 Related Work There is a growing body of research that uses the MSRPC (Dolan et al., 2004; Quirk et al., 2004) to build models of paraphrase. As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005). Qiu et al. (2006) used predicate-argument annotations. Most related to our approach, Wu (2005) used inversion transduction grammars—a synchronous context-free formalism (Wu, 1997)—for this task. Wu reported only positive-class (p) precision (not accuracy) on the test set. He obtained 76.1%, while our PoE model achieves 79.6% on that measure. Wu’s model can be understood as a strict hierarchical maximum-alignment method. In contrast, </context>
</contexts>
<marker>Zhang, Patrick, 2005</marker>
<rawString>Yitao Zhang and Jon Patrick. 2005. Paraphrase identification by text canonicalization. In Proc. of ALTW.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>