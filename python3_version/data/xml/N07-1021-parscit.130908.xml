<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000363">
<title confidence="0.988284">
Probabilistic Generation of Weather Forecast Texts
</title>
<author confidence="0.989552">
Anja Belz
</author>
<affiliation confidence="0.9875055">
Natural Language Technology Group
University of Brighton, UK
</affiliation>
<email confidence="0.996759">
a.s.belz@brighton.ac.uk
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964944444444">
This paper reports experiments in
which pCRU — a generation framework
that combines probabilistic generation
methodology with a comprehensive
model of the generation space — is
used to semi-automatically create sev-
eral versions of a weather forecast text
generator. The generators are evaluated
in terms of output quality, development
time and computational efficiency against
(i) human forecasters, (ii) a traditional
handcrafted pipelined NLG system, and
(iii) a HALOGEN-style statistical genera-
tor. The most striking result is that despite
acquiring all decision-making abilities
automatically, the best pCRU generators
receive higher scores from human judges
than forecasts written by experts.
</bodyText>
<sectionHeader confidence="0.982384" genericHeader="categories and subject descriptors">
1 Introduction and background
</sectionHeader>
<bodyText confidence="0.999953142857143">
Over the last decade, there has been a lot of in-
terest in statistical techniques among researchers in
natural language generation (NLG), a field that was
largely unaffected by the statistical revolution in
NLP that started in the 1980s. Since Langkilde and
Knight’s influential work on statistical surface real-
isation (Knight and Langkilde, 1998), a number of
statistical and corpus-based methods have been re-
ported. However, this interest does not appear to
have translated into practice: of the 30 implemented
systems and modules with development starting in
or after 2000 that are listed on a key NLG website1,
only five have any statistical component at all (an-
other six involve techniques that are in some way
corpus-based). The likely reasons for this lack of
take-up are that (i) many existing statistical NLG
techniques are inherently expensive, requiring the
set of alternatives to be generated in full before the
statistical model is applied to select the most likely;
and (ii) statistical NLG techniques have not been
shown to produce outputs of high enough quality.
There has also been a rethinking of the traditional
modular NLG architecture (Reiter, 1994). Some re-
search has moved towards a more comprehensive
view, e.g. construing the generation task as a single
constraint satisfaction problem. Precursors to cur-
rent approaches were Hovy’s PAULINE which kept
track of the satisfaction status of global ‘rhetori-
cal goals’ (Hovy, 1988), and Power et al.’s ICON-
OCLAST which allowed users to fine-tune different
combinations of global constraints (Power, 2000).
In recent comprehensive approaches, the focus is on
automatic adaptability, e.g. automatically determin-
ing degrees of constraint violability on the basis of
corpus frequencies. Examples include Langkilde’s
(2005) general approach to generation and parsing
based on constraint optimisation, and Marciniak and
Strube’s (2005) integrated, globally optimisable net-
work of classifiers and constraints.
Both probabilistic and recent comprehensive
trends have developed at least in part to address two
interrelated issues in NLG: the considerable amount
</bodyText>
<footnote confidence="0.988162666666667">
1Bateman and Zock’s list of NLG systems,
http://www.fb10.uni-bremen.de/anglistik/
langpro/NLG-table/, 20/01/2006.
</footnote>
<page confidence="0.932239">
164
</page>
<note confidence="0.801923">
Proceedings of NAACL HLT 2007, pages 164–171,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999975230769231">
of time and expense involved in building new sys-
tems, and the almost complete lack in the field of
reusable systems and modules. Both trends have
the potential to improve on development time and
reusability, but have drawbacks.
Existing statistical NLG (i) uses corpus statistics to
inform heuristic decisions in what is otherwise sym-
bolic generation (Varges and Mellish, 2001; White,
2004; Paiva and Evans, 2005); (ii) applies n-gram
models to select the overall most likely realisation
after generation (HALOGEN family); or (iii) reuses
an existing parsing grammar or treebank for surface
realisation (Velldal et al., 2004; Cahill and van Gen-
abith, 2006). N-gram models are not linguistically
informed, (i) and (iii) come with a substantial man-
ual overhead, and (ii) overgenerates vastly and has a
high computational cost (see also Section 3).
Existing comprehensive approaches tend to in-
cur a manual overhead (finetuning in ICONOCLAST,
corpus annotation in Langkilde and Marciniak &amp;
Strube). Handling violability of soft constraints is
problematic, and converting corpus-derived prob-
abilities into costs associated with constraints
(Langkilde, Marciniak &amp; Strube) turns straightfor-
ward statistics into an ad hoc search heuristic. Older
approaches are not globally optimisable (PAULINE)
or involve exhaustive search (ICONOCLAST).
The pCRU language generation framework com-
bines a probabilistic generation methodology with
a comprehensive model of the generation space,
where probabilistic choice informs generation as it
goes along, instead of after all alternatives have been
generated. pCRU uses existing techniques (Belz,
2005), but extends these substantially. This paper
describes the pCRU framework and reports experi-
ments designed to rigorously test pCRU in practice
and to determine whether improvements in develop-
ment time and reusability can be achieved without
sacrificing quality of outputs.
</bodyText>
<sectionHeader confidence="0.814341" genericHeader="method">
2 pCRU language generation
</sectionHeader>
<bodyText confidence="0.999728032258064">
pCRU (Belz, 2006) is a probabilistic language gen-
eration framework that was developed with the aim
of providing the formal underpinnings for creating
NLG systems that are driven by comprehensive prob-
abilistic models of the entire generation space (in-
cluding deep generation). NLG systems tend to be
composed of generation rules that apply transforma-
tions to representations (performing different tasks
in different modules). The basic idea in pCRU is
that as long as the generation rules are all of the
form relation(argl, ...argn) → relationl(argl, ...argp)
... relationm(arg1, ...argq), m &gt; 1, n, p, q &gt; 0, then the
set of all generation rules can be seen as defining
a context-free language and a single probabilistic
model can be estimated from raw or annotated text
to guide generation processes.
pCRU uses straightforward context-free technol-
ogy in combination with underspecification tech-
niques, to encode a base generator as a set of ex-
pansion rules G composed of n-ary relations with
variable and constant arguments (Section 2.1). In
non-probabilistic mode, the output is the set of fully
expanded (fully specified) forms that can be de-
rived from the input. The pCRU (probabilistic CRU)
decision-maker is created by estimating a proba-
bility distribution over the base generator from an
unannotated corpus of example texts. This distri-
bution is used in one of several ways to drive gen-
eration processes, maximising the likelihood either
of individual expansions or of entire generation pro-
cesses (Section 2.2).
</bodyText>
<subsectionHeader confidence="0.99802">
2.1 Specifying the range of alternatives
</subsectionHeader>
<bodyText confidence="0.999947285714286">
Using context-free representational underspecifica-
tion, or CRU, (Belz, 2004), the generation space is
encoded as (i) a set G of expansion rules composed
of n-ary relations relation(arg1, ...argn) where the
argi are constants or variables over constants; and
(ii) argument and relation type hierarchies. Any sen-
tential form licensed by G can be the input to the
generation process which expands it under unify-
ing variable substitution until no further expansion is
possible. The output (in non-probabilistic mode) is
the set of fully expanded forms (i.e. consisting only
of terminals) that can be derived from the input.
The rules in G define the steps in which inputs can
be incrementally specified from, say, content to se-
mantic, syntactic and finally surface representations.
G therefore defines specificity relations between all
sentential forms, i.e. defines which representation is
underspecified with respect to which other represen-
tations. The generation process is construed explic-
itly as the task of incrementally specifying one or
more word strings.
</bodyText>
<page confidence="0.998027">
165
</page>
<bodyText confidence="0.999907">
Within the limits of context-freeness and atom-
icity of feature values, CRU is neutral with respect
to actual linguistic knowledge representation for-
malisms used to encode generation spaces. The
main motivation for a context-free formalism is
the advantage of low computational cost, while the
inclusion of arguments on (non)terminals permits
keeping track of contextual features.
</bodyText>
<subsectionHeader confidence="0.999938">
2.2 Selection among alternatives
</subsectionHeader>
<bodyText confidence="0.972850365853659">
The pCRU decision-making component is created by
estimating a probability distribution over the set of
expansion rules that encodes the generation space
(the base generator), as follows:
1 Convert corpus into multi-treebank: determine
for each sentence all (left-most) derivation trees
licensed by the base generator’s CRU rules, us-
ing maximal partial derivations if there is no com-
plete derivation tree; annotate the (sub)strings in
the sentence with the derivation trees, resulting in
a set of generation trees for the sentence.
2 Train base generator: Obtain frequency counts
for each individual generation rule from the multi-
treebank, adding 1/n to the count for every rule,
where n is the number of alternative derivation
trees; convert counts into probability distributions
over alternative rules, using add-1 smoothing and
standard maximum likelihood estimation.
The resulting probability distribution is used in
one of the following three ways to control gener-
ation. Of these, only the first requires the genera-
tion forest to be created in full, whereas both greedy
modes prune the generation space to a single path:
1 Viterbi generation: do a Viterbi search of the gen-
eration forest for a given input, which maximises
the joint likelihood of all decisions taken in the
generation process. This selects the most likely
generation process, but is considerably more ex-
pensive than the greedy modes.
2 Greedy generation: make the single most likely
decision at each choice point (rule expansion) in
a generation process. This is not guaranteed to
result in the most likely generation process, but
the computational cost is very low.
3 Greedy roulette-wheel generation: use a non-
uniform random distribution proportional to the
likelihoods of alternatives. E.g. if there are two
alternative decisions D1 and D2, with the model
giving p(D1) = 0.8 and p(D2) = 0.2, then the
proportion of times the generator decides D1 ap-
proaches 80% and D2 20% in the limit.
</bodyText>
<subsectionHeader confidence="0.938152">
2.3 ThepCRU-1.0 generation package
</subsectionHeader>
<bodyText confidence="0.99986375">
The technology described in the two preceding sec-
tions has been implemented in the pCRU-1.0 soft-
ware package. The user defines a generation space
by creating a base generator composed of:
</bodyText>
<listItem confidence="0.9901095">
1. the set N of underspecified n-ary relations
2. the set W of fully specified n-ary relations
3. a set R of context-free generation rules n → α,
n E N, α E (W U N)∗
4. a typed feature hierarchy defining argument
types and values
</listItem>
<bodyText confidence="0.994081">
This base generator is then trained (as described
above) on raw text corpora to provide a probability
distribution over generation rules. Optionally, an n-
gram language model can also be created from the
same corpus. The generator is then run in one of the
three modes above or one of the following:
</bodyText>
<listItem confidence="0.9171028">
1. Random: ignoring pCRU probabilities, ran-
domly select generation rules.
2. N-gram: ignoring pCRU probabilities, gener-
ate set of alternatives and select the most likely
according to the n-gram language model.
</listItem>
<bodyText confidence="0.998755666666667">
The random mode serves as a baseline for gen-
eration quality: a trained generator must be able to
do better, otherwise all the work is done by the base
generator (and none by the probabilities). The n-
gram mode works exactly like HALOGEN-style gen-
eration: the generator generates all realisations that
the rules allow and then picks one based on the n-
gram model. This is a point of comparison with
existing statistical NLG techniques and also serves
as a baseline in terms of computational expense: a
generator using pCRU probabilities should be able
to produce realisations faster.
</bodyText>
<sectionHeader confidence="0.5807515" genericHeader="method">
3 Building and evaluatingpCRU wind
forecast text generators
</sectionHeader>
<bodyText confidence="0.999946666666667">
The automatic generation of weather forecasts is
one of the success stories of NLP. The restrictive-
ness of the sublanguage has made the domain of
</bodyText>
<page confidence="0.992">
166
</page>
<table confidence="0.652895529411765">
Oil1/Oil2/Oil3 _ FIELDS
05-10-00
05/06 SSW 18 22 27 3.0 4.8 SSW 2.59
05/09 S 16 20 25 2.7 4.3 SSW 2.39
05/12 S 14 17 21 2.5 4.0 SSW 2.29
05/15 S 14 17 21 2.3 3.7 SSW 2.28
...
FORECAST FOR:-
Oil1/Oil2/Oil3 FIELDS
...
2.FORECAST 06-24 GMT, THURSDAY, 05-Oct 2000
= = ===WARNINGS: RISK THUNDERSTORM. =======
WIND(KTS) CONFIDENCE: HIGH
10M: SSW 16-20 GRADUALLY BACKING SSE
THEN FALLING VARIABLE 04-08 BY
LATE EVENING
...
</table>
<figureCaption confidence="0.951926">
Figure 1: Meteorological data file and wind forecast
for 05-10-2000, a.m. (oil fields anonymised).
</figureCaption>
<bodyText confidence="0.9996005">
weather forecasting particularly attractive to NLG re-
searchers, and a number of weather forecast genera-
tion systems have been created.
A recent example of weather forecast text gener-
ation is the SUMTIME project (Reiter et al., 2005)
which developed a commercially used NLG system
that generates marine weather forecasts for offshore
oil rigs from numerical forecast data produced by
weather simulation programs. The SUMTIME cor-
pus is used in the experiments below.
</bodyText>
<subsectionHeader confidence="0.988013">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999968466666667">
Each instance in the SUMTIME corpus consists of
three numerical data files (the outputs of weather
simulators) and the forecast file written by the fore-
caster on the basis of the data (Figure 1 shows an
example). The experiments below focused on a.m.
forecasts of wind characteristics. Content determi-
nation (deciding which meteorological data to in-
clude in a forecast) was carried out off-line.
The corpus consists of 2,123 instances (22,985
words) of which half are a.m. forecasts. This may
not seem much, but considering the small number of
vocabulary items and syntactic structures, the cor-
pus provides extremely good coverage (an initial im-
pression confirmed by the small differences between
training and testing data results below).
</bodyText>
<subsectionHeader confidence="0.998534">
3.2 The base generator
</subsectionHeader>
<bodyText confidence="0.999796">
The base generator2 was written semi-auto-
matically in two steps. First, a simple chunker
was run over the corpus to split wind statements
</bodyText>
<footnote confidence="0.634592">
2For a fragment of the rule set, see Belz (2006).
</footnote>
<bodyText confidence="0.999965454545455">
into wind direction, wind speed, gust speed,
gust statements, time expressions, verb phrases,
pre-modifiers, and post-modifiers. Preterminal
generation rules were automatically created from
the resulting chunks. Then, higher-level rules which
combine chunks into larger components, taking
care of text structuring, aggregation and elision,
were manually authored. The top-level generation
rules interpret wind statements as sequences of
independent units of information, ensuring a linear
increase in complexity with increasing input length.
Inputs encode meteorological data (as shown in Ta-
ble 1), and were pre-processed to determine certain
types of information, including whether a change
in wind direction was clockwise or anti-clockwise,
and whether change in wind speed was an increase
or a decrease. The final generator takes as inputs
number vectors of length 7 to 60, and generates up
to 1.6 × 1031 alternative realisations for an input.
The job of the base generator is to describe the
textual variety found in the corpus. It makes no deci-
sions about when to prefer one variant over another.
</bodyText>
<subsectionHeader confidence="0.993566">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.9999966875">
The corpus was divided at random into 90% train-
ing data and 10% testing data. The training set
was multi-treebanked with the base generator and
the multi-treebank then used to create the probabil-
ity distribution for the base generator (as described
in Section 2.2). A back-off 2-gram model with
Good-Turing discounting and no lexical classes was
also created from the training set, using the SRILM
toolkit, (Stolcke, 2002). pCRU-1.0 was then run in
all five modes to generate forecasts for the inputs in
both training and test sets.
This procedure was repeated five times for hold-
out cross-validation. The small amount of variation
across the five repeats, and the small differences be-
tween results for training and test sets (Table 2) in-
dicated that five repeats were sufficient.
</bodyText>
<sectionHeader confidence="0.97774" genericHeader="method">
3.4 Evaluation
3.4.1 Evaluation methods
</sectionHeader>
<bodyText confidence="0.9995234">
The two automatic metrics used in the evalua-
tions, NIST and BLEU have been shown to correlate
highly with expert judgments (Pearson correlation
coefficients 0.82 and 0.79 respectively) in this do-
main (Belz and Reiter, 2006).
</bodyText>
<page confidence="0.987528">
167
</page>
<table confidence="0.9876098">
Input [[1,SSW,16,20,-,-,0600],[2,SSE,-,-,-,-,NOTIME],[3,VAR,04,08,-,-,2400]]
Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENING
Reference 1 SSW’LY 16-20 GRADUALLY BACKING SSE’LY THEN DECREASING VARIABLE 4-8 BY LATE EVENING
Reference 2 SSW 16-20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE 4-8 BY LATE EVENING
SUMTIME-Hyb. SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT
pCRU-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING
pCRU-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8
pCRU-viterbi SSW 16-20 BACKING SSE VARIABLE 4-8 LATER
pCRU-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATER
pCRU-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8
</table>
<tableCaption confidence="0.9829325">
Table 1: Forecast texts (for 05-10-2000) generated by each of the pCRU generators, the SUMTIME-Hybrid
system and three experts. The corresponding input to the generators is shown in the first row.
</tableCaption>
<bodyText confidence="0.997956911764706">
BLEU (Papineni et al., 2002) is a precision met-
ric that assesses the quality of a translation in terms
of the proportion of its word n-grams (n ≤ 4 has
become standard) that it shares with several refer-
ence translations. BLEU also incorporates a ‘brevity
penalty’ to counteract scores increasing as length de-
creases. BLEU scores range from 0 to 1.
The NIST metric (Doddington, 2002) is an adapta-
tion of BLEU, but where BLEU gives equal weight to
all n-grams, NIST gives more weight to less frequent
(hence more informative) n-grams. There is evi-
dence that NIST correlates better with human judg-
ments than BLEU (Doddington, 2002; Belz and Re-
iter, 2006).
The results below include human scores from two
separate experiments. The first was an experiment
with 9 subjects experienced in reading marine fore-
casts (Belz and Reiter, 2006), the second is a new
experiment with 14 similarly experienced subjects3.
The main differences were that in Experiment 1,
subjects rated on a scale from 0 to 5 and were asked
for overall quality scores, whereas in Experiment 2,
subjects rated on a 1–7 scale and were asked for lan-
guage quality scores.
In comparing different pCRU modes, NIST and
BLEU scores were computed against the test set part
of the corpus which contains texts by five different
authors. In the two human experiments, NIST and
BLEU scores were computed against sets of multi-
ple reference texts (2 for each date in Experiment 1,
and 3 in Experiment 2) written by forecasters who
had not contributed to the corpus. One-way ANOVAs
with post-hoc Tukey HSD tests were used to analyse
variance and statistical significance of all results.
</bodyText>
<tableCaption confidence="0.4913385">
Table 1 shows forecast texts generated by each of
3Belz and Reiter, in preparation.
</tableCaption>
<table confidence="0.999291">
NIST-5 BLEU-4
T pCRU-greedy 8.208 (0.033) 0.647 (0.002)
R pCRU-roulette 7.035 (0.138) 0.496 (0.010)
A pCRU-2gram 6.734 (0.086) 0.523 (0.008)
I pCRU-viterbi 6.643 (0.023) 0.524 (0.002)
N pCRU-random 4.799 (0.036) 0.296 (0.002)
pCRU-greedy 6.927 (0.131) 0.636 (0.016)
T pCRU-roulette 6.193 (0.121) 0.496 (0.022)
E pCRU-2gram 5.663 (0.185) 0.514 (0.019)
S pCRU-viterbi 5.650 (0.161) 0.519 (0.021)
T pCRU-random 4.535 (0.078) 0.313 (0.005)
</table>
<tableCaption confidence="0.968379">
Table 2: NIST-5 and BLEU-4 scores for training and
test sets (average variation from the mean).
</tableCaption>
<bodyText confidence="0.983870333333333">
the systems included in the evaluations reported be-
low, together with the corresponding input and three
texts created by humans for the same data.
</bodyText>
<subsectionHeader confidence="0.699363">
3.4.2 Comparing different generation modes
</subsectionHeader>
<bodyText confidence="0.9961526875">
Table 2 shows results for the five different pCRU
generation modes, for training sets (top) and test sets
(bottom), in terms of NIST-5 and BLEU-4 scores av-
eraged over the five runs of the hold-out validation,
with average mean deviation figures across the runs
shown in brackets.
The Tukey Test produced the following results for
the differences between means in Table 2. For the
training set, results are the same for NIST and BLEU
scores: all differences are significant at P &lt; 0.01,
except for the differences in scores for pCRU-2gram
andpCRU-viterbi. For the test set and NIST, again all
differences are significant at P &lt; 0.01, except for
pCRU-2gram vs. pCRU-viterbi. For the test set and
BLEU, three differences are non-significant: pCRU-
2gram vs. pCRU-viterbi, pCRU-2gram vs. pCRU-
</bodyText>
<page confidence="0.994527">
168
</page>
<table confidence="0.98650425">
Experiment 1 Experiment 2
SUMTIME-Hyb. 3.82 (1) 4.61 (2)
pCRU-greedy 3.59 (2) 4.79 (1)
pCRU-roulette 3.22 (3) 4.54 (3)
</table>
<tableCaption confidence="0.849881">
Table 3: Scores for handcrafted system and two best
pCRU-systems from two human experiments.
</tableCaption>
<bodyText confidence="0.994754615384615">
roulette, andpCRU-viterbi vs. pCRU-roulette.
NIST-5 depends on test set size, and is necessar-
ily lower for the (smaller) test set, but the BLEU-4
scores indicate that performance was slightly worse
on test sets. The deviation figures show that varia-
tion was also higher on the test sets.
The clearest result is that pCRU-greedy is ranked
highest, and pCRU-random lowest, by considerable
margins. pCRU-roulette is ranked second by NIST-
5 and fourth by BLEU-4. pCRU-2gram and pCRU-
viterbi are virtually indistinguishable.
Experts in both human experiments agreed with
the NIST-5 rankings of the modes exactly.
</bodyText>
<subsectionHeader confidence="0.870758">
3.4.3 Text quality against handcrafted system
</subsectionHeader>
<bodyText confidence="0.999975538461538">
The pCRU modes were also evaluated against
the SUMTIME-Hybrid system (running in ‘hybrid’
mode, taking inputs as in Table 1). Table 3 shows
averaged evaluation scores by subjects in the two in-
dependent experiments described above. There were
altogether 6 and 7 systems evaluated in these experi-
ments, respectively, and the differences between the
scores shown here were not significant when sub-
jected to the Tukey Test, meaning that both experi-
ments failed to show that experts can tell the differ-
ence in the language quality of the texts generated by
the handcrafted SUMTIME-Hybrid system and the
two best pCRU-greedy systems.
</bodyText>
<subsectionHeader confidence="0.85675">
3.4.4 Text quality against human forecasters
</subsectionHeader>
<bodyText confidence="0.99998">
In the first experiment, the human evaluators gave
an average score of 3.59 to pCRU-greedy, 3.22 to
the corpus texts, and 3.03 to another (human) fore-
caster. In Experiment 2, the average human scores
were 4.79 for pCRU-greedy, and 4.50 for the corpus
texts. Although in each experiment separately, sta-
tistical significance could not be shown for the dif-
ferences between these means, in combination the
scores provide evidence that the evaluators thought
pCRU-greedy better than the human-written texts.
</bodyText>
<subsectionHeader confidence="0.986959">
3.4.5 Computing time
</subsectionHeader>
<bodyText confidence="0.9996526">
The following table shows average number of sec-
onds taken to generate one forecast, averaged over
the five cross-validation runs (mean variation figures
across the runs in brackets):
pCRU-greedy:
pCRU-roulette:
pCRU-viterbi:
pCRU-2gram:
Forecasts for the test sets were generated some-
what faster than for the training sets in all modes.
Variation was greater for test sets. Differences
between pCRU-greedy and pCRU-roulette are very
small, but pCRU-viterbi took 1/10 of a second
longer, and pCRU-2gram took more than 1 second
longer to generate the average forecast4.
</bodyText>
<subsectionHeader confidence="0.863403">
3.4.6 Brevity bias
</subsectionHeader>
<bodyText confidence="0.9999178">
N-gram models have a built-in bias in favour of
shorter strings, because they calculate the likelihood
of a string of words as the joint probability of the
words, or, more precisely, as the product of the prob-
abilities of each word given the n − 1 preceding
words. The likelihood of any string will therefore
generally be lower than that of any of its substrings.
Using a smaller data set for which all systems had
outputs, the average number of words in the fore-
casts generated by the different systems was:
</bodyText>
<table confidence="0.971716142857143">
pCRU-random: 19.43
SUMTIME-Hybrid: 12.39
pCRU-greedy: 11.51
Corpus: 11.28
pCRU-roulette: 10.48
pCRU-2gram: 7.66
pCRU-viterbi: 7.54
</table>
<bodyText confidence="0.997994875">
pCRU-random has no preference for shorter
strings, its average string length is almost twice that
of the otherpCRU-generators. The 2-gram generator
prefers shorter strings, while the Viterbi generator
prefers shorter generation processes, and these pref-
erences result in the shortest texts. The poor evalu-
ation results above for the n-gram and Viterbi gen-
erators indicate that this brevity bias can be harm-
</bodyText>
<footnote confidence="0.997548">
4The Viterbi and the 2-gram generator were implemented
identically, except for the n-gram model look-up.
</footnote>
<table confidence="0.9935684">
Training sets Test sets
1.65s (= 0.02) 1.58s (&lt; 0.04)
1.61s (&lt; 0.02) 1.58s (&lt; 0.05)
1.74s (&lt; 0.02) 1.70s (= 0.04)
2.83s (&lt; 0.02) 2.78s (&lt; 0.09)
</table>
<page confidence="0.998038">
169
</page>
<bodyText confidence="0.999478">
ful in NLG. The remaining generators achieve good
matches to the average forecast length in the corpus.
</bodyText>
<subsectionHeader confidence="0.975618">
3.4.7 Development time
</subsectionHeader>
<bodyText confidence="0.999996586206896">
The most time-consuming part of NLG system de-
velopment is not encoding the range of alternatives,
but the decision-making capabilities that enable se-
lection among them. In SUMTIME (Section 3), these
were the result of corpus analysis and consultation
with writers and readers of marine forecasts. In the
pCRU wind forecast generators, the decision-making
capabilities are acquired automatically, no expert
knowledge or corpus annotation is used.
The SUMTIME team estimate5 that very approx-
imately 12 person months went directly into devel-
oping the SUMTIME microplanner and realiser (the
components functionally analogous to the pCRU-
generators), and 24 on generic activities such as
expert consultation, which also benefited the mi-
croplanner/realiser. The pCRU wind forecasters
were built in less than a month, including familiari-
sation with the corpus, building the chunker and cre-
ating the generation rules themselves. However, the
SUMTIME system also generates wave forecasts and
appropriate layout and canned text. A generous esti-
mate is that it would take another two person months
to equip the pCRU forecaster with these capabilities.
This is not to say that the two research efforts re-
sulted in exactly the same thing. It is clear that fore-
cast readers prefer the SUMTIME system, but the
point is that it did come with a substantial price tag
attached. The pCRU approach allows control over
the trade-off between cost and quality.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.984100404255319">
The main contributions of the research described
in this paper are: (i) a generation methodology
that improves substantially on development time and
reusability compared to traditional hand-crafted sys-
tems; (ii) techniques for training linguistically in-
formed decision-making components for probabilis-
tic NLG from raw corpora; and (iii) results that show
that probabilistic NLG can produce high-quality text.
Results also show that (i) a preference for shorter
realisations can be harmful in NLG; and that (ii)
linguistically literate, probabilistic NLG can outper-
5Personal communication with E. Reiter and S. Sripada.
form HALOGEN-style shallow statistical methods, in
terms of quality and efficiency.
An interesting question concerns the contribution
of the manually built component (the base genera-
tor) to the quality of the outputs. The random mode
serves as an absolute baseline in this respect: it in-
dicates how well a particular base generator per-
forms on its own. However, different base genera-
tors have different effects on the generation modes.
The base generator that was used in previous exper-
iments (Belz, 2005) encoded a less structured gen-
eration space and the set of concepts it used were
less fine-grained (e.g. it did not distinguish between
an increase and a decrease in wind speed, consid-
ering both simply a change), and therefore it lacked
some information necessary for deriving conditional
probabilities for lexical choice (e.g. freshening vs.
easing). As predicted (Belz, 2005, p. 21), improve-
ments to the base generator made little difference to
the results for pCRU-2gram (up from BLEU 0.45 to
0.5), but greatly improved the performance of the
greedy mode (up from 0.43 to 0.64).
A basic question for statistical NLG is whether
surface string likelihoods are enough to resolve re-
maining non-determinism in generators, or whether
likelihoods at the more abstract level of generation
rules are needed. The former always prefers the
most frequent variant regardless of context, whereas
in the latter probabilities can attach to linguistic ob-
jects and be conditioned on contextual features (e.g.
one useful feature in the forecast text generators en-
coded whether a rule was being applied at the be-
ginning of a text). The results reported in this paper
provide evidence that probabilistic generation can be
more powerful than n-gram based post-selection.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9998939">
The pCRU approach to generation makes it possi-
ble to combine the potential accuracy and subtlety
of symbolic generation rules with detailed linguis-
tic features on the one hand, and the robustness and
handle on nondeterminism provided by probabili-
ties associated with these rules, on the other. The
evaluation results for the pCRU generators show that
outputs of high quality can be produced with this
approach, that it can speed up development and im-
prove reusability of systems, and that in some modes
</bodyText>
<page confidence="0.987062">
170
</page>
<bodyText confidence="0.999913">
it is more efficient and less brevity-biased than exist-
ing HALOGEN-style n-gram techniques.
The current situation in NLG recalls NLU in the
late 1980s, when symbolic and statistical NLP were
separate research paradigms, a situation memorably
caricatured by Gazdar (1996), before rapidly mov-
ing towards a paradigm merger in the early 1990s.
A similar development is currently underway in MT
where — after several years of statistical MT dom-
inating the field — researchers are now beginning
to bring linguistic knowledge into statistical tech-
niques (Charniak et al., 2003; Huang et al., 2006),
and this trend looks set to continue. The lesson from
NLU and MT appears to be that higher quality re-
sults when the symbolic and statistical paradigms
join forces. The research reported in this paper is
intended to be a first step in this direction for NLG.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999121333333333">
This research was in part supported under UK EPSRC
Grant GR/S24480/01. Many thanks to the anony-
mous reviewers for very helpful comments.
</bodyText>
<sectionHeader confidence="0.999088" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946287878788">
A. Belz and E. Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proc. EACL’06,
pages 313–320.
A. Belz. 2004. Context-free representational underspec-
ification for NLG. Technical Report ITRI-04-08, Uni-
versity of Brighton.
A. Belz. 2005. Statistical generation: Three methods
compared and evaluated. In Proc. of ENLG’05, pages
15–23.
A. Belz. 2006. pCRU: Probabilistic generation using
representational underspecification. Technical Report
NLTG-06-01, University of Brighton.
A. Cahill and J. van Genabith. 2006. Robust PCFG-
based generation using automatically acquired LFG
approximations. In Proc. ACL’06, pages 1033–44.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proc. MT Summit IY
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of the ARPA Workshop on Human
Language Technology.
G. Gazdar. 1996. Paradigm merger in NLP. In Robin
Milner and Ian Wand, editors, Computing Tomor-
row: Future Research Directions in Computer Sci-
ence, pages 88–109. Cambridge University Press.
E. Hovy. 1988. Generating Natural Language under
Pragmatic Constraints. Lawrence Erlbaum.
L. Huang, K. Knight, and A. Joshi. 2006. Statistical
syntax-directed translation with extended domain of
locality. In Proc. AMTA, pages 66–73.
K. Knight and I. Langkilde. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proceed-
ings of COLING-ACL’98, pages 704–710.
I. Langkilde. 2005. An exploratory application of con-
straint optimization in Mozart to probabilistic natural
language processing. In Proceedings of CSLP’05, vol-
ume 3438 of LNAI. Springer-Verlag.
T. Marciniak and M. Strube. 2005. Using an annotated
corpus as a knowledge source for language generation.
In Proceedings of UCNLG’05, pages 19–24.
D. S. Paiva and R. Evans. 2005. Empirically-based con-
trol of natural language generation. In Proceedings
ACL’05.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proc. ACL ’02, pages 311–318.
R. Power. 2000. Planning texts by constraint satisfaction.
In Proceedings of COLING’00.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005. Choos-
ing words in computer-generated weather forecasts.
Arti�cial Intelligence, 167:137–169.
E. Reiter. 1994. Has a consensus NL generation architec-
ture appeared and is it psycholinguistically plausible?
In Proceedings ofINLG’94, pages 163–170.
A. Stolcke. 2002. SRILM: An extensible language mod-
eling toolkit. In Proceedings ofICSLP’02, pages 901–
904,.
S. Varges and C. Mellish. 2001. Instance-based NLG. In
Proc. ofNAACL’01, pages 1–8.
E. Velldal, S. Oepen, and D. Flickinger. 2004. Para-
phrasing treebanks for stochastic realization ranking.
In Proc. of TLT’04.
M. White. 2004. Reining in CCG chart realization. In
Proceedings INLG’04, volume 3123 of LNAI, pages
182–191. Springer.
</reference>
<page confidence="0.998206">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.634151">
<title confidence="0.999137">Probabilistic Generation of Weather Forecast Texts</title>
<author confidence="0.666462">Anja</author>
<affiliation confidence="0.918753">Natural Language Technology University of Brighton,</affiliation>
<email confidence="0.998031">a.s.belz@brighton.ac.uk</email>
<abstract confidence="0.999148368421053">This paper reports experiments in a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space — is used to semi-automatically create several versions of a weather forecast text generator. The generators are evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional pipelined and a statistical generator. The most striking result is that despite acquiring all decision-making abilities the best receive higher scores from human judges than forecasts written by experts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proc. EACL’06,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="16025" citStr="Belz and Reiter, 2006" startWordPosition="2479" endWordPosition="2482">. pCRU-1.0 was then run in all five modes to generate forecasts for the inputs in both training and test sets. This procedure was repeated five times for holdout cross-validation. The small amount of variation across the five repeats, and the small differences between results for training and test sets (Table 2) indicated that five repeats were sufficient. 3.4 Evaluation 3.4.1 Evaluation methods The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006). 167 Input [[1,SSW,16,20,-,-,0600],[2,SSE,-,-,-,-,NOTIME],[3,VAR,04,08,-,-,2400]] Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENING Reference 1 SSW’LY 16-20 GRADUALLY BACKING SSE’LY THEN DECREASING VARIABLE 4-8 BY LATE EVENING Reference 2 SSW 16-20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE 4-8 BY LATE EVENING SUMTIME-Hyb. SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT pCRU-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING pCRU-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8 pCRU-v</context>
<context position="17668" citStr="Belz and Reiter, 2006" startWordPosition="2737" endWordPosition="2741">is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams (n ≤ 4 has become standard) that it shares with several reference translations. BLEU also incorporates a ‘brevity penalty’ to counteract scores increasing as length decreases. BLEU scores range from 0 to 1. The NIST metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more weight to less frequent (hence more informative) n-grams. There is evidence that NIST correlates better with human judgments than BLEU (Doddington, 2002; Belz and Reiter, 2006). The results below include human scores from two separate experiments. The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects3. The main differences were that in Experiment 1, subjects rated on a scale from 0 to 5 and were asked for overall quality scores, whereas in Experiment 2, subjects rated on a 1–7 scale and were asked for language quality scores. In comparing different pCRU modes, NIST and BLEU scores were computed against the test set part of the corpus which con</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>A. Belz and E. Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proc. EACL’06, pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Context-free representational underspecification for NLG.</title>
<date>2004</date>
<tech>Technical Report ITRI-04-08,</tech>
<institution>University of Brighton.</institution>
<contexts>
<context position="6813" citStr="Belz, 2004" startWordPosition="1006" endWordPosition="1007">(Section 2.1). In non-probabilistic mode, the output is the set of fully expanded (fully specified) forms that can be derived from the input. The pCRU (probabilistic CRU) decision-maker is created by estimating a probability distribution over the base generator from an unannotated corpus of example texts. This distribution is used in one of several ways to drive generation processes, maximising the likelihood either of individual expansions or of entire generation processes (Section 2.2). 2.1 Specifying the range of alternatives Using context-free representational underspecification, or CRU, (Belz, 2004), the generation space is encoded as (i) a set G of expansion rules composed of n-ary relations relation(arg1, ...argn) where the argi are constants or variables over constants; and (ii) argument and relation type hierarchies. Any sentential form licensed by G can be the input to the generation process which expands it under unifying variable substitution until no further expansion is possible. The output (in non-probabilistic mode) is the set of fully expanded forms (i.e. consisting only of terminals) that can be derived from the input. The rules in G define the steps in which inputs can be i</context>
</contexts>
<marker>Belz, 2004</marker>
<rawString>A. Belz. 2004. Context-free representational underspecification for NLG. Technical Report ITRI-04-08, University of Brighton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Statistical generation: Three methods compared and evaluated.</title>
<date>2005</date>
<booktitle>In Proc. of ENLG’05,</booktitle>
<pages>15--23</pages>
<contexts>
<context position="4876" citStr="Belz, 2005" startWordPosition="706" endWordPosition="707">ft constraints is problematic, and converting corpus-derived probabilities into costs associated with constraints (Langkilde, Marciniak &amp; Strube) turns straightforward statistics into an ad hoc search heuristic. Older approaches are not globally optimisable (PAULINE) or involve exhaustive search (ICONOCLAST). The pCRU language generation framework combines a probabilistic generation methodology with a comprehensive model of the generation space, where probabilistic choice informs generation as it goes along, instead of after all alternatives have been generated. pCRU uses existing techniques (Belz, 2005), but extends these substantially. This paper describes the pCRU framework and reports experiments designed to rigorously test pCRU in practice and to determine whether improvements in development time and reusability can be achieved without sacrificing quality of outputs. 2 pCRU language generation pCRU (Belz, 2006) is a probabilistic language generation framework that was developed with the aim of providing the formal underpinnings for creating NLG systems that are driven by comprehensive probabilistic models of the entire generation space (including deep generation). NLG systems tend to be </context>
<context position="26896" citStr="Belz, 2005" startWordPosition="4194" endWordPosition="4195">nguistically literate, probabilistic NLG can outper5Personal communication with E. Reiter and S. Sripada. form HALOGEN-style shallow statistical methods, in terms of quality and efficiency. An interesting question concerns the contribution of the manually built component (the base generator) to the quality of the outputs. The random mode serves as an absolute baseline in this respect: it indicates how well a particular base generator performs on its own. However, different base generators have different effects on the generation modes. The base generator that was used in previous experiments (Belz, 2005) encoded a less structured generation space and the set of concepts it used were less fine-grained (e.g. it did not distinguish between an increase and a decrease in wind speed, considering both simply a change), and therefore it lacked some information necessary for deriving conditional probabilities for lexical choice (e.g. freshening vs. easing). As predicted (Belz, 2005, p. 21), improvements to the base generator made little difference to the results for pCRU-2gram (up from BLEU 0.45 to 0.5), but greatly improved the performance of the greedy mode (up from 0.43 to 0.64). A basic question f</context>
</contexts>
<marker>Belz, 2005</marker>
<rawString>A. Belz. 2005. Statistical generation: Three methods compared and evaluated. In Proc. of ENLG’05, pages 15–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>pCRU: Probabilistic generation using representational underspecification.</title>
<date>2006</date>
<tech>Technical Report NLTG-06-01,</tech>
<institution>University of Brighton.</institution>
<contexts>
<context position="5194" citStr="Belz, 2006" startWordPosition="753" endWordPosition="754">U language generation framework combines a probabilistic generation methodology with a comprehensive model of the generation space, where probabilistic choice informs generation as it goes along, instead of after all alternatives have been generated. pCRU uses existing techniques (Belz, 2005), but extends these substantially. This paper describes the pCRU framework and reports experiments designed to rigorously test pCRU in practice and to determine whether improvements in development time and reusability can be achieved without sacrificing quality of outputs. 2 pCRU language generation pCRU (Belz, 2006) is a probabilistic language generation framework that was developed with the aim of providing the formal underpinnings for creating NLG systems that are driven by comprehensive probabilistic models of the entire generation space (including deep generation). NLG systems tend to be composed of generation rules that apply transformations to representations (performing different tasks in different modules). The basic idea in pCRU is that as long as the generation rules are all of the form relation(argl, ...argn) → relationl(argl, ...argp) ... relationm(arg1, ...argq), m &gt; 1, n, p, q &gt; 0, then the</context>
<context position="13864" citStr="Belz (2006)" startWordPosition="2144" endWordPosition="2145">l data to include in a forecast) was carried out off-line. The corpus consists of 2,123 instances (22,985 words) of which half are a.m. forecasts. This may not seem much, but considering the small number of vocabulary items and syntactic structures, the corpus provides extremely good coverage (an initial impression confirmed by the small differences between training and testing data results below). 3.2 The base generator The base generator2 was written semi-automatically in two steps. First, a simple chunker was run over the corpus to split wind statements 2For a fragment of the rule set, see Belz (2006). into wind direction, wind speed, gust speed, gust statements, time expressions, verb phrases, pre-modifiers, and post-modifiers. Preterminal generation rules were automatically created from the resulting chunks. Then, higher-level rules which combine chunks into larger components, taking care of text structuring, aggregation and elision, were manually authored. The top-level generation rules interpret wind statements as sequences of independent units of information, ensuring a linear increase in complexity with increasing input length. Inputs encode meteorological data (as shown in Table 1),</context>
</contexts>
<marker>Belz, 2006</marker>
<rawString>A. Belz. 2006. pCRU: Probabilistic generation using representational underspecification. Technical Report NLTG-06-01, University of Brighton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>J van Genabith</author>
</authors>
<title>Robust PCFGbased generation using automatically acquired LFG approximations.</title>
<date>2006</date>
<booktitle>In Proc. ACL’06,</booktitle>
<pages>1033--44</pages>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>A. Cahill and J. van Genabith. 2006. Robust PCFGbased generation using automatically acquired LFG approximations. In Proc. ACL’06, pages 1033–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntaxbased language models for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. MT Summit IY</booktitle>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>E. Charniak, K. Knight, and K. Yamada. 2003. Syntaxbased language models for machine translation. In Proc. MT Summit IY</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="17399" citStr="Doddington, 2002" startWordPosition="2693" endWordPosition="2694">URING THE AFTERNOON THEN VARIABLE 4-8 Table 1: Forecast texts (for 05-10-2000) generated by each of the pCRU generators, the SUMTIME-Hybrid system and three experts. The corresponding input to the generators is shown in the first row. BLEU (Papineni et al., 2002) is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams (n ≤ 4 has become standard) that it shares with several reference translations. BLEU also incorporates a ‘brevity penalty’ to counteract scores increasing as length decreases. BLEU scores range from 0 to 1. The NIST metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more weight to less frequent (hence more informative) n-grams. There is evidence that NIST correlates better with human judgments than BLEU (Doddington, 2002; Belz and Reiter, 2006). The results below include human scores from two separate experiments. The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects3. The main differences were that in Experiment 1, subjects rated on a scale from</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Paradigm merger in NLP.</title>
<date>1996</date>
<booktitle>Computing Tomorrow: Future Research Directions in Computer Science,</booktitle>
<pages>88--109</pages>
<editor>In Robin Milner and Ian Wand, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="28939" citStr="Gazdar (1996)" startWordPosition="4519" endWordPosition="4520">he one hand, and the robustness and handle on nondeterminism provided by probabilities associated with these rules, on the other. The evaluation results for the pCRU generators show that outputs of high quality can be produced with this approach, that it can speed up development and improve reusability of systems, and that in some modes 170 it is more efficient and less brevity-biased than existing HALOGEN-style n-gram techniques. The current situation in NLG recalls NLU in the late 1980s, when symbolic and statistical NLP were separate research paradigms, a situation memorably caricatured by Gazdar (1996), before rapidly moving towards a paradigm merger in the early 1990s. A similar development is currently underway in MT where — after several years of statistical MT dominating the field — researchers are now beginning to bring linguistic knowledge into statistical techniques (Charniak et al., 2003; Huang et al., 2006), and this trend looks set to continue. The lesson from NLU and MT appears to be that higher quality results when the symbolic and statistical paradigms join forces. The research reported in this paper is intended to be a first step in this direction for NLG. Acknowledgments This</context>
</contexts>
<marker>Gazdar, 1996</marker>
<rawString>G. Gazdar. 1996. Paradigm merger in NLP. In Robin Milner and Ian Wand, editors, Computing Tomorrow: Future Research Directions in Computer Science, pages 88–109. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum.</title>
<date>1988</date>
<contexts>
<context position="2332" citStr="Hovy, 1988" startWordPosition="348" endWordPosition="349">ntly expensive, requiring the set of alternatives to be generated in full before the statistical model is applied to select the most likely; and (ii) statistical NLG techniques have not been shown to produce outputs of high enough quality. There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). Some research has moved towards a more comprehensive view, e.g. construing the generation task as a single constraint satisfaction problem. Precursors to current approaches were Hovy’s PAULINE which kept track of the satisfaction status of global ‘rhetorical goals’ (Hovy, 1988), and Power et al.’s ICONOCLAST which allowed users to fine-tune different combinations of global constraints (Power, 2000). In recent comprehensive approaches, the focus is on automatic adaptability, e.g. automatically determining degrees of constraint violability on the basis of corpus frequencies. Examples include Langkilde’s (2005) general approach to generation and parsing based on constraint optimisation, and Marciniak and Strube’s (2005) integrated, globally optimisable network of classifiers and constraints. Both probabilistic and recent comprehensive trends have developed at least in </context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>E. Hovy. 1988. Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Knight</author>
<author>A Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. AMTA,</booktitle>
<pages>66--73</pages>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>L. Huang, K. Knight, and A. Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. AMTA, pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Langkilde</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="1229" citStr="Knight and Langkilde, 1998" startWordPosition="170" endWordPosition="173">stem, and (iii) a HALOGEN-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators receive higher scores from human judges than forecasts written by experts. 1 Introduction and background Over the last decade, there has been a lot of interest in statistical techniques among researchers in natural language generation (NLG), a field that was largely unaffected by the statistical revolution in NLP that started in the 1980s. Since Langkilde and Knight’s influential work on statistical surface realisation (Knight and Langkilde, 1998), a number of statistical and corpus-based methods have been reported. However, this interest does not appear to have translated into practice: of the 30 implemented systems and modules with development starting in or after 2000 that are listed on a key NLG website1, only five have any statistical component at all (another six involve techniques that are in some way corpus-based). The likely reasons for this lack of take-up are that (i) many existing statistical NLG techniques are inherently expensive, requiring the set of alternatives to be generated in full before the statistical model is ap</context>
</contexts>
<marker>Knight, Langkilde, 1998</marker>
<rawString>K. Knight and I. Langkilde. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of COLING-ACL’98, pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
</authors>
<title>An exploratory application of constraint optimization in Mozart to probabilistic natural language processing.</title>
<date>2005</date>
<booktitle>In Proceedings of CSLP’05,</booktitle>
<volume>3438</volume>
<publisher>Springer-Verlag.</publisher>
<marker>Langkilde, 2005</marker>
<rawString>I. Langkilde. 2005. An exploratory application of constraint optimization in Mozart to probabilistic natural language processing. In Proceedings of CSLP’05, volume 3438 of LNAI. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Marciniak</author>
<author>M Strube</author>
</authors>
<title>Using an annotated corpus as a knowledge source for language generation.</title>
<date>2005</date>
<booktitle>In Proceedings of UCNLG’05,</booktitle>
<pages>19--24</pages>
<marker>Marciniak, Strube, 2005</marker>
<rawString>T. Marciniak and M. Strube. 2005. Using an annotated corpus as a knowledge source for language generation. In Proceedings of UCNLG’05, pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Paiva</author>
<author>R Evans</author>
</authors>
<title>Empirically-based control of natural language generation.</title>
<date>2005</date>
<booktitle>In Proceedings ACL’05.</booktitle>
<contexts>
<context position="3657" citStr="Paiva and Evans, 2005" startWordPosition="531" endWordPosition="534"> systems, http://www.fb10.uni-bremen.de/anglistik/ langpro/NLG-table/, 20/01/2006. 164 Proceedings of NAACL HLT 2007, pages 164–171, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics of time and expense involved in building new systems, and the almost complete lack in the field of reusable systems and modules. Both trends have the potential to improve on development time and reusability, but have drawbacks. Existing statistical NLG (i) uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation (Varges and Mellish, 2001; White, 2004; Paiva and Evans, 2005); (ii) applies n-gram models to select the overall most likely realisation after generation (HALOGEN family); or (iii) reuses an existing parsing grammar or treebank for surface realisation (Velldal et al., 2004; Cahill and van Genabith, 2006). N-gram models are not linguistically informed, (i) and (iii) come with a substantial manual overhead, and (ii) overgenerates vastly and has a high computational cost (see also Section 3). Existing comprehensive approaches tend to incur a manual overhead (finetuning in ICONOCLAST, corpus annotation in Langkilde and Marciniak &amp; Strube). Handling violabili</context>
</contexts>
<marker>Paiva, Evans, 2005</marker>
<rawString>D. S. Paiva and R. Evans. 2005. Empirically-based control of natural language generation. In Proceedings ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL ’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="17045" citStr="Papineni et al., 2002" startWordPosition="2630" endWordPosition="2633">THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT pCRU-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING pCRU-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8 pCRU-viterbi SSW 16-20 BACKING SSE VARIABLE 4-8 LATER pCRU-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATER pCRU-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8 Table 1: Forecast texts (for 05-10-2000) generated by each of the pCRU generators, the SUMTIME-Hybrid system and three experts. The corresponding input to the generators is shown in the first row. BLEU (Papineni et al., 2002) is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams (n ≤ 4 has become standard) that it shares with several reference translations. BLEU also incorporates a ‘brevity penalty’ to counteract scores increasing as length decreases. BLEU scores range from 0 to 1. The NIST metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more weight to less frequent (hence more informative) n-grams. There is evidence that NIST correlates better with human judgments than BLEU (Doddington, 2002;</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proc. ACL ’02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power</author>
</authors>
<title>Planning texts by constraint satisfaction.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING’00.</booktitle>
<contexts>
<context position="2455" citStr="Power, 2000" startWordPosition="366" endWordPosition="367">t the most likely; and (ii) statistical NLG techniques have not been shown to produce outputs of high enough quality. There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). Some research has moved towards a more comprehensive view, e.g. construing the generation task as a single constraint satisfaction problem. Precursors to current approaches were Hovy’s PAULINE which kept track of the satisfaction status of global ‘rhetorical goals’ (Hovy, 1988), and Power et al.’s ICONOCLAST which allowed users to fine-tune different combinations of global constraints (Power, 2000). In recent comprehensive approaches, the focus is on automatic adaptability, e.g. automatically determining degrees of constraint violability on the basis of corpus frequencies. Examples include Langkilde’s (2005) general approach to generation and parsing based on constraint optimisation, and Marciniak and Strube’s (2005) integrated, globally optimisable network of classifiers and constraints. Both probabilistic and recent comprehensive trends have developed at least in part to address two interrelated issues in NLG: the considerable amount 1Bateman and Zock’s list of NLG systems, http://www</context>
</contexts>
<marker>Power, 2000</marker>
<rawString>R. Power. 2000. Planning texts by constraint satisfaction. In Proceedings of COLING’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
<author>J Hunter</author>
<author>J Yu</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Arti�cial Intelligence,</journal>
<pages>167--137</pages>
<contexts>
<context position="12674" citStr="Reiter et al., 2005" startWordPosition="1951" endWordPosition="1954"> SSW 2.29 05/15 S 14 17 21 2.3 3.7 SSW 2.28 ... FORECAST FOR:- Oil1/Oil2/Oil3 FIELDS ... 2.FORECAST 06-24 GMT, THURSDAY, 05-Oct 2000 = = ===WARNINGS: RISK THUNDERSTORM. ======= WIND(KTS) CONFIDENCE: HIGH 10M: SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 04-08 BY LATE EVENING ... Figure 1: Meteorological data file and wind forecast for 05-10-2000, a.m. (oil fields anonymised). weather forecasting particularly attractive to NLG researchers, and a number of weather forecast generation systems have been created. A recent example of weather forecast text generation is the SUMTIME project (Reiter et al., 2005) which developed a commercially used NLG system that generates marine weather forecasts for offshore oil rigs from numerical forecast data produced by weather simulation programs. The SUMTIME corpus is used in the experiments below. 3.1 Data Each instance in the SUMTIME corpus consists of three numerical data files (the outputs of weather simulators) and the forecast file written by the forecaster on the basis of the data (Figure 1 shows an example). The experiments below focused on a.m. forecasts of wind characteristics. Content determination (deciding which meteorological data to include in </context>
</contexts>
<marker>Reiter, Sripada, Hunter, Yu, 2005</marker>
<rawString>E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005. Choosing words in computer-generated weather forecasts. Arti�cial Intelligence, 167:137–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
</authors>
<title>Has a consensus NL generation architecture appeared and is it psycholinguistically plausible?</title>
<date>1994</date>
<booktitle>In Proceedings ofINLG’94,</booktitle>
<pages>163--170</pages>
<contexts>
<context position="2052" citStr="Reiter, 1994" startWordPosition="305" endWordPosition="306">g in or after 2000 that are listed on a key NLG website1, only five have any statistical component at all (another six involve techniques that are in some way corpus-based). The likely reasons for this lack of take-up are that (i) many existing statistical NLG techniques are inherently expensive, requiring the set of alternatives to be generated in full before the statistical model is applied to select the most likely; and (ii) statistical NLG techniques have not been shown to produce outputs of high enough quality. There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). Some research has moved towards a more comprehensive view, e.g. construing the generation task as a single constraint satisfaction problem. Precursors to current approaches were Hovy’s PAULINE which kept track of the satisfaction status of global ‘rhetorical goals’ (Hovy, 1988), and Power et al.’s ICONOCLAST which allowed users to fine-tune different combinations of global constraints (Power, 2000). In recent comprehensive approaches, the focus is on automatic adaptability, e.g. automatically determining degrees of constraint violability on the basis of corpus frequencies. Examples include L</context>
</contexts>
<marker>Reiter, 1994</marker>
<rawString>E. Reiter. 1994. Has a consensus NL generation architecture appeared and is it psycholinguistically plausible? In Proceedings ofINLG’94, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM: An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP’02,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="15403" citStr="Stolcke, 2002" startWordPosition="2380" endWordPosition="2381">lisations for an input. The job of the base generator is to describe the textual variety found in the corpus. It makes no decisions about when to prefer one variant over another. 3.3 Training The corpus was divided at random into 90% training data and 10% testing data. The training set was multi-treebanked with the base generator and the multi-treebank then used to create the probability distribution for the base generator (as described in Section 2.2). A back-off 2-gram model with Good-Turing discounting and no lexical classes was also created from the training set, using the SRILM toolkit, (Stolcke, 2002). pCRU-1.0 was then run in all five modes to generate forecasts for the inputs in both training and test sets. This procedure was repeated five times for holdout cross-validation. The small amount of variation across the five repeats, and the small differences between results for training and test sets (Table 2) indicated that five repeats were sufficient. 3.4 Evaluation 3.4.1 Evaluation methods The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM: An extensible language modeling toolkit. In Proceedings ofICSLP’02, pages 901– 904,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Varges</author>
<author>C Mellish</author>
</authors>
<title>Instance-based NLG.</title>
<date>2001</date>
<booktitle>In Proc. ofNAACL’01,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3620" citStr="Varges and Mellish, 2001" startWordPosition="525" endWordPosition="528"> amount 1Bateman and Zock’s list of NLG systems, http://www.fb10.uni-bremen.de/anglistik/ langpro/NLG-table/, 20/01/2006. 164 Proceedings of NAACL HLT 2007, pages 164–171, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics of time and expense involved in building new systems, and the almost complete lack in the field of reusable systems and modules. Both trends have the potential to improve on development time and reusability, but have drawbacks. Existing statistical NLG (i) uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation (Varges and Mellish, 2001; White, 2004; Paiva and Evans, 2005); (ii) applies n-gram models to select the overall most likely realisation after generation (HALOGEN family); or (iii) reuses an existing parsing grammar or treebank for surface realisation (Velldal et al., 2004; Cahill and van Genabith, 2006). N-gram models are not linguistically informed, (i) and (iii) come with a substantial manual overhead, and (ii) overgenerates vastly and has a high computational cost (see also Section 3). Existing comprehensive approaches tend to incur a manual overhead (finetuning in ICONOCLAST, corpus annotation in Langkilde and Ma</context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>S. Varges and C. Mellish. 2001. Instance-based NLG. In Proc. ofNAACL’01, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Velldal</author>
<author>S Oepen</author>
<author>D Flickinger</author>
</authors>
<title>Paraphrasing treebanks for stochastic realization ranking.</title>
<date>2004</date>
<booktitle>In Proc. of TLT’04.</booktitle>
<contexts>
<context position="3868" citStr="Velldal et al., 2004" startWordPosition="562" endWordPosition="565">ime and expense involved in building new systems, and the almost complete lack in the field of reusable systems and modules. Both trends have the potential to improve on development time and reusability, but have drawbacks. Existing statistical NLG (i) uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation (Varges and Mellish, 2001; White, 2004; Paiva and Evans, 2005); (ii) applies n-gram models to select the overall most likely realisation after generation (HALOGEN family); or (iii) reuses an existing parsing grammar or treebank for surface realisation (Velldal et al., 2004; Cahill and van Genabith, 2006). N-gram models are not linguistically informed, (i) and (iii) come with a substantial manual overhead, and (ii) overgenerates vastly and has a high computational cost (see also Section 3). Existing comprehensive approaches tend to incur a manual overhead (finetuning in ICONOCLAST, corpus annotation in Langkilde and Marciniak &amp; Strube). Handling violability of soft constraints is problematic, and converting corpus-derived probabilities into costs associated with constraints (Langkilde, Marciniak &amp; Strube) turns straightforward statistics into an ad hoc search he</context>
</contexts>
<marker>Velldal, Oepen, Flickinger, 2004</marker>
<rawString>E. Velldal, S. Oepen, and D. Flickinger. 2004. Paraphrasing treebanks for stochastic realization ranking. In Proc. of TLT’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
</authors>
<title>Reining in CCG chart realization.</title>
<date>2004</date>
<booktitle>In Proceedings INLG’04,</booktitle>
<volume>3123</volume>
<pages>182--191</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3633" citStr="White, 2004" startWordPosition="529" endWordPosition="530">s list of NLG systems, http://www.fb10.uni-bremen.de/anglistik/ langpro/NLG-table/, 20/01/2006. 164 Proceedings of NAACL HLT 2007, pages 164–171, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics of time and expense involved in building new systems, and the almost complete lack in the field of reusable systems and modules. Both trends have the potential to improve on development time and reusability, but have drawbacks. Existing statistical NLG (i) uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation (Varges and Mellish, 2001; White, 2004; Paiva and Evans, 2005); (ii) applies n-gram models to select the overall most likely realisation after generation (HALOGEN family); or (iii) reuses an existing parsing grammar or treebank for surface realisation (Velldal et al., 2004; Cahill and van Genabith, 2006). N-gram models are not linguistically informed, (i) and (iii) come with a substantial manual overhead, and (ii) overgenerates vastly and has a high computational cost (see also Section 3). Existing comprehensive approaches tend to incur a manual overhead (finetuning in ICONOCLAST, corpus annotation in Langkilde and Marciniak &amp; Str</context>
</contexts>
<marker>White, 2004</marker>
<rawString>M. White. 2004. Reining in CCG chart realization. In Proceedings INLG’04, volume 3123 of LNAI, pages 182–191. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>