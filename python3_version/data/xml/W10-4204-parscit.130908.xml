<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000129">
<title confidence="0.996648">
Hierarchical Reinforcement Learning for Adaptive Text Generation
</title>
<author confidence="0.997045">
Nina Dethlefs
</author>
<affiliation confidence="0.843839">
University of Bremen, Germany
dethlefs@uni-bremen.de
</affiliation>
<author confidence="0.969248">
Heriberto Cuay´ahuitl
</author>
<affiliation confidence="0.767541">
University of Bremen, Germany
heriberto@uni-bremen.de
</affiliation>
<sectionHeader confidence="0.985247" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999663157894737">
We present a novel approach to natural lan-
guage generation (NLG) that applies hierar-
chical reinforcement learning to text genera-
tion in the wayfinding domain. Our approach
aims to optimise the integration of NLG tasks
that are inherently different in nature, such
as decisions of content selection, text struc-
ture, user modelling, referring expression gen-
eration (REG), and surface realisation. It
also aims to capture existing interdependen-
cies between these areas. We apply hierar-
chical reinforcement learning to learn a gen-
eration policy that captures these interdepen-
dencies, and that can be transferred to other
NLG tasks. Our experimental results—in a
simulated environment—show that the learnt
wayfinding policy outperforms a baseline pol-
icy that takes reasonable actions but without
optimization.
</bodyText>
<sectionHeader confidence="0.998125" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945293103449">
Automatic text generation involves a number of sub-
tasks. (Reiter and Dale, 1997) list the following as
core tasks of a complete NLG system: content se-
lection, discourse planning, sentence planning, sen-
tence aggregation, lexicalisation, referring expres-
sion generation and linguistic realisation. However,
decisions made for each of these core tasks are not
independent of each other. The value of one gen-
eration task can change the conditions of others,
as evidenced by studies in corpus linguistics, and
it can therefore be undesirable to treat them all as
isolated modules. In this paper, we focus on inter-
related decision making in the areas of content se-
lection, choice of text structure, referring expression
and surface form. Concretely, we generate route in-
structions that are tailored specifically towards dif-
ferent user types as well as different environmental
features. In addition, we aim to balance the degree
of variation and alignment in texts and produce lex-
ical and syntactic patterns of co-occurrence that re-
semble those of human texts of the same domain.
Evidence for the importance of this is provided by
(Halliday and Hasan, 1976) who note the way that
lexical cohesive ties contribute to text coherence as
well as by the theory of interactive alignment. Ac-
cording to (Pickering and Garrod, 2004) we would
expect significant traces of lexical and syntactic self-
alignment in texts.
Approaches to NLG in the past have been ei-
ther rule-based (Reiter and Dale, 1997) or statisti-
cal (Langkilde and Knight, 1998). However, the for-
mer relies on a large number of hand-crafted rules,
which makes it infeasible for controlling a large
number of interrelated variables. The latter typi-
cally requires training on a large corpus of the do-
main. While these approaches may be better suitable
for larger domains, for limited domains such as our
own, we propose to overcome these drawbacks by
applying Reinforcement Learning (RL)—with a hi-
erarchical approach. Previous work that has used RL
for NLG includes (Janarthanam and Lemon, 2009)
who employed it for alignment of referring expres-
sions based on user models. Also, (Lemon, 2008;
Rieser and Lemon, 2009) used RL for optimising
information presentation styles for search results.
While both approaches displayed significant effects
of adaptation, they focused on a single area of opti-
misation. For larger problems, however, such as the
one we are aiming to solve, flat RL will not be appli-
cable due to the large state space. We therefore sug-
gest to divide the problem into a number of subprob-
lems and apply hierarchical reinforcement learning
(HRL) (Barto and Mahadevan, 2003) to solve it.
We describe our problem in more detail in Sec-
tion 2, our proposed HRL architecture in Sections
3 and 4 and present some results in Section 5. We
show that our learnt policies outperform a baseline
that does not adapt to contextual features.
</bodyText>
<sectionHeader confidence="0.984147" genericHeader="introduction">
2 Generation tasks
</sectionHeader>
<bodyText confidence="0.99685192">
Our experiments are all drawn from an indoor
navigation dialogue system which provides users
with route instructions in a university building and
is described in (Cuay´ahuitl et al., 2010). We aim
to optimise generation within the areas of content
selection, text structure, referring expression gener-
ation and surface realisation.
Content Selection Content selection decisions
are subject to different user models. We distin-
guish users who are familiar with the navigation
environment and users who are not. In this way,
we can provide different routes for these users
corresponding to their particular information need.
Specifically, we provide more detail for unfamiliar
than familiar users by adding any or several of
the following: (a) landmarks at decision points,
(b) landmarks lying on long route segments, (c)
specifications of distance.
Text Structure Depending on the type of user
and the length of the route, we choose among three
different text generation strategies to ease the cogni-
tive load of the user. Examples of all strategies are
displayed in Table 1. All three types resulted from
an analysis of a corpus of 24 human-written driving
route instructions. We consider the first type (se-
quential) most appropriate for long or medium-long
routes and both types of user. The second type (tem-
poral) is appropriate for unfamiliar users and routes
of short or medium length. It divides the route into
an explicit sequence of consecutive actions. The
third type (schematic) is used in the remaining cases.
Referring Expression Generation We dis-
tinguish three types of referring expressions:
common names, familiar names and descriptions.
In this way, entities can be named according to
the users’ prior knowledge. For example, one
and the same room can be called either ‘the
student union room’, ‘room A3530’ or ‘the room
right at the corner beside the entrance to the terrace’.
Surface Realisation For surface realisation, we
aim to generate texts that display a natural balance
of (self-)alignment and variation. While it is a rule
of writing that texts should typically contain varia-
tion of surface forms in order not to appear repetitive
and stylistically poor, there is evidence that humans
also get influenced by self-alignment processes dur-
ing language production. Specifically, (Garrod and
Anderson, 1987; Pickering and Garrod, 2004) ar-
gue that the same mental representations are used
during language production and comprehension, so
that alignment occurs regardless of whether the last
utterance was made by another person or by the
speaker him- or herself (for experimental evidence
see (Branigan et al., 2000; Bock, 1986)). We can
therefore hypothesise that coherent texts will, be-
sides variation, also display a certain degree of self-
alignment. In order to determine a proper balance
of alignment and variation, we computed the degree
of lexical repetition from our corpus of 24 human
route descriptions. This analysis was based on (Hirst
and St-Onge, 1998) who retrieve lexical chains from
texts by identifying a number of relations between
lexical items. We focus here exclusively on Hirst
&amp; St-Onge’s ‘extra-strong’ relations, since these can
be computed from shallow properties of texts and do
not require a large corpus of the target domain. In
order to make a fair comparison between the human
texts and our own, we used a part-of-speech (POS)
tagger (Toutanova and Manning, 2000)1 to extract
those grammatical categories that we aim to control
within our framework, i.e. nouns, verbs, preposi-
tions, adjectives and adverbs. Based on these cat-
egories, we compute the proportion of tokens that
are members in lexical chains, the ‘alignment score’
(AS), according to the following equation:
</bodyText>
<equation confidence="0.754872">
Lexical tokens in chains
AS = × 100. (1)
</equation>
<bodyText confidence="0.963122">
Total number of tokens
We obtained an average alignment score of 43.3%
for 24 human route instructions. In contrast, the
</bodyText>
<footnote confidence="0.834845">
1http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<tableCaption confidence="0.99478">
Table 1: Different text generation strategies for the same underlying route.
</tableCaption>
<bodyText confidence="0.971928916666667">
Type 1: Sequential Type 2: Temporal Type 3: Schematic
Turn around, and go straight First, turn around. Second, - Turn around.
to the glass door in front of go straight to the glass door - Go straight until the glass door in front
you. Turn right, then follow in front of you. Third, turn of you. (20 m)
the corridor until the lift. It right. Fourth, follow the - Turn right
will be on your left-hand corridor until the lift. It will - Follow the corridor until the lift. (20 m)
side. be on your left-hand side. - It will be on your left-hand side.
same number of instructions generated by Google
Maps yielded 78.7%, i.e. an almost double amount
of repetition. We will therefore train our agent
to generate texts with an about medium alignment
score.
</bodyText>
<sectionHeader confidence="0.7971055" genericHeader="method">
3 Hierarchical Reinforcement Learning
for NLG
</sectionHeader>
<bodyText confidence="0.999853431034483">
The idea of text generation as an optimization
problem is as follows: given a set of genera-
tion states, a set of actions, and an objective
reward function, an optimal generation strategy
maximizes the objective function by choosing the
actions leading to the highest reward for every
reached state. Such states describe the system’s
knowledge about the generation task (e.g. con-
tent selection, text structure, REG, surface realiza-
tion). The action set describes the system’s ca-
pabilities (e.g. expand sequential aggregation, ex-
pand schematic aggregation, expand lexical items,
etc.). The reward function assigns a numeric value
for each taken action. In this way, text generation
can be seen as a finite sequence of states, actions
and rewards {s0, a0, r1, s1, a1, ..., rt−1, st}, where
the goal is to find an optimal strategy automatically.
To do that we use hierarchical reinforcement learn-
ing in order to optimize a hierarchy of text genera-
tion policies rather than a single policy.
The hierarchy of RL agents consists of L lev-
els and N models per level, denoted as M = Mij,
where j ∈ {0, ..., N − 1} and i ∈ {0, ..., L − 1}.
Each agent of the hierarchy is defined as a Semi-
Markov Decision Process (SMDP) consisting of a
4-tuple &lt; Sij, Aij,Tji, Rij &gt;. Sij is a set of states, Ai j
is a set of actions, and Tji is a transition function that
determines the next state s′ from the current state
s and the performed action a with a probability of
P(s′|s, a). Rij(s′, τ|s, a) is a reward function that
specifies the reward that an agent receives for taking
an action a in state s at time τ. Since SMDPs allow
for temporal abstraction, that is, actions may take a
variable number of time steps to complete, the ran-
dom variable τ represents this number of time steps.
Actions can be either primitive or composite. The
former yield single rewards, the latter (executed us-
ing a stack mechanism) correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optional policy π* that max-
imises the reward for each visited state, according to
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing π*. For learning a generation policy, we
use hierarchical Q-Learning (HSMQ) (Dietterich,
1999). The dynamics of SMDPs are as follows:
when an SMDP terminates its execution, it is popped
off the stack of models to execute, and control is
transferred to the next available SMDP in the stack,
and so on until popping off the root SMDP. An
SMDP terminates when it reaches one of its termi-
nal states. This algorithm is executed until the Q-
values of the root agent stabilize. The hierarchical
decomposition allows to find context-independent
policies with the advantages of policy reuse and fa-
cilitation for state-action abstraction. This hierarchi-
cal approach has been applied successfully to dia-
logue strategy learning (Cuayahuitl et al., 2010).
</bodyText>
<sectionHeader confidence="0.990404" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.999538">
4.1 Hierarchy of SMDPs
</subsectionHeader>
<bodyText confidence="0.9999845">
The hierarchy consists of 15 agents. It is depicted
in Figure 1. The root agent is responsible for deter-
</bodyText>
<equation confidence="0.906285">
π *j(s) = arg `( )
max
a�A
Q s, a (2)
</equation>
<figureCaption confidence="0.997465">
Figure 1: Hierarchy of agents for learning adaptive text generation strategies in the wayfinding domain
</figureCaption>
<bodyText confidence="0.999886444444444">
mining a route instruction type for a navigation situ-
ation. We distinguish turning, passing, locating, go-
ing and following instructions. It also chooses a text
generation strategy and the information structure of
the clause (i.e., marked or unmarked theme (Hall-
iday and Matthiessen, 2004)). Leaf agents are re-
sponsible for expanding constituents in which varia-
tion or alignment can occur, e.g. the choice of verb
or prepositional phrase.
</bodyText>
<subsectionHeader confidence="0.992231">
4.2 State and action sets
</subsectionHeader>
<bodyText confidence="0.99987288">
We distinguish three kinds of state representations,
displayed in Table 2. The first (M010 and M10) en-
codes information on the spatial environment and
user type so that texts can be tailored towards these
variables. These variables play a major part in our
simulated environment (Section 5.1). The second
representation (Mi - M15 and M23) controls sentence
structure and ensures that all required constituents
for a message have been realised. The third (all re-
maining models) encodes variants of linguistic sur-
face structure and represents the degree of alignment
of all variants. We address the way that these align-
ment values are computed in Section 4.4. Actions
can be either primitive or composite. Whereas the
former expand a logical form directly, the latter cor-
respond to SMDPs at different levels of the hierar-
chy. All parent agents have both types of actions,
only the leaf agents have exclusively primitive ac-
tions. The set of primitive actions is displayed in Ta-
ble 2, all composite actions, corresponding to mod-
els, are shown in Figure 1. The average number of
state-action pairs for a model is |S × A |= 77786.
While in the present work, the action set was de-
termined manually, future work can aim at learning
hierarchies of SMDPs automatically from data.
</bodyText>
<subsectionHeader confidence="0.995393">
4.3 Prior Knowledge
</subsectionHeader>
<bodyText confidence="0.999992615384615">
Agents contain prior knowledge of two sorts. First,
the root agents and agents at the first level of the hi-
erarchy contain prior probabilities of executing cer-
tain actions. For example, given an unfamiliar user
and a long route, model M01, text strategy, is initi-
ated with a higher probability of choosing a sequen-
tial text strategy than a schematic or temporal strat-
egy. Second, leaf agents of the hierarchy are initi-
ated with values of a hand-crafted language model.
These values indicate the probabilities of occurrence
of the different surface forms of the leaf agents listed
in Table 2. Both types of prior probabilities are used
by the reward functions described below.
</bodyText>
<subsectionHeader confidence="0.994561">
4.4 Reward functions
</subsectionHeader>
<bodyText confidence="0.938017428571429">
We use two types of reward function, both of which
are directly motivated by the principles we stated in
Section 2. The first addresses interaction length (the
shorter the better) and the choice of actions tailored
towards the user model and spatial environment.
0 for reaching the goal state
-10 for an already invoked subtask
</bodyText>
<equation confidence="0.373164">
p(a) otherwise
(3)
</equation>
<bodyText confidence="0.946218333333333">
p(a) corresponds to the probability of the last ac-
tion given the current state, described above as prior
knowledge. The second reward function addresses
</bodyText>
<table confidence="0.46967275">
⎧
⎨
⎩
R=
</table>
<tableCaption confidence="0.969831">
Table 2: State and action sets for learning adaptive text generation strategies in the wayfznding domain
</tableCaption>
<table confidence="0.910785948717949">
Model State Variables Action Set
M text strategy (FV), info structure (FV), instruction (FV), expand text strategy (M10), turning (M23),
slot in focus(0=action, 1=landmark), user type(0=unfamiliar, going (M21 ), passing (M25 ), following (M22),
1=familiar) subtask termination(0=continue, 1=halt) locating instr.(M24), expand unmarked theme
Mo end(0=continue, 1=halt), text strategy (FV), route length expand schematic aggregation, expand sequen-
(0=short, 1=medium, 2=long), user type(0=unfam., 1=fam.) ce aggregation, expand temporal aggregation
M1 going vp (FV), limit (FV), SV expand going vp (M2 0 ), expand limit
1
M1 following vp (FV), SV, limit (FV) expand following vp (M2 1 ), expand limit
2
M1 turning location (FV), turning vp (FV), expand turning vp (M3 2 ), expand turning loc.,
3 SV, turning direction (FV) expand turning direction (M34)
M1 np locatum (FV), locating vp (FV), expand np locatum, expand locating vp (M2 5 ),
4 static direction (FV), SV expand static dir. (M26 )
M1 np locatum (FV), passing vp (FV), SV, static direction (FV) expand pass. vp (M2 6 ), expand static dir. (M2 6 )
5
M� vp go straight ahead, vp go straight, vp move straight ahead, Actions correspond to expansions of
vp walk straight ahead, vp walk straight (all AS) lexemes
M� vp follow, vp go over, vp walk down, vp go down, Actions correspond to expansions of
1 vp go up, vp walk up, vp walk over (all AS) lexemes
M� vp walk, vp veer , vp hang, vp bear (all AS), vp go, vp head, Actions correspond to expansions of
2 vp turn (all AS) lexemes
M� identifiability(0=not id.,1=id.), user type(0=un-, expand relatum id., expand relatum, not id.,
3 fam.,1=fam,, relatum identifiability (FV), relatum name (FV) expand descriptive, expand common name
M� pp nonphoric, pp nonphoric handedness, Actions correspond to expansions of
4 pp nonphoric poss, pp phoric pp nonphoric side (all AS) lexemes
M� vp be, vp be located at, vp get to, vp see (all AS) Actions correspond to expansions of lexemes
5
M� direction on, direction poss, direction to (all AS) Actions correspond to expansions of lexemes
6
M� vp move past, vp pass, vp pass by, vp walk past (all AS) Actions correspond to expansions of lexemes
7
(FV = filling status): 0=unfilled, 1=filled. (SV = shared variables): the variables np actor (FV), relatum (FV),
sentence (FV) and information need (0=low, 1=high) are shared by several subagents; the same applies to their
corresponding expansion actions. (AS = alignment score): 0=unaligned, 1=low AS, 2=medium AS, 3=high AS.
the tradeoff between alignment and variation:
0 for reaching the goal state
p(a) for medium alignment (4)
-0.1 otherwise
</table>
<bodyText confidence="0.998926125">
Whilst the former reward function is used by the root
and models M1 - M15 and M2 , the latter is used by
models M0 - M1 and M3 - M7 . It rewards the agent
for a medium alignment score, which corresponds
to the score of typical human texts we computed
in Section 2. The alignment status of a constituent
is computed by the Constituent Alignment Score
(CAS) as follows, where MA stands for ‘medium
</bodyText>
<equation confidence="0.873614333333333">
alignment’.
Count of occurrences(a)
CAS(a) = 5
</equation>
<bodyText confidence="0.978393909090909">
( Occurences of a without MA )
From this score, we can determine the degree of
alignment of a constituent by assigning ‘no align-
ment’ for a constituent with a score of less than
0.25, ‘low alignment’ for a score between 0.25 and
0.5, ‘medium alignment’ for a score between 0.5 and
0.75 and ‘high alignment’ above. On the whole thus,
the agent’s task consists of finding a balance be-
tween choosing the most probable action given the
language model and choosing an action that aligns
with previous utterances.
</bodyText>
<figure confidence="0.9785294">
⎧
⎨
⎩
R=
Average Reward
</figure>
<sectionHeader confidence="0.865712" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.950988">
5.1 Simulated Environment
</subsectionHeader>
<bodyText confidence="0.999958857142857">
The simulated environment encodes information on
the current user type (un-/familiar with the environ-
ment) and corresponding information need (low or
high), the length of the current route (short, medium-
long, long), the next action to perform (turn, go
straight, follow a path, pass a landmark or take note
of a salient landmark) and the current focus of at-
tention (the action to be performed or some salient
landmark nearby). Thus, there are five different state
variables with altogether 120 combinations, sam-
pled from a uniform distribution. This simple form
of stochastic behaviour is used in our simulated en-
vironment. Future work can consider inducing a
learning environment from data.
</bodyText>
<subsectionHeader confidence="0.999298">
5.2 Comparison of learnt and baseline policies
</subsectionHeader>
<bodyText confidence="0.999966615384616">
In order to test our framework, we designed a sim-
ulated environment that simulates different naviga-
tional situations, routes of different lengths and dif-
ferent user types. We trained our HRL agent for
10.000 episodes with the following learning param-
eters: the step-size parameter α was initiated with 1
and then reduced over time by α = &apos;
&apos;�t, t being the
time step. The discount rate parameter γ was 0.99
and the probability of random action c was 0.01 (see
(Sutton and Barto, 1998) for details on these param-
eters). Figure 2 compares the learnt behaviour of
our agent with a baseline (averaged over 10 runs)
that chooses actions at random in models M&apos;0 and
Mo - M7 (i.e., the baseline does not adapt its text
strategy to user type or route length and neither per-
forms adaptation of referring expressions or align-
ment score). The user study reported in (Cuay´ahuitl
et al., 2010) provided users with instruction using
this baseline generation behaviour. The fact that
users had a user satisfaction score of 90% indicates
that this is a sensible baseline, producing intelligi-
ble instructions. We can observe that after a certain
number of episodes, the performance of the trained
agent begins to stabilise and it consistently outper-
forms the baseline.
</bodyText>
<sectionHeader confidence="0.907243" genericHeader="method">
6 Example of generation
</sectionHeader>
<bodyText confidence="0.9996375">
As an example, Figure 3 shows in detail the genera-
tion steps involved in producing the clause ‘Follow
</bodyText>
<figure confidence="0.984951833333333">
−25
−3
−3
Learnt Behaviour
Baseline
−45
−5
−5
−60
−65
102 103 104
Episodes
</figure>
<figureCaption confidence="0.99876">
Figure 2: Comparison of learnt and baseline behaviour in
the generation of route descriptions
</figureCaption>
<bodyText confidence="0.992113621212121">
the corridor until the copyroom’ for an unfamiliar
user and a route of medium length. Generation starts
with the root agent in state (0,0,0,0,0,0), which in-
dicates that text strategy, info structure and instruc-
tion are unfilled slots, the slot in focus of the sen-
tence is an action, the status of subtask termination
is ‘continue’ and the user type is unfamiliar. After
the primitive action expand unmarked theme was
executed, the state is updated to (0,1,0,0,0,0), in-
dicating the filled slot. Next, the composite action
text strategy is executed, corresponding to model
M&apos;0. The initial state (1,0,0) indicates a route
of medium length, an unfilled text strategy slot
and an unfamiliar user. After the primitive ac-
tion expand sequential text was chosen, the ter-
minal state is reached and control is returned to
the root agent. Here, the next action is follow-
ing instruction corresponding to model M&apos;2. The
initial state (0,1,0,0,0,0) here indicates unfilled slots
for following vp, np actor, sentence, path, limit
and relatum, as well as a high information need
of the current user. The required constituents
are expanded in turn. First, the primitive actions
expand limit, expand np actor, expand s and ex-
pand path cause their respective slots in the state
representation to be filled. Next, the composite ac-
tion expand relatum is executed with an initial state
(0,1,0,0) representing an identifiable landmark, un-
filled slots for a determiner and a referring expres-
sion for the landmark and an unfamiliar user. Two
primitive actions, expand relatum identifiable and
expand relatum common name, cause the agent to
−40
reach its terminal state. The generated referring ex-
pression thus treats the referenced entity as either
known or easily recoverable. Finally, model M1
executes the composite action expand following vp,
which is initialised with a number of variables cor-
responding to the alignment status of different verb
forms. Since this is the first time this agent is called,
none of them shows traces of alignment (i.e., all val-
ues are 0). Execution of the primitive action ex-
pand following vp causes the respective slot to be
updated and the agent to terminate. After this sub-
task, model M2 has also reached its terminal state
and control is returned to the root agent.
As a final step towards surface generation, all cho-
sen actions are transformed into an SPL (Kasper,
1989). The type ‘following instruction’ leads to the
initialisation of a semantically underspecified scaf-
fold of an SPL, all other actions serve to supplement
this scaffold to preselect specific syntactic structures
or lexical items. For example, the choice of ‘ex-
pand following vp’ leads to the lexical item ‘fol-
low’ being inserted. Similarly, the choice of ‘ex-
pand path’ leads to the insertion of ‘the corridor’
into the SPL to indicate the path the user should fol-
low. ‘expand limit’, in combination with the choice
of referring expression, leads to the insertion of the
PP ‘until the copy room’. For generation of more
than one instruction, aggregation has to take place.
This is done by iterating over all instructions of a
text and inserting them into a larger SPL that re-
alises the aggregation. Finally, the constructed SPL
is passed to the KPML surface generator (Bateman,
1997) for string realisation.
</bodyText>
<sectionHeader confidence="0.996925" genericHeader="evaluation">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9999883">
We have argued in this paper that HRL is an es-
pecially suited framework for generating texts that
are adaptive to different users, to environmental fea-
tures and properties of surface realisation such as
alignment and variation. While the former tasks ap-
pear intuitively likely to contribute to users’ com-
prehension of texts, it is often not recognised that
the latter task can have the same effect. Differing
surface forms of identical concepts in texts without
motivation can lead to user confusion and deterio-
rate task success. This is supported by Clark’s ‘prin-
ciple of contrast’ (Clark, 1987), according to which
new expressions are only introduced into an interac-
tion when the speaker wishes to contrast them with
other entities already present in the discourse. Si-
miliarly, a study by (Clark and Wilkes-Gibbs, 1986)
showed that interlocutors tend to align their referring
expressions and thereby achieve more efficient and
successful dialogues. We tackled the integration of
different NLG tasks by applying HRL and presented
results, which showed to be promising. As an al-
ternative to RL, other machine learning approaches
may be conceivable. However, supervised learning
requires a large amount of training data, which may
not always be available, and may also produce un-
predictable behaviour in cases where a user deviates
from the behaviour covered by the corpus (Levin
et al., 2000). Both arguments are directly trans-
ferable to NLG. If an agent is able to act only on
grounds of what it has observed in a training cor-
pus, it will not be able to react flexibly to new state
representations. Moreover, it has been argued that
a corpus for NLG cannot be regarded as an equiv-
alent gold standard to the ones of other domains of
NLP (Belz and Reiter, 2006; Scott and Moore, 2006;
Viethen and Dale, 2006). The fact that an expres-
sion for a semantic concept does not appear in a cor-
pus does not mean that it is an unsuited or impos-
sible expression. Another alternative to pure RL is
to apply semi-learnt behaviour, which can be help-
ful for tasks with very large state-action spaces. In
this way, the state-action space is reduced to only
sensible state-action pairs by providing the agent
with prior knowledge of the domain. All remain-
ing behaviour continues to be learnt. (Cuay´ahuitl,
2009) suggests such an approach for learning dia-
logue strategies, but again the principle is transfer-
able to NLG. While there is room for exploration
of different RL methods, it is clear that neither tra-
ditional rule-based accounts of generation, nor n-
gram-based generators can achieve the same flexible
generation behaviour given a large, and partially un-
known, number of state variables. Since state spaces
are typically very large, specifying rules for each
single condition is at best impractical. Especially for
tasks such as achieving a balanced alignment score,
as we have shown in this paper, decisions depend on
very fine-grained textual cues such as patterns of co-
occurrence which are hard to pin down accurately
by hand. On the other hand, statistical approaches
</bodyText>
<figure confidence="0.947677333333333">
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
⎣
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎤
⎦
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
⎢ ⎢ ⎣
</figure>
<equation confidence="0.996376371428571">
M00 (0, 0, 0, 0, 0, 0)
{action = expand unmarked
theme}
M00 (0, 1, 0, 0, 0, 0)
{action = text strategy}
M00, (1, 1, 0, 0, 0, 0),
{action = following instruction}
M00 (1, 1, 1, 0, 1, 0)(terminalstate)
M,0 (1, 0, 0)
{action = expand sequential text}
M,0 (1, 1, 0), (terminalstate)
Mi (0, 1, 0, 0, 0, 0)
{action = expand limit}
Mi (0, 1, 1, 0, 0, 0)
{action = expand np actor}
Mi (0, 1, 1, 1, 0, 0)
{action = expand s}
Mi (0, 1, 1, 0, 1, 0)
{action = expand path}
Mi (0, 1, 1, 1, 1, 0)
{action = expand relatum}
Mi (0, 1, 1, 1, 1, 0)
{action = expand following
vp}
Mi (1, 1, 1, 1, 1, 1)(terminalstate)
M3 (0, 0, 0, 0)
{action = expand relatum
identifiable}
M3 (0, 1, 0, 0)
{action = expand relatum
common name}
M3 (0, 1, 1, 0), (terminalstate)
Mi(0, 0, 0, 0, 0, 0, 0, 0, 0)
{action = follow}
Mi (1, 0, 0, 0, 0, 0, 0, 0, 0)
</equation>
<figure confidence="0.935443">
(terminalstate)
⎤
⎦⎥⎥
⎤
⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
⎤
⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
⎤
⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
</figure>
<figureCaption confidence="0.991801">
Figure 3: Example ofgeneration for the clause ‘Follow the corridor until the copy room’. This example shows decision
makingfor a single instruction, adaptation and alignment occurs over longer sequences of text.
</figureCaption>
<bodyText confidence="0.999810909090909">
to generation that are based on n-grams focus on the
frequency of constructions in a corpus without tak-
ing contextual variables such as user type or environ-
mental properties into account. Further, they share
the problem of supervised learning approaches dis-
cussed above, namely, that it can act only on grounds
of what it has observed in the past, and are not well
able to adapt to novel situations. For a more de-
tailed account of statistical and trainable approaches
to NLG as well as their advantages and drawbacks,
see (Lemon, 2008).
</bodyText>
<sectionHeader confidence="0.997806" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999990529411765">
We presented a novel approach to text generation
that applies hierarchical reinforcement learning to
optimise the following interrelated NLG tasks: con-
tent selection, choice of text structure, referring ex-
pressions and surface structure. Generation deci-
sions in these areas were learnt based on three differ-
ent variables: the type of user, the properties of the
spatial environment and the proportion of alignment
and variation in texts. Based on a simulated envi-
ronment, we compared the results of different poli-
cies and demonstrated that the learnt policy outper-
forms a baseline that chooses actions without taking
contextual variables into account. Future work can
transfer our approach to different domains of appli-
cation or to other NLG tasks. In addition, our pre-
liminary simulation results should be confirmed in
an evaluation study with real users.
</bodyText>
<sectionHeader confidence="0.9956" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9841755">
This work was partly supported by DFG SFB/TR8
“Spatial Cognition”.
</bodyText>
<sectionHeader confidence="0.995763" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999810515151515">
Barto, A. G. and Mahadevan, S. (2003). Recent Ad-
vances in Hierarchical Reinforcement Learning. Dis-
crete Event Dynamic Systems, 13:2003.
Bateman, J. A. (1997). Enabling technology for multi-
lingual natural language generation: the KPML devel-
opment environment. Natural Language Engineering,
3(1):15–55.
Belz, A. and Reiter, E. (2006). Comparing automatic and
human evaluation of nlg systems. In In Proc. EACL06,
pages 313–320.
Bock, K. (1986). Syntactic persistence in language pro-
duction. Cognitive Psychology, 18.
Branigan, H. P., Pickering, M. J., and Cleland, A. (2000).
Syntactic coordination in dialogue. Cognition, 75.
Clark, E. (1987). The principle of contrast: A constraint
on language acquisition. In MacWhinney, B., edi-
tor, Mechanisms ofLanguage Acquisition, pages 1–33.
Lawrence Erlbaum Assoc., Hillsdale, NJ.
Clark, H. H. and Wilkes-Gibbs, D. (1986). Referring as
a colloborative process. Cognition, 22.
Cuay´ahuitl, H. (2009). Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD thesis,
School of Informatics, University of Edinburgh.
Cuay´ahuitl, H., Dethlefs, N., Richter, K.-F., Tenbrink, T.,
and Bateman, J. (2010). A dialogue system for indoor
wayfinding using text-based natural language. In-
ternational Journal of Computational Linguistics and
Applications, ISSN 0976-0962.
Cuayahuitl, H., Renals, S., Lemon, O., and Shimodaira,
H. (2010). Evaluation of a hierarchical reinforcement
learning spoken dialogue system. Computer Speech
and Language, 24(2):395–429.
Dietterich, T. G. (1999). Hierarchical reinforcement
learning with the maxq value function decomposition.
Journal of Artificial Intelligence Research, 13:227–
303.
Garrod, S. and Anderson, A. (1987). Saying What You
Mean in Dialogue: A Study in conceptual and seman-
tic co-ordination. Cognition, 27.
Halliday, M. A. K. and Hasan, R. (1976). Cohesion in
English. Longman, London.
Halliday, M. A. K. and Matthiessen, C. M. I. M. (2004).
An Introduction to Functional Grammar. Edward
Arnold, London, 3rd edition.
Hirst, G. and St-Onge, D. (1998). Lexical chains as rep-
resentations of context for the detection and correction
of malapropisms. In Fellbaum, C., editor, WordNet:
An Electronic Database and Some of its Applications,
pages 305–332. MIT Press.
Janarthanam, S. and Lemon, O. (2009). Learning lexi-
cal alignment policies for generating referring expres-
sions in spoken dialogue systems. In ENLG ’09: Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 74–81, Morristown, NJ,
USA.
Kasper, R. (1989). SPL: A Sentence Plan Language for
text generation. Technical report, USC/ISI.
Langkilde, I. and Knight, K. (1998). Generation that ex-
ploits corpus-based statistical knowledge. In ACL-36:
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics,
pages 704–710.
Lemon, O. (2008). Adaptive Natural Language Gener-
ation in Dialogue using Reinforcement Learning. In
SemDial.
Levin, E., Pieraccini, R., and Eckert, W. (2000). A
stochastic model of computer-human interaction for
learning dialogue strategies. IEEE Transactions on
Speech and Audio Processing, 8.
Pickering, M. J. and Garrod, S. (2004). Toward a mecha-
nistc psychology of dialog. Behavioral and Brain Sci-
ences, 27.
Reiter, E. and Dale, R. (1997). Building applied natural
language generation systems. Natural Language En-
gineering, 3(1):57–87.
Rieser, V. and Lemon, O. (2009). Natural language gen-
eration as planning under uncertainty for spoken dia-
logue systems. In EACL ’09: Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 683–691,
Morristown, NJ, USA.
Scott, D. and Moore, J. (2006). An NLG evaluation com-
petition? eight reasons to be cautious. Technical re-
port.
Sutton, R. S. and Barto, A. G. (1998). Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA, USA.
Toutanova, K. and Manning, C. D. (2000). Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natural
language processing and very large corpora, pages
63–70, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Viethen, J. and Dale, R. (2006). Towards the evaluation
of referring expression generation. In In Proceedings
of the 4th Australiasian Language Technology Work-
shop, pages 115–122.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.842172">
<title confidence="0.999957">Hierarchical Reinforcement Learning for Adaptive Text Generation</title>
<author confidence="0.999804">Nina Dethlefs</author>
<affiliation confidence="0.98587">University of Bremen, Germany</affiliation>
<email confidence="0.9944">dethlefs@uni-bremen.de</email>
<author confidence="0.921757">Heriberto</author>
<affiliation confidence="0.999867">University of Bremen,</affiliation>
<email confidence="0.99622">heriberto@uni-bremen.de</email>
<abstract confidence="0.99676285">We present a novel approach to natural language generation (NLG) that applies hierarchical reinforcement learning to text generation in the wayfinding domain. Our approach aims to optimise the integration of NLG tasks that are inherently different in nature, such as decisions of content selection, text structure, user modelling, referring expression generation (REG), and surface realisation. It also aims to capture existing interdependencies between these areas. We apply hierarchical reinforcement learning to learn a generation policy that captures these interdependencies, and that can be transferred to other NLG tasks. Our experimental results—in a simulated environment—show that the learnt wayfinding policy outperforms a baseline policy that takes reasonable actions but without optimization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A G Barto</author>
<author>S Mahadevan</author>
</authors>
<date>2003</date>
<booktitle>Recent Advances in Hierarchical Reinforcement Learning. Discrete Event Dynamic Systems,</booktitle>
<pages>13--2003</pages>
<contexts>
<context position="3671" citStr="Barto and Mahadevan, 2003" startWordPosition="567" endWordPosition="570">des (Janarthanam and Lemon, 2009) who employed it for alignment of referring expressions based on user models. Also, (Lemon, 2008; Rieser and Lemon, 2009) used RL for optimising information presentation styles for search results. While both approaches displayed significant effects of adaptation, they focused on a single area of optimisation. For larger problems, however, such as the one we are aiming to solve, flat RL will not be applicable due to the large state space. We therefore suggest to divide the problem into a number of subproblems and apply hierarchical reinforcement learning (HRL) (Barto and Mahadevan, 2003) to solve it. We describe our problem in more detail in Section 2, our proposed HRL architecture in Sections 3 and 4 and present some results in Section 5. We show that our learnt policies outperform a baseline that does not adapt to contextual features. 2 Generation tasks Our experiments are all drawn from an indoor navigation dialogue system which provides users with route instructions in a university building and is described in (Cuay´ahuitl et al., 2010). We aim to optimise generation within the areas of content selection, text structure, referring expression generation and surface realisa</context>
</contexts>
<marker>Barto, Mahadevan, 2003</marker>
<rawString>Barto, A. G. and Mahadevan, S. (2003). Recent Advances in Hierarchical Reinforcement Learning. Discrete Event Dynamic Systems, 13:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bateman</author>
</authors>
<title>Enabling technology for multilingual natural language generation: the KPML development environment.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="24240" citStr="Bateman, 1997" startWordPosition="3989" endWordPosition="3990">lowing vp’ leads to the lexical item ‘follow’ being inserted. Similarly, the choice of ‘expand path’ leads to the insertion of ‘the corridor’ into the SPL to indicate the path the user should follow. ‘expand limit’, in combination with the choice of referring expression, leads to the insertion of the PP ‘until the copy room’. For generation of more than one instruction, aggregation has to take place. This is done by iterating over all instructions of a text and inserting them into a larger SPL that realises the aggregation. Finally, the constructed SPL is passed to the KPML surface generator (Bateman, 1997) for string realisation. 7 Discussion We have argued in this paper that HRL is an especially suited framework for generating texts that are adaptive to different users, to environmental features and properties of surface realisation such as alignment and variation. While the former tasks appear intuitively likely to contribute to users’ comprehension of texts, it is often not recognised that the latter task can have the same effect. Differing surface forms of identical concepts in texts without motivation can lead to user confusion and deteriorate task success. This is supported by Clark’s ‘pr</context>
</contexts>
<marker>Bateman, 1997</marker>
<rawString>Bateman, J. A. (1997). Enabling technology for multilingual natural language generation: the KPML development environment. Natural Language Engineering, 3(1):15–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of nlg systems.</title>
<date>2006</date>
<booktitle>In In Proc. EACL06,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="26037" citStr="Belz and Reiter, 2006" startWordPosition="4286" endWordPosition="4289">nceivable. However, supervised learning requires a large amount of training data, which may not always be available, and may also produce unpredictable behaviour in cases where a user deviates from the behaviour covered by the corpus (Levin et al., 2000). Both arguments are directly transferable to NLG. If an agent is able to act only on grounds of what it has observed in a training corpus, it will not be able to react flexibly to new state representations. Moreover, it has been argued that a corpus for NLG cannot be regarded as an equivalent gold standard to the ones of other domains of NLP (Belz and Reiter, 2006; Scott and Moore, 2006; Viethen and Dale, 2006). The fact that an expression for a semantic concept does not appear in a corpus does not mean that it is an unsuited or impossible expression. Another alternative to pure RL is to apply semi-learnt behaviour, which can be helpful for tasks with very large state-action spaces. In this way, the state-action space is reduced to only sensible state-action pairs by providing the agent with prior knowledge of the domain. All remaining behaviour continues to be learnt. (Cuay´ahuitl, 2009) suggests such an approach for learning dialogue strategies, but </context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Belz, A. and Reiter, E. (2006). Comparing automatic and human evaluation of nlg systems. In In Proc. EACL06, pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bock</author>
</authors>
<title>Syntactic persistence in language production.</title>
<date>1986</date>
<journal>Cognitive Psychology,</journal>
<volume>18</volume>
<contexts>
<context position="6590" citStr="Bock, 1986" startWordPosition="1033" endWordPosition="1034">t is a rule of writing that texts should typically contain variation of surface forms in order not to appear repetitive and stylistically poor, there is evidence that humans also get influenced by self-alignment processes during language production. Specifically, (Garrod and Anderson, 1987; Pickering and Garrod, 2004) argue that the same mental representations are used during language production and comprehension, so that alignment occurs regardless of whether the last utterance was made by another person or by the speaker him- or herself (for experimental evidence see (Branigan et al., 2000; Bock, 1986)). We can therefore hypothesise that coherent texts will, besides variation, also display a certain degree of selfalignment. In order to determine a proper balance of alignment and variation, we computed the degree of lexical repetition from our corpus of 24 human route descriptions. This analysis was based on (Hirst and St-Onge, 1998) who retrieve lexical chains from texts by identifying a number of relations between lexical items. We focus here exclusively on Hirst &amp; St-Onge’s ‘extra-strong’ relations, since these can be computed from shallow properties of texts and do not require a large co</context>
</contexts>
<marker>Bock, 1986</marker>
<rawString>Bock, K. (1986). Syntactic persistence in language production. Cognitive Psychology, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Branigan</author>
<author>M J Pickering</author>
<author>A Cleland</author>
</authors>
<title>Syntactic coordination in dialogue.</title>
<date>2000</date>
<journal>Cognition,</journal>
<volume>75</volume>
<contexts>
<context position="6577" citStr="Branigan et al., 2000" startWordPosition="1029" endWordPosition="1032"> and variation. While it is a rule of writing that texts should typically contain variation of surface forms in order not to appear repetitive and stylistically poor, there is evidence that humans also get influenced by self-alignment processes during language production. Specifically, (Garrod and Anderson, 1987; Pickering and Garrod, 2004) argue that the same mental representations are used during language production and comprehension, so that alignment occurs regardless of whether the last utterance was made by another person or by the speaker him- or herself (for experimental evidence see (Branigan et al., 2000; Bock, 1986)). We can therefore hypothesise that coherent texts will, besides variation, also display a certain degree of selfalignment. In order to determine a proper balance of alignment and variation, we computed the degree of lexical repetition from our corpus of 24 human route descriptions. This analysis was based on (Hirst and St-Onge, 1998) who retrieve lexical chains from texts by identifying a number of relations between lexical items. We focus here exclusively on Hirst &amp; St-Onge’s ‘extra-strong’ relations, since these can be computed from shallow properties of texts and do not requi</context>
</contexts>
<marker>Branigan, Pickering, Cleland, 2000</marker>
<rawString>Branigan, H. P., Pickering, M. J., and Cleland, A. (2000). Syntactic coordination in dialogue. Cognition, 75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Clark</author>
</authors>
<title>The principle of contrast: A constraint on language acquisition.</title>
<date>1987</date>
<booktitle>Mechanisms ofLanguage Acquisition,</booktitle>
<pages>1--33</pages>
<editor>In MacWhinney, B., editor,</editor>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="24874" citStr="Clark, 1987" startWordPosition="4092" endWordPosition="4093">n. 7 Discussion We have argued in this paper that HRL is an especially suited framework for generating texts that are adaptive to different users, to environmental features and properties of surface realisation such as alignment and variation. While the former tasks appear intuitively likely to contribute to users’ comprehension of texts, it is often not recognised that the latter task can have the same effect. Differing surface forms of identical concepts in texts without motivation can lead to user confusion and deteriorate task success. This is supported by Clark’s ‘principle of contrast’ (Clark, 1987), according to which new expressions are only introduced into an interaction when the speaker wishes to contrast them with other entities already present in the discourse. Similiarly, a study by (Clark and Wilkes-Gibbs, 1986) showed that interlocutors tend to align their referring expressions and thereby achieve more efficient and successful dialogues. We tackled the integration of different NLG tasks by applying HRL and presented results, which showed to be promising. As an alternative to RL, other machine learning approaches may be conceivable. However, supervised learning requires a large a</context>
</contexts>
<marker>Clark, 1987</marker>
<rawString>Clark, E. (1987). The principle of contrast: A constraint on language acquisition. In MacWhinney, B., editor, Mechanisms ofLanguage Acquisition, pages 1–33. Lawrence Erlbaum Assoc., Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>D Wilkes-Gibbs</author>
</authors>
<title>Referring as a colloborative process.</title>
<date>1986</date>
<journal>Cognition,</journal>
<volume>22</volume>
<contexts>
<context position="25099" citStr="Clark and Wilkes-Gibbs, 1986" startWordPosition="4126" endWordPosition="4129">on such as alignment and variation. While the former tasks appear intuitively likely to contribute to users’ comprehension of texts, it is often not recognised that the latter task can have the same effect. Differing surface forms of identical concepts in texts without motivation can lead to user confusion and deteriorate task success. This is supported by Clark’s ‘principle of contrast’ (Clark, 1987), according to which new expressions are only introduced into an interaction when the speaker wishes to contrast them with other entities already present in the discourse. Similiarly, a study by (Clark and Wilkes-Gibbs, 1986) showed that interlocutors tend to align their referring expressions and thereby achieve more efficient and successful dialogues. We tackled the integration of different NLG tasks by applying HRL and presented results, which showed to be promising. As an alternative to RL, other machine learning approaches may be conceivable. However, supervised learning requires a large amount of training data, which may not always be available, and may also produce unpredictable behaviour in cases where a user deviates from the behaviour covered by the corpus (Levin et al., 2000). Both arguments are directly</context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>Clark, H. H. and Wilkes-Gibbs, D. (1986). Referring as a colloborative process. Cognition, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cuay´ahuitl</author>
</authors>
<title>Hierarchical Reinforcement Learning for Spoken Dialogue Systems.</title>
<date>2009</date>
<tech>PhD thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<marker>Cuay´ahuitl, 2009</marker>
<rawString>Cuay´ahuitl, H. (2009). Hierarchical Reinforcement Learning for Spoken Dialogue Systems. PhD thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cuay´ahuitl</author>
<author>N Dethlefs</author>
<author>K-F Richter</author>
<author>T Tenbrink</author>
<author>J Bateman</author>
</authors>
<title>A dialogue system for indoor wayfinding using text-based natural language.</title>
<date>2010</date>
<journal>International Journal of Computational Linguistics and Applications, ISSN</journal>
<pages>0976--0962</pages>
<marker>Cuay´ahuitl, Dethlefs, Richter, Tenbrink, Bateman, 2010</marker>
<rawString>Cuay´ahuitl, H., Dethlefs, N., Richter, K.-F., Tenbrink, T., and Bateman, J. (2010). A dialogue system for indoor wayfinding using text-based natural language. International Journal of Computational Linguistics and Applications, ISSN 0976-0962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cuayahuitl</author>
<author>S Renals</author>
<author>O Lemon</author>
<author>H Shimodaira</author>
</authors>
<title>Evaluation of a hierarchical reinforcement learning spoken dialogue system.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="11664" citStr="Cuayahuitl et al., 2010" startWordPosition="1892" endWordPosition="1895">s are as follows: when an SMDP terminates its execution, it is popped off the stack of models to execute, and control is transferred to the next available SMDP in the stack, and so on until popping off the root SMDP. An SMDP terminates when it reaches one of its terminal states. This algorithm is executed until the Qvalues of the root agent stabilize. The hierarchical decomposition allows to find context-independent policies with the advantages of policy reuse and facilitation for state-action abstraction. This hierarchical approach has been applied successfully to dialogue strategy learning (Cuayahuitl et al., 2010). 4 Experimental Setting 4.1 Hierarchy of SMDPs The hierarchy consists of 15 agents. It is depicted in Figure 1. The root agent is responsible for deterπ *j(s) = arg `( ) max a�A Q s, a (2) Figure 1: Hierarchy of agents for learning adaptive text generation strategies in the wayfinding domain mining a route instruction type for a navigation situation. We distinguish turning, passing, locating, going and following instructions. It also chooses a text generation strategy and the information structure of the clause (i.e., marked or unmarked theme (Halliday and Matthiessen, 2004)). Leaf agents are</context>
</contexts>
<marker>Cuayahuitl, Renals, Lemon, Shimodaira, 2010</marker>
<rawString>Cuayahuitl, H., Renals, S., Lemon, O., and Shimodaira, H. (2010). Evaluation of a hierarchical reinforcement learning spoken dialogue system. Computer Speech and Language, 24(2):395–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Hierarchical reinforcement learning with the maxq value function decomposition.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>13</volume>
<pages>303</pages>
<contexts>
<context position="11018" citStr="Dietterich, 1999" startWordPosition="1788" endWordPosition="1789">ake a variable number of time steps to complete, the random variable τ represents this number of time steps. Actions can be either primitive or composite. The former yield single rewards, the latter (executed using a stack mechanism) correspond to SMDPs and yield cumulative discounted rewards. The goal of each SMDP is to find an optional policy π* that maximises the reward for each visited state, according to where Qij(s, a) specifies the expected cumulative reward for executing action a in state s and then following π*. For learning a generation policy, we use hierarchical Q-Learning (HSMQ) (Dietterich, 1999). The dynamics of SMDPs are as follows: when an SMDP terminates its execution, it is popped off the stack of models to execute, and control is transferred to the next available SMDP in the stack, and so on until popping off the root SMDP. An SMDP terminates when it reaches one of its terminal states. This algorithm is executed until the Qvalues of the root agent stabilize. The hierarchical decomposition allows to find context-independent policies with the advantages of policy reuse and facilitation for state-action abstraction. This hierarchical approach has been applied successfully to dialog</context>
</contexts>
<marker>Dietterich, 1999</marker>
<rawString>Dietterich, T. G. (1999). Hierarchical reinforcement learning with the maxq value function decomposition. Journal of Artificial Intelligence Research, 13:227– 303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Garrod</author>
<author>A Anderson</author>
</authors>
<title>Saying What You Mean in Dialogue: A Study in conceptual and semantic co-ordination.</title>
<date>1987</date>
<journal>Cognition,</journal>
<volume>27</volume>
<contexts>
<context position="6269" citStr="Garrod and Anderson, 1987" startWordPosition="981" endWordPosition="984">he users’ prior knowledge. For example, one and the same room can be called either ‘the student union room’, ‘room A3530’ or ‘the room right at the corner beside the entrance to the terrace’. Surface Realisation For surface realisation, we aim to generate texts that display a natural balance of (self-)alignment and variation. While it is a rule of writing that texts should typically contain variation of surface forms in order not to appear repetitive and stylistically poor, there is evidence that humans also get influenced by self-alignment processes during language production. Specifically, (Garrod and Anderson, 1987; Pickering and Garrod, 2004) argue that the same mental representations are used during language production and comprehension, so that alignment occurs regardless of whether the last utterance was made by another person or by the speaker him- or herself (for experimental evidence see (Branigan et al., 2000; Bock, 1986)). We can therefore hypothesise that coherent texts will, besides variation, also display a certain degree of selfalignment. In order to determine a proper balance of alignment and variation, we computed the degree of lexical repetition from our corpus of 24 human route descript</context>
</contexts>
<marker>Garrod, Anderson, 1987</marker>
<rawString>Garrod, S. and Anderson, A. (1987). Saying What You Mean in Dialogue: A Study in conceptual and semantic co-ordination. Cognition, 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="2187" citStr="Halliday and Hasan, 1976" startWordPosition="324" endWordPosition="327">undesirable to treat them all as isolated modules. In this paper, we focus on interrelated decision making in the areas of content selection, choice of text structure, referring expression and surface form. Concretely, we generate route instructions that are tailored specifically towards different user types as well as different environmental features. In addition, we aim to balance the degree of variation and alignment in texts and produce lexical and syntactic patterns of co-occurrence that resemble those of human texts of the same domain. Evidence for the importance of this is provided by (Halliday and Hasan, 1976) who note the way that lexical cohesive ties contribute to text coherence as well as by the theory of interactive alignment. According to (Pickering and Garrod, 2004) we would expect significant traces of lexical and syntactic selfalignment in texts. Approaches to NLG in the past have been either rule-based (Reiter and Dale, 1997) or statistical (Langkilde and Knight, 1998). However, the former relies on a large number of hand-crafted rules, which makes it infeasible for controlling a large number of interrelated variables. The latter typically requires training on a large corpus of the domain</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, M. A. K. and Hasan, R. (1976). Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>C M I M Matthiessen</author>
</authors>
<title>An Introduction to Functional Grammar. Edward Arnold, London, 3rd edition.</title>
<date>2004</date>
<contexts>
<context position="12246" citStr="Halliday and Matthiessen, 2004" startWordPosition="1988" endWordPosition="1992">logue strategy learning (Cuayahuitl et al., 2010). 4 Experimental Setting 4.1 Hierarchy of SMDPs The hierarchy consists of 15 agents. It is depicted in Figure 1. The root agent is responsible for deterπ *j(s) = arg `( ) max a�A Q s, a (2) Figure 1: Hierarchy of agents for learning adaptive text generation strategies in the wayfinding domain mining a route instruction type for a navigation situation. We distinguish turning, passing, locating, going and following instructions. It also chooses a text generation strategy and the information structure of the clause (i.e., marked or unmarked theme (Halliday and Matthiessen, 2004)). Leaf agents are responsible for expanding constituents in which variation or alignment can occur, e.g. the choice of verb or prepositional phrase. 4.2 State and action sets We distinguish three kinds of state representations, displayed in Table 2. The first (M010 and M10) encodes information on the spatial environment and user type so that texts can be tailored towards these variables. These variables play a major part in our simulated environment (Section 5.1). The second representation (Mi - M15 and M23) controls sentence structure and ensures that all required constituents for a message </context>
</contexts>
<marker>Halliday, Matthiessen, 2004</marker>
<rawString>Halliday, M. A. K. and Matthiessen, C. M. I. M. (2004). An Introduction to Functional Grammar. Edward Arnold, London, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Database and Some of its Applications,</booktitle>
<pages>305--332</pages>
<editor>In Fellbaum, C., editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6927" citStr="Hirst and St-Onge, 1998" startWordPosition="1085" endWordPosition="1088">e that the same mental representations are used during language production and comprehension, so that alignment occurs regardless of whether the last utterance was made by another person or by the speaker him- or herself (for experimental evidence see (Branigan et al., 2000; Bock, 1986)). We can therefore hypothesise that coherent texts will, besides variation, also display a certain degree of selfalignment. In order to determine a proper balance of alignment and variation, we computed the degree of lexical repetition from our corpus of 24 human route descriptions. This analysis was based on (Hirst and St-Onge, 1998) who retrieve lexical chains from texts by identifying a number of relations between lexical items. We focus here exclusively on Hirst &amp; St-Onge’s ‘extra-strong’ relations, since these can be computed from shallow properties of texts and do not require a large corpus of the target domain. In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000)1 to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. Based on these categories, </context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Hirst, G. and St-Onge, D. (1998). Lexical chains as representations of context for the detection and correction of malapropisms. In Fellbaum, C., editor, WordNet: An Electronic Database and Some of its Applications, pages 305–332. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>Learning lexical alignment policies for generating referring expressions in spoken dialogue systems.</title>
<date>2009</date>
<booktitle>In ENLG ’09: Proceedings of the 12th European Workshop on Natural Language Generation,</booktitle>
<pages>74--81</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3078" citStr="Janarthanam and Lemon, 2009" startWordPosition="470" endWordPosition="473">o NLG in the past have been either rule-based (Reiter and Dale, 1997) or statistical (Langkilde and Knight, 1998). However, the former relies on a large number of hand-crafted rules, which makes it infeasible for controlling a large number of interrelated variables. The latter typically requires training on a large corpus of the domain. While these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying Reinforcement Learning (RL)—with a hierarchical approach. Previous work that has used RL for NLG includes (Janarthanam and Lemon, 2009) who employed it for alignment of referring expressions based on user models. Also, (Lemon, 2008; Rieser and Lemon, 2009) used RL for optimising information presentation styles for search results. While both approaches displayed significant effects of adaptation, they focused on a single area of optimisation. For larger problems, however, such as the one we are aiming to solve, flat RL will not be applicable due to the large state space. We therefore suggest to divide the problem into a number of subproblems and apply hierarchical reinforcement learning (HRL) (Barto and Mahadevan, 2003) to sol</context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>Janarthanam, S. and Lemon, O. (2009). Learning lexical alignment policies for generating referring expressions in spoken dialogue systems. In ENLG ’09: Proceedings of the 12th European Workshop on Natural Language Generation, pages 74–81, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kasper</author>
</authors>
<title>SPL: A Sentence Plan Language for text generation.</title>
<date>1989</date>
<tech>Technical report, USC/ISI.</tech>
<contexts>
<context position="23359" citStr="Kasper, 1989" startWordPosition="3843" endWordPosition="3844">xecutes the composite action expand following vp, which is initialised with a number of variables corresponding to the alignment status of different verb forms. Since this is the first time this agent is called, none of them shows traces of alignment (i.e., all values are 0). Execution of the primitive action expand following vp causes the respective slot to be updated and the agent to terminate. After this subtask, model M2 has also reached its terminal state and control is returned to the root agent. As a final step towards surface generation, all chosen actions are transformed into an SPL (Kasper, 1989). The type ‘following instruction’ leads to the initialisation of a semantically underspecified scaffold of an SPL, all other actions serve to supplement this scaffold to preselect specific syntactic structures or lexical items. For example, the choice of ‘expand following vp’ leads to the lexical item ‘follow’ being inserted. Similarly, the choice of ‘expand path’ leads to the insertion of ‘the corridor’ into the SPL to indicate the path the user should follow. ‘expand limit’, in combination with the choice of referring expression, leads to the insertion of the PP ‘until the copy room’. For g</context>
</contexts>
<marker>Kasper, 1989</marker>
<rawString>Kasper, R. (1989). SPL: A Sentence Plan Language for text generation. Technical report, USC/ISI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In ACL-36: Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="2563" citStr="Langkilde and Knight, 1998" startWordPosition="387" endWordPosition="390"> to balance the degree of variation and alignment in texts and produce lexical and syntactic patterns of co-occurrence that resemble those of human texts of the same domain. Evidence for the importance of this is provided by (Halliday and Hasan, 1976) who note the way that lexical cohesive ties contribute to text coherence as well as by the theory of interactive alignment. According to (Pickering and Garrod, 2004) we would expect significant traces of lexical and syntactic selfalignment in texts. Approaches to NLG in the past have been either rule-based (Reiter and Dale, 1997) or statistical (Langkilde and Knight, 1998). However, the former relies on a large number of hand-crafted rules, which makes it infeasible for controlling a large number of interrelated variables. The latter typically requires training on a large corpus of the domain. While these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying Reinforcement Learning (RL)—with a hierarchical approach. Previous work that has used RL for NLG includes (Janarthanam and Lemon, 2009) who employed it for alignment of referring expressions based on user models. Also, (</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, I. and Knight, K. (1998). Generation that exploits corpus-based statistical knowledge. In ACL-36: Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
</authors>
<title>Adaptive Natural Language Generation in Dialogue using Reinforcement Learning.</title>
<date>2008</date>
<booktitle>In SemDial.</booktitle>
<contexts>
<context position="3174" citStr="Lemon, 2008" startWordPosition="488" endWordPosition="489">. However, the former relies on a large number of hand-crafted rules, which makes it infeasible for controlling a large number of interrelated variables. The latter typically requires training on a large corpus of the domain. While these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying Reinforcement Learning (RL)—with a hierarchical approach. Previous work that has used RL for NLG includes (Janarthanam and Lemon, 2009) who employed it for alignment of referring expressions based on user models. Also, (Lemon, 2008; Rieser and Lemon, 2009) used RL for optimising information presentation styles for search results. While both approaches displayed significant effects of adaptation, they focused on a single area of optimisation. For larger problems, however, such as the one we are aiming to solve, flat RL will not be applicable due to the large state space. We therefore suggest to divide the problem into a number of subproblems and apply hierarchical reinforcement learning (HRL) (Barto and Mahadevan, 2003) to solve it. We describe our problem in more detail in Section 2, our proposed HRL architecture in Sec</context>
<context position="29422" citStr="Lemon, 2008" startWordPosition="5054" endWordPosition="5055"> instruction, adaptation and alignment occurs over longer sequences of text. to generation that are based on n-grams focus on the frequency of constructions in a corpus without taking contextual variables such as user type or environmental properties into account. Further, they share the problem of supervised learning approaches discussed above, namely, that it can act only on grounds of what it has observed in the past, and are not well able to adapt to novel situations. For a more detailed account of statistical and trainable approaches to NLG as well as their advantages and drawbacks, see (Lemon, 2008). 8 Conclusion We presented a novel approach to text generation that applies hierarchical reinforcement learning to optimise the following interrelated NLG tasks: content selection, choice of text structure, referring expressions and surface structure. Generation decisions in these areas were learnt based on three different variables: the type of user, the properties of the spatial environment and the proportion of alignment and variation in texts. Based on a simulated environment, we compared the results of different policies and demonstrated that the learnt policy outperforms a baseline that</context>
</contexts>
<marker>Lemon, 2008</marker>
<rawString>Lemon, O. (2008). Adaptive Natural Language Generation in Dialogue using Reinforcement Learning. In SemDial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A stochastic model of computer-human interaction for learning dialogue strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<contexts>
<context position="25670" citStr="Levin et al., 2000" startWordPosition="4215" endWordPosition="4218">iarly, a study by (Clark and Wilkes-Gibbs, 1986) showed that interlocutors tend to align their referring expressions and thereby achieve more efficient and successful dialogues. We tackled the integration of different NLG tasks by applying HRL and presented results, which showed to be promising. As an alternative to RL, other machine learning approaches may be conceivable. However, supervised learning requires a large amount of training data, which may not always be available, and may also produce unpredictable behaviour in cases where a user deviates from the behaviour covered by the corpus (Levin et al., 2000). Both arguments are directly transferable to NLG. If an agent is able to act only on grounds of what it has observed in a training corpus, it will not be able to react flexibly to new state representations. Moreover, it has been argued that a corpus for NLG cannot be regarded as an equivalent gold standard to the ones of other domains of NLP (Belz and Reiter, 2006; Scott and Moore, 2006; Viethen and Dale, 2006). The fact that an expression for a semantic concept does not appear in a corpus does not mean that it is an unsuited or impossible expression. Another alternative to pure RL is to appl</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>Levin, E., Pieraccini, R., and Eckert, W. (2000). A stochastic model of computer-human interaction for learning dialogue strategies. IEEE Transactions on Speech and Audio Processing, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Pickering</author>
<author>S Garrod</author>
</authors>
<title>Toward a mechanistc psychology of dialog.</title>
<date>2004</date>
<journal>Behavioral and Brain Sciences,</journal>
<volume>27</volume>
<contexts>
<context position="2353" citStr="Pickering and Garrod, 2004" startWordPosition="352" endWordPosition="355">re, referring expression and surface form. Concretely, we generate route instructions that are tailored specifically towards different user types as well as different environmental features. In addition, we aim to balance the degree of variation and alignment in texts and produce lexical and syntactic patterns of co-occurrence that resemble those of human texts of the same domain. Evidence for the importance of this is provided by (Halliday and Hasan, 1976) who note the way that lexical cohesive ties contribute to text coherence as well as by the theory of interactive alignment. According to (Pickering and Garrod, 2004) we would expect significant traces of lexical and syntactic selfalignment in texts. Approaches to NLG in the past have been either rule-based (Reiter and Dale, 1997) or statistical (Langkilde and Knight, 1998). However, the former relies on a large number of hand-crafted rules, which makes it infeasible for controlling a large number of interrelated variables. The latter typically requires training on a large corpus of the domain. While these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying Reinforcem</context>
<context position="6298" citStr="Pickering and Garrod, 2004" startWordPosition="985" endWordPosition="988">For example, one and the same room can be called either ‘the student union room’, ‘room A3530’ or ‘the room right at the corner beside the entrance to the terrace’. Surface Realisation For surface realisation, we aim to generate texts that display a natural balance of (self-)alignment and variation. While it is a rule of writing that texts should typically contain variation of surface forms in order not to appear repetitive and stylistically poor, there is evidence that humans also get influenced by self-alignment processes during language production. Specifically, (Garrod and Anderson, 1987; Pickering and Garrod, 2004) argue that the same mental representations are used during language production and comprehension, so that alignment occurs regardless of whether the last utterance was made by another person or by the speaker him- or herself (for experimental evidence see (Branigan et al., 2000; Bock, 1986)). We can therefore hypothesise that coherent texts will, besides variation, also display a certain degree of selfalignment. In order to determine a proper balance of alignment and variation, we computed the degree of lexical repetition from our corpus of 24 human route descriptions. This analysis was based</context>
</contexts>
<marker>Pickering, Garrod, 2004</marker>
<rawString>Pickering, M. J. and Garrod, S. (2004). Toward a mechanistc psychology of dialog. Behavioral and Brain Sciences, 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building applied natural language generation systems.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="1117" citStr="Reiter and Dale, 1997" startWordPosition="153" endWordPosition="156">f content selection, text structure, user modelling, referring expression generation (REG), and surface realisation. It also aims to capture existing interdependencies between these areas. We apply hierarchical reinforcement learning to learn a generation policy that captures these interdependencies, and that can be transferred to other NLG tasks. Our experimental results—in a simulated environment—show that the learnt wayfinding policy outperforms a baseline policy that takes reasonable actions but without optimization. 1 Introduction Automatic text generation involves a number of subtasks. (Reiter and Dale, 1997) list the following as core tasks of a complete NLG system: content selection, discourse planning, sentence planning, sentence aggregation, lexicalisation, referring expression generation and linguistic realisation. However, decisions made for each of these core tasks are not independent of each other. The value of one generation task can change the conditions of others, as evidenced by studies in corpus linguistics, and it can therefore be undesirable to treat them all as isolated modules. In this paper, we focus on interrelated decision making in the areas of content selection, choice of tex</context>
<context position="2519" citStr="Reiter and Dale, 1997" startWordPosition="380" endWordPosition="383">ronmental features. In addition, we aim to balance the degree of variation and alignment in texts and produce lexical and syntactic patterns of co-occurrence that resemble those of human texts of the same domain. Evidence for the importance of this is provided by (Halliday and Hasan, 1976) who note the way that lexical cohesive ties contribute to text coherence as well as by the theory of interactive alignment. According to (Pickering and Garrod, 2004) we would expect significant traces of lexical and syntactic selfalignment in texts. Approaches to NLG in the past have been either rule-based (Reiter and Dale, 1997) or statistical (Langkilde and Knight, 1998). However, the former relies on a large number of hand-crafted rules, which makes it infeasible for controlling a large number of interrelated variables. The latter typically requires training on a large corpus of the domain. While these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying Reinforcement Learning (RL)—with a hierarchical approach. Previous work that has used RL for NLG includes (Janarthanam and Lemon, 2009) who employed it for alignment of referri</context>
</contexts>
<marker>Reiter, Dale, 1997</marker>
<rawString>Reiter, E. and Dale, R. (1997). Building applied natural language generation systems. Natural Language Engineering, 3(1):57–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Natural language generation as planning under uncertainty for spoken dialogue systems.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>683--691</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3199" citStr="Rieser and Lemon, 2009" startWordPosition="490" endWordPosition="493">e former relies on a large number of hand-crafted rules, which makes it infeasible for controlling a large number of interrelated variables. The latter typically requires training on a large corpus of the domain. While these approaches may be better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying Reinforcement Learning (RL)—with a hierarchical approach. Previous work that has used RL for NLG includes (Janarthanam and Lemon, 2009) who employed it for alignment of referring expressions based on user models. Also, (Lemon, 2008; Rieser and Lemon, 2009) used RL for optimising information presentation styles for search results. While both approaches displayed significant effects of adaptation, they focused on a single area of optimisation. For larger problems, however, such as the one we are aiming to solve, flat RL will not be applicable due to the large state space. We therefore suggest to divide the problem into a number of subproblems and apply hierarchical reinforcement learning (HRL) (Barto and Mahadevan, 2003) to solve it. We describe our problem in more detail in Section 2, our proposed HRL architecture in Sections 3 and 4 and present</context>
</contexts>
<marker>Rieser, Lemon, 2009</marker>
<rawString>Rieser, V. and Lemon, O. (2009). Natural language generation as planning under uncertainty for spoken dialogue systems. In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 683–691, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Scott</author>
<author>J Moore</author>
</authors>
<title>An NLG evaluation competition? eight reasons to be cautious.</title>
<date>2006</date>
<tech>Technical report.</tech>
<contexts>
<context position="26060" citStr="Scott and Moore, 2006" startWordPosition="4290" endWordPosition="4293">ervised learning requires a large amount of training data, which may not always be available, and may also produce unpredictable behaviour in cases where a user deviates from the behaviour covered by the corpus (Levin et al., 2000). Both arguments are directly transferable to NLG. If an agent is able to act only on grounds of what it has observed in a training corpus, it will not be able to react flexibly to new state representations. Moreover, it has been argued that a corpus for NLG cannot be regarded as an equivalent gold standard to the ones of other domains of NLP (Belz and Reiter, 2006; Scott and Moore, 2006; Viethen and Dale, 2006). The fact that an expression for a semantic concept does not appear in a corpus does not mean that it is an unsuited or impossible expression. Another alternative to pure RL is to apply semi-learnt behaviour, which can be helpful for tasks with very large state-action spaces. In this way, the state-action space is reduced to only sensible state-action pairs by providing the agent with prior knowledge of the domain. All remaining behaviour continues to be learnt. (Cuay´ahuitl, 2009) suggests such an approach for learning dialogue strategies, but again the principle is </context>
</contexts>
<marker>Scott, Moore, 2006</marker>
<rawString>Scott, D. and Moore, J. (2006). An NLG evaluation competition? eight reasons to be cautious. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
<author>A G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="19910" citStr="Sutton and Barto, 1998" startWordPosition="3270" endWordPosition="3273"> simulated environment. Future work can consider inducing a learning environment from data. 5.2 Comparison of learnt and baseline policies In order to test our framework, we designed a simulated environment that simulates different navigational situations, routes of different lengths and different user types. We trained our HRL agent for 10.000 episodes with the following learning parameters: the step-size parameter α was initiated with 1 and then reduced over time by α = &apos; &apos;�t, t being the time step. The discount rate parameter γ was 0.99 and the probability of random action c was 0.01 (see (Sutton and Barto, 1998) for details on these parameters). Figure 2 compares the learnt behaviour of our agent with a baseline (averaged over 10 runs) that chooses actions at random in models M&apos;0 and Mo - M7 (i.e., the baseline does not adapt its text strategy to user type or route length and neither performs adaptation of referring expressions or alignment score). The user study reported in (Cuay´ahuitl et al., 2010) provided users with instruction using this baseline generation behaviour. The fact that users had a user satisfaction score of 90% indicates that this is a sensible baseline, producing intelligible inst</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy partof-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7356" citStr="Toutanova and Manning, 2000" startWordPosition="1155" endWordPosition="1158">etermine a proper balance of alignment and variation, we computed the degree of lexical repetition from our corpus of 24 human route descriptions. This analysis was based on (Hirst and St-Onge, 1998) who retrieve lexical chains from texts by identifying a number of relations between lexical items. We focus here exclusively on Hirst &amp; St-Onge’s ‘extra-strong’ relations, since these can be computed from shallow properties of texts and do not require a large corpus of the target domain. In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000)1 to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. Based on these categories, we compute the proportion of tokens that are members in lexical chains, the ‘alignment score’ (AS), according to the following equation: Lexical tokens in chains AS = × 100. (1) Total number of tokens We obtained an average alignment score of 43.3% for 24 human route instructions. In contrast, the 1http://nlp.stanford.edu/software/tagger.shtml Table 1: Different text generation strategies for the same underlying route. Type 1</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Toutanova, K. and Manning, C. D. (2000). Enriching the knowledge sources used in a maximum entropy partof-speech tagger. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora, pages 63–70, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Viethen</author>
<author>R Dale</author>
</authors>
<title>Towards the evaluation of referring expression generation. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th Australiasian Language Technology Workshop,</booktitle>
<pages>115--122</pages>
<contexts>
<context position="26085" citStr="Viethen and Dale, 2006" startWordPosition="4294" endWordPosition="4297">es a large amount of training data, which may not always be available, and may also produce unpredictable behaviour in cases where a user deviates from the behaviour covered by the corpus (Levin et al., 2000). Both arguments are directly transferable to NLG. If an agent is able to act only on grounds of what it has observed in a training corpus, it will not be able to react flexibly to new state representations. Moreover, it has been argued that a corpus for NLG cannot be regarded as an equivalent gold standard to the ones of other domains of NLP (Belz and Reiter, 2006; Scott and Moore, 2006; Viethen and Dale, 2006). The fact that an expression for a semantic concept does not appear in a corpus does not mean that it is an unsuited or impossible expression. Another alternative to pure RL is to apply semi-learnt behaviour, which can be helpful for tasks with very large state-action spaces. In this way, the state-action space is reduced to only sensible state-action pairs by providing the agent with prior knowledge of the domain. All remaining behaviour continues to be learnt. (Cuay´ahuitl, 2009) suggests such an approach for learning dialogue strategies, but again the principle is transferable to NLG. Whil</context>
</contexts>
<marker>Viethen, Dale, 2006</marker>
<rawString>Viethen, J. and Dale, R. (2006). Towards the evaluation of referring expression generation. In In Proceedings of the 4th Australiasian Language Technology Workshop, pages 115–122.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>