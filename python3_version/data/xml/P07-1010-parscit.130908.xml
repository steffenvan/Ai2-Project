<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000207">
<title confidence="0.820122">
A Discriminative Language Model with Pseudo-Negative Samples
</title>
<author confidence="0.838919">
Daisuke Okanoharat Jun’ichi Tsujiit$�
</author>
<affiliation confidence="0.969602">
tDepartment of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
$School of Informatics, University of Manchester
</affiliation>
<address confidence="0.624002">
�NaCTeM (National Center for Text Mining)
</address>
<email confidence="0.999223">
{hillbig,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.996667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997256">
In this paper, we propose a novel discrim-
inative language model, which can be ap-
plied quite generally. Compared to the
well known N-gram language models, dis-
criminative language models can achieve
more accurate discrimination because they
can employ overlapping features and non-
local information. However, discriminative
language models have been used only for
re-ranking in specific applications because
negative examples are not available. We
propose sampling pseudo-negative examples
taken from probabilistic language models.
However, this approach requires prohibitive
computational cost if we are dealing with
quite a few features and training samples.
We tackle the problem by estimating the la-
tent information in sentences using a semi-
Markov class model, and then extracting
features from them. We also use an on-
line margin-based algorithm with efficient
kernel computation. Experimental results
show that pseudo-negative examples can be
treated as real negative examples and our
model can classify these sentences correctly.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989099325">
Language models (LMs) are fundamental tools for
many applications, such as speech recognition, ma-
chine translation and spelling correction. The goal
of LMs is to determine whether a sentence is correct
or incorrect in terms of grammars and pragmatics.
73
The most widely used LM is a probabilistic lan-
guage model (PLM), which assigns a probability to
a sentence or a word sequence. In particular, N-
grams with maximum likelihood estimation (NLMs)
are often used. Although NLMs are simple, they are
effective for many applications.
However, NLMs cannot determine correctness
of a sentence independently because the probabil-
ity depends on the length of the sentence and the
global frequencies of each word in it. For exam-
ple, p(Sj &lt; p(S2), where p(S) is the probability
of a sentence S given by an NLM, does not always
mean that S2 is more correct, but instead could occur
when S2 is shorter than Sl, or if S2 has more com-
mon words than Sl. Another problem is that NLMs
cannot handle overlapping information or non-local
information easily, which is important for more ac-
curate sentence classification. For example, a NLM
could assign a high probability to a sentence even if
it does not have a verb.
Discriminative language models (DLMs) have
been proposed to classify sentences directly as cor-
rect or incorrect (Gao et al., 2005; Roark et al.,
2007), and these models can handle both non-local
and overlapping information. However DLMs in
previous studies have been restricted to specific ap-
plications. Therefore the model cannot be used for
other applications. If we had negative examples
available, the models could be trained directly by
discriminating between correct and incorrect sen-
tences.
In this paper, we propose a generic DLM, which
can be used not only for specific applications, but
also more generally, similar to PLMs. To achieve
</bodyText>
<note confidence="0.9036405">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 73–80,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.990915951612903">
this goal, we need to solve two problems. The first Since the number of parameters in NLM is still
is that since we cannot obtain negative examples (in- large, several smoothing methods are used (Chen
correct sentences), we need to generate them. The and Goodman, 1998) to produce more accurate
second is the prohibitive computational cost because probabilities, and to assign nonzero probabilities to
the number of features and examples is very large. In any word string.
previous studies this problem did not arise because However, since the probabilities in NLMs depend
the amount of training data was limited and they did on the length of the sentence, two sentences of dif-
not use a combination of features, and thus the com- ferent length cannot be compared directly.
putational cost was negligible. Recently, Whole Sentence Maximum Entropy
To solve the first problem, we propose sampling Models (Rosenfeld et al., 2001) (WSMEs) have
incorrect sentences taken from a PLM and then been introduced. They assign a probability to
training a model to discriminate between correct and each sentence using a maximum entropy model.
incorrect sentences. We call these examples Pseudo- Although WSMEs can encode all features of a
Negative because they are not actually negative sen- sentence including non-local ones, they are only
tences. We call this method DLM-PN (DLM with slightly superior to NLMs, in that they have the dis-
Pseudo-Negative samples). advantage of being computationally expensive, and
To deal with the second problem, we employ an not all relevant features can be included.
online margin-based learning algorithm with fast A discriminative language model (DLM) assigns
kernel computation. This enables us to employ com- a score f(S) to a sentence S, measuring the correct-
binations of features, which are important for dis- ness of a sentence in terms of grammar and prag-
crimination between correct and incorrect sentences. matics, so that f(S) &gt; 0 implies S is correct and
We also estimate the latent information in sentences f(S) &lt; 0 implies S is incorrect. A PLM can be
by using a semi-Markov class model to extract fea- considered as a special case of a DLM by defining
tures. Although there are substantially fewer la- f using P(S). For example, we can take f(S) =
tent features than explicit features such as words or P(S)/ISI — a, where a is some threshold, and IS�
phrases, latent features contain essential information is the length of S.
for sentence classification. Given a sentence S, we extract a feature vector
Experimental results show that these pseudo- (O(S)) from it using a pre-defined set of feature
negative samples can be treated as incorrect exam- functions {OjIT,. The form of the function f we
ples, and that DLM-PN can learn to correctly dis- use is
criminate between correct and incorrect sentences f(S) = w - O(S), (2)
and can therefore classify these sentences correctly. where w is a feature weighting vector.
2 Previous work Since there is no restriction in designing O(S),
Probabilistic language models (PLMs) estimate the DLMs can make use of both over-lapping and non-
probability of word strings or sentences. Among local information in S. We estimate w using training
these models, N-gram language models (NLMs) are samples {(Si, yi)I for i = 1...t, where yi = 1 if Si
widely used. NLMs approximate the probability by is correct and yi = —1 if Si is incorrect.
conditioning only on the preceding N — 1 words. However, it is hard to obtain incorrect sentences
For example, let S denote a sentence of t words, because only correct sentences are available from
S := wI, w2, ... , wt. Then, by the chain rule of the corpus. This problem was not an issue for previ-
probability and the approximation, we have ous studies because they were concerned with spe-
P(S) = P(wi, w2, ... , wt) cific applications and therefore were able to obtain
= ri P(wiJwi-1v+1, ... , wi-1). (1) real negative examples easily. For example, Roark
i=1...t (2007) proposed a discriminative language model, in
The parameters can be estimated using the maxi- which a model is trained so that a correct sentence
mum likelihood method. should have higher score than others. The differ-
74 ence between their approach and ours is that we do
not assume just one application. Moreover, they had
For i=1,2,...
Choose a word wi at random
according to the distribution
P(wi wi-1v+1, ... ,wi-1)
If wi = &amp;quot;end of a sentence&amp;quot;
Break
End End
We know of no program, and animated
discussions about prospects for trade
barriers or regulations on the rules
of the game as a whole, and elements
of decoration of this peanut-shaped
to priorities tasks across both target
countries
</bodyText>
<figureCaption confidence="0.99793625">
Figure 2: Example of a sentence sampled by PLMs
(Trigram).
Figure 1: Sample procedure for pseudo-negative ex-
amples taken from N-gram language models.
</figureCaption>
<bodyText confidence="0.9982055">
training sets consisting of one correct sentence and
many incorrect sentences, which were very similar
because they were generated by the same input. Our
framework does not assume any such training sets,
and we treat correct or incorrect examples indepen-
dently in training.
</bodyText>
<sectionHeader confidence="0.996363" genericHeader="method">
3 Discriminative Language Model with
</sectionHeader>
<subsectionHeader confidence="0.87749">
Pseudo-Negative samples
</subsectionHeader>
<bodyText confidence="0.978265178571429">
We propose a novel discriminative language model;
a Discriminative Language Model with Pseudo-
Negative samples (DLM-PN). In this model,
pseudo-negative examples, which are all assumed to
be incorrect, are sampled from PLMs.
First a PLM is built using training data and then
examples, which are almost all negative, are sam-
pled independently from PLMs. DLMs are trained
using correct sentences from a corpus and negative
examples from a Pseudo-Negative generator.
An advantage of sampling is that as many nega-
tive examples can be collected as correct ones, and
a distinction can be clearly made between truly cor-
rect sentences and incorrect sentences, even though
the latter might be correct in a local sense.
For sampling, any PLMs can be used as long
as the model supports a sentence sampling proce-
dure. In this research we used NLMs with interpo-
lated smoothing because such models support effi-
cient sentence sampling. Figure 1 describes the sam-
pling procedure and figure 2 shows an example of a
pseudo-negative sentence.
Since the focus is on discriminating between cor-
rect sentences from a corpus and incorrect sentences
sampled from the NLM, DLM-PN may not able to
classify incorrect sentences that are not generated
from the NLM. However, this does not result in a se-
Return positive/negative label or score (margin)
</bodyText>
<figureCaption confidence="0.99872">
Figure 3: Framework of our classification process.
</figureCaption>
<bodyText confidence="0.997467">
rious problem, because these sentences, if they exist,
can be filtered out by NLMs.
</bodyText>
<sectionHeader confidence="0.824749" genericHeader="method">
4 Online margin-based learning with fast
kernel computation
</sectionHeader>
<bodyText confidence="0.999956882352941">
The DLM-PN can be trained by using any binary
classification learning methods. However, since the
number of training examples is very large, batch
training has suffered from prohibitively large com-
putational cost in terms of time and memory. There-
fore we make use of an online learning algorithm
proposed by (Crammer et al., 2006), which has a
much smaller computational cost. We follow the
definition in (Crammer et al., 2006).
The initiation vector wl is initialized to 0 and for
each round the algorithm observes a training exam-
ple xi := 0(Si) and predicts its label y/j to be either
+l or —l. After the prediction is made, the true la-
bel yi is revealed and the algorithm suffers an instan-
taneous hinge-loss l(w; (xi, yi)) = l — yi(wi • xi)
which reflects the degree to which its prediction was
wrong. If the prediction was wrong, the parameter
</bodyText>
<figure confidence="0.997648333333333">
test sentences
Positive (Pseudo-) Negative
Input training examples
Binary Classifier
Build a probabilistic language model
Probabilistic LM
(e.g. N-gram LM)
Sample sentences
Corpus
</figure>
<page confidence="0.947117">
75
</page>
<equation confidence="0.943072">
w is updated as
wi+1 = ar�mi�� �II� — �iII2 + �� (3)
1
subject to 1(w; (xi, yi)) &lt; � and &gt; 0, (4)
</equation>
<bodyText confidence="0.9998384">
where � is a slack term and C is a positive parameter
which controls the influence of the slack term on the
objective function. A large value of C will result in a
more aggressive update step. This has a closed form
solution as
</bodyText>
<equation confidence="0.972546">
wi+1 = Wi + Tigixi (5)
</equation>
<bodyText confidence="0.9852462">
where Ti = min{C ��
IIXiII2 }. As in SVMs, a fi-
nal weight vector can be represented as a kernel-
dependent combination of the stored training exam-
ples.
</bodyText>
<equation confidence="0.99317">
w•x=� Tigi(xi • x) (6)
i
</equation>
<bodyText confidence="0.999960657894737">
Using this formulation the inner product can be re-
placed with a general Mercer kernel K(xi, x) such
as a polynomial kernel or a Gaussian kernel.
The combination of features, which can capture
correlation information, is important in DLMs. If
the kernel-trick (Taylor and Cristianini, 2004) is ap-
plied to online margin-based learning, a subset of
the observed examples, called the active set, needs
to be stored. However in contrast to the support set
in SVMs, an example is added to the active set every
time the online algorithm makes a prediction mis-
take or when its confidence in a prediction is inad-
equately low. Therefore the active set can increase
in size significantly and thus the total computational
cost becomes proportional to the square of the num-
ber of training examples. Since the number of train-
ing examples is very large, the computational cost is
prohibitive even if we apply the kernel trick.
The calculation of the inner product between two
examples can be done by intersection of the acti-
vated features in each example. This is similar to
a merge sort and can be executed in O(M) time
where M is the average number of activated fea-
tures in an example. When the number of examples
in the active set is A, the total computational cost is
O(M • A). For fast kernel computation, the Poly-
nomial Kernel Inverted method (PKI)) is proposed
(Kudo and Matsumoto, 2003), which is an exten-
sion of Inverted Index in Information Retrieval. This
algorithm uses a table h(fi) for each feature item,
which stores examples where a feature fi is fired.
Let B be the average of Ih(fi)I over all feature item.
Then the kernel computation can be performed in
O(M • B) time which is much less than the normal
kernel computation time when B « A. We can eas-
ily extend this algorithm into the online setting by
updating h(fi) when an observed example is added
to an active set.
</bodyText>
<sectionHeader confidence="0.959357" genericHeader="method">
5 Latent features by semi-Markov class
model
</sectionHeader>
<bodyText confidence="0.99996428">
Another problem for DLMs is that the number of
features becomes very large, because all possible N-
grams are used as features. In particular, the mem-
ory requirement becomes a serious problem because
quite a few active sets with many features have to be
stored, not only at training time, but also at classi-
fication time. One way to deal with this is to filter
out low-confidence features, but it is difficult to de-
cide which features are important in online learning.
For this reason we cluster similar N-grams using a
semi-Markov class model.
The class model was originally proposed by (Mar-
tin et al., 1998). In the class model, determinis-
tic word-to-class mappings are estimated, keeping
the number of classes much smaller than the num-
ber of distinct words. A semi-Markov class model
(SMCM) is an extended version of the class model,
a part of which was proposed by (Deligne and BIM-
BOT, 1995). In SMCM, a word sequence is par-
titioned into a variable-length sequence of chunks
and then chunks are clustered into classes (Figure 4).
How a chunk is clustered depends on which chunks
are adjacent to it.
The probability of a sentence P(w1, ... , wt), in a
bi-gram class model is calculated by
</bodyText>
<equation confidence="0.975018">
ri P(wi+1Ici+1)P(ci+1Ici). (7)
i
</equation>
<bodyText confidence="0.740752">
On the other hand, the probabilities in a bi-gram
semi-Markov class model are calculated by
</bodyText>
<equation confidence="0.996645">
P(ciIci-1) • P(wt(i),t(i)+1,...,u(i)Ici). (8)
</equation>
<bodyText confidence="0.999979333333333">
where s varies over all possible partitions of S, t(i)
and u(i) denote the start and end positions respec-
tively of the i-th chunk in partition s, and t(i + 1) =
</bodyText>
<equation confidence="0.66540475">
ri
i
�
s
</equation>
<page confidence="0.816623">
76
</page>
<bodyText confidence="0.9991085">
!(i) + 1 for all i. Note that each word or variable-
length chunk belongs to only one class, in contrast
to a hidden Markov model where each word can be-
long to several classes.
Using a training corpus, the mapping is estimated
by maximum likelihood estimation. The log like-
lihood of the training corpus (w1, ... , w,,,) in a bi-
gram class model can be calculated as
</bodyText>
<equation confidence="0.999534375">
log11 P(wz+1Iwz) (9)
— 1: logP(wz+1 cz+1)P(cz+l Cz) (10)
z
� &amp;quot; �������
&amp;quot;���� ��� ��� &amp;quot; ����&amp;quot; (11)
C1 C2
+1: &amp;quot;(w) log &amp;quot;(w).
W
</equation>
<bodyText confidence="0.9999746">
where &amp;quot;(w), &amp;quot;(c) and &amp;quot;(c1, c2) are frequencies of
a word w, a class c and a class bi-gram c1, c2 in the
training corpus. In (11) only the first term is used,
since the second term does not depend on the class
allocation. The class allocation problem is solved by
an exchange algorithm as follows. First, all words
are assigned to a randomly determined class. Next,
for each word w, we move it to the class c for which
the log-likelihood is maximized. This procedure is
continued until the log-likelihood converges to a lo-
cal maximum. A naive implementation of the clus-
tering algorithm scales quadratically to the number
of classes, since each time a word is moved between
classes, all class bi-gram counts are potentially af-
fected. However, by considering only those counts
that actually change, the algorithm can be made to
scale somewhere between linearly and quadratically
to the number of classes (Martin et al., 1998).
In SMCM, partitions of each sentence are also de-
termined. We used a Viterbi decoding (Deligne and
BIMBOT, 1995) for the partition. We applied the
exchange algorithm and the Viterbi decoding alter-
nately until the log-likelihood converged to the local
maximum.
Since the number of chunks is very large, for ex-
ample, in our experiments we used about 3 million
chunks, the computational cost is still large. We
therefore employed the following two techniques.
The first was to approximate the computation in the
exchange algorithm; the second was to make use of
</bodyText>
<equation confidence="0.668546">
C1 C2 C3 C4
</equation>
<figureCaption confidence="0.98019025">
Figure 4: Example of assignment in semi-Markov
class model. A sentence is partitioned into variable-
length chunks and each chunk is assigned a unique
class number.
</figureCaption>
<bodyText confidence="0.979340692307692">
bottom-up clustering to strengthen the convergence.
In each step in the exchange algorithm, the ap-
proximate value of the change of the log-likelihood
was examined, and the exchange algorithm applied
only if the approximate value was larger than a pre-
defined threshold.
The second technique was to reduce memory re-
quirements. Since the matrices used in the exchange
algorithm could become very large, we clustered
chunks into 2 classes and then again we clustered
these two into 2 each, thus obtaining 4 classes. This
procedure was applied recursively until the number
of classes reached a pre-defined number.
</bodyText>
<sectionHeader confidence="0.999843" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997681">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999999">
We partitioned a BNC-corpus into model-train,
DLM-train-positive, and DLM-test-positive sets.
The numbers of sentences in model-train, DLM-
train-positive and DLM-test-positive were 4500k,
250k, and 10k respectively. An NLM was built
using model-train and Pseudo-Negative examples
(250k sentences) were sampled from it. We mixed
sentences from DLM-train-positive and the Pseudo-
Negative examples and then shuffled the order of
these sentences to make DLM-train. We also con-
structed DLM-test by mixing DLM-test-positive and
IOk new (not already used) sentences from the
Pseudo-Negative examples. We call the sentences
from DLM-train-positive “positive” examples and
the sentences from the Pseudo-Negative examples
“negative” examples in the following. From these
sentences the ones with less than 5 words were ex-
cluded beforehand because it was difficult to decide
whether these sentences were correct or not (e.g.
</bodyText>
<equation confidence="0.982505">
W1 W2
W3
W4 W5 W6
W7 W8
</equation>
<page confidence="0.995322">
77
</page>
<table confidence="0.999828454545455">
Accuracy (%) Training time (s)
Linear classifier
word tri-gram 51.28 137.1
POS tri-gram 52.64 85.0
SMCM bi-gram (G = 100) 51.79 304.9
SMCM bi-gram (G = 500) 54.45 422.1
3rd order Polynomial Kernel
word tri-gram 73.65 20143.7
POS tri-gram 66.58 29622.9
SMCM bi-gram (G = 100) 67.11 37181.6
SMCM bi-gram (G = 500) 74.11 34474.7
</table>
<tableCaption confidence="0.99996">
Table 1: Performance on the evaluation data.
</tableCaption>
<bodyText confidence="0.9465018">
compound words).
Let G be the number of classes in SMCMs. Two
SMCMs, one with G = 100 and the other with
G = 500, were constructed from model-train. Each
SMCM contained 2.8 million extracted chunks.
</bodyText>
<subsectionHeader confidence="0.999851">
6.2 Experiments on Pseudo-Examples
</subsectionHeader>
<bodyText confidence="0.999615666666667">
We examined the property of a sentence being
Pseudo-Negative, in order to justify our framework.
A native English speaker and two non-native En-
glish speaker were asked to assign correct/incorrect
labels to 100 sentences in DLM-train1. The result
for an native English speaker was that all positive
sentences were labeled as correct and all negative
sentences except for one were labeled as incorrect.
On the other hand, the results for non-native English
speakers are 67% and 70%. From this result, we
can say that the sampling method was able to gen-
erate incorrect sentences and if a classifier can dis-
criminate them, the classifier can also discriminate
between correct and incorrect sentences. Note that
it takes an average of 25 seconds for the native En-
glish speaker to assign the label, which suggests that
it is difficult even for a human to determine the cor-
rectness of a sentence.
We then examined whether it was possible to dis-
criminate between correct and incorrect sentences
using parsing methods, since if so, we could have
used parsing as a classification tool. We exam-
ined 100 sentences using a phrase structure parser
(Charniak and Johnson, 2005) and an HPSG parser
</bodyText>
<footnote confidence="0.979132666666667">
1Since the PLM also made use of the BNC-corpus for posi-
tive examples, we were not able to classify sentences based on
word occurrences
</footnote>
<bodyText confidence="0.9946242">
(Miyao and Tsujii, 2005). All sentences were parsed
correctly except for one positive example. This
result indicates that correct sentences and pseudo-
negative examples cannot be differentiated syntacti-
cally.
</bodyText>
<subsectionHeader confidence="0.998761">
6.3 Experiments on DLM-PN
</subsectionHeader>
<bodyText confidence="0.999855851851852">
We investigated the performance of classifiers and
the effect of different sets of features.
For N-grams and Part of Speech (POS), we used
tri-gram features. For SMCM, we used bi-gram fea-
tures. We used DLM-train as a training set. In all
experiments, we set C = 50.0 where C is a parame-
ter in the classification (Section 4). In all kernel ex-
periments, a 3rd order polynomial kernel was used
and values were computed using PKI (the inverted
indexing method). Table 1 shows the accuracy re-
sults with different features, or in the case of the
SMCMs, different numbers of classes. This result
shows that the kernel method is important in achiev-
ing high performance. Note that the classifier with
SMCM features performs as well as the one with
word.
Table 2 shows the number of features in each
method. Note that a new feature is added only if the
classifier needs to update its parameters. These num-
bers are therefore smaller than the possible number
of all candidate features. This result and the previ-
ous result indicate that SMCM achieves high perfor-
mance with very few features.
We then examined the effect of PKI. Table 3
shows the results of the classifier with 3rd order
polynomial kernel both with and without PKI. In
this experiment, only 200K sentences in DLM-train
</bodyText>
<page confidence="0.995585">
78
</page>
<table confidence="0.999817">
# of distinct features
word tri-gram 15773230
POS tri-gram 35376
SMCM (G = 100) 9335
SMCM (G = 500) 199745
</table>
<tableCaption confidence="0.904801">
Table 2: The number of features.
</tableCaption>
<table confidence="0.999258">
training time (s) prediction time (ms)
Baseline 37665.5 370.6
+ Index 4664.9 47.8
</table>
<tableCaption confidence="0.9297245">
Table 3: Comparison between classification perfor-
mance with/without index
</tableCaption>
<figureCaption confidence="0.9486765">
Figure 5: Margin distribution using SMCM bi-gram
features.
</figureCaption>
<bodyText confidence="0.837704529411765">
were used for both experiments because training us-
ing all the training data would have required a much
longer time than was possible with our experimental
setup.
Figure 5 shows the margin distribution for pos-
itive and negative examples using SMCM bi-gram
features. Although many examples are close to the
border line (margin = 0), positive and negative ex-
amples are distributed on either side of 0. Therefore
higher recall or precision could be achieved by using
a pre-defined margin threshold other than 0.
Finally, we generated learning curves to examine
the effect of the size of training data on performance.
Figure 6 shows the result of the classification task
using SMCM-bi-gram features. The result suggests
that the performance could be further improved by
enlarging the training data set.
</bodyText>
<figureCaption confidence="0.904201666666667">
Figure 6: A learning curve for SMCM (G = 500).
The accuracy is the percentage of sentences in the
evaluation set classified correctly.
</figureCaption>
<sectionHeader confidence="0.998829" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999953666666667">
Experimental results on pseudo-negative examples
indicate that combination of features is effective in
a sentence discrimination method. This could be
because negative examples include many unsuitable
combinations of words such as a sentence contain-
ing many nouns. Although in previous PLMs, com-
bination of features has not been discussed except
for the topic-based language model (David M. Blei,
2003; Wang et al., 2005), our result may encourage
the study of the combination of features for language
modeling.
A contrastive estimation method (Smith and Eis-
ner, 2005) is similar to ours with regard to construct-
ing pseudo-negative examples. They build a neigh-
borhood of input examples to allow unsupervised es-
timation when, for example, a word is changed or
deleted. A lattice is constructed, and then parame-
ters are estimated efficiently. On the other hand, we
construct independent pseudo-negative examples to
enable training. Although the motivations of these
studies are different, we could combine these two
methods to discriminate sentences finely.
In our experiments, we did not examine the result
of using other sampling methods, For example, it
would be possible to sample sentences from a whole
sentence maximum entropy model (Rosenfeld et al.,
2001) and this is a topic for future research.
</bodyText>
<page confidence="0.989266">
79
</page>
<note confidence="0.4702365">
8 Conclusion aggressive algorithms. Journal of Machine Learning
Research.
</note>
<bodyText confidence="0.9997564375">
In this paper we have presented a novel discrimi-
native language model using pseudo-negative exam-
ples. We also showed that an online margin-based
learning method enabled us to use half a million sen-
tences as training data and achieve 74% accuracy in
the task of discrimination between correct and in-
correct sentences. Experimental results indicate that
while pseudo-negative examples can be seen as in-
correct sentences, they are also close to correct sen-
tences in that parsers cannot discriminate between
them.
Our experimental results also showed that com-
bination of features is important for discrimination
between correct and incorrect sentences. This con-
cept has not been discussed in previous probabilistic
language models.
Our next step is to employ our model in machine
translation and speech recognition. One main diffi-
culty concerns how to encode global scores for the
classifier in the local search space, and another is
how to scale up the problem size in terms of the
number of examples and features. We would like to
see more refined online learning methods with ker-
nels (Cheng et al., 2006; Dekel et al., 2005) that we
could apply in these areas.
We are also interested in applications such as con-
structing an extended version of a spelling correc-
tion tool by identifying incorrect sentences.
Another interesting idea is to work with proba-
bilistic language models directly without sampling
and find ways to construct a more accurate discrim-
inative model.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999578245283019">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. ofACL 05, pages 173–180, June.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard Computer Science
Technical report TR-10-98.
Li Cheng, S V N Vishwanathan, Dale Schuurmans, Shao-
jun Wang, and Terry Caelli. 2006. Implicit online
learning with kernels. In NIPS 2006.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
Michael I. Jordan David M. Blei, Andrew Y. Ng. 2003.
Latent dirichlet allocation. Journal ofMachine Learn-
ing Research., 3:993–1022.
Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.
2005. The forgetron: A kernel-based perceptron on
a fixed budget. In Proc. ofNIPS.
Sabine Deligne and Fr´ed´eric BIMBOT. 1995. Language
modeling by variable length sequences: Theoretical
formulation and evaluation of multigrams. In Proc.
ICASSP ’95, pages 169–172.
Jianfeng Gao, Hao Yu, Wei Yuan, and Peng Xu. 2005.
Minimum sample risk methods for language modeling.
In Proc. ofHLT/EMNLP.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In ACL.
Sven Martin, J¨org Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
Speech Communicatoin, 24(1):19–37.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proc. ofACL 2005., pages 83–90, Ann Arbor,
Michigan, June.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. computer
speech and language. Computer Speech and Lan-
guage, 21(2):373–392.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001.
Whole-sentence exponential language models: a ve-
hicle for linguistic-statistical integration. Computers
Speech and Language, 15(1).
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. ofACL.
John S. Taylor and Nello. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambiridge Univsity
Press.
Shaojun Wang, Shaomin Wang, Russell Greiner, Dale
Schuurmans, and Li Cheng. 2005. Exploiting syntac-
tic, semantic and lexical regularities in language mod-
eling via directed markov random fields. In Proc. of
ICML.
</reference>
<page confidence="0.998194">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364088">
<title confidence="0.7610195">A Discriminative Language Model with Pseudo-Negative Samples of Computer Science, University of Tokyo</title>
<author confidence="0.500581">Tokyo Bunkyo-ku</author>
<author confidence="0.500581">Japan</author>
<affiliation confidence="0.725552">of Informatics, University of Manchester (National Center for Text Mining)</affiliation>
<abstract confidence="0.999600038461538">In this paper, we propose a novel discriminative language model, which can be applied quite generally. Compared to the well known N-gram language models, discriminative language models can achieve more accurate discrimination because they can employ overlapping features and nonlocal information. However, discriminative language models have been used only for re-ranking in specific applications because negative examples are not available. We propose sampling pseudo-negative examples taken from probabilistic language models. However, this approach requires prohibitive computational cost if we are dealing with quite a few features and training samples. We tackle the problem by estimating the latent information in sentences using a semi- Markov class model, and then extracting features from them. We also use an online margin-based algorithm with efficient kernel computation. Experimental results show that pseudo-negative examples can be treated as real negative examples and our model can classify these sentences correctly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL 05,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="20577" citStr="Charniak and Johnson, 2005" startWordPosition="3414" endWordPosition="3417">was able to generate incorrect sentences and if a classifier can discriminate them, the classifier can also discriminate between correct and incorrect sentences. Note that it takes an average of 25 seconds for the native English speaker to assign the label, which suggests that it is difficult even for a human to determine the correctness of a sentence. We then examined whether it was possible to discriminate between correct and incorrect sentences using parsing methods, since if so, we could have used parsing as a classification tool. We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser 1Since the PLM also made use of the BNC-corpus for positive examples, we were not able to classify sentences based on word occurrences (Miyao and Tsujii, 2005). All sentences were parsed correctly except for one positive example. This result indicates that correct sentences and pseudonegative examples cannot be differentiated syntactically. 6.3 Experiments on DLM-PN We investigated the performance of classifiers and the effect of different sets of features. For N-grams and Part of Speech (POS), we used tri-gram features. For SMCM, we used bi-gram features. We used DLM-train</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proc. ofACL 05, pages 173–180, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Harvard Computer Science</institution>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Harvard Computer Science Technical report TR-10-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Cheng</author>
<author>S V N Vishwanathan</author>
<author>Dale Schuurmans</author>
<author>Shaojun Wang</author>
<author>Terry Caelli</author>
</authors>
<title>Implicit online learning with kernels.</title>
<date>2006</date>
<booktitle>In NIPS</booktitle>
<marker>Cheng, Vishwanathan, Schuurmans, Wang, Caelli, 2006</marker>
<rawString>Li Cheng, S V N Vishwanathan, Dale Schuurmans, Shaojun Wang, and Terry Caelli. 2006. Implicit online learning with kernels. In NIPS 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<date>2006</date>
<note>Online passive-</note>
<contexts>
<context position="10404" citStr="Crammer et al., 2006" startWordPosition="1669" endWordPosition="1672"> the NLM. However, this does not result in a seReturn positive/negative label or score (margin) Figure 3: Framework of our classification process. rious problem, because these sentences, if they exist, can be filtered out by NLMs. 4 Online margin-based learning with fast kernel computation The DLM-PN can be trained by using any binary classification learning methods. However, since the number of training examples is very large, batch training has suffered from prohibitively large computational cost in terms of time and memory. Therefore we make use of an online learning algorithm proposed by (Crammer et al., 2006), which has a much smaller computational cost. We follow the definition in (Crammer et al., 2006). The initiation vector wl is initialized to 0 and for each round the algorithm observes a training example xi := 0(Si) and predicts its label y/j to be either +l or —l. After the prediction is made, the true label yi is revealed and the algorithm suffers an instantaneous hinge-loss l(w; (xi, yi)) = l — yi(wi • xi) which reflects the degree to which its prediction was wrong. If the prediction was wrong, the parameter test sentences Positive (Pseudo-) Negative Input training examples Binary Classifi</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passive-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Jordan David M Blei</author>
<author>Andrew Y Ng</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research.,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, 2003</marker>
<rawString>Michael I. Jordan David M. Blei, Andrew Y. Ng. 2003. Latent dirichlet allocation. Journal ofMachine Learning Research., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ofer Dekel</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>The forgetron: A kernel-based perceptron on a fixed budget. In</title>
<date>2005</date>
<booktitle>Proc. ofNIPS.</booktitle>
<marker>Dekel, Shalev-Shwartz, Singer, 2005</marker>
<rawString>Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer. 2005. The forgetron: A kernel-based perceptron on a fixed budget. In Proc. ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Deligne</author>
<author>Fr´ed´eric BIMBOT</author>
</authors>
<title>Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams.</title>
<date>1995</date>
<booktitle>In Proc. ICASSP ’95,</booktitle>
<pages>169--172</pages>
<contexts>
<context position="14440" citStr="Deligne and BIMBOT, 1995" startWordPosition="2389" endWordPosition="2393"> at training time, but also at classification time. One way to deal with this is to filter out low-confidence features, but it is difficult to decide which features are important in online learning. For this reason we cluster similar N-grams using a semi-Markov class model. The class model was originally proposed by (Martin et al., 1998). In the class model, deterministic word-to-class mappings are estimated, keeping the number of classes much smaller than the number of distinct words. A semi-Markov class model (SMCM) is an extended version of the class model, a part of which was proposed by (Deligne and BIMBOT, 1995). In SMCM, a word sequence is partitioned into a variable-length sequence of chunks and then chunks are clustered into classes (Figure 4). How a chunk is clustered depends on which chunks are adjacent to it. The probability of a sentence P(w1, ... , wt), in a bi-gram class model is calculated by ri P(wi+1Ici+1)P(ci+1Ici). (7) i On the other hand, the probabilities in a bi-gram semi-Markov class model are calculated by P(ciIci-1) • P(wt(i),t(i)+1,...,u(i)Ici). (8) where s varies over all possible partitions of S, t(i) and u(i) denote the start and end positions respectively of the i-th chunk in</context>
<context position="16604" citStr="Deligne and BIMBOT, 1995" startWordPosition="2776" endWordPosition="2779">r which the log-likelihood is maximized. This procedure is continued until the log-likelihood converges to a local maximum. A naive implementation of the clustering algorithm scales quadratically to the number of classes, since each time a word is moved between classes, all class bi-gram counts are potentially affected. However, by considering only those counts that actually change, the algorithm can be made to scale somewhere between linearly and quadratically to the number of classes (Martin et al., 1998). In SMCM, partitions of each sentence are also determined. We used a Viterbi decoding (Deligne and BIMBOT, 1995) for the partition. We applied the exchange algorithm and the Viterbi decoding alternately until the log-likelihood converged to the local maximum. Since the number of chunks is very large, for example, in our experiments we used about 3 million chunks, the computational cost is still large. We therefore employed the following two techniques. The first was to approximate the computation in the exchange algorithm; the second was to make use of C1 C2 C3 C4 Figure 4: Example of assignment in semi-Markov class model. A sentence is partitioned into variablelength chunks and each chunk is assigned a</context>
</contexts>
<marker>Deligne, BIMBOT, 1995</marker>
<rawString>Sabine Deligne and Fr´ed´eric BIMBOT. 1995. Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams. In Proc. ICASSP ’95, pages 169–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Hao Yu</author>
<author>Wei Yuan</author>
<author>Peng Xu</author>
</authors>
<title>Minimum sample risk methods for language modeling.</title>
<date>2005</date>
<booktitle>In Proc. ofHLT/EMNLP.</booktitle>
<contexts>
<context position="2700" citStr="Gao et al., 2005" startWordPosition="409" endWordPosition="412">or example, p(Sj &lt; p(S2), where p(S) is the probability of a sentence S given by an NLM, does not always mean that S2 is more correct, but instead could occur when S2 is shorter than Sl, or if S2 has more common words than Sl. Another problem is that NLMs cannot handle overlapping information or non-local information easily, which is important for more accurate sentence classification. For example, a NLM could assign a high probability to a sentence even if it does not have a verb. Discriminative language models (DLMs) have been proposed to classify sentences directly as correct or incorrect (Gao et al., 2005; Roark et al., 2007), and these models can handle both non-local and overlapping information. However DLMs in previous studies have been restricted to specific applications. Therefore the model cannot be used for other applications. If we had negative examples available, the models could be trained directly by discriminating between correct and incorrect sentences. In this paper, we propose a generic DLM, which can be used not only for specific applications, but also more generally, similar to PLMs. To achieve Proceedings of the 45th Annual Meeting of the Association of Computational Linguist</context>
</contexts>
<marker>Gao, Yu, Yuan, Xu, 2005</marker>
<rawString>Jianfeng Gao, Hao Yu, Wei Yuan, and Peng Xu. 2005. Minimum sample risk methods for language modeling. In Proc. ofHLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="13008" citStr="Kudo and Matsumoto, 2003" startWordPosition="2137" endWordPosition="2140">he number of training examples. Since the number of training examples is very large, the computational cost is prohibitive even if we apply the kernel trick. The calculation of the inner product between two examples can be done by intersection of the activated features in each example. This is similar to a merge sort and can be executed in O(M) time where M is the average number of activated features in an example. When the number of examples in the active set is A, the total computational cost is O(M • A). For fast kernel computation, the Polynomial Kernel Inverted method (PKI)) is proposed (Kudo and Matsumoto, 2003), which is an extension of Inverted Index in Information Retrieval. This algorithm uses a table h(fi) for each feature item, which stores examples where a feature fi is fired. Let B be the average of Ih(fi)I over all feature item. Then the kernel computation can be performed in O(M • B) time which is much less than the normal kernel computation time when B « A. We can easily extend this algorithm into the online setting by updating h(fi) when an observed example is added to an active set. 5 Latent features by semi-Markov class model Another problem for DLMs is that the number of features becom</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>J¨org Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering.</title>
<date>1998</date>
<journal>Speech Communicatoin,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="14154" citStr="Martin et al., 1998" startWordPosition="2340" endWordPosition="2344"> class model Another problem for DLMs is that the number of features becomes very large, because all possible Ngrams are used as features. In particular, the memory requirement becomes a serious problem because quite a few active sets with many features have to be stored, not only at training time, but also at classification time. One way to deal with this is to filter out low-confidence features, but it is difficult to decide which features are important in online learning. For this reason we cluster similar N-grams using a semi-Markov class model. The class model was originally proposed by (Martin et al., 1998). In the class model, deterministic word-to-class mappings are estimated, keeping the number of classes much smaller than the number of distinct words. A semi-Markov class model (SMCM) is an extended version of the class model, a part of which was proposed by (Deligne and BIMBOT, 1995). In SMCM, a word sequence is partitioned into a variable-length sequence of chunks and then chunks are clustered into classes (Figure 4). How a chunk is clustered depends on which chunks are adjacent to it. The probability of a sentence P(w1, ... , wt), in a bi-gram class model is calculated by ri P(wi+1Ici+1)P(</context>
<context position="16491" citStr="Martin et al., 1998" startWordPosition="2757" endWordPosition="2760">, all words are assigned to a randomly determined class. Next, for each word w, we move it to the class c for which the log-likelihood is maximized. This procedure is continued until the log-likelihood converges to a local maximum. A naive implementation of the clustering algorithm scales quadratically to the number of classes, since each time a word is moved between classes, all class bi-gram counts are potentially affected. However, by considering only those counts that actually change, the algorithm can be made to scale somewhere between linearly and quadratically to the number of classes (Martin et al., 1998). In SMCM, partitions of each sentence are also determined. We used a Viterbi decoding (Deligne and BIMBOT, 1995) for the partition. We applied the exchange algorithm and the Viterbi decoding alternately until the log-likelihood converged to the local maximum. Since the number of chunks is very large, for example, in our experiments we used about 3 million chunks, the computational cost is still large. We therefore employed the following two techniques. The first was to approximate the computation in the exchange algorithm; the second was to make use of C1 C2 C3 C4 Figure 4: Example of assignm</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, J¨org Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. Speech Communicatoin, 24(1):19–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage hpsg parsing.</title>
<date>2005</date>
<booktitle>In Proc. ofACL 2005.,</booktitle>
<pages>83--90</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="20756" citStr="Miyao and Tsujii, 2005" startWordPosition="3446" endWordPosition="3449"> average of 25 seconds for the native English speaker to assign the label, which suggests that it is difficult even for a human to determine the correctness of a sentence. We then examined whether it was possible to discriminate between correct and incorrect sentences using parsing methods, since if so, we could have used parsing as a classification tool. We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser 1Since the PLM also made use of the BNC-corpus for positive examples, we were not able to classify sentences based on word occurrences (Miyao and Tsujii, 2005). All sentences were parsed correctly except for one positive example. This result indicates that correct sentences and pseudonegative examples cannot be differentiated syntactically. 6.3 Experiments on DLM-PN We investigated the performance of classifiers and the effect of different sets of features. For N-grams and Part of Speech (POS), we used tri-gram features. For SMCM, we used bi-gram features. We used DLM-train as a training set. In all experiments, we set C = 50.0 where C is a parameter in the classification (Section 4). In all kernel experiments, a 3rd order polynomial kernel was used</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage hpsg parsing. In Proc. ofACL 2005., pages 83–90, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling. computer speech and language.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2721" citStr="Roark et al., 2007" startWordPosition="413" endWordPosition="416"> p(S2), where p(S) is the probability of a sentence S given by an NLM, does not always mean that S2 is more correct, but instead could occur when S2 is shorter than Sl, or if S2 has more common words than Sl. Another problem is that NLMs cannot handle overlapping information or non-local information easily, which is important for more accurate sentence classification. For example, a NLM could assign a high probability to a sentence even if it does not have a verb. Discriminative language models (DLMs) have been proposed to classify sentences directly as correct or incorrect (Gao et al., 2005; Roark et al., 2007), and these models can handle both non-local and overlapping information. However DLMs in previous studies have been restricted to specific applications. Therefore the model cannot be used for other applications. If we had negative examples available, the models could be trained directly by discriminating between correct and incorrect sentences. In this paper, we propose a generic DLM, which can be used not only for specific applications, but also more generally, similar to PLMs. To achieve Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 73–80, Pra</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. computer speech and language. Computer Speech and Language, 21(2):373–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
<author>Stanley F Chen</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Whole-sentence exponential language models: a vehicle for linguistic-statistical integration.</title>
<date>2001</date>
<journal>Computers Speech and Language,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="4327" citStr="Rosenfeld et al., 2001" startWordPosition="664" endWordPosition="667">is the prohibitive computational cost because probabilities, and to assign nonzero probabilities to the number of features and examples is very large. In any word string. previous studies this problem did not arise because However, since the probabilities in NLMs depend the amount of training data was limited and they did on the length of the sentence, two sentences of difnot use a combination of features, and thus the com- ferent length cannot be compared directly. putational cost was negligible. Recently, Whole Sentence Maximum Entropy To solve the first problem, we propose sampling Models (Rosenfeld et al., 2001) (WSMEs) have incorrect sentences taken from a PLM and then been introduced. They assign a probability to training a model to discriminate between correct and each sentence using a maximum entropy model. incorrect sentences. We call these examples Pseudo- Although WSMEs can encode all features of a Negative because they are not actually negative sen- sentence including non-local ones, they are only tences. We call this method DLM-PN (DLM with slightly superior to NLMs, in that they have the disPseudo-Negative samples). advantage of being computationally expensive, and To deal with the second p</context>
<context position="24803" citStr="Rosenfeld et al., 2001" startWordPosition="4101" endWordPosition="4104">es. They build a neighborhood of input examples to allow unsupervised estimation when, for example, a word is changed or deleted. A lattice is constructed, and then parameters are estimated efficiently. On the other hand, we construct independent pseudo-negative examples to enable training. Although the motivations of these studies are different, we could combine these two methods to discriminate sentences finely. In our experiments, we did not examine the result of using other sampling methods, For example, it would be possible to sample sentences from a whole sentence maximum entropy model (Rosenfeld et al., 2001) and this is a topic for future research. 79 8 Conclusion aggressive algorithms. Journal of Machine Learning Research. In this paper we have presented a novel discriminative language model using pseudo-negative examples. We also showed that an online margin-based learning method enabled us to use half a million sentences as training data and achieve 74% accuracy in the task of discrimination between correct and incorrect sentences. Experimental results indicate that while pseudo-negative examples can be seen as incorrect sentences, they are also close to correct sentences in that parsers canno</context>
</contexts>
<marker>Rosenfeld, Chen, Zhu, 2001</marker>
<rawString>Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001. Whole-sentence exponential language models: a vehicle for linguistic-statistical integration. Computers Speech and Language, 15(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data. In</title>
<date>2005</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="24110" citStr="Smith and Eisner, 2005" startWordPosition="3992" endWordPosition="3996"> the evaluation set classified correctly. 7 Discussion Experimental results on pseudo-negative examples indicate that combination of features is effective in a sentence discrimination method. This could be because negative examples include many unsuitable combinations of words such as a sentence containing many nouns. Although in previous PLMs, combination of features has not been discussed except for the topic-based language model (David M. Blei, 2003; Wang et al., 2005), our result may encourage the study of the combination of features for language modeling. A contrastive estimation method (Smith and Eisner, 2005) is similar to ours with regard to constructing pseudo-negative examples. They build a neighborhood of input examples to allow unsupervised estimation when, for example, a word is changed or deleted. A lattice is constructed, and then parameters are estimated efficiently. On the other hand, we construct independent pseudo-negative examples to enable training. Although the motivations of these studies are different, we could combine these two methods to discriminate sentences finely. In our experiments, we did not examine the result of using other sampling methods, For example, it would be poss</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis. Cambiridge</title>
<date>2004</date>
<publisher>Univsity Press.</publisher>
<contexts>
<context position="11918" citStr="Cristianini, 2004" startWordPosition="1946" endWordPosition="1947">erm on the objective function. A large value of C will result in a more aggressive update step. This has a closed form solution as wi+1 = Wi + Tigixi (5) where Ti = min{C �� IIXiII2 }. As in SVMs, a final weight vector can be represented as a kerneldependent combination of the stored training examples. w•x=� Tigi(xi • x) (6) i Using this formulation the inner product can be replaced with a general Mercer kernel K(xi, x) such as a polynomial kernel or a Gaussian kernel. The combination of features, which can capture correlation information, is important in DLMs. If the kernel-trick (Taylor and Cristianini, 2004) is applied to online margin-based learning, a subset of the observed examples, called the active set, needs to be stored. However in contrast to the support set in SVMs, an example is added to the active set every time the online algorithm makes a prediction mistake or when its confidence in a prediction is inadequately low. Therefore the active set can increase in size significantly and thus the total computational cost becomes proportional to the square of the number of training examples. Since the number of training examples is very large, the computational cost is prohibitive even if we a</context>
</contexts>
<marker>Cristianini, 2004</marker>
<rawString>John S. Taylor and Nello. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambiridge Univsity Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaojun Wang</author>
<author>Shaomin Wang</author>
<author>Russell Greiner</author>
<author>Dale Schuurmans</author>
<author>Li Cheng</author>
</authors>
<title>Exploiting syntactic, semantic and lexical regularities in language modeling via directed markov random fields.</title>
<date>2005</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="23963" citStr="Wang et al., 2005" startWordPosition="3970" endWordPosition="3973">her improved by enlarging the training data set. Figure 6: A learning curve for SMCM (G = 500). The accuracy is the percentage of sentences in the evaluation set classified correctly. 7 Discussion Experimental results on pseudo-negative examples indicate that combination of features is effective in a sentence discrimination method. This could be because negative examples include many unsuitable combinations of words such as a sentence containing many nouns. Although in previous PLMs, combination of features has not been discussed except for the topic-based language model (David M. Blei, 2003; Wang et al., 2005), our result may encourage the study of the combination of features for language modeling. A contrastive estimation method (Smith and Eisner, 2005) is similar to ours with regard to constructing pseudo-negative examples. They build a neighborhood of input examples to allow unsupervised estimation when, for example, a word is changed or deleted. A lattice is constructed, and then parameters are estimated efficiently. On the other hand, we construct independent pseudo-negative examples to enable training. Although the motivations of these studies are different, we could combine these two methods</context>
</contexts>
<marker>Wang, Wang, Greiner, Schuurmans, Cheng, 2005</marker>
<rawString>Shaojun Wang, Shaomin Wang, Russell Greiner, Dale Schuurmans, and Li Cheng. 2005. Exploiting syntactic, semantic and lexical regularities in language modeling via directed markov random fields. In Proc. of ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>