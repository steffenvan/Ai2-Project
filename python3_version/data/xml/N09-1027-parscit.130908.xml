<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000094">
<title confidence="0.997436">
Preference Grammars: Softening Syntactic Constraints
to Improve Statistical Machine Translation
</title>
<author confidence="0.971335">
Ashish Venugopal Andreas Zollmann Noah A. Smith Stephan Vogel
</author>
<affiliation confidence="0.91427675">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998858">
{ashishv,zollmann,nasmith,vogel}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999637285714286">
We propose a novel probabilistic syn-
choronous context-free grammar formalism
for statistical machine translation, in which
syntactic nonterminal labels are represented
as “soft” preferences rather than as “hard”
matching constraints. This formalism allows
us to efficiently score unlabeled synchronous
derivations without forgoing traditional
syntactic constraints. Using this score as a
feature in a log-linear model, we are able
to approximate the selection of the most
likely unlabeled derivation. This helps
reduce fragmentation of probability across
differently labeled derivations of the same
translation. It also allows the importance of
syntactic preferences to be learned alongside
other features (e.g., the language model)
and for particular labeling procedures. We
show improvements in translation quality on
small and medium sized Chinese-to-English
translation tasks.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996504972222222">
Probabilistic synchronous context-free grammars
(PSCFGs) define weighted production rules that are
automatically learned from parallel training data. As
in classical CFGs, these rules make use of nontermi-
nal symbols to generalize beyond lexical modeling
of sentences. In MT, this permits translation and re-
ordering to be conditioned on more abstract notions
of context. For example,
VP → ne VBi pas # do not VBi
represents the discontiguous translation of the
French words “ne” and “pas” to “do not”, in the con-
text of the labeled nonterminal symbol “VB” (rep-
resenting syntactic category “verb”). Translation
with PSCFGs is typically expressed as the problem
of finding the maximum-weighted derivation consis-
tent with the source sentence, where the scores are
defined (at least in part) by R-valued weights asso-
ciated with the rules. A PSCFG derivation is a syn-
chronous parse tree.
Defining the translation function as finding the
best derivation has the unfortunate side effect of
forcing differently-derived versions of the same tar-
get sentence to compete with each other. In other
words, the true score of each translation is “frag-
mented” across many derivations, so that each trans-
lation’s most probable derivation is the only one that
matters. The more Bayesian approach of finding
the most probable translation (integrating out the
derivations) instantiates an NP-hard inference prob-
lem even for simple word-based models (Knight,
1999); for grammar-based translation it is known
as the consensus problem (Casacuberta and de la
Higuera, 2000; Sima’an, 2002).
With weights interpreted as probabilities, the
maximum-weighted derivation is the maximum a
posteriori (MAP) derivation:
</bodyText>
<equation confidence="0.657946">
maxp(e,d  |f)
</equation>
<bodyText confidence="0.980575666666667">
d
where f is the source sentence, e ranges over tar-
get sentences, and d ranges over PSCFG deriva-
tions (synchronous trees). This is often described
as an approximation to the most probable transla-
tion, argmaxe Ed p(e, d  |f). In this paper, we
will describe a technique that aims to find the most
probable equivalence class of unlabeled derivations,
rather than a single labeled derivation, reducing the
fragmentation problem. Solving this problem ex-
actly is still an NP-hard consensus problem, but we
provide approximations that build on well-known
PSCFG decoding methods. Our model falls some-
where between PSCFGs that extract nonterminal
symbols from parse trees and treat them as part of
</bodyText>
<equation confidence="0.9209525">
eˆ ← argmax
e
</equation>
<page confidence="0.979249">
236
</page>
<note confidence="0.6547985">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236–244,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9730665625">
the derivation (Zollmann and Venugopal, 2006) and
unlabeled hierarchical structures (Chiang, 2005); we
treat nonterminal labels as random variables chosen
at each node, with each (unlabeled) rule express-
ing “preferences” for particular nonterminal labels,
learned from data.
The paper is organized as follows. In Section 2,
we summarize the use of PSCFG grammars for
translation. We describe our model (Section 3).
Section 4 explains the preference-related calcula-
tions, and Section 5 addresses decoding. Experi-
mental results using preference grammars in a log-
linear translation model are presented for two stan-
dard Chinese-to-English tasks in Section 6. We re-
view related work (Section 7) and conclude.
MAP approximation can be defined as:
</bodyText>
<equation confidence="0.9027605">
eˆ = tgt argmax �p(d) (1)
dED(G):src(d)=f
</equation>
<bodyText confidence="0.966087">
where tgt(d) is the target-side yield of a derivation
d, and D(G) is the set of G’s derivations. Using an
n-gram language model to score derivations and rule
labels to constraint the rules that form derivations,
we define p(d) as log-linear model in terms of the
rules r E R used in d as:
</bodyText>
<equation confidence="0.9992884">
p(d) = pLM(tgt(d))λ0 X
Xpsyn(d)λm+1/Z(�A)
�pi(d)λi
Hm
i=1
</equation>
<listItem confidence="0.883259166666667">
2 PSCFGs for Machine Translation pi(d) = H hi(r)freq(r;d) (2)
Probabilistic synchronous context-free grammars
(PSCFGs) are defined by a source terminal set
(source vocabulary) TS, a target terminal set (target
vocabulary) TT, a shared nonterminal set N and a
set R of rules of the form: X —* (-y, α, w) where
• X E N is a labeled nonterminal referred to as the
left-hand-side of the rule.
• -y E (N U TS)* is the source side of the rule.
• α E (N U TT)* is the target side of the rule.
• w E [0, oc) is a nonnegative real-valued weight
assigned to the rule.
</listItem>
<bodyText confidence="0.997998285714286">
For visual clarity, we will use the # character to sep-
arate the source side of the rule -y from the target
side α. PSCFG rules also have an implied one-to-
one mapping between nonterminal symbols in -y and
nonterminals symbols in α. Chiang (2005), Zoll-
mann and Venugopal (2006) and Galley et al. (2006)
all use parameterizations of this PSCFG formalism1.
Given a source sentence f and a PSCFG G, the
translation task can be expressed similarly to mono-
lingual parsing with a PCFG. We aim to find the
most likely derivation d of the input source sentence
and read off the English translation, identified by
composing α from each rule used in the derivation.
This search for the most likely translation under the
</bodyText>
<footnote confidence="0.9387365">
1Galley et al. (2006) rules are formally defined as tree trans-
ducers but have equivalent PSCFG forms.
</footnote>
<equation confidence="0.9818808">
rEIZ
� 1 if d respects label constraints
psyn(d) = 0 otherwise
3)
�
</equation>
<bodyText confidence="0.999853913043478">
where A = A0 · · · Am+1 are weights that reflect the
relative importance of features in the model. The
features include the n-gram language model (LM)
score of the target yield sequence, a collection of m
rule feature functions hi : R —* R&gt;0, and a “syn-
tax” feature that (redundantly) requires every non-
terminal token to be expanded by a rule with that
nonterminal on its left-hand side. freq(r; d) denotes
the frequency of the rule r in the derivation d. Note
that Am+1 can be effectively ignored when psyn is
defined as in Equation 3. Z( X) is a normalization
constant that does not need to be computed during
search under the argmax search criterion in Equa-
�
tion 1. Feature weights A are trained discrimina-
tively in concert with the language model weight
to maximize the BLEU (Papineni et al., 2002) au-
tomatic evaluation metric via Minimum Error Rate
Training (MERT) (Och, 2003).
We use the open-source PSCFG rule extraction
framework and decoder from Zollmann et al. (2008)
as the framework for our experiments. The asymp-
totic runtime of this decoder is:
</bodyText>
<equation confidence="0.9951375">
O (|f |3 [|N ||T
T |2 (n− 1)�Kl (4)
</equation>
<bodyText confidence="0.995572">
where K is the maximum number of nonterminal
symbols per rule, |f |the source sentence length, and
</bodyText>
<page confidence="0.977391">
237
</page>
<bodyText confidence="0.999990926829268">
have label preference distributions. These are esti-
mated as relative frequencies from the labelings of
the base, unlabeled rule. Our primary contribution
is how we compute psyn(d)—the probability that
an unlabeled derivation adheres to traditional syn-
tactic constraints—for derivations built from prefer-
ence grammar rules. By using psyn(d) as a feature
in the log-linear model, we allow the MERT frame-
work to evaluate the importance of syntactic struc-
ture relative to other features.
The example rules above highlight the potential
for psyn(d) to affect the choice of translation. The
translation of the Chinese word sequence R ;rt 09
ffi nztK can be performed by expanding the non-
terminal in the rule “a place where I can X1” with
either “eat” or “dish.” A hierarchical system (Chi-
ang, 2005) would allow either expansion, relying on
features like pLM to select the best translation since
both expansions occurred the same number of times
in the data.
A richly-labeled PSCFG as in Zollmann and
Venugopal (2006) would immediately reject the rule
generating “dish” due to hard label matching con-
straints, but would produce three identical, compet-
ing derivations. Two of these derivations would pro-
duce S as a root symbol, while one derivation would
produce SBAR. The two S-labeled derivations com-
pete, rather than reinforce the choice of the word
“eat,” which they both make. They will also com-
pete for consideration by any decoder that prunes
derivations to keep runtime down.
The rule preferences indicate that VB and VP are
both valid labels for the rule translating to “eat”, and
both of these labels are compatible with the argu-
ments expected by “a place where I can X1”. Al-
ternatively, “dish” produces a NN label which is
not compatible with the arguments of this higher-
up rule. We design psyn(d) to reflect compatibility
between two rules (one expanding a right-hand side
nonterminal in the other), based on label preference
distributions.
</bodyText>
<subsectionHeader confidence="0.996453">
3.2 Formal definition
</subsectionHeader>
<bodyText confidence="0.999159333333333">
Probabilistic synchronous context-free preference
grammars are defined as PSCFGs with the follow-
ing additional elements:
</bodyText>
<listItem confidence="0.932985714285714">
• H: a set of implicit labels, not to be confused
with the explicit label set N.
• 7r: H → N, a function that associates each im-
plicit label with a single explicit label. We can
therefore think of H symbols as refinements of
the nonterminals in N (Matsusaki et al., 2005).
• For each rule r, we define a probability distri-
</listItem>
<bodyText confidence="0.9028944">
�
bution over vectors h of implicit label bindings
for its nonterminals, denoted ppref( h  |r). h�
includes bindings for the left-hand side nonter-
minal (h0) as well as each right-hand side non-
terminal (h1,..., h|g|). Each hi ∈ H.
When N, H are defined to include just a single
generic symbol as in (Chiang, 2005), we produce the
unlabeled grammar discussed above. In this work,
we define
</bodyText>
<listItem confidence="0.999441">
• N = {S, X}
• H = {NP, DT, NN··· } = NSAMT
</listItem>
<bodyText confidence="0.9998524">
where N corresponds to the generic labels of Chi-
ang (2005) and H corresponds to the syntactically
motivated SAMT labels from (Zollmann and Venu-
gopal, 2006), and 7r maps all elements of H to
X. We will use hargs(r) to denote the set of all
h� = hh0, h1, ..., hki ∈ Hk+1 that are valid bindings
for the rule with nonzero preference probability.
The preference distributions ppref from each rule
used in d are used to compute psyn(d) as described
next.
</bodyText>
<sectionHeader confidence="0.748894" genericHeader="method">
4 Computing feature psyn(d)
</sectionHeader>
<bodyText confidence="0.99990975">
Let us view a derivation d as a collection of nonter-
minal tokens nj, j ∈ {1, ..., |d|}. Each nj takes an
explicit label in N. The score psyn(d) is a product,
with one factor per nj in the derivation d:
</bodyText>
<equation confidence="0.961310333333333">
|d|
psyn(d) = H φj (6)
j=1
</equation>
<bodyText confidence="0.999707166666667">
Each φj factor considers the two rules that nj partic-
ipates in. We will refer to the rule above nonterminal
token nj as rj (the nonterminal is a child in this rule)
and the rule that expands nonterminal token j as rj.
The intuition is that derivations in which these
two rules agree (at each j) about the implicit label
</bodyText>
<page confidence="0.991029">
239
</page>
<bodyText confidence="0.996973666666667">
for nj, in H are preferable to derivations in which
they do not. Rather than making a decision about
the implicit label, we want to reward psyn when rj
and rj are consistent. Our way of measuring this
consistency is an inner product of preference distri-
butions:
</bodyText>
<equation confidence="0.989802">
φj ∝ � ppref(h  |rj)ppref(h  |rj) (7)
hEW
</equation>
<bodyText confidence="0.9998934375">
This is not quite the whole story, because ppref(·  |r)
is defined as a joint distribution of all the implicit
labels within a rule; the implicit labels are not in-
dependent of each other. Indeed, we want the im-
plicit labels within each rule to be mutually consis-
tent, i.e., to correspond to one of the rule’s preferred
labelings, for both hargs(r) and hargs(r).
Our approach to calculating psyn within the dy-
namic programming algorithm is to recursively cal-
culate preferences for each chart item based on (a)
the smaller items used to construct the item and
(b) the rule that permits combination of the smaller
items into the larger one. We describe how the pref-
erences for chart items are calculated. Let a chart
item be denoted [X, i, j, u,...] where X ∈ N and i
and j are positions in the source sentence, and
</bodyText>
<equation confidence="0.664174">
u : {h ∈ H  |π(h) = X} → [0,1]
</equation>
<bodyText confidence="0.999198375">
(where Eh u(h) = 1) denotes a distribution over
possible X-refinement labels. We will refer to it
below as the left-hand-side preference distribution.
Additional information (such as language model
state) may also be included; it is not relevant here.
The simplest case is for a nonterminal token nj
that has no nonterminal children. Here the left-hand-
side preference distribution is simply given by
</bodyText>
<equation confidence="0.978055">
u(h) = ppref(h  |rj) .
</equation>
<bodyText confidence="0.9996865">
and we define the psyn factor to be φj = 1.
Now consider the dynamic programming step
of combining an already-built item [X, i, j, u, ...]
rooted by explicit nonterminal X, spanning source
sentence positions i to j, with left-hand-side prefer-
ence distribution u, to build a larger item rooted by
Y through a rule r = Y → hγX1γ&apos;, αX1α&apos;, wi with
preferences ppref(·  |r).2 The new item will have
</bodyText>
<footnote confidence="0.859311">
2We assume for the discussion that α, α0 E TS∗ and -y, -y0 E
</footnote>
<bodyText confidence="0.999408666666667">
signature [Y, i − |γ|, j + |γ&apos;|, v,...]. The left-hand-
side preferences v for the new item are calculated as
follows:
</bodyText>
<equation confidence="0.99870525">
v(h) = h (v(h&apos;) where (8)
�
˜v(h) =
h0EW:(h,h0)Ehargs(r)
</equation>
<bodyText confidence="0.9983165">
Renormalizing keeps the preference vectors on the
same scale as those in the rules. The psyn factor φ,
which is factored into the value of the new item, is
calculated as:
</bodyText>
<equation confidence="0.9984285">
φ = � u(h&apos;) (9)
h0EW:(h,h0)Ehargs(r)
</equation>
<bodyText confidence="0.9999144">
so that the value considered for the new item is w ×
φ × ..., where factors relating to pLM, for example,
may also be included. Coming back to our example,
if we let r be the leaf rule producing “eat” at shared
nonterminal n1, we generate an item with:
</bodyText>
<equation confidence="0.9989295">
u = hu(VB) = 0.8, u(VP) = 0.1, u(NP) = 0.1i
φ1 = 1
</equation>
<bodyText confidence="0.990329">
Combining this item with X → h �rt us te, X1
# a place where I can X1 i as r2 at nonterminal n2
generates a new target item with translation “a place
where I can eat”, φ2 = 0.9 and v as calculated in
Fig. 1. In contrast, φ2 = 0 for the derivation where
r is the leaf rule that produces “dish”.
This calculation can be seen as a kind of single-
pass, bottom-up message passing inference method
embedded within the usual dynamic programming
search.
</bodyText>
<sectionHeader confidence="0.990662" genericHeader="method">
5 Decoding Approximations
</sectionHeader>
<bodyText confidence="0.99998775">
As defined above, accurately computing psyn(d) re-
quires extending the chart item structure with u. For
models that use the n-gram LM feature, the item
structure would be:
</bodyText>
<equation confidence="0.703562">
[X, i, j, q(α), u] : w (10)
</equation>
<bodyText confidence="0.990637">
Since u effectively summarizes the choice of rules
in a derivation, this extension would partition the
TT∗. If there are multiple nonterminals on the right-hand side
of the rule, we sum over the longer sequences in hargs(r) and
include appropriate values from the additional “child” items’
preference vectors in the product.
</bodyText>
<equation confidence="0.975268666666667">
ppref(hh,h&apos;i  |r) × u(h&apos;)
240
˜v(S) = ppref((h = S, h&apos; = VB)  |r)u(VB) + ppref((h = S, h&apos; = VP)  |r)u(VP) = (0.4 x 0.8) + (0.3 x 0.1) = 0.35
˜v(SBAR) = p((h = SBAR, h&apos; = VP)  |r)u(VP) = (0.2 x 0.1) = 0.02
v = (v(S) = 0.35/(˜v(S) + ˜v(SBAR)), v(SBAR) = 0.02/˜v(S) + ˜v(SBAR)) = (v(S) = 0.35/0.37, v(SBAR) = 0.02/0.37)
02 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9
</equation>
<figureCaption confidence="0.993706">
Figure 1: Calculating v and 02 for the running example.
</figureCaption>
<bodyText confidence="0.999976047619047">
search space further. To prevent this partitioning, we
follow the approach of Venugopal et al. (2007). We
keep track of u for the best performing derivation
from the set of derivations that share [X, i, j, q(α)]
in a first-pass decoding. In a second top-down pass
similar to Huang and Chiang (2007), we can recal-
culate psyn(d) for alternative derivations in the hy-
pergraph; potentially correcting search errors made
in the first pass.
We face another significant practical challenge
during decoding. In real data conditions, the size
of the preference vector for a single rule can be very
high, especially for rules that include multiple non-
terminal symbols that are located on the left and
right boundaries of γ. For example, the Chinese-
to-English rule X → ( X1 f`] X2 # X1 ’s X2 ) has
over 24K elements in hargs(r) when learned for the
medium-sized NIST task used below. In order to
limit the explosive growth of nonterminals during
decoding for both memory and runtime reasons, we
define the following label pruning parameters:
</bodyText>
<listItem confidence="0.9442399">
• OR: This parameter limits the size of hargs(r) to
the OR top-scoring preferences, defaulting other
values to zero.
• OL: This parameter is the same as OR but applied
only to rules with no nonterminals. The stricter of
OL and OR is applied if both thresholds apply.
• OP: This parameter limits the number labels in
item preference vectors (Equation 8) to the OP
most likely labels during decoding, defaulting
other preferences to zero.
</listItem>
<sectionHeader confidence="0.994694" genericHeader="method">
6 Empirical Results
</sectionHeader>
<bodyText confidence="0.999776652173913">
We evaluate our preference grammar model on
small (IWSLT) and medium (NIST) data Chinese-
to-English translation tasks (described in Table 1).
IWSLT is a limited domain, limited resource task
(Paul, 2006), while NIST is a broadcast news task
with wide genre and domain coverage. We use a
subset of the full training data (67M words of En-
glish text) from the annual NIST MT Evaluation.
Development corpora are used to train model pa-
rameters via MERT. We use a variant of MERT that
prefers sparse solutions where AZ = 0 for as many
features as possible. At each MERT iteration, a sub-
set of features A are assigned 0 weight and optimiza-
tion is repeated. If the resulting BLEU score is not
lower, these features are left at zero.
All systems are built on the SAMT framework
described in Zollmann et al. (2008), using a tri-
gram LM during search and the full-order LM dur-
ing a second hypergraph rescoring pass. Reorder-
ing limits are set to 10 words for all systems. Prun-
ing parameters during decoding limit the number of
derivations at each source span to 300.
The system “Hier.” uses a grammar with a single
nonterminal label as in Chiang (2005). The system
“Syntax” applies the grammar from Zollmann and
Venugopal (2006) that generates a large number of
syntactically motivated nonterminal labels. For the
NIST task, rare rules are discarded based on their
frequency in the training data. Purely lexical rules
(that include no terminal symbols) that occur less
than 2 times, or non-lexical rules that occur less than
4 times are discarded.
IWSLT task: We evaluate the preference gram-
mar system “Pref.” with parameters OR = 100,
OL = 5, OP = 2. Results comparing systems Pref.
to Hier. and Syntax are shown in Table 2. Auto-
matic evaluation results using the preference gram-
mar translation model are positive. The preference
grammar system shows improvements over both the
Hier. and Syntax based systems on both unseen eval-
uation sets IWSLT 2007 and 2008. The improve-
ments are clearest on the BLEU metric (matching
the MERT training criteria). On 2007 test data,
Pref. shows a 1.2-point improvement over Hier.,
while on the 2008 data, there is a 0.6-point improve-
ment. For the IWSLT task, we report additional au-
</bodyText>
<page confidence="0.993717">
241
</page>
<table confidence="0.999486">
System Name Words in Target Text LM singleton 1-n-grams (n) Dev. Test
IWSLT 632K 431K (5) IWSLT06 IWSLT07,08
NIST 67M 102M (4) MT05 MT06
</table>
<tableCaption confidence="0.934797666666667">
Table 1: Training data configurations used to evaluate preference grammars. The number of words in the target text
and the number of singleton 1-n-grams represented in the complete model are the defining statistics that characterize
the scale of each task. For each LM we also indicate the order of the n-gram model.
</tableCaption>
<table confidence="0.999930222222222">
System Dev 2007 2008 2008 2008 PER 2008 2008
BLEU BLEU BLEU WER ↓ ↓ MET. ↑ GTM ↑
(lpen) ↑ (lpen) ↑ (lpen) ↑
Hier. 28.0 37.0 45.9 44.5 39.9 61.8 70.7
(0.89) (0.89) (0.91)
Syntax 30.9 35.5 45.3 45.7 40.4 62.1 71.5
(0.96) (0.94) (0.95)
Pref. 28.3 38.2 46.3 43.8 40.0 61.7 71.2
(0.88) (0.90) (0.91)
</table>
<tableCaption confidence="0.91081575">
Table 2: Translation quality metrics on the IWSLT translation task, with IWSLT 2006 as the development corpora, and
IWSLT 2007 and 2008 as test corpora. Each metric is annotated with an ↑ if increases in the metric value correspond
to increase in translation quality and a ↓ if the opposite is true. We also list length penalties for the BLEU metric to
show that improvements are not due to length optimizations alone.
</tableCaption>
<bodyText confidence="0.999154222222222">
tomatic evaluation metrics that generally rank the
Pref. system higher than Hier. and Syntax. As a fur-
ther confirmation, our feature selection based MERT
chooses to retain Am+1 in the model. While the
IWSLT results are promising, we perform a more
complete evaluation on the NIST translation task.
NIST task: This task generates much larger rule
preference vectors than the IWSLT task simply due
to the size of the training corpora. We build sys-
tems with both OR = 100,10 varying OP. Vary-
ing OP isolates the relative impact of propagating
alternative nonterminal labels within the preference
grammar model. OL = 5 for all NIST systems. Pa-
rameters A are trained via MERT on the OR = 100,
OL = 5, OP = 2 system. BLEU scores for each
preference grammar and baseline system are shown
in Table 3, along with translation times on the test
corpus. We also report length penalties to show that
improvements are not simply due to better tuning of
output length.
The preference grammar systems outperform the
Hier. baseline by 0.5 points on development data,
and upto 0.8 points on unseen test data. While sys-
tems with OR = 100 take significantly longer to
translate the test data than Hier., setting OR = 10
takes approximately as long as the Syntax based sys-
tem but produces better slightly better results (0.3
points).
The improvements in translation quality with the
preference grammar are encouraging, but how much
of this improvement can simply be attributed to
MERT finding a better local optimum for parame-
ters A? To answer this question, we use parameters
A* optimized by MERT for the preference grammar
system to run a purely hierarchical system, denoted
Hier.(A*), which ignores the value of Am+1 during
decoding. While almost half of the improvement
comes from better parameters learned via MERT for
the preference grammar systems, 0.5 points can be
still be attributed purely to the feature pgyn. In addi-
tion, MERT does not set parameter Am+1 to 0, cor-
roborating the value of the pgyn feature again. Note
that Hier.(A*) achieves better scores than the Hier.
system which was trained via MERT without pgyn.
This highlights the local nature of MERT parameter
search, but also points to the possibility that train-
ing with the feature pgyn produced a more diverse
derivation space, resulting in better parameters A.
We see a very small improvement (0.1 point) by al-
lowing the runtime propagation of more than 1 non-
terminal label in the left-hand side posterior distribu-
tion, but the improvement doesn’t extend to OP = 5.
Improved integration of the feature psyn(d) into de-
coding might help to widen this gap.
</bodyText>
<page confidence="0.992411">
242
</page>
<table confidence="0.999536666666667">
System Dev. Test Test
BLEU (lpen) BLEU (lpen) time
(h:mm)
Baseline Systems
Hier. 34.1 (0.99) 31.8 (0.95) 0:12
Syntax 34.7 (0.99) 32.3 (0.95) 0:45
Hier.(A*) - 32.1 (0.95) 0:12
Preference Grammar: NR = 100
NP = 1 - 32.5 (0.96) 3:00
N/�P = 2 34.6 (0.99) 32.6 (0.95) 3:00
NP = 5 - 32.5 (0.95) 3:20
Preference Grammar: NR = 10
NP = 1 - 32.5 (0.95) 1:03
NP = 2 - 32.6 (0.95) 1:10
NP = 5 - 32.5 (0.95) 1:10
</table>
<tableCaption confidence="0.859545">
Table 3: Translation quality and test set translation time
(using 50 machines with 2 tasks per machine) measured
by the BLEU metric for the NIST task. NIST 2006 is
used as the development (Dev.) corpus and NIST 2007 is
used as the unseen evaluation corpus (Test). Dev. scores
are reported for systems that have been separately MERT
trained, Pref. systems share parameters from a single
MERT training. Systems are described in the text.
</tableCaption>
<sectionHeader confidence="0.999916" genericHeader="evaluation">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999987558823529">
There have been significant efforts in the both the
monolingual parsing and machine translation liter-
ature to address the impact of the MAP approxi-
mation and the choice of labels in their respective
models; we survey the work most closely related to
our approach. May and Knight (2006) extract n-
best lists containing unique translations rather than
unique derivations, while Kumar and Byrne (2004)
use the Minimum Bayes Risk decision rule to se-
lect the lowest risk (highest BLEU score) translation
rather than derivation from an n-best list. Tromble
et al. (2008) extend this work to lattice structures.
All of these approaches only marginalize over alter-
native candidate derivations generated by a MAP-
driven decoding process. More recently, work by
Blunsom et al. (2007) propose a purely discrimina-
tive model whose decoding step approximates the
selection of the most likely translation via beam
search. Matsusaki et al. (2005) and Petrov et al.
(2006) propose automatically learning annotations
that add information to categories to improve mono-
lingual parsing quality. Since the parsing task re-
quires selecting the most non-annotated tree, the an-
notations add an additional level of structure that
must be marginalized during search. They demon-
strate improvements in parse quality only when a
variational approximation is used to select the most
likely unannotated tree rather than simply stripping
annotations from the MAP annotated tree. In our
work, we focused on approximating the selection of
the most likely unlabeled derivation during search,
rather than as a post-processing operation; the meth-
ods described above might improve this approxima-
tion, at some computational expense.
</bodyText>
<sectionHeader confidence="0.997386" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999985192307692">
We have proposed a novel grammar formalism that
replaces hard syntactic constraints with “soft” pref-
erences. These preferences are used to compute a
machine translation feature (pgyn(d)) that scores un-
labeled derivations, taking into account traditional
syntactic constraints. Representing syntactic con-
straints as a feature allows MERT to train the cor-
responding weight for this feature relative to others
in the model, allowing systems to learn the relative
importance of labels for particular resource and lan-
guage scenarios as well as for alternative approaches
to labeling PSCFG rules.
This approach takes a step toward addressing
the fragmentation problems of decoding based on
maximum-weighted derivations, by summing the
contributions of compatible label configurations
rather than forcing them to compete. We have sug-
gested an efficient technique to approximate pgyn(d)
that takes advantage of a natural factoring of deriva-
tion scores. Our approach results in improvements
in translation quality on small and medium resource
translation tasks. In future work we plan to focus on
methods to improve on the integration of the pgyn(d)
feature during decoding and techniques that allow us
consider more of the search space through less prun-
ing.
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.8439345">
We appreciate helpful comments from three anony-
mous reviewers. Venugopal and Zollmann were sup-
ported by a Google Research Award. Smith was sup-
ported by NSF grant IIS-0836431.
</bodyText>
<page confidence="0.998189">
243
</page>
<sectionHeader confidence="0.99639" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999893317647059">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2007.
A discriminative latent variable model for statistical
machine translation. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Francisco Casacuberta and Colin de la Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In Proc. of the 5th Inter-
national Colloquium on Grammatical Inference: Al-
gorithms and Applications.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compua-
tional Linguistics (ACL).
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proceed-
ings of the Annual Meeting of the Association for Com-
puational Linguistics (ACL), Sydney, Australia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the Annual Meeting of the Association
for Compuational Linguistics (ACL).
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs and Discussion.
Shankar Kumar and William Byrne. 2004. Min-
imum Bayes-risk decoding for statistical machine
translation. In Proceedings of the Human Lan-
guage Technology and North American Association for
Computational Linguistics Conference (HLT/NAACL),
Boston,MA, May 27-June 1.
Takuya Matsusaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Jonathan May and Kevin Knight. 2006. A better N-best
list: Practical determinization of weighted finite tree
automata. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
Conference (HLT/NAACL).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Compuational
Linguistics (ACL).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Compuational
Linguistics (ACL).
Michael Paul. 2006. Overview of the IWSLT 2006 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the Annual
Meeting of the Association for Compuational Linguis-
tics (ACL).
Khalil Sima’an. 2002. Computational complexity of
probabilistic disambiguation. Grammars, 5(2):125–
151.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum Bayes-risk decod-
ing for statistical machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
Synchronous-CFG driven statistical MT. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics Conference (HLT/NAACL).
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL, New York, June.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the Conference on Computa-
tional Linguistics (COLING).
</reference>
<page confidence="0.998534">
244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.831648">
<title confidence="0.996895">Preference Grammars: Softening Syntactic to Improve Statistical Machine Translation</title>
<author confidence="0.978876">Ashish Venugopal Andreas Zollmann Noah A Smith Stephan</author>
<affiliation confidence="0.952592">Language Technologies School of Computer Carnegie Mellon</affiliation>
<address confidence="0.993026">Pittsburgh, PA 15213,</address>
<abstract confidence="0.999206681818182">We propose a novel probabilistic synchoronous context-free grammar formalism for statistical machine translation, in which syntactic nonterminal labels are represented as “soft” preferences rather than as “hard” matching constraints. This formalism allows to efficiently score derivations without forgoing traditional syntactic constraints. Using this score as a feature in a log-linear model, we are able to approximate the selection of the most likely unlabeled derivation. This reduce fragmentation of probability across differently labeled derivations of the same translation. It also allows the importance of syntactic preferences to be learned alongside other features (e.g., the language model) and for particular labeling procedures. We show improvements in translation quality on small and medium sized Chinese-to-English translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="24658" citStr="Blunsom et al. (2007)" startWordPosition="4219" endWordPosition="4222">ion and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality. Since the parsing task requires selecting the most non-annotated tree, the annotations add an additional level of structure that must be marginalized during search. They demonstrate improvements in parse quality only when a variational approximation is used to select the most likely unannotated tre</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2007</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2007. A discriminative latent variable model for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Colin de la Higuera</author>
</authors>
<title>Computational complexity of problems on probabilistic grammars and transducers.</title>
<date>2000</date>
<booktitle>In Proc. of the 5th International Colloquium on Grammatical Inference: Algorithms and Applications.</booktitle>
<marker>Casacuberta, Higuera, 2000</marker>
<rawString>Francisco Casacuberta and Colin de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In Proc. of the 5th International Colloquium on Grammatical Inference: Algorithms and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</booktitle>
<contexts>
<context position="3914" citStr="Chiang, 2005" startWordPosition="568" endWordPosition="569">ion, reducing the fragmentation problem. Solving this problem exactly is still an NP-hard consensus problem, but we provide approximations that build on well-known PSCFG decoding methods. Our model falls somewhere between PSCFGs that extract nonterminal symbols from parse trees and treat them as part of eˆ ← argmax e 236 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236–244, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics the derivation (Zollmann and Venugopal, 2006) and unlabeled hierarchical structures (Chiang, 2005); we treat nonterminal labels as random variables chosen at each node, with each (unlabeled) rule expressing “preferences” for particular nonterminal labels, learned from data. The paper is organized as follows. In Section 2, we summarize the use of PSCFG grammars for translation. We describe our model (Section 3). Section 4 explains the preference-related calculations, and Section 5 addresses decoding. Experimental results using preference grammars in a loglinear translation model are presented for two standard Chinese-to-English tasks in Section 6. We review related work (Section 7) and conc</context>
<context position="5748" citStr="Chiang (2005)" startWordPosition="889" endWordPosition="890">terminal set (target vocabulary) TT, a shared nonterminal set N and a set R of rules of the form: X —* (-y, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • -y E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a nonnegative real-valued weight assigned to the rule. For visual clarity, we will use the # character to separate the source side of the rule -y from the target side α. PSCFG rules also have an implied one-toone mapping between nonterminal symbols in -y and nonterminals symbols in α. Chiang (2005), Zollmann and Venugopal (2006) and Galley et al. (2006) all use parameterizations of this PSCFG formalism1. Given a source sentence f and a PSCFG G, the translation task can be expressed similarly to monolingual parsing with a PCFG. We aim to find the most likely derivation d of the input source sentence and read off the English translation, identified by composing α from each rule used in the derivation. This search for the most likely translation under the 1Galley et al. (2006) rules are formally defined as tree transducers but have equivalent PSCFG forms. rEIZ � 1 if d respects label const</context>
<context position="8373" citStr="Chiang, 2005" startWordPosition="1342" endWordPosition="1344">—the probability that an unlabeled derivation adheres to traditional syntactic constraints—for derivations built from preference grammar rules. By using psyn(d) as a feature in the log-linear model, we allow the MERT framework to evaluate the importance of syntactic structure relative to other features. The example rules above highlight the potential for psyn(d) to affect the choice of translation. The translation of the Chinese word sequence R ;rt 09 ffi nztK can be performed by expanding the nonterminal in the rule “a place where I can X1” with either “eat” or “dish.” A hierarchical system (Chiang, 2005) would allow either expansion, relying on features like pLM to select the best translation since both expansions occurred the same number of times in the data. A richly-labeled PSCFG as in Zollmann and Venugopal (2006) would immediately reject the rule generating “dish” due to hard label matching constraints, but would produce three identical, competing derivations. Two of these derivations would produce S as a root symbol, while one derivation would produce SBAR. The two S-labeled derivations compete, rather than reinforce the choice of the word “eat,” which they both make. They will also com</context>
<context position="10302" citStr="Chiang, 2005" startWordPosition="1670" endWordPosition="1671">it labels, not to be confused with the explicit label set N. • 7r: H → N, a function that associates each implicit label with a single explicit label. We can therefore think of H symbols as refinements of the nonterminals in N (Matsusaki et al., 2005). • For each rule r, we define a probability distri� bution over vectors h of implicit label bindings for its nonterminals, denoted ppref( h |r). h� includes bindings for the left-hand side nonterminal (h0) as well as each right-hand side nonterminal (h1,..., h|g|). Each hi ∈ H. When N, H are defined to include just a single generic symbol as in (Chiang, 2005), we produce the unlabeled grammar discussed above. In this work, we define • N = {S, X} • H = {NP, DT, NN··· } = NSAMT where N corresponds to the generic labels of Chiang (2005) and H corresponds to the syntactically motivated SAMT labels from (Zollmann and Venugopal, 2006), and 7r maps all elements of H to X. We will use hargs(r) to denote the set of all h� = hh0, h1, ..., hki ∈ Hk+1 that are valid bindings for the rule with nonzero preference probability. The preference distributions ppref from each rule used in d are used to compute psyn(d) as described next. 4 Computing feature psyn(d) Le</context>
<context position="18186" citStr="Chiang (2005)" startWordPosition="3111" endWordPosition="3112">ny features as possible. At each MERT iteration, a subset of features A are assigned 0 weight and optimization is repeated. If the resulting BLEU score is not lower, these features are left at zero. All systems are built on the SAMT framework described in Zollmann et al. (2008), using a trigram LM during search and the full-order LM during a second hypergraph rescoring pass. Reordering limits are set to 10 words for all systems. Pruning parameters during decoding limit the number of derivations at each source span to 300. The system “Hier.” uses a grammar with a single nonterminal label as in Chiang (2005). The system “Syntax” applies the grammar from Zollmann and Venugopal (2006) that generates a large number of syntactically motivated nonterminal labels. For the NIST task, rare rules are discarded based on their frequency in the training data. Purely lexical rules (that include no terminal symbols) that occur less than 2 times, or non-lexical rules that occur less than 4 times are discarded. IWSLT task: We evaluate the preference grammar system “Pref.” with parameters OR = 100, OL = 5, OP = 2. Results comparing systems Pref. to Hier. and Syntax are shown in Table 2. Automatic evaluation resul</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase based translation. Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="15858" citStr="Chiang (2007)" startWordPosition="2707" endWordPosition="2708">P) |r)u(VP) = (0.4 x 0.8) + (0.3 x 0.1) = 0.35 ˜v(SBAR) = p((h = SBAR, h&apos; = VP) |r)u(VP) = (0.2 x 0.1) = 0.02 v = (v(S) = 0.35/(˜v(S) + ˜v(SBAR)), v(SBAR) = 0.02/˜v(S) + ˜v(SBAR)) = (v(S) = 0.35/0.37, v(SBAR) = 0.02/0.37) 02 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9 Figure 1: Calculating v and 02 for the running example. search space further. To prevent this partitioning, we follow the approach of Venugopal et al. (2007). We keep track of u for the best performing derivation from the set of derivations that share [X, i, j, q(α)] in a first-pass decoding. In a second top-down pass similar to Huang and Chiang (2007), we can recalculate psyn(d) for alternative derivations in the hypergraph; potentially correcting search errors made in the first pass. We face another significant practical challenge during decoding. In real data conditions, the size of the preference vector for a single rule can be very high, especially for rules that include multiple nonterminal symbols that are located on the left and right boundaries of γ. For example, the Chineseto-English rule X → ( X1 f`] X2 # X1 ’s X2 ) has over 24K elements in hargs(r) when learned for the medium-sized NIST task used below. In order to limit the exp</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase based translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inferences and training of context-rich syntax translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="5804" citStr="Galley et al. (2006)" startWordPosition="897" endWordPosition="900">terminal set N and a set R of rules of the form: X —* (-y, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • -y E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a nonnegative real-valued weight assigned to the rule. For visual clarity, we will use the # character to separate the source side of the rule -y from the target side α. PSCFG rules also have an implied one-toone mapping between nonterminal symbols in -y and nonterminals symbols in α. Chiang (2005), Zollmann and Venugopal (2006) and Galley et al. (2006) all use parameterizations of this PSCFG formalism1. Given a source sentence f and a PSCFG G, the translation task can be expressed similarly to monolingual parsing with a PCFG. We aim to find the most likely derivation d of the input source sentence and read off the English translation, identified by composing α from each rule used in the derivation. This search for the most likely translation under the 1Galley et al. (2006) rules are formally defined as tree transducers but have equivalent PSCFG forms. rEIZ � 1 if d respects label constraints psyn(d) = 0 otherwise 3) � where A = A0 · · · Am+</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inferences and training of context-rich syntax translation models. In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</booktitle>
<contexts>
<context position="15858" citStr="Huang and Chiang (2007)" startWordPosition="2705" endWordPosition="2708"> S, h&apos; = VP) |r)u(VP) = (0.4 x 0.8) + (0.3 x 0.1) = 0.35 ˜v(SBAR) = p((h = SBAR, h&apos; = VP) |r)u(VP) = (0.2 x 0.1) = 0.02 v = (v(S) = 0.35/(˜v(S) + ˜v(SBAR)), v(SBAR) = 0.02/˜v(S) + ˜v(SBAR)) = (v(S) = 0.35/0.37, v(SBAR) = 0.02/0.37) 02 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9 Figure 1: Calculating v and 02 for the running example. search space further. To prevent this partitioning, we follow the approach of Venugopal et al. (2007). We keep track of u for the best performing derivation from the set of derivations that share [X, i, j, q(α)] in a first-pass decoding. In a second top-down pass similar to Huang and Chiang (2007), we can recalculate psyn(d) for alternative derivations in the hypergraph; potentially correcting search errors made in the first pass. We face another significant practical challenge during decoding. In real data conditions, the size of the preference vector for a single rule can be very high, especially for rules that include multiple nonterminal symbols that are located on the left and right boundaries of γ. For example, the Chineseto-English rule X → ( X1 f`] X2 # X1 ’s X2 ) has over 24K elements in hargs(r) when learned for the medium-sized NIST task used below. In order to limit the exp</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models. Computational Linguistics, Squibs and Discussion.</title>
<date>1999</date>
<contexts>
<context position="2660" citStr="Knight, 1999" startWordPosition="378" endWordPosition="379">ules. A PSCFG derivation is a synchronous parse tree. Defining the translation function as finding the best derivation has the unfortunate side effect of forcing differently-derived versions of the same target sentence to compete with each other. In other words, the true score of each translation is “fragmented” across many derivations, so that each translation’s most probable derivation is the only one that matters. The more Bayesian approach of finding the most probable translation (integrating out the derivations) instantiates an NP-hard inference problem even for simple word-based models (Knight, 1999); for grammar-based translation it is known as the consensus problem (Casacuberta and de la Higuera, 2000; Sima’an, 2002). With weights interpreted as probabilities, the maximum-weighted derivation is the maximum a posteriori (MAP) derivation: maxp(e,d |f) d where f is the source sentence, e ranges over target sentences, and d ranges over PSCFG derivations (synchronous trees). This is often described as an approximation to the most probable translation, argmaxe Ed p(e, d |f). In this paper, we will describe a technique that aims to find the most probable equivalence class of unlabeled derivati</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, Squibs and Discussion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL), Boston,MA,</booktitle>
<contexts>
<context position="24284" citStr="Kumar and Byrne (2004)" startWordPosition="4159" endWordPosition="4162"> the unseen evaluation corpus (Test). Dev. scores are reported for systems that have been separately MERT trained, Pref. systems share parameters from a single MERT training. Systems are described in the text. 7 Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations </context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL), Boston,MA, May 27-June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsusaki</author>
<author>Yusuke Miyao</author>
<author>Junichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="9940" citStr="Matsusaki et al., 2005" startWordPosition="1602" endWordPosition="1605">ible with the arguments of this higherup rule. We design psyn(d) to reflect compatibility between two rules (one expanding a right-hand side nonterminal in the other), based on label preference distributions. 3.2 Formal definition Probabilistic synchronous context-free preference grammars are defined as PSCFGs with the following additional elements: • H: a set of implicit labels, not to be confused with the explicit label set N. • 7r: H → N, a function that associates each implicit label with a single explicit label. We can therefore think of H symbols as refinements of the nonterminals in N (Matsusaki et al., 2005). • For each rule r, we define a probability distri� bution over vectors h of implicit label bindings for its nonterminals, denoted ppref( h |r). h� includes bindings for the left-hand side nonterminal (h0) as well as each right-hand side nonterminal (h1,..., h|g|). Each hi ∈ H. When N, H are defined to include just a single generic symbol as in (Chiang, 2005), we produce the unlabeled grammar discussed above. In this work, we define • N = {S, X} • H = {NP, DT, NN··· } = NSAMT where N corresponds to the generic labels of Chiang (2005) and H corresponds to the syntactically motivated SAMT label</context>
<context position="24815" citStr="Matsusaki et al. (2005)" startWordPosition="4243" endWordPosition="4246"> containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality. Since the parsing task requires selecting the most non-annotated tree, the annotations add an additional level of structure that must be marginalized during search. They demonstrate improvements in parse quality only when a variational approximation is used to select the most likely unannotated tree rather than simply stripping annotations from the MAP annotated tree. In our work, we focused on approximating the selection of the most likely unlabeled d</context>
</contexts>
<marker>Matsusaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsusaki, Yusuke Miyao, and Junichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>A better N-best list: Practical determinization of weighted finite tree automata.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</booktitle>
<contexts>
<context position="24172" citStr="May and Knight (2006)" startWordPosition="4143" endWordPosition="4146"> the BLEU metric for the NIST task. NIST 2006 is used as the development (Dev.) corpus and NIST 2007 is used as the unseen evaluation corpus (Test). Dev. scores are reported for systems that have been separately MERT trained, Pref. systems share parameters from a single MERT training. Systems are described in the text. 7 Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translati</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jonathan May and Kevin Knight. 2006. A better N-best list: Practical determinization of weighted finite tree automata. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</booktitle>
<contexts>
<context position="7264" citStr="Och, 2003" startWordPosition="1158" endWordPosition="1159">redundantly) requires every nonterminal token to be expanded by a rule with that nonterminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that Am+1 can be effectively ignored when psyn is defined as in Equation 3. Z( X) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equa� tion 1. Feature weights A are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is: O (|f |3 [|N ||T T |2 (n− 1)�Kl (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and 237 have label preference distributions. These are estimated as relative frequencies from the labelings of the base, unlabeled rule. Our primary contribution is how we compute psyn(d)—the probability that an unlabeled derivation adheres to traditional syntactic constraints—for derivatio</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</booktitle>
<contexts>
<context position="7185" citStr="Papineni et al., 2002" startWordPosition="1144" endWordPosition="1147">ence, a collection of m rule feature functions hi : R —* R&gt;0, and a “syntax” feature that (redundantly) requires every nonterminal token to be expanded by a rule with that nonterminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that Am+1 can be effectively ignored when psyn is defined as in Equation 3. Z( X) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equa� tion 1. Feature weights A are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is: O (|f |3 [|N ||T T |2 (n− 1)�Kl (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and 237 have label preference distributions. These are estimated as relative frequencies from the labelings of the base, unlabeled rule. Our primary contribution is how we compute psyn(d)—the probability that an </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="17250" citStr="Paul, 2006" startWordPosition="2939" endWordPosition="2940"> to the OR top-scoring preferences, defaulting other values to zero. • OL: This parameter is the same as OR but applied only to rules with no nonterminals. The stricter of OL and OR is applied if both thresholds apply. • OP: This parameter limits the number labels in item preference vectors (Equation 8) to the OP most likely labels during decoding, defaulting other preferences to zero. 6 Empirical Results We evaluate our preference grammar model on small (IWSLT) and medium (NIST) data Chineseto-English translation tasks (described in Table 1). IWSLT is a limited domain, limited resource task (Paul, 2006), while NIST is a broadcast news task with wide genre and domain coverage. We use a subset of the full training data (67M words of English text) from the annual NIST MT Evaluation. Development corpora are used to train model parameters via MERT. We use a variant of MERT that prefers sparse solutions where AZ = 0 for as many features as possible. At each MERT iteration, a subset of features A are assigned 0 weight and optimization is repeated. If the resulting BLEU score is not lower, these features are left at zero. All systems are built on the SAMT framework described in Zollmann et al. (2008</context>
</contexts>
<marker>Paul, 2006</marker>
<rawString>Michael Paul. 2006. Overview of the IWSLT 2006 evaluation campaign. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</booktitle>
<contexts>
<context position="24840" citStr="Petrov et al. (2006)" startWordPosition="4248" endWordPosition="4251">ons rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality. Since the parsing task requires selecting the most non-annotated tree, the annotations add an additional level of structure that must be marginalized during search. They demonstrate improvements in parse quality only when a variational approximation is used to select the most likely unannotated tree rather than simply stripping annotations from the MAP annotated tree. In our work, we focused on approximating the selection of the most likely unlabeled derivation during search, </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the Annual Meeting of the Association for Compuational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Computational complexity of probabilistic disambiguation.</title>
<date>2002</date>
<journal>Grammars,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>151</pages>
<marker>Sima’an, 2002</marker>
<rawString>Khalil Sima’an. 2002. Computational complexity of probabilistic disambiguation. Grammars, 5(2):125– 151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="24450" citStr="Tromble et al. (2008)" startWordPosition="4187" endWordPosition="4190">aining. Systems are described in the text. 7 Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality. Since the parsing task requires selecting the most non-annotated tree, the annotations add </context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>An efficient two-pass approach to Synchronous-CFG driven statistical MT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</booktitle>
<contexts>
<context position="15661" citStr="Venugopal et al. (2007)" startWordPosition="2669" endWordPosition="2672">es in hargs(r) and include appropriate values from the additional “child” items’ preference vectors in the product. ppref(hh,h&apos;i |r) × u(h&apos;) 240 ˜v(S) = ppref((h = S, h&apos; = VB) |r)u(VB) + ppref((h = S, h&apos; = VP) |r)u(VP) = (0.4 x 0.8) + (0.3 x 0.1) = 0.35 ˜v(SBAR) = p((h = SBAR, h&apos; = VP) |r)u(VP) = (0.2 x 0.1) = 0.02 v = (v(S) = 0.35/(˜v(S) + ˜v(SBAR)), v(SBAR) = 0.02/˜v(S) + ˜v(SBAR)) = (v(S) = 0.35/0.37, v(SBAR) = 0.02/0.37) 02 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9 Figure 1: Calculating v and 02 for the running example. search space further. To prevent this partitioning, we follow the approach of Venugopal et al. (2007). We keep track of u for the best performing derivation from the set of derivations that share [X, i, j, q(α)] in a first-pass decoding. In a second top-down pass similar to Huang and Chiang (2007), we can recalculate psyn(d) for alternative derivations in the hypergraph; potentially correcting search errors made in the first pass. We face another significant practical challenge during decoding. In real data conditions, the size of the preference vector for a single rule can be very high, especially for rules that include multiple nonterminal symbols that are located on the left and right boun</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, and Stephan Vogel. 2007. An efficient two-pass approach to Synchronous-CFG driven statistical MT. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation, HLT/NAACL,</booktitle>
<location>New York,</location>
<contexts>
<context position="3861" citStr="Zollmann and Venugopal, 2006" startWordPosition="560" endWordPosition="563"> class of unlabeled derivations, rather than a single labeled derivation, reducing the fragmentation problem. Solving this problem exactly is still an NP-hard consensus problem, but we provide approximations that build on well-known PSCFG decoding methods. Our model falls somewhere between PSCFGs that extract nonterminal symbols from parse trees and treat them as part of eˆ ← argmax e 236 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236–244, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics the derivation (Zollmann and Venugopal, 2006) and unlabeled hierarchical structures (Chiang, 2005); we treat nonterminal labels as random variables chosen at each node, with each (unlabeled) rule expressing “preferences” for particular nonterminal labels, learned from data. The paper is organized as follows. In Section 2, we summarize the use of PSCFG grammars for translation. We describe our model (Section 3). Section 4 explains the preference-related calculations, and Section 5 addresses decoding. Experimental results using preference grammars in a loglinear translation model are presented for two standard Chinese-to-English tasks in S</context>
<context position="5779" citStr="Zollmann and Venugopal (2006)" startWordPosition="891" endWordPosition="895">arget vocabulary) TT, a shared nonterminal set N and a set R of rules of the form: X —* (-y, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • -y E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a nonnegative real-valued weight assigned to the rule. For visual clarity, we will use the # character to separate the source side of the rule -y from the target side α. PSCFG rules also have an implied one-toone mapping between nonterminal symbols in -y and nonterminals symbols in α. Chiang (2005), Zollmann and Venugopal (2006) and Galley et al. (2006) all use parameterizations of this PSCFG formalism1. Given a source sentence f and a PSCFG G, the translation task can be expressed similarly to monolingual parsing with a PCFG. We aim to find the most likely derivation d of the input source sentence and read off the English translation, identified by composing α from each rule used in the derivation. This search for the most likely translation under the 1Galley et al. (2006) rules are formally defined as tree transducers but have equivalent PSCFG forms. rEIZ � 1 if d respects label constraints psyn(d) = 0 otherwise 3)</context>
<context position="8591" citStr="Zollmann and Venugopal (2006)" startWordPosition="1376" endWordPosition="1379">allow the MERT framework to evaluate the importance of syntactic structure relative to other features. The example rules above highlight the potential for psyn(d) to affect the choice of translation. The translation of the Chinese word sequence R ;rt 09 ffi nztK can be performed by expanding the nonterminal in the rule “a place where I can X1” with either “eat” or “dish.” A hierarchical system (Chiang, 2005) would allow either expansion, relying on features like pLM to select the best translation since both expansions occurred the same number of times in the data. A richly-labeled PSCFG as in Zollmann and Venugopal (2006) would immediately reject the rule generating “dish” due to hard label matching constraints, but would produce three identical, competing derivations. Two of these derivations would produce S as a root symbol, while one derivation would produce SBAR. The two S-labeled derivations compete, rather than reinforce the choice of the word “eat,” which they both make. They will also compete for consideration by any decoder that prunes derivations to keep runtime down. The rule preferences indicate that VB and VP are both valid labels for the rule translating to “eat”, and both of these labels are com</context>
<context position="10577" citStr="Zollmann and Venugopal, 2006" startWordPosition="1719" endWordPosition="1723">each rule r, we define a probability distri� bution over vectors h of implicit label bindings for its nonterminals, denoted ppref( h |r). h� includes bindings for the left-hand side nonterminal (h0) as well as each right-hand side nonterminal (h1,..., h|g|). Each hi ∈ H. When N, H are defined to include just a single generic symbol as in (Chiang, 2005), we produce the unlabeled grammar discussed above. In this work, we define • N = {S, X} • H = {NP, DT, NN··· } = NSAMT where N corresponds to the generic labels of Chiang (2005) and H corresponds to the syntactically motivated SAMT labels from (Zollmann and Venugopal, 2006), and 7r maps all elements of H to X. We will use hargs(r) to denote the set of all h� = hh0, h1, ..., hki ∈ Hk+1 that are valid bindings for the rule with nonzero preference probability. The preference distributions ppref from each rule used in d are used to compute psyn(d) as described next. 4 Computing feature psyn(d) Let us view a derivation d as a collection of nonterminal tokens nj, j ∈ {1, ..., |d|}. Each nj takes an explicit label in N. The score psyn(d) is a product, with one factor per nj in the derivation d: |d| psyn(d) = H φj (6) j=1 Each φj factor considers the two rules that nj p</context>
<context position="18262" citStr="Zollmann and Venugopal (2006)" startWordPosition="3120" endWordPosition="3123"> features A are assigned 0 weight and optimization is repeated. If the resulting BLEU score is not lower, these features are left at zero. All systems are built on the SAMT framework described in Zollmann et al. (2008), using a trigram LM during search and the full-order LM during a second hypergraph rescoring pass. Reordering limits are set to 10 words for all systems. Pruning parameters during decoding limit the number of derivations at each source span to 300. The system “Hier.” uses a grammar with a single nonterminal label as in Chiang (2005). The system “Syntax” applies the grammar from Zollmann and Venugopal (2006) that generates a large number of syntactically motivated nonterminal labels. For the NIST task, rare rules are discarded based on their frequency in the training data. Purely lexical rules (that include no terminal symbols) that occur less than 2 times, or non-lexical rules that occur less than 4 times are discarded. IWSLT task: We evaluate the preference grammar system “Pref.” with parameters OR = 100, OL = 5, OP = 2. Results comparing systems Pref. to Hier. and Syntax are shown in Table 2. Automatic evaluation results using the preference grammar translation model are positive. The preferen</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, HLT/NAACL, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz J Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="7360" citStr="Zollmann et al. (2008)" startWordPosition="1171" endWordPosition="1174">terminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that Am+1 can be effectively ignored when psyn is defined as in Equation 3. Z( X) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equa� tion 1. Feature weights A are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is: O (|f |3 [|N ||T T |2 (n− 1)�Kl (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and 237 have label preference distributions. These are estimated as relative frequencies from the labelings of the base, unlabeled rule. Our primary contribution is how we compute psyn(d)—the probability that an unlabeled derivation adheres to traditional syntactic constraints—for derivations built from preference grammar rules. By using psyn(d) as a feature in the log-linear model, w</context>
<context position="17851" citStr="Zollmann et al. (2008)" startWordPosition="3049" endWordPosition="3052">rce task (Paul, 2006), while NIST is a broadcast news task with wide genre and domain coverage. We use a subset of the full training data (67M words of English text) from the annual NIST MT Evaluation. Development corpora are used to train model parameters via MERT. We use a variant of MERT that prefers sparse solutions where AZ = 0 for as many features as possible. At each MERT iteration, a subset of features A are assigned 0 weight and optimization is repeated. If the resulting BLEU score is not lower, these features are left at zero. All systems are built on the SAMT framework described in Zollmann et al. (2008), using a trigram LM during search and the full-order LM during a second hypergraph rescoring pass. Reordering limits are set to 10 words for all systems. Pruning parameters during decoding limit the number of derivations at each source span to 300. The system “Hier.” uses a grammar with a single nonterminal label as in Chiang (2005). The system “Syntax” applies the grammar from Zollmann and Venugopal (2006) that generates a large number of syntactically motivated nonterminal labels. For the NIST task, rare rules are discarded based on their frequency in the training data. Purely lexical rules</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Franz J. Och, and Jay Ponte. 2008. A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT. In Proceedings of the Conference on Computational Linguistics (COLING).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>