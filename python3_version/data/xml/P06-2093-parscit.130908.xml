<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.910277">
Continuous Space Language Models for Statistical Machine Translation
</title>
<author confidence="0.544305">
Holger Schwenk and Daniel Dchelotte and Jean-Luc Gauvain
</author>
<note confidence="0.5998195">
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
</note>
<email confidence="0.989853">
{schwenk,dechelot,gauvain}@limsi.fr
</email>
<sectionHeader confidence="0.99542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803041666667">
Statistical machine translation systems are
based on one or more translation mod-
els and a language model of the target
language. While many different trans-
lation models and phrase extraction al-
gorithms have been proposed, a standard
word n-gram back-off language model is
used in most systems.
In this work, we propose to use a new sta-
tistical language model that is based on a
continuous representation of the words in
the vocabulary. A neural network is used
to perform the projection and the proba-
bility estimation. We consider the trans-
lation of European Parliament Speeches.
This task is part of an international evalua-
tion organized by the TC-STAR project in
2006. The proposed method achieves con-
sistent improvements in the BLEU score
on the development and test data.
We also present algorithms to improve the
estimation of the language model proba-
bilities when splitting long sentences into
shorter chunks.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996995">
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source sen-
tence f. Among all possible target sentences the
one with maximal probability is chosen. The clas-
sical Bayes relation is used to introduce a target
language model (Brown et al., 1993):
where Pr(f|e) is the translation model and Pr(e)
is the target language model. This approach is
usually referred to as the noisy source-channel ap-
proach in statistical machine translation.
Since the introduction of this basic model, many
improvements have been made, but it seems that
research is mainly focused on better translation
and alignment models or phrase extraction algo-
rithms as demonstrated by numerous publications
on these topics. On the other hand, we are aware
of only a small amount of papers investigating
new approaches to language modeling for statis-
tical machine translation. Traditionally, statistical
machine translation systems use a simple 3-gram
back-off language model (LM) during decoding to
generate n-best lists. These n-best lists are then
rescored using a log-linear combination of feature
functions (Och and Ney, 2002):
</bodyText>
<equation confidence="0.9510435">
e� Pz� arg max Pr(e)A1 Pr(f|e)A2 (1)
e
</equation>
<bodyText confidence="0.999880722222222">
where the coefficients Ai are optimized on a devel-
opment set, usually maximizing the BLEU score.
In addition to the standard feature functions, many
others have been proposed, in particular several
ones that aim at improving the modeling of the tar-
get language. In most SMT systems the use of a
4-gram back-off language model usually achieves
improvements in the BLEU score in comparison
to the 3-gram LM used during decoding. It seems
however difficult to improve upon the 4-gram LM.
Many different feature functions were explored in
(Och et al., 2004). In that work, the incorporation
of part-of-speech (POS) information gave only a
small improvement compared to a 3-gram back-
off LM. In another study, a factored LM using
POS information achieved the same results as the
4-gram LM (Kirchhoff and Yang, 2005). Syntax-
based LMs were investigated in (Charniak et al.,
</bodyText>
<note confidence="0.529254">
e� = arg max Pr(e|f) = argmax Pr(f|e) Pr(e)
e e
</note>
<page confidence="0.976368">
723
</page>
<note confidence="0.724535">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723–730,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999894734375">
2003), and reranking of translation hypothesis us-
ing structural properties in (Hasan et al., 2006).
An interesting experiment was reported at the
NIST 2005 MT evaluation workshop (Och, 2005):
starting with a 5-gram LM trained on 75 million
words of Broadcast News data, a gain of about
0.5 point BLEU was observed each time when the
amount of LM training data was doubled, using at
the end 237 billion words of texts. Most of this
additional data was collected by Google on the In-
ternet. We believe that this kind of approach is dif-
ficult to apply to other tasks than Broadcast News
and other target languages than English. There are
many areas where automatic machine translation
could be deployed and for which considerably less
appropriate in-domain training data is available.
We could for instance mention automatic trans-
lation of medical records, translation systems for
tourism related tasks or even any task for which
Broadcast news and Web texts is of limited help.
In this work, we consider the translation of Eu-
ropean Parliament Speeches from Spanish to En-
glish, in the framework of an international evalua-
tion organized by the European TC-STAR project
in February 2006. The training data consists of
about 35M words of aligned texts that are also
used to train the target LM. In our experiments,
adding more than 580M words of Broadcast News
data had no impact on the BLEU score, despite
a notable decrease of the perplexity of the target
LM. Therefore, we suggest to use more complex
statistical LMs that are expected to take better ad-
vantage of the limited amount of appropriate train-
ing data. Promising candidates are random forest
LMs (Xu and Jelinek, 2004), random cluster LMs
(Emami and Jelinek, 2005) and the neural network
LM (Bengio et al., 2003). In this paper, we inves-
tigate whether the latter approach can be used in a
statistical machine translation system.
The basic idea of the neural network LM, also
called continuous space LM, is to project the word
indices onto a continuous space and to use a prob-
ability estimator operating on this space. Since the
resulting probability functions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. A neu-
ral network can be used to simultaneously learn
the projection of the words onto the continuous
space and to estimate the n-gram probabilities.
This is still a n-gram approach, but the LM pos-
terior probabilities are ”interpolated” for any pos-
sible context of length n-1 instead of backing-off
to shorter contexts. This approach was success-
fully used in large vocabulary speech recognition
(Schwenk and Gauvain, 2005), and we are inter-
ested here if similar ideas can be applied to statis-
tical machine translation.
This paper is organized as follows. In the next
section we first describe the baseline statistical
machine translation system. Section 3 presents
the architecture of the continuous space LM and
section 4 summarizes the experimental evaluation.
The paper concludes with a discussion of future
research directions.
</bodyText>
<sectionHeader confidence="0.960048" genericHeader="method">
2 Statistical Translation Engine
</sectionHeader>
<bodyText confidence="0.999646875">
A word-based translation engine is used based on
the so-called IBM-4 model (Brown et al., 1993).
A brief description of this model is given below
along with the decoding algorithm.
The search algorithm aims at finding what tar-
get sentence e is most likely to have produced the
observed source sentence f. The translation model
Pr(f|e) is decomposed into four components:
</bodyText>
<listItem confidence="0.956492111111111">
1. a fertility model;
2. a lexical model of the form t(f|e), which
gives the probability that the target word e
translates into the source word f;
3. a distortion model, that characterizes how
words are reordered when translated;
4. and probabilities to model the insertion of
source words that are not aligned to any tar-
get words.
</listItem>
<bodyText confidence="0.99985375">
An A* search was implemented to find the best
translation as predicted by the model, when given
enough time and memory, i.e., provided pruning
did not eliminate it. The decoder manages par-
tial hypotheses, each of which translates a subset
of source words into a sequence of target words.
Expanding a partial hypothesis consists of cover-
ing one extra source position (in random order)
and, by doing so, appending one, several or possi-
bly zero target words to its target word sequence.
For details about the implemented algorithm, the
reader is referred to (D´echelotte et al., 2006).
Decoding uses a 3-gram back-off target lan-
guage model. Equivalent hypotheses are merged,
and only the best scoring one is further expanded.
The decoder generates a lattice representing the
</bodyText>
<page confidence="0.998415">
724
</page>
<figureCaption confidence="0.998732">
Figure 1: Example of a translation lattice. Source
</figureCaption>
<bodyText confidence="0.930652375">
sentence: “conviene recordarlo , porque puede
que se haya olvidado .”, Reference 1: “it is ap-
propriate to remember this, because it may have
been forgotten .” Reference 2: “it is good to re-
member this, because maybe we forgot it .”
explored search space. Figure 1 shows an example
of such a search space, here heavily pruned for the
sake of clarity.
</bodyText>
<subsectionHeader confidence="0.99903">
2.1 Sentence Splitting
</subsectionHeader>
<bodyText confidence="0.982548416666666">
The execution complexity of our SMT decoder in-
creases non-linearly with the length of the sen-
tence to be translated. Therefore, the source text
is split into smaller chunks, each one being trans-
lated separately. The chunks are then concatenated
together. Several algorithms have been proposed
in the literature that try to find the best splits, see
for instance (Berger et al., 1996). In this work, we
first split long sentences at punctuation marks, the
remaining segments that still exceed the allowed
length being split linearly. In a second pass, ad-
joining very short chunks are merged together.
During decoding, target LM probabilities of the
type Pr(wi&lt;s&gt;) and Pr(&lt;/s&gt;|wn_1wn) will be
requested at the beginning and at the end of the
hypothesized target sentence respectively.1 This is
correct when a whole sentence is translated, but
leads to wrong LM probabilities when processing
smaller chunks. Therefore, we define a sentence
break symbol, &lt;b&gt;, that is used at the beginning
and at the end of a chunk. During decoding a 3-
gram back-off LM is used that was trained on text
where sentence break symbols have been added.
Each chunk is translated and a lattice is gen-
</bodyText>
<footnote confidence="0.990784">
1The symbols &lt;s&gt; and &lt;/s&gt; denote the begin and end of
sentence marker respectively.
</footnote>
<bodyText confidence="0.999514857142857">
erated. The individual lattices are then joined,
omitting the sentence break symbols. Finally, the
resulting lattice is rescored with a LM that was
trained on text without sentence breaks. In that
way we find the best junction of the chunks. Sec-
tion 4.1 provides comparative results of the differ-
ent algorithms to split and join sentences.
</bodyText>
<subsectionHeader confidence="0.998588">
2.2 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.999962142857143">
It is nowadays common practice to optimize the
coefficients of the log-linear combination of fea-
ture functions by maximizing the BLEU score on
the development data (Och and Ney, 2002). This
is usually done by first creating n-best lists that
are then reranked using an iterative optimization
algorithm.
In this work, a slightly different procedure was
used that operates directly on the translation lat-
tices. We believe that this is more efficient than
reranking n-best lists since it guarantees that al-
ways all possible hypotheses are considered. The
decoder first generates large lattices using the cur-
rent set of parameters. These lattices are then
processed by a separate tool that extracts the best
path, given the coefficients of six feature functions
(translations, distortion, fertility, spontaneous in-
sertion, target language model probability, and a
sentence length penalty). Then, the BLEU score
of the extracted solution is calculated. This tool is
called in a loop by the public numerical optimiza-
tion tool Condor (Berghen and Bersini, 2005). The
solution vector was usually found after about 100
iterations. In our experiments, only two cycles
of lattice generation and parameter optimization
were necessary (with a very small difference in the
BLEU score).
In all our experiments, the 4-gram back-off and
the neural network LM are used to calculate lan-
guage model probabilities that replace those of the
default 3-gram LM. An alternative would be to de-
fine each LM as a feature function and to combine
them under the log-linear model framework, us-
ing maximum BLEU training. We believe that this
would not make a notable difference in our experi-
ments since we do interpolate the individual LMs,
the coefficients being optimized to minimize per-
plexity on the development data. However, this
raises the interesting question whether the two cri-
teria lead to equivalent performance. The result
section provides some experimental evidence on
this topic.
</bodyText>
<figure confidence="0.997007323943662">
has
been
it
forgotten
say
can
that
it
has
been
.
forgotten
.
remind
should
you
I
.
,
because
forgotten
we
I
should
remember
,
that
,
because
because
they
it
can
can
be
be
has
forgotten
.
forgotten
.
.
,
we
that
,
that
can
be
have
forgotten
.
must
remember
be
have
that
that
, because
it
they
we
can
can
have
be
has forgotten
forgotten
.
be
can
</figure>
<page confidence="0.975564">
725
</page>
<sectionHeader confidence="0.955989" genericHeader="method">
3 Continuous Space Language Models
</sectionHeader>
<bodyText confidence="0.999234285714286">
The architecture of the neural network LM is
shown in Figure 2. A standard fully-connected
multi-layer perceptron is used. The inputs to
the neural network are the indices of the n−1
previous words in the vocabulary hj=wj_n+1,
..., wj_2, wj_1 and the outputs are the posterior
probabilities of all words of the vocabulary:
</bodyText>
<equation confidence="0.986801">
P(wj = i|hj) Vi E [1, N] (2)
</equation>
<bodyText confidence="0.999252384615385">
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith word
of the vocabulary is coded by setting the ith ele-
ment of the vector to 1 and all the other elements
to 0. The ith line of the N x P dimensional pro-
jection matrix corresponds to the continuous rep-
resentation of the ith word. Let us denote cl these
projections, dj the hidden layer activities, oi the
outputs, pi their softmax normalization, and mjl,
bj, vij and ki the hidden and output layer weights
and the corresponding biases. Using these nota-
tions, the neural network performs the following
operations:
</bodyText>
<equation confidence="0.996334">
IE �
tanh mjl cl + bj (3)
l
vij dj + ki (4)
j
N
eoi / E eo, (5)
r=1
</equation>
<bodyText confidence="0.999977">
The value of the output neuron pi corresponds di-
rectly to the probability P(wj =i|hj). Training is
performed with the standard back-propagation al-
gorithm minimizing the following error function:
</bodyText>
<equation confidence="0.99447">
� �
�E E
ti log pi + Q m2 jl + v2 ij �(6)
jl ij
</equation>
<bodyText confidence="0.999497272727273">
where ti denotes the desired output, i.e., the prob-
ability should be 1.0 for the next word in the train-
ing sentence and 0.0 for all the other ones. The
first part of this equation is the cross-entropy be-
tween the output and the target probability dis-
tributions, and the second part is a regulariza-
tion term that aims to prevent the neural network
from overfitting the training data (weight decay).
The parameter Q has to be determined experimen-
tally. Training is done using a resampling algo-
rithm (Schwenk and Gauvain, 2005).
</bodyText>
<figure confidence="0.375077">
Neural Network
</figure>
<figureCaption confidence="0.931294">
Figure 2: Architecture of the continuous space
</figureCaption>
<bodyText confidence="0.982484473684211">
LM. hj denotes the context wj_n+1, ... , wj_1. P
is the size of one projection and H,N is the size
of the hidden and output layer respectively. When
short-lists are used the size of the output layer is
much smaller then the size of the vocabulary.
It can be shown that the outputs of a neural net-
work trained in this manner converge to the poste-
rior probabilities. Therefore, the neural network
directly minimizes the perplexity on the train-
ing data. Note also that the gradient is back-
propagated through the projection-layer, which
means that the neural network learns the projec-
tion of the words onto the continuous space that is
best for the probability estimation task.
The complexity to calculate one probability
with this basic version of the neural network LM is
quite high due to the large output layer. To speed
up the processing several improvements were used
(Schwenk, 2004):
</bodyText>
<listItem confidence="0.995844384615385">
1. Lattice rescoring: the statistical machine
translation decoder generates a lattice using
a 3-gram back-off LM. The neural network
LM is then used to rescore the lattice.
2. Shortlists: the neural network is only used to
predict the LM probabilities of a subset of the
whole vocabulary.
3. Efficient implementation: collection of all
LM probability requests with the same con-
text ht in one lattice, propagation of several
examples at once through the neural network
and utilization of libraries with CPU opti-
mized matrix-operations.
</listItem>
<bodyText confidence="0.854622">
The idea behind short-lists is to use the neural
</bodyText>
<equation confidence="0.7262844">
P1 =
P(wj=1|hj )
Pi =
P(wj=i|hj)
PN =
</equation>
<figure confidence="0.885476085714286">
P(wj=N|hj )
N
N
discrete continuous LM probabilities
representation: representation: for all words
indices in wordlist P dimensional vectors
input
projections
shared
projection
layer hidden
layer
P
cl
probability estimation
M
H
dj V
output
layer
oi
wj_n+1
wj_n+2
wj_1
dj =
�oi =
pi =
N
E=
i=1
726
Spanish English
1.2M
37.7M 33.8M
129k 74k
</figure>
<bodyText confidence="0.999885625">
network only to predict the s most frequent words,
s being much smaller than the size of the vocab-
ulary. All words in the vocabulary are still con-
sidered at the input of the neural network. The
LM probabilities of words in the short-list ( PN)
are calculated by the neural network and the LM
probabilities of the remaining words ( PB) are ob-
tained from a standard 4-gram back-off LM:
</bodyText>
<equation confidence="0.99957175">
P�(wt|ht) = PN(wt|ht)PS(ht) if wt E short-list (7)
PB(wt|ht) else
PS(ht) = � �PB(w|ht) (8)
wEshort−list(ht)
</equation>
<bodyText confidence="0.999557058823529">
It can be considered that the neural network redis-
tributes the probability mass of all the words in the
short-list. This probability mass is precalculated
and stored in the data structures of the back-off
LM. A back-off technique is used if the probability
mass for a input context is not directly available.
It was not envisaged to use the neural network
LM directly during decoding. First, this would
probably lead to slow translation times due to the
higher complexity of the proposed LM. Second, it
is quite difficult to incorporate n-gram language
models into decoding, for n&gt;3. Finally, we be-
lieve that the lattice framework can give the same
performances than direct decoding, under the con-
dition that the alternative hypotheses in the lattices
are rich enough. Estimates of the lattice oracle
BLEU score are given in the result section.
</bodyText>
<sectionHeader confidence="0.998538" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999447764705882">
The experimental results provided here were ob-
tained in the framework of an international evalua-
tion organized by the European TC-STAR project2
in February 2006. This project is envisaged as a
long-term effort to advance research in all core
technologies for speech-to-speech translation.
The main goal of this evaluation is to trans-
late public European Parliament Plenary Sessions
(EPPS). The training material consists of the min-
utes edited by the European Parliament in sev-
eral languages, also known as the Final Text Edi-
tions (Gollan et al., 2005). These texts were
aligned at the sentence level and they are used
to train the statistical translation models (see Ta-
ble 1 for some statistics). In addition, about 100h
of Parliament plenary sessions were recorded and
transcribed. This data is mainly used to train
</bodyText>
<footnote confidence="0.927174">
2http://www.tc-star.org/
</footnote>
<table confidence="0.878893666666667">
Sentence Pairs
Total # Words
Vocabulary size
</table>
<tableCaption confidence="0.979309">
Table 1: Statistics of the parallel texts used to train
</tableCaption>
<bodyText confidence="0.990263142857143">
the statistical machine translation system.
the speech recognizers, but the transcriptions were
also used for the target LM of the translation sys-
tem (about 740k words).
Three different conditions are considered in
the TC-STAR evaluation: translation of the Fi-
nal Text Edition (text), translation of the tran-
scriptions of the acoustic development data (ver-
batim) and translation of speech recognizer output
(ASR). Here we only consider the verbatim condi-
tion, translating from Spanish to English. For this
task, the development data consists of 792 sen-
tences (25k words) and the evaluation data of 1597
sentences (61k words). Parts of the test data ori-
gins from the Spanish parliament which results in
a (small) mismatch between the development and
test data. Two reference translations are provided.
The scoring is case sensitive and includes punctu-
ation symbols.
The translation model was trained on 1.2M sen-
tences of parallel text using the Giza++ tool. All
back-off LMs were built using modified Kneser-
Ney smoothing and the SRI LM-toolkit (Stolcke,
2002). Separate LMs were first trained on the
English EPPS texts (33.8M words) and the tran-
scriptions of the acoustic training material (740k
words) respectively. These two LMs were then in-
terpolated together. Interpolation usually results in
lower perplexities than training directly one LM
on the pooled data, in particular if the corpora
come from different sources. An EM procedure
was used to find the interpolation coefficients that
minimize the perplexity on the development data.
The optimal coefficients are 0.78 for the Final Text
edition and 0.22 for the transcriptions.
</bodyText>
<subsectionHeader confidence="0.9794845">
4.1 Performance of the sentence splitting
algorithm
</subsectionHeader>
<bodyText confidence="0.999939833333333">
In this section, we first analyze the performance of
the sentence split algorithm. Table 2 compares the
results for different ways to translate the individ-
ual chunks (using a standard 3-gram LM versus
an LM trained on texts with sentence breaks in-
serted), and to extracted the global solution (con-
</bodyText>
<page confidence="0.990864">
727
</page>
<table confidence="0.998981333333333">
LM used Concatenate Lattice
during decoding 1-best join
Without 40.20 41.63
sentence breaks
With 41.45 42.35
sentence breaks
</table>
<tableCaption confidence="0.978993">
Table 2: BLEU scores for different ways to trans-
</tableCaption>
<bodyText confidence="0.991247055555556">
late sentence chunks and to extract the global so-
lution (see text for details).
catenating the 1-best solutions versus joining the
lattices followed by LM rescoring). It can be
clearly seen that joining the lattices and recalculat-
ing the LM probabilities gives better results than
just concatenating the 1-best solutions of the in-
dividual chunks (first line: BLEU score of 41.63
compared to 40.20). Using a LM trained on texts
with sentence breaks during decoding gives an ad-
ditional improvement of about 0.7 points BLEU
(42.35 compared to 41.63).
In our current implementation, the selection of
the sentence splits is based on punctuation marks
in the source text, but our procedure is compatible
with other methods. We just need to apply the sen-
tence splitting algorithm on the training data used
to build the LM during decoding.
</bodyText>
<subsectionHeader confidence="0.8391815">
4.2 Using the continuous space language
model
</subsectionHeader>
<bodyText confidence="0.999971818181818">
The continuous space language model was trained
on exactly the same data than the back-off refer-
ence language model, using the resampling algo-
rithm described in (Schwenk and Gauvain, 2005).
In this work, we use only 4-gram LMs, but the
complexity of the neural network LM increases
only slightly with the order of the LM. For each
experiment, the parameters of the log-linear com-
bination were optimized on the development data.
Perplexity on the development data set is a pop-
ular and easy to calculate measure to evaluate the
quality of a language model. However, it is not
clear if perplexity is a good criterion to predict
the improvements when the language model will
be used in a SMT system. For information, and
comparison with the back-off LM, Figure 3 shows
the perplexities for different configurations of the
continuous space LM. The perplexity clearly de-
creases with increasing size of the short-list and a
value of 8192 was used. In this case, 99% of the
requested LM probabilities are calculated by the
neural network when rescoring a lattice.
</bodyText>
<figure confidence="0.996186">
0 5 10 15 20 25 30 35
Number of training epochs
</figure>
<figureCaption confidence="0.993568">
Figure 3: Perplexity of different configurations of
the continuous space LM.
</figureCaption>
<bodyText confidence="0.999321888888889">
Although the neural network LM could be used
alone, better results are obtained when interpolat-
ing it with the 4-gram back-off LM. It has even
turned out that it was advantageous to train several
neural network LMs with different context sizes3
and to interpolate them altogether. In that way,
a perplexity decrease from 79.6 to 65.0 was ob-
tained. For the sake of simplicity we will still call
this interpolation the neural network LM.
</bodyText>
<tableCaption confidence="0.959252666666667">
Table 3: Result comparison for the different LMs.
BLEU uses 2 reference translations. WER=word
error rate, PER=position independent WER.
</tableCaption>
<bodyText confidence="0.860442142857143">
Table 3 summarizes the results on the devel-
opment and evaluation data. The coefficients of
the feature functions are always those optimized
on the development data. The joined translation
lattices were rescored with a 4-gram back-off and
the neural network LM. Using a 4-gram back-
off LM gives an improvement of 1 point BLEU
</bodyText>
<footnote confidence="0.98894725">
3The values are in the range 150...400. The other param-
eters are: H=500, β=0.00003 and the initial learning rate was
0.005 with an exponential decay. The networks were trained
for 20 epochs through the training data.
</footnote>
<figure confidence="0.929318666666667">
82
81
80
79
78
77
76
75
74
73
72
4-gram back-off LM
short-list of 2k
short-list of 4k
short-list of 8k
Perplexity 85.5 79.6 65.0
Neural LM
4-gram
3-gram
Back-off LM
4-gram
</figure>
<table confidence="0.931157636363636">
Dev data:
BLEU
WER 45.9% 45.1% 44.4%
PER 31.8% 31.3% 30.8%
Eval data:
BLEU
WER 48.2% 47.4% 46.7%
PER 33.6% 33.1% 32.8%
42.35 43.36 44.42
39.77 40.62 41.45
Perplexity
</table>
<page confidence="0.99022">
728
</page>
<bodyText confidence="0.81164225">
Spanish: es el nico premio Sajarov que no ha podido recibir su premio despus de ms de tres
mil quinientos das de cautiverio .
Backoff LM: it is only the Sakharov Prize has not been able to receive the prize after three thousand
, five days of detention .
</bodyText>
<note confidence="0.50936">
CSLM : it is the only Sakharov Prize has not been able to receive the prize after three thousand
five days of detention.
</note>
<figureCaption confidence="0.9859832">
Reference 1: she is the only Sakharov laureate who has not been able to receive her prize after
more than three thousandfive hundred days in captivity .
Reference 2: she is the only Sacharov prizewinner who couldn’t yet pick up her prize after more
than three thousandfive hundred days of imprisonment.
Figure 4: Example translation using the back-off and the continuous space language model (CSLM).
</figureCaption>
<bodyText confidence="0.9887065">
on the Dev data (+0.8 on Test set) compared to
the 3-gram back-off LM. The neural network LM
achieves an additional improvement of 1 point
BLEU (+0.8 on Test data), on top of the 4-gram
back-off LM. Small improvements of the word er-
ror rate (WER) and the position independent word
error rate (PER) were also observed.
As usually observed in SMT, the improvements
on the test data are smaller than those on the de-
velopment data which was used to tune the param-
eters. As a rule of thumb, the gain on the test data
is often half as large as on the Dev-data. The 4-
gram back-off and neural network LM show both
a good generalization behavior.
</bodyText>
<figure confidence="0.986764090909091">
78
76
74
Perplexity
72
70
68
66
64
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Interpolation coefficient
</figure>
<figureCaption confidence="0.988630666666667">
Figure 5: BLEU score and perplexity in function
of the interpolation coefficient of the back-off 4-
gram LM.
</figureCaption>
<bodyText confidence="0.998347631578948">
Figure 5 shows the perplexity and the BLEU
score for different interpolation coefficients of the
4-gram back-off LM. For a value of 1.0 the back-
off LM is used alone, while only the neural net-
work LMs are used for a value of 0.0. Using an
EM procedure to minimize perplexity of the inter-
polated model gives a value of 0.189. This value
also seems to correspond to the best BLEU score.
This is a surprising result, and has the advan-
tage that we do not need to tune the interpola-
tion coefficient in the framework of the log-linear
feature function combination. The weights of the
other feature functions were optimized separately
for each experiment. We noticed a tendency to
a slightly higher weight for the continuous space
LM and a lower sentence length penalty.
In a contrastive experiment, the LM training
data was substantially increased by adding 352M
words of commercial Broadcast News data and
232M words of CNN news collected on the Inter-
net. Although the perplexity of the 4-gram back-
off LM decreased by 5 points to 74.1, we observed
no change in the BLEU score. In order to estimate
the oracle BLEU score of the lattices we build a 4-
gram back-off LM on the development data. Lat-
tice rescoring achieved a BLEU score of 59.10.
There are many discussions about the BLEU
score being or not a meaningful measure to as-
sess the quality of an automatic translation sys-
tem. It would be interesting to verify if the contin-
uous space LM has an impact when human judg-
ments of the translation quality are used, in partic-
ular with respect to fluency. Unfortunately, this is
not planed in the TC-STAR evaluation campaign,
and we give instead an example translation (see
Figure 4). In this case, two errors were corrected
(insertion of the word ”the” and deletion of the
comma).
</bodyText>
<sectionHeader confidence="0.994848" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.998326">
Some SMT decoders have an execution complex-
ity that increases rapidly with the length of the
sentences to be translated, which are usually split
</bodyText>
<figure confidence="0.978517181818182">
BLEU score
44.2
43.8
43.6
43.4
43.2
42.8
44
43
BLEU score
Perplexity
</figure>
<page confidence="0.992726">
729
</page>
<bodyText confidence="0.999991051282051">
into smaller chunks and translated separately. This
can lead to translation errors and bad modeling
of the LM probabilities of the words at both ends
of the chunks. We have presented a lattice join-
ing and rescoring approach that obtained signifi-
cant improvements in the BLEU score compared
to simply concatenating the 1-best solutions of
the individual chunks. The task considered is the
translation of European Parliament Speeches in
the framework of the TC-STAR project.
We have also presented a neural network LM
that performs probability estimation in a contin-
uous space. Since the resulting probability func-
tions are smooth functions of the word represen-
tation, better generalization to unknown n-grams
can be expected. This is particularly interesting
for tasks where only limited amounts of appropri-
ate LM training material are available, but the pro-
posed LM can be also trained on several hundred
millions words. The continuous space LM is used
to rescore the translation lattices. We obtained
an improvement of 0.8 points BLEU on the test
data compared to a 4-gram back-off LM, which it-
self had already achieved the same improvement
in comparison to a 3-gram LM.
The results reported in this paper have been ob-
tained with a word based SMT system, but the
continuous space LM can also be used with a
phrase-based system. One could expect that the
target language model plays a different role in
a phrase-based system since the phrases induce
some local coherency on the target sentence. This
will be studied in the future. Another promis-
ing direction that we have not yet explored, is to
build long-span LM, i.e. with n much greater than
4. The complexity of our approach increases only
slightly with n. Long-span LM could possibly im-
prove the word-ordering of the generated sentence
if the translation lattices include the correct paths.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999720870967742">
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(2):1137–1155.
A. Berger, S. Della Pietra, and Vincent J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22:39–71.
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
powell’s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157–
175.
P. Brown, S. Della Pietra, Vincent J. Della Pietra, and
R. Mercer. 1993. The mathematics of statisti-
cal machine translation. Computational Linguistics,
19(2):263–311.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for machine transla-
tion. In Machine Translation Summit.
Daniel D´echelotte, Holger Schwenk, and Jean-Luc
Gauvain. 2006. The 2006 LIMSI statistical ma-
chine translation system for TC-STAR. In TC-STAR
Speech to Speech Translation Workshop, Barcelona.
Ahmad Emami and Frederick Jelinek. 2005. Ran-
dom clusterings for language modeling. In ICASSP,
pages I:581–584.
C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and
H. Ney. 2005. Cross domain automatic transcrip-
tion on the TC-STAR EPPS corpus. In ICASSP.
Sasa Hasan, Olivier Bender, and Hermann Ney. 2006.
Reranking translation hypothesis using structural
properties. In LREC.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation.
In ACL’05 workshop on Building and Using Paral-
lel Text, pages 125–128.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295–302,
University of Pennsylvania.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In NAACL, pages 161–168.
Franz-Joseph Och. 2005. The Google statistical ma-
chine translation system for the 2005 Nist MT eval-
uation, Oral presentation at the 2005 Nist MT Eval-
uation workshop, June 20.
Holger Schwenk and Jean-Luc Gauvain. 2005. Train-
ing neural network language models on very large
corpora. In EMNLP, pages 201–208.
Holger Schwenk. 2004. Efficient training of large
neural networks for language modeling. In IJCNN,
pages 3059–3062.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP, pages II: 901–
904.
Peng Xu and Frederick Jelinek. 2004. Random forest
in language modeling. In EMNLP, pages 325–332.
</reference>
<page confidence="0.997217">
730
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636022">
<title confidence="0.99835">Continuous Space Language Models for Statistical Machine Translation</title>
<author confidence="0.836454">Schwenk Dchelotte Gauvain</author>
<address confidence="0.9194825">LIMSI-CNRS, BP 133 91403 Orsay cedex, FRANCE</address>
<abstract confidence="0.99555328">Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard back-off language model is used in most systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluaorganized by the in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="5161" citStr="Bengio et al., 2003" startWordPosition="832" endWordPosition="835"> TC-STAR project in February 2006. The training data consists of about 35M words of aligned texts that are also used to train the target LM. In our experiments, adding more than 580M words of Broadcast News data had no impact on the BLEU score, despite a notable decrease of the perplexity of the target LM. Therefore, we suggest to use more complex statistical LMs that are expected to take better advantage of the limited amount of appropriate training data. Promising candidates are random forest LMs (Xu and Jelinek, 2004), random cluster LMs (Emami and Jelinek, 2005) and the neural network LM (Bengio et al., 2003). In this paper, we investigate whether the latter approach can be used in a statistical machine translation system. The basic idea of the neural network LM, also called continuous space LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space. Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the n-gram probabiliti</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3(2):1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--39</pages>
<contexts>
<context position="8766" citStr="Berger et al., 1996" startWordPosition="1425" endWordPosition="1428">have been forgotten .” Reference 2: “it is good to remember this, because maybe we forgot it .” explored search space. Figure 1 shows an example of such a search space, here heavily pruned for the sake of clarity. 2.1 Sentence Splitting The execution complexity of our SMT decoder increases non-linearly with the length of the sentence to be translated. Therefore, the source text is split into smaller chunks, each one being translated separately. The chunks are then concatenated together. Several algorithms have been proposed in the literature that try to find the best splits, see for instance (Berger et al., 1996). In this work, we first split long sentences at punctuation marks, the remaining segments that still exceed the allowed length being split linearly. In a second pass, adjoining very short chunks are merged together. During decoding, target LM probabilities of the type Pr(wi&lt;s&gt;) and Pr(&lt;/s&gt;|wn_1wn) will be requested at the beginning and at the end of the hypothesized target sentence respectively.1 This is correct when a whole sentence is translated, but leads to wrong LM probabilities when processing smaller chunks. Therefore, we define a sentence break symbol, &lt;b&gt;, that is used at the beginni</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Vanden Berghen</author>
<author>Hugues Bersini</author>
</authors>
<title>CONDOR, a new parallel, constrained extension of powell’s UOBYQA algorithm: Experimental results and comparison with the DFO algorithm.</title>
<date>2005</date>
<journal>Journal of Computational and Applied Mathematics,</journal>
<volume>181</volume>
<pages>175</pages>
<contexts>
<context position="11057" citStr="Berghen and Bersini, 2005" startWordPosition="1795" endWordPosition="1798">s is more efficient than reranking n-best lists since it guarantees that always all possible hypotheses are considered. The decoder first generates large lattices using the current set of parameters. These lattices are then processed by a separate tool that extracts the best path, given the coefficients of six feature functions (translations, distortion, fertility, spontaneous insertion, target language model probability, and a sentence length penalty). Then, the BLEU score of the extracted solution is calculated. This tool is called in a loop by the public numerical optimization tool Condor (Berghen and Bersini, 2005). The solution vector was usually found after about 100 iterations. In our experiments, only two cycles of lattice generation and parameter optimization were necessary (with a very small difference in the BLEU score). In all our experiments, the 4-gram back-off and the neural network LM are used to calculate language model probabilities that replace those of the default 3-gram LM. An alternative would be to define each LM as a feature function and to combine them under the log-linear model framework, using maximum BLEU training. We believe that this would not make a notable difference in our e</context>
</contexts>
<marker>Berghen, Bersini, 2005</marker>
<rawString>Frank Vanden Berghen and Hugues Bersini. 2005. CONDOR, a new parallel, constrained extension of powell’s UOBYQA algorithm: Experimental results and comparison with the DFO algorithm. Journal of Computational and Applied Mathematics, 181:157– 175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1431" citStr="Brown et al., 1993" startWordPosition="222" endWordPosition="225">t of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks. 1 Introduction The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f. Among all possible target sentences the one with maximal probability is chosen. The classical Bayes relation is used to introduce a target language model (Brown et al., 1993): where Pr(f|e) is the translation model and Pr(e) is the target language model. This approach is usually referred to as the noisy source-channel approach in statistical machine translation. Since the introduction of this basic model, many improvements have been made, but it seems that research is mainly focused on better translation and alignment models or phrase extraction algorithms as demonstrated by numerous publications on these topics. On the other hand, we are aware of only a small amount of papers investigating new approaches to language modeling for statistical machine translation. T</context>
<context position="6575" citStr="Brown et al., 1993" startWordPosition="1058" endWordPosition="1061">cessfully used in large vocabulary speech recognition (Schwenk and Gauvain, 2005), and we are interested here if similar ideas can be applied to statistical machine translation. This paper is organized as follows. In the next section we first describe the baseline statistical machine translation system. Section 3 presents the architecture of the continuous space LM and section 4 summarizes the experimental evaluation. The paper concludes with a discussion of future research directions. 2 Statistical Translation Engine A word-based translation engine is used based on the so-called IBM-4 model (Brown et al., 1993). A brief description of this model is given below along with the decoding algorithm. The search algorithm aims at finding what target sentence e is most likely to have produced the observed source sentence f. The translation model Pr(f|e) is decomposed into four components: 1. a fertility model; 2. a lexical model of the form t(f|e), which gives the probability that the target word e translates into the source word f; 3. a distortion model, that characterizes how words are reordered when translated; 4. and probabilities to model the insertion of source words that are not aligned to any target</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. Della Pietra, Vincent J. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntax-based language models for machine translation.</title>
<date>2003</date>
<booktitle>In Machine Translation Summit.</booktitle>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-based language models for machine translation. In Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D´echelotte</author>
<author>Holger Schwenk</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>LIMSI statistical machine translation system for TC-STAR. In TC-STAR Speech to Speech Translation Workshop,</title>
<date>2006</date>
<publisher>The</publisher>
<location>Barcelona.</location>
<marker>D´echelotte, Schwenk, Gauvain, 2006</marker>
<rawString>Daniel D´echelotte, Holger Schwenk, and Jean-Luc Gauvain. 2006. The 2006 LIMSI statistical machine translation system for TC-STAR. In TC-STAR Speech to Speech Translation Workshop, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random clusterings for language modeling.</title>
<date>2005</date>
<booktitle>In ICASSP,</booktitle>
<pages>581--584</pages>
<contexts>
<context position="5113" citStr="Emami and Jelinek, 2005" startWordPosition="823" endWordPosition="826">n international evaluation organized by the European TC-STAR project in February 2006. The training data consists of about 35M words of aligned texts that are also used to train the target LM. In our experiments, adding more than 580M words of Broadcast News data had no impact on the BLEU score, despite a notable decrease of the perplexity of the target LM. Therefore, we suggest to use more complex statistical LMs that are expected to take better advantage of the limited amount of appropriate training data. Promising candidates are random forest LMs (Xu and Jelinek, 2004), random cluster LMs (Emami and Jelinek, 2005) and the neural network LM (Bengio et al., 2003). In this paper, we investigate whether the latter approach can be used in a statistical machine translation system. The basic idea of the neural network LM, also called continuous space LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space. Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continu</context>
</contexts>
<marker>Emami, Jelinek, 2005</marker>
<rawString>Ahmad Emami and Frederick Jelinek. 2005. Random clusterings for language modeling. In ICASSP, pages I:581–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gollan</author>
<author>M Bisani</author>
<author>S Kanthak</author>
<author>R Schlueter</author>
<author>H Ney</author>
</authors>
<title>Cross domain automatic transcription on the TC-STAR EPPS corpus.</title>
<date>2005</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="17971" citStr="Gollan et al., 2005" startWordPosition="2989" endWordPosition="2992">LEU score are given in the result section. 4 Experimental Evaluation The experimental results provided here were obtained in the framework of an international evaluation organized by the European TC-STAR project2 in February 2006. This project is envisaged as a long-term effort to advance research in all core technologies for speech-to-speech translation. The main goal of this evaluation is to translate public European Parliament Plenary Sessions (EPPS). The training material consists of the minutes edited by the European Parliament in several languages, also known as the Final Text Editions (Gollan et al., 2005). These texts were aligned at the sentence level and they are used to train the statistical translation models (see Table 1 for some statistics). In addition, about 100h of Parliament plenary sessions were recorded and transcribed. This data is mainly used to train 2http://www.tc-star.org/ Sentence Pairs Total # Words Vocabulary size Table 1: Statistics of the parallel texts used to train the statistical machine translation system. the speech recognizers, but the transcriptions were also used for the target LM of the translation system (about 740k words). Three different conditions are conside</context>
</contexts>
<marker>Gollan, Bisani, Kanthak, Schlueter, Ney, 2005</marker>
<rawString>C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and H. Ney. 2005. Cross domain automatic transcription on the TC-STAR EPPS corpus. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Hasan</author>
<author>Olivier Bender</author>
<author>Hermann Ney</author>
</authors>
<title>Reranking translation hypothesis using structural properties.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="3492" citStr="Hasan et al., 2006" startWordPosition="548" endWordPosition="551">2004). In that work, the incorporation of part-of-speech (POS) information gave only a small improvement compared to a 3-gram backoff LM. In another study, a factored LM using POS information achieved the same results as the 4-gram LM (Kirchhoff and Yang, 2005). Syntaxbased LMs were investigated in (Charniak et al., e� = arg max Pr(e|f) = argmax Pr(f|e) Pr(e) e e 723 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723–730, Sydney, July 2006. c�2006 Association for Computational Linguistics 2003), and reranking of translation hypothesis using structural properties in (Hasan et al., 2006). An interesting experiment was reported at the NIST 2005 MT evaluation workshop (Och, 2005): starting with a 5-gram LM trained on 75 million words of Broadcast News data, a gain of about 0.5 point BLEU was observed each time when the amount of LM training data was doubled, using at the end 237 billion words of texts. Most of this additional data was collected by Google on the Internet. We believe that this kind of approach is difficult to apply to other tasks than Broadcast News and other target languages than English. There are many areas where automatic machine translation could be deployed</context>
</contexts>
<marker>Hasan, Bender, Ney, 2006</marker>
<rawString>Sasa Hasan, Olivier Bender, and Hermann Ney. 2006. Reranking translation hypothesis using structural properties. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Mei Yang</author>
</authors>
<title>Improved language modeling for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL’05 workshop on Building and Using Parallel Text,</booktitle>
<pages>125--128</pages>
<contexts>
<context position="3134" citStr="Kirchhoff and Yang, 2005" startWordPosition="493" endWordPosition="496">cular several ones that aim at improving the modeling of the target language. In most SMT systems the use of a 4-gram back-off language model usually achieves improvements in the BLEU score in comparison to the 3-gram LM used during decoding. It seems however difficult to improve upon the 4-gram LM. Many different feature functions were explored in (Och et al., 2004). In that work, the incorporation of part-of-speech (POS) information gave only a small improvement compared to a 3-gram backoff LM. In another study, a factored LM using POS information achieved the same results as the 4-gram LM (Kirchhoff and Yang, 2005). Syntaxbased LMs were investigated in (Charniak et al., e� = arg max Pr(e|f) = argmax Pr(f|e) Pr(e) e e 723 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723–730, Sydney, July 2006. c�2006 Association for Computational Linguistics 2003), and reranking of translation hypothesis using structural properties in (Hasan et al., 2006). An interesting experiment was reported at the NIST 2005 MT evaluation workshop (Och, 2005): starting with a 5-gram LM trained on 75 million words of Broadcast News data, a gain of about 0.5 point BLEU was observed each time when the amount </context>
</contexts>
<marker>Kirchhoff, Yang, 2005</marker>
<rawString>Katrin Kirchhoff and Mei Yang. 2005. Improved language modeling for statistical machine translation. In ACL’05 workshop on Building and Using Parallel Text, pages 125–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>295--302</pages>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2284" citStr="Och and Ney, 2002" startWordPosition="351" endWordPosition="354">many improvements have been made, but it seems that research is mainly focused on better translation and alignment models or phrase extraction algorithms as demonstrated by numerous publications on these topics. On the other hand, we are aware of only a small amount of papers investigating new approaches to language modeling for statistical machine translation. Traditionally, statistical machine translation systems use a simple 3-gram back-off language model (LM) during decoding to generate n-best lists. These n-best lists are then rescored using a log-linear combination of feature functions (Och and Ney, 2002): e� Pz� arg max Pr(e)A1 Pr(f|e)A2 (1) e where the coefficients Ai are optimized on a development set, usually maximizing the BLEU score. In addition to the standard feature functions, many others have been proposed, in particular several ones that aim at improving the modeling of the target language. In most SMT systems the use of a 4-gram back-off language model usually achieves improvements in the BLEU score in comparison to the 3-gram LM used during decoding. It seems however difficult to improve upon the 4-gram LM. Many different feature functions were explored in (Och et al., 2004). In t</context>
<context position="10186" citStr="Och and Ney, 2002" startWordPosition="1660" endWordPosition="1663">ls &lt;s&gt; and &lt;/s&gt; denote the begin and end of sentence marker respectively. erated. The individual lattices are then joined, omitting the sentence break symbols. Finally, the resulting lattice is rescored with a LM that was trained on text without sentence breaks. In that way we find the best junction of the chunks. Section 4.1 provides comparative results of the different algorithms to split and join sentences. 2.2 Parameter Tuning It is nowadays common practice to optimize the coefficients of the log-linear combination of feature functions by maximizing the BLEU score on the development data (Och and Ney, 2002). This is usually done by first creating n-best lists that are then reranked using an iterative optimization algorithm. In this work, a slightly different procedure was used that operates directly on the translation lattices. We believe that this is more efficient than reranking n-best lists since it guarantees that always all possible hypotheses are considered. The decoder first generates large lattices using the current set of parameters. These lattices are then processed by a separate tool that extracts the best path, given the coefficients of six feature functions (translations, distortion</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL, pages 295–302, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F-J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation. In</title>
<date>2004</date>
<booktitle>NAACL,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="2878" citStr="Och et al., 2004" startWordPosition="451" endWordPosition="454">ons (Och and Ney, 2002): e� Pz� arg max Pr(e)A1 Pr(f|e)A2 (1) e where the coefficients Ai are optimized on a development set, usually maximizing the BLEU score. In addition to the standard feature functions, many others have been proposed, in particular several ones that aim at improving the modeling of the target language. In most SMT systems the use of a 4-gram back-off language model usually achieves improvements in the BLEU score in comparison to the 3-gram LM used during decoding. It seems however difficult to improve upon the 4-gram LM. Many different feature functions were explored in (Och et al., 2004). In that work, the incorporation of part-of-speech (POS) information gave only a small improvement compared to a 3-gram backoff LM. In another study, a factored LM using POS information achieved the same results as the 4-gram LM (Kirchhoff and Yang, 2005). Syntaxbased LMs were investigated in (Charniak et al., e� = arg max Pr(e|f) = argmax Pr(f|e) Pr(e) e e 723 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723–730, Sydney, July 2006. c�2006 Association for Computational Linguistics 2003), and reranking of translation hypothesis using structural properties in (Hasan</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of features for statistical machine translation. In NAACL, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz-Joseph Och</author>
</authors>
<title>The Google statistical machine translation system for the 2005 Nist MT evaluation, Oral presentation at the 2005 Nist MT Evaluation workshop,</title>
<date>2005</date>
<contexts>
<context position="3584" citStr="Och, 2005" startWordPosition="564" endWordPosition="565">ent compared to a 3-gram backoff LM. In another study, a factored LM using POS information achieved the same results as the 4-gram LM (Kirchhoff and Yang, 2005). Syntaxbased LMs were investigated in (Charniak et al., e� = arg max Pr(e|f) = argmax Pr(f|e) Pr(e) e e 723 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723–730, Sydney, July 2006. c�2006 Association for Computational Linguistics 2003), and reranking of translation hypothesis using structural properties in (Hasan et al., 2006). An interesting experiment was reported at the NIST 2005 MT evaluation workshop (Och, 2005): starting with a 5-gram LM trained on 75 million words of Broadcast News data, a gain of about 0.5 point BLEU was observed each time when the amount of LM training data was doubled, using at the end 237 billion words of texts. Most of this additional data was collected by Google on the Internet. We believe that this kind of approach is difficult to apply to other tasks than Broadcast News and other target languages than English. There are many areas where automatic machine translation could be deployed and for which considerably less appropriate in-domain training data is available. We could </context>
</contexts>
<marker>Och, 2005</marker>
<rawString>Franz-Joseph Och. 2005. The Google statistical machine translation system for the 2005 Nist MT evaluation, Oral presentation at the 2005 Nist MT Evaluation workshop, June 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Training neural network language models on very large corpora.</title>
<date>2005</date>
<booktitle>In EMNLP,</booktitle>
<pages>201--208</pages>
<contexts>
<context position="6037" citStr="Schwenk and Gauvain, 2005" startWordPosition="975" endWordPosition="978">to use a probability estimator operating on this space. Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the n-gram probabilities. This is still a n-gram approach, but the LM posterior probabilities are ”interpolated” for any possible context of length n-1 instead of backing-off to shorter contexts. This approach was successfully used in large vocabulary speech recognition (Schwenk and Gauvain, 2005), and we are interested here if similar ideas can be applied to statistical machine translation. This paper is organized as follows. In the next section we first describe the baseline statistical machine translation system. Section 3 presents the architecture of the continuous space LM and section 4 summarizes the experimental evaluation. The paper concludes with a discussion of future research directions. 2 Statistical Translation Engine A word-based translation engine is used based on the so-called IBM-4 model (Brown et al., 1993). A brief description of this model is given below along with </context>
<context position="14145" citStr="Schwenk and Gauvain, 2005" startWordPosition="2350" endWordPosition="2353">on algorithm minimizing the following error function: � � �E E ti log pi + Q m2 jl + v2 ij �(6) jl ij where ti denotes the desired output, i.e., the probability should be 1.0 for the next word in the training sentence and 0.0 for all the other ones. The first part of this equation is the cross-entropy between the output and the target probability distributions, and the second part is a regularization term that aims to prevent the neural network from overfitting the training data (weight decay). The parameter Q has to be determined experimentally. Training is done using a resampling algorithm (Schwenk and Gauvain, 2005). Neural Network Figure 2: Architecture of the continuous space LM. hj denotes the context wj_n+1, ... , wj_1. P is the size of one projection and H,N is the size of the hidden and output layer respectively. When short-lists are used the size of the output layer is much smaller then the size of the vocabulary. It can be shown that the outputs of a neural network trained in this manner converge to the posterior probabilities. Therefore, the neural network directly minimizes the perplexity on the training data. Note also that the gradient is backpropagated through the projection-layer, which mea</context>
<context position="21590" citStr="Schwenk and Gauvain, 2005" startWordPosition="3566" endWordPosition="3569"> breaks during decoding gives an additional improvement of about 0.7 points BLEU (42.35 compared to 41.63). In our current implementation, the selection of the sentence splits is based on punctuation marks in the source text, but our procedure is compatible with other methods. We just need to apply the sentence splitting algorithm on the training data used to build the LM during decoding. 4.2 Using the continuous space language model The continuous space language model was trained on exactly the same data than the back-off reference language model, using the resampling algorithm described in (Schwenk and Gauvain, 2005). In this work, we use only 4-gram LMs, but the complexity of the neural network LM increases only slightly with the order of the LM. For each experiment, the parameters of the log-linear combination were optimized on the development data. Perplexity on the development data set is a popular and easy to calculate measure to evaluate the quality of a language model. However, it is not clear if perplexity is a good criterion to predict the improvements when the language model will be used in a SMT system. For information, and comparison with the back-off LM, Figure 3 shows the perplexities for di</context>
</contexts>
<marker>Schwenk, Gauvain, 2005</marker>
<rawString>Holger Schwenk and Jean-Luc Gauvain. 2005. Training neural network language models on very large corpora. In EMNLP, pages 201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Efficient training of large neural networks for language modeling.</title>
<date>2004</date>
<booktitle>In IJCNN,</booktitle>
<pages>3059--3062</pages>
<contexts>
<context position="15094" citStr="Schwenk, 2004" startWordPosition="2514" endWordPosition="2515">e outputs of a neural network trained in this manner converge to the posterior probabilities. Therefore, the neural network directly minimizes the perplexity on the training data. Note also that the gradient is backpropagated through the projection-layer, which means that the neural network learns the projection of the words onto the continuous space that is best for the probability estimation task. The complexity to calculate one probability with this basic version of the neural network LM is quite high due to the large output layer. To speed up the processing several improvements were used (Schwenk, 2004): 1. Lattice rescoring: the statistical machine translation decoder generates a lattice using a 3-gram back-off LM. The neural network LM is then used to rescore the lattice. 2. Shortlists: the neural network is only used to predict the LM probabilities of a subset of the whole vocabulary. 3. Efficient implementation: collection of all LM probability requests with the same context ht in one lattice, propagation of several examples at once through the neural network and utilization of libraries with CPU optimized matrix-operations. The idea behind short-lists is to use the neural P1 = P(wj=1|hj</context>
</contexts>
<marker>Schwenk, 2004</marker>
<rawString>Holger Schwenk. 2004. Efficient training of large neural networks for language modeling. In IJCNN, pages 3059–3062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="19421" citStr="Stolcke, 2002" startWordPosition="3220" endWordPosition="3221">atim condition, translating from Spanish to English. For this task, the development data consists of 792 sentences (25k words) and the evaluation data of 1597 sentences (61k words). Parts of the test data origins from the Spanish parliament which results in a (small) mismatch between the development and test data. Two reference translations are provided. The scoring is case sensitive and includes punctuation symbols. The translation model was trained on 1.2M sentences of parallel text using the Giza++ tool. All back-off LMs were built using modified KneserNey smoothing and the SRI LM-toolkit (Stolcke, 2002). Separate LMs were first trained on the English EPPS texts (33.8M words) and the transcriptions of the acoustic training material (740k words) respectively. These two LMs were then interpolated together. Interpolation usually results in lower perplexities than training directly one LM on the pooled data, in particular if the corpora come from different sources. An EM procedure was used to find the interpolation coefficients that minimize the perplexity on the development data. The optimal coefficients are 0.78 for the Final Text edition and 0.22 for the transcriptions. 4.1 Performance of the </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In ICSLP, pages II: 901– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random forest in language modeling.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>325--332</pages>
<contexts>
<context position="5067" citStr="Xu and Jelinek, 2004" startWordPosition="816" endWordPosition="819">m Spanish to English, in the framework of an international evaluation organized by the European TC-STAR project in February 2006. The training data consists of about 35M words of aligned texts that are also used to train the target LM. In our experiments, adding more than 580M words of Broadcast News data had no impact on the BLEU score, despite a notable decrease of the perplexity of the target LM. Therefore, we suggest to use more complex statistical LMs that are expected to take better advantage of the limited amount of appropriate training data. Promising candidates are random forest LMs (Xu and Jelinek, 2004), random cluster LMs (Emami and Jelinek, 2005) and the neural network LM (Bengio et al., 2003). In this paper, we investigate whether the latter approach can be used in a statistical machine translation system. The basic idea of the neural network LM, also called continuous space LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space. Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously lear</context>
</contexts>
<marker>Xu, Jelinek, 2004</marker>
<rawString>Peng Xu and Frederick Jelinek. 2004. Random forest in language modeling. In EMNLP, pages 325–332.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>