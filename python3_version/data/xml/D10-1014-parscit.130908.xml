<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000642">
<title confidence="0.9898795">
Soft Syntactic Constraints for Hierarchical Phrase-based Translation
Using Latent Syntactic Distributions
</title>
<author confidence="0.999426">
Zhongqiang Huang
</author>
<affiliation confidence="0.997226">
Institute for Advanced Computer Studies
University of Maryland
</affiliation>
<address confidence="0.976628">
College Park, MD 20742
</address>
<email confidence="0.999377">
zqhuang@umiacs.umd.edu
</email>
<note confidence="0.928596">
Martin Cmejrek and Bowen Zhou
IBM T. J. Watson Research Center
</note>
<address confidence="0.657276">
Yorktown Heights, NY 10598
</address>
<email confidence="0.992385">
imartin.cmejrek,zhoul@us.ibm.com
</email>
<sectionHeader confidence="0.996588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999666">
In this paper, we present a novel approach
to enhance hierarchical phrase-based machine
translation systems with linguistically moti-
vated syntactic features. Rather than directly
using treebank categories as in previous stud-
ies, we learn a set of linguistically-guided la-
tent syntactic categories automatically from a
source-side parsed, word-aligned parallel cor-
pus, based on the hierarchical structure among
phrase pairs as well as the syntactic structure
of the source side. In our model, each X non-
terminal in a SCFG rule is decorated with a
real-valued feature vector computed based on
its distribution of latent syntactic categories.
These feature vectors are utilized at decod-
ing time to measure the similarity between the
syntactic analysis of the source side and the
syntax of the SCFG rules that are applied to
derive translations. Our approach maintains
the advantages of hierarchical phrase-based
translation systems while at the same time nat-
urally incorporates soft syntactic constraints.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906159090909">
In recent years, syntax-based translation mod-
els (Chiang, 2007; Galley et al., 2004; Liu et
al., 2006) have shown promising progress in im-
proving translation quality, thanks to the incorpora-
tion of phrasal translation adopted from the widely
used phrase-based models (Och and Ney, 2004) to
handle local fluency and the engagement of syn-
chronous context-free grammars (SCFG) to handle
non-local phrase reordering. Approaches to syntax-
based translation models can be largely categorized
into two classes based on their dependency on anno-
tated corpus (Chiang, 2007). Linguistically syntax-
based models (e.g., (Yamada and Knight, 2001; Gal-
ley et al., 2004; Liu et al., 2006)) utilize structures
defined over linguistic theory and annotations (e.g.,
Penn Treebank) and guide the derivation of SCFG
rules with explicit parsing on at least one side of
the parallel corpus. Formally syntax-based mod-
els (e.g., (Wu, 1997; Chiang, 2007)) extract syn-
chronous grammars from parallel corpora based on
the hierarchical structure of natural language pairs
without any explicit linguistic knowledge or anno-
tations. In this work, we focus on the hierarchi-
cal phrase-based models of Chiang (2007), which
is formally syntax-based, and always refer the term
SCFG, from now on, to the grammars of this model
class.
On the one hand, hierarchical phrase-based mod-
els do not suffer from errors in syntactic constraints
that are unavoidable in linguistically syntax-based
models. Despite the complete lack of linguistic
guidance, the performance of hierarchical phrase-
based models is competitive when compared to lin-
guistically syntax-based models. As shown in (Mi
and Huang, 2008), hierarchical phrase-based models
significantly outperform tree-to-string models (Liu
et al., 2006; Huang et al., 2006), even when at-
tempts are made to alleviate parsing errors using
either forest-based decoding (Mi et al., 2008) or
forest-based rule extraction (Mi and Huang, 2008).
On the other hand, when properly used, syntac-
tic constraints can provide invaluable benefits to im-
prove translation quality. The tree-to-string mod-
els of Mi and Huang (2008) can actually signif-
</bodyText>
<page confidence="0.967825">
138
</page>
<note confidence="0.817855">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999947282608696">
icantly outperform hierarchical phrase-based mod-
els when using forest-based rule extraction together
with forest-based decoding. Chiang (2010) also ob-
tained significant improvement over his hierarchi-
cal baseline by using syntactic parse trees on both
source and target sides to induce fuzzy (not exact)
tree-to-tree rules and by also allowing syntactically
mismatched substitutions.
In this paper, we augment rules in hierarchical
phrase-based translation systems with novel syntac-
tic features. Unlike previous studies (e.g., (Zoll-
mann and Venugopal, 2006)) that directly use ex-
plicit treebank categories such as NP, NP/PP (NP
missing PP from the right) to annotate phrase pairs,
we induce a set of latent categories to capture the
syntactic dependencies inherent in the hierarchical
structure of phrase pairs, and derive a real-valued
feature vector for each X nonterminal of a SCFG
rule based on the distribution of the latent cate-
gories. Moreover, we convert the equality test of
two sequences of syntactic categories, which are ei-
ther identical or different, into the computation of
a similarity score between their corresponding fea-
ture vectors. In our model, two symbolically dif-
ferent sequences of syntactic categories could have
a high similarity score in the feature vector repre-
sentation if they are syntactically similar, and a low
score otherwise. In decoding, these feature vectors
are utilized to measure the similarity between the
syntactic analysis of the source side and the syntax
of the SCFG rules that are applied to derive trans-
lations. Our approach maintains the advantages of
hierarchical phrase-based translation systems while
at the same time naturally incorporates soft syntactic
constraints. To the best of our knowledge, this is the
first work that applies real-valued syntactic feature
vectors to machine translation.
The rest of the paper is organized as follows.
Section 2 briefly reviews hierarchical phrase-based
translation models. Section 3 presents an overview
of our approach, followed by Section 4 describing
the hierarchical structure of aligned phrase pairs and
Section 5 describing how to induce latent syntactic
categories. Experimental results are reported in Sec-
tion 6, followed by discussions in Section 7. Sec-
tion 8 concludes this paper.
</bodyText>
<sectionHeader confidence="0.883141" genericHeader="introduction">
2 Hierarchical Phrase-Based Translation
</sectionHeader>
<bodyText confidence="0.999322076923077">
An SCFG is a synchronous rewriting system gener-
ating source and target side string pairs simultane-
ously based on a context-free grammar. Each syn-
chronous production (i.e., rule) rewrites a nonter-
minal into a pair of strings, -y and α, where -y (or
α) contains terminal and nonterminal symbols from
the source (or target) language and there is a one-to-
one correspondence between the nonterminal sym-
bols on both sides. In particular, the hierarchical
model (Chiang, 2007) studied in this paper explores
hierarchical structures of natural language and uti-
lize only a unified nonterminal symbol X in the
grammar,
</bodyText>
<equation confidence="0.994012">
X — (-y, α, —)
</equation>
<bodyText confidence="0.997012">
where — is the one-to-one correspondence between
X’s in -y and α, and it can be indicated by un-
derscripted co-indexes. Two example English-to-
Chinese translation rules are represented as follows:
</bodyText>
<equation confidence="0.8089965">
X (give the pen to me, 钢笔 给 我) (1)
X (give X1 to me, X1 给 我) (2)
</equation>
<bodyText confidence="0.998443304347826">
The SCFG rules of hierarchical phrase-based
models are extracted automatically from corpora of
word-aligned parallel sentence pairs (Brown et al.,
1993; Och and Ney, 2000). An aligned sentence pair
is a tuple (E, F, A), where E = e1 · · · e,,, can be in-
terpreted as an English sentence of length n, F =
f1 · · · fm its translation of length m in a foreign lan-
guage, and A a set of links between words of the two
sentences. Figure 1 (a) shows an example of aligned
English-to-Chinese sentence pair. Widely adopted
in phrase-based models (Och and Ney, 2004), a pair
of consecutive sequences of words from E and F is
a phrase pair if all words are aligned only within the
sequences and not to any word outside. We call a se-
quence of words a phrase if it corresponds to either
side of a phrase pair, and a non-phrase otherwise.
Note that the boundary words of a phrase pair may
not be aligned to any other word. We call the phrase
pairs with all boundary words aligned tight phrase
pairs (Zhang et al., 2008). A tight phrase pair is the
minimal phrase pair among all that share the same
set of alignment links. Figure 1 (b) highlights the
tight phrase pairs in the example sentence pair.
</bodyText>
<page confidence="0.994647">
139
</page>
<figure confidence="0.99230025">
6
5
4
3
2
1
1 2 3 4 5
(a) (b)
</figure>
<figureCaption confidence="0.991191">
Figure 1: An example of word-aligned sentence pair (a)
with tight phrase pairs marked in a matrix representation
(b).
</figureCaption>
<bodyText confidence="0.998118071428572">
The extraction of SCFG rules proceeds as fol-
lows. In the first step, all phrase pairs below a max-
imum length are extracted as phrasal rules. In the
second step, abstract rules are extracted from tight
phrase pairs that contain other tight phrase pairs by
replacing the sub phrase pairs with co-indexed X-
nonterminals. Chiang (2007) also introduced several
requirements (e.g., there are at most two nontermi-
nals at the right hand side of a rule) to safeguard
the quality of the abstract rules as well as keeping
decoding efficient. In our example above, rule (2)
can be extracted from rule (1) with the following sub
phrase pair:
X , (the pen, 钢笔)
The use of a unified X nonterminal makes hier-
archical phrase-based models flexible at capturing
non-local reordering of phrases. However, such flex-
ibility also comes at the cost that it is not able to
differentiate between different syntactic usages of
phrases. Suppose rule X —* (I am reading X1, · · · )
is extracted from a phrase pair with I am reading a
book on the source side where X1 is abstracted from
the noun phrase pair. If this rule is used to translate
I am reading the brochure of a book fair, it would
be better to apply it over the entire string than over
sub-strings such as I ... the brochure of. This is be-
cause the nonterminal X1 in the rule was abstracted
from a noun phrase on the source side of the training
data and would thus be better (more informative) to
be applied to phrases of the same type. Hierarchi-
cal phrase-based models are not able to distinguish
syntactic differences like this.
Zollmann and Venugopal (2006) attempted to ad-
dress this problem by annotating phrase pairs with
treebank categories based on automatic parse trees.
They introduced an extended set of categories (e.g.,
NP+V for she went and DT\NP for great wall, an
noun phrase with a missing determiner on the left)
to annotate phrase pairs that do not align with syn-
tactic constituents. Their hard syntactic constraint
requires that the nonterminals should match exactly
to rewrite with a rule, which could rule out poten-
tially correct derivations due to errors in the syn-
tactic parses as well as to data sparsity. For exam-
ple, NP cannot be instantiated with phrase pairs of
type DT+NN, in spite of their syntactic similarity.
Venugopal et al. (2009) addressed this problem by
directly introducing soft syntactic preferences into
SCFG rules using preference grammars, but they
had to face the computational challenges of large
preference vectors. Chiang (2010) also avoided hard
constraints and took a soft alternative that directly
models the cost of mismatched rule substitutions.
This, however, would require a large number of pa-
rameters to be tuned on a generally small-sized held-
out set, and it could thus suffer from over-tuning.
</bodyText>
<sectionHeader confidence="0.955993" genericHeader="method">
3 Approach Overview
</sectionHeader>
<bodyText confidence="0.999896631578947">
In this work, we take a different approach to intro-
duce linguistic syntax to hierarchical phrase-based
translation systems and impose soft syntactic con-
straints between derivation rules and the syntactic
parse of the sentence to be translated. For each
phrase pair extracted from a sentence pair of a
source-side parsed parallel corpus, we abstract its
syntax by the sequence of highest root categories,
which we call a tag sequence, that exactly1 domi-
nates the syntactic tree fragments of the source-side
phrase. Figure 3 (b) shows the source-side parse tree
of a sentence pair. The tag sequence for “the pen”
is simply “NP” because it is a noun phrase, while
phrase “give the pen” is dominated by a verb fol-
lowed by a noun phrase, and thus its tag sequence is
“VBP NP”.
Let T5 = {ts1, · · · , tsm} be the set of all tag se-
quences extracted from a parallel corpus. The syntax
of each X nonterminal2 in a SCFG rule can be then
</bodyText>
<footnote confidence="0.9460732">
1In case of a non-tight phrase pair, we only abstract and
compare the syntax of the largest tight part.
2There are three X nonterminals (one on the left and two on
the right) for binary abstract rules, two for unary abstract rules,
and one for phrasal rules.
</footnote>
<page confidence="0.980225">
140
</page>
<table confidence="0.95726425">
Tag Sequence Probability
NP 0.40
DT NN 0.35
DT NN NN 0.25
</table>
<bodyText confidence="0.9887695">
sequences weighted by the distribution of tag se-
quences of the nonterminal X:
</bodyText>
<equation confidence="0.9774455">
F~(X) = � pX(ts)F~(ts)
tsETS
</equation>
<tableCaption confidence="0.974312">
Table 1: The distribution of tag sequences for X1 in X →
</tableCaption>
<bodyText confidence="0.935979421052632">
hI am reading X1, · · · i.
characterized by the distribution of tag sequences
~PX(TS) = (pX(ts1), · · · , pX(tsm)), based on the
phrase pairs it is abstracted from. Table 1 shows
an example distribution of tag sequences for X1 in
X → hI am reading X1, · · · i.
Instead of directly using tag sequences, as we
discussed their disadvantages above, we represent
each of them by a real-valued feature vector. Sup-
pose we have a collection of n latent syntactic cate-
gories C = {c1, · · · , cn}. For each tag sequence ts,
we compute its distribution of latent syntactic cate-
gories ~Pts(C) = (pts(c1), · · · , pts(cn)). For exam-
ple, ~P“NP VP”(C) = {0.5, 0.2, 0.3} means that the la-
tent syntactic categories c1, c2, and c3 are distributed
as p(c1) = 0.5, p(c2) = 0.2, and p(c3) = 0.3 for tag
sequence “NP VP”. We further convert the distribu-
tion to a normalized feature vector F~(ts) to repre-
sent tag sequence ts:
</bodyText>
<equation confidence="0.994758">
F~(ts) = (f1(ts), ··· , fn(ts))
= (pts(c1), · · · ,pts(cn))
k(pts(c1),··· ,pts(cn))k
</equation>
<bodyText confidence="0.999299">
The advantage of using real-valued feature vec-
tors is that the degree of similarity between two tag
sequences ts and ts&apos; in the space of the latent syn-
tactic categories C can be simply computed as a dot-
product3 of their feature vectors:
</bodyText>
<equation confidence="0.9627705">
F~ (ts) · F~ (ts&apos;) = � fi(ts)fi(ts&apos;)
1&lt;i&lt;n
</equation>
<bodyText confidence="0.970278214285714">
which computes a syntactic similarity score in the
range of 0 (totally syntactically different) to 1 (com-
pletely syntactically identical).
Similarly, we can represent the syntax of each X
nonterminal in a rule with a feature vector F~(X),
computed as the sum of the feature vectors of tag
3Other measures such as KL-divergence in the probability
space are also feasible.
Now we can impose soft syntactic constraints us-
ing these feature vectors when a SCFG rule is used
to translate a parsed source sentence. Given that a X
nonterminal in the rule is applied to a span with tag
sequence4 ts as determined by a syntactic parser, we
can compute the following syntax similarity feature:
</bodyText>
<equation confidence="0.789287">
SynSim(X, ts) = − log(F~(ts) · F~(X))
</equation>
<bodyText confidence="0.999790411764706">
Except that it is computed on the fly, this feature
can be used in the same way as the regular features
in hierarchical translation systems to determine the
best translation, and its feature weight can be tuned
in the same way together with the other features on
a held-out data set.
In our approach, the set of latent syntactic cate-
gories is automatically induced from a source-side
parsed, word-aligned parallel corpus based on the
hierarchical structure among phrase pairs along with
the syntactic parse of the source side. In what fol-
lows, we will explain the two critical aspects of
our approach, i.e., how to identify the hierarchi-
cal structures among all phrase pairs in a sentence
pair, and how to induce the latent syntactic cate-
gories from the hierarchy to syntactically explain the
phrase pairs.
</bodyText>
<sectionHeader confidence="0.993063" genericHeader="method">
4 Alignment-based Hierarchy
</sectionHeader>
<bodyText confidence="0.999917">
The aforementioned abstract rule extraction algo-
rithm of Chiang (2007) is based on the property that
a tight phrase pair can contain other tight phrase
pairs. Given two non-disjoint tight phrase pairs that
share at least one common alignment link, there are
only two relationships: either one completely in-
cludes another or they do not include one another
but have a non-empty overlap, which we call a non-
trivial overlap. In the second case, the intersection,
differences, and union of the two phrase pairs are
</bodyText>
<footnote confidence="0.913718666666667">
4A normalized uniform feature vector is used for tag se-
quences (of parsed test sentences) that are not seen on the train-
ing corpus.
</footnote>
<page confidence="0.990689">
141
</page>
<figureCaption confidence="0.98415025">
Figure 2: A decomposition tree of tight phrase pairs with
all tight phrase pairs listed on the right. As highlighted,
the two non-maximal phrase pairs are generated by con-
secutive sibling nodes.
</figureCaption>
<bodyText confidence="0.999923870967742">
also tight phrase pairs (see Figure 1 (b) for exam-
ple), and the two phrase pairs, as well as their inter-
section and differences, are all sub phrase pairs of
their union.
Zhang et al. (2008) exploited this property to con-
struct a hierarchical decomposition tree (Bui-Xuan
et al., 2005) of phrase pairs from a sentence pair to
extract all phrase pairs in linear time. In this pa-
per, we focus on learning the syntactic dependencies
along the hierarchy of phrase pairs. Our hierarchy
construction follows Heber and Stoye (2001).
Let P be the set of tight phrase pairs extracted
from a sentence pair. We call a sequentially-ordered
list5 L = (p1, · · · , pk) of unique phrase pairs pi E P
a chain if every two successive phrase pairs in L
have a non-trivial overlap. A chain is maximal if
it can not be extended to its left or right with other
phrase pairs. Note that any sub-sequence of phrase
pairs in a chain generates a tight phrase pair. In par-
ticular, chain L generates a tight phrase pair T(L)
that corresponds exactly to the union of the align-
ment links in p E L. We call the phrase pairs
generated by maximal chains maximal phrase pairs
and call the other phrase pairs non-maximal. Non-
maximal phrase pairs always overlap non-trivially
with some other phrase pairs while maximal phrase
pairs do not, and it can be shown that any non-
maximal phrase pair can be generated by a sequence
of maximal phrase pairs. Note that the largest tight
phrase pair that includes all alignment links in A is
also a maximal phrase pair.
</bodyText>
<footnote confidence="0.981353666666667">
5The phrase pairs can be sequentially ordered first by the
boundary positions of the source-side phrase and then by the
boundary positions of the target-side phrase.
</footnote>
<figureCaption confidence="0.838406571428571">
Figure 3: (a) decomposition tree for the English side of
the example sentence pair with all phrases underlined, (b)
automatic parse tree of the English side, (c) two example
binarized decomposition trees with syntactic emissions
in depicted in (d), where the two dotted curves give an
example I(·) and O(·) that separate the forest into two
parts.
</figureCaption>
<bodyText confidence="0.939926">
Lemma 1 Given two different maximal phrase
pairs p1 and p2, exactly one of the following alter-
natives is true: p1 and p2 are disjoint, p1 is a sub
phrase pair of p2, or p2 is a sub phrase pair of p1.
A direct outcome of Lemma 1 is that there is an
unique decomposition tree T = (N, E) covering all
of the tight phrase pairs of a sentence pair, where N
is the set of maximal phrase pairs and E is the set of
edges that connect between pairs of maximal phrase
pairs if one is a sub phrase pair of another. All of the
tight phrase pairs of a sentence pair can be extracted
directly from the nodes of the decomposition tree
(these phrase pairs are maximal), or generated by se-
quences of consecutive sibling nodes6 (these phrase
pairs are non-maximal). Figure 2 shows the decom-
position tree as well as all of the tight phrase pairs
that can be extracted from the example sentence pair
in Figure 1.
We focus on the source side of the decomposition
tree, and expand it to include all of the non-phrase
</bodyText>
<footnote confidence="0.734723">
6Unaligned words may be added.
</footnote>
<figure confidence="0.997878826923077">
X
X
VP
S
PP
X
NP
X B B B X X
give the pen to me .
VBP DT NN TO PRP .
give the pen to me .
(a) (b)
O(!)
I(!)
X
X
NP PP
X
X
X
X
X B B B X X
X B B B X X
VBP DT NN TO PRP .
O(!)
VP
X
X
CR
X
X
X
S
X
X
X
I(!)
X
NP PP
X
X B B B X X
DT NN TO PRP .
VBP
X B B B X X
(c) (d)
S
X
X
X
CR
X
CR
</figure>
<page confidence="0.991418">
142
</page>
<bodyText confidence="0.99999059375">
single words within the scope of the decomposition
tree as frontiers and attach each as a child of the low-
est node that contains the word. We then abstract the
trees nodes with two symbol, X for phrases, and B
for non-phrases, and call the result the decomposi-
tion tree of the source side phrases. Figure 3 (a) de-
picts such tree for the English side of our example
sentence pair. We further recursively binarize7 the
decomposition tree into a binarized decomposition
forest such that all phrases are directly represented
as nodes in the forest. Figure 3 (c) shows two of the
many binarized decomposition trees in the forest.
The binarized decomposition forest compactly
encodes the hierarchical structure among phrases
and non-phrases. However, the coarse abstraction
of phrases with X and non-phrases with B provides
little information on the constraints of the hierarchy.
In order to bring in syntactic constraints, we anno-
tate the nodes in the decomposition forest with syn-
tactic observations based on the automatic syntactic
parse tree of the source side. If a node aligns with
a constituent in the parse tree, we add the syntactic
category (e.g., NP) of the constituent as an emitted
observation of the node, otherwise, it crosses con-
stituent boundaries and we add a designated crossing
category CR as its observation. We call the resulting
forest a syntactic decomposition forest. Figure 3 (d)
shows two syntactic decomposition trees of the for-
est based on the parse tree in Figure 3 (b). We will
next describe how to learn finer-grained X and B
categories based on the hierarchical syntactic con-
straints.
</bodyText>
<sectionHeader confidence="0.991298" genericHeader="method">
5 Inducing Latent Syntactic Categories
</sectionHeader>
<bodyText confidence="0.9989992">
If we designate a unique symbol 5 as the new root
of the syntactic decomposition forests introduced
in the previous section, it can be shown that these
forests can be generated by a probabilistic context-
free grammar G = (V, E, 5, R, 0), where
</bodyText>
<listItem confidence="0.916106214285714">
• V = {5, X, B} is the set of nonterminals,
• E is the set of terminals comprising treebank
categories plus the CR tag (the crossing cate-
gory),
7The intermediate binarization nodes are also labeled as ei-
ther X or B based on whether they exactly cover a phrase or
not.
• 5 E V is the unique start symbol,
• R is the union of the set of production rules
each rewriting a nonterminal to a sequence of
nonterminals and the set of emission rules each
generating a terminal from a nonterminal,
• and 0 assigns a probability score to each rule
rER.
</listItem>
<bodyText confidence="0.944204675675676">
Such a grammar can be derived from the set of
syntactic decomposition forests extracted from a
source-side parsed parallel corpus, with rule prob-
ability scores estimated as the relative frequencies
of the production and emission rules.
The X and B nonterminals in the grammar are
coarse representations of phrase and non-phrases
and do not carry any syntactic information at all.
In order to introduce syntax to these nonterminals,
we incrementally split8 them into a set of latent
categories {X1, • • • , X�} for X and another set
{B1, • • • , B�} for B, and then learn a set of rule
probabilities9 0 on the latent categories so that the
likelihood of the training forests are maximized. The
motivation is to let the latent categories learn differ-
ent preferences of (emitted) syntactic categories as
well as structural dependencies along the hierarchy
so that they can carry syntactic information. We call
them latent syntactic categories. The learned Xi’s
represent syntactically-induced finer-grained cate-
gories of phrases and are used as the set of latent
syntactic categories C described in Section 3. In re-
lated research, Matsuzaki et al. (2005) and Petrov et
al. (2006) introduced latent variables to learn finer-
grained distinctions of treebank categories for pars-
ing, and Huang et al. (2009) used a similar approach
to learn finer-grained part-of-speech tags for tag-
ging. Our method is in spirit similar to these ap-
proaches.
Optimization of grammar parameters to maximize
the likelihood of training forests can be achieved
8We incrementally split each nonterminal to 2, 4, 8, and fi-
nally 16 categories, with each splitting followed by several EM
iterations to tune model parameters. We consider 16 an appro-
priate number for latent categories, not too small to differentiate
between different syntactic usages and not too large for the extra
computational and storage costs.
</bodyText>
<footnote confidence="0.736064666666667">
9Each binary production rule is now associated with a 3-
dimensional matrix of probabilities, and each emission rule as-
sociated with a 1-dimensional array of probabilities.
</footnote>
<page confidence="0.998916">
143
</page>
<bodyText confidence="0.999640368421053">
by a variant of Expectation-Maximization (EM) al-
gorithm. Recall that our decomposition forests are
fully binarized (except the root). In the hypergraph
representation (Huang and Chiang, 2005), the hy-
peredges of our forests all have the same format10
((V, W), U), meaning that node U expands to nodes
V and W with production rule U → V W. Given
a forest F with root node R, we denote e(U) the
emitted syntactic category at node U and LR(U) (or
PL(W), or PR(V ))11 the set of node pairs (V, W)
(or (U, V ), or (U, W)) such that ((V, W), U) is ahy-
peredge of the forest. Now consider node U, which
is either S, X, or B, in the forest. Let Ux be the
latent syntactic category12 of node U. We define
I(Ux) the part of the forest (includes e(U) but not
Ux) inside U, and O(Ux) the other part of the forest
(includes Ux but not e(U)) outside U, as illustrated
in Figure 3 (d). The inside-outside probabilities are
defined as:
</bodyText>
<equation confidence="0.999605">
PIN(Ux) = P(I(Ux)|Ux)
POUT(Ux) = P(O(Ux)|S)
which can be computed recursively as:
O(U. → e(U))
XO(U. → VyWz)
XPIN(Vy)PIN(Wz)
O(Vy → e(V ))
XO(Vy → WzU.)
XPOUT(Vy)PIN(Wz)
O(Vy → e(V ))
XO(Vy → U.Wz)
XPOUT(Vy)PIN(Wz)
</equation>
<bodyText confidence="0.989115">
In the E-step, the posterior probability of the oc-
currence of production rule13 Ux → VyWz is com-
puted as:
</bodyText>
<equation confidence="0.9857965">
O(U. → e(U))
XO(U. → VyWz)
XPOUT(U.)PIN(Vy)PIN(W.)
PIN(R)
</equation>
<footnote confidence="0.92891025">
10The hyperedge corresponding to the root node has a differ-
ent format because it is unary, but it can be handled similarly.
When clear from context, we use the same variable to present
both a node and its label.
11LR stands for the left and right children, PL for the parent
and left children, and PR for the parent and right children.
12We never split the start symbol S, and denote So = S.
13The emission rules can be handled similarly.
</footnote>
<bodyText confidence="0.9998678">
In the M-step, the expected counts of rule Ux →
VyWz for all latent categories Vy and Wz are accu-
mulated together and then normalized to obtain an
update of the probability estimation:
Recall that each node U labeled as X in a forest is
associated with a phrase whose syntax is abstracted
by a tag sequence. Once a grammar is learned, for
each such node with a corresponding tag sequence
ts in forest F, we compute the posterior probability
that the latent category of node U being Xi as:
</bodyText>
<equation confidence="0.995180666666667">
POUT(Ui)PIN(Ui)
P(Xi|ts) =
PIN(R)
</equation>
<bodyText confidence="0.999878">
This contributes P(Xi|ts) evidence that tag se-
quence ts belongs to a Xi category. When all
of the evidences are computed and accumulated in
#(Xi, ts), they can then be normalized to obtain the
probability that the latent category of ts is Xi:
</bodyText>
<equation confidence="0.991836333333333">
pts(Xi) = #(Xi, ts)
P
i #(Xi, ts)
</equation>
<bodyText confidence="0.999944333333333">
As described in Section 3, the distributions of latent
categories are used to compute the syntactic feature
vectors for the SCFG rules.
</bodyText>
<sectionHeader confidence="0.998718" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999959538461538">
We conduct experiments on two tasks, English-to-
German and English-to-Chinese, both aimed for
speech-to-speech translation. The training data for
the English-to-German task is a filtered subset of the
Europarl corpus (Koehn, 2005), containing —300k
parallel bitext with —4.5M tokens on each side. The
dev and test sets both contain 1k sentences with one
reference for each. The training data for the English-
to-Chinese task is collected from transcription and
human translation of conversations in travel domain.
It consists of —500k parallel bitext with —3M to-
kens14 on each side. Both dev and test sets contain
—1.3k sentences, each with two references. Both
</bodyText>
<footnote confidence="0.706764">
14The Chinese sentences are automatically segmented into
words. However, BLEU scores are computed at character level
for tuning and evaluation.
</footnote>
<equation confidence="0.990375636363636">
PIN(U.) = X X
(V,W)ELR(U) y,z
POUT(U.) = X X
(V,W)EPL(U) y,z
+ X X
(V,W)EPR(U) y,z
P(U. → VyWz|F) =
O(Ux → VyWz) = #(Ux → VyWz)
P P #(Ux → VyWz)
(V&apos;,Wl)
y,z
</equation>
<page confidence="0.994785">
144
</page>
<bodyText confidence="0.998510916666667">
corpora are also preprocessed with punctuation re-
moved and words down-cased to make them suitable
for speech translation.
The baseline system is our implementation of the
hierarchical phrase-based model of Chiang (2007),
and it includes basic features such as rule and
lexicalized rule translation probabilities, language
model scores, rule counts, etc. We use 4-gram lan-
guage models in both tasks, and conduct minimum-
error-rate training (Och, 2003) to optimize feature
weights on the dev set. Our baseline hierarchical
model has 8.3M and 9.7M rules for the English-to-
German and English-to-Chinese tasks, respectively.
The English side of the parallel data is
parsed by our implementation of the Berkeley
parser (Huang and Harper, 2009) trained on the
combination of Broadcast News treebank from
Ontonotes (Weischedel et al., 2008) and a speechi-
fied version of the WSJ treebank (Marcus et al.,
1999) to achieve higher parsing accuracy (Huang et
al., 2010). Our approach introduces a new syntactic
feature and its feature weight is tuned in the same
way together with the features in the baseline model.
In this study, we induce 16 latent categories for both
X and B nonterminals.
Our approach identifies —180k unique tag se-
quences for the English side of phrase pairs in both
tasks. As shown by the examples in Table 2, the syn-
tactic feature vector representation is able to identify
similar and dissimilar tag sequences. For instance,
it determines that the sequence of “DT JJ NN” is
syntactically very similar to “DT ADJP NN” while
very dissimilar to “NN CD VP”. Notice that our la-
tent categories are learned automatically to maxi-
mize the likelihood of the training forests extracted
based on alignment and are not explicitly instructed
to discriminate between syntactically different tag
sequences. Our approach is not guaranteed to al-
ways assign similar feature vectors to syntactically
similar tag sequences. However, as the experimental
results show below, the latent categories are able to
capture some similarities among tag sequences that
are beneficial for translation.
Table 3 and 4 report the experimental results
on the English-to-German and English-to-Chinese
tasks, respectively. The addition of the syntax fea-
ture achieves a statistically significant improvement
(p &lt; 0.01) of 0.6 in BLEU on the test set of the
</bodyText>
<table confidence="0.979126666666667">
Baseline +Syntax 0
Dev 16.26 17.06 0.80
Test 16.41 17.01 0.60
</table>
<tableCaption confidence="0.927397">
Table 3: BLEU scores of the English-to-German task
(one reference).
</tableCaption>
<table confidence="0.997172">
Baseline +Syntax 0
Dev 46.47 47.39 0.92
Test 45.45 45.86 0.41
</table>
<tableCaption confidence="0.9943295">
Table 4: BLEU scores of the English-to-Chinese task
(two references).
</tableCaption>
<bodyText confidence="0.99971625">
English-to-German task. This improvement is sub-
stantial given that only one reference is used for each
test sentence. On the English-to-Chinese task, the
syntax feature achieves a smaller improvement of
0.41 BLEU on the test set. One potential explanation
for the smaller improvement is that the sentences on
the English-to-Chinese task are much shorter, with
an average of only 6 words per sentence, compared
to 15 words in the English-to-German task. The
hypothesis space of translating a longer sentence is
much larger than that of a shorter sentence. There-
fore, there is more potential gain from using syn-
tax features to rule out unlikely derivations of longer
sentences, while phrasal rules might be adequate for
shorter sentences, leaving less room for syntax to
help as in the case of the English-to-Chinese task.
</bodyText>
<sectionHeader confidence="0.998607" genericHeader="method">
7 Discussions
</sectionHeader>
<bodyText confidence="0.999982733333333">
The incorporation of the syntactic feature into the
hierarchical phrase-based translation system also
brings in additional memory load and computational
cost. In the worst case, our approach requires stor-
ing one feature vector for each tag sequence and one
feature vector for each nonterminal of a SCFG rule,
with the latter taking the majority of the extra mem-
ory storage. We observed that about 90% of the
X nonterminals in the rules only have one tag se-
quence, and thus the required memory space can be
significantly reduced by only storing a pointer to the
feature vector of the tag sequence for these nonter-
minals. Our approach also requires computing one
dot-product of two feature vectors for each nonter-
minal when a SCFG rule is applied to a source span.
</bodyText>
<page confidence="0.996475">
145
</page>
<table confidence="0.944638363636363">
Very similar Not so similar Very dissimilar
F(ts) · F(ts&apos;) &gt; 0.9 0.4 &lt; F(ts) · F(ts&apos;) &lt; 0.6 F(ts) · F(ts&apos;) &lt; 0.1
DT NN DT JJ JJ NML NN PP NP NN
DT JJ NN DT JJ JJ NN DT JJ CC INTJ VB NN CD VP
DT ADJP NN DT NN NN JJ RB NP IN CD
VB VP PP JJ NN JJ NN TO VP
VP VB RB VB PP VB NN NN VB JJ WHNP DT NN
VB DT DT NN VB RB IN JJ IN INTJ NP
JJ ADJP JJ JJ CC ADJP IN NP JJ
ADJP PDT JJ ADJP VB JJ JJ AUX RB ADJP
RB JJ ADVP WHNP JJ ADJP VP
</table>
<tableCaption confidence="0.999544">
Table 2: Examples of similar and dissimilar tag sequences.
</tableCaption>
<bodyText confidence="0.999975918918919">
This cost can be reduced, however, by caching the
dot-products of the tag sequences that are frequently
accessed.
There are other successful investigations to
impose soft syntactic constraints to hierarchical
phrase-based models by either introducing syntax-
based rule features such as the prior derivation
model of Zhou et al. (2008) or by imposing con-
straints on translation spans at decoding time, e.g.,
(Marton and Resnik, 2008; Xiong et al., 2009;
Xiong et al., 2010). These approaches are all or-
thogonal to ours and it is expected that they can be
combined with our approach to achieve greater im-
provement.
This work is an initial effort to investigate latent
syntactic categories to enhance hierarchical phrase-
based translation models, and there are many direc-
tions to continue this line of research. First, while
the current approach imposes soft syntactic con-
straints between the parse structure of the source
sentence and the SCFG rules used to derive the
translation, the real-valued syntactic feature vectors
can also be used to impose soft constraints between
SCFG rules when rule rewrite occurs. In this case,
target side parse trees could also be used alone or to-
gether with the source side parse trees to induce the
latent syntactic categories. Second, instead of using
single parse trees during both training and decod-
ing, our approach is likely to benefit from exploring
parse forests as in (Mi and Huang, 2008). Third,
in addition to the treebank categories obtained by
syntactic parsing, lexical cues directly available in
sentence pairs could also to explored to guide the
learning of latent categories. Last but not the least,
it would be interesting to investigate discriminative
training approaches to learn latent categories that di-
rectly optimize on translation quality.
</bodyText>
<sectionHeader confidence="0.998218" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999991615384615">
We have presented a novel approach to enhance
hierarchical phrase-based machine translation sys-
tems with real-valued linguistically motivated fea-
ture vectors. Our approach maintains the advan-
tages of hierarchical phrase-based translation sys-
tems while at the same time naturally incorpo-
rates soft syntactic constraints. Experimental results
showed that this approach improves the baseline hi-
erarchical phrase-based translation models on both
English-to-German and English-to-Chinese tasks.
We will continue this line of research and exploit
better ways to learn syntax and apply syntactic con-
straints to machine translation.
</bodyText>
<sectionHeader confidence="0.996521" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999824">
This work was done when the first author was visit-
ing IBM T. J. Watson Research Center as a research
intern. We would like to thank Mary Harper for
lots of insightful discussions and suggestions and the
anonymous reviewers for the helpful comments.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9921005">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
</reference>
<page confidence="0.987602">
146
</page>
<reference confidence="0.998969741573034">
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics.
Binh Minh Bui-Xuan, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura’s al-
gorithm. In ISAAC.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2004. What’s in a translation rule. In
HLT/NAACL.
Steffen Heber and Jens Stoye. 2001. Finding all common
intervals of k permutations. In CPM.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In International Workshop on Parsing Tech-
nology.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In CHSLP.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm part-
of-speech tagger by latent annotation and self-training.
In NAACL.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable. In
EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In ACL.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor, 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In EMNLP.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In NAACL.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009.
A syntax-driven bracketing model for phrase-based
translation. In ACL-IJCNLP.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In NAACL-HLT.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In COLING.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntactic
parsing and tree kernels. In SSST.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
StatMT.
</reference>
<page confidence="0.998044">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438766">
<title confidence="0.9978455">Soft Syntactic Constraints for Hierarchical Phrase-based Using Latent Syntactic Distributions</title>
<author confidence="0.922845">Zhongqiang</author>
<affiliation confidence="0.9984595">Institute for Advanced Computer University of</affiliation>
<address confidence="0.999752">College Park, MD 20742</address>
<email confidence="0.999604">zqhuang@umiacs.umd.edu</email>
<affiliation confidence="0.7193265">IBM T. J. Watson Research Yorktown Heights, NY</affiliation>
<abstract confidence="0.998502608695652">In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features. Rather than directly using treebank categories as in previous studies, we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed, word-aligned parallel corpus, based on the hierarchical structure among phrase pairs as well as the syntactic structure the source side. In our model, each nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="7074" citStr="Brown et al., 1993" startWordPosition="1069" endWordPosition="1072">In particular, the hierarchical model (Chiang, 2007) studied in this paper explores hierarchical structures of natural language and utilize only a unified nonterminal symbol X in the grammar, X — (-y, α, —) where — is the one-to-one correspondence between X’s in -y and α, and it can be indicated by underscripted co-indexes. Two example English-toChinese translation rules are represented as follows: X (give the pen to me, 钢笔 给 我) (1) X (give X1 to me, X1 给 我) (2) The SCFG rules of hierarchical phrase-based models are extracted automatically from corpora of word-aligned parallel sentence pairs (Brown et al., 1993; Och and Ney, 2000). An aligned sentence pair is a tuple (E, F, A), where E = e1 · · · e,,, can be interpreted as an English sentence of length n, F = f1 · · · fm its translation of length m in a foreign language, and A a set of links between words of the two sentences. Figure 1 (a) shows an example of aligned English-to-Chinese sentence pair. Widely adopted in phrase-based models (Och and Ney, 2004), a pair of consecutive sequences of words from E and F is a phrase pair if all words are aligned only within the sequences and not to any word outside. We call a sequence of words a phrase if it </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binh Minh Bui-Xuan</author>
<author>Michel Habib</author>
<author>Christophe Paul</author>
</authors>
<title>Revisiting T. Uno and M. Yagiura’s algorithm.</title>
<date>2005</date>
<booktitle>In ISAAC.</booktitle>
<contexts>
<context position="16425" citStr="Bui-Xuan et al., 2005" startWordPosition="2733" endWordPosition="2736">rs are 4A normalized uniform feature vector is used for tag sequences (of parsed test sentences) that are not seen on the training corpus. 141 Figure 2: A decomposition tree of tight phrase pairs with all tight phrase pairs listed on the right. As highlighted, the two non-maximal phrase pairs are generated by consecutive sibling nodes. also tight phrase pairs (see Figure 1 (b) for example), and the two phrase pairs, as well as their intersection and differences, are all sub phrase pairs of their union. Zhang et al. (2008) exploited this property to construct a hierarchical decomposition tree (Bui-Xuan et al., 2005) of phrase pairs from a sentence pair to extract all phrase pairs in linear time. In this paper, we focus on learning the syntactic dependencies along the hierarchy of phrase pairs. Our hierarchy construction follows Heber and Stoye (2001). Let P be the set of tight phrase pairs extracted from a sentence pair. We call a sequentially-ordered list5 L = (p1, · · · , pk) of unique phrase pairs pi E P a chain if every two successive phrase pairs in L have a non-trivial overlap. A chain is maximal if it can not be extended to its left or right with other phrase pairs. Note that any sub-sequence of p</context>
</contexts>
<marker>Bui-Xuan, Habib, Paul, 2005</marker>
<rawString>Binh Minh Bui-Xuan, Michel Habib, and Christophe Paul. 2005. Revisiting T. Uno and M. Yagiura’s algorithm. In ISAAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="1444" citStr="Chiang, 2007" startWordPosition="203" endWordPosition="204">rce side. In our model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints. 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized into two classes based on their dependency on annotated corpus (Chiang, 2007). Linguistically syntaxbased models (e.g., (Yamada and Knight, 2001; Galley et al., 2004; Liu et al.</context>
<context position="6508" citStr="Chiang, 2007" startWordPosition="973" endWordPosition="974">esults are reported in Section 6, followed by discussions in Section 7. Section 8 concludes this paper. 2 Hierarchical Phrase-Based Translation An SCFG is a synchronous rewriting system generating source and target side string pairs simultaneously based on a context-free grammar. Each synchronous production (i.e., rule) rewrites a nonterminal into a pair of strings, -y and α, where -y (or α) contains terminal and nonterminal symbols from the source (or target) language and there is a one-toone correspondence between the nonterminal symbols on both sides. In particular, the hierarchical model (Chiang, 2007) studied in this paper explores hierarchical structures of natural language and utilize only a unified nonterminal symbol X in the grammar, X — (-y, α, —) where — is the one-to-one correspondence between X’s in -y and α, and it can be indicated by underscripted co-indexes. Two example English-toChinese translation rules are represented as follows: X (give the pen to me, 钢笔 给 我) (1) X (give X1 to me, X1 给 我) (2) The SCFG rules of hierarchical phrase-based models are extracted automatically from corpora of word-aligned parallel sentence pairs (Brown et al., 1993; Och and Ney, 2000). An aligned s</context>
<context position="8589" citStr="Chiang (2007)" startWordPosition="1361" endWordPosition="1362">among all that share the same set of alignment links. Figure 1 (b) highlights the tight phrase pairs in the example sentence pair. 139 6 5 4 3 2 1 1 2 3 4 5 (a) (b) Figure 1: An example of word-aligned sentence pair (a) with tight phrase pairs marked in a matrix representation (b). The extraction of SCFG rules proceeds as follows. In the first step, all phrase pairs below a maximum length are extracted as phrasal rules. In the second step, abstract rules are extracted from tight phrase pairs that contain other tight phrase pairs by replacing the sub phrase pairs with co-indexed Xnonterminals. Chiang (2007) also introduced several requirements (e.g., there are at most two nonterminals at the right hand side of a rule) to safeguard the quality of the abstract rules as well as keeping decoding efficient. In our example above, rule (2) can be extracted from rule (1) with the following sub phrase pair: X , (the pen, 钢笔) The use of a unified X nonterminal makes hierarchical phrase-based models flexible at capturing non-local reordering of phrases. However, such flexibility also comes at the cost that it is not able to differentiate between different syntactic usages of phrases. Suppose rule X —* (I a</context>
<context position="15369" citStr="Chiang (2007)" startWordPosition="2553" endWordPosition="2554">pproach, the set of latent syntactic categories is automatically induced from a source-side parsed, word-aligned parallel corpus based on the hierarchical structure among phrase pairs along with the syntactic parse of the source side. In what follows, we will explain the two critical aspects of our approach, i.e., how to identify the hierarchical structures among all phrase pairs in a sentence pair, and how to induce the latent syntactic categories from the hierarchy to syntactically explain the phrase pairs. 4 Alignment-based Hierarchy The aforementioned abstract rule extraction algorithm of Chiang (2007) is based on the property that a tight phrase pair can contain other tight phrase pairs. Given two non-disjoint tight phrase pairs that share at least one common alignment link, there are only two relationships: either one completely includes another or they do not include one another but have a non-empty overlap, which we call a nontrivial overlap. In the second case, the intersection, differences, and union of the two phrase pairs are 4A normalized uniform feature vector is used for tag sequences (of parsed test sentences) that are not seen on the training corpus. 141 Figure 2: A decompositi</context>
<context position="27850" citStr="Chiang (2007)" startWordPosition="4779" endWordPosition="4780">ach side. Both dev and test sets contain —1.3k sentences, each with two references. Both 14The Chinese sentences are automatically segmented into words. However, BLEU scores are computed at character level for tuning and evaluation. PIN(U.) = X X (V,W)ELR(U) y,z POUT(U.) = X X (V,W)EPL(U) y,z + X X (V,W)EPR(U) y,z P(U. → VyWz|F) = O(Ux → VyWz) = #(Ux → VyWz) P P #(Ux → VyWz) (V&apos;,Wl) y,z 144 corpora are also preprocessed with punctuation removed and words down-cased to make them suitable for speech translation. The baseline system is our implementation of the hierarchical phrase-based model of Chiang (2007), and it includes basic features such as rule and lexicalized rule translation probabilities, language model scores, rule counts, etc. We use 4-gram language models in both tasks, and conduct minimumerror-rate training (Och, 2003) to optimize feature weights on the dev set. Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively. The English side of the parallel data is parsed by our implementation of the Berkeley parser (Huang and Harper, 2009) trained on the combination of Broadcast News treebank from Ontonotes (Weischedel e</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3857" citStr="Chiang (2010)" startWordPosition="563" endWordPosition="564">oding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics icantly outperform hierarchical phrase-based models when using forest-based rule extraction together with forest-based decoding. Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. In this paper, we augment rules in hierarchical phrase-based translation systems with novel syntactic features. Unlike previous studies (e.g., (Zollmann and Venugopal, 2006)) that directly use explicit treebank categories such as NP, NP/PP (NP missing PP from the right) to annotate phrase pairs, we induce a set of latent categories to capture the syntactic </context>
<context position="10766" citStr="Chiang (2010)" startWordPosition="1737" endWordPosition="1738"> not align with syntactic constituents. Their hard syntactic constraint requires that the nonterminals should match exactly to rewrite with a rule, which could rule out potentially correct derivations due to errors in the syntactic parses as well as to data sparsity. For example, NP cannot be instantiated with phrase pairs of type DT+NN, in spite of their syntactic similarity. Venugopal et al. (2009) addressed this problem by directly introducing soft syntactic preferences into SCFG rules using preference grammars, but they had to face the computational challenges of large preference vectors. Chiang (2010) also avoided hard constraints and took a soft alternative that directly models the cost of mismatched rule substitutions. This, however, would require a large number of parameters to be tuned on a generally small-sized heldout set, and it could thus suffer from over-tuning. 3 Approach Overview In this work, we take a different approach to introduce linguistic syntax to hierarchical phrase-based translation systems and impose soft syntactic constraints between derivation rules and the syntactic parse of the sentence to be translated. For each phrase pair extracted from a sentence pair of a sou</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>What’s in a translation rule.</title>
<date>2004</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="1465" citStr="Galley et al., 2004" startWordPosition="205" endWordPosition="208">ur model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints. 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized into two classes based on their dependency on annotated corpus (Chiang, 2007). Linguistically syntaxbased models (e.g., (Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006)) utilize stru</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2004</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2004. What’s in a translation rule. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Heber</author>
<author>Jens Stoye</author>
</authors>
<title>Finding all common intervals of k permutations.</title>
<date>2001</date>
<booktitle>In CPM.</booktitle>
<contexts>
<context position="16664" citStr="Heber and Stoye (2001)" startWordPosition="2773" endWordPosition="2776">ight. As highlighted, the two non-maximal phrase pairs are generated by consecutive sibling nodes. also tight phrase pairs (see Figure 1 (b) for example), and the two phrase pairs, as well as their intersection and differences, are all sub phrase pairs of their union. Zhang et al. (2008) exploited this property to construct a hierarchical decomposition tree (Bui-Xuan et al., 2005) of phrase pairs from a sentence pair to extract all phrase pairs in linear time. In this paper, we focus on learning the syntactic dependencies along the hierarchy of phrase pairs. Our hierarchy construction follows Heber and Stoye (2001). Let P be the set of tight phrase pairs extracted from a sentence pair. We call a sequentially-ordered list5 L = (p1, · · · , pk) of unique phrase pairs pi E P a chain if every two successive phrase pairs in L have a non-trivial overlap. A chain is maximal if it can not be extended to its left or right with other phrase pairs. Note that any sub-sequence of phrase pairs in a chain generates a tight phrase pair. In particular, chain L generates a tight phrase pair T(L) that corresponds exactly to the union of the alignment links in p E L. We call the phrase pairs generated by maximal chains max</context>
</contexts>
<marker>Heber, Stoye, 2001</marker>
<rawString>Steffen Heber and Jens Stoye. 2001. Finding all common intervals of k permutations. In CPM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In International Workshop on Parsing Technology.</booktitle>
<contexts>
<context position="24172" citStr="Huang and Chiang, 2005" startWordPosition="4127" endWordPosition="4130">ing followed by several EM iterations to tune model parameters. We consider 16 an appropriate number for latent categories, not too small to differentiate between different syntactic usages and not too large for the extra computational and storage costs. 9Each binary production rule is now associated with a 3- dimensional matrix of probabilities, and each emission rule associated with a 1-dimensional array of probabilities. 143 by a variant of Expectation-Maximization (EM) algorithm. Recall that our decomposition forests are fully binarized (except the root). In the hypergraph representation (Huang and Chiang, 2005), the hyperedges of our forests all have the same format10 ((V, W), U), meaning that node U expands to nodes V and W with production rule U → V W. Given a forest F with root node R, we denote e(U) the emitted syntactic category at node U and LR(U) (or PL(W), or PR(V ))11 the set of node pairs (V, W) (or (U, V ), or (U, W)) such that ((V, W), U) is ahyperedge of the forest. Now consider node U, which is either S, X, or B, in the forest. Let Ux be the latent syntactic category12 of node U. We define I(Ux) the part of the forest (includes e(U) but not Ux) inside U, and O(Ux) the other part of the</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In International Workshop on Parsing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Selftraining PCFG grammars with latent annotations across languages. In EMNLP.</title>
<date>2009</date>
<contexts>
<context position="28367" citStr="Huang and Harper, 2009" startWordPosition="4857" endWordPosition="4860">slation. The baseline system is our implementation of the hierarchical phrase-based model of Chiang (2007), and it includes basic features such as rule and lexicalized rule translation probabilities, language model scores, rule counts, etc. We use 4-gram language models in both tasks, and conduct minimumerror-rate training (Och, 2003) to optimize feature weights on the dev set. Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively. The English side of the parallel data is parsed by our implementation of the Berkeley parser (Huang and Harper, 2009) trained on the combination of Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and a speechified version of the WSJ treebank (Marcus et al., 1999) to achieve higher parsing accuracy (Huang et al., 2010). Our approach introduces a new syntactic feature and its feature weight is tuned in the same way together with the features in the baseline model. In this study, we induce 16 latent categories for both X and B nonterminals. Our approach identifies —180k unique tag sequences for the English side of phrase pairs in both tasks. As shown by the examples in Table 2, the syntactic fe</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. Selftraining PCFG grammars with latent annotations across languages. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>A syntax-directed translator with extended domain of locality.</title>
<date>2006</date>
<booktitle>In CHSLP.</booktitle>
<contexts>
<context position="3157" citStr="Huang et al., 2006" startWordPosition="461" endWordPosition="464">dels of Chiang (2007), which is formally syntax-based, and always refer the term SCFG, from now on, to the grammars of this model class. On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models. As shown in (Mi and Huang, 2008), hierarchical phrase-based models significantly outperform tree-to-string models (Liu et al., 2006; Huang et al., 2006), even when attempts are made to alleviate parsing errors using either forest-based decoding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics icantly outperform hierarchical phrase-bas</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A syntax-directed translator with extended domain of locality. In CHSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Vladimir Eidelman</author>
<author>Mary Harper</author>
</authors>
<title>Improving a simple bigram hmm partof-speech tagger by latent annotation and self-training.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="23221" citStr="Huang et al. (2009)" startWordPosition="3981" endWordPosition="3984">ximized. The motivation is to let the latent categories learn different preferences of (emitted) syntactic categories as well as structural dependencies along the hierarchy so that they can carry syntactic information. We call them latent syntactic categories. The learned Xi’s represent syntactically-induced finer-grained categories of phrases and are used as the set of latent syntactic categories C described in Section 3. In related research, Matsuzaki et al. (2005) and Petrov et al. (2006) introduced latent variables to learn finergrained distinctions of treebank categories for parsing, and Huang et al. (2009) used a similar approach to learn finer-grained part-of-speech tags for tagging. Our method is in spirit similar to these approaches. Optimization of grammar parameters to maximize the likelihood of training forests can be achieved 8We incrementally split each nonterminal to 2, 4, 8, and finally 16 categories, with each splitting followed by several EM iterations to tune model parameters. We consider 16 an appropriate number for latent categories, not too small to differentiate between different syntactic usages and not too large for the extra computational and storage costs. 9Each binary prod</context>
</contexts>
<marker>Huang, Eidelman, Harper, 2009</marker>
<rawString>Zhongqiang Huang, Vladimir Eidelman, and Mary Harper. 2009. Improving a simple bigram hmm partof-speech tagger by latent annotation and self-training. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Slav Petrov</author>
</authors>
<title>Self-training with products of latent variable.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="28586" citStr="Huang et al., 2010" startWordPosition="4893" endWordPosition="4896">rule counts, etc. We use 4-gram language models in both tasks, and conduct minimumerror-rate training (Och, 2003) to optimize feature weights on the dev set. Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively. The English side of the parallel data is parsed by our implementation of the Berkeley parser (Huang and Harper, 2009) trained on the combination of Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and a speechified version of the WSJ treebank (Marcus et al., 1999) to achieve higher parsing accuracy (Huang et al., 2010). Our approach introduces a new syntactic feature and its feature weight is tuned in the same way together with the features in the baseline model. In this study, we induce 16 latent categories for both X and B nonterminals. Our approach identifies —180k unique tag sequences for the English side of phrase pairs in both tasks. As shown by the examples in Table 2, the syntactic feature vector representation is able to identify similar and dissimilar tag sequences. For instance, it determines that the sequence of “DT JJ NN” is syntactically very similar to “DT ADJP NN” while very dissimilar to “N</context>
</contexts>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with products of latent variable. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="26896" citStr="Koehn, 2005" startWordPosition="4621" endWordPosition="4622">sequence ts belongs to a Xi category. When all of the evidences are computed and accumulated in #(Xi, ts), they can then be normalized to obtain the probability that the latent category of ts is Xi: pts(Xi) = #(Xi, ts) P i #(Xi, ts) As described in Section 3, the distributions of latent categories are used to compute the syntactic feature vectors for the SCFG rules. 6 Experiments We conduct experiments on two tasks, English-toGerman and English-to-Chinese, both aimed for speech-to-speech translation. The training data for the English-to-German task is a filtered subset of the Europarl corpus (Koehn, 2005), containing —300k parallel bitext with —4.5M tokens on each side. The dev and test sets both contain 1k sentences with one reference for each. The training data for the Englishto-Chinese task is collected from transcription and human translation of conversations in travel domain. It consists of —500k parallel bitext with —3M tokens14 on each side. Both dev and test sets contain —1.3k sentences, each with two references. Both 14The Chinese sentences are automatically segmented into words. However, BLEU scores are computed at character level for tuning and evaluation. PIN(U.) = X X (V,W)ELR(U) </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1484" citStr="Liu et al., 2006" startWordPosition="209" endWordPosition="212">erminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints. 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized into two classes based on their dependency on annotated corpus (Chiang, 2007). Linguistically syntaxbased models (e.g., (Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006)) utilize structures defined over</context>
<context position="3136" citStr="Liu et al., 2006" startWordPosition="457" endWordPosition="460">al phrase-based models of Chiang (2007), which is formally syntax-based, and always refer the term SCFG, from now on, to the grammars of this model class. On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models. As shown in (Mi and Huang, 2008), hierarchical phrase-based models significantly outperform tree-to-string models (Liu et al., 2006; Huang et al., 2006), even when attempts are made to alleviate parsing errors using either forest-based decoding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics icantly outperform hi</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
</authors>
<date>1999</date>
<booktitle>Treebank-3. Linguistic Data Consortium,</booktitle>
<location>Mary Ann Marcinkiewicz, and Ann Taylor,</location>
<marker>Marcus, Santorini, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor, 1999. Treebank-3. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="32737" citStr="Marton and Resnik, 2008" startWordPosition="5615" endWordPosition="5618">NN VB DT DT NN VB RB IN JJ IN INTJ NP JJ ADJP JJ JJ CC ADJP IN NP JJ ADJP PDT JJ ADJP VB JJ JJ AUX RB ADJP RB JJ ADVP WHNP JJ ADJP VP Table 2: Examples of similar and dissimilar tag sequences. This cost can be reduced, however, by caching the dot-products of the tag sequences that are frequently accessed. There are other successful investigations to impose soft syntactic constraints to hierarchical phrase-based models by either introducing syntaxbased rule features such as the prior derivation model of Zhou et al. (2008) or by imposing constraints on translation spans at decoding time, e.g., (Marton and Resnik, 2008; Xiong et al., 2009; Xiong et al., 2010). These approaches are all orthogonal to ours and it is expected that they can be combined with our approach to achieve greater improvement. This work is an initial effort to investigate latent syntactic categories to enhance hierarchical phrasebased translation models, and there are many directions to continue this line of research. First, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and the SCFG rules used to derive the translation, the real-valued syntactic feature vectors can also b</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23073" citStr="Matsuzaki et al. (2005)" startWordPosition="3957" endWordPosition="3960">set {B1, • • • , B�} for B, and then learn a set of rule probabilities9 0 on the latent categories so that the likelihood of the training forests are maximized. The motivation is to let the latent categories learn different preferences of (emitted) syntactic categories as well as structural dependencies along the hierarchy so that they can carry syntactic information. We call them latent syntactic categories. The learned Xi’s represent syntactically-induced finer-grained categories of phrases and are used as the set of latent syntactic categories C described in Section 3. In related research, Matsuzaki et al. (2005) and Petrov et al. (2006) introduced latent variables to learn finergrained distinctions of treebank categories for parsing, and Huang et al. (2009) used a similar approach to learn finer-grained part-of-speech tags for tagging. Our method is in spirit similar to these approaches. Optimization of grammar parameters to maximize the likelihood of training forests can be achieved 8We incrementally split each nonterminal to 2, 4, 8, and finally 16 categories, with each splitting followed by several EM iterations to tune model parameters. We consider 16 an appropriate number for latent categories, </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3037" citStr="Mi and Huang, 2008" startWordPosition="446" endWordPosition="449">irs without any explicit linguistic knowledge or annotations. In this work, we focus on the hierarchical phrase-based models of Chiang (2007), which is formally syntax-based, and always refer the term SCFG, from now on, to the grammars of this model class. On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models. As shown in (Mi and Huang, 2008), hierarchical phrase-based models significantly outperform tree-to-string models (Liu et al., 2006; Huang et al., 2006), even when attempts are made to alleviate parsing errors using either forest-based decoding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, MIT, Massachuse</context>
<context position="33735" citStr="Mi and Huang, 2008" startWordPosition="5781" endWordPosition="5784">rst, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and the SCFG rules used to derive the translation, the real-valued syntactic feature vectors can also be used to impose soft constraints between SCFG rules when rule rewrite occurs. In this case, target side parse trees could also be used alone or together with the source side parse trees to induce the latent syntactic categories. Second, instead of using single parse trees during both training and decoding, our approach is likely to benefit from exploring parse forests as in (Mi and Huang, 2008). Third, in addition to the treebank categories obtained by syntactic parsing, lexical cues directly available in sentence pairs could also to explored to guide the learning of latent categories. Last but not the least, it would be interesting to investigate discriminative training approaches to learn latent categories that directly optimize on translation quality. 8 Conclusion We have presented a novel approach to enhance hierarchical phrase-based machine translation systems with real-valued linguistically motivated feature vectors. Our approach maintains the advantages of hierarchical phrase</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3267" citStr="Mi et al., 2008" startWordPosition="479" endWordPosition="482">rs of this model class. On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models. As shown in (Mi and Huang, 2008), hierarchical phrase-based models significantly outperform tree-to-string models (Liu et al., 2006; Huang et al., 2006), even when attempts are made to alleviate parsing errors using either forest-based decoding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics icantly outperform hierarchical phrase-based models when using forest-based rule extraction together with forest-based decoding. Chiang (2010) also obta</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7094" citStr="Och and Ney, 2000" startWordPosition="1073" endWordPosition="1076">ierarchical model (Chiang, 2007) studied in this paper explores hierarchical structures of natural language and utilize only a unified nonterminal symbol X in the grammar, X — (-y, α, —) where — is the one-to-one correspondence between X’s in -y and α, and it can be indicated by underscripted co-indexes. Two example English-toChinese translation rules are represented as follows: X (give the pen to me, 钢笔 给 我) (1) X (give X1 to me, X1 给 我) (2) The SCFG rules of hierarchical phrase-based models are extracted automatically from corpora of word-aligned parallel sentence pairs (Brown et al., 1993; Och and Ney, 2000). An aligned sentence pair is a tuple (E, F, A), where E = e1 · · · e,,, can be interpreted as an English sentence of length n, F = f1 · · · fm its translation of length m in a foreign language, and A a set of links between words of the two sentences. Figure 1 (a) shows an example of aligned English-to-Chinese sentence pair. Widely adopted in phrase-based models (Och and Ney, 2004), a pair of consecutive sequences of words from E and F is a phrase pair if all words are aligned only within the sequences and not to any word outside. We call a sequence of words a phrase if it corresponds to eithe</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation. Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="1668" citStr="Och and Ney, 2004" startWordPosition="237" endWordPosition="240"> time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints. 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized into two classes based on their dependency on annotated corpus (Chiang, 2007). Linguistically syntaxbased models (e.g., (Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006)) utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank) and guide the derivation of SCFG rules with explicit parsing on at least one side of the parallel corpus. Formally syntax-based</context>
<context position="7478" citStr="Och and Ney, 2004" startWordPosition="1151" endWordPosition="1154">(give the pen to me, 钢笔 给 我) (1) X (give X1 to me, X1 给 我) (2) The SCFG rules of hierarchical phrase-based models are extracted automatically from corpora of word-aligned parallel sentence pairs (Brown et al., 1993; Och and Ney, 2000). An aligned sentence pair is a tuple (E, F, A), where E = e1 · · · e,,, can be interpreted as an English sentence of length n, F = f1 · · · fm its translation of length m in a foreign language, and A a set of links between words of the two sentences. Figure 1 (a) shows an example of aligned English-to-Chinese sentence pair. Widely adopted in phrase-based models (Och and Ney, 2004), a pair of consecutive sequences of words from E and F is a phrase pair if all words are aligned only within the sequences and not to any word outside. We call a sequence of words a phrase if it corresponds to either side of a phrase pair, and a non-phrase otherwise. Note that the boundary words of a phrase pair may not be aligned to any other word. We call the phrase pairs with all boundary words aligned tight phrase pairs (Zhang et al., 2008). A tight phrase pair is the minimal phrase pair among all that share the same set of alignment links. Figure 1 (b) highlights the tight phrase pairs i</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28080" citStr="Och, 2003" startWordPosition="4814" endWordPosition="4815">PIN(U.) = X X (V,W)ELR(U) y,z POUT(U.) = X X (V,W)EPL(U) y,z + X X (V,W)EPR(U) y,z P(U. → VyWz|F) = O(Ux → VyWz) = #(Ux → VyWz) P P #(Ux → VyWz) (V&apos;,Wl) y,z 144 corpora are also preprocessed with punctuation removed and words down-cased to make them suitable for speech translation. The baseline system is our implementation of the hierarchical phrase-based model of Chiang (2007), and it includes basic features such as rule and lexicalized rule translation probabilities, language model scores, rule counts, etc. We use 4-gram language models in both tasks, and conduct minimumerror-rate training (Och, 2003) to optimize feature weights on the dev set. Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively. The English side of the parallel data is parsed by our implementation of the Berkeley parser (Huang and Harper, 2009) trained on the combination of Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and a speechified version of the WSJ treebank (Marcus et al., 1999) to achieve higher parsing accuracy (Huang et al., 2010). Our approach introduces a new syntactic feature and its feature weight is tuned in the same </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23098" citStr="Petrov et al. (2006)" startWordPosition="3962" endWordPosition="3965">and then learn a set of rule probabilities9 0 on the latent categories so that the likelihood of the training forests are maximized. The motivation is to let the latent categories learn different preferences of (emitted) syntactic categories as well as structural dependencies along the hierarchy so that they can carry syntactic information. We call them latent syntactic categories. The learned Xi’s represent syntactically-induced finer-grained categories of phrases and are used as the set of latent syntactic categories C described in Section 3. In related research, Matsuzaki et al. (2005) and Petrov et al. (2006) introduced latent variables to learn finergrained distinctions of treebank categories for parsing, and Huang et al. (2009) used a similar approach to learn finer-grained part-of-speech tags for tagging. Our method is in spirit similar to these approaches. Optimization of grammar parameters to maximize the likelihood of training forests can be achieved 8We incrementally split each nonterminal to 2, 4, 8, and finally 16 categories, with each splitting followed by several EM iterations to tune model parameters. We consider 16 an appropriate number for latent categories, not too small to differen</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference grammars: softening syntactic constraints to improve statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="10556" citStr="Venugopal et al. (2009)" startWordPosition="1706" endWordPosition="1709">ries based on automatic parse trees. They introduced an extended set of categories (e.g., NP+V for she went and DT\NP for great wall, an noun phrase with a missing determiner on the left) to annotate phrase pairs that do not align with syntactic constituents. Their hard syntactic constraint requires that the nonterminals should match exactly to rewrite with a rule, which could rule out potentially correct derivations due to errors in the syntactic parses as well as to data sparsity. For example, NP cannot be instantiated with phrase pairs of type DT+NN, in spite of their syntactic similarity. Venugopal et al. (2009) addressed this problem by directly introducing soft syntactic preferences into SCFG rules using preference grammars, but they had to face the computational challenges of large preference vectors. Chiang (2010) also avoided hard constraints and took a soft alternative that directly models the cost of mismatched rule substitutions. This, however, would require a large number of parameters to be tuned on a generally small-sized heldout set, and it could thus suffer from over-tuning. 3 Approach Overview In this work, we take a different approach to introduce linguistic syntax to hierarchical phra</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Preference grammars: softening syntactic constraints to improve statistical machine translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Martha Palmer</author>
</authors>
<title>OntoNotes Release 2.0. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Nianwen Xue, Mitchell Marcus, Ann Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin, and Ann Houston,</location>
<contexts>
<context position="28462" citStr="Weischedel et al., 2008" startWordPosition="4871" endWordPosition="4874">hiang (2007), and it includes basic features such as rule and lexicalized rule translation probabilities, language model scores, rule counts, etc. We use 4-gram language models in both tasks, and conduct minimumerror-rate training (Och, 2003) to optimize feature weights on the dev set. Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively. The English side of the parallel data is parsed by our implementation of the Berkeley parser (Huang and Harper, 2009) trained on the combination of Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and a speechified version of the WSJ treebank (Marcus et al., 1999) to achieve higher parsing accuracy (Huang et al., 2010). Our approach introduces a new syntactic feature and its feature weight is tuned in the same way together with the features in the baseline model. In this study, we induce 16 latent categories for both X and B nonterminals. Our approach identifies —180k unique tag sequences for the English side of phrase pairs in both tasks. As shown by the examples in Table 2, the syntactic feature vector representation is able to identify similar and dissimilar tag sequences. For insta</context>
</contexts>
<marker>Weischedel, Pradhan, Ramshaw, Palmer, 2008</marker>
<rawString>Ralph Weischedel, Sameer Pradhan, Lance Ramshaw, Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin, and Ann Houston, 2008. OntoNotes Release 2.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="2292" citStr="Wu, 1997" startWordPosition="335" endWordPosition="336"> fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized into two classes based on their dependency on annotated corpus (Chiang, 2007). Linguistically syntaxbased models (e.g., (Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006)) utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank) and guide the derivation of SCFG rules with explicit parsing on at least one side of the parallel corpus. Formally syntax-based models (e.g., (Wu, 1997; Chiang, 2007)) extract synchronous grammars from parallel corpora based on the hierarchical structure of natural language pairs without any explicit linguistic knowledge or annotations. In this work, we focus on the hierarchical phrase-based models of Chiang (2007), which is formally syntax-based, and always refer the term SCFG, from now on, to the grammars of this model class. On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the pe</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>A syntax-driven bracketing model for phrase-based translation.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="32757" citStr="Xiong et al., 2009" startWordPosition="5619" endWordPosition="5622">J IN INTJ NP JJ ADJP JJ JJ CC ADJP IN NP JJ ADJP PDT JJ ADJP VB JJ JJ AUX RB ADJP RB JJ ADVP WHNP JJ ADJP VP Table 2: Examples of similar and dissimilar tag sequences. This cost can be reduced, however, by caching the dot-products of the tag sequences that are frequently accessed. There are other successful investigations to impose soft syntactic constraints to hierarchical phrase-based models by either introducing syntaxbased rule features such as the prior derivation model of Zhou et al. (2008) or by imposing constraints on translation spans at decoding time, e.g., (Marton and Resnik, 2008; Xiong et al., 2009; Xiong et al., 2010). These approaches are all orthogonal to ours and it is expected that they can be combined with our approach to achieve greater improvement. This work is an initial effort to investigate latent syntactic categories to enhance hierarchical phrasebased translation models, and there are many directions to continue this line of research. First, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and the SCFG rules used to derive the translation, the real-valued syntactic feature vectors can also be used to impose sof</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2009</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009. A syntax-driven bracketing model for phrase-based translation. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Learning translation boundaries for phrase-based decoding.</title>
<date>2010</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="32778" citStr="Xiong et al., 2010" startWordPosition="5623" endWordPosition="5626"> JJ JJ CC ADJP IN NP JJ ADJP PDT JJ ADJP VB JJ JJ AUX RB ADJP RB JJ ADVP WHNP JJ ADJP VP Table 2: Examples of similar and dissimilar tag sequences. This cost can be reduced, however, by caching the dot-products of the tag sequences that are frequently accessed. There are other successful investigations to impose soft syntactic constraints to hierarchical phrase-based models by either introducing syntaxbased rule features such as the prior derivation model of Zhou et al. (2008) or by imposing constraints on translation spans at decoding time, e.g., (Marton and Resnik, 2008; Xiong et al., 2009; Xiong et al., 2010). These approaches are all orthogonal to ours and it is expected that they can be combined with our approach to achieve greater improvement. This work is an initial effort to investigate latent syntactic categories to enhance hierarchical phrasebased translation models, and there are many directions to continue this line of research. First, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and the SCFG rules used to derive the translation, the real-valued syntactic feature vectors can also be used to impose soft constraints between</context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learning translation boundaries for phrase-based decoding. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2011" citStr="Yamada and Knight, 2001" startWordPosition="287" endWordPosition="290">ent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized into two classes based on their dependency on annotated corpus (Chiang, 2007). Linguistically syntaxbased models (e.g., (Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006)) utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank) and guide the derivation of SCFG rules with explicit parsing on at least one side of the parallel corpus. Formally syntax-based models (e.g., (Wu, 1997; Chiang, 2007)) extract synchronous grammars from parallel corpora based on the hierarchical structure of natural language pairs without any explicit linguistic knowledge or annotations. In this work, we focus on the hierarchical phrase-based models of Chiang (2007), which is formally syntax-based, and always refer t</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting synchronous grammar rules from word-level alignments in linear time.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="7927" citStr="Zhang et al., 2008" startWordPosition="1238" endWordPosition="1241">s between words of the two sentences. Figure 1 (a) shows an example of aligned English-to-Chinese sentence pair. Widely adopted in phrase-based models (Och and Ney, 2004), a pair of consecutive sequences of words from E and F is a phrase pair if all words are aligned only within the sequences and not to any word outside. We call a sequence of words a phrase if it corresponds to either side of a phrase pair, and a non-phrase otherwise. Note that the boundary words of a phrase pair may not be aligned to any other word. We call the phrase pairs with all boundary words aligned tight phrase pairs (Zhang et al., 2008). A tight phrase pair is the minimal phrase pair among all that share the same set of alignment links. Figure 1 (b) highlights the tight phrase pairs in the example sentence pair. 139 6 5 4 3 2 1 1 2 3 4 5 (a) (b) Figure 1: An example of word-aligned sentence pair (a) with tight phrase pairs marked in a matrix representation (b). The extraction of SCFG rules proceeds as follows. In the first step, all phrase pairs below a maximum length are extracted as phrasal rules. In the second step, abstract rules are extracted from tight phrase pairs that contain other tight phrase pairs by replacing the</context>
<context position="16330" citStr="Zhang et al. (2008)" startWordPosition="2719" endWordPosition="2722"> overlap. In the second case, the intersection, differences, and union of the two phrase pairs are 4A normalized uniform feature vector is used for tag sequences (of parsed test sentences) that are not seen on the training corpus. 141 Figure 2: A decomposition tree of tight phrase pairs with all tight phrase pairs listed on the right. As highlighted, the two non-maximal phrase pairs are generated by consecutive sibling nodes. also tight phrase pairs (see Figure 1 (b) for example), and the two phrase pairs, as well as their intersection and differences, are all sub phrase pairs of their union. Zhang et al. (2008) exploited this property to construct a hierarchical decomposition tree (Bui-Xuan et al., 2005) of phrase pairs from a sentence pair to extract all phrase pairs in linear time. In this paper, we focus on learning the syntactic dependencies along the hierarchy of phrase pairs. Our hierarchy construction follows Heber and Stoye (2001). Let P be the set of tight phrase pairs extracted from a sentence pair. We call a sequentially-ordered list5 L = (p1, · · · , pk) of unique phrase pairs pi E P a chain if every two successive phrase pairs in L have a non-trivial overlap. A chain is maximal if it ca</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting synchronous grammar rules from word-level alignments in linear time. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bowen Zhou</author>
<author>Bing Xiang</author>
<author>Xiaodan Zhu</author>
<author>Yuqing Gao</author>
</authors>
<title>Prior derivation models for formally syntax-based translation using linguistically syntactic parsing and tree kernels.</title>
<date>2008</date>
<booktitle>In SSST.</booktitle>
<contexts>
<context position="32640" citStr="Zhou et al. (2008)" startWordPosition="5599" endWordPosition="5602">NN DT NN NN JJ RB NP IN CD VB VP PP JJ NN JJ NN TO VP VP VB RB VB PP VB NN NN VB JJ WHNP DT NN VB DT DT NN VB RB IN JJ IN INTJ NP JJ ADJP JJ JJ CC ADJP IN NP JJ ADJP PDT JJ ADJP VB JJ JJ AUX RB ADJP RB JJ ADVP WHNP JJ ADJP VP Table 2: Examples of similar and dissimilar tag sequences. This cost can be reduced, however, by caching the dot-products of the tag sequences that are frequently accessed. There are other successful investigations to impose soft syntactic constraints to hierarchical phrase-based models by either introducing syntaxbased rule features such as the prior derivation model of Zhou et al. (2008) or by imposing constraints on translation spans at decoding time, e.g., (Marton and Resnik, 2008; Xiong et al., 2009; Xiong et al., 2010). These approaches are all orthogonal to ours and it is expected that they can be combined with our approach to achieve greater improvement. This work is an initial effort to investigate latent syntactic categories to enhance hierarchical phrasebased translation models, and there are many directions to continue this line of research. First, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and th</context>
</contexts>
<marker>Zhou, Xiang, Zhu, Gao, 2008</marker>
<rawString>Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing Gao. 2008. Prior derivation models for formally syntax-based translation using linguistically syntactic parsing and tree kernels. In SSST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In StatMT.</booktitle>
<contexts>
<context position="4271" citStr="Zollmann and Venugopal, 2006" startWordPosition="620" endWordPosition="624">USA, 9-11 October 2010. c�2010 Association for Computational Linguistics icantly outperform hierarchical phrase-based models when using forest-based rule extraction together with forest-based decoding. Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. In this paper, we augment rules in hierarchical phrase-based translation systems with novel syntactic features. Unlike previous studies (e.g., (Zollmann and Venugopal, 2006)) that directly use explicit treebank categories such as NP, NP/PP (NP missing PP from the right) to annotate phrase pairs, we induce a set of latent categories to capture the syntactic dependencies inherent in the hierarchical structure of phrase pairs, and derive a real-valued feature vector for each X nonterminal of a SCFG rule based on the distribution of the latent categories. Moreover, we convert the equality test of two sequences of syntactic categories, which are either identical or different, into the computation of a similarity score between their corresponding feature vectors. In ou</context>
<context position="9851" citStr="Zollmann and Venugopal (2006)" startWordPosition="1588" endWordPosition="1591">d from a phrase pair with I am reading a book on the source side where X1 is abstracted from the noun phrase pair. If this rule is used to translate I am reading the brochure of a book fair, it would be better to apply it over the entire string than over sub-strings such as I ... the brochure of. This is because the nonterminal X1 in the rule was abstracted from a noun phrase on the source side of the training data and would thus be better (more informative) to be applied to phrases of the same type. Hierarchical phrase-based models are not able to distinguish syntactic differences like this. Zollmann and Venugopal (2006) attempted to address this problem by annotating phrase pairs with treebank categories based on automatic parse trees. They introduced an extended set of categories (e.g., NP+V for she went and DT\NP for great wall, an noun phrase with a missing determiner on the left) to annotate phrase pairs that do not align with syntactic constituents. Their hard syntactic constraint requires that the nonterminals should match exactly to rewrite with a rule, which could rule out potentially correct derivations due to errors in the syntactic parses as well as to data sparsity. For example, NP cannot be inst</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In StatMT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>