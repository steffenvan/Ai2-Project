<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003155">
<title confidence="0.964814">
Learning to predict pitch accents and prosodic boundaries in Dutch
</title>
<author confidence="0.9987595">
Erwin Marsi1, Martin Reynaert1, Antal van den Bosch1,
Walter Daelemans2, V´eronique Hoste2
</author>
<affiliation confidence="0.944546666666667">
1 Tilburg University
ILK / Computational Linguistics and AI
Tilburg, The Netherlands
</affiliation>
<email confidence="0.735247">
{e.c.marsi,reynaert,
antal.vdnbosch}@uvt.nl
</email>
<affiliation confidence="0.678755333333333">
2 University of Antwerp,
CNTS
Antwerp, Belgium
</affiliation>
<email confidence="0.996405">
{daelem,hoste}@uia.ua.ac.be
</email>
<sectionHeader confidence="0.998582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993124">
We train a decision tree inducer (CART)
and a memory-based classifier (MBL)
on predicting prosodic pitch accents and
breaks in Dutch text, on the basis of shal-
low, easy-to-compute features. We train
the algorithms on both tasks individu-
ally and on the two tasks simultaneously.
The parameters of both algorithms and
the selection of features are optimized per
task with iterative deepening, an efficient
wrapper procedure that uses progressive
sampling of training data. Results show
a consistent significant advantage of MBL
over CART, and also indicate that task
combination can be done at the cost of
little generalization score loss. Tests on
cross-validated data and on held-out data
yield F-scores of MBL on accent place-
ment of 84 and 87, respectively, and on
breaks of 88 and 91, respectively. Accent
placement is shown to outperform an in-
formed baseline rule; reliably predicting
breaks other than those already indicated
by intra-sentential punctuation, however,
appears to be more challenging.
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999864191489362">
Any text-to-speech (TTS) system that aims at pro-
ducing understandable and natural-sounding out-
put needs to have on-board methods for predict-
ing prosody. Most systems start with generating
a prosodic representation at the linguistic or sym-
bolic level, followed by the actual phonetic real-
ization in terms of (primarily) pitch, pauses, and
segmental durations. The first step involves plac-
ing pitch accents and inserting prosodic boundaries
at the right locations (and may involve tune choice
as well). Pitch accents correspond roughly to pitch
movements that lend emphasis to certain words in
an utterance. Prosodic breaks are audible interrup-
tions in the flow of speech, typically realized by a
combination of a pause, a boundary-marking pitch
movement, and lengthening of the phrase-final seg-
ments. Errors at this level may impede the listener
in the correct understanding of the spoken utterance
(Cutler et al., 1997). Predicting prosody is known to
be a hard problem that is thought to require informa-
tion on syntactic boundaries, syntactic and seman-
tic relations between constituents, discourse-level
knowledge, and phonological well-formedness con-
straints (Hirschberg, 1993). However, producing all
this information – using full parsing, including es-
tablishing semanto-syntactic relations, and full dis-
course analysis – is currently infeasible for a real-
time system. Resolving this dilemma has been the
topic of several studies in pitch accent placement
(Hirschberg, 1993; Black, 1995; Pan and McKe-
own, 1999; Pan and Hirschberg, 2000; Marsi et al.,
2002) and in prosodic boundary placement (Wang
and Hirschberg, 1997; Taylor and Black, 1998). The
commonly adopted solution is to use shallow infor-
mation sources that approximate full syntactic, se-
mantic and discourse information, such as the words
of the text themselves, their part-of-speech tags, or
their information content (in general, or in the text
at hand), since words with a high (semantic) infor-
mation content or load tend to receive pitch accents
(Ladd, 1996).
Within this research paradigm, we investigate
pitch accent and prosodic boundary placement for
Dutch, using an annotated corpus of newspaper text,
and machine learning algorithms to produce classi-
fiers for both tasks. We address two questions that
have been left open thus far in previous work:
</bodyText>
<listItem confidence="0.982957142857143">
1. Is there an advantage in inducing decision trees
for both tasks, or is it better to not abstract from
individual instances and use a memory-based
k-nearest neighbour classifier?
2. Is there an advantage in inducing classifiers for
both tasks individually, or can both tasks be
learned together.
</listItem>
<bodyText confidence="0.999853169014085">
The first question deals with a key difference be-
tween standard decision tree induction and memory-
based classification: how to deal with exceptional
instances. Decision trees, CART (Classification
and Regression Tree) in particular (Breiman et al.,
1984), have been among the first successful machine
learning algorithms applied to predicting pitch ac-
cents and prosodic boundaries for TTS (Hirschberg,
1993; Wang and Hirschberg, 1997). Decision tree
induction finds, through heuristics, a minimally-
sized decision tree that is estimated to generalize
well to unseen data. Its minimality strategy makes
the algorithm reluctant to remember individual out-
lier instances that would take long paths in the tree:
typically, these are discarded. This may work well
when outliers do not reoccur, but as demonstrated
by (Daelemans et al., 1999), exceptions do typically
reoccur in language data. Hence, machine learn-
ing algorithms that retain a memory trace of indi-
vidual instances, like memory-based learning algo-
rithms based on the k-nearest neighbour classifier,
outperform decision tree or rule inducers precisely
for this reason.
Comparing the performance of machine learning
algorithms is not straightforward, and deserves care-
ful methodological consideration. For a fair com-
parison, both algorithms should be objectively and
automatically optimized for the task to be learned.
This point is made by (Daelemans and Hoste, 2002),
who show that, for tasks such as word-sense dis-
ambiguation and part-of-speech tagging, tuning al-
gorithms in terms of feature selection and classifier
parameters gives rise to significant improvements in
performance. In this paper, therefore, we optimize
both CART and MBL individually and per task, us-
ing a heuristic optimization method called iterative
deepening.
The second issue, that of task combination, stems
from the intuition that the two tasks have a lot
in common. For instance, (Hirschberg, 1993) re-
ports that knowledge of the location of breaks facil-
itates accent placement. Although pitch accents and
breaks do not consistently occur at the same posi-
tions, they are to some extent analogous to phrase
chunks and head words in parsing: breaks mark
boundaries of intonational phrases, in which typi-
cally at least one accent is placed. A learner may
thus be able to learn both tasks at the same time.
Apart from the two issues raised, our work is also
practically motivated. Our goal is a good algorithm
for real-time TTS. This is reflected in the type of
features that we use as input. These can be com-
puted in real-time, and are language independent.
We intend to show that this approach goes a long
way towards generating high-quality prosody, cast-
ing doubt on the need for more expensive sentence
and discourse analysis.
The remainder of this paper has the following
structure. In Section 2 we define the task, describe
the data, and the feature generation process which
involves POS tagging, syntactic chunking, and com-
puting several information-theoretic metrics. Fur-
thermore, a brief overview is given of the algorithms
we used (CART and MBL). Section 3 describes the
experimental procedure (ten-fold iterative deepen-
ing) and the evaluation metrics (F-scores). Section 4
reports the results for predicting accents and major
prosodic boundaries with both classifiers. It also re-
ports their performance on held-out data and on two
fully independent test sets. The final section offers
some discussion and concluding remarks.
</bodyText>
<sectionHeader confidence="0.967126" genericHeader="method">
2 Task definition, data, and machine
learners
</sectionHeader>
<bodyText confidence="0.998115862068966">
To explore the generalization abilities of machine
learning algorithms trained on placing pitch accents
and breaks in Dutch text, we define three classifica-
tion tasks:
Pitch accent placement – given a word form in its
sentential context, decide whether it should be
accented. This is a binary classification task.
Break insertion – given a word form in its senten-
tial context, decide whether it should be fol-
lowed by a boundary. This is a binary classi-
fication task.
Combined accent placement and break insertion
– given a word form in its sentential context,
decide whether it should be accented and
whether it should be followed by a break. This
is a four-class task: no accent and no break; an
accent and no break; no accent and a break;
an accent and a break.
Finer-grained classifications could be envisioned,
e.g. predicting the type of pitch accent, but we assert
that finer classification, apart from being arguably
harder to annotate, could be deferred to later pro-
cessing given an adequate level of precision and re-
call on the present task.
In the next subsections we describe which data we
selected for annotation and how we annotated it with
respect to pitch accents and prosodic breaks. We
then describe the implementation of memory-based
learning applied to the task.
</bodyText>
<subsectionHeader confidence="0.990034">
2.1 Prosodic annotation of the data
</subsectionHeader>
<bodyText confidence="0.999985238095238">
The data used in our experiments consists of 201
articles from the ILK corpus (a large collection of
Dutch newspaper text), totalling 4,493 sentences
and 58,097 tokens (excluding punctuation). We set
apart 10 articles, containing 2,905 tokens (excluding
punctuation) as held-out data for testing purposes.
As a preprocessing step, the data was tokenised by
a rule-based Dutch tokeniser, splitting punctuation
from words, and marking sentence endings.
The articles were then prosodically annotated,
without overlap, by four different annotators, and
were corrected in a second stage, again without over-
lap, by two corrector-annotators. The annotators’
task was to indicate the locations of accents and/or
breaks that they preferred. They used a custom an-
notation tool which provided feedback in the form
of synthesized speech. In total, 23,488 accents were
placed, which amounts to roughly one accent in two
and a half words. 8627 breaks were marked; 4601
of these were sentence-internal breaks; the remain-
der consisted of breaks at the end of sentences.
</bodyText>
<subsectionHeader confidence="0.978352">
2.2 Generating shallow features
</subsectionHeader>
<bodyText confidence="0.999335409638554">
The 201 prosodically-annotated articles were subse-
quently processed through the following 15 feature
construction steps, each contributing one feature per
word form token. An excerpt of the annotated data
with all generated symbolic and numeric1 features is
presented in Table 1.
Word forms (Wrd) – The word form tokens form
the central unit to which other features are added.
Pre- and post-punctuation – All punctuation
marks in the data are transferred to two separate fea-
tures: a pre-punctuation feature (PreP) for punctua-
tion marks such as quotation marks appearing before
the token, and a post-punctuation feature (PostP) for
punctuation marks such as periods, commas, and
question marks following the token.
Part-of-speech (POS) tagging – We used MBT
version 1.0 (Daelemans et al., 1996) to develop a
memory-based POS tagger trained on the Eindhoven
corpus of written Dutch, which does not overlap
with our base data. We split up the full POS tags into
two features, the first (PosC) containing the main
POS category, the second (PosF) the POS subfea-
tures.
Diacritical accent – Some tokens bear an ortho-
graphical diacritical accent put there by the author to
particularly emphasize the token in question. These
accents were stripped off the accented letter, and
transferred to a binary feature (DiA).
NP and VP chunking (NpC &amp; VpC) – An ap-
proximation of the syntactic structure is provided by
simple noun phrase and verb phrase chunkers, which
take word and POS information as input and are
based on a small number of manually written reg-
ular expressions. Phrase boundaries are encoded per
word using three tags: ‘B’ for chunk-initial words,
‘I’ for chunk-internal words, and ‘O’ for words out-
side chunks. The NPs are identified according to the
base principle of one semantic head per chunk (non-
recursive, base NPs). VPs include only verbs, not
the verbal complements.
IC – Information content (IC) of a word w is
given by IC(w) = −log(P(w)), where P(w) is esti-
1Numeric features were rounded off to two decimal points,
where appropriate.
mated by the observed frequency of w in a large dis-
joint corpus of about 1.7 GB of unannotated Dutch
text garnered from various sources. Word forms not
in this corpus were given the highest IC score, i.e.
the value for hapax legomenae (words that occur
once).
Bigram IC – IC on bigrams (BIC) was calculated
for the bigrams (pairs of words) in the data, accord-
ing to the same formula and corpus material as for
unigram IC.
TF*IDF – The TF*IDF metric (Salton, 1989) es-
timates the relevance of a word in a document. Doc-
ument frequency counts for all token types were ob-
tained from a subset of the same corpus as used
for IC calculations. TF*IDF and IC (previous two
features) have been succesfully tested as features
for accent prediction by (Pan and McKeown, 1999),
who assert that IC is a more powerful predictor than
TF*IDF.
Phrasometer – The phrasometer feature (PM) is
the summed log-likelihood of all n-grams the word
form occurs in, with n ranging from 1 to 25, and
computed in an iterative growth procedure: log-
likelihoods of n + 1-grams were computed by ex-
panding all stored n-grams one word to the left
and to the right; only the n + 1-grams with higher
log-likelihood than that of the original n-gram are
stored. Computations are based on the complete ILK
Corpus.
Distance to previous occurrence – The distance,
counted in the number of tokens, to previous occur-
rence of a token within the same article (D2P). Un-
seen words were assigned the arbitrary high default
distance of 9999.
Distance to sentence boundaries – Distance of
the current token to the start of the sentence (D2S)
and to the end of the sentence (D2E), both measured
as a proportion of the total sentence length measured
in tokens.
</bodyText>
<subsectionHeader confidence="0.995199">
2.3 CART: Classification and regression trees
</subsectionHeader>
<bodyText confidence="0.9999525">
CART (Breiman et al., 1984) is a statistical method
to induce a classification or regression tree from a
given set of instances. An instance consists of a
fixed-length vector of n feature-value pairs, and an
information field containing the classification of that
particular feature-value vector. Each node in the
CART tree contains a binary test on some categor-
ical or numerical feature in the input vector. In the
case of classification, the leaves contain the most
likely class. The tree building algorithm starts by
selecting the feature test that splits the data in such a
way that the mean impurity (entropy times the num-
ber of instances) of the two partitions is minimal.
The algorithm continues to split each partition recur-
sively until some stop criterion is met (e.g. a mini-
mal number of instances in the partition). Alterna-
tively, a small stop value can be used to build a tree
that is probably overfitted, but is then pruned back
to where it best matches some amount of held-out
data. In our experiments, we used the CART imple-
mentation that is part of the Edinburgh Speech Tools
(Taylor et al., 1999).
</bodyText>
<subsectionHeader confidence="0.914382">
2.4 Memory-based learning
</subsectionHeader>
<bodyText confidence="0.999977583333333">
Memory-based learning (MBL), also known as
instance-based, example-based, or lazy learning
(Stanfill and Waltz, 1986; Aha et al., 1991), is a
supervised inductive learning algorithm for learning
classification tasks. Memory-based learning treats
a set of training instances as points in a multi-
dimensional feature space, and stores them as such
in an instance base in memory (rather than perform-
ing some abstraction over them). After the instance
base is stored, new (test) instances are classified
by matching them to all instances in memory, and
by calculating with each match the distance, given
by a distance function between the new instance
X and the memory instance Y . Cf. (Daelemans
et al., 2002) for details. Classification in memory-
based learning is performed by the k-NN algorithm
(Fix and Hodges, 1951; Cover and Hart, 1967) that
searches for the k ‘nearest neighbours’ according
to the distance function. The majority class of the
k nearest neighbours then determines the class of
the new case. In our k-NN implementation2, equi-
distant neighbours are taken as belonging to the
same k, so this implementation is effectively a k-
nearest distance classifier.
</bodyText>
<sectionHeader confidence="0.979969" genericHeader="method">
3 Optimization by iterative deepening
</sectionHeader>
<bodyText confidence="0.9978875">
Iterative deepening (ID) is a heuristic search algo-
rithm for the optimization of algorithmic parameter
</bodyText>
<footnote confidence="0.9962215">
2All experiments with memory-based learning were per-
formed with TiMBL, version 4.3 (Daelemans et al., 2002).
</footnote>
<equation confidence="0.865579866666667">
Wrd PreP PostP PosC PosF DiA NpC VpC IC BIC Tf*Idf PM D2P D2S D2E A B AB
De = = Art bep,zijdofmv,neut 0 B O 2.11 5.78 0.00 4 9999 0.00 0.94 - - - -
bomen = = N soort,mv,neut 0 I O 4.37 7.38 0.16 4 17 0.06 0.89 A - A-
rondom = = Prep voor 0 O O 4.58 5.09 0.04 4 17 0.11 0.83 - - - -
de = = Art bep,zijdofmv,neut 0 B O 1.31 5.22 0.00 5 20 0.17 0.78 - - - -
molen = = N soort,ev,neut 0 I O 5.00 7.50 0.18 5 9 0.22 0.72 A - A-
bij = = Prep voor 0 O O 2.50 3.04 0.00 6 9999 0.28 0.67 - - - -
de = = Art bep,zijdofmv,neut 0 B O 1.31 6.04 0.00 6 3 0.33 0.61 - - - -
scheepswerf = = N soort,ev,neut 0 I O 5.63 8.02 0.03 4 9999 0.39 0.56 - - - -
Verolme = = N eigen,ev,neut 0 I O 6.38 7.59 0.05 0 9999 0.44 0.50 A - A-
moeten = = V trans,ott,3,ev 0 B O 2.99 6.77 0.01 4 9999 0.61 0.33 - - - -
verkassen = , V trans,inf 0 I O 5.75 5.99 0.02 4 9999 0.67 0.28 A B AB
vindt = = V trans,ott,3,ev 0 O B 3.51 8.50 0.00 6 9999 0.72 0.22 - - - -
molenaar = = N soort,ev,neut 0 B O 5.95 8.50 0.05 0 9999 0.78 0.17 - - - -
Wijbrand = = N eigen,ev,neut 0 I O 7.89 8.50 0.11 0 38 0.83 0.11 A - A-
</equation>
<tableCaption confidence="0.9893975">
Table 1: Symbolic and numerical features and class for the sentence De bomen rondom de scheepswerf Verolme moeten verkassen,
vindt molenaar Wijbrandt. ‘Miller Wijbrand thinks that the trees surrounding the mill near shipyard Verolme have to relocate.’
</tableCaption>
<bodyText confidence="0.999817893333334">
and feature selection, that combines classifier wrap-
ping (using the training material internally to test ex-
perimental variants) (Kohavi and John, 1997) with
progressive sampling of training material (Provost et
al., 1999). We start with a large pool of experiments,
each with a unique combination of input features
and algorithmic parameter settings. In the first step,
each attempted setting is applied to a small amount
of training material and tested on a fixed amount
of held-out data (which is a part of the full train-
ing set). Only the best settings are kept; all others
are removed from the pool of competing settings.
In subsequent iterations, this step is repeated, ex-
ponentially decreasing the number of settings in the
pool, while at the same time exponentially increas-
ing the amount of training material. The idea is that
the increasing amount of time required for training
is compensated by running fewer experiments, in ef-
fect keeping processing time approximately constant
across iterations. This process terminates when only
the single best experiment is left (or, the n best ex-
periments).
This ID procedure can in fact be embedded in a
standard 10-fold cross-validation procedure. In such
a 10-fold CV ID experiment, the ID procedure is car-
ried out on the 90% training partition, and the result-
ing optimal setting is tested on the remaining 10%
test partition. The average score of the 10 optimized
folds can then be considered, as that of a normal 10-
fold CV experiment, to be a good estimation of the
performance of a classifier optimized on the full data
set.
For current purposes, our specific realization of
this general procedure was as follows. We used folds
of approximately equal size. Within each ID ex-
periment, the amount of held-out data was approx-
imately 5%; the initial amount of training data was
5% as well. Eight iterations were performed, dur-
ing which the number of experiments was decreased,
and the amount of training data was increased, so
that in the end only the 3 best experiments used
all available training data (i.e. the remaining 95%).
Increasing the training data set was accomplished
by random sampling from the total of training data
available. Selection of the best experiments was
based on their F-score (van Rijsbergen, 1979) on
the target class (accent or break). F-score, the har-
monic mean of precision and recall, is chosen since
it directly evaluates the tasks (placement of accents
or breaks), in contrast with classification accuracy
(the percentage of correctly classified test instances)
which is biased to the majority class (to place no ac-
cent or break). Moreover, accuracy masks relevant
differences between certain inappropriate classifiers
that do not place accents or breaks, and better clas-
sifiers that do place them, but partly erroneously.
The initial pool of experiments was created by
systematically varying feature selection (the input
features to the classifier) and the classifier set-
tings (the parameters of the classifiers). We re-
stricted these selections and settings within reason-
able bounds to keep our experiments computation-
ally feasible. In particular, feature selection was lim-
ited to varying the size of the window that was used
to model the local context of an instance. A uni-
form window (i.e. the same size for all features) was
applied to all features except DiA, D2P, D2S, and
D2E. Its size (win) could be 1, 3, 5, 7, or 9, where
win = 1 implies no modeling of context, whereas
win = 9 means that during classification not only
the features of the current instance are taken into ac-
count, but also those of the preceding and following
four instances.
For CART, we varied the following parameter val-
ues, resulting in a first ID step with 480 experiments:
</bodyText>
<listItem confidence="0.886341857142857">
• the minimum number of examples for leaf
nodes (stop): 1, 10, 25, 50, and 100
• the number of partitions to split a float feature
range into (frs): 2, 5, 10, and 25
• the percentage of training material held out for
pruning (held-out): 0, 5, 10, 15, 20, and 25 (0
implies no pruning)
</listItem>
<bodyText confidence="0.987340333333333">
For MBL, we varied the following parameter val-
ues, which led to 1184 experiments in the first ID
step:
</bodyText>
<listItem confidence="0.999307333333333">
• the number of nearest neighbours (k): 1, 4, 7,
10, 13, 16, 19, 22, 25, and 28
• the type of feature weighting: Gain Ratio (GR),
and Shared Variance (SV)
• the feature value similarity metric: Overlap,
or Modified Value Difference Metric (MVDM)
with back-off to Overlap at value frequency
tresholds 1 (L=1, no back-off), 2, and 10
• the type of distance weighting: None, Inverse
Distance, Inverse Linear Distance, and Expo-
nential Decay with α = 1.0 (ED1) and α = 4.0
(ED4)
</listItem>
<sectionHeader confidence="0.999911" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999502">
4.1 Tenfold iterative deepening results
</subsectionHeader>
<bodyText confidence="0.995986384615385">
We first determined two sharp, informed baselines;
see Table 2. The informed baseline for accent place-
ment is based on the content versus function word
distinction, commonly employed in TTS systems
(Taylor and Black, 1998). We refer to this baseline
as CF-rule. It is constructed by accenting all content
words, while leaving all function words (determin-
ers, prepositions, conjunctions/complementisers and
auxiliaries) unaccented. The required word class in-
formation is obtained from the POS tags. The base-
line for break placement, henceforth PUNC-rule, re-
lies solely on punctuation. A break is inserted after
any sequence of punctuation symbols containing one
</bodyText>
<table confidence="0.996363363636364">
Target: Method: Prec : Rec : F :
Accent CF-rule 66.7 94.9 78.3
CART 78.6 ±2.8 85.7 ±1.1 82.0 ±1.7
MBL 80.0 ±2.7 86.6 ±1.4 83.6 ±1.6*
CART° 78.7 ±3.0 85.6 ±0.8 82.0 ±1.6
MBL° 81.0 ±2.7 86.1 ±1.1 83.4 ±1.5*
Break PUNC-rule 99.2 75.7 85.9
CART 93.1 ±1.5 82.2 ±3.0 87.3 ±1.5
MBL 95.1 ±1.4 81.9 ±2.8 88.0 ±1.5*
CART° 94.5 ±0.8 80.2 ±3.1 86.7 ±1.6
MBL° 95.7 ±1.1 80.7 ±3.1 87.6 ±1.7*
</table>
<tableCaption confidence="0.913496666666667">
Table 2: Precision, recall, and F-scores on accent, break
and combined prediction by means of CART and MBL, for
baselines and for average results over 10 folds of the Iterative
Deepening experiment; a * indicates a significant difference
(p &lt; 0.01) between CART and MBL according to a paired
t-test. Superscript ° refers to the combined task.
</tableCaption>
<bodyText confidence="0.99925653125">
or more characters from the set {,!?:;()}. It should
be noted that both baselines are simple rule-based
algorithms that have been manually optimized for
the current training set. They perform well above
chance level, and pose a serious challenge to any ML
approach.
From the results displayed in Table 2, the follow-
ing can be concluded. First, MBL attains the highest
F-scores on accent placement, 83.6, and break place-
ment, 88.0. It does so when trained on the ACCENT
and BREAK tasks individually. On these tasks, MBL
performs significantly better than CART (paired t-
tests yield p &lt; 0.01 for both differences).
Second, the performances of MBL and CART on
the combined task, when split in F-scores on accent
and break placement, are rather close to those on the
accent and break tasks. For both MBL and CART,
the scores on accent placement as part of the com-
bined task versus accent placement in isolation are
not significantly different. For break insertion, how-
ever, a small but significant drop in performance can
be seen with MBL (p &lt; 0.05) and CART (p &lt; 0.01)
when it is performed as part of the COMBINED task.
As is to be expected, the optimal feature selec-
tions and classifier settings obtained by iterative
deepening turned out to vary over the ten folds for
both MBL and CART. Table 3 lists the settings pro-
ducing the best F-score on accents or breaks. A win-
dow of 7 (i.e. the features of the three preceding and
following word form tokens) is used by CART and
MBL for accent placement, and also for break in-
sertion by CART, whereas MBL uses a window of
</bodyText>
<note confidence="0.570614">
Target: Method: Setting:
</note>
<table confidence="0.99802925">
Accent CART win=7, stop=50, frs=5, held-out=5
MBL win=7, MVDM with L=5, k=25, GR, ED4
Break CART win=7, stop=25, frs=2, held-out=5
MBL win=3, MVDM with L=2, k=28, GR, ED4
</table>
<tableCaption confidence="0.9599065">
Table 3: Optimal parameter settings for CART and MBL with
respect to accent and break prediction
</tableCaption>
<bodyText confidence="0.998431625">
just 3. Both algorithms (stop in CART, and k in
MBL) base classifications on minimally around 25
instances. Furthermore, MBL uses the Gain Ratio
feature weighting and Exponential Decay distance
weighting. Although no pruning was part of the Iter-
ative Deepening experiment, CART prefers to hold
out 5% of its training material to prune the decision
tree resulting from the remaining 95%.
</bodyText>
<subsectionHeader confidence="0.981845">
4.2 External validation
</subsectionHeader>
<bodyText confidence="0.999937866666667">
We tested our optimized approach on our held-out
data of 10 articles (2,905 tokens), and on an indepen-
dent test corpus (van Herwijnen and Terken, 2001).
The latter contains two types of text: 2 newspaper
texts (55 sentences, 786 words excluding punctua-
tion), and 17 email messages (70 sentences, 1133
words excluding punctuation). This material was an-
notated by 10 experts, who were asked to indicate
the preferred accents and breaks. For the purpose
of evaluation, words were assumed to be accented if
they received an accent by at least 7 of the annota-
tors. Furthermore, of the original four break levels
annotated (i.e. no break, light, medium, or heavy ),
only medium and heavy level breaks were consid-
ered to be a break in our evaluation. Table 4 lists the
precision, recall, and F-scores obtained on the two
tasks using the single-best scoring setting from the
10-fold CV ID experiment per task. It can be seen
that both CART and MBL outperformed the CF-rule
baseline on our own held-out data and on the news
and email texts, with similar margins as observed in
our 10-fold CV ID experiment. MBL attains an F-
score of 86.6 on accents, and 91.0 on breaks; both
are improvements over the cross-validation estima-
tions. On breaks, however, both CART and MBL
failed to improve on the PUNC-rule baseline; on the
news and email texts they perform even worse. In-
specting MBLs output on these text, it turned out
that MBL does emulate the PUNC-rule baseline,
but that it places additional breaks at positions not
</bodyText>
<table confidence="0.999234052631579">
Target: Test set Method: Prec : Rec : F :
Accent Held-out CF-rule 73.5 94.8 82.8
CART 84.3 86.1 85.2
MBL 87.0 86.3 86.6
News CF-rule 52.2 92.9 66.9
CART 62.7 92.5 74.6
MBL 66.3 89.2 76.0
Email CF-rule 54.3 91.0 68.0
CART 66.8 88.5 76.1
MBL 71.0 88.5 78.8
Break Held-out PUNC-rule 99.5 83.7 90.9
CART 92.6 88.9 90.7
MBL 95.5 87.0 91.0
News PUNC-rule 98.8 93.1 95.9
CART 80.6 95.4 87.4
MBL 89.3 95.4 92.2
Email PUNC-rule 93.9 87.0 90.3
CART 81.6 90.2 85.7
MBL 83.0 91.1 86.8
</table>
<tableCaption confidence="0.784612">
Table 4: Precision, recall, and F-scores on accent and break
prediction for our held-out corpus and two external corpora of
news and email texts, using the best settings for CART and
MBL as determined by the ID experiments.
</tableCaption>
<bodyText confidence="0.9998476">
marked by punctuation. A considerable portion of
these non-punctuation breaks is placed incorrectly –
or at least different from what the annotators pre-
ferred – resulting in a lower precision that does not
outweigh the higher recall.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999993822222222">
With shallow features as input, we trained machine
learning algorithms on predicting the placement of
pitch accents and prosodic breaks in Dutch text,
a desirable function for a TTS system to produce
synthetic speech with good prosody. Both algo-
rithms, the memory-based classifier MBL and de-
cision tree inducer CART, were automatically opti-
mized by an Iterative Deepening procedure, a classi-
fier wrapper technique with progressive sampling of
training data. It was shown that MBL significantly
outperforms CART on both tasks, as well as on the
combined task (predicting accents and breaks simul-
taneously). This again provides an indication that
it is advantageous to retain individual instances in
memory (MBL) rather than to discard outlier cases
as noise (CART).
Training on both tasks simultaneously, in one
model rather than divided over two, results in
generalization accuracies similar to that of the
individually-learned models (identical on accent
placement, and slightly lower for break placement).
This shows that learning one task does not seriously
hinder learning the other. From a practical point of
view, it means that a TTS developer can resort to one
system for both tasks instead of two.
Pitch accent placement can be learned from shal-
low input features with fair accuracy. Break in-
sertion seems a harder task, certainly in view of
the informed punctuation baseline PUNC-rule. Es-
pecially the precision of the insertion of breaks at
other points than those already indicated by com-
mas and other ‘pseudo-prosodic’ orthographic mark
up is hard. This may be due to the lack of crucial
information in the shallow features, to inherent lim-
itations of the ML algorithms, but may as well point
to a certain amount of optionality or personal pref-
erence, which puts an upper bound on what can be
achieved in break prediction (Koehn et al., 2000).
We plan to integrate the placement of pitch ac-
cents and breaks in a TTS system for Dutch, which
will enable the closed-loop annotation of more data
using the TTS itself and on-line (active) learning.
Moreover, we plan to investigate the perceptual
cost of false insertions and deletions of accents and
breaks in experiments with human listeners.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996608">
Our thanks go out to Olga van Herwijnen and Jacques Terken
for the use of their TTS evaluation corpus. All research in
this paper was funded by the Flemish-Dutch Committee (VNC)
of the National Foundations for Research in the Netherlands
(NWO) and Belgium (FWO).
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849493975904">
D. W. Aha, D. Kibler, and M. Albert. 1991. Instance-based
learning algorithms. Machine Learning, 6:37–66.
A.W. Black. 1995. Comparison of algorithms for predicting
pitch accent placement in English speech synthesis. In Pro-
ceedings of the Spring Meeting of the Acoustical Society of
Japan.
L. Breiman, J. Friedman, R. Ohlsen, and C. Stone. 1984.
Classification and regression trees. Wadsworth International
Group, Belmont, CA.
C.J. van Rijsbergen. 1979. Information Retrieval. Butter-
sworth, London.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor pattern
classification. Institute of Electrical and Electronics Engi-
neers Transactions on Information Theory, 13:21–27.
A. Cutler, D. Dahan, and W.A. Van Donselaar. 1997. Prosody
in the comprehension of spoken language: A literature re-
view. Language and Speech, 40(2):141–202.
W. Daelemans and V. Hoste. 2002. Evaluation of machine
learning methods for natural language processing tasks. In
Proceedings of LREC-2002, the third International Confer-
ence on Language Resources and Evaluation, pages 755–
760.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT:
A memory-based part of speech tagger generator. In E. Ejer-
hed and I. Dagan, editors, Proc. of Fourth Workshop on Very
Large Corpora, pages 14–27. ACL SIGDAT.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. For-
getting exceptions is harmful in language learning. Ma-
chine Learning, Special issue on Natural Language Learn-
ing, 34:11–41.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2002. TiMBL: Tilburg Memory
Based Learner, version 4.3, reference guide. Technical
Report ILK-0210, ILK, Tilburg University.
E. Fix and J. L. Hodges. 1951. Discriminatory analysis—
nonparametric discrimination; consistency properties. Tech-
nical Report Project 21-49-004, Report No. 4, USAF School
of Aviation Medicine.
J. Hirschberg. 1993. Pitch accent in context: Predicting intona-
tional prominence from text. Artificial Intelligence, 63:305–
340.
P. Koehn, S. Abney, J. Hirschberg, and M. Collins. 2000. Im-
proving intonational phrasing with syntactic information. In
ICASSP, pages 1289–1290.
R. Kohavi and G. John. 1997. Wrappers for feature subset
selection. Artificial Intelligence Journal, 97(1–2):273–324.
D. R. Ladd. 1996. Intonational phonology. Cambridge Uni-
versity Press.
E. Marsi, G.J. Busser, W. Daelemans, V. Hoste, M. Reynaert,
and A. van den Bosch. 2002. Combining information
sources for memory-based pitch accent placement. In Pro-
ceedings of the International Conference on Spoken Lan-
guage Processing, ICSLP-2002, pages 1273–1276.
S. Pan and J. Hirschberg. 2000. Modeling local context for
pitch accent prediction. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguistics,
Hong Kong.
S. Pan and K. McKeown. 1999. Word informativeness
and automatic pitch accent modeling. In Proceedings of
EMNLP/VLC’99, New Brunswick, NJ, USA. ACL.
F. Provost, D. Jensen, and T. Oates. 1999. Efficient progressive
sampling. In Proceedings of the Fifth International Con-
ference on Knowledge Discovery and Data Mining, pages
23–32.
G. Salton. 1989. Automatic text processing: The transfor-
mation, analysis, and retrieval of information by computer.
Addison–Wesley, Reading, MA, USA.
C. Stanfill and D. Waltz. 1986. Toward memory-based reason-
ing. Communications of the ACM, 29(12):1213–1228, De-
cember.
P. Taylor and A. Black. 1998. Assigning phrase breaks from
part-of-speech sequences. Computer Speech and Language,
12:99–117.
P. Taylor, R. Caley, A. W. Black, and S. King, 1999. Edin-
burgh Speech Tools Library, System Documentation Edition
1.2. CSTR, University of Edinburgh.
O. van Herwijnen and J. Terken. 2001. Evaluation ofpros-3 for
the assignment of prosodic structure, compared to assign-
ment by human experts. In Proceedings Eurospeech 2001
Scandinavia, Vol.1, pages 529–532.
M. Q. Wang and J. Hirschberg. 1997. Automatic classification
of intonational phrasing boundaries. Computer Speech and
Language, 6(2):175–196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.176928">
<title confidence="0.999647">Learning to predict pitch accents and prosodic boundaries in Dutch</title>
<author confidence="0.7890575">Martin Antal van_den_V´eronique</author>
<affiliation confidence="0.8574035">1Tilburg University ILK / Computational Linguistics and AI</affiliation>
<address confidence="0.918992">Tilburg, The Netherlands</address>
<email confidence="0.9299">{e.c.marsi,reynaert,</email>
<affiliation confidence="0.82392">2University of Antwerp, CNTS</affiliation>
<address confidence="0.898283">Antwerp, Belgium</address>
<abstract confidence="0.991817653846154">We train a decision tree inducer (CART) and a memory-based classifier (MBL) on predicting prosodic pitch accents and breaks in Dutch text, on the basis of shallow, easy-to-compute features. We train the algorithms on both tasks individually and on the two tasks simultaneously. The parameters of both algorithms and the selection of features are optimized per task with iterative deepening, an efficient wrapper procedure that uses progressive sampling of training data. Results show a consistent significant advantage of MBL over CART, and also indicate that task combination can be done at the cost of little generalization score loss. Tests on cross-validated data and on held-out data yield F-scores of MBL on accent placement of 84 and 87, respectively, and on breaks of 88 and 91, respectively. Accent placement is shown to outperform an informed baseline rule; reliably predicting breaks other than those already indicated by intra-sentential punctuation, however, appears to be more challenging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="14999" citStr="Aha et al., 1991" startWordPosition="2395" endWordPosition="2398">titions is minimal. The algorithm continues to split each partition recursively until some stop criterion is met (e.g. a minimal number of instances in the partition). Alternatively, a small stop value can be used to build a tree that is probably overfitted, but is then pruned back to where it best matches some amount of held-out data. In our experiments, we used the CART implementation that is part of the Edinburgh Speech Tools (Taylor et al., 1999). 2.4 Memory-based learning Memory-based learning (MBL), also known as instance-based, example-based, or lazy learning (Stanfill and Waltz, 1986; Aha et al., 1991), is a supervised inductive learning algorithm for learning classification tasks. Memory-based learning treats a set of training instances as points in a multidimensional feature space, and stores them as such in an instance base in memory (rather than performing some abstraction over them). After the instance base is stored, new (test) instances are classified by matching them to all instances in memory, and by calculating with each match the distance, given by a distance function between the new instance X and the memory instance Y . Cf. (Daelemans et al., 2002) for details. Classification i</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. W. Aha, D. Kibler, and M. Albert. 1991. Instance-based learning algorithms. Machine Learning, 6:37–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A W Black</author>
</authors>
<title>Comparison of algorithms for predicting pitch accent placement in English speech synthesis.</title>
<date>1995</date>
<booktitle>In Proceedings of the Spring Meeting of the Acoustical Society of Japan.</booktitle>
<contexts>
<context position="2879" citStr="Black, 1995" startWordPosition="426" endWordPosition="427">he spoken utterance (Cutler et al., 1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent an</context>
</contexts>
<marker>Black, 1995</marker>
<rawString>A.W. Black. 1995. Comparison of algorithms for predicting pitch accent placement in English speech synthesis. In Proceedings of the Spring Meeting of the Acoustical Society of Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J Friedman</author>
<author>R Ohlsen</author>
<author>C Stone</author>
</authors>
<title>Classification and regression trees.</title>
<date>1984</date>
<publisher>Wadsworth International Group,</publisher>
<location>Belmont, CA.</location>
<contexts>
<context position="4265" citStr="Breiman et al., 1984" startWordPosition="642" endWordPosition="645">ss two questions that have been left open thus far in previous work: 1. Is there an advantage in inducing decision trees for both tasks, or is it better to not abstract from individual instances and use a memory-based k-nearest neighbour classifier? 2. Is there an advantage in inducing classifiers for both tasks individually, or can both tasks be learned together. The first question deals with a key difference between standard decision tree induction and memorybased classification: how to deal with exceptional instances. Decision trees, CART (Classification and Regression Tree) in particular (Breiman et al., 1984), have been among the first successful machine learning algorithms applied to predicting pitch accents and prosodic boundaries for TTS (Hirschberg, 1993; Wang and Hirschberg, 1997). Decision tree induction finds, through heuristics, a minimallysized decision tree that is estimated to generalize well to unseen data. Its minimality strategy makes the algorithm reluctant to remember individual outlier instances that would take long paths in the tree: typically, these are discarded. This may work well when outliers do not reoccur, but as demonstrated by (Daelemans et al., 1999), exceptions do typi</context>
<context position="13750" citStr="Breiman et al., 1984" startWordPosition="2187" endWordPosition="2190">er log-likelihood than that of the original n-gram are stored. Computations are based on the complete ILK Corpus. Distance to previous occurrence – The distance, counted in the number of tokens, to previous occurrence of a token within the same article (D2P). Unseen words were assigned the arbitrary high default distance of 9999. Distance to sentence boundaries – Distance of the current token to the start of the sentence (D2S) and to the end of the sentence (D2E), both measured as a proportion of the total sentence length measured in tokens. 2.3 CART: Classification and regression trees CART (Breiman et al., 1984) is a statistical method to induce a classification or regression tree from a given set of instances. An instance consists of a fixed-length vector of n feature-value pairs, and an information field containing the classification of that particular feature-value vector. Each node in the CART tree contains a binary test on some categorical or numerical feature in the input vector. In the case of classification, the leaves contain the most likely class. The tree building algorithm starts by selecting the feature test that splits the data in such a way that the mean impurity (entropy times the num</context>
</contexts>
<marker>Breiman, Friedman, Ohlsen, Stone, 1984</marker>
<rawString>L. Breiman, J. Friedman, R. Ohlsen, and C. Stone. 1984. Classification and regression trees. Wadsworth International Group, Belmont, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>Buttersworth, London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>C.J. van Rijsbergen. 1979. Information Retrieval. Buttersworth, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>P E Hart</author>
</authors>
<title>Nearest neighbor pattern classification.</title>
<date>1967</date>
<booktitle>Institute of Electrical and Electronics Engineers Transactions on Information Theory,</booktitle>
<pages>13--21</pages>
<contexts>
<context position="15701" citStr="Cover and Hart, 1967" startWordPosition="2509" endWordPosition="2512"> Memory-based learning treats a set of training instances as points in a multidimensional feature space, and stores them as such in an instance base in memory (rather than performing some abstraction over them). After the instance base is stored, new (test) instances are classified by matching them to all instances in memory, and by calculating with each match the distance, given by a distance function between the new instance X and the memory instance Y . Cf. (Daelemans et al., 2002) for details. Classification in memorybased learning is performed by the k-NN algorithm (Fix and Hodges, 1951; Cover and Hart, 1967) that searches for the k ‘nearest neighbours’ according to the distance function. The majority class of the k nearest neighbours then determines the class of the new case. In our k-NN implementation2, equidistant neighbours are taken as belonging to the same k, so this implementation is effectively a knearest distance classifier. 3 Optimization by iterative deepening Iterative deepening (ID) is a heuristic search algorithm for the optimization of algorithmic parameter 2All experiments with memory-based learning were performed with TiMBL, version 4.3 (Daelemans et al., 2002). Wrd PreP PostP Pos</context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>T. M. Cover and P. E. Hart. 1967. Nearest neighbor pattern classification. Institute of Electrical and Electronics Engineers Transactions on Information Theory, 13:21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cutler</author>
<author>D Dahan</author>
<author>W A Van Donselaar</author>
</authors>
<title>Prosody in the comprehension of spoken language: A literature review.</title>
<date>1997</date>
<journal>Language and Speech,</journal>
<volume>40</volume>
<issue>2</issue>
<marker>Cutler, Dahan, Van Donselaar, 1997</marker>
<rawString>A. Cutler, D. Dahan, and W.A. Van Donselaar. 1997. Prosody in the comprehension of spoken language: A literature review. Language and Speech, 40(2):141–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>V Hoste</author>
</authors>
<title>Evaluation of machine learning methods for natural language processing tasks.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC-2002, the third International Conference on Language Resources and Evaluation,</booktitle>
<pages>755--760</pages>
<contexts>
<context position="5434" citStr="Daelemans and Hoste, 2002" startWordPosition="816" endWordPosition="819">onstrated by (Daelemans et al., 1999), exceptions do typically reoccur in language data. Hence, machine learning algorithms that retain a memory trace of individual instances, like memory-based learning algorithms based on the k-nearest neighbour classifier, outperform decision tree or rule inducers precisely for this reason. Comparing the performance of machine learning algorithms is not straightforward, and deserves careful methodological consideration. For a fair comparison, both algorithms should be objectively and automatically optimized for the task to be learned. This point is made by (Daelemans and Hoste, 2002), who show that, for tasks such as word-sense disambiguation and part-of-speech tagging, tuning algorithms in terms of feature selection and classifier parameters gives rise to significant improvements in performance. In this paper, therefore, we optimize both CART and MBL individually and per task, using a heuristic optimization method called iterative deepening. The second issue, that of task combination, stems from the intuition that the two tasks have a lot in common. For instance, (Hirschberg, 1993) reports that knowledge of the location of breaks facilitates accent placement. Although pi</context>
</contexts>
<marker>Daelemans, Hoste, 2002</marker>
<rawString>W. Daelemans and V. Hoste. 2002. Evaluation of machine learning methods for natural language processing tasks. In Proceedings of LREC-2002, the third International Conference on Language Resources and Evaluation, pages 755– 760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>Proc. of Fourth Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<editor>In E. Ejerhed and I. Dagan, editors,</editor>
<publisher>ACL SIGDAT.</publisher>
<contexts>
<context position="10719" citStr="Daelemans et al., 1996" startWordPosition="1661" endWordPosition="1664">. An excerpt of the annotated data with all generated symbolic and numeric1 features is presented in Table 1. Word forms (Wrd) – The word form tokens form the central unit to which other features are added. Pre- and post-punctuation – All punctuation marks in the data are transferred to two separate features: a pre-punctuation feature (PreP) for punctuation marks such as quotation marks appearing before the token, and a post-punctuation feature (PostP) for punctuation marks such as periods, commas, and question marks following the token. Part-of-speech (POS) tagging – We used MBT version 1.0 (Daelemans et al., 1996) to develop a memory-based POS tagger trained on the Eindhoven corpus of written Dutch, which does not overlap with our base data. We split up the full POS tags into two features, the first (PosC) containing the main POS category, the second (PosF) the POS subfeatures. Diacritical accent – Some tokens bear an orthographical diacritical accent put there by the author to particularly emphasize the token in question. These accents were stripped off the accented letter, and transferred to a binary feature (DiA). NP and VP chunking (NpC &amp; VpC) – An approximation of the syntactic structure is provid</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT: A memory-based part of speech tagger generator. In E. Ejerhed and I. Dagan, editors, Proc. of Fourth Workshop on Very Large Corpora, pages 14–27. ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning, Special issue on Natural Language Learning,</booktitle>
<pages>34--11</pages>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, Special issue on Natural Language Learning, 34:11–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 4.3, reference guide.</title>
<date>2002</date>
<tech>Technical Report ILK-0210,</tech>
<institution>ILK, Tilburg University.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2002</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2002. TiMBL: Tilburg Memory Based Learner, version 4.3, reference guide. Technical Report ILK-0210, ILK, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fix</author>
<author>J L Hodges</author>
</authors>
<title>Discriminatory analysis— nonparametric discrimination; consistency properties.</title>
<date>1951</date>
<tech>Technical Report Project 21-49-004, Report No. 4,</tech>
<institution>USAF School of Aviation Medicine.</institution>
<contexts>
<context position="15678" citStr="Fix and Hodges, 1951" startWordPosition="2505" endWordPosition="2508"> classification tasks. Memory-based learning treats a set of training instances as points in a multidimensional feature space, and stores them as such in an instance base in memory (rather than performing some abstraction over them). After the instance base is stored, new (test) instances are classified by matching them to all instances in memory, and by calculating with each match the distance, given by a distance function between the new instance X and the memory instance Y . Cf. (Daelemans et al., 2002) for details. Classification in memorybased learning is performed by the k-NN algorithm (Fix and Hodges, 1951; Cover and Hart, 1967) that searches for the k ‘nearest neighbours’ according to the distance function. The majority class of the k nearest neighbours then determines the class of the new case. In our k-NN implementation2, equidistant neighbours are taken as belonging to the same k, so this implementation is effectively a knearest distance classifier. 3 Optimization by iterative deepening Iterative deepening (ID) is a heuristic search algorithm for the optimization of algorithmic parameter 2All experiments with memory-based learning were performed with TiMBL, version 4.3 (Daelemans et al., 20</context>
</contexts>
<marker>Fix, Hodges, 1951</marker>
<rawString>E. Fix and J. L. Hodges. 1951. Discriminatory analysis— nonparametric discrimination; consistency properties. Technical Report Project 21-49-004, Report No. 4, USAF School of Aviation Medicine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
</authors>
<title>Pitch accent in context: Predicting intonational prominence from text.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<volume>63</volume>
<pages>340</pages>
<contexts>
<context position="2569" citStr="Hirschberg, 1993" startWordPosition="380" endWordPosition="381"> emphasis to certain words in an utterance. Prosodic breaks are audible interruptions in the flow of speech, typically realized by a combination of a pause, a boundary-marking pitch movement, and lengthening of the phrase-final segments. Errors at this level may impede the listener in the correct understanding of the spoken utterance (Cutler et al., 1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse informa</context>
<context position="4417" citStr="Hirschberg, 1993" startWordPosition="666" endWordPosition="667">ot abstract from individual instances and use a memory-based k-nearest neighbour classifier? 2. Is there an advantage in inducing classifiers for both tasks individually, or can both tasks be learned together. The first question deals with a key difference between standard decision tree induction and memorybased classification: how to deal with exceptional instances. Decision trees, CART (Classification and Regression Tree) in particular (Breiman et al., 1984), have been among the first successful machine learning algorithms applied to predicting pitch accents and prosodic boundaries for TTS (Hirschberg, 1993; Wang and Hirschberg, 1997). Decision tree induction finds, through heuristics, a minimallysized decision tree that is estimated to generalize well to unseen data. Its minimality strategy makes the algorithm reluctant to remember individual outlier instances that would take long paths in the tree: typically, these are discarded. This may work well when outliers do not reoccur, but as demonstrated by (Daelemans et al., 1999), exceptions do typically reoccur in language data. Hence, machine learning algorithms that retain a memory trace of individual instances, like memory-based learning algori</context>
<context position="5943" citStr="Hirschberg, 1993" startWordPosition="896" endWordPosition="897">ly and automatically optimized for the task to be learned. This point is made by (Daelemans and Hoste, 2002), who show that, for tasks such as word-sense disambiguation and part-of-speech tagging, tuning algorithms in terms of feature selection and classifier parameters gives rise to significant improvements in performance. In this paper, therefore, we optimize both CART and MBL individually and per task, using a heuristic optimization method called iterative deepening. The second issue, that of task combination, stems from the intuition that the two tasks have a lot in common. For instance, (Hirschberg, 1993) reports that knowledge of the location of breaks facilitates accent placement. Although pitch accents and breaks do not consistently occur at the same positions, they are to some extent analogous to phrase chunks and head words in parsing: breaks mark boundaries of intonational phrases, in which typically at least one accent is placed. A learner may thus be able to learn both tasks at the same time. Apart from the two issues raised, our work is also practically motivated. Our goal is a good algorithm for real-time TTS. This is reflected in the type of features that we use as input. These can </context>
</contexts>
<marker>Hirschberg, 1993</marker>
<rawString>J. Hirschberg. 1993. Pitch accent in context: Predicting intonational prominence from text. Artificial Intelligence, 63:305– 340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>S Abney</author>
<author>J Hirschberg</author>
<author>M Collins</author>
</authors>
<title>Improving intonational phrasing with syntactic information.</title>
<date>2000</date>
<booktitle>In ICASSP,</booktitle>
<pages>1289--1290</pages>
<contexts>
<context position="30211" citStr="Koehn et al., 2000" startWordPosition="5049" endWordPosition="5052">e learned from shallow input features with fair accuracy. Break insertion seems a harder task, certainly in view of the informed punctuation baseline PUNC-rule. Especially the precision of the insertion of breaks at other points than those already indicated by commas and other ‘pseudo-prosodic’ orthographic mark up is hard. This may be due to the lack of crucial information in the shallow features, to inherent limitations of the ML algorithms, but may as well point to a certain amount of optionality or personal preference, which puts an upper bound on what can be achieved in break prediction (Koehn et al., 2000). We plan to integrate the placement of pitch accents and breaks in a TTS system for Dutch, which will enable the closed-loop annotation of more data using the TTS itself and on-line (active) learning. Moreover, we plan to investigate the perceptual cost of false insertions and deletions of accents and breaks in experiments with human listeners. Acknowledgements Our thanks go out to Olga van Herwijnen and Jacques Terken for the use of their TTS evaluation corpus. All research in this paper was funded by the Flemish-Dutch Committee (VNC) of the National Foundations for Research in the Netherlan</context>
</contexts>
<marker>Koehn, Abney, Hirschberg, Collins, 2000</marker>
<rawString>P. Koehn, S. Abney, J. Hirschberg, and M. Collins. 2000. Improving intonational phrasing with syntactic information. In ICASSP, pages 1289–1290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kohavi</author>
<author>G John</author>
</authors>
<title>Wrappers for feature subset selection.</title>
<date>1997</date>
<journal>Artificial Intelligence Journal,</journal>
<pages>97--1</pages>
<contexts>
<context position="17753" citStr="Kohavi and John, 1997" startWordPosition="2936" endWordPosition="2939"> vindt = = V trans,ott,3,ev 0 O B 3.51 8.50 0.00 6 9999 0.72 0.22 - - - - molenaar = = N soort,ev,neut 0 B O 5.95 8.50 0.05 0 9999 0.78 0.17 - - - - Wijbrand = = N eigen,ev,neut 0 I O 7.89 8.50 0.11 0 38 0.83 0.11 A - ATable 1: Symbolic and numerical features and class for the sentence De bomen rondom de scheepswerf Verolme moeten verkassen, vindt molenaar Wijbrandt. ‘Miller Wijbrand thinks that the trees surrounding the mill near shipyard Verolme have to relocate.’ and feature selection, that combines classifier wrapping (using the training material internally to test experimental variants) (Kohavi and John, 1997) with progressive sampling of training material (Provost et al., 1999). We start with a large pool of experiments, each with a unique combination of input features and algorithmic parameter settings. In the first step, each attempted setting is applied to a small amount of training material and tested on a fixed amount of held-out data (which is a part of the full training set). Only the best settings are kept; all others are removed from the pool of competing settings. In subsequent iterations, this step is repeated, exponentially decreasing the number of settings in the pool, while at the sa</context>
</contexts>
<marker>Kohavi, John, 1997</marker>
<rawString>R. Kohavi and G. John. 1997. Wrappers for feature subset selection. Artificial Intelligence Journal, 97(1–2):273–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Ladd</author>
</authors>
<title>Intonational phonology.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3416" citStr="Ladd, 1996" startWordPosition="513" endWordPosition="514">f several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an annotated corpus of newspaper text, and machine learning algorithms to produce classifiers for both tasks. We address two questions that have been left open thus far in previous work: 1. Is there an advantage in inducing decision trees for both tasks, or is it better to not abstract from individual instances and use a memory-based k-nearest neighbour classifier? 2. Is there an advantage in inducing classifiers for both tasks individually, or can both tasks be learned together. The f</context>
</contexts>
<marker>Ladd, 1996</marker>
<rawString>D. R. Ladd. 1996. Intonational phonology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsi</author>
<author>G J Busser</author>
<author>W Daelemans</author>
<author>V Hoste</author>
<author>M Reynaert</author>
<author>A van den Bosch</author>
</authors>
<title>Combining information sources for memory-based pitch accent placement.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing, ICSLP-2002,</booktitle>
<pages>1273--1276</pages>
<marker>Marsi, Busser, Daelemans, Hoste, Reynaert, van den Bosch, 2002</marker>
<rawString>E. Marsi, G.J. Busser, W. Daelemans, V. Hoste, M. Reynaert, and A. van den Bosch. 2002. Combining information sources for memory-based pitch accent placement. In Proceedings of the International Conference on Spoken Language Processing, ICSLP-2002, pages 1273–1276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>J Hirschberg</author>
</authors>
<title>Modeling local context for pitch accent prediction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="2928" citStr="Pan and Hirschberg, 2000" startWordPosition="433" endWordPosition="436">1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an</context>
</contexts>
<marker>Pan, Hirschberg, 2000</marker>
<rawString>S. Pan and J. Hirschberg. 2000. Modeling local context for pitch accent prediction. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>K McKeown</author>
</authors>
<title>Word informativeness and automatic pitch accent modeling.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC’99,</booktitle>
<publisher>ACL.</publisher>
<location>New Brunswick, NJ, USA.</location>
<contexts>
<context position="2902" citStr="Pan and McKeown, 1999" startWordPosition="428" endWordPosition="432">erance (Cutler et al., 1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary pla</context>
<context position="12733" citStr="Pan and McKeown, 1999" startWordPosition="2011" endWordPosition="2014">d forms not in this corpus were given the highest IC score, i.e. the value for hapax legomenae (words that occur once). Bigram IC – IC on bigrams (BIC) was calculated for the bigrams (pairs of words) in the data, according to the same formula and corpus material as for unigram IC. TF*IDF – The TF*IDF metric (Salton, 1989) estimates the relevance of a word in a document. Document frequency counts for all token types were obtained from a subset of the same corpus as used for IC calculations. TF*IDF and IC (previous two features) have been succesfully tested as features for accent prediction by (Pan and McKeown, 1999), who assert that IC is a more powerful predictor than TF*IDF. Phrasometer – The phrasometer feature (PM) is the summed log-likelihood of all n-grams the word form occurs in, with n ranging from 1 to 25, and computed in an iterative growth procedure: loglikelihoods of n + 1-grams were computed by expanding all stored n-grams one word to the left and to the right; only the n + 1-grams with higher log-likelihood than that of the original n-gram are stored. Computations are based on the complete ILK Corpus. Distance to previous occurrence – The distance, counted in the number of tokens, to previo</context>
</contexts>
<marker>Pan, McKeown, 1999</marker>
<rawString>S. Pan and K. McKeown. 1999. Word informativeness and automatic pitch accent modeling. In Proceedings of EMNLP/VLC’99, New Brunswick, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Provost</author>
<author>D Jensen</author>
<author>T Oates</author>
</authors>
<title>Efficient progressive sampling.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>23--32</pages>
<contexts>
<context position="17823" citStr="Provost et al., 1999" startWordPosition="2946" endWordPosition="2949"> - molenaar = = N soort,ev,neut 0 B O 5.95 8.50 0.05 0 9999 0.78 0.17 - - - - Wijbrand = = N eigen,ev,neut 0 I O 7.89 8.50 0.11 0 38 0.83 0.11 A - ATable 1: Symbolic and numerical features and class for the sentence De bomen rondom de scheepswerf Verolme moeten verkassen, vindt molenaar Wijbrandt. ‘Miller Wijbrand thinks that the trees surrounding the mill near shipyard Verolme have to relocate.’ and feature selection, that combines classifier wrapping (using the training material internally to test experimental variants) (Kohavi and John, 1997) with progressive sampling of training material (Provost et al., 1999). We start with a large pool of experiments, each with a unique combination of input features and algorithmic parameter settings. In the first step, each attempted setting is applied to a small amount of training material and tested on a fixed amount of held-out data (which is a part of the full training set). Only the best settings are kept; all others are removed from the pool of competing settings. In subsequent iterations, this step is repeated, exponentially decreasing the number of settings in the pool, while at the same time exponentially increasing the amount of training material. The </context>
</contexts>
<marker>Provost, Jensen, Oates, 1999</marker>
<rawString>F. Provost, D. Jensen, and T. Oates. 1999. Efficient progressive sampling. In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining, pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic text processing: The transformation, analysis, and retrieval of information by computer.</title>
<date>1989</date>
<publisher>Addison–Wesley,</publisher>
<location>Reading, MA, USA.</location>
<contexts>
<context position="12434" citStr="Salton, 1989" startWordPosition="1960" endWordPosition="1961">ntent (IC) of a word w is given by IC(w) = −log(P(w)), where P(w) is esti1Numeric features were rounded off to two decimal points, where appropriate. mated by the observed frequency of w in a large disjoint corpus of about 1.7 GB of unannotated Dutch text garnered from various sources. Word forms not in this corpus were given the highest IC score, i.e. the value for hapax legomenae (words that occur once). Bigram IC – IC on bigrams (BIC) was calculated for the bigrams (pairs of words) in the data, according to the same formula and corpus material as for unigram IC. TF*IDF – The TF*IDF metric (Salton, 1989) estimates the relevance of a word in a document. Document frequency counts for all token types were obtained from a subset of the same corpus as used for IC calculations. TF*IDF and IC (previous two features) have been succesfully tested as features for accent prediction by (Pan and McKeown, 1999), who assert that IC is a more powerful predictor than TF*IDF. Phrasometer – The phrasometer feature (PM) is the summed log-likelihood of all n-grams the word form occurs in, with n ranging from 1 to 25, and computed in an iterative growth procedure: loglikelihoods of n + 1-grams were computed by exp</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>G. Salton. 1989. Automatic text processing: The transformation, analysis, and retrieval of information by computer. Addison–Wesley, Reading, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stanfill</author>
<author>D Waltz</author>
</authors>
<title>Toward memory-based reasoning.</title>
<date>1986</date>
<journal>Communications of the ACM,</journal>
<volume>29</volume>
<issue>12</issue>
<contexts>
<context position="14980" citStr="Stanfill and Waltz, 1986" startWordPosition="2391" endWordPosition="2394"> instances) of the two partitions is minimal. The algorithm continues to split each partition recursively until some stop criterion is met (e.g. a minimal number of instances in the partition). Alternatively, a small stop value can be used to build a tree that is probably overfitted, but is then pruned back to where it best matches some amount of held-out data. In our experiments, we used the CART implementation that is part of the Edinburgh Speech Tools (Taylor et al., 1999). 2.4 Memory-based learning Memory-based learning (MBL), also known as instance-based, example-based, or lazy learning (Stanfill and Waltz, 1986; Aha et al., 1991), is a supervised inductive learning algorithm for learning classification tasks. Memory-based learning treats a set of training instances as points in a multidimensional feature space, and stores them as such in an instance base in memory (rather than performing some abstraction over them). After the instance base is stored, new (test) instances are classified by matching them to all instances in memory, and by calculating with each match the distance, given by a distance function between the new instance X and the memory instance Y . Cf. (Daelemans et al., 2002) for detail</context>
</contexts>
<marker>Stanfill, Waltz, 1986</marker>
<rawString>C. Stanfill and D. Waltz. 1986. Toward memory-based reasoning. Communications of the ACM, 29(12):1213–1228, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Taylor</author>
<author>A Black</author>
</authors>
<title>Assigning phrase breaks from part-of-speech sequences. Computer Speech and Language,</title>
<date>1998</date>
<pages>12--99</pages>
<contexts>
<context position="3036" citStr="Taylor and Black, 1998" startWordPosition="450" endWordPosition="453">oundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an annotated corpus of newspaper text, and machine learning algorithms to produce classifiers for both tasks. </context>
<context position="22478" citStr="Taylor and Black, 1998" startWordPosition="3737" endWordPosition="3740"> (GR), and Shared Variance (SV) • the feature value similarity metric: Overlap, or Modified Value Difference Metric (MVDM) with back-off to Overlap at value frequency tresholds 1 (L=1, no back-off), 2, and 10 • the type of distance weighting: None, Inverse Distance, Inverse Linear Distance, and Exponential Decay with α = 1.0 (ED1) and α = 4.0 (ED4) 4 Results 4.1 Tenfold iterative deepening results We first determined two sharp, informed baselines; see Table 2. The informed baseline for accent placement is based on the content versus function word distinction, commonly employed in TTS systems (Taylor and Black, 1998). We refer to this baseline as CF-rule. It is constructed by accenting all content words, while leaving all function words (determiners, prepositions, conjunctions/complementisers and auxiliaries) unaccented. The required word class information is obtained from the POS tags. The baseline for break placement, henceforth PUNC-rule, relies solely on punctuation. A break is inserted after any sequence of punctuation symbols containing one Target: Method: Prec : Rec : F : Accent CF-rule 66.7 94.9 78.3 CART 78.6 ±2.8 85.7 ±1.1 82.0 ±1.7 MBL 80.0 ±2.7 86.6 ±1.4 83.6 ±1.6* CART° 78.7 ±3.0 85.6 ±0.8 82</context>
</contexts>
<marker>Taylor, Black, 1998</marker>
<rawString>P. Taylor and A. Black. 1998. Assigning phrase breaks from part-of-speech sequences. Computer Speech and Language, 12:99–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Taylor</author>
<author>R Caley</author>
<author>A W Black</author>
<author>S King</author>
</authors>
<date>1999</date>
<booktitle>Edinburgh Speech Tools Library, System Documentation Edition 1.2.</booktitle>
<institution>CSTR, University of Edinburgh.</institution>
<contexts>
<context position="14836" citStr="Taylor et al., 1999" startWordPosition="2373" endWordPosition="2376">ilding algorithm starts by selecting the feature test that splits the data in such a way that the mean impurity (entropy times the number of instances) of the two partitions is minimal. The algorithm continues to split each partition recursively until some stop criterion is met (e.g. a minimal number of instances in the partition). Alternatively, a small stop value can be used to build a tree that is probably overfitted, but is then pruned back to where it best matches some amount of held-out data. In our experiments, we used the CART implementation that is part of the Edinburgh Speech Tools (Taylor et al., 1999). 2.4 Memory-based learning Memory-based learning (MBL), also known as instance-based, example-based, or lazy learning (Stanfill and Waltz, 1986; Aha et al., 1991), is a supervised inductive learning algorithm for learning classification tasks. Memory-based learning treats a set of training instances as points in a multidimensional feature space, and stores them as such in an instance base in memory (rather than performing some abstraction over them). After the instance base is stored, new (test) instances are classified by matching them to all instances in memory, and by calculating with each</context>
</contexts>
<marker>Taylor, Caley, Black, King, 1999</marker>
<rawString>P. Taylor, R. Caley, A. W. Black, and S. King, 1999. Edinburgh Speech Tools Library, System Documentation Edition 1.2. CSTR, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O van Herwijnen</author>
<author>J Terken</author>
</authors>
<title>Evaluation ofpros-3 for the assignment of prosodic structure, compared to assignment by human experts.</title>
<date>2001</date>
<booktitle>In Proceedings Eurospeech 2001 Scandinavia, Vol.1,</booktitle>
<pages>529--532</pages>
<marker>van Herwijnen, Terken, 2001</marker>
<rawString>O. van Herwijnen and J. Terken. 2001. Evaluation ofpros-3 for the assignment of prosodic structure, compared to assignment by human experts. In Proceedings Eurospeech 2001 Scandinavia, Vol.1, pages 529–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Q Wang</author>
<author>J Hirschberg</author>
</authors>
<title>Automatic classification of intonational phrasing boundaries.</title>
<date>1997</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="3011" citStr="Wang and Hirschberg, 1997" startWordPosition="446" endWordPosition="449"> information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an annotated corpus of newspaper text, and machine learning algorithms to produce cla</context>
<context position="4445" citStr="Wang and Hirschberg, 1997" startWordPosition="668" endWordPosition="671">ndividual instances and use a memory-based k-nearest neighbour classifier? 2. Is there an advantage in inducing classifiers for both tasks individually, or can both tasks be learned together. The first question deals with a key difference between standard decision tree induction and memorybased classification: how to deal with exceptional instances. Decision trees, CART (Classification and Regression Tree) in particular (Breiman et al., 1984), have been among the first successful machine learning algorithms applied to predicting pitch accents and prosodic boundaries for TTS (Hirschberg, 1993; Wang and Hirschberg, 1997). Decision tree induction finds, through heuristics, a minimallysized decision tree that is estimated to generalize well to unseen data. Its minimality strategy makes the algorithm reluctant to remember individual outlier instances that would take long paths in the tree: typically, these are discarded. This may work well when outliers do not reoccur, but as demonstrated by (Daelemans et al., 1999), exceptions do typically reoccur in language data. Hence, machine learning algorithms that retain a memory trace of individual instances, like memory-based learning algorithms based on the k-nearest </context>
</contexts>
<marker>Wang, Hirschberg, 1997</marker>
<rawString>M. Q. Wang and J. Hirschberg. 1997. Automatic classification of intonational phrasing boundaries. Computer Speech and Language, 6(2):175–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>