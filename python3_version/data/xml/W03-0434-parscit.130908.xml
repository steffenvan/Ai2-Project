<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008753">
<title confidence="0.99672">
A Robust Risk Minimization based Named Entity Recognition System
</title>
<author confidence="0.979966">
Tong Zhang
</author>
<affiliation confidence="0.6355025">
IBM T.J. Watson Research Center
Yorktown Heights
</affiliation>
<address confidence="0.965377">
New York, 10598, USA
</address>
<email confidence="0.998337">
tzhang@watson.ibm.com
</email>
<author confidence="0.782785">
David Johnson
</author>
<affiliation confidence="0.5122265">
IBM T.J. Watson Research Center
Yorktown Heights
</affiliation>
<address confidence="0.960336">
New York, 10598, USA
</address>
<email confidence="0.998878">
dejohns@us.ibm.com
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986944444444">
This paper describes a robust linear classifica-
tion system for Named Entity Recognition. A
similar system has been applied to the CoNLL
text chunking shared task with state of the art
performance. By using different linguistic fea-
tures, we can easily adapt this system to other
token-based linguistic tagging problems. The
main focus of the current paper is to investigate
the impact of various local linguistic features
for named entity recognition on the CoNLL-
2003 (Tjong Kim Sang and De Meulder, 2003)
shared task data. We show that the system per-
formance can be enhanced significantly with
some relative simple token-based features that
are available for many languages. Although
more sophisticated linguistic features will also
be helpful, they provide much less improve-
ment than might be expected.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871355555556">
An important research area in the field of information ex-
traction is Named Entity Recognition. This topic was a
central theme in the message understanding conferences
(MUCs). It has become more important nowadays due
to the large amount of available electronic text, which
makes it necessary to build systems that can automati-
cally process and extract information from text.
In spite of significant work in this area, the problem
itself has not been solved. Although some earlier re-
ports suggested accuracy (F1-number) of machine learn-
ing based systems to be in the lower 90s with relatively
small amount of labeled data (for example, (Bikel et al.,
1999; Mikheev et al., 1998; Sundheim, 1995)), these
studies were often performed on relatively restricted do-
mains. Our experience indicates that the performance of
a statistically based named entity extraction system can
vary significantly depending on the underlying domain.
There are still open challenges to make the performance
of a statistical system consistent across different types of
data sources.
In this paper we present a system for named entity
recognition based on our earlier work on text chunking
(Zhang et al., 2002). One advantage of the proposed sys-
tem is that it can easily incorporate a large number of lin-
guistic features. This advantage is similar to a number of
other approaches, such as the maximum entropy method,
which has been widely used to solve NLP problems, see
(Borthwick, 1999; Ratnaparkhi, 1999) for example.
The performance of our system can be significantly af-
fected by the choice of available linguistic features. The
main focus of this paper is to investigate the impact of
some local features. Specifically we show that the system
performance can be enhanced significantly with some
relatively simple token-based features. More sophisti-
cated linguistic features, although helpful, yield much
less improvement in system performance than might be
expected.
We believe that this study provides useful insight into
the usefulness of various available local linguistic fea-
tures. Since these simple features are readily available
for many languages, it suggests the possibility of setting
up a language independent named entity recognition sys-
tem quickly so that its performance is close to a system
that uses much more sophisticated, language dependent
features.
</bodyText>
<sectionHeader confidence="0.945491" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.999688517241379">
Following the approach employed in our text chunking
system (Zhang et al., 2002), we treat the named entity
recognition problem as a sequential token-based tagging
problem. We denote by {wi} (i = 0, 1, ... , m) the se-
quence of tokenized text, which is the input to our system.
In token-based tagging, the goal is to assign a class-label
ti, taking its value from a predefined set of labels, to ev-
ery token wi.
For named entity recognition, and text segmentation
in general, the entities (segments) can be encoded as a
token-based tagging problem by using various encoding
schemes. In this paper, we shall only use the IOB1 encod-
ing scheme which is provided in the CoNLL-2003 shared
task.
The goal of our learning system is to predict the class-
label value ti associated with each token wi. In our sys-
tem, this is achieved by estimating the conditional proba-
bility P(ti = c|xi) for every possible class-label value c,
where xi is a feature vector associated with token i. It is
essentially a sufficient statistic in our model: we assume
that P(ti = c|xi) = P(ti = c|{wi}, {tj}j&lt;i). The fea-
ture vector xi can depend on previously predicted class-
labels {tj}j&lt;i, but the dependency is typically assumed
to be local. Given such a conditional probability model,
in the decoding stage, we estimate the best possible se-
quence of ti’s using a dynamic programming approach,
similar to what is described in (Zhang et al., 2002).
In our system, the conditional probability model has
the following parametric form:
</bodyText>
<equation confidence="0.983952">
P(ti = c|xi, {ti−P, ... , ti−1}) = T(wT cxi + bc),
</equation>
<bodyText confidence="0.991226125">
where T(y) = min(1, max(0, y)) is the truncation of y
into the interval [0, 1]. wc is a linear weight vector and
bc is a constant. Parameters wc and bc can be estimated
from the training data.
Given training data (xi, ti) for i = 1, ... , n. It was
shown in (Zhang et al., 2002) that such a model can be
estimated by solving the following optimization problem
for each c:
</bodyText>
<equation confidence="0.808830571428571">
f(wTc xi + bc,yic),
where yic = 1 when ti = c and yic = −1 otherwise. The
function f is defined as:
−2py py &lt; −1
2(py − 1)2 py ∈ [−1, 1]
1
0 py &gt; 1.
</equation>
<bodyText confidence="0.999864777777778">
This risk function is closely related to Huber’s loss
function in robust estimation. We shall call a classifica-
tion method that is based on approximately minimizing
this risk function robust risk minimization. The general-
ized Winnow method in (Zhang et al., 2002) implements
such a method. The numerical algorithm used for exper-
iments in this paper is a variant, and is similar to the one
given in (Damerau et al., 2003).
The main purpose of the paper is to investigate the im-
pact of local linguistic features for the Named Entity de-
tection task. The basic linguistic features considered here
are all aligned with the tokens. Specifically we will con-
sider features listed in Table 1. These features are repre-
sented using a binary encoding scheme where each com-
ponent of the feature vector x corresponds to an occur-
rence of a feature that is listed above. We use a window
of ±1 centered at the current token unless indicated oth-
erwise in Table 1.
</bodyText>
<sectionHeader confidence="0.997765" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999960733333334">
We study the performance of our system with different
feature combinations on the English development set.
Our results are presented in Table 2. All of these results
are significantly better than the baseline performance of
71.18. We will now discuss the implications of these ex-
perimental results.
The small difference between Experiment 1 and Ex-
periment 2 implies that tokens by themselves, whether
represented as mixed case text or not, do not significantly
affect the system performance.
Experiment 3 shows that even without case informa-
tion, the performance of a statistical named entity recog-
nition system can be greatly enhanced with token prefix
and suffix information. Intuitively, such information al-
lows us to build a character-based token-model which can
predict whether an (unseen) English word looks like an
entity-type or not. The performance of this experiment
is comparable to that of the mixed-case English text plus
capitalization feature reported in Experiment 4.
Experiment 4 suggests that capitalization is a very use-
ful feature for mixed case text, and can greatly enhance
the perform of a named entity recognition system. With
token prefix and suffix information that incorporates a
character-based entity model, the system performance is
further enhanced, as reported in Experiment 5.
Up to Experiment 5, we have only used very simple
token-based linguistic features. Despite their simplicity,
these features give very significant performance enhance-
ment. In addition, such features are readily available
for many languages, implying that they can be used in
a language independent statistical named entity recogni-
tion system.
In Experiment 6, we added the provided part-of-speech
and chunking information. Clearly they only lead to a
relatively small improvement. We believe that most in-
formation contained in part-of-speech has already been
captured in the capitalization and prefix/suffix features.
The chunking information might be more useful, though
its value is still quite limited.
By adding the four supplied dictionaries, we observe a
small, but statistically significant improvement. The per-
formance is reported in Experiment 7. At this point we
have only used information provided by the shared task.
Further performance enhancement can be achieved by
using extra information that is not provided in the shared
</bodyText>
<equation confidence="0.995834285714286">
1
n
inf
w,b
n
i=1
f(p, y) = I
</equation>
<table confidence="0.9999524">
Feature ID Feature description
A Tokens that are turned into all upper-case, in a window of ±2.
B Tokens themselves, in a window of ±2.
C The previous two predicted tags, and
the conjunction of the previous tag and the current token.
D Initial capitalization of tokens in a window of ±2.
E More elaborated word type information:
initial capitalization, all capitalization, all digitals, or digitals containing punctuations.
F Token prefix (length three and four), and token suffix (length from one to four).
G POS tagged information provided in shared the task.
H chunking information provided in the shared task:
we use a bag-of-word representation of the chunk at the current token.
I The four dictionaries provided in the shared task: PER, ORG, LOC, and MISC.
J A number of additional dictionaries from different sources:
some trigger words for ORG, PER, LOC; lists of location, person, and organizations.
</table>
<tableCaption confidence="0.992753">
Table 1: feature definition
</tableCaption>
<table confidence="0.994783555555556">
Experiment ID Features used Precision Recall F0=1
1 A+C 91.94 74.25 82.15
2 B+C 93.70 74.89 83.25
3 A+F 89.96 82.50 86.07
4 B+C+D 88.79 86.01 87.38
5 B+C+D+E+F 90.11 88.67 89.39
6 B+C+D+E+F+G+H 91.00 89.53 90.26
7 B+C+D+E+F+G+H+I 92.14 90.73 91.43
8 B+C+D+E+F+G+H+I+J 92.76 91.42 92.08
</table>
<tableCaption confidence="0.999868">
Table 2: Performance with different features on the English development set
</tableCaption>
<bodyText confidence="0.999875045454546">
task. In this study, we will only report performance with
additional dictionaries we have gathered from various dif-
ferent sources. With these additional dictionaries, our
system achieved a performance of 92, as reported in Ex-
periment 8. Table 4 presents the performance of each
entity type separately.
Clearly the construction of extra linguistic features is
open ended. It is possible to improve system performance
with additional and higher quality dictionaries. Although
dictionaries are language dependent, they are often fairly
readily available and providing them does not pose a ma-
jor impediment to customizing a language independent
system. However, for more difficult cases, it may be
necessary to provide high precision, manually developed
rules to capture particular linguistic patterns. Language
dependent features of this kind are harder to develop than
dictionaries and correspondingly pose a greater obstacle
to customizing a language independent system. We have
found that such features can appreciably improve the per-
formance of our system, but discussion is beyond the
scope of this paper. A related idea is to combine the out-
puts of different systems. See (Florian et al., 2003) for
such a study. Fortunately, as our experiments indicate,
special purpose patterns may not be necessary for quite
reasonable accuracy.
In Table 4, we report the performance of our system on
the German data. We shall note that the performance is
significantly lower than the corresponding English per-
formance. Our experience indicates that even for En-
glish, the real-world performance of a statistical named
entity recognizer can be very low. The performance we
reported for the German data is achieved by using the
following features: B+C+D+E+F+G+H+I+J (with some
small modifications), plus the German word lemma fea-
ture provided by the task.
The additional German dictionaries were provided to
us by Radu Florian. Without these additional dictionaries
(in this case, all information we use is provided by the
CoNLL task), the overall performance is listed in Table 3.
It is also interesting to note that without any dictionary
information, the overall performance drops to an F0=1
score of 65.5 on the development set, and to 70.2 on the
test set. Clearly for this data, dictionary information helps
more on the development data than on the test data.
</bodyText>
<table confidence="0.999359666666667">
Precision Recall F0=1
devel. set 82.49 61.22 70.29
test set 81.59 62.73 70.93
</table>
<tableCaption confidence="0.9243875">
Table 3: System performance with only features that are
provided by the CoNLL task on German data.
</tableCaption>
<sectionHeader confidence="0.997198" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999951285714286">
In this paper, we presented a general token-based NLP
tagging system using a robust risk minimization classifi-
cation method. The system can take advantage of differ-
ent kinds of linguistic features.
We have studied the impact of various local linguistic
features on the performance of our system. It is interest-
ing to note that most performance improvement can be
achieved with some relatively simple token features that
are easy to construct. Although more sophisticated lin-
guistic features will also be helpful, they provide much
less improvement than might be expected. This observa-
tion supports the view that language independent named
entity recognition systems can, with relatively small ef-
fort, achieve competitive levels of accuracy.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999625">
The authors would like to thank Radu Florian for prepar-
ing the German data and for providing additional German
dictionaries that helped to achieve the performance pre-
sented in the paper.
</bodyText>
<sectionHeader confidence="0.990686" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.6647835">
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what’s
in a name. Machine Learning, 34(1-3):211–231.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Fred J. Damerau, Tong Zhang, Sholom M. Weiss, and
Nitin Indurkhya. 2003. Text categorization for a com-
prehensive time-dependent benchmark. Information
Processing &amp; Management, to appear.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Proceedings CoNLL-2003.
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the ltg system used for MUC-7. In Proceed-
ings of the Seventh Message Understanding Confer-
ence (MUC-7).
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151–175.
</reference>
<table confidence="0.996672541666667">
English devel. Precision Recall F0=1
LOC 95.65 94.56 95.10
MISC 90.15 84.38 87.17
ORG 89.03 86.50 87.75
PER 93.76 95.39 94.56
Overall 92.76 91.42 92.08
English test Precision Recall F0=1
LOC 89.51 89.57 89.54
MISC 77.45 74.36 75.87
ORG 82.57 78.45 80.46
PER 89.67 91.22 90.44
Overall 86.13 84.88 85.50
German devel. Precision Recall F0=1
LOC 80.69 72.90 76.60
MISC 80.38 42.18 55.32
ORG 83.19 61.40 70.65
PER 86.36 65.95 74.79
Overall 82.98 61.51 70.65
German test Precision Recall F0=1
LOC 77.71 71.40 74.42
MISC 71.51 39.70 51.06
ORG 79.55 55.37 65.29
PER 91.68 73.81 81.78
Overall 82.00 63.03 71.27
</table>
<tableCaption confidence="0.990001">
Table 4: System performance with all features on the En-
</tableCaption>
<reference confidence="0.883345">
glish and the German data
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003.
B.M. Sundheim. 1995. Overview of results of the MUC-
6 evaluation. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
Tong Zhang, Fred Damerau, and David E. Johnson.
2002. Text chunking based on a generalization of Win-
now. Journal of Machine Learning Research, 2:615–
637.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.413174">
<title confidence="0.999909">A Robust Risk Minimization based Named Entity Recognition System</title>
<author confidence="0.981134">Tong</author>
<affiliation confidence="0.998088">IBM T.J. Watson Research</affiliation>
<address confidence="0.8375905">Yorktown New York, 10598,</address>
<email confidence="0.998915">tzhang@watson.ibm.com</email>
<author confidence="0.971407">David</author>
<affiliation confidence="0.99854">IBM T.J. Watson Research</affiliation>
<address confidence="0.838914">Yorktown New York, 10598,</address>
<email confidence="0.999794">dejohns@us.ibm.com</email>
<abstract confidence="0.996882526315789">This paper describes a robust linear classification system for Named Entity Recognition. A similar system has been applied to the CoNLL text chunking shared task with state of the art performance. By using different linguistic features, we can easily adapt this system to other token-based linguistic tagging problems. The main focus of the current paper is to investigate the impact of various local linguistic features for named entity recognition on the CoNLL- 2003 (Tjong Kim Sang and De Meulder, 2003) shared task data. We show that the system performance can be enhanced significantly with some relative simple token-based features that are available for many languages. Although more sophisticated linguistic features will also be helpful, they provide much less improvement than might be expected.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard L Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1753" citStr="Bikel et al., 1999" startWordPosition="270" endWordPosition="273">eld of information extraction is Named Entity Recognition. This topic was a central theme in the message understanding conferences (MUCs). It has become more important nowadays due to the large amount of available electronic text, which makes it necessary to build systems that can automatically process and extract information from text. In spite of significant work in this area, the problem itself has not been solved. Although some earlier reports suggested accuracy (F1-number) of machine learning based systems to be in the lower 90s with relatively small amount of labeled data (for example, (Bikel et al., 1999; Mikheev et al., 1998; Sundheim, 1995)), these studies were often performed on relatively restricted domains. Our experience indicates that the performance of a statistically based named entity extraction system can vary significantly depending on the underlying domain. There are still open challenges to make the performance of a statistical system consistent across different types of data sources. In this paper we present a system for named entity recognition based on our earlier work on text chunking (Zhang et al., 2002). One advantage of the proposed system is that it can easily incorporat</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard L. Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="2559" citStr="Borthwick, 1999" startWordPosition="401" endWordPosition="402">ity extraction system can vary significantly depending on the underlying domain. There are still open challenges to make the performance of a statistical system consistent across different types of data sources. In this paper we present a system for named entity recognition based on our earlier work on text chunking (Zhang et al., 2002). One advantage of the proposed system is that it can easily incorporate a large number of linguistic features. This advantage is similar to a number of other approaches, such as the maximum entropy method, which has been widely used to solve NLP problems, see (Borthwick, 1999; Ratnaparkhi, 1999) for example. The performance of our system can be significantly affected by the choice of available linguistic features. The main focus of this paper is to investigate the impact of some local features. Specifically we show that the system performance can be enhanced significantly with some relatively simple token-based features. More sophisticated linguistic features, although helpful, yield much less improvement in system performance than might be expected. We believe that this study provides useful insight into the usefulness of various available local linguistic featur</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
<author>Tong Zhang</author>
<author>Sholom M Weiss</author>
<author>Nitin Indurkhya</author>
</authors>
<title>Text categorization for a comprehensive time-dependent benchmark.</title>
<date>2003</date>
<journal>Information Processing &amp; Management,</journal>
<note>to appear.</note>
<contexts>
<context position="5961" citStr="Damerau et al., 2003" startWordPosition="989" endWordPosition="992">ving the following optimization problem for each c: f(wTc xi + bc,yic), where yic = 1 when ti = c and yic = −1 otherwise. The function f is defined as: −2py py &lt; −1 2(py − 1)2 py ∈ [−1, 1] 1 0 py &gt; 1. This risk function is closely related to Huber’s loss function in robust estimation. We shall call a classification method that is based on approximately minimizing this risk function robust risk minimization. The generalized Winnow method in (Zhang et al., 2002) implements such a method. The numerical algorithm used for experiments in this paper is a variant, and is similar to the one given in (Damerau et al., 2003). The main purpose of the paper is to investigate the impact of local linguistic features for the Named Entity detection task. The basic linguistic features considered here are all aligned with the tokens. Specifically we will consider features listed in Table 1. These features are represented using a binary encoding scheme where each component of the feature vector x corresponds to an occurrence of a feature that is listed above. We use a window of ±1 centered at the current token unless indicated otherwise in Table 1. 3 Experimental Results We study the performance of our system with differe</context>
</contexts>
<marker>Damerau, Zhang, Weiss, Indurkhya, 2003</marker>
<rawString>Fred J. Damerau, Tong Zhang, Sholom M. Weiss, and Nitin Indurkhya. 2003. Text categorization for a comprehensive time-dependent benchmark. Information Processing &amp; Management, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Proceedings CoNLL-2003.</booktitle>
<contexts>
<context position="11384" citStr="Florian et al., 2003" startWordPosition="1846" endWordPosition="1849">ot pose a major impediment to customizing a language independent system. However, for more difficult cases, it may be necessary to provide high precision, manually developed rules to capture particular linguistic patterns. Language dependent features of this kind are harder to develop than dictionaries and correspondingly pose a greater obstacle to customizing a language independent system. We have found that such features can appreciably improve the performance of our system, but discussion is beyond the scope of this paper. A related idea is to combine the outputs of different systems. See (Florian et al., 2003) for such a study. Fortunately, as our experiments indicate, special purpose patterns may not be necessary for quite reasonable accuracy. In Table 4, we report the performance of our system on the German data. We shall note that the performance is significantly lower than the corresponding English performance. Our experience indicates that even for English, the real-world performance of a statistical named entity recognizer can be very low. The performance we reported for the German data is achieved by using the following features: B+C+D+E+F+G+H+I+J (with some small modifications), plus the Ge</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In Proceedings CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>C Grover</author>
<author>M Moens</author>
</authors>
<title>Description of the ltg system used for MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<contexts>
<context position="1775" citStr="Mikheev et al., 1998" startWordPosition="274" endWordPosition="277">xtraction is Named Entity Recognition. This topic was a central theme in the message understanding conferences (MUCs). It has become more important nowadays due to the large amount of available electronic text, which makes it necessary to build systems that can automatically process and extract information from text. In spite of significant work in this area, the problem itself has not been solved. Although some earlier reports suggested accuracy (F1-number) of machine learning based systems to be in the lower 90s with relatively small amount of labeled data (for example, (Bikel et al., 1999; Mikheev et al., 1998; Sundheim, 1995)), these studies were often performed on relatively restricted domains. Our experience indicates that the performance of a statistically based named entity extraction system can vary significantly depending on the underlying domain. There are still open challenges to make the performance of a statistical system consistent across different types of data sources. In this paper we present a system for named entity recognition based on our earlier work on text chunking (Zhang et al., 2002). One advantage of the proposed system is that it can easily incorporate a large number of li</context>
</contexts>
<marker>Mikheev, Grover, Moens, 1998</marker>
<rawString>A. Mikheev, C. Grover, and M. Moens. 1998. Description of the ltg system used for MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--151</pages>
<contexts>
<context position="2579" citStr="Ratnaparkhi, 1999" startWordPosition="403" endWordPosition="404">stem can vary significantly depending on the underlying domain. There are still open challenges to make the performance of a statistical system consistent across different types of data sources. In this paper we present a system for named entity recognition based on our earlier work on text chunking (Zhang et al., 2002). One advantage of the proposed system is that it can easily incorporate a large number of linguistic features. This advantage is similar to a number of other approaches, such as the maximum entropy method, which has been widely used to solve NLP problems, see (Borthwick, 1999; Ratnaparkhi, 1999) for example. The performance of our system can be significantly affected by the choice of available linguistic features. The main focus of this paper is to investigate the impact of some local features. Specifically we show that the system performance can be enhanced significantly with some relatively simple token-based features. More sophisticated linguistic features, although helpful, yield much less improvement in system performance than might be expected. We believe that this study provides useful insight into the usefulness of various available local linguistic features. Since these simp</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34:151–175.</rawString>
</citation>
<citation valid="false">
<authors>
<author>glish</author>
</authors>
<title>the German data</title>
<marker>glish, </marker>
<rawString>glish and the German data</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans and</booktitle>
<editor>Miles Osborne, editors,</editor>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language independent named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Sundheim</author>
</authors>
<title>Overview of results of the MUC6 evaluation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<contexts>
<context position="1792" citStr="Sundheim, 1995" startWordPosition="278" endWordPosition="279">ity Recognition. This topic was a central theme in the message understanding conferences (MUCs). It has become more important nowadays due to the large amount of available electronic text, which makes it necessary to build systems that can automatically process and extract information from text. In spite of significant work in this area, the problem itself has not been solved. Although some earlier reports suggested accuracy (F1-number) of machine learning based systems to be in the lower 90s with relatively small amount of labeled data (for example, (Bikel et al., 1999; Mikheev et al., 1998; Sundheim, 1995)), these studies were often performed on relatively restricted domains. Our experience indicates that the performance of a statistically based named entity extraction system can vary significantly depending on the underlying domain. There are still open challenges to make the performance of a statistical system consistent across different types of data sources. In this paper we present a system for named entity recognition based on our earlier work on text chunking (Zhang et al., 2002). One advantage of the proposed system is that it can easily incorporate a large number of linguistic features</context>
</contexts>
<marker>Sundheim, 1995</marker>
<rawString>B.M. Sundheim. 1995. Overview of results of the MUC6 evaluation. In Proceedings of the Sixth Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Fred Damerau</author>
<author>David E Johnson</author>
</authors>
<title>Text chunking based on a generalization of Winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2</volume>
<pages>637</pages>
<contexts>
<context position="2282" citStr="Zhang et al., 2002" startWordPosition="351" endWordPosition="354">e lower 90s with relatively small amount of labeled data (for example, (Bikel et al., 1999; Mikheev et al., 1998; Sundheim, 1995)), these studies were often performed on relatively restricted domains. Our experience indicates that the performance of a statistically based named entity extraction system can vary significantly depending on the underlying domain. There are still open challenges to make the performance of a statistical system consistent across different types of data sources. In this paper we present a system for named entity recognition based on our earlier work on text chunking (Zhang et al., 2002). One advantage of the proposed system is that it can easily incorporate a large number of linguistic features. This advantage is similar to a number of other approaches, such as the maximum entropy method, which has been widely used to solve NLP problems, see (Borthwick, 1999; Ratnaparkhi, 1999) for example. The performance of our system can be significantly affected by the choice of available linguistic features. The main focus of this paper is to investigate the impact of some local features. Specifically we show that the system performance can be enhanced significantly with some relatively</context>
<context position="3548" citStr="Zhang et al., 2002" startWordPosition="547" endWordPosition="550">d linguistic features, although helpful, yield much less improvement in system performance than might be expected. We believe that this study provides useful insight into the usefulness of various available local linguistic features. Since these simple features are readily available for many languages, it suggests the possibility of setting up a language independent named entity recognition system quickly so that its performance is close to a system that uses much more sophisticated, language dependent features. 2 System description Following the approach employed in our text chunking system (Zhang et al., 2002), we treat the named entity recognition problem as a sequential token-based tagging problem. We denote by {wi} (i = 0, 1, ... , m) the sequence of tokenized text, which is the input to our system. In token-based tagging, the goal is to assign a class-label ti, taking its value from a predefined set of labels, to every token wi. For named entity recognition, and text segmentation in general, the entities (segments) can be encoded as a token-based tagging problem by using various encoding schemes. In this paper, we shall only use the IOB1 encoding scheme which is provided in the CoNLL-2003 share</context>
<context position="4883" citStr="Zhang et al., 2002" startWordPosition="781" endWordPosition="784">system, this is achieved by estimating the conditional probability P(ti = c|xi) for every possible class-label value c, where xi is a feature vector associated with token i. It is essentially a sufficient statistic in our model: we assume that P(ti = c|xi) = P(ti = c|{wi}, {tj}j&lt;i). The feature vector xi can depend on previously predicted classlabels {tj}j&lt;i, but the dependency is typically assumed to be local. Given such a conditional probability model, in the decoding stage, we estimate the best possible sequence of ti’s using a dynamic programming approach, similar to what is described in (Zhang et al., 2002). In our system, the conditional probability model has the following parametric form: P(ti = c|xi, {ti−P, ... , ti−1}) = T(wT cxi + bc), where T(y) = min(1, max(0, y)) is the truncation of y into the interval [0, 1]. wc is a linear weight vector and bc is a constant. Parameters wc and bc can be estimated from the training data. Given training data (xi, ti) for i = 1, ... , n. It was shown in (Zhang et al., 2002) that such a model can be estimated by solving the following optimization problem for each c: f(wTc xi + bc,yic), where yic = 1 when ti = c and yic = −1 otherwise. The function f is def</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>Tong Zhang, Fred Damerau, and David E. Johnson. 2002. Text chunking based on a generalization of Winnow. Journal of Machine Learning Research, 2:615– 637.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>