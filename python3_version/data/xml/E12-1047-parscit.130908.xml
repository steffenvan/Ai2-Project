<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.4845805">
Efficient Parsing with Linear Context-Free Rewriting Systems
Andreas van Cranenburgh
</title>
<author confidence="0.706129">
Huygens ING &amp; ILLC, University of Amsterdam
</author>
<affiliation confidence="0.771264">
Royal Netherlands Academy of Arts and Sciences
</affiliation>
<address confidence="0.845094">
Postbus 90754, 2509 LT The Hague, the Netherlands
</address>
<email confidence="0.995361">
andreas.van.cranenburgh@huygens.knaw.nl
</email>
<sectionHeader confidence="0.997345" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9973169375">
Previous work on treebank parsing with
discontinuous constituents using Linear
Context-Free Rewriting systems (LCFRS)
has been limited to sentences of up to 30
words, for reasons of computational com-
plexity. There have been some results on
binarizing an LCFRS in a manner that min-
imizes parsing complexity, but the present
work shows that parsing long sentences with
such an optimally binarized grammar re-
mains infeasible. Instead, we introduce a
technique which removes this length restric-
tion, while maintaining a respectable accu-
racy. The resulting parser has been applied
to a discontinuous treebank with favorable
results.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997508">
Discontinuity in constituent structures (cf. figure 1
&amp; 2) is important for a variety of reasons. For
one, it allows a tight correspondence between
syntax and semantics by letting constituent struc-
ture express argument structure (Skut et al., 1997).
Other reasons are phenomena such as extraposi-
tion and word-order freedom, which arguably re-
quire discontinuous annotations to be treated sys-
tematically in phrase-structures (McCawley, 1982;
Levy, 2005). Empirical investigations demon-
strate that discontinuity is present in non-negligible
amounts: around 30% of sentences contain dis-
continuity in two German treebanks (Maier and
Søgaard, 2008; Maier and Lichte, 2009). Re-
cent work on treebank parsing with discontinuous
constituents (Kallmeyer and Maier, 2010; Maier,
2010; Evang and Kallmeyer, 2011; van Cranen-
burgh et al., 2011) shows that it is feasible to
directly parse discontinuous constituency anno-
tations, as given in the German Negra (Skut et al.,
</bodyText>
<figure confidence="0.957353666666667">
SBARQ
WHNP MD NP VB
What should I do ?
</figure>
<figureCaption confidence="0.997339333333333">
Figure 1: A tree with WH-movement from the Penn
treebank, in which traces have been converted to dis-
continuity. Taken from Evang and Kallmeyer (2011).
</figureCaption>
<bodyText confidence="0.99957428">
1997) and Tiger (Brants et al., 2002) corpora, or
those that can be extracted from traces such as in
the Penn treebank (Marcus et al., 1993) annota-
tion. However, the computational complexity is
such that until now, the length of sentences needed
to be restricted. In the case of Kallmeyer and
Maier (2010) and Evang and Kallmeyer (2011) the
limit was 25 words. Maier (2010) and van Cranen-
burgh et al. (2011) manage to parse up to 30 words
with heuristics and optimizations, but no further.
Algorithms have been suggested to binarize the
grammars in such a way as to minimize parsing
complexity, but the current paper shows that these
techniques are not sufficient to parse longer sen-
tences. Instead, this work presents a novel form
of coarse-to-fine parsing which does alleviate this
limitation.
The rest of this paper is structured as follows.
First, we introduce linear context-free rewriting
systems (LCFRS). Next, we discuss and evalu-
ate binarization strategies for LCFRS. Third, we
present a technique for approximating an LCFRS
by a PCFG in a coarse-to-fine framework. Lastly,
we evaluate this technique on a large corpus with-
out the usual length restrictions.
</bodyText>
<equation confidence="0.3134735">
SQ
VP
</equation>
<page confidence="0.987352">
460
</page>
<note confidence="0.738550833333333">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 460–470,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
ROOT
PROAV VAFIN NN NN VVPP $.
Danach habe Kohlenstaub Feuer gefangen .
Afterwards had coal dust fire caught .
</note>
<figureCaption confidence="0.988428">
Figure 2: A discontinuous tree from the Negra corpus.
</figureCaption>
<keyword confidence="0.375199">
Translation: After that coal dust had caught fire.
</keyword>
<sectionHeader confidence="0.992791" genericHeader="introduction">
2 Linear Context-Free Rewriting
Systems
</sectionHeader>
<bodyText confidence="0.988429393939394">
Linear Context-Free Rewriting Systems (LCFRS;
Vijay-Shanker et al., 1987; Weir, 1988) subsume
a wide variety of mildly context-sensitive for-
malisms, such as Tree-Adjoining Grammar (TAG),
Combinatory Categorial Grammar (CCG), Min-
imalist Grammar, Multiple Context-Free Gram-
mar (MCFG) and synchronous CFG (Vijay-Shanker
and Weir, 1994; Kallmeyer, 2010). Furthermore,
they can be used to parse dependency struc-
tures (Kuhlmann and Satta, 2009). Since LCFRS
subsumes various synchronous grammars, they are
also important for machine translation. This makes
it possible to use LCFRS as a syntactic backbone
with which various formalisms can be parsed by
compiling grammars into an LCFRS, similar to the
TuLiPa system (Kallmeyer et al., 2008). As all
mildly context-sensitive formalisms, LCFRS are
parsable in polynomial time, where the degree
depends on the productions of the grammar. In-
tuitively, LCFRS can be seen as a generalization
of context-free grammars to rewriting other ob-
jects than just continuous strings: productions are
context-free, but instead of strings they can rewrite
tuples, trees or graphs.
We focus on the use of LCFRS for parsing with
discontinuous constituents. This follows up on
recent work on parsing the discontinuous anno-
tations in German corpora with LCFRS (Maier,
2010; van Cranenburgh et al., 2011) and work on
parsing the Wall Street journal corpus in which
traces have been converted to discontinuous con-
stituents (Evang and Kallmeyer, 2011). In the case
of parsing with discontinuous constituents a non-
</bodyText>
<equation confidence="0.999830555555555">
ROOT(ab) —* S(a) $.(b)
S(abcd) —* VAFIN(b) NN(c) VP2(a, d)
VP2(a, bc) —* PROAV(a) NN(b) VVPP(c)
PROAV(Danach) —* c
VAFIN(habe) —* c
NN(Kohlenstaub) —* c
NN(Feuer) —* c
VVPP(gefangen) —* c
$.(.) —* E
</equation>
<figureCaption confidence="0.986848666666667">
Figure 3: The productions that can be read off from the
tree in figure 2. Note that lexical productions rewrite to
e, because they do not rewrite to any non-terminals.
</figureCaption>
<bodyText confidence="0.998997666666667">
terminal may cover a tuple of discontinuous strings
instead of a single, contiguous sequence of termi-
nals. The number of components in such a tuple
is called the fan-out of a rule, which is equal to
the number of gaps plus one; the fan-out of the
grammar is the maximum fan-out of its production.
A context-free grammar is a LCFRS with a fan-out
of 1. For convenience we will will use the rule
notation of simple RCG (Boullier, 1998), which
is a syntactic variant of LCFRS, with an arguably
more transparent notation.
A LCFRS is a tuple G = (N, T, V, P, 5). N
is a finite set of non-terminals; a function dim :
N —* N specifies the unique fan-out for every non-
terminal symbol. T and V are disjoint finite sets
of terminals and variables. 5 is the distinguished
start symbol with dim(5) = 1. P is a finite set of
rewrite rules (productions) of the form:
</bodyText>
<equation confidence="0.851726">
A(α1, ... αdim(A)) —*B1(X11, ... , X1dim(B1))
. . .Bm(Xm1 ,... , Xmdim(B.))
for m &gt; 0, where A, B1, ..., Bm E N,
each Xij E V for 1 &lt; i &lt; m, 1 &lt; j &lt; dim(Aj)
</equation>
<bodyText confidence="0.984929363636364">
and αi E (T U V)* for 1 &lt; i &lt; dim(Ai).
Productions must be linear: if a variable occurs
in a rule, it occurs exactly once on the left hand
side (LHS), and exactly once on the right hand side
(RHS). A rule is ordered if for any two variables
X1 and X2 occurring in a non-terminal on the RHS,
X1 precedes X2 on the LHS iff X1 precedes X2
on the RHS.
Every production has a fan-out determined by
the fan-out of the non-terminal symbol on the left-
hand side. Apart from the fan-out productions also
</bodyText>
<equation confidence="0.64547">
S
VP
</equation>
<page confidence="0.995816">
461
</page>
<bodyText confidence="0.999924625">
have a rank: the number of non-terminals on the
right-hand side. These two variables determine
the time complexity of parsing with a grammar. A
production can be instantiated when its variables
can be bound to non-overlapping spans such that
for each component αi of the LHS, the concatena-
tion of its terminals and bound variables forms a
contiguous span in the input, while the endpoints
of each span are non-contiguous.
As in the case of a PCFG, we can read off LCFRS
productions from a treebank (Maier and Søgaard,
2008), and the relative frequencies of productions
form a maximum likelihood estimate, for a prob-
abilistic LCFRS (PLCFRS), i.e., a (discontinuous)
treebank grammar. As an example, figure 3 shows
the productions extracted from the tree in figure 2.
</bodyText>
<sectionHeader confidence="0.995799" genericHeader="method">
3 Binarization
</sectionHeader>
<bodyText confidence="0.999931875">
A probabilistic LCFRS can be parsed using a CKY-
like tabular parsing algorithm (cf. Kallmeyer and
Maier, 2010; van Cranenburgh et al., 2011), but
this requires a binarized grammar.1 Any LCFRS
can be binarized. Crescenzi et al. (2011) state
“while CFGs can always be reduced to rank two
(Chomsky Normal Form), this is not the case for
LCFRS with any fan-out greater than one.” How-
ever, this assertion is made under the assumption of
a fixed fan-out. If this assumption is relaxed then
it is easy to binarize either deterministically or, as
will be investigated in this work, optimally with
a dynamic programming approach. Binarizing an
LCFRS may increase its fan-out, which results in
an increase in asymptotic complexity. Consider
the following production:
</bodyText>
<equation confidence="0.998699">
X(pqrs) → A(p,r) B(q) C(s) (1)
</equation>
<bodyText confidence="0.9995194">
Henceforth, we assume that non-terminals on the
right-hand side are ordered by the order of their
first variable on the left-hand side. There are two
ways to binarize this production. The first is from
left to right:
</bodyText>
<equation confidence="0.979532666666667">
X(ps) →XAB(p) C(s) (2)
XAB(pqr) →A(p, r) B(q) (3)
This binarization maintains the fan-out of 1. The
second way is from right to left:
X(pqrs) →A(p, r) XBC(q, s) (4)
XBC(q, s) →B(q) C(s) (5)
</equation>
<footnote confidence="0.8654315">
1Other algorithms exist which support n-ary productions,
but these are less suitable for statistical treebank parsing.
</footnote>
<bodyText confidence="0.992755971428572">
This binarization introduces a production with
a fan-out of 2, which could have been avoided.
After binarization, an LCFRS can be parsed in
O(|G |· |w|p) time, where |G |is the size of the
grammar, |w |is the length of the sentence. The de-
gree p of the polynomial is the maximum parsing
complexity of a rule, defined as:
parsing complexity := ϕ + ϕ1 + ϕ2 (6)
where ϕ is the fan-out of the left-hand side and
ϕ1 and ϕ2 are the fan-outs of the right-hand side
of the rule in question (Gildea, 2010). As Gildea
(2010) shows, there is no one to one correspon-
dence between fan-out and parsing complexity: it
is possible that parsing complexity can be reduced
by increasing the fan-out of a production. In other
words, there can be a production which can be bi-
narized with a parsing complexity that is minimal
while its fan-out is sub-optimal. Therefore we fo-
cus on parsing complexity rather than fan-out in
this work, since parsing complexity determines the
actual time complexity of parsing with a grammar.
There has been some work investigating whether
the increase in complexity can be minimized ef-
fectively (G´omez-Rodr´ıguez et al., 2009; Gildea,
2010; Crescenzi et al., 2011).
More radically, it has been suggested that the
power of LCFRS should be limited to well-nested
structures, which gives an asymptotic improve-
ment in parsing time (G´omez-Rodr´ıguez et al.,
2010). However, there is linguistic evidence that
not all language use can be described in well-
nested structures (Chen-Main and Joshi, 2010).
Therefore we will use the full power of LCFRS in
this work—parsing complexity is determined by
the treebank, not by a priori constraints.
</bodyText>
<subsectionHeader confidence="0.998826">
3.1 Further binarization strategies
</subsectionHeader>
<bodyText confidence="0.999991692307692">
Apart from optimizing for parsing complexity, for
linguistic reasons it can also be useful to parse
the head of a constituent first, yielding so-called
head-driven binarizations (Collins, 1999). Addi-
tionally, such a head-driven binarization can be
‘Markovized’–i.e., the resulting production can be
constrained to apply to a limited amount of hor-
izontal context as opposed to the full context in
the original constituent (e.g., Klein and Manning,
2003), which can have a beneficial effect on accu-
racy. In the notation of Klein and Manning (2003)
there are two Markovization parameters: h and
v. The first parameter describes the amount of
</bodyText>
<page confidence="0.996449">
462
</page>
<figure confidence="0.985139809523809">
X
X
X
XD
X
X
XB,C,D,E
XC,D,E
B
B
XD,E
XB,C,D,E
XB,C,D
XB,C
B
XB
B XE
XD
XA
XB
B
</figure>
<equation confidence="0.9323362">
A X C Y
0 1 2 3
original
P = 4, V =
X C Y D E
1 2 3 4 5
right branching
P = 5, V = 2
X C Y D E
1 2 3 4 5
optimal
P = 4, V = 2
A X C Y D E
0 1 2 3 4 5
head-driven
P = 5, V = 2
X C Y D
1 2 3 4
optimal head-driven
P = 4, V = 2
</equation>
<figureCaption confidence="0.996395">
Figure 4: The four binarization strategies. C is the head node. Underneath each tree is the maximum parsing
complexity and fan-out among its productions.
</figureCaption>
<bodyText confidence="0.999976697674419">
horizontal context for the artificial labels of a bi-
narized production. In a normal form binarization,
this parameter equals infinity, because the bina-
rized production should only apply in the exact
same context as the context in which it originally
belongs, as otherwise the set of strings accepted
by the grammar would be affected. An artificial
label will have the form XA�B�C for a binarized
production of a constituent X that has covered
children A, B, and C of X. The other extreme,
h = 1, enables generalizations by stringing parts
of binarized constituents together, as long as they
share one non-terminal. In the previous example,
the label would become just XA, i.e., the pres-
ence of B and C would no longer be required,
which enables switching to any binarized produc-
tion that has covered A as the last node. Limit-
ing the amount of horizontal context on which a
production is conditioned is important when the
treebank contains many unique constituents which
can only be parsed by stringing together different
binarized productions; in other words, it is a way
of dealing with the data sparseness about n-ary
productions in the treebank.
The second parameter describes parent annota-
tion, which will not be investigated in this work;
the default value is v = 1 which implies only in-
cluding the immediate parent of the constituent
that is being binarized; including grandparents is a
way of weakening independence assumptions.
Crescenzi et al. (2011) also remark that
an optimal head-driven binarization allows for
Markovization. However, it is questionable
whether such a binarization is worthy of the name
Markovization, as the non-terminals are not intro-
duced deterministically from left to right, but in
an arbitrary fashion dictated by concerns of pars-
ing complexity; as such there is not a Markov
process based on a meaningful (e.g., temporal) or-
dering and there is no probabilistic interpretation
of Markovization in such a setting.
To summarize, we have at least four binarization
strategies (cf. figure 4 for an illustration):
</bodyText>
<listItem confidence="0.9975874">
1. right branching: A right-to-left binarization.
No regard for optimality or statistical tweaks.
2. optimal: A binarization which minimizes pars-
ing complexity, introduced in Gildea (2010).
Binarizing with this strategy is exponential in
the resulting optimal fan-out (Gildea, 2010).
3. head-driven: Head-outward binarization with
horizontal Markovization. No regard for opti-
mality.
4. optimal head-driven: Head-outward binariza-
</listItem>
<bodyText confidence="0.5747175">
tion with horizontal Markovization. Min-
imizes parsing complexity. Introduced in
and proven to be NP-hard by Crescenzi et al.
(2011).
</bodyText>
<subsectionHeader confidence="0.999802">
3.2 Finding optimal binarizations
</subsectionHeader>
<bodyText confidence="0.99990425">
An issue with the minimal binarizations is that
the algorithm for finding them has a high compu-
tational complexity, and has not been evaluated
empirically on treebank data.2 Empirical inves-
tigation is interesting for two reasons. First of
all, the high computational complexity may not
be relevant with constant factors of constituents,
which can reasonably be expected to be relatively
small. Second, it is important to establish whether
an asymptotic improvement is actually obtained
through optimal binarizations, and whether this
translates to an improvement in practice.
Gildea (2010) presents a general algorithm to
binarize an LCFRS while minimizing a given scor-
ing function. We will use this algorithm with two
different scoring functions.
</bodyText>
<footnote confidence="0.966151333333333">
2Gildea (2010) evaluates on a dependency bank, but does
not report whether any improvement is obtained over a naive
binarization.
</footnote>
<page confidence="0.999386">
463
</page>
<figure confidence="0.999694833333333">
3 4 5 6 7 8 9
Parsing complexity
right branching
optimal
head-driven
optimal head-driven
Frequency 100000
10000
1000
100
10
1
Frequency 100000
10000
1000
100
10
1
</figure>
<figureCaption confidence="0.772892333333333">
Figure 5: The distribution of parsing complexity
among productions in binarized grammars read off from
NEGRA-25. The y-axis has a logarithmic scale.
</figureCaption>
<bodyText confidence="0.999983833333333">
The first directly optimizes parsing complexity.
Given a (partially) binarized constituent c, the func-
tion returns a tuple of scores, for which a linear
order is defined by comparing elements starting
from the most significant (left-most) element. The
tuples contain the parsing complexity p, and the
fan-out ϕ to break ties in parsing complexity; if
there are still ties after considering the fan-out, the
sum of the parsing complexities of the subtrees of
c is considered, which will give preference to a bi-
narization where the worst case complexity occurs
once instead of twice. The formula is then:
</bodyText>
<equation confidence="0.96288">
opt(c) = hp, ϕ, si
</equation>
<bodyText confidence="0.999756">
The second function is the similar except that
only head-driven strategies are accepted. A head-
driven strategy is a binarization in which the head
is introduced first, after which the rest of the chil-
dren are introduced one at a time.
</bodyText>
<equation confidence="0.963063666666667">
r
opt-hd(c) (p, ϕ, si if c is head-driven
= Sl h∞, ∞, ∞i
</equation>
<bodyText confidence="0.999742857142857">
Given a (partial) binarization c, the score should
reflect the maximum complexity and fan-out in
that binarization, to optimize for the worst case, as
well as the sum, to optimize the average case. This
aspect appears to be glossed over by Gildea (2010).
Considering only the score of the last production in
a binarization produces suboptimal binarizations.
</bodyText>
<subsectionHeader confidence="0.934015">
3.3 Experiments
</subsectionHeader>
<bodyText confidence="0.95382">
As data we use version 2 of the Negra (Skut et al.,
1997) treebank, with the common training, devel-
</bodyText>
<figureCaption confidence="0.999040666666667">
Figure 6: The distribution of parsing complexity among
productions in Markovized, head-driven grammars read
off from NEGRA-25. The y-axis has a logarithmic scale.
</figureCaption>
<bodyText confidence="0.9998826">
opment and test splits (Dubey and Keller, 2003).
Following common practice, punctuation, which
is left out of the phrase-structure in Negra, is re-
attached to the nearest constituent.
In the course of experiments it was discovered
that the heuristic method for punctuation attach-
ment used in previous work (e.g., Maier, 2010;
van Cranenburgh et al., 2011), as implemented in
rparse,3 introduces additional discontinuity. We
applied a slightly different heuristic: punctuation
is attached to the highest constituent that contains a
neighbor to its right. The result is that punctuation
can be introduced into the phrase-structure with-
out any additional discontinuity, and thus without
artificially inflating the fan-out and complexity of
grammars read off from the treebank. This new
heuristic provides a significant improvement: in-
stead of a fan-out of 9 and a parsing complexity of
19, we obtain values of 4 and 9 respectively.
The parser is presented with the gold part-of-
speech tags from the corpus. For reasons of effi-
ciency we restrict sentences to 25 words (includ-
ing punctuation) in this experiment: NEGRA-25.
A grammar was read off from the training part
of NEGRA-25, and sentences of up to 25 words
in the development set were parsed using the re-
sulting PLCFRS, using the different binarization
schemes. First with aright-branching, right-to-left
binarization, and second with the minimal bina-
rization according to parsing complexity andfan-
</bodyText>
<figure confidence="0.692147666666667">
3 4 5 6 7 8 9
Parsing complexity
from
</figure>
<footnote confidence="0.8224">
rparse/downloads. Retri
3Available
http://www.wolfgang-maier.net/
eved March 25th, 2011
</footnote>
<table confidence="0.915370846153846">
otherwise
464
right optimal head-driven optimal
branching head-driven
Markovization v=1, h=oo v=1, h=oo v=1, h=2 v=1, h=2
fan-out 4 4 4 4
complexity 8 8 9 8
labels 12861 12388 4576 3187
clauses 62072 62097 53050 52966
time to binarize 1.83 s 46.37 s 2.74 s 28.9 s
time to parse 246.34 s 193.94 s 2860.26 s 716.58 s
coverage 96.08 % 96.08 % 98.99 % 98.73 %
Fi score 66.83 % 66.75 % 72.37 % 71.79 %
</table>
<tableCaption confidence="0.9940085">
Table 1: The effect of binarization strategies on parsing efficiency, with sentences from the development section of
NEGRA-25.
</tableCaption>
<bodyText confidence="0.998754515151515">
out. The last two binarizations are head-driven
and Markovized—the first straightforwardly from
left-to-right, the latter optimized for minimal pars-
ing complexity. With Markovization we are forced
to add a level of parent annotation to tame the
increase in productivity caused by h = 1.
The distribution of parsing complexity (mea-
sured with eq. 6) in the grammars with different
binarization strategies is shown in figure 5 and
6. Although the optimal binarizations do seem
to have some effect on the distribution of parsing
complexities, it remains to be seen whether this
can be cashed out as a performance improvement
in practice. To this end, we also parse using the
binarized grammars.
In this work we binarize and parse with
disco-dop introduced in van Cranenburgh et al.
(2011).4 In this experiment we report scores of the
(exact) Viterbi derivations of a treebank PLCFRS;
cf. table 1 for the results. Times represent CPU
time (single core); accuracy is given with a gener-
alization of PARSEVAL to discontinuous structures,
described in Maier (2010).
Instead of using Maier’s implementation of dis-
continuous Fi scores in rparse, we employ a vari-
ant that ignores (a) punctuation, and (b) the root
node of each tree. This makes our evaluation in-
comparable to previous results on discontinuous
parsing, but brings it in line with common practice
on the Wall street journal benchmark. Note that
this change yields scores about 2 or 3 percentage
points lower than those of rparse.
Despite the fact that obtaining optimal bina-
</bodyText>
<footnote confidence="0.8061245">
4All code is available from: http://github.com/
andreasvc/disco-dop.
</footnote>
<bodyText confidence="0.999903133333333">
rizations is exponential (Gildea, 2010) and NP-
hard (Crescenzi et al., 2011), they can be computed
relatively quickly on this data set.5 Importantly, in
the first case there is no improvement on fan-out
or parsing complexity, while in the head-driven
case there is a minimal improvement because of a
single production with parsing complexity 15 with-
out optimal binarization. On the other hand, the
optimal binarizations might still have a significant
effect on the average case complexity, rather than
the worst-case complexities. Indeed, in both cases
parsing with the optimal grammar is faster; in the
first case, however, when the time for binariza-
tion is considered as well, this advantage mostly
disappears.
The difference in Fi scores might relate to the
efficacy of Markovization in the binarizations. It
should be noted that it makes little theoretical
sense to ‘Markovize’ a binarization when it is not
a left-to-right or right-to-left binarization, because
with an optimal binarization the non-terminals of
a constituent are introduced in an arbitrary order.
More importantly, in our experiments, these
techniques of optimal binarizations did not scale
to longer sentences. While it is possible to obtain
an optimal binarization of the unrestricted Negra
corpus, parsing long sentences with the resulting
grammar remains infeasible. Therefore we need to
look at other techniques for parsing longer sen-
tences. We will stick with the straightforward
</bodyText>
<footnote confidence="0.683462">
5The implementation exploits two important optimiza-
tions. The first is the use of bit vectors to keep track of which
non-terminals are covered by a partial binarization. The sec-
ond is to skip constituents without discontinuity, which are
equivalent to CFG productions.
</footnote>
<page confidence="0.999446">
465
</page>
<bodyText confidence="0.999932125">
head-driven, head-outward binarization strategy,
despite this being a computationally sub-optimal
binarization.
One technique for efficient parsing of LCFRS is
the use of context-summary estimates (Kallmeyer
and Maier, 2010), as part of a best-first parsing
algorithm. This allowed Maier (2010) to parse
sentences of up to 30 words. However, the calcu-
lation of these estimates is not feasible for longer
sentences and large grammars (van Cranenburgh
et al., 2011).
Another strategy is to perform an online approx-
imation of the sentence to be parsed, after which
parsing with the LCFRS can be pruned effectively.
This is the strategy that will be explored in the
current work.
</bodyText>
<sectionHeader confidence="0.7488435" genericHeader="method">
4 Context-free grammar approximation
for coarse-to-fine parsing
</sectionHeader>
<bodyText confidence="0.999807361445783">
Coarse-to-fine parsing (Charniak et al., 2006) is
a technique to speed up parsing by exploiting the
information that can be gained from parsing with
simpler, coarser grammars—e.g., a grammar with
a smaller set of labels on which the original gram-
mar can be projected. Constituents that do not
contribute to a full parse tree with a coarse gram-
mar can be ruled out for finer grammars as well,
which greatly reduces the number of edges that
need to be explored. However, by changing just
the labels only the grammar constant is affected.
With discontinuous treebank parsing the asymp-
totic complexity of the grammar also plays a major
role. Therefore we suggest to parse not just with
a coarser grammar, but with a coarser grammar
formalism, following a suggestion in van Cranen-
burgh et al. (2011).
This idea is inspired by the work of Barth´elemy
et al. (2001), who apply it in a non-probabilistic
setting where the coarse grammar acts as a guide to
the non-deterministic choices of the fine grammar.
Within the coarse-to-fine approach the technique
becomes a matter of pruning with some probabilis-
tic threshold. Instead of using the coarse gram-
mar only as a guide to solve non-deterministic
choices, we apply it as a pruning step which also
discards the most suboptimal parses. The basic
idea is to extract a grammar that defines a superset
of the language we want to parse, but with a fan-
out of 1. More concretely, a context-free grammar
can be read off from discontinuous trees that have
been transformed to context-free trees by the pro-
cedure introduced in Boyd (2007). Each discontin-
uous node is split into a set of new nodes, one for
each component; for example a node NP2 will be
split into two nodes labeled NP*1 and NP*2 (like
Barth´elemy et al., we mark components with an
index to reduce overgeneration). Because Boyd’s
transformation is reversible, chart items from this
grammar can be converted back to discontinuous
chart items, and can guide parsing of an LCFRS.
This guiding takes the form of a white list. Af-
ter parsing with the coarse grammar, the result-
ing chart is pruned by removing all items that
fail to meet a certain criterion. In our case this
is whether a chart item is part of one of the k-best
derivations—we use k = 50 in all experiments (as
in van Cranenburgh et al., 2011). This has simi-
lar effects as removing items below a threshold
of marginalized posterior probability; however,
the latter strategy requires computation of outside
probabilities from a parse forest, which is more
involved with an LCFRS than with a PCFG. When
parsing with the fine grammar, whenever a new
item is derived, the white list is consulted to see
whether this item is allowed to be used in further
derivations; otherwise it is immediately discarded.
This coarse-to-fine approach will be referred to as
CFG-CTF, and the transformed, coarse grammar
will be referred to as a split-PCFG.
Splitting discontinuous nodes for the coarse
grammar introduces new nodes, so obviously we
need to binarize after this transformation. On the
other hand, the coarse-to-fine approach requires a
mapping between the grammars, so after reversing
the transformation of splitting nodes, the resulting
discontinuous trees must be binarized (and option-
ally Markovized) in the same manner as those on
which the fine grammar is based.
To resolve this tension we elect to binarize twice.
The first time is before splitting discontinuous
nodes, and this is where we introduce Markoviza-
tion. This same binarization will be used for the
fine grammar as well, which ensures the models
make the same kind of generalizations. The sec-
ond binarization is after splitting nodes, this time
with a binary normal form (2NF; all productions
are either unary, binary, or lexical).
Parsing with this grammar proceeds as fol-
lows. After obtaining an exhaustive chart from
the coarse stage, the chart is pruned so as to only
contain items occurring in the k-best derivations.
When parsing in the fine stage, each new item is
</bodyText>
<page confidence="0.998732">
466
</page>
<table confidence="0.97244075">
S S
SA SA
S SB
SA
SB B&apos;0 SB : SC&apos;0,B&apos;1,SC&apos;1
B SB SC&apos;0 SB : B&apos;1,SC&apos;1
B&apos;1 SC&apos;1
SD
SC B&apos;0 SC&apos;0 B&apos;1 SC&apos;1
SD SD
S
B SE SE SE
Y D E Y D E X C Y D E
3 4 5 3 4 5 1 2 3 4 5
A X C Y D
0 1 2 3 4
</table>
<figureCaption confidence="0.831564">
Figure 7: Transformations for a context-free coarse grammar. From left to right: the original constituent,
Markovized with v = 1, h = 1, discontinuities resolved, normal form (second binarization).
</figureCaption>
<figure confidence="0.461968125">
model train dev test rules labels fan-out
Split-PCFG 17988 975 968 57969 2026 1
PLCFRS 17988 975 968 55778 947 4
Disco-DOP 17988 975 968 2657799 702246 4
complexity
3
9
9
</figure>
<tableCaption confidence="0.99544">
Table 2: Some statistics on the coarse and fine grammars read off from NEGRA-40.
</tableCaption>
<bodyText confidence="0.9982228">
looked up in this pruned coarse chart, with multi-
ple lookups if the item is discontinuous (one for
each component).
To summarize, the transformation happens in
four steps (cf. figure 7 for an illustration):
</bodyText>
<listItem confidence="0.979356">
1. Treebank tree: Original (discontinuous) tree
2. Binarization: Binarize discontinuous tree, op-
tionally with Markovization
3. Resolve discontinuity: Split discontinuous
nodes into components, marked with indices
4. 2NF: A binary normal form is applied; all pro-
ductions are either unary, binary, or lexical.
</listItem>
<sectionHeader confidence="0.993636" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999970066666667">
We evaluate on Negra with the same setup as in
section 3.3. We report discontinuous Fi scores as
well as exact match scores. For previous results on
discontinuous parsing with Negra, see table 3. For
results with the CFG-CTF method see table 4.
We first establish the viability of the CFG-CTF
method on NEGRA-25, with a head-driven v = 1,
h = 2 binarization, and reporting again the scores
of the exact Viterbi derivations from a treebank
PLCFRS versus a PCFG using our transformations.
Figure 8 compares the parsing times of LCFRS
with and without the new CFG-CTF method. The
graph shows a steep incline for parsing with LCFRS
directly, which makes it infeasible to parse longer
sentences, while the CFG-CTF method is faster for
</bodyText>
<figure confidence="0.9563105">
0 5 10 15 20 25
Sentence length
</figure>
<figureCaption confidence="0.9779948">
Figure 8: Efficiency of parsing PLCFRS with and with-
out coarse-to-fine. The latter includes time for both
coarse &amp; fine grammar. Datapoints represent the aver-
age time to parse sentences of that length; each length
is made up of 20–40 sentences.
</figureCaption>
<bodyText confidence="0.995723111111111">
sentences of length &gt; 22 despite its overhead of
parsing twice.
The second experiment demonstrates the CFG-
CTF technique on longer sentences. We restrict the
length of sentences in the training, development
and test corpora to 40 words: NEGRA-40. As a first
step we apply the CFG-CTF technique to parse with
a PLCFRS as the fine grammar, pruning away all
items not occurring in the 10,000 best derivations
</bodyText>
<figure confidence="0.992107384615385">
cpu time (s)
45
40
35
30
25
20
15
10
0
5
PLCFRS
CFG-CTF (Split-PCFG ==&gt;. PLCFRS)
</figure>
<page confidence="0.987132">
467
</page>
<table confidence="0.7881094">
words PARSEVAL Exact
(F1) match
DPSG: Plaehn (2004) &lt; 15 73.16 39.0
PLCFRS: Maier (2010) &lt; 30 71.52 31.65
Disco-DOP: van Cranenburgh et al. (2011) &lt; 30 73.98 34.80
</table>
<tableCaption confidence="0.998405">
Table 3: Previous work on discontinuous parsing of Negra.
</tableCaption>
<table confidence="0.9987723">
words PARSEVAL Exact
(F1) match
PLCFRS, dev set &lt; 25 72.37 36.58
Split-PCFG, dev set &lt; 25 70.74 33.80
Split-PCFG, dev set &lt; 40 66.81 27.59
CFG-CTF, PLCFRS, dev set &lt; 40 67.26 27.90
CFG-CTF, Disco-DOP, dev set &lt; 40 74.27 34.26
CFG-CTF, Disco-DOP, test set &lt; 40 72.33 33.16
CFG-CTF, Disco-DOP, dev set 00 73.32 33.40
CFG-CTF, Disco-DOP, test set 00 71.08 32.10
</table>
<tableCaption confidence="0.918992666666667">
Table 4: Results on NEGRA-25 and NEGRA-40 with the CFG-CTF method. NB: As explained in section 3.3, these
Fl scores are incomparable to the results in table 3; for comparison, the Fl score for Disco-DOP on the dev set
&lt; 40 is 77.13 % using that evaluation scheme.
</tableCaption>
<bodyText confidence="0.999955870967742">
from the PCFG chart. The result shows that the
PLCFRS gives a slight improvement over the split--
pcfg, which accords with the observation that the
latter makes stronger independence assumptions
in the case of discontinuity.
In the next experiments we turn to an all-
fragments grammar encoded in a PLCFRS using
Goodman’s (2003) reduction, to realize a (dis-
continuous) Data-Oriented Parsing (DOP; Scha,
1990) model—which goes by the name of Disco-
DOP (van Cranenburgh et al., 2011). This provides
an effective yet conceptually simple method to
weaken the independence assumptions of treebank
grammars. Table 2 gives statistics on the gram-
mars, including the parsing complexities. The fine
grammar has a parsing complexity of 9, which
means that parsing with this grammar has com-
plexity O(|w|9). We use the same parameters as
van Cranenburgh et al. (2011), except that unlike
van Cranenburgh et al., we can use v = 1, h = 1
Markovization, in order to obtain a higher cover-
age. The DOP grammar is added as a third stage in
the coarse-to-fine pipeline. This gave slightly bet-
ter results than substituting the the DOP grammar
for the PLCFRS stage. Parsing with NEGRA-40
took about 11 hours and 4 GB of memory. The
same model from NEGRA-40 can also be used to
parse the full development set, without length re-
strictions, establishing that the CFG-CTF method
effectively eliminates any limitation of length for
parsing with LCFRS.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999986666666667">
Our results show that optimal binarizations are
clearly not the answer to parsing LCFRS efficiently,
as they do not significantly reduce parsing com-
plexity in our experiments. While they provide
some efficiency gains, they do not help with the
main problem of longer sentences.
We have presented a new technique for large-
scale parsing with LCFRS, which makes it possible
to parse sentences of any length, with favorable
accuracies. The availability of this technique may
lead to a wider acceptance of LCFRS as a syntactic
backbone in computational linguistics.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.975312">
I am grateful to Willem Zuidema, Remko Scha,
Rens Bod, and three anonymous reviewers for
comments.
</bodyText>
<page confidence="0.999264">
468
</page>
<sectionHeader confidence="0.996199" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997484776595745">
Franc¸ois Barth´elemy, Pierre Boullier, Philippe De-
schamp, and ´Eric de la Clergerie. 2001. Guided
parsing of range concatenation languages. In
Proc. of ACL, pages 42–49.
Pierre Boullier. 1998. Proposal for a natural lan-
guage processing syntactic backbone. Techni-
cal Report RR-3342, INRIA-Rocquencourt, Le
Chesnay, France. URL http://www.inria.
fr/RRRT/RR-3342.html.
Adriane Boyd. 2007. Discontinuity revisited: An
improved conversion to context-free representa-
tions. In Proceedings of the Linguistic Annota-
tion Workshop, pages 41–44.
Sabine Brants, Stefanie Dipper, Silvia Hansen,
Wolfgang Lezius, and George Smith. 2002. The
Tiger treebank. In Proceedings of the workshop
on treebanks and linguistic theories, pages 24–
41.
Eugene Charniak, Mark Johnson, M. Elsner,
J. Austerweil, D. Ellis, I. Haxton, C. Hill,
R. Shrivaths, J. Moore, M. Pozar, et al. 2006.
Multilevel coarse-to-fine PCFG parsing. In Pro-
ceedings of NAACL-HLT, pages 168–175.
Joan Chen-Main and Aravind K. Joshi. 2010. Un-
avoidable ill-nestedness in natural language and
the adequacy of tree local-mctag induced depen-
dency structures. In Proceedings of TAG+. URL
http://www.research.att.com/∼srini/
TAG+10/papers/chenmainjoshi.pdf.
Michael Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. the-
sis, University of Pennsylvania.
Pierluigi Crescenzi, Daniel Gildea, Aandrea
Marino, Gianluca Rossi, and Giorgio Satta.
2011. Optimal head-driven parsing complex-
ity for linear context-free rewriting systems. In
Proc. of ACL.
Amit Dubey and Frank Keller. 2003. Parsing ger-
man with sister-head dependencies. In Proc. of
ACL, pages 96–103.
Kilian Evang and Laura Kallmeyer. 2011.
PLCFRS parsing of English discontinuous con-
stituents. In Proceedings of IWPT, pages 104–
116.
Daniel Gildea. 2010. Optimal parsing strategies
for linear context-free rewriting systems. In
Proceedings of NAACL HLT 2010., pages 769–
776.
Carlos G´omez-Rodr´ıguez, Marco Kuhlmann, and
Giorgio Satta. 2010. Efficient parsing of well-
nested linear context-free rewriting systems. In
Proceedings of NAACL HLT 2010., pages 276–
284.
Carlos G´omez-Rodr´ıguez, Marco Kuhlmann, Gior-
gio Satta, and David Weir. 2009. Optimal reduc-
tion of rule length in linear context-free rewrit-
ing systems. In Proceedings of NAACL HLT
2009, pages 539–547.
Joshua Goodman. 2003. Efficient parsing of
DOP with PCFG-reductions. In Rens Bod,
Remko Scha, and Khalil Sima’an, editors, Data-
Oriented Parsing. The University of Chicago
Press.
Laura Kallmeyer. 2010. Parsing Beyond Context-
Free Grammars. Cognitive Technologies.
Springer Berlin Heidelberg.
Laura Kallmeyer, Timm Lichte, Wolfgang Maier,
Yannick Parmentier, Johannes Dellert, and Kil-
ian Evang. 2008. Tulipa: Towards a multi-
formalism parsing environment for grammar
engineering. In Proceedings of the Workshop
on Grammar Engineering Across Frameworks,
pages 1–8.
Laura Kallmeyer and Wolfgang Maier. 2010. Data-
driven parsing with probabilistic linear context-
free rewriting systems. In Proceedings of the
23rd International Conference on Computa-
tional Linguistics, pages 537–545.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL,
volume 1, pages 423–430.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective de-
pendency parsing. In Proceedings of EACL,
pages 478–486.
Roger Levy. 2005. Probabilistic models of word
order and syntactic discontinuity. Ph.D. thesis,
Stanford University.
Wolfgang Maier. 2010. Direct parsing of discon-
tinuous constituents in German. In Proceedings
of the SPMRL workshop at NAACL HLT 2010,
pages 58–66.
Wolfgang Maier and Timm Lichte. 2009. Charac-
terizing discontinuity in constituent treebanks.
</reference>
<page confidence="0.998869">
469
</page>
<note confidence="0.533083666666667">
Ph.D. thesis, University of Pennsylvania.
URL http://repository.upenn.edu/
dissertations/AAI8908403/.
</note>
<reference confidence="0.999745541666667">
In Proceedings of Formal Grammar 2009, pages
167–182. Springer.
Wolfgang Maier and Anders Søgaard. 2008. Tree-
banks and mild context-sensitivity. In Proceed-
ings of Formal Grammar 2008, page 61.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large an-
notated corpus of english: The penn treebank.
Computational linguistics, 19(2):313–330.
James D. McCawley. 1982. Parentheticals and
discontinuous constituent structure. Linguistic
Inquiry, 13(1):91–106.
Oliver Plaehn. 2004. Computing the most prob-
able parse for a discontinuous phrase structure
grammar. In Harry Bunt, John Carroll, and Gior-
gio Satta, editors, New developments in parsing
technology, pages 91–106. Kluwer Academic
Publishers, Norwell, MA, USA.
Remko Scha. 1990. Language theory and language
technology; competence and performance. In
Q.A.M. de Kort and G.L.J. Leerdam, editors,
Computertoepassingen in de Neerlandistiek,
pages 7–22. LVVN, Almere, the Netherlands.
Original title: Taaltheorie en taaltechnologie;
competence en performance. Translation avail-
able at http://iaaa.nl/rs/LeerdamE.html.
Stuart M. Shieber. 1985. Evidence against the
context-freeness of natural language. Linguis-
tics and Philosophy, 8:333–343.
Wojciech Skut, Brigitte Krenn, Thorten Brants,
and Hans Uszkoreit. 1997. An annotation
scheme for free word order languages. In Pro-
ceedings of ANLP, pages 88–95.
Andreas van Cranenburgh, Remko Scha, and
Federico Sangati. 2011. Discontinuous data-
oriented parsing: A mildly context-sensitive all-
fragments grammar. In Proceedings of SPMRL,
pages 34–44.
K. Vijay-Shanker and David J. Weir. 1994. The
equivalence of four extensions of context-free
grammars. Theory of Computing Systems,
27(6):511–546.
K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descrip-
tions produced by various grammatical for-
malisms. In Proc. ofACL, pages 104–111.
David J. Weir. 1988. Characterizing mildly
context-sensitive grammar formalisms.
</reference>
<page confidence="0.998194">
470
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235301">
<title confidence="0.999992">Efficient Parsing with Linear Context-Free Rewriting Systems</title>
<author confidence="0.994847">Andreas van</author>
<note confidence="0.370883666666667">Huygens ING &amp; ILLC, University of Royal Netherlands Academy of Arts and Postbus 90754, 2509 LT The Hague, the Netherlands</note>
<email confidence="0.946886">andreas.van.cranenburgh@huygens.knaw.nl</email>
<abstract confidence="0.998270882352941">Previous work on treebank parsing with discontinuous constituents using Linear Rewriting systems has been limited to sentences of up to 30 words, for reasons of computational complexity. There have been some results on an a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible. Instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy. The resulting parser has been applied to a discontinuous treebank with favorable results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Franc¸ois Barth´elemy</author>
<author>Pierre Boullier</author>
<author>Philippe Deschamp</author>
<author>´Eric de la Clergerie</author>
</authors>
<title>Guided parsing of range concatenation languages.</title>
<date>2001</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>42--49</pages>
<marker>Barth´elemy, Boullier, Deschamp, Clergerie, 2001</marker>
<rawString>Franc¸ois Barth´elemy, Pierre Boullier, Philippe Deschamp, and ´Eric de la Clergerie. 2001. Guided parsing of range concatenation languages. In Proc. of ACL, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
</authors>
<title>Proposal for a natural language processing syntactic backbone.</title>
<date>1998</date>
<tech>Technical Report RR-3342, INRIA-Rocquencourt,</tech>
<institution>Le Chesnay,</institution>
<note>URL http://www.inria. fr/RRRT/RR-3342.html.</note>
<contexts>
<context position="6040" citStr="Boullier, 1998" startWordPosition="947" endWordPosition="948"> c $.(.) —* E Figure 3: The productions that can be read off from the tree in figure 2. Note that lexical productions rewrite to e, because they do not rewrite to any non-terminals. terminal may cover a tuple of discontinuous strings instead of a single, contiguous sequence of terminals. The number of components in such a tuple is called the fan-out of a rule, which is equal to the number of gaps plus one; the fan-out of the grammar is the maximum fan-out of its production. A context-free grammar is a LCFRS with a fan-out of 1. For convenience we will will use the rule notation of simple RCG (Boullier, 1998), which is a syntactic variant of LCFRS, with an arguably more transparent notation. A LCFRS is a tuple G = (N, T, V, P, 5). N is a finite set of non-terminals; a function dim : N —* N specifies the unique fan-out for every nonterminal symbol. T and V are disjoint finite sets of terminals and variables. 5 is the distinguished start symbol with dim(5) = 1. P is a finite set of rewrite rules (productions) of the form: A(α1, ... αdim(A)) —*B1(X11, ... , X1dim(B1)) . . .Bm(Xm1 ,... , Xmdim(B.)) for m &gt; 0, where A, B1, ..., Bm E N, each Xij E V for 1 &lt; i &lt; m, 1 &lt; j &lt; dim(Aj) and αi E (T U V)* for 1</context>
</contexts>
<marker>Boullier, 1998</marker>
<rawString>Pierre Boullier. 1998. Proposal for a natural language processing syntactic backbone. Technical Report RR-3342, INRIA-Rocquencourt, Le Chesnay, France. URL http://www.inria. fr/RRRT/RR-3342.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriane Boyd</author>
</authors>
<title>Discontinuity revisited: An improved conversion to context-free representations.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop,</booktitle>
<pages>41--44</pages>
<contexts>
<context position="25114" citStr="Boyd (2007)" startWordPosition="4128" endWordPosition="4129">erministic choices of the fine grammar. Within the coarse-to-fine approach the technique becomes a matter of pruning with some probabilistic threshold. Instead of using the coarse grammar only as a guide to solve non-deterministic choices, we apply it as a pruning step which also discards the most suboptimal parses. The basic idea is to extract a grammar that defines a superset of the language we want to parse, but with a fanout of 1. More concretely, a context-free grammar can be read off from discontinuous trees that have been transformed to context-free trees by the procedure introduced in Boyd (2007). Each discontinuous node is split into a set of new nodes, one for each component; for example a node NP2 will be split into two nodes labeled NP*1 and NP*2 (like Barth´elemy et al., we mark components with an index to reduce overgeneration). Because Boyd’s transformation is reversible, chart items from this grammar can be converted back to discontinuous chart items, and can guide parsing of an LCFRS. This guiding takes the form of a white list. After parsing with the coarse grammar, the resulting chart is pruned by removing all items that fail to meet a certain criterion. In our case this is</context>
</contexts>
<marker>Boyd, 2007</marker>
<rawString>Adriane Boyd. 2007. Discontinuity revisited: An improved conversion to context-free representations. In Proceedings of the Linguistic Annotation Workshop, pages 41–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The Tiger treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the workshop on treebanks and linguistic theories,</booktitle>
<pages>24--41</pages>
<contexts>
<context position="2102" citStr="Brants et al., 2002" startWordPosition="312" endWordPosition="315">tences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the Penn treebank, in which traces have been converted to discontinuity. Taken from Evang and Kallmeyer (2011). 1997) and Tiger (Brants et al., 2002) corpora, or those that can be extracted from traces such as in the Penn treebank (Marcus et al., 1993) annotation. However, the computational complexity is such that until now, the length of sentences needed to be restricted. In the case of Kallmeyer and Maier (2010) and Evang and Kallmeyer (2011) the limit was 25 words. Maier (2010) and van Cranenburgh et al. (2011) manage to parse up to 30 words with heuristics and optimizations, but no further. Algorithms have been suggested to binarize the grammars in such a way as to minimize parsing complexity, but the current paper shows that these tec</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The Tiger treebank. In Proceedings of the workshop on treebanks and linguistic theories, pages 24– 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>M Elsner</author>
<author>J Austerweil</author>
<author>D Ellis</author>
<author>I Haxton</author>
<author>C Hill</author>
<author>R Shrivaths</author>
<author>J Moore</author>
<author>M Pozar</author>
</authors>
<title>Multilevel coarse-to-fine PCFG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>168--175</pages>
<contexts>
<context position="23590" citStr="Charniak et al., 2006" startWordPosition="3864" endWordPosition="3867">use of context-summary estimates (Kallmeyer and Maier, 2010), as part of a best-first parsing algorithm. This allowed Maier (2010) to parse sentences of up to 30 words. However, the calculation of these estimates is not feasible for longer sentences and large grammars (van Cranenburgh et al., 2011). Another strategy is to perform an online approximation of the sentence to be parsed, after which parsing with the LCFRS can be pruned effectively. This is the strategy that will be explored in the current work. 4 Context-free grammar approximation for coarse-to-fine parsing Coarse-to-fine parsing (Charniak et al., 2006) is a technique to speed up parsing by exploiting the information that can be gained from parsing with simpler, coarser grammars—e.g., a grammar with a smaller set of labels on which the original grammar can be projected. Constituents that do not contribute to a full parse tree with a coarse grammar can be ruled out for finer grammars as well, which greatly reduces the number of edges that need to be explored. However, by changing just the labels only the grammar constant is affected. With discontinuous treebank parsing the asymptotic complexity of the grammar also plays a major role. Therefor</context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, 2006</marker>
<rawString>Eugene Charniak, Mark Johnson, M. Elsner, J. Austerweil, D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, et al. 2006. Multilevel coarse-to-fine PCFG parsing. In Proceedings of NAACL-HLT, pages 168–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Chen-Main</author>
<author>Aravind K Joshi</author>
</authors>
<title>Unavoidable ill-nestedness in natural language and the adequacy of tree local-mctag induced dependency structures.</title>
<date>2010</date>
<booktitle>In Proceedings of TAG+. URL http://www.research.att.com/∼srini/ TAG+10/papers/chenmainjoshi.pdf.</booktitle>
<contexts>
<context position="10720" citStr="Chen-Main and Joshi, 2010" startWordPosition="1768" endWordPosition="1771">ather than fan-out in this work, since parsing complexity determines the actual time complexity of parsing with a grammar. There has been some work investigating whether the increase in complexity can be minimized effectively (G´omez-Rodr´ıguez et al., 2009; Gildea, 2010; Crescenzi et al., 2011). More radically, it has been suggested that the power of LCFRS should be limited to well-nested structures, which gives an asymptotic improvement in parsing time (G´omez-Rodr´ıguez et al., 2010). However, there is linguistic evidence that not all language use can be described in wellnested structures (Chen-Main and Joshi, 2010). Therefore we will use the full power of LCFRS in this work—parsing complexity is determined by the treebank, not by a priori constraints. 3.1 Further binarization strategies Apart from optimizing for parsing complexity, for linguistic reasons it can also be useful to parse the head of a constituent first, yielding so-called head-driven binarizations (Collins, 1999). Additionally, such a head-driven binarization can be ‘Markovized’–i.e., the resulting production can be constrained to apply to a limited amount of horizontal context as opposed to the full context in the original constituent (e.</context>
</contexts>
<marker>Chen-Main, Joshi, 2010</marker>
<rawString>Joan Chen-Main and Aravind K. Joshi. 2010. Unavoidable ill-nestedness in natural language and the adequacy of tree local-mctag induced dependency structures. In Proceedings of TAG+. URL http://www.research.att.com/∼srini/ TAG+10/papers/chenmainjoshi.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="11089" citStr="Collins, 1999" startWordPosition="1825" endWordPosition="1826">well-nested structures, which gives an asymptotic improvement in parsing time (G´omez-Rodr´ıguez et al., 2010). However, there is linguistic evidence that not all language use can be described in wellnested structures (Chen-Main and Joshi, 2010). Therefore we will use the full power of LCFRS in this work—parsing complexity is determined by the treebank, not by a priori constraints. 3.1 Further binarization strategies Apart from optimizing for parsing complexity, for linguistic reasons it can also be useful to parse the head of a constituent first, yielding so-called head-driven binarizations (Collins, 1999). Additionally, such a head-driven binarization can be ‘Markovized’–i.e., the resulting production can be constrained to apply to a limited amount of horizontal context as opposed to the full context in the original constituent (e.g., Klein and Manning, 2003), which can have a beneficial effect on accuracy. In the notation of Klein and Manning (2003) there are two Markovization parameters: h and v. The first parameter describes the amount of 462 X X X XD X X XB,C,D,E XC,D,E B B XD,E XB,C,D,E XB,C,D XB,C B XB B XE XD XA XB B A X C Y 0 1 2 3 original P = 4, V = X C Y D E 1 2 3 4 5 right branchin</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierluigi Crescenzi</author>
<author>Daniel Gildea</author>
<author>Aandrea Marino</author>
<author>Gianluca Rossi</author>
<author>Giorgio Satta</author>
</authors>
<title>Optimal head-driven parsing complexity for linear context-free rewriting systems.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="8133" citStr="Crescenzi et al. (2011)" startWordPosition="1333" endWordPosition="1336">f each span are non-contiguous. As in the case of a PCFG, we can read off LCFRS productions from a treebank (Maier and Søgaard, 2008), and the relative frequencies of productions form a maximum likelihood estimate, for a probabilistic LCFRS (PLCFRS), i.e., a (discontinuous) treebank grammar. As an example, figure 3 shows the productions extracted from the tree in figure 2. 3 Binarization A probabilistic LCFRS can be parsed using a CKYlike tabular parsing algorithm (cf. Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011), but this requires a binarized grammar.1 Any LCFRS can be binarized. Crescenzi et al. (2011) state “while CFGs can always be reduced to rank two (Chomsky Normal Form), this is not the case for LCFRS with any fan-out greater than one.” However, this assertion is made under the assumption of a fixed fan-out. If this assumption is relaxed then it is easy to binarize either deterministically or, as will be investigated in this work, optimally with a dynamic programming approach. Binarizing an LCFRS may increase its fan-out, which results in an increase in asymptotic complexity. Consider the following production: X(pqrs) → A(p,r) B(q) C(s) (1) Henceforth, we assume that non-terminals on t</context>
<context position="10390" citStr="Crescenzi et al., 2011" startWordPosition="1717" endWordPosition="1720">ndence between fan-out and parsing complexity: it is possible that parsing complexity can be reduced by increasing the fan-out of a production. In other words, there can be a production which can be binarized with a parsing complexity that is minimal while its fan-out is sub-optimal. Therefore we focus on parsing complexity rather than fan-out in this work, since parsing complexity determines the actual time complexity of parsing with a grammar. There has been some work investigating whether the increase in complexity can be minimized effectively (G´omez-Rodr´ıguez et al., 2009; Gildea, 2010; Crescenzi et al., 2011). More radically, it has been suggested that the power of LCFRS should be limited to well-nested structures, which gives an asymptotic improvement in parsing time (G´omez-Rodr´ıguez et al., 2010). However, there is linguistic evidence that not all language use can be described in wellnested structures (Chen-Main and Joshi, 2010). Therefore we will use the full power of LCFRS in this work—parsing complexity is determined by the treebank, not by a priori constraints. 3.1 Further binarization strategies Apart from optimizing for parsing complexity, for linguistic reasons it can also be useful to </context>
<context position="13456" citStr="Crescenzi et al. (2011)" startWordPosition="2269" endWordPosition="2272">unt of horizontal context on which a production is conditioned is important when the treebank contains many unique constituents which can only be parsed by stringing together different binarized productions; in other words, it is a way of dealing with the data sparseness about n-ary productions in the treebank. The second parameter describes parent annotation, which will not be investigated in this work; the default value is v = 1 which implies only including the immediate parent of the constituent that is being binarized; including grandparents is a way of weakening independence assumptions. Crescenzi et al. (2011) also remark that an optimal head-driven binarization allows for Markovization. However, it is questionable whether such a binarization is worthy of the name Markovization, as the non-terminals are not introduced deterministically from left to right, but in an arbitrary fashion dictated by concerns of parsing complexity; as such there is not a Markov process based on a meaningful (e.g., temporal) ordering and there is no probabilistic interpretation of Markovization in such a setting. To summarize, we have at least four binarization strategies (cf. figure 4 for an illustration): 1. right branc</context>
<context position="21149" citStr="Crescenzi et al., 2011" startWordPosition="3491" endWordPosition="3494">r (2010). Instead of using Maier’s implementation of discontinuous Fi scores in rparse, we employ a variant that ignores (a) punctuation, and (b) the root node of each tree. This makes our evaluation incomparable to previous results on discontinuous parsing, but brings it in line with common practice on the Wall street journal benchmark. Note that this change yields scores about 2 or 3 percentage points lower than those of rparse. Despite the fact that obtaining optimal bina4All code is available from: http://github.com/ andreasvc/disco-dop. rizations is exponential (Gildea, 2010) and NPhard (Crescenzi et al., 2011), they can be computed relatively quickly on this data set.5 Importantly, in the first case there is no improvement on fan-out or parsing complexity, while in the head-driven case there is a minimal improvement because of a single production with parsing complexity 15 without optimal binarization. On the other hand, the optimal binarizations might still have a significant effect on the average case complexity, rather than the worst-case complexities. Indeed, in both cases parsing with the optimal grammar is faster; in the first case, however, when the time for binarization is considered as wel</context>
</contexts>
<marker>Crescenzi, Gildea, Marino, Rossi, Satta, 2011</marker>
<rawString>Pierluigi Crescenzi, Daniel Gildea, Aandrea Marino, Gianluca Rossi, and Giorgio Satta. 2011. Optimal head-driven parsing complexity for linear context-free rewriting systems. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Parsing german with sister-head dependencies.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="17428" citStr="Dubey and Keller, 2003" startWordPosition="2892" endWordPosition="2895">he maximum complexity and fan-out in that binarization, to optimize for the worst case, as well as the sum, to optimize the average case. This aspect appears to be glossed over by Gildea (2010). Considering only the score of the last production in a binarization produces suboptimal binarizations. 3.3 Experiments As data we use version 2 of the Negra (Skut et al., 1997) treebank, with the common training, develFigure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25. The y-axis has a logarithmic scale. opment and test splits (Dubey and Keller, 2003). Following common practice, punctuation, which is left out of the phrase-structure in Negra, is reattached to the nearest constituent. In the course of experiments it was discovered that the heuristic method for punctuation attachment used in previous work (e.g., Maier, 2010; van Cranenburgh et al., 2011), as implemented in rparse,3 introduces additional discontinuity. We applied a slightly different heuristic: punctuation is attached to the highest constituent that contains a neighbor to its right. The result is that punctuation can be introduced into the phrase-structure without any additio</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>Amit Dubey and Frank Keller. 2003. Parsing german with sister-head dependencies. In Proc. of ACL, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Evang</author>
<author>Laura Kallmeyer</author>
</authors>
<title>PLCFRS parsing of English discontinuous constituents.</title>
<date>2011</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>104--116</pages>
<contexts>
<context position="1716" citStr="Evang and Kallmeyer, 2011" startWordPosition="243" endWordPosition="246">etting constituent structure express argument structure (Skut et al., 1997). Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005). Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the Penn treebank, in which traces have been converted to discontinuity. Taken from Evang and Kallmeyer (2011). 1997) and Tiger (Brants et al., 2002) corpora, or those that can be extracted from traces such as in the Penn treebank (Marcus et al., 1993) annotation. However, the computational complexity is such that until now, the length of sentences needed to be</context>
<context position="5178" citStr="Evang and Kallmeyer, 2011" startWordPosition="792" endWordPosition="795">on the productions of the grammar. Intuitively, LCFRS can be seen as a generalization of context-free grammars to rewriting other objects than just continuous strings: productions are context-free, but instead of strings they can rewrite tuples, trees or graphs. We focus on the use of LCFRS for parsing with discontinuous constituents. This follows up on recent work on parsing the discontinuous annotations in German corpora with LCFRS (Maier, 2010; van Cranenburgh et al., 2011) and work on parsing the Wall Street journal corpus in which traces have been converted to discontinuous constituents (Evang and Kallmeyer, 2011). In the case of parsing with discontinuous constituents a nonROOT(ab) —* S(a) $.(b) S(abcd) —* VAFIN(b) NN(c) VP2(a, d) VP2(a, bc) —* PROAV(a) NN(b) VVPP(c) PROAV(Danach) —* c VAFIN(habe) —* c NN(Kohlenstaub) —* c NN(Feuer) —* c VVPP(gefangen) —* c $.(.) —* E Figure 3: The productions that can be read off from the tree in figure 2. Note that lexical productions rewrite to e, because they do not rewrite to any non-terminals. terminal may cover a tuple of discontinuous strings instead of a single, contiguous sequence of terminals. The number of components in such a tuple is called the fan-out o</context>
</contexts>
<marker>Evang, Kallmeyer, 2011</marker>
<rawString>Kilian Evang and Laura Kallmeyer. 2011. PLCFRS parsing of English discontinuous constituents. In Proceedings of IWPT, pages 104– 116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Optimal parsing strategies for linear context-free rewriting systems.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL HLT 2010.,</booktitle>
<pages>769--776</pages>
<contexts>
<context position="9710" citStr="Gildea, 2010" startWordPosition="1609" endWordPosition="1610">ther algorithms exist which support n-ary productions, but these are less suitable for statistical treebank parsing. This binarization introduces a production with a fan-out of 2, which could have been avoided. After binarization, an LCFRS can be parsed in O(|G |· |w|p) time, where |G |is the size of the grammar, |w |is the length of the sentence. The degree p of the polynomial is the maximum parsing complexity of a rule, defined as: parsing complexity := ϕ + ϕ1 + ϕ2 (6) where ϕ is the fan-out of the left-hand side and ϕ1 and ϕ2 are the fan-outs of the right-hand side of the rule in question (Gildea, 2010). As Gildea (2010) shows, there is no one to one correspondence between fan-out and parsing complexity: it is possible that parsing complexity can be reduced by increasing the fan-out of a production. In other words, there can be a production which can be binarized with a parsing complexity that is minimal while its fan-out is sub-optimal. Therefore we focus on parsing complexity rather than fan-out in this work, since parsing complexity determines the actual time complexity of parsing with a grammar. There has been some work investigating whether the increase in complexity can be minimized ef</context>
<context position="14230" citStr="Gildea (2010)" startWordPosition="2388" endWordPosition="2389">kovization, as the non-terminals are not introduced deterministically from left to right, but in an arbitrary fashion dictated by concerns of parsing complexity; as such there is not a Markov process based on a meaningful (e.g., temporal) ordering and there is no probabilistic interpretation of Markovization in such a setting. To summarize, we have at least four binarization strategies (cf. figure 4 for an illustration): 1. right branching: A right-to-left binarization. No regard for optimality or statistical tweaks. 2. optimal: A binarization which minimizes parsing complexity, introduced in Gildea (2010). Binarizing with this strategy is exponential in the resulting optimal fan-out (Gildea, 2010). 3. head-driven: Head-outward binarization with horizontal Markovization. No regard for optimality. 4. optimal head-driven: Head-outward binarization with horizontal Markovization. Minimizes parsing complexity. Introduced in and proven to be NP-hard by Crescenzi et al. (2011). 3.2 Finding optimal binarizations An issue with the minimal binarizations is that the algorithm for finding them has a high computational complexity, and has not been evaluated empirically on treebank data.2 Empirical investiga</context>
<context position="16998" citStr="Gildea (2010)" startWordPosition="2828" endWordPosition="2829">curs once instead of twice. The formula is then: opt(c) = hp, ϕ, si The second function is the similar except that only head-driven strategies are accepted. A headdriven strategy is a binarization in which the head is introduced first, after which the rest of the children are introduced one at a time. r opt-hd(c) (p, ϕ, si if c is head-driven = Sl h∞, ∞, ∞i Given a (partial) binarization c, the score should reflect the maximum complexity and fan-out in that binarization, to optimize for the worst case, as well as the sum, to optimize the average case. This aspect appears to be glossed over by Gildea (2010). Considering only the score of the last production in a binarization produces suboptimal binarizations. 3.3 Experiments As data we use version 2 of the Negra (Skut et al., 1997) treebank, with the common training, develFigure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25. The y-axis has a logarithmic scale. opment and test splits (Dubey and Keller, 2003). Following common practice, punctuation, which is left out of the phrase-structure in Negra, is reattached to the nearest constituent. In the course of experiments it wa</context>
<context position="21113" citStr="Gildea, 2010" startWordPosition="3486" endWordPosition="3487">uctures, described in Maier (2010). Instead of using Maier’s implementation of discontinuous Fi scores in rparse, we employ a variant that ignores (a) punctuation, and (b) the root node of each tree. This makes our evaluation incomparable to previous results on discontinuous parsing, but brings it in line with common practice on the Wall street journal benchmark. Note that this change yields scores about 2 or 3 percentage points lower than those of rparse. Despite the fact that obtaining optimal bina4All code is available from: http://github.com/ andreasvc/disco-dop. rizations is exponential (Gildea, 2010) and NPhard (Crescenzi et al., 2011), they can be computed relatively quickly on this data set.5 Importantly, in the first case there is no improvement on fan-out or parsing complexity, while in the head-driven case there is a minimal improvement because of a single production with parsing complexity 15 without optimal binarization. On the other hand, the optimal binarizations might still have a significant effect on the average case complexity, rather than the worst-case complexities. Indeed, in both cases parsing with the optimal grammar is faster; in the first case, however, when the time f</context>
</contexts>
<marker>Gildea, 2010</marker>
<rawString>Daniel Gildea. 2010. Optimal parsing strategies for linear context-free rewriting systems. In Proceedings of NAACL HLT 2010., pages 769– 776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing of wellnested linear context-free rewriting systems.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL HLT 2010.,</booktitle>
<pages>276--284</pages>
<marker>G´omez-Rodr´ıguez, Kuhlmann, Satta, 2010</marker>
<rawString>Carlos G´omez-Rodr´ıguez, Marco Kuhlmann, and Giorgio Satta. 2010. Efficient parsing of wellnested linear context-free rewriting systems. In Proceedings of NAACL HLT 2010., pages 276– 284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
<author>David Weir</author>
</authors>
<title>Optimal reduction of rule length in linear context-free rewriting systems.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>539--547</pages>
<marker>G´omez-Rodr´ıguez, Kuhlmann, Satta, Weir, 2009</marker>
<rawString>Carlos G´omez-Rodr´ıguez, Marco Kuhlmann, Giorgio Satta, and David Weir. 2009. Optimal reduction of rule length in linear context-free rewriting systems. In Proceedings of NAACL HLT 2009, pages 539–547.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2003</date>
<editor>In Rens Bod, Remko Scha, and Khalil Sima’an, editors, DataOriented Parsing.</editor>
<publisher>The University of Chicago Press.</publisher>
<marker>Goodman, 2003</marker>
<rawString>Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Rens Bod, Remko Scha, and Khalil Sima’an, editors, DataOriented Parsing. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
</authors>
<title>Parsing Beyond ContextFree Grammars. Cognitive Technologies.</title>
<date>2010</date>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="4057" citStr="Kallmeyer, 2010" startWordPosition="619" endWordPosition="620">inguistics ROOT PROAV VAFIN NN NN VVPP $. Danach habe Kohlenstaub Feuer gefangen . Afterwards had coal dust fire caught . Figure 2: A discontinuous tree from the Negra corpus. Translation: After that coal dust had caught fire. 2 Linear Context-Free Rewriting Systems Linear Context-Free Rewriting Systems (LCFRS; Vijay-Shanker et al., 1987; Weir, 1988) subsume a wide variety of mildly context-sensitive formalisms, such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Minimalist Grammar, Multiple Context-Free Grammar (MCFG) and synchronous CFG (Vijay-Shanker and Weir, 1994; Kallmeyer, 2010). Furthermore, they can be used to parse dependency structures (Kuhlmann and Satta, 2009). Since LCFRS subsumes various synchronous grammars, they are also important for machine translation. This makes it possible to use LCFRS as a syntactic backbone with which various formalisms can be parsed by compiling grammars into an LCFRS, similar to the TuLiPa system (Kallmeyer et al., 2008). As all mildly context-sensitive formalisms, LCFRS are parsable in polynomial time, where the degree depends on the productions of the grammar. Intuitively, LCFRS can be seen as a generalization of context-free gra</context>
</contexts>
<marker>Kallmeyer, 2010</marker>
<rawString>Laura Kallmeyer. 2010. Parsing Beyond ContextFree Grammars. Cognitive Technologies. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
<author>Timm Lichte</author>
<author>Wolfgang Maier</author>
<author>Yannick Parmentier</author>
<author>Johannes Dellert</author>
<author>Kilian Evang</author>
</authors>
<title>Tulipa: Towards a multiformalism parsing environment for grammar engineering.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Grammar Engineering Across Frameworks,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4442" citStr="Kallmeyer et al., 2008" startWordPosition="677" endWordPosition="680"> mildly context-sensitive formalisms, such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Minimalist Grammar, Multiple Context-Free Grammar (MCFG) and synchronous CFG (Vijay-Shanker and Weir, 1994; Kallmeyer, 2010). Furthermore, they can be used to parse dependency structures (Kuhlmann and Satta, 2009). Since LCFRS subsumes various synchronous grammars, they are also important for machine translation. This makes it possible to use LCFRS as a syntactic backbone with which various formalisms can be parsed by compiling grammars into an LCFRS, similar to the TuLiPa system (Kallmeyer et al., 2008). As all mildly context-sensitive formalisms, LCFRS are parsable in polynomial time, where the degree depends on the productions of the grammar. Intuitively, LCFRS can be seen as a generalization of context-free grammars to rewriting other objects than just continuous strings: productions are context-free, but instead of strings they can rewrite tuples, trees or graphs. We focus on the use of LCFRS for parsing with discontinuous constituents. This follows up on recent work on parsing the discontinuous annotations in German corpora with LCFRS (Maier, 2010; van Cranenburgh et al., 2011) and work</context>
</contexts>
<marker>Kallmeyer, Lichte, Maier, Parmentier, Dellert, Evang, 2008</marker>
<rawString>Laura Kallmeyer, Timm Lichte, Wolfgang Maier, Yannick Parmentier, Johannes Dellert, and Kilian Evang. 2008. Tulipa: Towards a multiformalism parsing environment for grammar engineering. In Proceedings of the Workshop on Grammar Engineering Across Frameworks, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
<author>Wolfgang Maier</author>
</authors>
<title>Datadriven parsing with probabilistic linear contextfree rewriting systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>537--545</pages>
<contexts>
<context position="1676" citStr="Kallmeyer and Maier, 2010" startWordPosition="237" endWordPosition="240">ndence between syntax and semantics by letting constituent structure express argument structure (Skut et al., 1997). Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005). Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the Penn treebank, in which traces have been converted to discontinuity. Taken from Evang and Kallmeyer (2011). 1997) and Tiger (Brants et al., 2002) corpora, or those that can be extracted from traces such as in the Penn treebank (Marcus et al., 1993) annotation. However, the computational complexity is such that until n</context>
<context position="8009" citStr="Kallmeyer and Maier, 2010" startWordPosition="1313" endWordPosition="1316">of the LHS, the concatenation of its terminals and bound variables forms a contiguous span in the input, while the endpoints of each span are non-contiguous. As in the case of a PCFG, we can read off LCFRS productions from a treebank (Maier and Søgaard, 2008), and the relative frequencies of productions form a maximum likelihood estimate, for a probabilistic LCFRS (PLCFRS), i.e., a (discontinuous) treebank grammar. As an example, figure 3 shows the productions extracted from the tree in figure 2. 3 Binarization A probabilistic LCFRS can be parsed using a CKYlike tabular parsing algorithm (cf. Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011), but this requires a binarized grammar.1 Any LCFRS can be binarized. Crescenzi et al. (2011) state “while CFGs can always be reduced to rank two (Chomsky Normal Form), this is not the case for LCFRS with any fan-out greater than one.” However, this assertion is made under the assumption of a fixed fan-out. If this assumption is relaxed then it is easy to binarize either deterministically or, as will be investigated in this work, optimally with a dynamic programming approach. Binarizing an LCFRS may increase its fan-out, which results in an increase in asymptotic</context>
<context position="23028" citStr="Kallmeyer and Maier, 2010" startWordPosition="3774" endWordPosition="3777">s infeasible. Therefore we need to look at other techniques for parsing longer sentences. We will stick with the straightforward 5The implementation exploits two important optimizations. The first is the use of bit vectors to keep track of which non-terminals are covered by a partial binarization. The second is to skip constituents without discontinuity, which are equivalent to CFG productions. 465 head-driven, head-outward binarization strategy, despite this being a computationally sub-optimal binarization. One technique for efficient parsing of LCFRS is the use of context-summary estimates (Kallmeyer and Maier, 2010), as part of a best-first parsing algorithm. This allowed Maier (2010) to parse sentences of up to 30 words. However, the calculation of these estimates is not feasible for longer sentences and large grammars (van Cranenburgh et al., 2011). Another strategy is to perform an online approximation of the sentence to be parsed, after which parsing with the LCFRS can be pruned effectively. This is the strategy that will be explored in the current work. 4 Context-free grammar approximation for coarse-to-fine parsing Coarse-to-fine parsing (Charniak et al., 2006) is a technique to speed up parsing by</context>
</contexts>
<marker>Kallmeyer, Maier, 2010</marker>
<rawString>Laura Kallmeyer and Wolfgang Maier. 2010. Datadriven parsing with probabilistic linear contextfree rewriting systems. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 537–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<volume>1</volume>
<pages>423--430</pages>
<contexts>
<context position="11348" citStr="Klein and Manning, 2003" startWordPosition="1863" endWordPosition="1866">erefore we will use the full power of LCFRS in this work—parsing complexity is determined by the treebank, not by a priori constraints. 3.1 Further binarization strategies Apart from optimizing for parsing complexity, for linguistic reasons it can also be useful to parse the head of a constituent first, yielding so-called head-driven binarizations (Collins, 1999). Additionally, such a head-driven binarization can be ‘Markovized’–i.e., the resulting production can be constrained to apply to a limited amount of horizontal context as opposed to the full context in the original constituent (e.g., Klein and Manning, 2003), which can have a beneficial effect on accuracy. In the notation of Klein and Manning (2003) there are two Markovization parameters: h and v. The first parameter describes the amount of 462 X X X XD X X XB,C,D,E XC,D,E B B XD,E XB,C,D,E XB,C,D XB,C B XB B XE XD XA XB B A X C Y 0 1 2 3 original P = 4, V = X C Y D E 1 2 3 4 5 right branching P = 5, V = 2 X C Y D E 1 2 3 4 5 optimal P = 4, V = 2 A X C Y D E 0 1 2 3 4 5 head-driven P = 5, V = 2 X C Y D 1 2 3 4 optimal head-driven P = 4, V = 2 Figure 4: The four binarization strategies. C is the head node. Underneath each tree is the maximum parsi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL, volume 1, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Treebank grammar techniques for non-projective dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>478--486</pages>
<contexts>
<context position="4146" citStr="Kuhlmann and Satta, 2009" startWordPosition="631" endWordPosition="634"> . Afterwards had coal dust fire caught . Figure 2: A discontinuous tree from the Negra corpus. Translation: After that coal dust had caught fire. 2 Linear Context-Free Rewriting Systems Linear Context-Free Rewriting Systems (LCFRS; Vijay-Shanker et al., 1987; Weir, 1988) subsume a wide variety of mildly context-sensitive formalisms, such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Minimalist Grammar, Multiple Context-Free Grammar (MCFG) and synchronous CFG (Vijay-Shanker and Weir, 1994; Kallmeyer, 2010). Furthermore, they can be used to parse dependency structures (Kuhlmann and Satta, 2009). Since LCFRS subsumes various synchronous grammars, they are also important for machine translation. This makes it possible to use LCFRS as a syntactic backbone with which various formalisms can be parsed by compiling grammars into an LCFRS, similar to the TuLiPa system (Kallmeyer et al., 2008). As all mildly context-sensitive formalisms, LCFRS are parsable in polynomial time, where the degree depends on the productions of the grammar. Intuitively, LCFRS can be seen as a generalization of context-free grammars to rewriting other objects than just continuous strings: productions are context-fr</context>
</contexts>
<marker>Kuhlmann, Satta, 2009</marker>
<rawString>Marco Kuhlmann and Giorgio Satta. 2009. Treebank grammar techniques for non-projective dependency parsing. In Proceedings of EACL, pages 478–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Probabilistic models of word order and syntactic discontinuity.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1369" citStr="Levy, 2005" startWordPosition="196" endWordPosition="197">restriction, while maintaining a respectable accuracy. The resulting parser has been applied to a discontinuous treebank with favorable results. 1 Introduction Discontinuity in constituent structures (cf. figure 1 &amp; 2) is important for a variety of reasons. For one, it allows a tight correspondence between syntax and semantics by letting constituent structure express argument structure (Skut et al., 1997). Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005). Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the Penn treeban</context>
</contexts>
<marker>Levy, 2005</marker>
<rawString>Roger Levy. 2005. Probabilistic models of word order and syntactic discontinuity. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
</authors>
<title>Direct parsing of discontinuous constituents in German.</title>
<date>2010</date>
<booktitle>In Proceedings of the SPMRL workshop at NAACL HLT</booktitle>
<pages>58--66</pages>
<contexts>
<context position="1676" citStr="Maier, 2010" startWordPosition="239" endWordPosition="240"> syntax and semantics by letting constituent structure express argument structure (Skut et al., 1997). Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005). Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the Penn treebank, in which traces have been converted to discontinuity. Taken from Evang and Kallmeyer (2011). 1997) and Tiger (Brants et al., 2002) corpora, or those that can be extracted from traces such as in the Penn treebank (Marcus et al., 1993) annotation. However, the computational complexity is such that until n</context>
<context position="5002" citStr="Maier, 2010" startWordPosition="766" endWordPosition="767">milar to the TuLiPa system (Kallmeyer et al., 2008). As all mildly context-sensitive formalisms, LCFRS are parsable in polynomial time, where the degree depends on the productions of the grammar. Intuitively, LCFRS can be seen as a generalization of context-free grammars to rewriting other objects than just continuous strings: productions are context-free, but instead of strings they can rewrite tuples, trees or graphs. We focus on the use of LCFRS for parsing with discontinuous constituents. This follows up on recent work on parsing the discontinuous annotations in German corpora with LCFRS (Maier, 2010; van Cranenburgh et al., 2011) and work on parsing the Wall Street journal corpus in which traces have been converted to discontinuous constituents (Evang and Kallmeyer, 2011). In the case of parsing with discontinuous constituents a nonROOT(ab) —* S(a) $.(b) S(abcd) —* VAFIN(b) NN(c) VP2(a, d) VP2(a, bc) —* PROAV(a) NN(b) VVPP(c) PROAV(Danach) —* c VAFIN(habe) —* c NN(Kohlenstaub) —* c NN(Feuer) —* c VVPP(gefangen) —* c $.(.) —* E Figure 3: The productions that can be read off from the tree in figure 2. Note that lexical productions rewrite to e, because they do not rewrite to any non-termin</context>
<context position="8009" citStr="Maier, 2010" startWordPosition="1315" endWordPosition="1316">e concatenation of its terminals and bound variables forms a contiguous span in the input, while the endpoints of each span are non-contiguous. As in the case of a PCFG, we can read off LCFRS productions from a treebank (Maier and Søgaard, 2008), and the relative frequencies of productions form a maximum likelihood estimate, for a probabilistic LCFRS (PLCFRS), i.e., a (discontinuous) treebank grammar. As an example, figure 3 shows the productions extracted from the tree in figure 2. 3 Binarization A probabilistic LCFRS can be parsed using a CKYlike tabular parsing algorithm (cf. Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011), but this requires a binarized grammar.1 Any LCFRS can be binarized. Crescenzi et al. (2011) state “while CFGs can always be reduced to rank two (Chomsky Normal Form), this is not the case for LCFRS with any fan-out greater than one.” However, this assertion is made under the assumption of a fixed fan-out. If this assumption is relaxed then it is easy to binarize either deterministically or, as will be investigated in this work, optimally with a dynamic programming approach. Binarizing an LCFRS may increase its fan-out, which results in an increase in asymptotic</context>
<context position="17704" citStr="Maier, 2010" startWordPosition="2937" endWordPosition="2938">izations. 3.3 Experiments As data we use version 2 of the Negra (Skut et al., 1997) treebank, with the common training, develFigure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25. The y-axis has a logarithmic scale. opment and test splits (Dubey and Keller, 2003). Following common practice, punctuation, which is left out of the phrase-structure in Negra, is reattached to the nearest constituent. In the course of experiments it was discovered that the heuristic method for punctuation attachment used in previous work (e.g., Maier, 2010; van Cranenburgh et al., 2011), as implemented in rparse,3 introduces additional discontinuity. We applied a slightly different heuristic: punctuation is attached to the highest constituent that contains a neighbor to its right. The result is that punctuation can be introduced into the phrase-structure without any additional discontinuity, and thus without artificially inflating the fan-out and complexity of grammars read off from the treebank. This new heuristic provides a significant improvement: instead of a fan-out of 9 and a parsing complexity of 19, we obtain values of 4 and 9 respectiv</context>
<context position="20534" citStr="Maier (2010)" startWordPosition="3395" endWordPosition="3396"> optimal binarizations do seem to have some effect on the distribution of parsing complexities, it remains to be seen whether this can be cashed out as a performance improvement in practice. To this end, we also parse using the binarized grammars. In this work we binarize and parse with disco-dop introduced in van Cranenburgh et al. (2011).4 In this experiment we report scores of the (exact) Viterbi derivations of a treebank PLCFRS; cf. table 1 for the results. Times represent CPU time (single core); accuracy is given with a generalization of PARSEVAL to discontinuous structures, described in Maier (2010). Instead of using Maier’s implementation of discontinuous Fi scores in rparse, we employ a variant that ignores (a) punctuation, and (b) the root node of each tree. This makes our evaluation incomparable to previous results on discontinuous parsing, but brings it in line with common practice on the Wall street journal benchmark. Note that this change yields scores about 2 or 3 percentage points lower than those of rparse. Despite the fact that obtaining optimal bina4All code is available from: http://github.com/ andreasvc/disco-dop. rizations is exponential (Gildea, 2010) and NPhard (Crescenz</context>
<context position="23028" citStr="Maier, 2010" startWordPosition="3776" endWordPosition="3777">Therefore we need to look at other techniques for parsing longer sentences. We will stick with the straightforward 5The implementation exploits two important optimizations. The first is the use of bit vectors to keep track of which non-terminals are covered by a partial binarization. The second is to skip constituents without discontinuity, which are equivalent to CFG productions. 465 head-driven, head-outward binarization strategy, despite this being a computationally sub-optimal binarization. One technique for efficient parsing of LCFRS is the use of context-summary estimates (Kallmeyer and Maier, 2010), as part of a best-first parsing algorithm. This allowed Maier (2010) to parse sentences of up to 30 words. However, the calculation of these estimates is not feasible for longer sentences and large grammars (van Cranenburgh et al., 2011). Another strategy is to perform an online approximation of the sentence to be parsed, after which parsing with the LCFRS can be pruned effectively. This is the strategy that will be explored in the current work. 4 Context-free grammar approximation for coarse-to-fine parsing Coarse-to-fine parsing (Charniak et al., 2006) is a technique to speed up parsing by</context>
<context position="30286" citStr="Maier (2010)" startWordPosition="5029" endWordPosition="5030">th is made up of 20–40 sentences. sentences of length &gt; 22 despite its overhead of parsing twice. The second experiment demonstrates the CFGCTF technique on longer sentences. We restrict the length of sentences in the training, development and test corpora to 40 words: NEGRA-40. As a first step we apply the CFG-CTF technique to parse with a PLCFRS as the fine grammar, pruning away all items not occurring in the 10,000 best derivations cpu time (s) 45 40 35 30 25 20 15 10 0 5 PLCFRS CFG-CTF (Split-PCFG ==&gt;. PLCFRS) 467 words PARSEVAL Exact (F1) match DPSG: Plaehn (2004) &lt; 15 73.16 39.0 PLCFRS: Maier (2010) &lt; 30 71.52 31.65 Disco-DOP: van Cranenburgh et al. (2011) &lt; 30 73.98 34.80 Table 3: Previous work on discontinuous parsing of Negra. words PARSEVAL Exact (F1) match PLCFRS, dev set &lt; 25 72.37 36.58 Split-PCFG, dev set &lt; 25 70.74 33.80 Split-PCFG, dev set &lt; 40 66.81 27.59 CFG-CTF, PLCFRS, dev set &lt; 40 67.26 27.90 CFG-CTF, Disco-DOP, dev set &lt; 40 74.27 34.26 CFG-CTF, Disco-DOP, test set &lt; 40 72.33 33.16 CFG-CTF, Disco-DOP, dev set 00 73.32 33.40 CFG-CTF, Disco-DOP, test set 00 71.08 32.10 Table 4: Results on NEGRA-25 and NEGRA-40 with the CFG-CTF method. NB: As explained in section 3.3, these F</context>
</contexts>
<marker>Maier, 2010</marker>
<rawString>Wolfgang Maier. 2010. Direct parsing of discontinuous constituents in German. In Proceedings of the SPMRL workshop at NAACL HLT 2010, pages 58–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Timm Lichte</author>
</authors>
<title>Characterizing discontinuity in constituent treebanks.</title>
<date>2009</date>
<contexts>
<context position="1584" citStr="Maier and Lichte, 2009" startWordPosition="224" endWordPosition="227">. figure 1 &amp; 2) is important for a variety of reasons. For one, it allows a tight correspondence between syntax and semantics by letting constituent structure express argument structure (Skut et al., 1997). Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005). Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the Penn treebank, in which traces have been converted to discontinuity. Taken from Evang and Kallmeyer (2011). 1997) and Tiger (Brants et al., 2002) corpora, or those that can be extracted from traces such as in the Penn treebank </context>
</contexts>
<marker>Maier, Lichte, 2009</marker>
<rawString>Wolfgang Maier and Timm Lichte. 2009. Characterizing discontinuity in constituent treebanks.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>In Proceedings of Formal Grammar</booktitle>
<pages>167--182</pages>
<publisher>Springer.</publisher>
<marker>2009</marker>
<rawString>In Proceedings of Formal Grammar 2009, pages 167–182. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Anders Søgaard</author>
</authors>
<title>Treebanks and mild context-sensitivity.</title>
<date>2008</date>
<booktitle>In Proceedings of Formal Grammar</booktitle>
<pages>61</pages>
<contexts>
<context position="1559" citStr="Maier and Søgaard, 2008" startWordPosition="220" endWordPosition="223">onstituent structures (cf. figure 1 &amp; 2) is important for a variety of reasons. For one, it allows a tight correspondence between syntax and semantics by letting constituent structure express argument structure (Skut et al., 1997). Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005). Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the Penn treebank, in which traces have been converted to discontinuity. Taken from Evang and Kallmeyer (2011). 1997) and Tiger (Brants et al., 2002) corpora, or those that can be extracted from traces such</context>
<context position="7643" citStr="Maier and Søgaard, 2008" startWordPosition="1256" endWordPosition="1259">ut of the non-terminal symbol on the lefthand side. Apart from the fan-out productions also S VP 461 have a rank: the number of non-terminals on the right-hand side. These two variables determine the time complexity of parsing with a grammar. A production can be instantiated when its variables can be bound to non-overlapping spans such that for each component αi of the LHS, the concatenation of its terminals and bound variables forms a contiguous span in the input, while the endpoints of each span are non-contiguous. As in the case of a PCFG, we can read off LCFRS productions from a treebank (Maier and Søgaard, 2008), and the relative frequencies of productions form a maximum likelihood estimate, for a probabilistic LCFRS (PLCFRS), i.e., a (discontinuous) treebank grammar. As an example, figure 3 shows the productions extracted from the tree in figure 2. 3 Binarization A probabilistic LCFRS can be parsed using a CKYlike tabular parsing algorithm (cf. Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011), but this requires a binarized grammar.1 Any LCFRS can be binarized. Crescenzi et al. (2011) state “while CFGs can always be reduced to rank two (Chomsky Normal Form), this is not the case for LCFRS wit</context>
</contexts>
<marker>Maier, Søgaard, 2008</marker>
<rawString>Wolfgang Maier and Anders Søgaard. 2008. Treebanks and mild context-sensitivity. In Proceedings of Formal Grammar 2008, page 61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="2205" citStr="Marcus et al., 1993" startWordPosition="331" endWordPosition="334"> Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the Penn treebank, in which traces have been converted to discontinuity. Taken from Evang and Kallmeyer (2011). 1997) and Tiger (Brants et al., 2002) corpora, or those that can be extracted from traces such as in the Penn treebank (Marcus et al., 1993) annotation. However, the computational complexity is such that until now, the length of sentences needed to be restricted. In the case of Kallmeyer and Maier (2010) and Evang and Kallmeyer (2011) the limit was 25 words. Maier (2010) and van Cranenburgh et al. (2011) manage to parse up to 30 words with heuristics and optimizations, but no further. Algorithms have been suggested to binarize the grammars in such a way as to minimize parsing complexity, but the current paper shows that these techniques are not sufficient to parse longer sentences. Instead, this work presents a novel form of coars</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James D McCawley</author>
</authors>
<title>Parentheticals and discontinuous constituent structure.</title>
<date>1982</date>
<journal>Linguistic Inquiry,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="1356" citStr="McCawley, 1982" startWordPosition="194" endWordPosition="195">ves this length restriction, while maintaining a respectable accuracy. The resulting parser has been applied to a discontinuous treebank with favorable results. 1 Introduction Discontinuity in constituent structures (cf. figure 1 &amp; 2) is important for a variety of reasons. For one, it allows a tight correspondence between syntax and semantics by letting constituent structure express argument structure (Skut et al., 1997). Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005). Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al., SBARQ WHNP MD NP VB What should I do ? Figure 1: A tree with WH-movement from the</context>
</contexts>
<marker>McCawley, 1982</marker>
<rawString>James D. McCawley. 1982. Parentheticals and discontinuous constituent structure. Linguistic Inquiry, 13(1):91–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Plaehn</author>
</authors>
<title>Computing the most probable parse for a discontinuous phrase structure grammar.</title>
<date>2004</date>
<booktitle>New developments in parsing technology,</booktitle>
<pages>91--106</pages>
<editor>In Harry Bunt, John Carroll, and Giorgio Satta, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="30249" citStr="Plaehn (2004)" startWordPosition="5022" endWordPosition="5023">se sentences of that length; each length is made up of 20–40 sentences. sentences of length &gt; 22 despite its overhead of parsing twice. The second experiment demonstrates the CFGCTF technique on longer sentences. We restrict the length of sentences in the training, development and test corpora to 40 words: NEGRA-40. As a first step we apply the CFG-CTF technique to parse with a PLCFRS as the fine grammar, pruning away all items not occurring in the 10,000 best derivations cpu time (s) 45 40 35 30 25 20 15 10 0 5 PLCFRS CFG-CTF (Split-PCFG ==&gt;. PLCFRS) 467 words PARSEVAL Exact (F1) match DPSG: Plaehn (2004) &lt; 15 73.16 39.0 PLCFRS: Maier (2010) &lt; 30 71.52 31.65 Disco-DOP: van Cranenburgh et al. (2011) &lt; 30 73.98 34.80 Table 3: Previous work on discontinuous parsing of Negra. words PARSEVAL Exact (F1) match PLCFRS, dev set &lt; 25 72.37 36.58 Split-PCFG, dev set &lt; 25 70.74 33.80 Split-PCFG, dev set &lt; 40 66.81 27.59 CFG-CTF, PLCFRS, dev set &lt; 40 67.26 27.90 CFG-CTF, Disco-DOP, dev set &lt; 40 74.27 34.26 CFG-CTF, Disco-DOP, test set &lt; 40 72.33 33.16 CFG-CTF, Disco-DOP, dev set 00 73.32 33.40 CFG-CTF, Disco-DOP, test set 00 71.08 32.10 Table 4: Results on NEGRA-25 and NEGRA-40 with the CFG-CTF method. NB:</context>
</contexts>
<marker>Plaehn, 2004</marker>
<rawString>Oliver Plaehn. 2004. Computing the most probable parse for a discontinuous phrase structure grammar. In Harry Bunt, John Carroll, and Giorgio Satta, editors, New developments in parsing technology, pages 91–106. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remko Scha</author>
</authors>
<title>Language theory and language technology; competence and performance.</title>
<date>1990</date>
<booktitle>Computertoepassingen in de Neerlandistiek,</booktitle>
<pages>7--22</pages>
<editor>In Q.A.M. de Kort and G.L.J. Leerdam, editors,</editor>
<publisher>LVVN,</publisher>
<location>Almere, the</location>
<contexts>
<context position="31449" citStr="Scha, 1990" startWordPosition="5228" endWordPosition="5229">method. NB: As explained in section 3.3, these Fl scores are incomparable to the results in table 3; for comparison, the Fl score for Disco-DOP on the dev set &lt; 40 is 77.13 % using that evaluation scheme. from the PCFG chart. The result shows that the PLCFRS gives a slight improvement over the split-- pcfg, which accords with the observation that the latter makes stronger independence assumptions in the case of discontinuity. In the next experiments we turn to an allfragments grammar encoded in a PLCFRS using Goodman’s (2003) reduction, to realize a (discontinuous) Data-Oriented Parsing (DOP; Scha, 1990) model—which goes by the name of DiscoDOP (van Cranenburgh et al., 2011). This provides an effective yet conceptually simple method to weaken the independence assumptions of treebank grammars. Table 2 gives statistics on the grammars, including the parsing complexities. The fine grammar has a parsing complexity of 9, which means that parsing with this grammar has complexity O(|w|9). We use the same parameters as van Cranenburgh et al. (2011), except that unlike van Cranenburgh et al., we can use v = 1, h = 1 Markovization, in order to obtain a higher coverage. The DOP grammar is added as a thi</context>
</contexts>
<marker>Scha, 1990</marker>
<rawString>Remko Scha. 1990. Language theory and language technology; competence and performance. In Q.A.M. de Kort and G.L.J. Leerdam, editors, Computertoepassingen in de Neerlandistiek, pages 7–22. LVVN, Almere, the Netherlands. Original title: Taaltheorie en taaltechnologie; competence en performance. Translation available at http://iaaa.nl/rs/LeerdamE.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Evidence against the context-freeness of natural language. Linguistics and Philosophy,</title>
<date>1985</date>
<pages>8--333</pages>
<marker>Shieber, 1985</marker>
<rawString>Stuart M. Shieber. 1985. Evidence against the context-freeness of natural language. Linguistics and Philosophy, 8:333–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="1166" citStr="Skut et al., 1997" startWordPosition="166" endWordPosition="169">hat minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible. Instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy. The resulting parser has been applied to a discontinuous treebank with favorable results. 1 Introduction Discontinuity in constituent structures (cf. figure 1 &amp; 2) is important for a variety of reasons. For one, it allows a tight correspondence between syntax and semantics by letting constituent structure express argument structure (Skut et al., 1997). Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (McCawley, 1982; Levy, 2005). Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks (Maier and Søgaard, 2008; Maier and Lichte, 2009). Recent work on treebank parsing with discontinuous constituents (Kallmeyer and Maier, 2010; Maier, 2010; Evang and Kallmeyer, 2011; van Cranenburgh et al., 2011) shows that it is f</context>
<context position="17176" citStr="Skut et al., 1997" startWordPosition="2855" endWordPosition="2858">ategy is a binarization in which the head is introduced first, after which the rest of the children are introduced one at a time. r opt-hd(c) (p, ϕ, si if c is head-driven = Sl h∞, ∞, ∞i Given a (partial) binarization c, the score should reflect the maximum complexity and fan-out in that binarization, to optimize for the worst case, as well as the sum, to optimize the average case. This aspect appears to be glossed over by Gildea (2010). Considering only the score of the last production in a binarization produces suboptimal binarizations. 3.3 Experiments As data we use version 2 of the Negra (Skut et al., 1997) treebank, with the common training, develFigure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25. The y-axis has a logarithmic scale. opment and test splits (Dubey and Keller, 2003). Following common practice, punctuation, which is left out of the phrase-structure in Negra, is reattached to the nearest constituent. In the course of experiments it was discovered that the heuristic method for punctuation attachment used in previous work (e.g., Maier, 2010; van Cranenburgh et al., 2011), as implemented in rparse,3 introduces a</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order languages. In Proceedings of ANLP, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas van Cranenburgh</author>
<author>Remko Scha</author>
<author>Federico Sangati</author>
</authors>
<title>Discontinuous dataoriented parsing: A mildly context-sensitive allfragments grammar.</title>
<date>2011</date>
<booktitle>In Proceedings of SPMRL,</booktitle>
<pages>34--44</pages>
<marker>van Cranenburgh, Scha, Sangati, 2011</marker>
<rawString>Andreas van Cranenburgh, Remko Scha, and Federico Sangati. 2011. Discontinuous dataoriented parsing: A mildly context-sensitive allfragments grammar. In Proceedings of SPMRL, pages 34–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>The equivalence of four extensions of context-free grammars.</title>
<date>1994</date>
<journal>Theory of Computing Systems,</journal>
<volume>27</volume>
<issue>6</issue>
<contexts>
<context position="4039" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="615" endWordPosition="618">ssociation for Computational Linguistics ROOT PROAV VAFIN NN NN VVPP $. Danach habe Kohlenstaub Feuer gefangen . Afterwards had coal dust fire caught . Figure 2: A discontinuous tree from the Negra corpus. Translation: After that coal dust had caught fire. 2 Linear Context-Free Rewriting Systems Linear Context-Free Rewriting Systems (LCFRS; Vijay-Shanker et al., 1987; Weir, 1988) subsume a wide variety of mildly context-sensitive formalisms, such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Minimalist Grammar, Multiple Context-Free Grammar (MCFG) and synchronous CFG (Vijay-Shanker and Weir, 1994; Kallmeyer, 2010). Furthermore, they can be used to parse dependency structures (Kuhlmann and Satta, 2009). Since LCFRS subsumes various synchronous grammars, they are also important for machine translation. This makes it possible to use LCFRS as a syntactic backbone with which various formalisms can be parsed by compiling grammars into an LCFRS, similar to the TuLiPa system (Kallmeyer et al., 2008). As all mildly context-sensitive formalisms, LCFRS are parsable in polynomial time, where the degree depends on the productions of the grammar. Intuitively, LCFRS can be seen as a generalization o</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1994</marker>
<rawString>K. Vijay-Shanker and David J. Weir. 1994. The equivalence of four extensions of context-free grammars. Theory of Computing Systems, 27(6):511–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="3780" citStr="Vijay-Shanker et al., 1987" startWordPosition="580" endWordPosition="583">luate this technique on a large corpus without the usual length restrictions. SQ VP 460 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 460–470, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics ROOT PROAV VAFIN NN NN VVPP $. Danach habe Kohlenstaub Feuer gefangen . Afterwards had coal dust fire caught . Figure 2: A discontinuous tree from the Negra corpus. Translation: After that coal dust had caught fire. 2 Linear Context-Free Rewriting Systems Linear Context-Free Rewriting Systems (LCFRS; Vijay-Shanker et al., 1987; Weir, 1988) subsume a wide variety of mildly context-sensitive formalisms, such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Minimalist Grammar, Multiple Context-Free Grammar (MCFG) and synchronous CFG (Vijay-Shanker and Weir, 1994; Kallmeyer, 2010). Furthermore, they can be used to parse dependency structures (Kuhlmann and Satta, 2009). Since LCFRS subsumes various synchronous grammars, they are also important for machine translation. This makes it possible to use LCFRS as a syntactic backbone with which various formalisms can be parsed by compiling grammars into a</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proc. ofACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>Characterizing mildly context-sensitive grammar formalisms.</title>
<date>1988</date>
<contexts>
<context position="3793" citStr="Weir, 1988" startWordPosition="584" endWordPosition="585">rge corpus without the usual length restrictions. SQ VP 460 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 460–470, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics ROOT PROAV VAFIN NN NN VVPP $. Danach habe Kohlenstaub Feuer gefangen . Afterwards had coal dust fire caught . Figure 2: A discontinuous tree from the Negra corpus. Translation: After that coal dust had caught fire. 2 Linear Context-Free Rewriting Systems Linear Context-Free Rewriting Systems (LCFRS; Vijay-Shanker et al., 1987; Weir, 1988) subsume a wide variety of mildly context-sensitive formalisms, such as Tree-Adjoining Grammar (TAG), Combinatory Categorial Grammar (CCG), Minimalist Grammar, Multiple Context-Free Grammar (MCFG) and synchronous CFG (Vijay-Shanker and Weir, 1994; Kallmeyer, 2010). Furthermore, they can be used to parse dependency structures (Kuhlmann and Satta, 2009). Since LCFRS subsumes various synchronous grammars, they are also important for machine translation. This makes it possible to use LCFRS as a syntactic backbone with which various formalisms can be parsed by compiling grammars into an LCFRS, simi</context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>David J. Weir. 1988. Characterizing mildly context-sensitive grammar formalisms.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>