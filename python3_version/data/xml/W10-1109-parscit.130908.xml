<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000581">
<title confidence="0.988447">
Extracting Medication Information from Discharge Summaries
</title>
<author confidence="0.999771">
Scott Halgrim, Fei Xia, Imre Solti, Eithon Cadag ...zlem Uzuner
</author>
<affiliation confidence="0.999755">
University of Washington University of Albany, SUNY
</affiliation>
<address confidence="0.8841995">
PO Box 543450 135 Western Ave
Seattle, WA 98195, USA Albany, NY 12222, USA
</address>
<email confidence="0.941586">
{captnpi,fxia,solti,ecadag}@uw.edu ouzuner@uamail.albany.edu
</email>
<sectionHeader confidence="0.993821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996905">
Extracting medication information from
clinical records has many potential appli-
cations and was the focus of the i2b2
challenge in 2009. We present a hybrid
system, comprised of machine learning
and rule-based modules, for medication
information extraction. With only a hand-
ful of template-filling rules, the system’s
core is a cascade of statistical classifiers
for field detection. It achieved good per-
formance that was comparable to the top
systems in the i2b2 challenge, demon-
strating that a heavily statistical ap-
proach can perform as well or better than
systems with many sophisticated rules.
The system can easily incorporate addi-
tional resources such as medication name
lists to further improve performance.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999091307692308">
Narrative clinical records store patient medical
information, and extracting this information from
these narratives supports data management and
enables many applications (Levin et al., 2007).
Informatics for Integrating the Biology and the
Bedside (i2b2) is an NIH-funded National Center
for Biomedical Computing based at Partners
HealthCare System, and it has organized annual
NLP shared tasks and challenges since 2006
(https://www.i2b2.org/). The Third i2b2 Workshop
on NLP Challenges for Clinical Records in 2009
studied the extraction of medication information
from hospital discharge summaries
</bodyText>
<page confidence="0.995609">
61
</page>
<bodyText confidence="0.999269894736842">
(https://www.i2b2.org/NLP/Medication/), a task
we refer to as the i2b2 challenge in this paper.
In the past decade, there has been extensive re-
search on information extraction in both the gen-
eral and biomedical domains (Wellner et al., 2004;
Grenager et al., 2005; Poon and Domingos, 2007;
Meystre et al, 2008; Rozenfeld and Feldman,
2008). Interestingly, despite the recent prevalence
of statistical approaches in most NLP tasks (in-
cluding information extraction), most of the sys-
tems developed for the i2b2 challenge were rule-
based. In this paper we present our hybrid system,
whose core is a cascade of statistical classifiers that
identify medication fields such as medication
names and dosages. The fields are then assembled
to form medication entries. While our system did
not participate in the i2b2 challenge (as we were
part of the organizing team), it achieved good re-
sults that matched the top i2b2 systems.
</bodyText>
<sectionHeader confidence="0.735227" genericHeader="method">
2 The i2b2 Challenge
</sectionHeader>
<bodyText confidence="0.9913795">
This section provides a brief introduction to the
i2b2 challenge.
</bodyText>
<subsectionHeader confidence="0.977156">
2.1 The task
</subsectionHeader>
<bodyText confidence="0.999978181818182">
The i2b2 challenge studied the automatic extrac-
tion of information corresponding to the following
fields from hospital discharge summaries (Uzuner,
et al., 2010a): names of medications (m) taken by
the patient, dosages (do), modes (mo), frequencies
(f), durations (du), and reasons (r) for taking these
medications. We refer to the medication field as
the name field and the other five fields as the non-
name fields. All non-name fields correspond to
some name field mention; if they were specified
within a two-line window of that name mention,
</bodyText>
<note confidence="0.5363695">
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 61–67,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999849">
the i2b2 challenge required such fields to be linked
to the name field to form an entry. For each entry,
a system must determine whether the entry ap-
peared in a list of medications or in narrative text.
Table 1 shows an excerpt from a discharge sum-
mary and the corresponding entries in the gold
standard. The first entry appears in narrative text,
and the second in a list of medication information.
</bodyText>
<subsectionHeader confidence="0.658096">
Excerpt of Discharge Summary
</subsectionHeader>
<figure confidence="0.964374066666667">
55 the patient noted that he had a recurrence of this
56 vague chest discomfort as he was sitting and
57 talking to friends. He took a sublingual
58 Nitroglycerin without relief.
...
65 Flomax ( Tamsulosin ) 0.4 mg, po, qd,...
Gold standard:
m=“Nitroglycerin” 58:0 58:0 ||do=“nm”||
mo=“sublingual” 57:6 57:6 ||f=“nm” ||du=“nm” ||
r=“vague chest discomfort” 56:0 56:2 ||
ln=”narrative”
...
m=&amp;quot;flomax ( tamsulosin )&amp;quot; 65:0 65:3||do=&amp;quot;0.4 mg&amp;quot;
65:4 65:5||mo=&amp;quot;po&amp;quot; 65:6 65:6||f=&amp;quot;qd&amp;quot; 65:7
65:7||du=&amp;quot;nm&amp;quot;||r=&amp;quot;nm&amp;quot;||ln=&amp;quot;list&amp;quot;
</figure>
<tableCaption confidence="0.973418">
Table 1: A sample discharge summary excerpt and
</tableCaption>
<bodyText confidence="0.994424833333333">
the corresponding entries in the gold standard. The
fields inside an entry are separated by “||”. Each
field is represented by the string and its position
(i.e., “line number: token number” offsets). “nm”
means the field value is not mentioned for this me-
dication name.
</bodyText>
<subsectionHeader confidence="0.999102">
2.2 Data Sets
</subsectionHeader>
<bodyText confidence="0.9946205">
The i2b2 challenge used a total of 1243 discharge
summaries:
</bodyText>
<listItem confidence="0.9475">
• 696 of these summaries were released to par-
ticipants for system development, and the i2b2
organizing team provided the gold standard
annotation for 17 of them.
• Participating teams could choose to annotate
more files themselves. The University of Syd-
ney team annotated 145 out of the 696 summa-
ries (including re-annotating 14 of the 17 files
annotated by the i2b2 organizing team) and
generously shared their annotations with i2b2
after the challenge for future research. We ob-
tained and used 110 of their annotations as our
training set and the remaining 35 summaries as
our development set.
• The participating teams produced system out-
puts for 547 discharge summaries set aside for
testing. After the challenge, 251 of these sum-
</listItem>
<bodyText confidence="0.924133416666667">
maries were annotated by the challenge par-
ticipants, and these 251 summaries formed the
final test set (Uzuner et al., 2010b).
The sizes of the data sets used in our experiments
are shown in Table 2. The training and develop-
ment sets were created by the University of Syd-
ney, and the test data is the i2b2 official challenge
test set. The average number of entries and fields
vary among the three sets because the summaries
in the test set were chosen randomly from the 547
summaries, whereas the University of Sydney team
annotated the longest summaries.
</bodyText>
<subsectionHeader confidence="0.998776">
2.3 Additional resources
</subsectionHeader>
<bodyText confidence="0.999965">
Besides the training data, the participating teams
were allowed to use any additional tools and re-
sources that they had access to, including resources
not available to the public. All challenge partici-
pants used additional resources such as UMLS
(www.nlm.nih.gov/research/umls/), but the exact
resources used varied from team to team. There-
fore, the challenge was similar to the so-called
open-track challenge in the general NLP field, as
opposed to a closed-track challenge that could re-
quire that all the participants use only the list of
resources specified by the challenge organizers
</bodyText>
<subsectionHeader confidence="0.987506">
2.4 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.99995725">
The i2b2 challenge used two sets of evaluation
metrics: horizontal and vertical metrics. Horizon-
tal metrics measured system performance at the
entry level, whereas vertical metrics measured sys-
tem performance at the field level. Both sets of
metrics compared the system output and the gold
standard at the span level for exact match and at
the token level for inexact match, using precision,
recall, and F-score (Uzuner et al., 2010a). The pri-
mary metric for the challenge is exact horizontal F-
score, which is the metric we use to evaluate our
system.
</bodyText>
<page confidence="0.99678">
62
</page>
<table confidence="0.991568">
Data # of # of # of # of # of # of # of # of # of
Sets Summaries Entries Fields Names Doses Freq Modes Duration Reason
Training 110 5970 14886 5684 2929 2740 2146 302 1085
set (54.3) (135.3) (51.7) (26.6) (24.9) (19.5) (2.7) (9.9)
Dev 35 2401 5988 2302 1163 1096 880 111 436
set (68.6) (171.1) (65.8) (33.2) (31.3) (25.1) (3.2) (12.5)
Test 251 8936 22041 8495 4387 3999 3307 511 1342
set (35.6) (87.8) (33.8) (17.5) (15.9) (13.2) (2.0) (5.3)
</table>
<tableCaption confidence="0.9107335">
Table 2: The data sets used in our experiments. The numbers in parentheses are the average numbers
of entries or fields per discharge summary.
</tableCaption>
<subsectionHeader confidence="0.999315">
2.5 Participating systems
</subsectionHeader>
<bodyText confidence="0.999891857142857">
Twenty teams participated in the challenge. Fifteen
teams used rule-based approaches, and the rest
used statistical or hybrid approaches. The perform-
ances of the top five systems are shown in Table 3.
Among them, only the top system, developed by
the University of Sydney, used a hybrid approach,
whereas the rest were rule-based.
</bodyText>
<table confidence="0.9996205">
Rank Precision Recall F-score
1 89.6 82.0 85.7
2 84.0 80.3 82.1
3 86.4 76.6 81.2
4 78.4 82.3 80.3
5 84.1 75.8 79.7
</table>
<tableCaption confidence="0.998405">
Table 3: The performance (exact horizontal preci-
</tableCaption>
<bodyText confidence="0.9306685">
sion/recall/F-score) of the top five i2b2 systems on
the test set.
</bodyText>
<sectionHeader confidence="0.973921" genericHeader="method">
3 System description
</sectionHeader>
<bodyText confidence="0.999992125">
We developed a hybrid system with three process-
ing steps: (1) a preprocessing step that finds sec-
tion boundaries, (2) a field detection step that
identifies the six fields, and (3) a field linking step
that links fields together to form entries. The sec-
ond step is a statistical system, whereas the other
two steps are rule-based. The second step was the
main focus of this study.
</bodyText>
<subsectionHeader confidence="0.998346">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999981444444444">
In addition to common processing steps such as
part-of-speech (POS) tagging, our preprocessing
step includes a section segmenter that breaks dis-
charge summaries into sections. Discharge summa-
ries tend to consist of sections such as ‘ADMIT
DIAGNOSIS’, ‘PAST MEDICAL HISTORY’,
and ‘DISCHARGE MEDICATIONS’. Knowing
section boundaries is important for the i2b2 chal-
lenge because, according to the annotation guide-
lines for creating the gold standard, medications
occurring under certain sections (e.g., family his-
tory and allergic reaction) should be excluded from
the system output. Furthermore, knowing the types
of sections could be useful for field detection and
field linking; for example, entries in the
‘DISCHARGE MEDICATIONS’ section are more
likely to appear in a list of medications than in nar-
rative text.
The set of sections and the exact spelling of
section headings vary across discharge summaries.
The section segmenter uses regular expressions
(e.g., ‘^\s*([A-Z\s]+):’ -- a line starting with a se-
quence of capitalized words followed by a colon)
to collect potential section headings from the train-
ing data, and the headings whose frequencies are
higher than a threshold are used to identify section
boundaries in the discharge summaries.
</bodyText>
<subsectionHeader confidence="0.998966">
3.2 Field detection
</subsectionHeader>
<bodyText confidence="0.999888636363636">
This step consists of three modules: the first mod-
ule, find_name, finds medication names in a dis-
charge summary; the second module, context_type,
processes each medication name identified by
find_name and determines whether the medication
appears in narrative text or in a list of medications;
the third module, find_others, detects the five non-
name field types.
For find_name and find_others, we follow the
common practice of treating named-entity (NE)
detection as a sequence labeling task with the BIO
</bodyText>
<page confidence="0.997525">
63
</page>
<bodyText confidence="0.981156142857143">
tagging scheme; that is, each token in the input is
tagged with B-x (beginning an NE of type x), I-x
(inside an NE of type x) and O (outside any NE).
(F2) look at the output of previous modules: e.g.,
the location of nearby medication names as this
information can be provided by the find_name
module at test time.
</bodyText>
<subsectionHeader confidence="0.726887">
3.2.1 The find_name module
</subsectionHeader>
<bodyText confidence="0.999585">
As this module identifies medication names only,
the tagset under the BIO scheme has three tags: B-
m for beginning of a name, I-m for inside a name,
and O for outside. Various features are used for
this module, which we group into four types:
</bodyText>
<listItem confidence="0.998504375">
• (F1) includes word n-gram features (n=1,2,3).
For instance, the bigram wi-1 wi looks at the
current word and the previous word.
• (F2) contains features that check properties of
the current word and its neighbors (e.g., the
POS tag, the affixes and the length of a word,
the type of section that a word appears in,
whether a word is capitalized, whether a word
is a number, etc.)
• (F3) checks the BIO tags of previous words
• (F4) contains features that check whether n-
grams formed by neighboring words appear as
part of medication names in given medication
name lists. The name lists can come from la-
beled training data or additional resources such
as UMLS.
</listItem>
<subsectionHeader confidence="0.576475">
3.2.2 The context_type module
</subsectionHeader>
<bodyText confidence="0.99997875">
This module is a binary classifier which deter-
mines whether a medication name occurs in a list
or narrative context. Features used by this module
include the section name as identified by the pre-
processing step, the number of commas and words
on the current line, the position of the medication
name on the current line, and the current and near-
by words.
</bodyText>
<subsectionHeader confidence="0.745972">
3.2.3 The find_others module
</subsectionHeader>
<bodyText confidence="0.999981625">
This module complements the find_name module
and uses eleven BIO tags to identify five non-name
fields. The feature set used in this module is very
similar to the one used in find_name except that
some features in (F2) and (F4) are modified to suit
the needs of the non-name fields. For instance, a
feature will check whether a word fits a common
pattern for dosage. In addition, some features in
</bodyText>
<subsectionHeader confidence="0.999551">
3.3 Field linking
</subsectionHeader>
<bodyText confidence="0.999893384615385">
Once medication names and other fields have been
found, the final step is to form entries by associat-
ing each medication name with its related fields.
Our current implementation uses simple heuristics.
First, we go over each non-name field and link it
with the closest preceding medication name unless
the distance between the non-name field and its
closest following medication name is much shorter.
Second, we assemble the (name, non-name) pairs
to form medication entries with a few rules.
More information about the modules discussed in
this section and features used by the modules is
available in (Halgrim, 2009).
</bodyText>
<sectionHeader confidence="0.997526" genericHeader="method">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999983357142857">
In this section, we report the performance of our
system on the development set (Section 4.1-4.3)
and the test set (Section 4.4). The data sets are de-
scribed in Table 2. For all the experiments in this
section, unless specified otherwise, we report exact
horizontal precision/recall/F-score, the primary
metrics for the i2b2 challenge.
For the three modules in the field detection step,
we use the Maximum Entropy (MaxEnt) learner in
the Mallet package (McCallum, 2002) because, in
general, MaxEnt produces good results without
much parameter tuning and the training time for
MaxEnt is much faster than more sophisticated
algorithms such as CRF (Lafferty et al., 2001).
To determine whether the difference between
two systems’ performances is statistically signifi-
cant, we use approximate randomization tests (No-
reen, 1989) as follows. Given two systems that we
would like to compare, we first calculate the dif-
ference between exact horizontal F-scores. Then
two pseudo-system outputs are generated by ran-
domly swapping (at 0.5 probability) the two sys-
tem outputs for each discharge summary. If the
difference between F-scores of these pseudo-
outputs is no less than the original F-score differ-
ence, a counter, cnt, is increased by one. This
process was repeated n=10,000 times, and the p-
value of the significance is equal to (cnt+1)/(n+1).
</bodyText>
<page confidence="0.998684">
64
</page>
<bodyText confidence="0.999882666666667">
If the p-value is smaller than a predefined thresh-
old (e.g., 0.05), we conclude that the difference
between the two systems is statistically significant.
</bodyText>
<subsectionHeader confidence="0.9897615">
4.1 Performance of the whole system
4.1.1 Effect of feature sets
</subsectionHeader>
<bodyText confidence="0.999942125">
To test the effect of feature sets on system per-
formance, we trained find_name and find_others
with different feature sets and tested the whole sys-
tem on the development set. For (F4), we used two
medication name lists. The first list consists of
medication names gathered from the training data.
The second list includes drug names from the FDA
database
(www.accessdata.fda.gov/scripts/cder/ndc/). We
use the second list to test whether adding features
that check the information in an additional re-
source could improve the system performance.
The results are in Table 4. For the last two rows,
F1-F4a uses the first medication name list, and F1-
F4b uses both lists. The F-score difference between
all adjacent rows is statistically significant at
p&lt;0.01, except for the pair F1-F3 vs. F1-F4a. It is
not surprising that using the first medication name
list on top of F1-F3 does not improve the perform-
ance, as the same kind of information has already
been captured by F1 features. The improvement of
F1-F4b over F1-F4a shows that the system can
easily incorporate additional resources and achieve
a statistically significant (at p&lt;0.01) gain.
</bodyText>
<table confidence="0.999434333333333">
Features Precision Recall F-score
F1 72.5 60.3 65.8
F1-F2 82.5 78.2 80.3
F1-F3 88.4 77.9 82.8
F1-F4a 87.4 77.9 82.4
F1-F4b 88.1 79.4 83.5
</table>
<tableCaption confidence="0.9695345">
Table 4: System performance on the development
set with different feature sets
</tableCaption>
<subsectionHeader confidence="0.805911">
4.1.2 Effect of training data size
</subsectionHeader>
<bodyText confidence="0.995496">
Figure 1 shows the system performance on the de-
velopment set when different portions of the train-
ing set are used for training. The curve with “+”
signs represents the results for F1-F4b, and the
curve with circles represents the results for F1-F4a.
The figure illustrates that, as the training data size
increases, the F-score with both feature sets im-
proves. In addition, the additional resource is most
helpful when the training data size is small, as in-
dicated by the decreasing gap between the two sets
of F-scores when the size of training data in-
creases.
</bodyText>
<figureCaption confidence="0.92922">
Figure 1: System performance on the development
</figureCaption>
<bodyText confidence="0.901617333333333">
set with different training data sizes (Legend: ○
represents F-scores with features in F1-F4a; + rep-
resents F-scores with features in F1-F4b)
</bodyText>
<subsubsectionHeader confidence="0.51882">
4.1.3 Pipeline vs. find_all
</subsubsectionHeader>
<bodyText confidence="0.998674882352941">
The current field detection step is a pipeline ap-
proach with three modules: find_name, con-
text_type, and find_others. Having three separate
modules allows each module to choose the features
that are most appropriate for it. In addition, later
modules can use features that check the output of
the previous modules. A potential downside of the
pipeline system is that the errors in the early mod-
ule would propagate to later modules. An alterna-
tive is to use a single module to detect all six field
types together.
Figure 2 shows the result of find_all in compari-
son to the result for the three-module pipeline.
Both use the F1-F4b feature sets, except that
find_others uses some features that check the out-
put of previous modules which are not available to
find_all.
</bodyText>
<page confidence="0.998263">
65
</page>
<table confidence="0.9999695">
Precision Recall F-score
Name 91.2 88.5 89.9
Dosage 96.6 90.8 93.6
Frequency 93.9 89.0 91.8
Mode 95.7 90.3 92.9
Duration 73.8 43.2 54.5
Reason 72.2 31.0 43.3
All fields 92.6 84.5 88.4
</table>
<tableCaption confidence="0.863524666666667">
Table 5: The performance (exact preci-
sion/recall/F-score) of field detection on the devel-
opment set.
</tableCaption>
<figureCaption confidence="0.965985">
Figure 2: Pipeline vs. find_all for field detection
(Legend: ○ represents F-scores with find_all; +
represents F-scores with the three-module pipeline)
</figureCaption>
<bodyText confidence="0.999963733333333">
Interestingly, when 10% of the training set is
used for training, find_all has a higher F-score than
the pipeline approach, although the difference is
not statistically significant at p≤0.05. As more data
is used for training, the pipeline outperforms
find_all, and when at least 50% of the training data
is used, the difference between the two is statisti-
cally significant at p≤0.05. One possible explana-
tion for this phenomenon is that as more training
data becomes available, the early modules in the
pipeline make fewer errors; as a result, the disad-
vantage of the pipeline approach caused by error
propagation is outweighed by the advantage that
the later modules in the pipeline can use features
that check the output of the earlier modules.
</bodyText>
<subsectionHeader confidence="0.99355">
4.2 Performance of the field detection step
</subsectionHeader>
<bodyText confidence="0.999854625">
Table 5 shows the exact precision/recall/F-score on
identifying the six field types, using all the training
data, F1-F4b features, and the pipeline approach
for field detection. A span in the system output
exactly matches a span in the gold standard if the
two spans are identical and have the same field
type. Among the six fields, the results for duration
and reason are the lowest. That is because duration
and reason are longer phrases than the other four
field types and there are fewer strong, reliable cues
to signal their presence.
When making the narrative/list distinction, the
accuracy of our context_type module is 95.4%. In
contrast, the accuracy of the baseline (which treats
each medication name as in a list context) is only
55.6%.
</bodyText>
<subsectionHeader confidence="0.994737">
4.3 Performance of the field linking step
</subsectionHeader>
<bodyText confidence="0.9997636875">
In order to evaluate the field linking step, we gen-
erated a list of (name, non-name) pairs from the
gold standard, where the name and non-name
fields appear in the same entry in the gold stan-
dard. We then compared these pairs with the ones
produced by the field linking step and calculated
precision/recall/F-score. Table 6 shows the result
of two experiments: in the cheating experiment, the
input to the field linking step is the fields from the
gold standard; in the non-cheating experiment, the
input is the output of the field detection step. These
experiments show that, while the heuristic rules
used in this step work reasonably well when the
input is accurate, the performance deteriorates con-
siderably when the input is noisy, an issue we plan
to address in future work.
</bodyText>
<table confidence="0.999522333333333">
Precision Recall F-score
Non-cheating 87.4 75.1 80.8
Cheating 96.2 94.5 95.3
</table>
<tableCaption confidence="0.989541">
Table 6: The performance of the field linking step
</tableCaption>
<bodyText confidence="0.852158333333333">
on the development set (cheating: assuming perfect
field input; non-cheating: using the output of the
field detection step)
</bodyText>
<subsectionHeader confidence="0.90459">
4.4 Results on the test data
</subsectionHeader>
<bodyText confidence="0.99952075">
Table 7 shows the system performance on the i2b2
official test data. The system was trained on the
union of the training and development data. Com-
pared with the top five i2b2 systems (see Table 3),
our system was second only to the best i2b2 sys-
tem, which used more resources and more sophis-
ticated rules for field linking (Patrick and Li,
2009).
</bodyText>
<page confidence="0.967345">
66
</page>
<table confidence="0.999977333333333">
Precision Recall F-score
Horizontal 88.6 80.2 84.1
Name 92.6 87.1 89.8
Dosage 96.3 90.2 93.1
Frequency 95.6 90.8 93.2
Mode 96.7 90.2 93.3
Duration 70.6 40.5 51.5
Reason 73.4 34.7 47.1
All fields 91.6 82.7 86.9
</table>
<tableCaption confidence="0.688295">
Table 7: System performance on the test set when
trained on the union of the training and the devel-
opment sets with F1-F4b features.
</tableCaption>
<sectionHeader confidence="0.998287" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999996555555556">
We present a hybrid system for medication extrac-
tion. The system is built around a pipeline of cas-
cading statistical classifiers for field detection. It
achieves good performance that is comparable to
the top systems in the i2b2 challenge, and incorpo-
rating additional resources as features further im-
proves the performance. In the future, we plan to
replace the current rule-based field linking module
with a statistical module to improve accuracy.
</bodyText>
<sectionHeader confidence="0.998218" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9963248">
This work was supported in part by US DOD grant
N00244-091-0081 and NIH Grants
1K99LM010227-0110, U54LM008748, and
T15LM007442-06. We would also like to thank
three anonymous reviewers for helpful comments.
</bodyText>
<sectionHeader confidence="0.999241" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99952014">
Trond Grenager, Dan Klein, and Christopher Manning.
2005. Unsupervised learning of field segmentation
models for information extraction. In Proc. of ACL-
2005.
Scott Halgrim. 2009. A Pipeline Machine Learning Ap-
proach to Biomedical Information Extraction. Master
Thesis. University of Washington.
J. Lafferty and A. McCallum and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the
18th International Conference on Machine Learning
(ICML-2001).
Matthew A. Levin, Marina Krol, Ankur M. Doshi, and
David L. Reich. 2007. Extraction and mapping of
drug names from free text to a standardized nomen-
clature. AMIA Symposium Proceedings, pp 438-442.
S. Meystre, G. Savova, K. Kipper-Schuler, and J. Hur-
dle. 2008. Extracting Information from Textual
Documents in the Electronic Health Record: A Re-
view of Recent Research. IMIA Yearbook of Medical
Informatics Methods Inf Med 2008; 47 Suppl 1:128-
44.
Andrew McCallum. 2002. Mallet: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu
Eric W. Noreen. 1989. Computer intensive methods for
testing hypotheses: an introduction. John Wiley &amp;
Sons.
Jon Patrick and Min Li, 2009. A Cascade Approach to
Extract Medication Event (i2b2 challenge 2009).
Presentation at the Third i2b2 Workshop, November
2009, San Francisco, CA.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proc. of the Na-
tional Conference on Artificial Intelligence (AAAI),
pp 913-918.
Benjamin Rozenfeld and Ronen Feldman. 2008. Self-
supervised relation extraction from the web. Knowl-
edge and Information Systems, 17(1):17-33.
...zlem Uzuner, Imre Solti, and Eithon Cadag, 2010a.
Extracting Medication Information from Clinical
Text. Submitted to JAMIA.
...zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag,
2010b. Community Annotation Experiment for
Ground Truth Generation for the i2b2 Medication
Challenge. Submitted to JAMIA.
Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Proc. of the 20th Con-
ference on Uncertainty in AI (UAI-2004).
</reference>
<page confidence="0.999501">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.500693">
<title confidence="0.999619">Extracting Medication Information from Discharge Summaries</title>
<author confidence="0.966257">Fei Xia Halgrim</author>
<author confidence="0.966257">Imre Solti</author>
<author confidence="0.966257">Eithon Cadag Uzuner</author>
<affiliation confidence="0.7601235">University of Washington University of Albany, SUNY PO Box 543450 135 Western Ave</affiliation>
<address confidence="0.997419">Seattle, WA 98195, USA Albany, NY 12222, USA</address>
<email confidence="0.999681">captnpi@uw.eduouzuner@uamail.albany.edu</email>
<email confidence="0.999681">fxia@uw.eduouzuner@uamail.albany.edu</email>
<email confidence="0.999681">solti@uw.eduouzuner@uamail.albany.edu</email>
<email confidence="0.999681">ecadag@uw.eduouzuner@uamail.albany.edu</email>
<abstract confidence="0.998192789473685">Extracting medication information from clinical records has many potential applications and was the focus of the i2b2 challenge in 2009. We present a hybrid system, comprised of machine learning and rule-based modules, for medication information extraction. With only a handful of template-filling rules, the system’s core is a cascade of statistical classifiers for field detection. It achieved good performance that was comparable to the top systems in the i2b2 challenge, demonstrating that a heavily statistical approach can perform as well or better than systems with many sophisticated rules. The system can easily incorporate additional resources such as medication name lists to further improve performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Unsupervised learning of field segmentation models for information extraction.</title>
<date>2005</date>
<booktitle>In Proc. of ACL2005.</booktitle>
<contexts>
<context position="1918" citStr="Grenager et al., 2005" startWordPosition="273" endWordPosition="276">de (i2b2) is an NIH-funded National Center for Biomedical Computing based at Partners HealthCare System, and it has organized annual NLP shared tasks and challenges since 2006 (https://www.i2b2.org/). The Third i2b2 Workshop on NLP Challenges for Clinical Records in 2009 studied the extraction of medication information from hospital discharge summaries 61 (https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper. In the past decade, there has been extensive research on information extraction in both the general and biomedical domains (Wellner et al., 2004; Grenager et al., 2005; Poon and Domingos, 2007; Meystre et al, 2008; Rozenfeld and Feldman, 2008). Interestingly, despite the recent prevalence of statistical approaches in most NLP tasks (including information extraction), most of the systems developed for the i2b2 challenge were rulebased. In this paper we present our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the organizing team), it a</context>
</contexts>
<marker>Grenager, Klein, Manning, 2005</marker>
<rawString>Trond Grenager, Dan Klein, and Christopher Manning. 2005. Unsupervised learning of field segmentation models for information extraction. In Proc. of ACL2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Halgrim</author>
</authors>
<title>A Pipeline Machine Learning Approach to Biomedical Information Extraction. Master Thesis.</title>
<date>2009</date>
<institution>University of Washington.</institution>
<contexts>
<context position="13299" citStr="Halgrim, 2009" startWordPosition="2160" endWordPosition="2161"> medication names and other fields have been found, the final step is to form entries by associating each medication name with its related fields. Our current implementation uses simple heuristics. First, we go over each non-name field and link it with the closest preceding medication name unless the distance between the non-name field and its closest following medication name is much shorter. Second, we assemble the (name, non-name) pairs to form medication entries with a few rules. More information about the modules discussed in this section and features used by the modules is available in (Halgrim, 2009). 4 Experimental results In this section, we report the performance of our system on the development set (Section 4.1-4.3) and the test set (Section 4.4). The data sets are described in Table 2. For all the experiments in this section, unless specified otherwise, we report exact horizontal precision/recall/F-score, the primary metrics for the i2b2 challenge. For the three modules in the field detection step, we use the Maximum Entropy (MaxEnt) learner in the Mallet package (McCallum, 2002) because, in general, MaxEnt produces good results without much parameter tuning and the training time for</context>
</contexts>
<marker>Halgrim, 2009</marker>
<rawString>Scott Halgrim. 2009. A Pipeline Machine Learning Approach to Biomedical Information Extraction. Master Thesis. University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of the 18th International Conference on Machine Learning (ICML-2001).</booktitle>
<contexts>
<context position="13992" citStr="Lafferty et al., 2001" startWordPosition="2267" endWordPosition="2270">our system on the development set (Section 4.1-4.3) and the test set (Section 4.4). The data sets are described in Table 2. For all the experiments in this section, unless specified otherwise, we report exact horizontal precision/recall/F-score, the primary metrics for the i2b2 challenge. For the three modules in the field detection step, we use the Maximum Entropy (MaxEnt) learner in the Mallet package (McCallum, 2002) because, in general, MaxEnt produces good results without much parameter tuning and the training time for MaxEnt is much faster than more sophisticated algorithms such as CRF (Lafferty et al., 2001). To determine whether the difference between two systems’ performances is statistically significant, we use approximate randomization tests (Noreen, 1989) as follows. Given two systems that we would like to compare, we first calculate the difference between exact horizontal F-scores. Then two pseudo-system outputs are generated by randomly swapping (at 0.5 probability) the two system outputs for each discharge summary. If the difference between F-scores of these pseudooutputs is no less than the original F-score difference, a counter, cnt, is increased by one. This process was repeated n=10,0</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty and A. McCallum and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of the 18th International Conference on Machine Learning (ICML-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew A Levin</author>
<author>Marina Krol</author>
<author>Ankur M Doshi</author>
<author>David L Reich</author>
</authors>
<title>Extraction and mapping of drug names from free text to a standardized nomenclature. AMIA Symposium Proceedings,</title>
<date>2007</date>
<pages>438--442</pages>
<contexts>
<context position="1242" citStr="Levin et al., 2007" startWordPosition="173" endWordPosition="176">, the system’s core is a cascade of statistical classifiers for field detection. It achieved good performance that was comparable to the top systems in the i2b2 challenge, demonstrating that a heavily statistical approach can perform as well or better than systems with many sophisticated rules. The system can easily incorporate additional resources such as medication name lists to further improve performance. 1 Introduction Narrative clinical records store patient medical information, and extracting this information from these narratives supports data management and enables many applications (Levin et al., 2007). Informatics for Integrating the Biology and the Bedside (i2b2) is an NIH-funded National Center for Biomedical Computing based at Partners HealthCare System, and it has organized annual NLP shared tasks and challenges since 2006 (https://www.i2b2.org/). The Third i2b2 Workshop on NLP Challenges for Clinical Records in 2009 studied the extraction of medication information from hospital discharge summaries 61 (https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper. In the past decade, there has been extensive research on information extraction in both the</context>
</contexts>
<marker>Levin, Krol, Doshi, Reich, 2007</marker>
<rawString>Matthew A. Levin, Marina Krol, Ankur M. Doshi, and David L. Reich. 2007. Extraction and mapping of drug names from free text to a standardized nomenclature. AMIA Symposium Proceedings, pp 438-442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Meystre</author>
<author>G Savova</author>
<author>K Kipper-Schuler</author>
<author>J Hurdle</author>
</authors>
<title>Extracting Information from Textual Documents in the Electronic Health Record: A Review of Recent Research.</title>
<date>2008</date>
<booktitle>IMIA Yearbook of Medical Informatics Methods Inf Med 2008; 47 Suppl</booktitle>
<pages>1--128</pages>
<contexts>
<context position="1964" citStr="Meystre et al, 2008" startWordPosition="281" endWordPosition="284">iomedical Computing based at Partners HealthCare System, and it has organized annual NLP shared tasks and challenges since 2006 (https://www.i2b2.org/). The Third i2b2 Workshop on NLP Challenges for Clinical Records in 2009 studied the extraction of medication information from hospital discharge summaries 61 (https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper. In the past decade, there has been extensive research on information extraction in both the general and biomedical domains (Wellner et al., 2004; Grenager et al., 2005; Poon and Domingos, 2007; Meystre et al, 2008; Rozenfeld and Feldman, 2008). Interestingly, despite the recent prevalence of statistical approaches in most NLP tasks (including information extraction), most of the systems developed for the i2b2 challenge were rulebased. In this paper we present our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the organizing team), it achieved good results that matched the top i2b2</context>
</contexts>
<marker>Meystre, Savova, Kipper-Schuler, Hurdle, 2008</marker>
<rawString>S. Meystre, G. Savova, K. Kipper-Schuler, and J. Hurdle. 2008. Extracting Information from Textual Documents in the Electronic Health Record: A Review of Recent Research. IMIA Yearbook of Medical Informatics Methods Inf Med 2008; 47 Suppl 1:128-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Mallet: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu</note>
<contexts>
<context position="13793" citStr="McCallum, 2002" startWordPosition="2238" endWordPosition="2239"> information about the modules discussed in this section and features used by the modules is available in (Halgrim, 2009). 4 Experimental results In this section, we report the performance of our system on the development set (Section 4.1-4.3) and the test set (Section 4.4). The data sets are described in Table 2. For all the experiments in this section, unless specified otherwise, we report exact horizontal precision/recall/F-score, the primary metrics for the i2b2 challenge. For the three modules in the field detection step, we use the Maximum Entropy (MaxEnt) learner in the Mallet package (McCallum, 2002) because, in general, MaxEnt produces good results without much parameter tuning and the training time for MaxEnt is much faster than more sophisticated algorithms such as CRF (Lafferty et al., 2001). To determine whether the difference between two systems’ performances is statistically significant, we use approximate randomization tests (Noreen, 1989) as follows. Given two systems that we would like to compare, we first calculate the difference between exact horizontal F-scores. Then two pseudo-system outputs are generated by randomly swapping (at 0.5 probability) the two system outputs for e</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. Mallet: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer intensive methods for testing hypotheses: an introduction.</title>
<date>1989</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="14147" citStr="Noreen, 1989" startWordPosition="2289" endWordPosition="2291"> unless specified otherwise, we report exact horizontal precision/recall/F-score, the primary metrics for the i2b2 challenge. For the three modules in the field detection step, we use the Maximum Entropy (MaxEnt) learner in the Mallet package (McCallum, 2002) because, in general, MaxEnt produces good results without much parameter tuning and the training time for MaxEnt is much faster than more sophisticated algorithms such as CRF (Lafferty et al., 2001). To determine whether the difference between two systems’ performances is statistically significant, we use approximate randomization tests (Noreen, 1989) as follows. Given two systems that we would like to compare, we first calculate the difference between exact horizontal F-scores. Then two pseudo-system outputs are generated by randomly swapping (at 0.5 probability) the two system outputs for each discharge summary. If the difference between F-scores of these pseudooutputs is no less than the original F-score difference, a counter, cnt, is increased by one. This process was repeated n=10,000 times, and the pvalue of the significance is equal to (cnt+1)/(n+1). 64 If the p-value is smaller than a predefined threshold (e.g., 0.05), we conclude </context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer intensive methods for testing hypotheses: an introduction. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Patrick</author>
<author>Min Li</author>
</authors>
<title>A Cascade Approach to Extract Medication Event</title>
<date>2009</date>
<booktitle>(i2b2 challenge 2009). Presentation at the Third i2b2 Workshop,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="21258" citStr="Patrick and Li, 2009" startWordPosition="3466" endWordPosition="3469">sion Recall F-score Non-cheating 87.4 75.1 80.8 Cheating 96.2 94.5 95.3 Table 6: The performance of the field linking step on the development set (cheating: assuming perfect field input; non-cheating: using the output of the field detection step) 4.4 Results on the test data Table 7 shows the system performance on the i2b2 official test data. The system was trained on the union of the training and development data. Compared with the top five i2b2 systems (see Table 3), our system was second only to the best i2b2 system, which used more resources and more sophisticated rules for field linking (Patrick and Li, 2009). 66 Precision Recall F-score Horizontal 88.6 80.2 84.1 Name 92.6 87.1 89.8 Dosage 96.3 90.2 93.1 Frequency 95.6 90.8 93.2 Mode 96.7 90.2 93.3 Duration 70.6 40.5 51.5 Reason 73.4 34.7 47.1 All fields 91.6 82.7 86.9 Table 7: System performance on the test set when trained on the union of the training and the development sets with F1-F4b features. 5 Conclusion We present a hybrid system for medication extraction. The system is built around a pipeline of cascading statistical classifiers for field detection. It achieves good performance that is comparable to the top systems in the i2b2 challenge,</context>
</contexts>
<marker>Patrick, Li, 2009</marker>
<rawString>Jon Patrick and Min Li, 2009. A Cascade Approach to Extract Medication Event (i2b2 challenge 2009). Presentation at the Third i2b2 Workshop, November 2009, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint inference in information extraction.</title>
<date>2007</date>
<booktitle>In Proc. of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>913--918</pages>
<contexts>
<context position="1943" citStr="Poon and Domingos, 2007" startWordPosition="277" endWordPosition="280">ded National Center for Biomedical Computing based at Partners HealthCare System, and it has organized annual NLP shared tasks and challenges since 2006 (https://www.i2b2.org/). The Third i2b2 Workshop on NLP Challenges for Clinical Records in 2009 studied the extraction of medication information from hospital discharge summaries 61 (https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper. In the past decade, there has been extensive research on information extraction in both the general and biomedical domains (Wellner et al., 2004; Grenager et al., 2005; Poon and Domingos, 2007; Meystre et al, 2008; Rozenfeld and Feldman, 2008). Interestingly, despite the recent prevalence of statistical approaches in most NLP tasks (including information extraction), most of the systems developed for the i2b2 challenge were rulebased. In this paper we present our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the organizing team), it achieved good results that</context>
</contexts>
<marker>Poon, Domingos, 2007</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2007. Joint inference in information extraction. In Proc. of the National Conference on Artificial Intelligence (AAAI), pp 913-918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rozenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>Selfsupervised relation extraction from the web. Knowledge and Information Systems,</title>
<date>2008</date>
<pages>17--1</pages>
<contexts>
<context position="1994" citStr="Rozenfeld and Feldman, 2008" startWordPosition="285" endWordPosition="288">ased at Partners HealthCare System, and it has organized annual NLP shared tasks and challenges since 2006 (https://www.i2b2.org/). The Third i2b2 Workshop on NLP Challenges for Clinical Records in 2009 studied the extraction of medication information from hospital discharge summaries 61 (https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper. In the past decade, there has been extensive research on information extraction in both the general and biomedical domains (Wellner et al., 2004; Grenager et al., 2005; Poon and Domingos, 2007; Meystre et al, 2008; Rozenfeld and Feldman, 2008). Interestingly, despite the recent prevalence of statistical approaches in most NLP tasks (including information extraction), most of the systems developed for the i2b2 challenge were rulebased. In this paper we present our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the organizing team), it achieved good results that matched the top i2b2 systems. 2 The i2b2 Challenge</context>
</contexts>
<marker>Rozenfeld, Feldman, 2008</marker>
<rawString>Benjamin Rozenfeld and Ronen Feldman. 2008. Selfsupervised relation extraction from the web. Knowledge and Information Systems, 17(1):17-33.</rawString>
</citation>
<citation valid="false">
<authors>
<author>zlem Uzuner</author>
</authors>
<title>Imre Solti, and Eithon Cadag, 2010a. Extracting Medication Information from Clinical Text.</title>
<note>Submitted to JAMIA.</note>
<marker>Uzuner, </marker>
<rawString>...zlem Uzuner, Imre Solti, and Eithon Cadag, 2010a. Extracting Medication Information from Clinical Text. Submitted to JAMIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>zlem Uzuner</author>
<author>Imre Solti</author>
<author>Fei Xia</author>
<author>Eithon Cadag</author>
</authors>
<title>Community Annotation Experiment for Ground Truth Generation for the i2b2 Medication Challenge.</title>
<date>2010</date>
<note>Submitted to JAMIA.</note>
<contexts>
<context position="2834" citStr="Uzuner, et al., 2010" startWordPosition="418" endWordPosition="421"> our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the organizing team), it achieved good results that matched the top i2b2 systems. 2 The i2b2 Challenge This section provides a brief introduction to the i2b2 challenge. 2.1 The task The i2b2 challenge studied the automatic extraction of information corresponding to the following fields from hospital discharge summaries (Uzuner, et al., 2010a): names of medications (m) taken by the patient, dosages (do), modes (mo), frequencies (f), durations (du), and reasons (r) for taking these medications. We refer to the medication field as the name field and the other five fields as the nonname fields. All non-name fields correspond to some name field mention; if they were specified within a two-line window of that name mention, Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 61–67, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics the i2b2 challen</context>
<context position="5610" citStr="Uzuner et al., 2010" startWordPosition="871" endWordPosition="874">iversity of Sydney team annotated 145 out of the 696 summaries (including re-annotating 14 of the 17 files annotated by the i2b2 organizing team) and generously shared their annotations with i2b2 after the challenge for future research. We obtained and used 110 of their annotations as our training set and the remaining 35 summaries as our development set. • The participating teams produced system outputs for 547 discharge summaries set aside for testing. After the challenge, 251 of these summaries were annotated by the challenge participants, and these 251 summaries formed the final test set (Uzuner et al., 2010b). The sizes of the data sets used in our experiments are shown in Table 2. The training and development sets were created by the University of Sydney, and the test data is the i2b2 official challenge test set. The average number of entries and fields vary among the three sets because the summaries in the test set were chosen randomly from the 547 summaries, whereas the University of Sydney team annotated the longest summaries. 2.3 Additional resources Besides the training data, the participating teams were allowed to use any additional tools and resources that they had access to, including r</context>
<context position="7112" citStr="Uzuner et al., 2010" startWordPosition="1114" endWordPosition="1117">NLP field, as opposed to a closed-track challenge that could require that all the participants use only the list of resources specified by the challenge organizers 2.4 Evaluation metrics The i2b2 challenge used two sets of evaluation metrics: horizontal and vertical metrics. Horizontal metrics measured system performance at the entry level, whereas vertical metrics measured system performance at the field level. Both sets of metrics compared the system output and the gold standard at the span level for exact match and at the token level for inexact match, using precision, recall, and F-score (Uzuner et al., 2010a). The primary metric for the challenge is exact horizontal Fscore, which is the metric we use to evaluate our system. 62 Data # of # of # of # of # of # of # of # of # of Sets Summaries Entries Fields Names Doses Freq Modes Duration Reason Training 110 5970 14886 5684 2929 2740 2146 302 1085 set (54.3) (135.3) (51.7) (26.6) (24.9) (19.5) (2.7) (9.9) Dev 35 2401 5988 2302 1163 1096 880 111 436 set (68.6) (171.1) (65.8) (33.2) (31.3) (25.1) (3.2) (12.5) Test 251 8936 22041 8495 4387 3999 3307 511 1342 set (35.6) (87.8) (33.8) (17.5) (15.9) (13.2) (2.0) (5.3) Table 2: The data sets used in our </context>
</contexts>
<marker>Uzuner, Solti, Xia, Cadag, 2010</marker>
<rawString>...zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag, 2010b. Community Annotation Experiment for Ground Truth Generation for the i2b2 Medication Challenge. Submitted to JAMIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
<author>Andrew McCallum</author>
<author>Fuchun Peng</author>
<author>Michael Hay</author>
</authors>
<title>An integrated, conditional model of information extraction and coreference with application to citation matching.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th Conference on Uncertainty in AI (UAI-2004).</booktitle>
<contexts>
<context position="1895" citStr="Wellner et al., 2004" startWordPosition="269" endWordPosition="272"> Biology and the Bedside (i2b2) is an NIH-funded National Center for Biomedical Computing based at Partners HealthCare System, and it has organized annual NLP shared tasks and challenges since 2006 (https://www.i2b2.org/). The Third i2b2 Workshop on NLP Challenges for Clinical Records in 2009 studied the extraction of medication information from hospital discharge summaries 61 (https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper. In the past decade, there has been extensive research on information extraction in both the general and biomedical domains (Wellner et al., 2004; Grenager et al., 2005; Poon and Domingos, 2007; Meystre et al, 2008; Rozenfeld and Feldman, 2008). Interestingly, despite the recent prevalence of statistical approaches in most NLP tasks (including information extraction), most of the systems developed for the i2b2 challenge were rulebased. In this paper we present our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the</context>
</contexts>
<marker>Wellner, McCallum, Peng, Hay, 2004</marker>
<rawString>Ben Wellner, Andrew McCallum, Fuchun Peng, and Michael Hay. 2004. An integrated, conditional model of information extraction and coreference with application to citation matching. In Proc. of the 20th Conference on Uncertainty in AI (UAI-2004).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>