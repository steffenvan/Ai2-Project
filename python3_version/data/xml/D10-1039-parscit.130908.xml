<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003084">
<title confidence="0.999618">
A Semi-Supervised Approach to Improve Classification of
Infrequent Discourse Relations using Feature Vector Extension
</title>
<author confidence="0.732079">
Hugo Hernault Danushka Bollegala Mitsuru Ishizuka
</author>
<email confidence="0.311875">
hugo@mi.ci.i. danushka@iba.t. ishizuka@i.
</email>
<author confidence="0.36377">
u-tokyo.ac.jp u-tokyo.ac.jp u-tokyo.ac.jp
</author>
<affiliation confidence="0.979251">
Graduate School of Information Science &amp; Technology
The University of Tokyo
</affiliation>
<address confidence="0.704232">
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
</address>
<sectionHeader confidence="0.953627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819777777778">
Several recent discourse parsers have em-
ployed fully-supervised machine learning ap-
proaches. These methods require human an-
notators to beforehand create an extensive
training corpus, which is a time-consuming
and costly process. On the other hand, un-
labeled data is abundant and cheap to col-
lect. In this paper, we propose a novel
semi-supervised method for discourse rela-
tion classification based on the analysis of co-
occurring features in unlabeled data, which is
then taken into account for extending the fea-
ture vectors given to a classifier. Our exper-
imental results on the RST Discourse Tree-
bank corpus and Penn Discourse Treebank in-
dicate that the proposed method brings a sig-
nificant improvement in classification accu-
racy and macro-average F-score when small
training datasets are used. For instance, with
training sets of c.a. 1000 labeled instances, the
proposed method brings improvements in ac-
curacy and macro-average F-score up to 50%
compared to a baseline classifier. We believe
that the proposed method is a first step towards
detecting low-occurrence relations, which is
useful for domains with a lack of annotated
data.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999830476190476">
Automatic detection of discourse relations in natu-
ral language text is important for numerous tasks in
NLP, such as sentiment analysis (Somasundaran et
al., 2009), text summarization (Marcu, 2000) and di-
alogue generation (Piwek et al., 2007). However,
most of the recent work employing discourse re-
lation classifiers are based on fully-supervised ma-
chine learning approaches (duVerle and Prendinger,
2009; Pitler et al., 2009; Lin et al., 2009). Two
of the main corpora with discourse annotations are
the RST Discourse Treebank (RSTDT) (Carlson et
al., 2001) and the Penn Discourse Treebank (PDTB)
(Prasad et al., 2008a), which are both based on the
Wall Street Journal (WSJ) corpus.
In the RSTDT, annotation is done using 78
fine-grained discourse relations, which are usually
grouped into 18 coarser-grained relations. Each of
these relations has furthermore several possible con-
figurations for its arguments—its ‘nuclearity’ (Mann
and Thompson, 1988). In practice, a classifier
trained on these coarse-grained relations must solve
a 41-class classification problem. Some of the re-
lations corresponding to these classes are relatively
more frequent in the corpus, such as the ELAB-
ORATION[N][S] relation (4441 instances), or the
ATTRIBUTION[S][N] relation (1612 instances).1
However, other relation types occur very rarely,
such as TOPIC-COMMENT[S][N] (2 instances), or
EVALUATION[N][N] (3 instances). A similar phe-
nomenon can be observed in PDTB, in which 15
level-two relations are employed: Some, such as
EXPANSION.CONJUNCTION, occur as often as 8759
times throughout the corpus, whereas the remainder
of the relations, such as EXPANSION.EXCEPTION
and COMPARISON.PRAGMATIC CONCESSION, can
appear as rarely as 17 and 12 times respectively. Al-
though supervised approaches to discourse relation
learning achieve good results on frequent relations,
performance is poor on rare relation types (duVerle
and Prendinger, 2009).
Nonetheless, certain infrequent relation types
might be important for specific tasks. For instance,
</bodyText>
<footnote confidence="0.9283675">
1We use the notation [N] and [S] respectively to denote the
nucleus and satellite in a RST discourse relation.
</footnote>
<page confidence="0.960541">
399
</page>
<note confidence="0.8185885">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 399–409,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997443642857143">
capturing the RST TOPIC-COMMENT[S][N] and
EVALUATION[N][N] relations can be useful for
sentiment analysis (Pang and Lee, 2008).
Another situation where detection of low-
occurring relations is desirable is the case where we
have only a small training set at our disposal, for in-
stance when there is not enough annotated data for
all the relation types described in a discourse the-
ory. In this case, all the dataset’s relations can be
considered rare, and being able to build an efficient
classifier depends on the capacity to deal with this
lack of annotated data.
Our contributions in this paper are summarized as
follows.
</bodyText>
<listItem confidence="0.891238">
• We propose a semi-supervised method that
exploits the abundant, freely-available unla-
beled data, which is harvested for feature co-
occurrence information, and used as a basis to
extend feature vectors to help classification for
cases where unknown features are found in test
vectors.
• The proposed method is evaluated on the
RSTDT and PDTB corpus, where it signifi-
cantly improves accuracy and macro-average
</listItem>
<bodyText confidence="0.995700666666667">
F-score when small training sets are used. For
instance, when trained on moderately small
datasets with ca. 1000 instances, the proposed
method increases the macro-average F-score
and accuracy up to 50%, compared to a base-
line classifier.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999957206349206">
Since the release in 2001 of the RSTDT corpus,
several fully-supervised discourse parsers have been
built in the RST framework. In the recent work of
duVerle and Prendinger (2009), a discourse parser
based on Support Vector Machines (SVM) (Vapnik,
1995) is proposed. SVMs are employed to train two
classifiers: One, binary, for determining the pres-
ence of a relation, and another, multi-class, for deter-
mining the relation label between related text spans.
For the discourse relation classifier, shallow lexical,
syntactic and structural features, including ‘domi-
nance sets’ (Soricut and Marcu, 2003) are used. For
relation classification, they report an accuracy of
0.668, and an F-score of 0.509 for the creation of
the full discourse tree.
The unsupervised method of Marcu and Echihabi
(2002) was the first that tried to detect implicit rela-
tions (i.e. relations not accompanied by a cue phrase,
such as ‘however’, ‘but’), using word pairs extracted
from two spans of text. Their method attempts to
capture the difference of polarity in words. For ex-
ample, the word pair (sell, hold) indicates a CON-
TRAST relation.
Discourse relation classifiers have also been
trained using PDTB. Pitler et al. (2008) performed a
corpus study of the PDTB, and found that ‘explicit’
relations can be most of the times distinguished by
their discourse connectives. Their discourse relation
classifier reported an accuracy of 0.93 for explicit
relations and in overall an accuracy of 0.744 for all
relations in PDTB.
Lin et al. (2009) studied the problem of detecting
implicit relations in PDTB. Their relational classi-
fier is trained using features extracted from depen-
dency paths, contextual information, word pairs and
production rules in parse trees. They reported for
their classifier an accuracy of 0.402, which is an im-
provement of 14.1% over the previous state-of-the-
art for implicit relation classification in PDTB. For
the same task, Pitler et al. (2009) also used word
pairs, as well as several other types of features such
as verb classes, modality, context, and lexical fea-
tures.
In text classification, similarity measures have
been employed in kernel methods, where they have
been shown to improve accuracy over ‘bag-of-
words’ approaches. In Siolas and d’Alch´e-Buc
(2000), a semantic proximity measure based on
WordNet (Fellbaum, 1998) is defined, as a basis to
create a proximity matrix for all terms of the prob-
lem. This matrix is then used to smooth the vectorial
data, and the resulting ‘semantic’ metric is incorpo-
rated into a SVM kernel, resulting in a significant
increase of accuracy and F-score over a baseline.
Cristianini et al. (2002) have used a lexical sim-
ilarity measure derived from Latent Semantic In-
dexing (Deerwester et al., 1990), where the seman-
tic similarity between two terms is inferred from
the analysis of their co-occurrence patterns: Terms
that co-occur often in the same documents are con-
sidered as related. In this work, the statistical co-
occurrence information is extracted by the means of
singular value decomposition. The authors observe
</bodyText>
<page confidence="0.993762">
400
</page>
<bodyText confidence="0.999731833333333">
substantial improvements in performance for some
datasets, while little effect is obtained for others.
Semantic kernels have also been shown to be effi-
cient for text classification tasks, in the case in of un-
balanced and sparse datasets. In Basili et al. (2006),
a ‘conceptual density’ metric based on WordNet is
introduced, and employed in a SVM kernel. Using
this metric results in improved accuracy of 10% for
text classification in poor training conditions. How-
ever, the authors observe that when the number of
training documents is increased, the improvement
produced by the semantic kernel is lower.
Bloehdorn et al. (2006) compare the performance
of different semantic kernels, based on several mea-
sures of semantic relatedness in WordNet. For each
measure, the authors note a performance increase
when little training data is available, or when the
feature representations are very sparse. However,
for our task, classification of discourse relations, we
employ not only words but also other types of fea-
tures such as parse tree production rules, and thus
cannot compute semantic kernels using WordNet.
In this paper, we are not aiming at defining
novel features for improving performance in RST or
PDTB relation classification. Instead we incorporate
numerous features that have been shown to be useful
for discourse relation learning and explore the pos-
sibilities of using unlabeled data for this task. One
of our goals is to improve classification accuracy for
rare discourse relations.
</bodyText>
<sectionHeader confidence="0.980075" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999931838709678">
Given a set of unlabeled instances U and labeled in-
stances L, our objective is to learn an n-class rela-
tion classifier H such that for a given test instance
x return its correct relation type H(x). In the case
of discourse relation learning we are interested in
the situation where |U |&gt;&gt; |L|. Here, we use the
notation |A |to denote the number of elements in a
set A. A fundamental problem that one encounters
when trying to learn a classifier for a large number
of relations with small training dataset is that most
of the features that appear in the test instances ei-
ther never occur in training instances or appear a
small number of times. Therefore, the classifica-
tion algorithm does not have sufficient information
to correctly predict the relation type of the given test
instance. We propose a method that first computes
the co-occurrence between features using unlabeled
data and use that information to extend the feature
vectors during training and testing, thereby reducing
the sparseness in test feature vectors. In Section 3.1,
we introduce the concept of feature co-occurrence
matrix and describe how it is computed using unla-
beled data. A method to extend feature vectors dur-
ing training and testing is presented in Section 3.2.
We defer the details on exact features used in the
method to Section 3.3. It is noteworthy that the
proposed method does not depend or assume a par-
ticular multi-class classification algorithm. Conse-
quently, it can be used with any multi-class classifi-
cation algorithm to learn a discourse relation classi-
fier.
</bodyText>
<subsectionHeader confidence="0.999515">
3.1 Feature Co-occurrence Matrix
</subsectionHeader>
<bodyText confidence="0.999977368421053">
We represent an instance using a d dimensional fea-
ture vector f = [f1, ... , fd]T, where fi E R. We
define a feature co-occurrence matrix, C such that
the (i, j)-th element of C, C(i,j) E [0, 1] denotes
the degree of co-occurrence between the two fea-
tures fi and fj. If both fi and fj appear in a fea-
ture vector then we define them to be co-occurring.
The number of different feature vectors in which fi
and fj co-occur is denoted by the function h(fi, fj).
From our definition of co-occurrence it follows that
h(fi, fj) = h(fj, fi). Importantly, feature co-
occurrences can be calculated only using unlabeled
data.
Feature co-occurrence matrices can be computed
using any co-occurrence measure. For the current
task we use the χ2-measure (Plackett, 1983) as the
preferred co-occurrence measure because of its sim-
plicity. χ2-measure between two features fi and fj
is defined as follows,
</bodyText>
<equation confidence="0.9928632">
� i,j 2 (Ok,l − Ei,j
k,l)2
χi,j = . (1)
Ei,j
k=1l=1 k,l
</equation>
<bodyText confidence="0.87997175">
Therein, Oi,j and Ei,j are the 2x2 matrices contain-
ing respectively observed frequencies and expected
frequencies, which are respectively computed using
C as,
</bodyText>
<equation confidence="0.987763">
Oi,j = h(fi,fj) Zi − h(fi, fj) (2)
(Zj − h(fi, fj) Zs − Zi − Zj
2 2
</equation>
<page confidence="0.862188">
401
</page>
<bodyText confidence="0.830274">
and
</bodyText>
<equation confidence="0.94320325">
Zi·(Zs−Zj)
Zs
(Zs−Zi)·(Zs−Zj)
Zs
</equation>
<bodyText confidence="0.984139333333333">
Here, Zi = --&apos;k6=i h(fi, fk), and Zs = --&apos;n i=1 Zi.
Finally, we create the feature co-occurrence ma-
trix C, such that, for all pairs of features (fi, fj),
</bodyText>
<equation confidence="0.8540795">
i2i j if χ2i,j &gt; c
C(i,j) = { 0 otherwise . (4)
</equation>
<bodyText confidence="0.99675080952381">
Here X2i j = χ2maxxχ2minmin E [0, 1], and c is the critical
value, which, for a confidence level of 0.05 and one
degree of freedom, can be set to 3.84. Keeping C(i,j)
in the range [0, 1] makes it convenient to filter out
low-relevance co-occurrences at the feature vector
extension step of Section 3.2.
In discourse relation learning, the feature space
can be extremely large. For example, with word
pair features (discussed later in Section 3.3), any
two words that appear in two adjoining discourse
units can form a feature. Because the number of
elements in the feature co-occurrence matrix is pro-
portional to the square of the feature space’s dimen-
sion, computing co-occurrences for all pairs of fea-
tures can be computationally costly. Moreover, stor-
ing a large matrix in memory for further computa-
tions can be problematic. To reduce the dimension-
ality and improve the sparseness in the feature co-
occurrence matrix, we use entropy-based feature se-
lection (Manning and Sch¨utze, 1999). The negative
entropy, E(fi), of a feature fi is defined as follows,
</bodyText>
<equation confidence="0.992126">
E(fi) = − � p(i, j) · log (p (i, j)) . (5)
j6=i
</equation>
<bodyText confidence="0.998276176470588">
Here, p(i, j) is the probability that feature fi co-
occurs with feature fj, and is given by p(i, j) =
h(fi, fj)/Zi.
If a particular feature fi co-occurs with many
other features, then its negative entropy E(fi) de-
creases. Because we are interested in identifying
salient co-occurrences between features, we can ig-
nore the features that tend to co-occur with many
other features. Consequently, we sort the features in
the descending order of their entropy, and select the
top ranked N number of features to build the feature
co-occurrence matrix. This feature selection proce-
dure can efficiently reduce the dimensions of the fea-
ture co-occurrence matrix to N x N. Because the
feature co-occurrence matrix is symmetric, we must
only store the elements for the upper (or lower) tri-
angular portion of it.
</bodyText>
<subsectionHeader confidence="0.999557">
3.2 Feature Vector Extension
</subsectionHeader>
<bodyText confidence="0.9828335">
Once the feature co-occurrence matrix is computed
using unlabeled data as described in Section 3.1, we
can use it to extend a feature vector during train-
ing and testing. The proposed feature vector exten-
sion method is inspired by query expansion in the
field of Information Retrieval (Salton and Buckley,
1983; Fang, 2008). One of the reasons that a clas-
sifier might perform poorly on a test instance is that
there are features in the test instance that were not
observed during training. We call FU = {fi} the
set of features that were not observed by the clas-
sifier during training (i.e. occurring in test data but
not in training data). For each of those features, we
use the feature co-occurrence matrix to find the set
of co-occurring features, Fc(fi).
Let us denote the feature vector corresponding to
a training or test instance x by fx. We use the su-
perscript notation, fix to denote the i-th feature in fx.
Moreover, the total number of features of fx is indi-
cated by d(x). For a feature fix in fx, we define n(i)
number of expansion features, f(i,1)
x ,...,f(i,n(i))
x as
follows. First, we require that each expansion fea-
ture(
fxi,j) belongs to Fc(fi). Second, the value of
f(i,j) xis set to fix · C(i,j). The expansion features
for each feature fix are then appended to the orig-
inal feature vector fx to create an extended feature
vector, f0x, where,
</bodyText>
<equation confidence="0.997564428571429">
= (f1 x, . . . ,fd(x)
x , (6)
f(i,1)
x , . . . , f(i,n(i)) , . . . ,
x
f(d(x),1) , . . . , f(d(x),n(d(x)) )
x x
</equation>
<bodyText confidence="0.99981225">
In total, doing so augments the original vector’s size
by --&apos;fi∈U |Fc(fi)|. All training and test instances
are extended in this fashion.
Note that because this process can potentially in-
crease the dimension too much, it is possible to re-
tain only candidate co-occurring features of Fc(fi)
possessing a co-occurrence value C(i,j) above a cer-
tain threshold. In the experiments of Section 4 how-
</bodyText>
<figure confidence="0.944508285714286">
Zi·Zj
Ei,j = Zs
Zj·(Zs−Zi)
Zs
I. (3)
f0 x
.
</figure>
<page confidence="0.99337">
402
</page>
<bodyText confidence="0.9997735">
ever, we experienced dimension increase of 10000 at
most, which did not require us to use thresholding.
</bodyText>
<subsectionHeader confidence="0.967665">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.9999645">
We use three types of features: Word pairs, produc-
tion rules from the parse tree, as well as features en-
coding the lexico-syntactic context at the border be-
tween two units of text (Soricut and Marcu, 2003).
Our word pairs are lemmatized using the Wordnet-
based lemmatizer of NLTK (Loper and Bird, 2002).
Figure 1 shows the parse tree for a sentence com-
posed of two discourse units, which serve as argu-
ments of a discourse relation we want to generate a
feature vector from. Lexical heads have been calcu-
lated using the projection rules of Magerman (1995),
and annotated between brackets. Surrounded by
dots is, for each argument, the minimal set of sub-
parse trees containing strictly all the words of the
argument.
We first extract all possible lemmatized word-
pairs from the two arguments, such as (Mr., when),
(decline, ask) or (comment, sale). Next, we extract
from left and right argument separately, all produc-
tion rules from the sub-parse trees, such as NP H
NNP NNP, NNP H “Sherry” or TO H “to”.
Finally, we encode in our features three nodes of
the parse tree, which capture the local context at the
connection point between the two arguments: The
first node, which we call N,,,, is the highest ances-
tor of the first argument’s last word w, and is such
that N,,,’s right-sibling is the ancestor of the second
argument’s first word. N,,,’s right-sibling node is
called N,.. Finally, we call Np the parent of N,,, and
N,.. For each node, we encode in the feature vec-
tor its part-of-speech (POS) and lexical head. For
instance, in Figure 1, we have N,,, = S(comment),
N,. = SBAR(when), and Np = VP(declined). In the
PDTB, certain discourse relations have disjoint ar-
guments. In this case, as well as in the case where
the two arguments belong to different sentences, the
nodes N,,,, N,., Np cannot be defined, and their cor-
responding features are given the value zero.
</bodyText>
<sectionHeader confidence="0.999756" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999915081632653">
The proposed method is independent of any partic-
ular classification algorithm. Because our goal is
strictly to evaluate the relative benefit of employing
the proposed method, and not the absolute perfor-
mance when used with a specific classification algo-
rithm, we select a logistic regression classifier, for its
simplicity. We use the multi-class logistic regression
(maximum entropy model) implemented in the Clas-
sias toolkit (Okazaki, 2009). Regularization param-
eters are set to their default value of one and are fixed
throughout the experiments described in the paper.
To create our unlabeled dataset, we use sentences
extracted from the English Wikipedia2, as they are
freely available and relatively easy to collect. For
further extraction of syntactic features, these sen-
tences are automatically parsed using the Stanford
parser (Klein and Manning, 2003). Then, they are
segmented into elementary discourse units (EDUs)
using our sequential discourse segmenter (Hernault
et al., 2010). The relatively high performance of
this RST segmenter, which has an F-score of 0.95
compared to that of 0.98 between human annota-
tors (Soricut and Marcu, 2003), is acceptable for this
task. We collect and parse 100000 sentences from
random Wikipedia articles. As there is no segmen-
tation tool for the PDTB framework, we assume that
co-occurrence information taken from EDUs created
using a RST segmenter is also useful for extending
feature vectors of PDTB relations. Unless other-
wise noted, the experiments presented in the rest of
this paper are done using those 100000 unlabeled in-
stances.
In the unlabeled data, any two consecutive dis-
course units might not always be connected by a dis-
course relation. Therefore, we introduce an artificial
NONE relation in the training set, in order to facil-
itate this. Instances of the NONE relation are gen-
erated randomly by pairing consecutive discourse
units which are not connected by a discourse relation
in the training data. NONE is also learnt as a separate
discourse relation class by the multi-class classifica-
tion algorithm. This enables us to detect discourse
units between which there exist no discourse rela-
tion, thereby improving the classification accuracy
for other relation types.
We follow the common practice in discourse re-
search for partitioning the discourse corpora into
training and test set. For the RST classifier, the
dedicated training and test sets of the RSTDT are
</bodyText>
<footnote confidence="0.993012">
2http://en.wikipedia.org
</footnote>
<page confidence="0.998118">
403
</page>
<figure confidence="0.984763">
S (declined)
Argument 1 Argument 2
</figure>
<figureCaption confidence="0.990426">
Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them—lexical heads
are indicated between brackets.
</figureCaption>
<figure confidence="0.972698368421053">
NNP NNP VBD (declined)
Mr. Sherry declined to
NP (Sherry)
TO VP
VP (comment)
S (comment)
VP (declined)
comment when asked about the sales
VB
WHADVP (when)
WRB
SBAR (when)
S (asked)
VP (asked)
VBN PP (about)
IN NP (sales)
DT NNS
. (.)
.
</figure>
<bodyText confidence="0.999632359375">
employed. For the PDTB classifier, we conform to
the guidelines of Prasad et al. (2008b, 5): The por-
tion of the corpus corresponding to sections 2–21
of the WSJ is used for training the classifier, while
the portion corresponding to WSJ section 23 is used
for testing. In order to extract syntactic features, all
training and test data are furthermore aligned with
their corresponding parse trees in the Penn Treebank
(Marcus et al., 1993).
Because in the PDTB an instance can be
annotated with several discourse relations
simultaneously—called ‘senses’ in Prasad et
al. (2008b)—for each instance with n senses in
the corpus, we create n identical feature vectors,
each being labeled by one of the instance’s senses.
However, in the RST framework, only one relation
is allowed to hold between two EDUs. Conse-
quently, each instance from the RSTDT is labeled
with a single discourse relation, from which a
single feature vector is created. For RSTDT, we
extract 25078 training vectors and 1633 test vectors.
For PDTB we extract 49748 training vectors and
1688 test vectors. There are 41 classes (relation
types) in the RSTDT relation classification task,
and 29 classes in the PDTB task. For the PDTB,
we selected level-two relations, because they have
better expressivity and are not too fine-grained.
We experimentally set the entropy-based feature
selection parameter to N = 5000. With large N
values, we must store and process large feature
co-occurrence matrices. For example, doubling
the number of selected features, N to 10000 did
not improve the classification accuracy, although
it required 4GB of memory to store the feature
co-occurrence matrix.
Figure 2 shows the number of features that occur
in test data but not in labeled training data, against
the number of training instances. It can be seen from
Figure 2 that, with less training data available to the
classifier, we can potentially obtain more informa-
tion regarding features by looking at unlabeled data.
However, when the training dataset’s size increases,
the number of features that only appear in test data
decreases rapidly. This inverse relation between the
training dataset size and the number of features that
only appear in test data can be observed in both
RSTDT and PDTB datasets. For a training set of
100 instances, there are 23580 unseen features in
the case of RSTDT, and 27757 in the case of PDTB.
The number of unseen features is halved for a train-
ing set of 1800 instances in the case of RSTDT, and
for a training set of 1300 instances in the case of
PDTB. Finally, when selecting all available training
data, we count only 1365 unseen test features in the
case of RSTDT, and 87 in the case of PDTB.
In the following experiments, we use macro-
averaged F-scores to evaluate the performance of the
proposed discourse relation classifier on test data.
Macro-averaged F-score is not influenced by the
number of instances that exist in each relation type.
It equally weights the performance on both frequent
relation types and infrequent relation types. Because
we are interested in measuring the overall perfor-
mance of a discourse relation classifier across all re-
</bodyText>
<page confidence="0.999068">
404
</page>
<figureCaption confidence="0.998369">
Figure 2: Number of features seen only in the test set, as
a function of the number of training instances used.
</figureCaption>
<bodyText confidence="0.994800555555555">
lation types we use macro-averaged F-score as the
preferred evaluation metric for this task.
We train a multi-class logistic regression model
without extending the feature vectors as a baseline
method. This baseline is expected to show the ef-
fect of using the proposed feature vector extension
approach for the task of discourse relation learn-
ing. Experimental results on RSTDT and PDTB
datasets are depicted in Figures 3 and 4. From
these figures, we see that the proposed feature ex-
tension method outperforms the baseline for both
RSTDT and PDTB datasets for the full range of
training dataset sizes. However, whereas the differ-
ence of scores between the two methods is obvious
for small amounts of training data, this difference
progressively decreases as we increase the amount
of training data. Specifically, with 100 training in-
stances, the difference between baseline and pro-
posed method is the largest: For RSTDT, the base-
line has a macro-averaged F-score of 0.084, whereas
the the proposed method has a macro-averaged F-
score of 0.189 (ca. 119% increase in F-score). For
PDTB, the baseline has an F-score of 0.016, while
the proposed method has an F-score of 0.089 (459%
increase). The difference of scores between the two
methods then progressively diminishes as the num-
ber of training instances is increased, and fades be-
yond 10000 training instances. The reason for this
behavior is given by Figure 2: For a small number
of training instances, the number of unseen features
in training data is large. In this case, the feature vec-
tor extension process is comprehensive, and score
can be increased by the use of unlabeled data. When
more training data is progressively used, the num-
ber of unseen test features sharply diminishes, which
means feature vector extension becomes more lim-
ited, and the performance of the proposed method
gets progressively closer to the baseline. Note that
we plotted PDTB performance up to 25000 train-
ing instances, as the number of unseen test features
becomes so small past this point that the perfor-
mances of the proposed method and baseline are
identical. Using all PDTB training data (49748 in-
stances), both baseline and proposed method reach a
macro-average F-score of 0.308.
</bodyText>
<figure confidence="0.997205380952381">
30000
RSTDT
PDTB
25000
20000
15000
10000
5000
00 5000 10000 15000 20000 25000
Number of training instances
Number of unseen test features
Macro-average F-score
0.30
0.20
0.10
0.25
0.15
0.050 5000 10000 15000 20000 25000
Number of training instances
Baseline RSTDT
Proposed method
</figure>
<figureCaption confidence="0.9994585">
Figure 3: Macro-average F-score (RSTDT) as a function
of the number of training instances used.
</figureCaption>
<figure confidence="0.999589909090909">
Macro-average F-score
0.30
0.20
0.10
0.000 5000 10000 15000 20000 25000
Number of training instances
0.25
0.15
0.05
Baseline PDTB
Proposed method
</figure>
<figureCaption confidence="0.997246">
Figure 4: Macro-average F-score (PDTB) as a function
of the number of training instances used.
</figureCaption>
<page confidence="0.982152">
405
</page>
<table confidence="0.999737210526316">
Relation name #-D = 1 #-D = 2 #-D = 3 #-D = 5 #-D = 7
B. P.M. B. P.M. B. P.M. B. P.M. B. P.M.
Attribution[N][S] – 0.127 – 0.237 – 0.458 0.038 0.290 0.724 0.773
Attribution[S][N] – 0.597 – 0.449 0.009 0.639 0.250 0.721 0.579 0.623
Background[N][S] – 0.113 – – 0.036 – 0.095 – 0.089
Cause[N][S] – – 0.128 – – – 0.034 0.057 0.187
Comparison[N][S] – 0.118 – 0.037 – 0.133 0.130 0.143 0.031
Condition[N][S] – 0.041 – 0.136 – 0.113 – 0.154 0.242 0.152
Condition[S][N] – – 0.122 0.133 0.148 0.214 0.233 0.390 0.308
Contrast[N][N] – – 0.086 – 0.073 0.050 0.111 – 0.109
Contrast[N][S] – 0.071 – – 0.188 – 0.087 – 0.136
Elaboration[N][S] – 0.134 – 0.126 0.004 0.067 0.004 0.340 – 0.165
Enablement[N][S] – – – 0.462 – 0.579 0.115 0.423 0.419 0.438
Joint[N][N] – 0.030 – 0.015 – 0.016 0.059 0.015 0.155
Manner-Means[N][S] – – – 0.056 – 0.103 0.345 0.372 0.412 0.383
Summary[N][S] – 0.429 – 0.453 0.080 0.358 – 0.349 0.154 0.471
Temporal[N][S] – 0.158 – – – 0.091 – 0.052 0.204 0.101
Accuracy 0.000 0.110 0.000 0.105 0.004 0.146 0.034 0.222 0.122 0.213
Macro-average F-score 0.000 0.060 0.000 0.069 0.008 0.101 0.038 0.118 0.107 0.134
</table>
<tableCaption confidence="0.996857">
Table 1: F-scores for RSTDT relations, using a training set containing #Tr instances of each relation. B. indicates
F-score for baseline, P.M. for the proposed method. A boldface indicates the best classifier for each relation.
</tableCaption>
<bodyText confidence="0.999987145833333">
Although the distribution of discourse relations
in RSTDT and PDTB is not uniform, it is possi-
ble to study the performance of the proposed method
when all relations are made equally rare. We evalu-
ate performance on artificially-created training sets
containing an equal amount of each discourse rela-
tion. Table 1 contains the F-score for each RSTDT
relation, using training sets containing respectively
one, two, three, five and seven instances of each
relation. For space considerations, only relations
with significant results are shown. We observe that,
when using respectively one and two instances of
each relation, the baseline classifier is unable to de-
tect any relation, and has a macro-average F-score
of zero. Contrastingly, the classifier built with fea-
ture vector extension reaches in those cases an F-
score of 0.06. Furthermore, when employing the
proposed method, certain relations have relatively
high F-scores even with very little labeled data: With
one training instance, ATTRIBUTION[S][N] has an
F-score of 0.597, while SUMMARY[N][S] has an F-
score of 0.429. With three training instances, EN-
ABLEMENT[N][S] has an F-score of 0.579. When
the amount of each relation is increased, the baseline
classifier starts detecting more relations. In all cases,
the proposed method performs better in terms of ac-
curacy and macro-average F-score. With a train-
ing set containing seven instances of each relation,
the baseline’s macro-average F-score is starting to
get closer to the extended classifier’s, with superior
performances for several relations, such as COM-
PARISON[N][S], CONDITION[N][S], and TEMPO-
RAL[N][S]. Still, in this case, the extended classi-
fier’s accuracy is higher than the baseline (0.213 ver-
sus 0.122). Table 2 summarizes the outcome of the
same experiments performed on the PDTB dataset.
The results exhibit a similar trend, despite the base-
line classifier having a relatively high accuracy for
each case.
Using the data from Figures 2, 3 and 4, it is pos-
sible to calculate the relative score change occur-
ring when using the proposed method, as a func-
tion of the number of unseen features found in test
data. This graph is plotted in Figure 5. Besides
macro-average F-score, we additionally plot accu-
racy change. In the top subfigure, representing the
case of RSTDT, we see that, for the lowest amount
of unseen test features, the proposed method does
</bodyText>
<page confidence="0.997108">
406
</page>
<table confidence="0.999080882352941">
Relation name #-D = 1 #-D = 2 #-D = 3 #-D = 5 #-D = 7
B. P.M. B. P.M. B. P.M. B. P.M. B. P.M.
Comparison.Concession[2][1] – 0.056 – – – 0.133 – – – 0.154
Comparison.Contrast[2][1] – – – 0.333 – – – 0.190 0.105 0.368
Contingency.Cause[1][2] – 0.013 – 0.007 – – – 0.026 – 0.013
Contingency.Condition[1][2] – 0.082 – 0.160 – 0.127 0.250 0.253 0.214 0.171
Contingency.Condition[2][1] – – – – – 0.074 – 0.143 0.250 0.296
Contingency.Prag. cond.[1][2] – – – 0.133 – 0.034 – – 0.133 0.043
Contingency.Prag. cond.[2][1] – – – – – – 0.133 0.087 0.154 0.087
Expansion.Conjunction[1][2] 0.326 0.352 0.326 0.351 0.326 0.368 0.332 0.371 0.335 0.384
Expansion.Instantiation[1][2] – – – – – 0.042 – 0.057 – 0.131
Temporal.Asynchronous[1][2] – 0.204 – – – 0.142 0.039 0.148 – 0.035
Temporal.Asynchronous[2][1] – – – – – 0.316 – 0.483 0.143 –
Temporal.Synchrony[1][2] – – – 0.032 – 0.162 0.032 0.103 0.032 0.157
Temporal.Synchrony[2][1] – – – 0.083 – 0.143 0.200 0.308 0.211 0.174
Accuracy 0.195 0.201 0.195 0.202 0.195 0.212 0.202 0.214 0.204 0.213
Macro-average F-score 0.015 0.033 0.015 0.054 0.015 0.084 0.045 0.108 0.072 0.100
</table>
<tableCaption confidence="0.913473">
Table 2: F-scores for PDTB relations.
</tableCaption>
<bodyText confidence="0.99993825">
not bring any change in F-score or accuracy. In-
deed, as the number of unknown features is low,
feature vector extension is very limited, and does
not improve the performance compared to the base-
line. Then, a progressive increase of both accuracy
and macro-average F-score is observed, as the num-
ber of unseen test features is incremented. For in-
stance, for 8500 unseen test features, the macro-
average F-score increase (resp. accuracy increase)
is 25% (resp. 2.5%), while it is 20% (resp. 1%) for
11000 unseen test instances. These values reach a
maximum of 119% macro-average F-score increase,
and 66% accuracy increase, when 23500 features
unseen during training are present in test data. This
situation corresponds in Figures 3 and 4 to the case
of very small training sets. The bottom subfigure
of Figure 2, for the case of PDTB, reveals a sim-
ilar tendency. The macro-average F-score increase
(resp. accuracy increase) is negligible for 1000 un-
seen test features, while this increase is 21% for both
macro-average F-score and accuracy in the case of
9700 unseen test features, and 459% (resp. 630% for
accuracy) when 28000 unseen features are found in
test data. This shows that the proposed method is
useful when large numbers of features are missing
from the training set, which corresponds in practice
to small training sets, with few training instances for
each relation type. For large training sets, most fea-
tures are encountered by the classifier during train-
ing, and feature vector extension does not bring use-
ful information.
We empirically evaluate the effect of using differ-
ent amounts of unlabeled data on the performance of
the proposed method. We use respectively 100 and
10000 labeled training instances, create feature co-
occurrence matrices with different amounts of unla-
beled data, and evaluate the performance in relation
classification. Experimental results for RSTDT are
illustrated in Figure 6 (top). From Figure 6 it appears
clearly that macro-average F-scores improve with
increased number of unlabeled instances. However,
the benefit of using larger amounts of unlabeled data
is more pronounced when only a small number of la-
beled training instances are employed (ca. 100). In
fact, with 100 labeled training instances, the maxi-
mum improvement in F-score is 119% (corresponds
to using all our 100000 unlabeled instances). How-
ever, the maximum improvement in F-score with
10000 labeled training instances is small, only 2.5%
(corresponds to 10000 unlabeled instances).
The effect of using unlabeled data on PDTB rela-
tion classification is illustrated in Figure 6 (bottom).
Similarly, we consecutively set the labeled training
dataset size to 100 and 10000 instances, and plot the
macro-average F-score against the unlabeled dataset
size. As in the RSTDT experiment, the benefit of us-
</bodyText>
<page confidence="0.98543">
407
</page>
<figure confidence="0.999625480769231">
Score change over baseline (%)
120
100
40
80
60
20
0
Accuracy change RSTDT
Macro-av. F-score change RSTDT
0 5000 10000 15000 20000 25000
Number of unseen features in test data
Macro-average F-score
0.20
0.10
0.00
101 102 103 104 105
Number of unlabeled instances
0.25
0.15
0.05
RSTDT (100)
Baseline RSTDT (100)
RSTDT (10000)
Baseline RSTDT (10000)
Macro-average F-score
0.20
0.10
0.00
0.25
0.15
0.05
0.10
101 102 103 104 105
Number of unlabeled instances
0.05
PDTB (100)
Baseline PDTB (100)
PDTB (10000)
Baseline PDTB (10000)
Score change over baseline (%)
400
600
500
300
200
700
100
00 5000 10000 15000 20000 25000 30000
Number of unseen features in test data
Accuracy change PDTB
Macro-av. F-score change PDTB
</figure>
<figureCaption confidence="0.9974835">
Figure 5: Score change as a function of unseen test fea-
tures for RSTDT (top) and PDTB (bottom).
</figureCaption>
<bodyText confidence="0.999915333333333">
ing unlabeled data is more obvious when the num-
ber of labeled training instances is small. In par-
ticular, with 100 training instances, the maximum
improvement in F-score is 459% (corresponds to
100000 unlabeled instances). However, with 10000
labeled training instances the maximum improve-
ment in F-score is 15% (corresponds to 100 unla-
beled instances). These results confirm that, on the
one hand performance improvement is more promi-
nent for smaller training sets, and that on the other
hand, performance is increased when using larger
amounts of unlabeled data.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9998555">
We presented a semi-supervised method which ex-
ploits the co-occurrence of features in unlabeled
data, to extend feature vectors during training and
testing in a discourse relation classifier. Despite the
</bodyText>
<figureCaption confidence="0.996232333333333">
Figure 6: Macro-average F-score for RSTDT (top) and
PDTB (bottom), for 100 and 10000 training instances,
against the number of unlabeled instances.
</figureCaption>
<bodyText confidence="0.9999938125">
simplicity of the proposed method, it significantly
improved the macro-average F-score in discourse re-
lation classification for small training datasets, con-
taining low-occurrence relations. We performed an
evaluation on two popular datasets, the RSTDT and
PDTB. We empirically evaluated the benefit of using
a variable amount of unlabeled data for the proposed
method. Although the macro-average F-scores of
the classifiers described are too low to be used di-
rectly as discourse analyzers, the gain in F-score and
accuracy for small labeled datasets are a promising
perspective for improving classification accuracy for
infrequent relation types. In particular, the proposed
method can be employed in existing discourse clas-
sifiers that work well on popular relations, and be
expected to improve the overall accuracy.
</bodyText>
<page confidence="0.998321">
408
</page>
<sectionHeader confidence="0.993894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985942">
R. Basili, M. Cammisa, and A. Moschitti. 2006. A se-
mantic kernel to classify texts with very few training
examples. Informatica (Slovenia), 30(2):163–172.
S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.
2006. Semantic kernels for text classification based on
topological measures of feature similarity. In Proc. of
ICDM’06, pages 808–812.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of Rhetorical Structure Theory. Proc. of Second SIG-
dial Workshop on Discourse and Dialogue-Volume 16,
pages 1–10.
N. Cristianini, J. Shawe-Taylor, and H. Lodhi. 2002. La-
tent semantic kernels. Journal of Intelligent Informa-
tion Systems, 18:127–152.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391–407.
D. A. duVerle and H. Prendinger. 2009. A novel dis-
course parser based on Support Vector Machine clas-
sification. In Proc. of ACL’09, pages 665–673.
H. Fang. 2008. A re-examination of query expansion us-
ing lexical resources. In Proc. of ACL’08, pages 139–
147.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A
sequential model for discourse segmentation. In Proc.
of CICLing’10, pages 315–326.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in Neural Information Processing Systems,
volume 15. MIT Press.
Z. Lin, M-Y. Kan, and H. T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In Proc. of EMNLP’09, pages 343–351.
E. Loper and S. Bird. 2002. NLTK: The natural lan-
guage toolkit. In Proc. of ACL’02 Workshop on Effec-
tive tools and methodologies for teaching natural lan-
guage processing and computational linguistics, pages
63–70.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. Proc. of ACL’95, pages 276–283.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
Structure Theory: Toward a functional theory of text
organization. Text, 8(3):243–281.
C. D. Manning and H. Sch¨utze. 1999. Foundations of
Statistical Natural Language processing. MIT Press.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc. of
ACL’02, pages 368–375.
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
N. Okazaki. 2009. Classias: A collection of machine-
learning algorithms for classification. http://
www.chokkan.org/software/classias/.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,
and A. Joshi. 2008. Easily identifiable discourse rela-
tions. In Proc. of COLING’08 (Posters), pages 87–90.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in text.
In Proc. of ACL’09, pages 683–691.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. Generating dialogues between virtual agents au-
tomatically from text. In Proc. of IVA’07, pages 161–
174.
R. L. Plackett. 1983. Karl Pearson and the chi-squared
test. International Statistical Review /Revue Interna-
tionale de Statistique, 51(1):59–72.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008a. The Penn Discourse
TreeBank 2.0. In Proc. of LREC’08.
R. Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,
L. Robaldo, and B. Webber. 2008b. The Penn Dis-
course Treebank 2.0 annotation manual. Technical re-
port, University of Pennsylvania Institute for Research
in Cognitive Science.
G. Salton and C. Buckley. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill Book Company.
G. Siolas and F. d’Alch´e-Buc. 2000. Support Vector Ma-
chines based on a semantic kernel for text categoriza-
tion. In Proc. of IJCNN’00, volume 5, page 5205.
S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor.
2009. Supervised and unsupervised methods in em-
ploying discourse relations for improving opinion po-
larity classification. In Proc. of EMNLP’09, pages
170–179.
R. Soricut and D. Marcu. 2003. Sentence level discourse
parsing using syntactic and lexical information. Proc.
of NA-ACL’03, 1:149–156.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag New York, Inc.
</reference>
<page confidence="0.9991">
409
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.706506">
<title confidence="0.9988435">A Semi-Supervised Approach to Improve Classification Infrequent Discourse Relations using Feature Vector Extension</title>
<author confidence="0.959603">Hugo Hernault Danushka Bollegala Mitsuru Ishizuka</author>
<abstract confidence="0.9308625">hugo@mi.ci.i. danushka@iba.t. ishizuka@i. u-tokyo.ac.jp u-tokyo.ac.jp</abstract>
<affiliation confidence="0.9829825">Graduate School of Information Science &amp; The University of</affiliation>
<address confidence="0.901209">7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan</address>
<abstract confidence="0.997510535714286">Several recent discourse parsers have employed fully-supervised machine learning approaches. These methods require human annotators to beforehand create an extensive training corpus, which is a time-consuming and costly process. On the other hand, unlabeled data is abundant and cheap to collect. In this paper, we propose a novel semi-supervised method for discourse relation classification based on the analysis of cooccurring features in unlabeled data, which is then taken into account for extending the feature vectors given to a classifier. Our experimental results on the RST Discourse Treebank corpus and Penn Discourse Treebank indicate that the proposed method brings a significant improvement in classification accuracy and macro-average F-score when small training datasets are used. For instance, with training sets of c.a. 1000 labeled instances, the proposed method brings improvements in accuracy and macro-average F-score up to 50% compared to a baseline classifier. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M Cammisa</author>
<author>A Moschitti</author>
</authors>
<title>A semantic kernel to classify texts with very few training examples.</title>
<date>2006</date>
<journal>Informatica (Slovenia),</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="8492" citStr="Basili et al. (2006)" startWordPosition="1301" endWordPosition="1304">ndexing (Deerwester et al., 1990), where the semantic similarity between two terms is inferred from the analysis of their co-occurrence patterns: Terms that co-occur often in the same documents are considered as related. In this work, the statistical cooccurrence information is extracted by the means of singular value decomposition. The authors observe 400 substantial improvements in performance for some datasets, while little effect is obtained for others. Semantic kernels have also been shown to be efficient for text classification tasks, in the case in of unbalanced and sparse datasets. In Basili et al. (2006), a ‘conceptual density’ metric based on WordNet is introduced, and employed in a SVM kernel. Using this metric results in improved accuracy of 10% for text classification in poor training conditions. However, the authors observe that when the number of training documents is increased, the improvement produced by the semantic kernel is lower. Bloehdorn et al. (2006) compare the performance of different semantic kernels, based on several measures of semantic relatedness in WordNet. For each measure, the authors note a performance increase when little training data is available, or when the feat</context>
</contexts>
<marker>Basili, Cammisa, Moschitti, 2006</marker>
<rawString>R. Basili, M. Cammisa, and A. Moschitti. 2006. A semantic kernel to classify texts with very few training examples. Informatica (Slovenia), 30(2):163–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bloehdorn</author>
<author>R Basili</author>
<author>M Cammisa</author>
<author>A Moschitti</author>
</authors>
<title>Semantic kernels for text classification based on topological measures of feature similarity.</title>
<date>2006</date>
<booktitle>In Proc. of ICDM’06,</booktitle>
<pages>808--812</pages>
<contexts>
<context position="8860" citStr="Bloehdorn et al. (2006)" startWordPosition="1359" endWordPosition="1362">ntial improvements in performance for some datasets, while little effect is obtained for others. Semantic kernels have also been shown to be efficient for text classification tasks, in the case in of unbalanced and sparse datasets. In Basili et al. (2006), a ‘conceptual density’ metric based on WordNet is introduced, and employed in a SVM kernel. Using this metric results in improved accuracy of 10% for text classification in poor training conditions. However, the authors observe that when the number of training documents is increased, the improvement produced by the semantic kernel is lower. Bloehdorn et al. (2006) compare the performance of different semantic kernels, based on several measures of semantic relatedness in WordNet. For each measure, the authors note a performance increase when little training data is available, or when the feature representations are very sparse. However, for our task, classification of discourse relations, we employ not only words but also other types of features such as parse tree production rules, and thus cannot compute semantic kernels using WordNet. In this paper, we are not aiming at defining novel features for improving performance in RST or PDTB relation classifi</context>
</contexts>
<marker>Bloehdorn, Basili, Cammisa, Moschitti, 2006</marker>
<rawString>S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti. 2006. Semantic kernels for text classification based on topological measures of feature similarity. In Proc. of ICDM’06, pages 808–812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory.</title>
<date>2001</date>
<booktitle>Proc. of Second SIGdial Workshop on Discourse and Dialogue-Volume 16,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="2095" citStr="Carlson et al., 2001" startWordPosition="308" endWordPosition="311">eful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations must solve a 41-class classification problem. Some of the relations corresponding to these classes are relatively more frequent in the </context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>L. Carlson, D. Marcu, and M. E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory. Proc. of Second SIGdial Workshop on Discourse and Dialogue-Volume 16, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
<author>H Lodhi</author>
</authors>
<title>Latent semantic kernels.</title>
<date>2002</date>
<journal>Journal of Intelligent Information Systems,</journal>
<pages>18--127</pages>
<contexts>
<context position="7802" citStr="Cristianini et al. (2002)" startWordPosition="1190" endWordPosition="1193">s verb classes, modality, context, and lexical features. In text classification, similarity measures have been employed in kernel methods, where they have been shown to improve accuracy over ‘bag-ofwords’ approaches. In Siolas and d’Alch´e-Buc (2000), a semantic proximity measure based on WordNet (Fellbaum, 1998) is defined, as a basis to create a proximity matrix for all terms of the problem. This matrix is then used to smooth the vectorial data, and the resulting ‘semantic’ metric is incorporated into a SVM kernel, resulting in a significant increase of accuracy and F-score over a baseline. Cristianini et al. (2002) have used a lexical similarity measure derived from Latent Semantic Indexing (Deerwester et al., 1990), where the semantic similarity between two terms is inferred from the analysis of their co-occurrence patterns: Terms that co-occur often in the same documents are considered as related. In this work, the statistical cooccurrence information is extracted by the means of singular value decomposition. The authors observe 400 substantial improvements in performance for some datasets, while little effect is obtained for others. Semantic kernels have also been shown to be efficient for text class</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2002</marker>
<rawString>N. Cristianini, J. Shawe-Taylor, and H. Lodhi. 2002. Latent semantic kernels. Journal of Intelligent Information Systems, 18:127–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="7905" citStr="Deerwester et al., 1990" startWordPosition="1207" endWordPosition="1210">e been employed in kernel methods, where they have been shown to improve accuracy over ‘bag-ofwords’ approaches. In Siolas and d’Alch´e-Buc (2000), a semantic proximity measure based on WordNet (Fellbaum, 1998) is defined, as a basis to create a proximity matrix for all terms of the problem. This matrix is then used to smooth the vectorial data, and the resulting ‘semantic’ metric is incorporated into a SVM kernel, resulting in a significant increase of accuracy and F-score over a baseline. Cristianini et al. (2002) have used a lexical similarity measure derived from Latent Semantic Indexing (Deerwester et al., 1990), where the semantic similarity between two terms is inferred from the analysis of their co-occurrence patterns: Terms that co-occur often in the same documents are considered as related. In this work, the statistical cooccurrence information is extracted by the means of singular value decomposition. The authors observe 400 substantial improvements in performance for some datasets, while little effect is obtained for others. Semantic kernels have also been shown to be efficient for text classification tasks, in the case in of unbalanced and sparse datasets. In Basili et al. (2006), a ‘conceptu</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A duVerle</author>
<author>H Prendinger</author>
</authors>
<title>A novel discourse parser based on Support Vector Machine classification.</title>
<date>2009</date>
<booktitle>In Proc. of ACL’09,</booktitle>
<pages>665--673</pages>
<contexts>
<context position="1941" citStr="duVerle and Prendinger, 2009" startWordPosition="282" endWordPosition="285"> F-score up to 50% compared to a baseline classifier. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse</context>
<context position="3459" citStr="duVerle and Prendinger, 2009" startWordPosition="503" endWordPosition="506">lation types occur very rarely, such as TOPIC-COMMENT[S][N] (2 instances), or EVALUATION[N][N] (3 instances). A similar phenomenon can be observed in PDTB, in which 15 level-two relations are employed: Some, such as EXPANSION.CONJUNCTION, occur as often as 8759 times throughout the corpus, whereas the remainder of the relations, such as EXPANSION.EXCEPTION and COMPARISON.PRAGMATIC CONCESSION, can appear as rarely as 17 and 12 times respectively. Although supervised approaches to discourse relation learning achieve good results on frequent relations, performance is poor on rare relation types (duVerle and Prendinger, 2009). Nonetheless, certain infrequent relation types might be important for specific tasks. For instance, 1We use the notation [N] and [S] respectively to denote the nucleus and satellite in a RST discourse relation. 399 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 399–409, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics capturing the RST TOPIC-COMMENT[S][N] and EVALUATION[N][N] relations can be useful for sentiment analysis (Pang and Lee, 2008). Another situation where detection of lowoccurring relations</context>
<context position="5336" citStr="duVerle and Prendinger (2009)" startWordPosition="796" endWordPosition="799">ssification for cases where unknown features are found in test vectors. • The proposed method is evaluated on the RSTDT and PDTB corpus, where it significantly improves accuracy and macro-average F-score when small training sets are used. For instance, when trained on moderately small datasets with ca. 1000 instances, the proposed method increases the macro-average F-score and accuracy up to 50%, compared to a baseline classifier. 2 Related Work Since the release in 2001 of the RSTDT corpus, several fully-supervised discourse parsers have been built in the RST framework. In the recent work of duVerle and Prendinger (2009), a discourse parser based on Support Vector Machines (SVM) (Vapnik, 1995) is proposed. SVMs are employed to train two classifiers: One, binary, for determining the presence of a relation, and another, multi-class, for determining the relation label between related text spans. For the discourse relation classifier, shallow lexical, syntactic and structural features, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. For relation classification, they report an accuracy of 0.668, and an F-score of 0.509 for the creation of the full discourse tree. The unsupervised method of Marcu and</context>
</contexts>
<marker>duVerle, Prendinger, 2009</marker>
<rawString>D. A. duVerle and H. Prendinger. 2009. A novel discourse parser based on Support Vector Machine classification. In Proc. of ACL’09, pages 665–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fang</author>
</authors>
<title>A re-examination of query expansion using lexical resources.</title>
<date>2008</date>
<booktitle>In Proc. of ACL’08,</booktitle>
<pages>139--147</pages>
<contexts>
<context position="14992" citStr="Fang, 2008" startWordPosition="2406" endWordPosition="2407">rix. This feature selection procedure can efficiently reduce the dimensions of the feature co-occurrence matrix to N x N. Because the feature co-occurrence matrix is symmetric, we must only store the elements for the upper (or lower) triangular portion of it. 3.2 Feature Vector Extension Once the feature co-occurrence matrix is computed using unlabeled data as described in Section 3.1, we can use it to extend a feature vector during training and testing. The proposed feature vector extension method is inspired by query expansion in the field of Information Retrieval (Salton and Buckley, 1983; Fang, 2008). One of the reasons that a classifier might perform poorly on a test instance is that there are features in the test instance that were not observed during training. We call FU = {fi} the set of features that were not observed by the classifier during training (i.e. occurring in test data but not in training data). For each of those features, we use the feature co-occurrence matrix to find the set of co-occurring features, Fc(fi). Let us denote the feature vector corresponding to a training or test instance x by fx. We use the superscript notation, fix to denote the i-th feature in fx. Moreov</context>
</contexts>
<marker>Fang, 2008</marker>
<rawString>H. Fang. 2008. A re-examination of query expansion using lexical resources. In Proc. of ACL’08, pages 139– 147.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hernault</author>
<author>D Bollegala</author>
<author>M Ishizuka</author>
</authors>
<title>A sequential model for discourse segmentation.</title>
<date>2010</date>
<booktitle>In Proc. of CICLing’10,</booktitle>
<pages>315--326</pages>
<contexts>
<context position="19579" citStr="Hernault et al., 2010" startWordPosition="3199" endWordPosition="3202">aximum entropy model) implemented in the Classias toolkit (Okazaki, 2009). Regularization parameters are set to their default value of one and are fixed throughout the experiments described in the paper. To create our unlabeled dataset, we use sentences extracted from the English Wikipedia2, as they are freely available and relatively easy to collect. For further extraction of syntactic features, these sentences are automatically parsed using the Stanford parser (Klein and Manning, 2003). Then, they are segmented into elementary discourse units (EDUs) using our sequential discourse segmenter (Hernault et al., 2010). The relatively high performance of this RST segmenter, which has an F-score of 0.95 compared to that of 0.98 between human annotators (Soricut and Marcu, 2003), is acceptable for this task. We collect and parse 100000 sentences from random Wikipedia articles. As there is no segmentation tool for the PDTB framework, we assume that co-occurrence information taken from EDUs created using a RST segmenter is also useful for extending feature vectors of PDTB relations. Unless otherwise noted, the experiments presented in the rest of this paper are done using those 100000 unlabeled instances. In th</context>
</contexts>
<marker>Hernault, Bollegala, Ishizuka, 2010</marker>
<rawString>H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A sequential model for discourse segmentation. In Proc. of CICLing’10, pages 315–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>15</volume>
<publisher>MIT Press.</publisher>
<contexts>
<context position="19449" citStr="Klein and Manning, 2003" startWordPosition="3181" endWordPosition="3184">ssification algorithm, we select a logistic regression classifier, for its simplicity. We use the multi-class logistic regression (maximum entropy model) implemented in the Classias toolkit (Okazaki, 2009). Regularization parameters are set to their default value of one and are fixed throughout the experiments described in the paper. To create our unlabeled dataset, we use sentences extracted from the English Wikipedia2, as they are freely available and relatively easy to collect. For further extraction of syntactic features, these sentences are automatically parsed using the Stanford parser (Klein and Manning, 2003). Then, they are segmented into elementary discourse units (EDUs) using our sequential discourse segmenter (Hernault et al., 2010). The relatively high performance of this RST segmenter, which has an F-score of 0.95 compared to that of 0.98 between human annotators (Soricut and Marcu, 2003), is acceptable for this task. We collect and parse 100000 sentences from random Wikipedia articles. As there is no segmentation tool for the PDTB framework, we assume that co-occurrence information taken from EDUs created using a RST segmenter is also useful for extending feature vectors of PDTB relations. </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems, volume 15. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Lin</author>
<author>M-Y Kan</author>
<author>H T Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP’09,</booktitle>
<pages>343--351</pages>
<contexts>
<context position="1981" citStr="Lin et al., 2009" startWordPosition="290" endWordPosition="293">r. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations must solve a 41-class</context>
<context position="6675" citStr="Lin et al. (2009)" startWordPosition="1009" endWordPosition="1012">ch as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. For example, the word pair (sell, hold) indicates a CONTRAST relation. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2008) performed a corpus study of the PDTB, and found that ‘explicit’ relations can be most of the times distinguished by their discourse connectives. Their discourse relation classifier reported an accuracy of 0.93 for explicit relations and in overall an accuracy of 0.744 for all relations in PDTB. Lin et al. (2009) studied the problem of detecting implicit relations in PDTB. Their relational classifier is trained using features extracted from dependency paths, contextual information, word pairs and production rules in parse trees. They reported for their classifier an accuracy of 0.402, which is an improvement of 14.1% over the previous state-of-theart for implicit relation classification in PDTB. For the same task, Pitler et al. (2009) also used word pairs, as well as several other types of features such as verb classes, modality, context, and lexical features. In text classification, similarity measur</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Z. Lin, M-Y. Kan, and H. T. Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proc. of EMNLP’09, pages 343–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02 Workshop on Effective tools</booktitle>
<pages>63--70</pages>
<contexts>
<context position="17003" citStr="Loper and Bird, 2002" startWordPosition="2774" endWordPosition="2777">etain only candidate co-occurring features of Fc(fi) possessing a co-occurrence value C(i,j) above a certain threshold. In the experiments of Section 4 howZi·Zj Ei,j = Zs Zj·(Zs−Zi) Zs I. (3) f0 x . 402 ever, we experienced dimension increase of 10000 at most, which did not require us to use thresholding. 3.3 Features We use three types of features: Word pairs, production rules from the parse tree, as well as features encoding the lexico-syntactic context at the border between two units of text (Soricut and Marcu, 2003). Our word pairs are lemmatized using the Wordnetbased lemmatizer of NLTK (Loper and Bird, 2002). Figure 1 shows the parse tree for a sentence composed of two discourse units, which serve as arguments of a discourse relation we want to generate a feature vector from. Lexical heads have been calculated using the projection rules of Magerman (1995), and annotated between brackets. Surrounded by dots is, for each argument, the minimal set of subparse trees containing strictly all the words of the argument. We first extract all possible lemmatized wordpairs from the two arguments, such as (Mr., when), (decline, ask) or (comment, sale). Next, we extract from left and right argument separately</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>E. Loper and S. Bird. 2002. NLTK: The natural language toolkit. In Proc. of ACL’02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>Proc. of ACL’95,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="17255" citStr="Magerman (1995)" startWordPosition="2821" endWordPosition="2822">t, which did not require us to use thresholding. 3.3 Features We use three types of features: Word pairs, production rules from the parse tree, as well as features encoding the lexico-syntactic context at the border between two units of text (Soricut and Marcu, 2003). Our word pairs are lemmatized using the Wordnetbased lemmatizer of NLTK (Loper and Bird, 2002). Figure 1 shows the parse tree for a sentence composed of two discourse units, which serve as arguments of a discourse relation we want to generate a feature vector from. Lexical heads have been calculated using the projection rules of Magerman (1995), and annotated between brackets. Surrounded by dots is, for each argument, the minimal set of subparse trees containing strictly all the words of the argument. We first extract all possible lemmatized wordpairs from the two arguments, such as (Mr., when), (decline, ask) or (comment, sale). Next, we extract from left and right argument separately, all production rules from the sub-parse trees, such as NP H NNP NNP, NNP H “Sherry” or TO H “to”. Finally, we encode in our features three nodes of the parse tree, which capture the local context at the connection point between the two arguments: The</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. M. Magerman. 1995. Statistical decision-tree models for parsing. Proc. of ACL’95, pages 276–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="2490" citStr="Mann and Thompson, 1988" startWordPosition="366" endWordPosition="369"> fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations must solve a 41-class classification problem. Some of the relations corresponding to these classes are relatively more frequent in the corpus, such as the ELABORATION[N][S] relation (4441 instances), or the ATTRIBUTION[S][N] relation (1612 instances).1 However, other relation types occur very rarely, such as TOPIC-COMMENT[S][N] (2 instances), or EVALUATION[N][N] (3 instances). A similar phenomenon can be observed in PDTB, in which 15 level-two relations are employed: Some, such as EXPANSION.CONJUNCTION, occur as often as 875</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. C. Mann and S. A. Thompson. 1988. Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C. D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>A Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02,</booktitle>
<pages>368--375</pages>
<contexts>
<context position="5952" citStr="Marcu and Echihabi (2002)" startWordPosition="890" endWordPosition="893">er (2009), a discourse parser based on Support Vector Machines (SVM) (Vapnik, 1995) is proposed. SVMs are employed to train two classifiers: One, binary, for determining the presence of a relation, and another, multi-class, for determining the relation label between related text spans. For the discourse relation classifier, shallow lexical, syntactic and structural features, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. For relation classification, they report an accuracy of 0.668, and an F-score of 0.509 for the creation of the full discourse tree. The unsupervised method of Marcu and Echihabi (2002) was the first that tried to detect implicit relations (i.e. relations not accompanied by a cue phrase, such as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. For example, the word pair (sell, hold) indicates a CONTRAST relation. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2008) performed a corpus study of the PDTB, and found that ‘explicit’ relations can be most of the times distinguished by their discourse connectives. Their discourse relation classifier reported </context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>D. Marcu and A. Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proc. of ACL’02, pages 368–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1733" citStr="Marcu, 2000" startWordPosition="253" endWordPosition="254">ro-average F-score when small training datasets are used. For instance, with training sets of c.a. 1000 labeled instances, the proposed method brings improvements in accuracy and macro-average F-score up to 50% compared to a baseline classifier. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 co</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="21901" citStr="Marcus et al., 1993" startWordPosition="3581" endWordPosition="3584">y) TO VP VP (comment) S (comment) VP (declined) comment when asked about the sales VB WHADVP (when) WRB SBAR (when) S (asked) VP (asked) VBN PP (about) IN NP (sales) DT NNS . (.) . employed. For the PDTB classifier, we conform to the guidelines of Prasad et al. (2008b, 5): The portion of the corpus corresponding to sections 2–21 of the WSJ is used for training the classifier, while the portion corresponding to WSJ section 23 is used for testing. In order to extract syntactic features, all training and test data are furthermore aligned with their corresponding parse trees in the Penn Treebank (Marcus et al., 1993). Because in the PDTB an instance can be annotated with several discourse relations simultaneously—called ‘senses’ in Prasad et al. (2008b)—for each instance with n senses in the corpus, we create n identical feature vectors, each being labeled by one of the instance’s senses. However, in the RST framework, only one relation is allowed to hold between two EDUs. Consequently, each instance from the RSTDT is labeled with a single discourse relation, from which a single feature vector is created. For RSTDT, we extract 25078 training vectors and 1633 test vectors. For PDTB we extract 49748 trainin</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
</authors>
<title>Classias: A collection of machinelearning algorithms for classification.</title>
<date>2009</date>
<note>http:// www.chokkan.org/software/classias/.</note>
<contexts>
<context position="19030" citStr="Okazaki, 2009" startWordPosition="3119" endWordPosition="3120"> where the two arguments belong to different sentences, the nodes N,,,, N,., Np cannot be defined, and their corresponding features are given the value zero. 4 Experiments The proposed method is independent of any particular classification algorithm. Because our goal is strictly to evaluate the relative benefit of employing the proposed method, and not the absolute performance when used with a specific classification algorithm, we select a logistic regression classifier, for its simplicity. We use the multi-class logistic regression (maximum entropy model) implemented in the Classias toolkit (Okazaki, 2009). Regularization parameters are set to their default value of one and are fixed throughout the experiments described in the paper. To create our unlabeled dataset, we use sentences extracted from the English Wikipedia2, as they are freely available and relatively easy to collect. For further extraction of syntactic features, these sentences are automatically parsed using the Stanford parser (Klein and Manning, 2003). Then, they are segmented into elementary discourse units (EDUs) using our sequential discourse segmenter (Hernault et al., 2010). The relatively high performance of this RST segme</context>
</contexts>
<marker>Okazaki, 2009</marker>
<rawString>N. Okazaki. 2009. Classias: A collection of machinelearning algorithms for classification. http:// www.chokkan.org/software/classias/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="3998" citStr="Pang and Lee, 2008" startWordPosition="578" endWordPosition="581">tions, performance is poor on rare relation types (duVerle and Prendinger, 2009). Nonetheless, certain infrequent relation types might be important for specific tasks. For instance, 1We use the notation [N] and [S] respectively to denote the nucleus and satellite in a RST discourse relation. 399 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 399–409, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics capturing the RST TOPIC-COMMENT[S][N] and EVALUATION[N][N] relations can be useful for sentiment analysis (Pang and Lee, 2008). Another situation where detection of lowoccurring relations is desirable is the case where we have only a small training set at our disposal, for instance when there is not enough annotated data for all the relation types described in a discourse theory. In this case, all the dataset’s relations can be considered rare, and being able to build an efficient classifier depends on the capacity to deal with this lack of annotated data. Our contributions in this paper are summarized as follows. • We propose a semi-supervised method that exploits the abundant, freely-available unlabeled data, which</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>M Raghupathy</author>
<author>H Mehta</author>
<author>A Nenkova</author>
<author>A Lee</author>
<author>A Joshi</author>
</authors>
<title>Easily identifiable discourse relations.</title>
<date>2008</date>
<booktitle>In Proc. of COLING’08 (Posters),</booktitle>
<pages>87--90</pages>
<contexts>
<context position="6361" citStr="Pitler et al. (2008)" startWordPosition="958" endWordPosition="961">and Marcu, 2003) are used. For relation classification, they report an accuracy of 0.668, and an F-score of 0.509 for the creation of the full discourse tree. The unsupervised method of Marcu and Echihabi (2002) was the first that tried to detect implicit relations (i.e. relations not accompanied by a cue phrase, such as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. For example, the word pair (sell, hold) indicates a CONTRAST relation. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2008) performed a corpus study of the PDTB, and found that ‘explicit’ relations can be most of the times distinguished by their discourse connectives. Their discourse relation classifier reported an accuracy of 0.93 for explicit relations and in overall an accuracy of 0.744 for all relations in PDTB. Lin et al. (2009) studied the problem of detecting implicit relations in PDTB. Their relational classifier is trained using features extracted from dependency paths, contextual information, word pairs and production rules in parse trees. They reported for their classifier an accuracy of 0.402, which is</context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee, and A. Joshi. 2008. Easily identifiable discourse relations. In Proc. of COLING’08 (Posters), pages 87–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proc. of ACL’09,</booktitle>
<pages>683--691</pages>
<contexts>
<context position="1962" citStr="Pitler et al., 2009" startWordPosition="286" endWordPosition="289"> a baseline classifier. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations mu</context>
<context position="7105" citStr="Pitler et al. (2009)" startWordPosition="1076" endWordPosition="1079">iscourse connectives. Their discourse relation classifier reported an accuracy of 0.93 for explicit relations and in overall an accuracy of 0.744 for all relations in PDTB. Lin et al. (2009) studied the problem of detecting implicit relations in PDTB. Their relational classifier is trained using features extracted from dependency paths, contextual information, word pairs and production rules in parse trees. They reported for their classifier an accuracy of 0.402, which is an improvement of 14.1% over the previous state-of-theart for implicit relation classification in PDTB. For the same task, Pitler et al. (2009) also used word pairs, as well as several other types of features such as verb classes, modality, context, and lexical features. In text classification, similarity measures have been employed in kernel methods, where they have been shown to improve accuracy over ‘bag-ofwords’ approaches. In Siolas and d’Alch´e-Buc (2000), a semantic proximity measure based on WordNet (Fellbaum, 1998) is defined, as a basis to create a proximity matrix for all terms of the problem. This matrix is then used to smooth the vectorial data, and the resulting ‘semantic’ metric is incorporated into a SVM kernel, resul</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proc. of ACL’09, pages 683–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Piwek</author>
<author>H Hernault</author>
<author>H Prendinger</author>
<author>M Ishizuka</author>
</authors>
<title>Generating dialogues between virtual agents automatically from text.</title>
<date>2007</date>
<booktitle>In Proc. of IVA’07,</booktitle>
<pages>161--174</pages>
<contexts>
<context position="1778" citStr="Piwek et al., 2007" startWordPosition="259" endWordPosition="262"> datasets are used. For instance, with training sets of c.a. 1000 labeled instances, the proposed method brings improvements in accuracy and macro-average F-score up to 50% compared to a baseline classifier. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relati</context>
</contexts>
<marker>Piwek, Hernault, Prendinger, Ishizuka, 2007</marker>
<rawString>P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka. 2007. Generating dialogues between virtual agents automatically from text. In Proc. of IVA’07, pages 161– 174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Plackett</author>
</authors>
<title>Karl Pearson and the chi-squared test.</title>
<date>1983</date>
<journal>International Statistical Review /Revue Internationale de Statistique,</journal>
<volume>51</volume>
<issue>1</issue>
<contexts>
<context position="12078" citStr="Plackett, 1983" startWordPosition="1898" endWordPosition="1899"> C such that the (i, j)-th element of C, C(i,j) E [0, 1] denotes the degree of co-occurrence between the two features fi and fj. If both fi and fj appear in a feature vector then we define them to be co-occurring. The number of different feature vectors in which fi and fj co-occur is denoted by the function h(fi, fj). From our definition of co-occurrence it follows that h(fi, fj) = h(fj, fi). Importantly, feature cooccurrences can be calculated only using unlabeled data. Feature co-occurrence matrices can be computed using any co-occurrence measure. For the current task we use the χ2-measure (Plackett, 1983) as the preferred co-occurrence measure because of its simplicity. χ2-measure between two features fi and fj is defined as follows, � i,j 2 (Ok,l − Ei,j k,l)2 χi,j = . (1) Ei,j k=1l=1 k,l Therein, Oi,j and Ei,j are the 2x2 matrices containing respectively observed frequencies and expected frequencies, which are respectively computed using C as, Oi,j = h(fi,fj) Zi − h(fi, fj) (2) (Zj − h(fi, fj) Zs − Zi − Zj 2 2 401 and Zi·(Zs−Zj) Zs (Zs−Zi)·(Zs−Zj) Zs Here, Zi = --&apos;k6=i h(fi, fk), and Zs = --&apos;n i=1 Zi. Finally, we create the feature co-occurrence matrix C, such that, for all pairs of features </context>
</contexts>
<marker>Plackett, 1983</marker>
<rawString>R. L. Plackett. 1983. Karl Pearson and the chi-squared test. International Statistical Review /Revue Internationale de Statistique, 51(1):59–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0. In</title>
<date>2008</date>
<booktitle>Proc. of LREC’08.</booktitle>
<contexts>
<context position="2155" citStr="Prasad et al., 2008" startWordPosition="318" endWordPosition="321"> Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations must solve a 41-class classification problem. Some of the relations corresponding to these classes are relatively more frequent in the corpus, such as the ELABORATION[N][S] relation (4441 instanc</context>
<context position="21548" citStr="Prasad et al. (2008" startWordPosition="3522" endWordPosition="3525">test set. For the RST classifier, the dedicated training and test sets of the RSTDT are 2http://en.wikipedia.org 403 S (declined) Argument 1 Argument 2 Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them—lexical heads are indicated between brackets. NNP NNP VBD (declined) Mr. Sherry declined to NP (Sherry) TO VP VP (comment) S (comment) VP (declined) comment when asked about the sales VB WHADVP (when) WRB SBAR (when) S (asked) VP (asked) VBN PP (about) IN NP (sales) DT NNS . (.) . employed. For the PDTB classifier, we conform to the guidelines of Prasad et al. (2008b, 5): The portion of the corpus corresponding to sections 2–21 of the WSJ is used for training the classifier, while the portion corresponding to WSJ section 23 is used for testing. In order to extract syntactic features, all training and test data are furthermore aligned with their corresponding parse trees in the Penn Treebank (Marcus et al., 1993). Because in the PDTB an instance can be annotated with several discourse relations simultaneously—called ‘senses’ in Prasad et al. (2008b)—for each instance with n senses in the corpus, we create n identical feature vectors, each being labeled by</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008a. The Penn Discourse TreeBank 2.0. In Proc. of LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>E Miltsakaki</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>A Joshi</author>
<author>L Robaldo</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0 annotation manual.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania Institute for Research in Cognitive Science.</institution>
<contexts>
<context position="2155" citStr="Prasad et al., 2008" startWordPosition="318" endWordPosition="321"> Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations must solve a 41-class classification problem. Some of the relations corresponding to these classes are relatively more frequent in the corpus, such as the ELABORATION[N][S] relation (4441 instanc</context>
<context position="21548" citStr="Prasad et al. (2008" startWordPosition="3522" endWordPosition="3525">test set. For the RST classifier, the dedicated training and test sets of the RSTDT are 2http://en.wikipedia.org 403 S (declined) Argument 1 Argument 2 Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them—lexical heads are indicated between brackets. NNP NNP VBD (declined) Mr. Sherry declined to NP (Sherry) TO VP VP (comment) S (comment) VP (declined) comment when asked about the sales VB WHADVP (when) WRB SBAR (when) S (asked) VP (asked) VBN PP (about) IN NP (sales) DT NNS . (.) . employed. For the PDTB classifier, we conform to the guidelines of Prasad et al. (2008b, 5): The portion of the corpus corresponding to sections 2–21 of the WSJ is used for training the classifier, while the portion corresponding to WSJ section 23 is used for testing. In order to extract syntactic features, all training and test data are furthermore aligned with their corresponding parse trees in the Penn Treebank (Marcus et al., 1993). Because in the PDTB an instance can be annotated with several discourse relations simultaneously—called ‘senses’ in Prasad et al. (2008b)—for each instance with n senses in the corpus, we create n identical feature vectors, each being labeled by</context>
</contexts>
<marker>Prasad, Miltsakaki, Dinesh, Lee, Joshi, Robaldo, Webber, 2008</marker>
<rawString>R. Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi, L. Robaldo, and B. Webber. 2008b. The Penn Discourse Treebank 2.0 annotation manual. Technical report, University of Pennsylvania Institute for Research in Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill Book Company.</publisher>
<contexts>
<context position="14979" citStr="Salton and Buckley, 1983" startWordPosition="2402" endWordPosition="2405"> feature co-occurrence matrix. This feature selection procedure can efficiently reduce the dimensions of the feature co-occurrence matrix to N x N. Because the feature co-occurrence matrix is symmetric, we must only store the elements for the upper (or lower) triangular portion of it. 3.2 Feature Vector Extension Once the feature co-occurrence matrix is computed using unlabeled data as described in Section 3.1, we can use it to extend a feature vector during training and testing. The proposed feature vector extension method is inspired by query expansion in the field of Information Retrieval (Salton and Buckley, 1983; Fang, 2008). One of the reasons that a classifier might perform poorly on a test instance is that there are features in the test instance that were not observed during training. We call FU = {fi} the set of features that were not observed by the classifier during training (i.e. occurring in test data but not in training data). For each of those features, we use the feature co-occurrence matrix to find the set of co-occurring features, Fc(fi). Let us denote the feature vector corresponding to a training or test instance x by fx. We use the superscript notation, fix to denote the i-th feature </context>
</contexts>
<marker>Salton, Buckley, 1983</marker>
<rawString>G. Salton and C. Buckley. 1983. Introduction to Modern Information Retrieval. McGraw-Hill Book Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Siolas</author>
<author>F d’Alch´e-Buc</author>
</authors>
<title>Support Vector Machines based on a semantic kernel for text categorization.</title>
<date>2000</date>
<booktitle>In Proc. of IJCNN’00,</booktitle>
<volume>5</volume>
<pages>5205</pages>
<marker>Siolas, d’Alch´e-Buc, 2000</marker>
<rawString>G. Siolas and F. d’Alch´e-Buc. 2000. Support Vector Machines based on a semantic kernel for text categorization. In Proc. of IJCNN’00, volume 5, page 5205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>G Namata</author>
<author>J Wiebe</author>
<author>L Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP’09,</booktitle>
<pages>170--179</pages>
<contexts>
<context position="1699" citStr="Somasundaran et al., 2009" startWordPosition="247" endWordPosition="250">t improvement in classification accuracy and macro-average F-score when small training datasets are used. For instance, with training sets of c.a. 1000 labeled instances, the proposed method brings improvements in accuracy and macro-average F-score up to 50% compared to a baseline classifier. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, wh</context>
</contexts>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proc. of EMNLP’09, pages 170–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>Proc. of NA-ACL’03,</booktitle>
<pages>1--149</pages>
<contexts>
<context position="5757" citStr="Soricut and Marcu, 2003" startWordPosition="858" endWordPosition="861">ssifier. 2 Related Work Since the release in 2001 of the RSTDT corpus, several fully-supervised discourse parsers have been built in the RST framework. In the recent work of duVerle and Prendinger (2009), a discourse parser based on Support Vector Machines (SVM) (Vapnik, 1995) is proposed. SVMs are employed to train two classifiers: One, binary, for determining the presence of a relation, and another, multi-class, for determining the relation label between related text spans. For the discourse relation classifier, shallow lexical, syntactic and structural features, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. For relation classification, they report an accuracy of 0.668, and an F-score of 0.509 for the creation of the full discourse tree. The unsupervised method of Marcu and Echihabi (2002) was the first that tried to detect implicit relations (i.e. relations not accompanied by a cue phrase, such as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. For example, the word pair (sell, hold) indicates a CONTRAST relation. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2</context>
<context position="16907" citStr="Soricut and Marcu, 2003" startWordPosition="2758" endWordPosition="2761">Note that because this process can potentially increase the dimension too much, it is possible to retain only candidate co-occurring features of Fc(fi) possessing a co-occurrence value C(i,j) above a certain threshold. In the experiments of Section 4 howZi·Zj Ei,j = Zs Zj·(Zs−Zi) Zs I. (3) f0 x . 402 ever, we experienced dimension increase of 10000 at most, which did not require us to use thresholding. 3.3 Features We use three types of features: Word pairs, production rules from the parse tree, as well as features encoding the lexico-syntactic context at the border between two units of text (Soricut and Marcu, 2003). Our word pairs are lemmatized using the Wordnetbased lemmatizer of NLTK (Loper and Bird, 2002). Figure 1 shows the parse tree for a sentence composed of two discourse units, which serve as arguments of a discourse relation we want to generate a feature vector from. Lexical heads have been calculated using the projection rules of Magerman (1995), and annotated between brackets. Surrounded by dots is, for each argument, the minimal set of subparse trees containing strictly all the words of the argument. We first extract all possible lemmatized wordpairs from the two arguments, such as (Mr., wh</context>
<context position="19740" citStr="Soricut and Marcu, 2003" startWordPosition="3226" endWordPosition="3229">hout the experiments described in the paper. To create our unlabeled dataset, we use sentences extracted from the English Wikipedia2, as they are freely available and relatively easy to collect. For further extraction of syntactic features, these sentences are automatically parsed using the Stanford parser (Klein and Manning, 2003). Then, they are segmented into elementary discourse units (EDUs) using our sequential discourse segmenter (Hernault et al., 2010). The relatively high performance of this RST segmenter, which has an F-score of 0.95 compared to that of 0.98 between human annotators (Soricut and Marcu, 2003), is acceptable for this task. We collect and parse 100000 sentences from random Wikipedia articles. As there is no segmentation tool for the PDTB framework, we assume that co-occurrence information taken from EDUs created using a RST segmenter is also useful for extending feature vectors of PDTB relations. Unless otherwise noted, the experiments presented in the rest of this paper are done using those 100000 unlabeled instances. In the unlabeled data, any two consecutive discourse units might not always be connected by a discourse relation. Therefore, we introduce an artificial NONE relation </context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. Proc. of NA-ACL’03, 1:149–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="5410" citStr="Vapnik, 1995" startWordPosition="809" endWordPosition="810">hod is evaluated on the RSTDT and PDTB corpus, where it significantly improves accuracy and macro-average F-score when small training sets are used. For instance, when trained on moderately small datasets with ca. 1000 instances, the proposed method increases the macro-average F-score and accuracy up to 50%, compared to a baseline classifier. 2 Related Work Since the release in 2001 of the RSTDT corpus, several fully-supervised discourse parsers have been built in the RST framework. In the recent work of duVerle and Prendinger (2009), a discourse parser based on Support Vector Machines (SVM) (Vapnik, 1995) is proposed. SVMs are employed to train two classifiers: One, binary, for determining the presence of a relation, and another, multi-class, for determining the relation label between related text spans. For the discourse relation classifier, shallow lexical, syntactic and structural features, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. For relation classification, they report an accuracy of 0.668, and an F-score of 0.509 for the creation of the full discourse tree. The unsupervised method of Marcu and Echihabi (2002) was the first that tried to detect implicit relations (i.</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>