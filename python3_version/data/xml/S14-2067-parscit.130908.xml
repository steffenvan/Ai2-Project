<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025909">
<title confidence="0.87361">
KUNLPLab:Sentiment Analysis on Twitter Data
Beakal Gizachew Assefa
</title>
<author confidence="0.733181">
Koc Unversity
</author>
<email confidence="0.959188">
bassefa13@ku.edu.tr
</email>
<sectionHeader confidence="0.992815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919818181818">
This paper presents the system submitted
by KUNLPLab for SemEval-2014 Task9
- Subtask B: Message Polarity on Twitter
data. Lexicon features and bag-of-words
features are mainly used to represent the
datasets. We trained a logistic regression
classifier and got an accuracy of 6% in-
crease from the baseline feature represen-
tation. The effect of pre-processing on the
classifier’s accuracy is also discussed in
this work.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9876204">
Microblogging sites has become a common
way of reflecting peoples’ opinion. Unlike the
regular blogs, the size of a message on a mi-
croblogging site is relatively small. The need to
automatically detect and summarize the sentiment
of messages from users on a given topic or prod-
uct has gained the interest of researchers.
The sentiment of a message can be negative,
positive, or neutral. In the broader sense, automat-
ically detecting the polarity of a message would
help business firms easily detect customers’ feed-
back on their product or services. Which in turn
helps them improve their decision making by
providing information of user preferences, prod-
uct trend, and user categories.(Chew and Eysen-
bach, 2010; Salethe and Khandelwal,2011). Sen-
timent analysis is also used in other do-
mains.(Mandel et al.,2012).
Twitter is one of the mostly widely used mi-
croblogging web site with over 200 million users
send over 400 million tweets daily(September
2013). A peculiar characteristic of a Twitter data
are as follow: emoticons are widely used, the
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons. org/licenses/by/4.0/
maximum length of a tweet is 140 character, some
words are abbreviated, or some are elongated by
repeating letters of a word multiple times.
The organizers of the SemEval-2014 has pro-
vided a corpus of tweets and posted a task to au-
tomatically detect their respective sentiments.
Sub task B of Task 9: Sentiment Analysis on
Twitter is describe as follows
Task B - Message Polarity Classification
“Given a message, classify whether the mes-
sage is of positive, negative, or neutral sentiment.
For messages conveying both a positive and neg-
ative sentiment, whichever is the stronger senti-
ment should be chosen.”
This paper describes the system submitted by
KUNLBLab for participation in SemEval-2014
Task 9 subtask B. Models were trained using the
LIBLINEAR classification library (Fan et al.,
2008). An accuracy of 66.11% is attained by the
classifier by testing on the development set.
The remaining of the document is organized as
follows: Section 2 presents a brief literature re-
view on sentiment analysis on Twitter data. Sec-
tion 3 discusses the system developed to solve the
above task, characteristics of the dataset, prepress-
ing on the dataset, and various feature representa-
tion. Section 4 illustrates the evaluation results.
Section 5 presents conclusion and remarks.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998867555555556">
Sentiment analysis has been studied in Natural
Language Processing. Different approaches have
been implemented to automatically detect senti-
ment on texts (Pang et al., 2002; Pang and Lee,
2004; Wiebe and Riloff, 2005; Glance et al., 2005;
Wilson et al., 2005).
There is also an active research on Sentiment
analysis on Twitter data. (Go et al., 2009,
Bermingham and Smeaton, 2010, and Pak and
</bodyText>
<page confidence="0.984723">
391
</page>
<note confidence="0.7305725">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391–394,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999491428571429">
Paroubek 2010) consider tweets with good emot-
icons as positive examples and tweets with bad
emoticons as negative examples for the training
data, and built a classifier using unigrams and bi-
grams as features.
Barbosa and Feng (2010) classified the subjec-
tivity of tweets based on traditional features with
the inclusion of some witter specific clues such as
retweets, hashtags, links, uppercase words, emot-
icons, and exclamation and question marks.
(Agarwal et al. 2011 ) introduced a POS-
specific prior polarity features and used a tree
kernel to obviate the need for tedious feature
engineering.
</bodyText>
<sectionHeader confidence="0.991119" genericHeader="method">
3 System Description
</sectionHeader>
<subsectionHeader confidence="0.962605">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999010333333333">
The organizer of SemEval-2014 have provided
training and development sets. Table 1 bellow il-
lustrates the characteristics of the dataset.
</bodyText>
<table confidence="0.983389">
Positive Negative Neutral
Train 3045 1,209 4004
Dev 575 340 739
</table>
<tableCaption confidence="0.998959">
Table 1. Dataset characteristics
</tableCaption>
<subsectionHeader confidence="0.999651">
3.2 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999992666666667">
We employed two major pre-processing in
the datasets. Converting terms to their correct rep-
resentation, and stemming.
Mostly, in Twitter, words are not written in
their correct/full form. For instance, love,
loooove, looove convey the same meaning as the
word love alone regardless of the extent of the em-
phasis intended to describe. Reducing this various
representations of the same term to common word
helps in better matching them even if they are
written in different way. This is more problematic
if our features are based on term matching and
hence increase the number of unknown terms.
The second pre-processing we employed is
stemming the terms in the dataset. In most cases,
morphological variants of words have similar se-
mantic interpretations and can be considered as
equivalent. The advantage of stemming is two-
fold. Primarily it reduces the number of OOVs
(Out Of Vocabulary) terms. The second one is
feature reduction.
</bodyText>
<subsectionHeader confidence="0.965676">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999975655172414">
There are two main categories of features used
in the development of this system. Bag-of-Words
and sentiment lexicon features.
Bag-of-Words features takes a given input text
and extracts the raw words as features independ-
ent of one another. One issue in using this feature
is how to represent negations. In the texts “I like
the movie. “, and “I do not like the movie.”, the
sentiment of the words in the two texts is opposite
since the two statements are negations of one an-
other. One way of representing the negated word
is by appending the tag _NOT (Chen (2001) and
Pang et al. (2002). The _NOT tag suffixes all
words between the negation word and the first
punctuation mark after the negation word. In the
above example the second text is transformed to “
I do like_IOT the_IOT movie _IOT”. In repre-
sentation of the negations, we employ the above
approach. Lee Becker et al. (2013) directly inte-
grated the polarized word representation in their
system. One disadvantage of this representation
is the number of features doubles in worst case.
Sentiment lexicons are words, which have as-
sociation with positive or negative sentiments.
Unlike the Bag-Of-Words, instead of taking the
raw word as a feature, every word has a score,
which is a measure of how much positive or neg-
ative sentiment the lexicon has. In this work we
use the NRC Hashtag Sentiment Lexicon, and
Sentiment140 Lexicon (Mohammad 2013). Both
list of lexicons are used in the SemEval 2013 by
NRC-Canada team.
The IRCHashtag Sentiment Lexicon is based
on the common practice that users use the # sym-
bol to emphasis on a topic or a word. The hashtag
lexicon was created from a collection of tweets
that had a positive or a negative word hashtag
such as #good, #excellent, #bad, and #terrible
(Mohammad 2012). It was created from 775,310
tweets posted between April and December 2012
using a list of 78 positive and negative word
hashtags. They have provided unigram, bigram,
and trigram datasest. In this work however, we
used the unigram features which contains 54,129
terms.
The Sentiment140 is also a list of words with
associations to positive an negative sentiments. It
has the same format as the NRC Hashtag
Sentiment Lexicon. However, it was created from
the sentiment140 corpus of 1.6 million tweets, and
emoticons were used as positive and negative
labels (instead of hashtagged words).
In order to investigate the effect of the features
listed above, we have used various combination of
them. Table 2 shows 12 kinds of features used for
the system we have developed.
The converted versions of the features are the
ones where the enlongated words are shortened to
</bodyText>
<page confidence="0.992332">
392
</page>
<bodyText confidence="0.939764">
their normal form and terms with less than 5
occurances in the training set are ignored.
</bodyText>
<table confidence="0.944874846153846">
Code Features
F1 RawBag-Of-Word
F2 Bag-Of-WordStemmed
F3 ConvertedStemedBag-Of-Word
F4 Hashtag
F5 Sentiment140
F6 CombinedLexicons
F7 ConvertedHashtag
F8 ConvertedSentiment140
F9 ConvertedNegatedHashtag
F10 ConvertedNegatedSentiment140
F11 ConvertedStemmeLexicon
F12 AllCombined
</table>
<tableCaption confidence="0.997423">
Table 2. Code of features and their names
</tableCaption>
<bodyText confidence="0.999791956521739">
The description of the features is as follow, F1
is a raw Bag-Of-Word features in which terms
with more than five frequency are taken as fea-
tures. F2 takes the stem of the words whereas F3
applies both stemmig and shortening of elongeted
words to the corpus then takes Bag-Of-Word fea-
tures of the converted corpus.
F4 and F5 are sentiment lexcon features
hashtag. F6 is a combined Sentiment140, and
Hashtag features. F7 and F8 are applications of
the sentiment lexicons after applying shortening
and steming. Negative message representation is
included in features F9 and F10. F11 is the com-
bination of a preprocessed corpus by applicaiton
of stemming and short represenation of elnogated
terms, negative message representation, and ex-
tracting a combined sentiword140 and hash tag
features.
Feature F12 is the combination of all the fea-
tures. If a term after being preprocessed is found
in one of the lexicon features, the lexicon polarity
measure is taken as feature value.Otherwise; we
resort to the Bag-Of-Word feature.
</bodyText>
<subsectionHeader confidence="0.993687">
3.4 The classifier
</subsectionHeader>
<bodyText confidence="0.9999825">
For this task, we have used L2 regularized lo-
gistic regression and used the LIBLINEAR imple-
mentation (Rong-En Fan et al.).To estimate the
hyper parameters, we applied a 10 fold cross val-
idation on the training set. Liblinear implementa-
tion of a L2 regularized logistic regression takes a
single cost C parameter. The value of the cost C
parameter decides the weight between the L1 reg-
ularization term and L2 regularization term. If the
value of C is less than one, it means the more
weight it given to the L1 regularization term. On
the other hand C values more than one gives more
weight to the L2 regularizing term. The cost pa-
rameter C=1 gives the best result on the cross val-
idation test. The same value is used to train our
model.
</bodyText>
<sectionHeader confidence="0.976863" genericHeader="evaluation">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.9995577">
As described in Table 2 of section 3.3, the ma-
jor features used in this work are bag-of-word and
sentiment lexicon features. In addition to the fea-
ture representation, pre-processing has been done
on the datasets.
F1 is a baseline feature (raw Bag-Of-Word),
with a total accuracy of 60.16. Simply converting
the elongated terms to their normal form and ap-
plying stemming on the corpus increase the accu-
racy from 60.16 to 64.92 (4.76%).
</bodyText>
<table confidence="0.999933846153846">
Positive Negative Neutral Total
F1 61.71 52.48 60.55 60.16
F2` 61.71 51.43 61.18 60.36
F3 67.64 62.86 63.64 64.92
F4 66.67 52.94 60.10 61.65
F5 67.91 54.72 61.00 62.54
F6 64.86 55.24 61.47 61.94
F7 67.72 60.42 63.07 63.51
F8 70.29 58.93 63.02 64.17
F9 70.27 56.12 62.28 63.36
F10 71.73 59.29 62.86 64.65
F11 67.25 62.89 63.14 64.52
F12 71.12 61.4 64.13 66.11
</table>
<tableCaption confidence="0.574766142857143">
Table 3. Results of the evaluation on the devel-
opment set
F6 (the combined lexicon feature- senti-
word140 and hashtag) yields an accuracy of
61.94. Applying conversion, negative representa-
tion and stemming raises the accuracy to 64.52
(F11)
</tableCaption>
<table confidence="0.9846385">
Testset MacroF1
LiveJournal2014 63.77
SMS2013 55.89
Twitter2013 58.12
Twitter2014 61.72
Twitter2014Sarcasm 44.60
</table>
<tableCaption confidence="0.99987">
Table 4. Evaluation result on test set
</tableCaption>
<page confidence="0.999042">
393
</page>
<bodyText confidence="0.999839">
The accuracy of identifying negative sentiment
is the least in all features. This shows that we need
a better representation of negated messages.
A test dataset was also provided by the organ-
izer of semEval-2014. Table 4 show the accuracy
of the KUNPLab classifier.
Our model has performed poorly on the Twit-
ter2014Sarcasm test set (44.60%). The perfor-
mance of our classifier on LiveJournal2014 is
similar to the development set test performance.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999965066666667">
The performance of a classifier depends on fea-
ture representation, hyperparameter optimization
and regularization. In this work, we mainly used
bag-of-word features and sentiment lexicon fea-
tures. We trained a L2 regularized logistic regres-
sion model. Two major features are used to repre-
sent the datasets; Bag-of-Word features and Lex-
ical features. It has been shown that stemming the
terms increases accuracy of the classifier in either
case. The accuracy of the classifier on develop-
ment set and training set is reported and has
shown an increase of 6% in accuracy form the
baseline with 95% confidence interval..The eval-
uation of our system on SemEval-2014 test data is
also shown with an F measure of 44.60 to 63.77%.
</bodyText>
<sectionHeader confidence="0.998383" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999940666666666">
I would like to acknowledge Ass.Prof. Dr.
Deniz YURET for his advice, guidance, encour-
agement and inspiration to participate in
SemEval-2014. I also like to thank Mohammad
Khuram SALEEM, and Mohamad IRFAN for
proof reading this document.
</bodyText>
<sectionHeader confidence="0.935797" genericHeader="references">
Reference
</sectionHeader>
<bodyText confidence="0.616293375">
Cynthia Chew and Gunther Eysenbach. 2010. Pandem-
ics in the Age of Twitter: Content Analysis of
Tweets during the 2009 H1N1 Outbreak. PLoS
ONE, 5(11):e14118+, November.
Marcel Salath´e and Shashank Khandelwal. 2011. As-
sessing vaccination sentiments with online social
media: Implications for infectious disease dynamics
and control. PLoS Computational Biology, 7(10).
</bodyText>
<reference confidence="0.99970175">
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Ro-
drigue. 2012. A demographic analysis of online sen-
timent during hurricane irene. In Proceedngs of the
Second Workshop on Language in Social Media,
LSM’12, pages 27–36, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Sanjiv Das and Mike Chen. 2001. Yahoo! for amazon:
extracting market sentiment from stock message
boards. In Proceedings of the 8th Asia Pacific Fi-
nance Association Annual Conference.
Lee Becker, George Erhart, David Skiba and Valentine
Matula. 2013. AVAYA: Sentiment Analysis on
Twitter with Self-Training and Polarity Lexicon Ex-
pansion. Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013)
Saif Mohammad. 2012. Emotional Tweets. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics (*SEM), pages 246–255,
Montr´eal, Canada. Association for Computational
Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification Using
Machine Learning Techniques. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002)
Mohammad, Saif and Kiritchenko, Svetlana and Zhu,
Xiaodan 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets
Proceedings of the seventh international workshop
on Semantic Evaluation Exercises (SemEval-2013).
Go, A., Bhayani, R., Huang, L.: Twitter sentiment clas-
sification using distant supervision. CS224N Project
Report, Stanford (2009)
Barbosa, L., Feng, J.: Robust sentiment detection on
Twitter from biased and noisy data. In: Proceedings
of COLING. pp. 36–44 (2010)
Agarwal, A., Xie, B., Vovsha, I., Rambow, O., Pas-
sonneau, R.: Sentiment analysis of Twitter data. In:
Proc. ACL 2011 Workshop on Languages in Social
Media. pp. 30–38 (2011)
Adam Bermingham and Alan Smeaton. 2010. Classi-
fying sentiment in microblogs: is brevity an ad-
vantage is brevity an advantage? ACM, pages 1833–
1836.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
Proceedings of LREC.
Glance, N., M. Hurst, K. Nigam, M. Siegler, R. Stock
ton, and T. Tomokiyo. 2005. Deriving marketing in-
telligence from online discussion. In Proceedings of
the eleventh ACM SIGKDD, pages 419–428. ACM.
Wiebe, J. and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated
texts. Computational Linguistics and Intelligent
Text Processing, pages 486–497.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. LIBLINEAR: A Library for Large Linear
Classification, Journal of Machine Learning Re-
search 9(2008), 1871-1874
</reference>
<page confidence="0.998997">
394
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563311">
<title confidence="0.889639">KUNLPLab:Sentiment Analysis on Twitter Data Beakal Gizachew</title>
<author confidence="0.814897">Koc Unversity</author>
<email confidence="0.813633">bassefa13@ku.edu.tr</email>
<abstract confidence="0.99531725">This paper presents the system submitted KUNLPLab for SemEval-2014 - Subtask B: Message Polarity on Twitter data. Lexicon features and bag-of-words features are mainly used to represent the datasets. We trained a logistic regression classifier and got an accuracy of 6% increase from the baseline feature representation. The effect of pre-processing on the classifier’s accuracy is also discussed in this work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Benjamin Mandel</author>
<author>Aron Culotta</author>
<author>John Boulahanis</author>
<author>Danielle Stark</author>
<author>Bonnie Lewis</author>
<author>Jeremy Rodrigue</author>
</authors>
<title>A demographic analysis of online sentiment during hurricane irene.</title>
<date>2012</date>
<booktitle>In Proceedngs of the Second Workshop on Language in Social Media, LSM’12,</booktitle>
<pages>27--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Mandel, Culotta, Boulahanis, Stark, Lewis, Rodrigue, 2012</marker>
<rawString>Benjamin Mandel, Aron Culotta, John Boulahanis, Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue. 2012. A demographic analysis of online sentiment during hurricane irene. In Proceedngs of the Second Workshop on Language in Social Media, LSM’12, pages 27–36, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv Das</author>
<author>Mike Chen</author>
</authors>
<title>Yahoo! for amazon: extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th Asia Pacific Finance Association Annual Conference.</booktitle>
<marker>Das, Chen, 2001</marker>
<rawString>Sanjiv Das and Mike Chen. 2001. Yahoo! for amazon: extracting market sentiment from stock message boards. In Proceedings of the 8th Asia Pacific Finance Association Annual Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Becker</author>
<author>George Erhart</author>
<author>David Skiba</author>
<author>Valentine Matula</author>
</authors>
<date>2013</date>
<booktitle>AVAYA: Sentiment Analysis on Twitter with Self-Training and Polarity Lexicon Expansion. Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="6343" citStr="Becker et al. (2013)" startWordPosition="1011" endWordPosition="1014">feature is how to represent negations. In the texts “I like the movie. “, and “I do not like the movie.”, the sentiment of the words in the two texts is opposite since the two statements are negations of one another. One way of representing the negated word is by appending the tag _NOT (Chen (2001) and Pang et al. (2002). The _NOT tag suffixes all words between the negation word and the first punctuation mark after the negation word. In the above example the second text is transformed to “ I do like_IOT the_IOT movie _IOT”. In representation of the negations, we employ the above approach. Lee Becker et al. (2013) directly integrated the polarized word representation in their system. One disadvantage of this representation is the number of features doubles in worst case. Sentiment lexicons are words, which have association with positive or negative sentiments. Unlike the Bag-Of-Words, instead of taking the raw word as a feature, every word has a score, which is a measure of how much positive or negative sentiment the lexicon has. In this work we use the NRC Hashtag Sentiment Lexicon, and Sentiment140 Lexicon (Mohammad 2013). Both list of lexicons are used in the SemEval 2013 by NRC-Canada team. The IRC</context>
</contexts>
<marker>Becker, Erhart, Skiba, Matula, 2013</marker>
<rawString>Lee Becker, George Erhart, David Skiba and Valentine Matula. 2013. AVAYA: Sentiment Analysis on Twitter with Self-Training and Polarity Lexicon Expansion. Seventh International Workshop on Semantic Evaluation (SemEval 2013)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>Emotional Tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>246--255</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7232" citStr="Mohammad 2012" startWordPosition="1163" endWordPosition="1164">, instead of taking the raw word as a feature, every word has a score, which is a measure of how much positive or negative sentiment the lexicon has. In this work we use the NRC Hashtag Sentiment Lexicon, and Sentiment140 Lexicon (Mohammad 2013). Both list of lexicons are used in the SemEval 2013 by NRC-Canada team. The IRCHashtag Sentiment Lexicon is based on the common practice that users use the # symbol to emphasis on a topic or a word. The hashtag lexicon was created from a collection of tweets that had a positive or a negative word hashtag such as #good, #excellent, #bad, and #terrible (Mohammad 2012). It was created from 775,310 tweets posted between April and December 2012 using a list of 78 positive and negative word hashtags. They have provided unigram, bigram, and trigram datasest. In this work however, we used the unigram features which contains 54,129 terms. The Sentiment140 is also a list of words with associations to positive an negative sentiments. It has the same format as the NRC Hashtag Sentiment Lexicon. However, it was created from the sentiment140 corpus of 1.6 million tweets, and emoticons were used as positive and negative labels (instead of hashtagged words). In order to</context>
</contexts>
<marker>Mohammad, 2012</marker>
<rawString>Saif Mohammad. 2012. Emotional Tweets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pages 246–255, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification Using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="3266" citStr="Pang et al., 2002" startWordPosition="506" endWordPosition="509">fier by testing on the development set. The remaining of the document is organized as follows: Section 2 presents a brief literature review on sentiment analysis on Twitter data. Section 3 discusses the system developed to solve the above task, characteristics of the dataset, prepressing on the dataset, and various feature representation. Section 4 illustrates the evaluation results. Section 5 presents conclusion and remarks. 2 Related Work Sentiment analysis has been studied in Natural Language Processing. Different approaches have been implemented to automatically detect sentiment on texts (Pang et al., 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Glance et al., 2005; Wilson et al., 2005). There is also an active research on Sentiment analysis on Twitter data. (Go et al., 2009, Bermingham and Smeaton, 2010, and Pak and 391 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391–394, Dublin, Ireland, August 23-24, 2014. Paroubek 2010) consider tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and built a classifier using unigrams and bigrams as features. Barbosa and Feng (2010) class</context>
<context position="6045" citStr="Pang et al. (2002)" startWordPosition="959" endWordPosition="962">eature reduction. 3.3 Features There are two main categories of features used in the development of this system. Bag-of-Words and sentiment lexicon features. Bag-of-Words features takes a given input text and extracts the raw words as features independent of one another. One issue in using this feature is how to represent negations. In the texts “I like the movie. “, and “I do not like the movie.”, the sentiment of the words in the two texts is opposite since the two statements are negations of one another. One way of representing the negated word is by appending the tag _NOT (Chen (2001) and Pang et al. (2002). The _NOT tag suffixes all words between the negation word and the first punctuation mark after the negation word. In the above example the second text is transformed to “ I do like_IOT the_IOT movie _IOT”. In representation of the negations, we employ the above approach. Lee Becker et al. (2013) directly integrated the polarized word representation in their system. One disadvantage of this representation is the number of features doubles in worst case. Sentiment lexicons are words, which have association with positive or negative sentiments. Unlike the Bag-Of-Words, instead of taking the raw</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification Using Machine Learning Techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2002)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the Stateof-the-Art in Sentiment Analysis of Tweets</title>
<date>2013</date>
<booktitle>Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013).</booktitle>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Mohammad, Saif and Kiritchenko, Svetlana and Zhu, Xiaodan 2013. NRC-Canada: Building the Stateof-the-Art in Sentiment Analysis of Tweets Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Go</author>
<author>R Bhayani</author>
<author>L Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<location>Stanford</location>
<contexts>
<context position="3443" citStr="Go et al., 2009" startWordPosition="538" endWordPosition="541">Section 3 discusses the system developed to solve the above task, characteristics of the dataset, prepressing on the dataset, and various feature representation. Section 4 illustrates the evaluation results. Section 5 presents conclusion and remarks. 2 Related Work Sentiment analysis has been studied in Natural Language Processing. Different approaches have been implemented to automatically detect sentiment on texts (Pang et al., 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Glance et al., 2005; Wilson et al., 2005). There is also an active research on Sentiment analysis on Twitter data. (Go et al., 2009, Bermingham and Smeaton, 2010, and Pak and 391 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391–394, Dublin, Ireland, August 23-24, 2014. Paroubek 2010) consider tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and built a classifier using unigrams and bigrams as features. Barbosa and Feng (2010) classified the subjectivity of tweets based on traditional features with the inclusion of some witter specific clues such as retweets, hashtags, links, uppercase words, emoticons, an</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Go, A., Bhayani, R., Huang, L.: Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford (2009)</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Barbosa</author>
<author>J Feng</author>
</authors>
<title>Robust sentiment detection on Twitter from biased and noisy data. In:</title>
<date>2010</date>
<booktitle>Proceedings of COLING.</booktitle>
<pages>36--44</pages>
<contexts>
<context position="3860" citStr="Barbosa and Feng (2010)" startWordPosition="602" endWordPosition="605">t on texts (Pang et al., 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Glance et al., 2005; Wilson et al., 2005). There is also an active research on Sentiment analysis on Twitter data. (Go et al., 2009, Bermingham and Smeaton, 2010, and Pak and 391 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391–394, Dublin, Ireland, August 23-24, 2014. Paroubek 2010) consider tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and built a classifier using unigrams and bigrams as features. Barbosa and Feng (2010) classified the subjectivity of tweets based on traditional features with the inclusion of some witter specific clues such as retweets, hashtags, links, uppercase words, emoticons, and exclamation and question marks. (Agarwal et al. 2011 ) introduced a POSspecific prior polarity features and used a tree kernel to obviate the need for tedious feature engineering. 3 System Description 3.1 Dataset The organizer of SemEval-2014 have provided training and development sets. Table 1 bellow illustrates the characteristics of the dataset. Positive Negative Neutral Train 3045 1,209 4004 Dev 575 340 739 </context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Barbosa, L., Feng, J.: Robust sentiment detection on Twitter from biased and noisy data. In: Proceedings of COLING. pp. 36–44 (2010)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Agarwal</author>
<author>B Xie</author>
<author>I Vovsha</author>
<author>O Rambow</author>
<author>R Passonneau</author>
</authors>
<title>Sentiment analysis of Twitter data. In:</title>
<date>2011</date>
<booktitle>Proc. ACL 2011 Workshop on Languages in Social Media.</booktitle>
<pages>30--38</pages>
<contexts>
<context position="4097" citStr="Agarwal et al. 2011" startWordPosition="638" endWordPosition="641"> Pak and 391 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391–394, Dublin, Ireland, August 23-24, 2014. Paroubek 2010) consider tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and built a classifier using unigrams and bigrams as features. Barbosa and Feng (2010) classified the subjectivity of tweets based on traditional features with the inclusion of some witter specific clues such as retweets, hashtags, links, uppercase words, emoticons, and exclamation and question marks. (Agarwal et al. 2011 ) introduced a POSspecific prior polarity features and used a tree kernel to obviate the need for tedious feature engineering. 3 System Description 3.1 Dataset The organizer of SemEval-2014 have provided training and development sets. Table 1 bellow illustrates the characteristics of the dataset. Positive Negative Neutral Train 3045 1,209 4004 Dev 575 340 739 Table 1. Dataset characteristics 3.2 Pre-processing We employed two major pre-processing in the datasets. Converting terms to their correct representation, and stemming. Mostly, in Twitter, words are not written in their correct/full for</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Agarwal, A., Xie, B., Vovsha, I., Rambow, O., Passonneau, R.: Sentiment analysis of Twitter data. In: Proc. ACL 2011 Workshop on Languages in Social Media. pp. 30–38 (2011)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Bermingham</author>
<author>Alan Smeaton</author>
</authors>
<title>Classifying sentiment in microblogs: is brevity an advantage is brevity an advantage?</title>
<date>2010</date>
<pages>1833--1836</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="3473" citStr="Bermingham and Smeaton, 2010" startWordPosition="542" endWordPosition="545">es the system developed to solve the above task, characteristics of the dataset, prepressing on the dataset, and various feature representation. Section 4 illustrates the evaluation results. Section 5 presents conclusion and remarks. 2 Related Work Sentiment analysis has been studied in Natural Language Processing. Different approaches have been implemented to automatically detect sentiment on texts (Pang et al., 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Glance et al., 2005; Wilson et al., 2005). There is also an active research on Sentiment analysis on Twitter data. (Go et al., 2009, Bermingham and Smeaton, 2010, and Pak and 391 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391–394, Dublin, Ireland, August 23-24, 2014. Paroubek 2010) consider tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and built a classifier using unigrams and bigrams as features. Barbosa and Feng (2010) classified the subjectivity of tweets based on traditional features with the inclusion of some witter specific clues such as retweets, hashtags, links, uppercase words, emoticons, and exclamation and question mar</context>
</contexts>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Adam Bermingham and Alan Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage is brevity an advantage? ACM, pages 1833– 1836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of LREC.</booktitle>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Glance</author>
<author>M Hurst</author>
<author>K Nigam</author>
<author>M Siegler</author>
<author>R Stock ton</author>
<author>T Tomokiyo</author>
</authors>
<title>Deriving marketing intelligence from online discussion.</title>
<date>2005</date>
<booktitle>In Proceedings of the eleventh ACM SIGKDD,</booktitle>
<pages>419--428</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3331" citStr="Glance et al., 2005" startWordPosition="518" endWordPosition="521">cument is organized as follows: Section 2 presents a brief literature review on sentiment analysis on Twitter data. Section 3 discusses the system developed to solve the above task, characteristics of the dataset, prepressing on the dataset, and various feature representation. Section 4 illustrates the evaluation results. Section 5 presents conclusion and remarks. 2 Related Work Sentiment analysis has been studied in Natural Language Processing. Different approaches have been implemented to automatically detect sentiment on texts (Pang et al., 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Glance et al., 2005; Wilson et al., 2005). There is also an active research on Sentiment analysis on Twitter data. (Go et al., 2009, Bermingham and Smeaton, 2010, and Pak and 391 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391–394, Dublin, Ireland, August 23-24, 2014. Paroubek 2010) consider tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and built a classifier using unigrams and bigrams as features. Barbosa and Feng (2010) classified the subjectivity of tweets based on traditional features wi</context>
</contexts>
<marker>Glance, Hurst, Nigam, Siegler, ton, Tomokiyo, 2005</marker>
<rawString>Glance, N., M. Hurst, K. Nigam, M. Siegler, R. Stock ton, and T. Tomokiyo. 2005. Deriving marketing intelligence from online discussion. In Proceedings of the eleventh ACM SIGKDD, pages 419–428. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>E Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>486--497</pages>
<contexts>
<context position="3310" citStr="Wiebe and Riloff, 2005" startWordPosition="514" endWordPosition="517"> The remaining of the document is organized as follows: Section 2 presents a brief literature review on sentiment analysis on Twitter data. Section 3 discusses the system developed to solve the above task, characteristics of the dataset, prepressing on the dataset, and various feature representation. Section 4 illustrates the evaluation results. Section 5 presents conclusion and remarks. 2 Related Work Sentiment analysis has been studied in Natural Language Processing. Different approaches have been implemented to automatically detect sentiment on texts (Pang et al., 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Glance et al., 2005; Wilson et al., 2005). There is also an active research on Sentiment analysis on Twitter data. (Go et al., 2009, Bermingham and Smeaton, 2010, and Pak and 391 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391–394, Dublin, Ireland, August 23-24, 2014. Paroubek 2010) consider tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and built a classifier using unigrams and bigrams as features. Barbosa and Feng (2010) classified the subjectivity of tweets based on tr</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Wiebe, J. and E. Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. Computational Linguistics and Intelligent Text Processing, pages 486–497.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification,</title>
<journal>Journal of Machine Learning Research</journal>
<volume>9</volume>
<issue>2008</issue>
<pages>1871--1874</pages>
<marker>Fan, Chang, Hsieh, Wang, Lin, </marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A Library for Large Linear Classification, Journal of Machine Learning Research 9(2008), 1871-1874</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>