<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000515">
<title confidence="0.991207">
Mining Script-Like Structures from the Web
</title>
<author confidence="0.996958">
Niels Kasch
</author>
<affiliation confidence="0.984370333333333">
Department of Computer Science
and Electrical Engineering
University of Maryland,
</affiliation>
<address confidence="0.861399">
Baltimore County
Baltimore, MD 21250, USA
</address>
<email confidence="0.998899">
nkasch1@umbc.edu
</email>
<author confidence="0.994491">
Tim Oates
</author>
<affiliation confidence="0.984354">
Department of Computer Science
and Electrical Engineering
University of Maryland,
</affiliation>
<address confidence="0.861474">
Baltimore County
Baltimore, MD 21250, USA
</address>
<email confidence="0.999603">
oates@cs.umbc.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999500416666667">
This paper presents preliminary work to ex-
tract script-like structures, called events and
event sets, from collections of web docu-
ments. Our approach, contrary to existing
methods, is topic-driven in the sense that event
sets are extracted for a specified topic. We
introduce an iterative system architecture and
present methods to reduce noise problems
with web corpora. Preliminary results show
that LSA-based event relatedness yields bet-
ter event sets from web corpora than previous
methods.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998560425925926">
In this paper, we present a preliminary system to ex-
tract script-like structures in a goal-directed fashion
from the web. For language processing purposes,
humans appear to have knowledge of many stylized
situations, such as what typically happens when go-
ing to a restaurant or riding a bus. This knowledge
is shared among a large part of the population and
lets us predict the next step in a sequence in a fa-
miliar situation, allows us to act appropriately, and
enables us to omit details when communicating with
others while ensuring common ground is maintained
between communication partners. It seems we have
such knowledge for a vast variety of situations and
scenarios, and thus natural language processing sys-
tems need access to equivalent information if they
are to understand, converse, or reason about these
situations.
These knowledge structures, comparable to
scripts (Schank and Abelson, 1977) or narrative
34
chains (Chambers and Jurafsky, 2008), describe typ-
ical sequences of events in a particular context.
Given the number of potential scripts, their develop-
ment by hand becomes a resource intensive process.
In the past, some work has been devoted to automat-
ically construct script-like structures from compiled
corpora (Fujiki et al., 2003) (Chambers and Juraf-
sky, 2008). Such approaches, however, only produce
scripts that are directly related to the topics repre-
sented in such corpora. Therefore, newspaper cor-
pora (e.g., the Reuters Corpus) are likely to contain
scripts relating to government, crime and financials,
but neglect other subject areas. We present a system
that extracts scripts from the web and removes the
constraints of specialized corpora and domain lim-
itations. We hope our iterative technique will pro-
duce scripts for a vast variety of topics, and has the
potential to produce more complete scripts.
Another drawback of existing approaches lies
with their passive extraction mechanisms. A
user/system does not have the ability to obtain
scripts for a specific topic, but rather is bound to
obtain the most prevalent scripts for the underlying
corpus. Furthermore, scripts derived in this fashion
lack an absolute labeling or description of their top-
ics. This can be problematic when a user/system is
looking for specific scripts to apply to a given sce-
nario. In contrast, our system facilitates the search
for scripts given a topic. This goal oriented approach
is superior in that (1) scripts are labeled by a de-
scriptive topic and can be organized, accessed and
searched by topic, (2) scripts can be constructed by
topic and are not reliant on existing and potentially
limiting corpora and (3) script coarseness and de-
</bodyText>
<note confidence="0.906534">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 34–42,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.993272604166667">
tail can be be controlled through iterative script im- event pairs. A global narrative score, aiming to max-
provement and augmentation based on additional in- imize the PMI of a set of events is utilized to gener-
formation retrieved from the web. ate a ranked list of events most likely to participate
2 Related Work in the narrative chain. Temporal order is established
Lin and Pantel describe an unsupervised algorithm by labeling events with temporal attributes and us-
for discovering inference rules from text (DIRT) ing those labels, along with other linguistic features,
(Lin and Pantel, 2001a) (Lin and Pantel, 2001b). In- to classify the relationship (before or other) between
ference rules are derived from paths in dependency two events.
trees. If two paths occur in similar contexts (i.e., the For the purposes of our work, finding documents
words/fillers of their slots are distributionally simi- related to a term and identifying similar terms is an
lar) then the meaning of the paths is similar. Ver- important step in the script creation process. (Deer-
bOcean (Chklovski and Pantel, 2004) is a resource wester et al., 1990) describe Latent Semantic Anal-
of strongly associated verb pairs and their semantic ysis/Indexing (LSA) as a technique superior to term
relationship. Verbs are considered strongly associ- matching document retrieval. LSA aims to facili-
ated if DIRT deems dependency paths, which con- tate document retrieval based on the conceptual con-
tain the verbs, as being similar. A form of mutual tent of documents, thereby avoiding problems with
information between verb pairs and lexico-syntactic synonomy and polysemy of individual search terms
patterns indicative of semantic relationship types is (or in documents). LSA employs singular-value-
used to categorize the verb pairs according to sim- decomposition (SVD) of a term-by-document ma-
ilarity, strength, antonymy, happens-before and en- trix to construct a “semantic space” in which related
ablement. documents and terms are clustered together.
(Fujiki et al., 2003) describe a method to ex- 3 Approach
tract script knowledge from the first paragraph of In this work we aim to extract scripts from the
Japanese newspaper articles. The first paragraph of web. We define a script as a collection of typi-
such articles is assumed to narrate its contents in cally related events that participate in temporal re-
temporal order. This circumvents the need to order lationships amongst each other. For example, e1
events as they can be extracted in presumed order. happens-before e2 denotes a relationship such
Events are defined in terms of actions, where an ac- that event e1 occurs prior to event e2. An event is de-
tion consists of a tuple composed of a transitive verb fined as a tuple consisting of a verb, a grammatical
and its subject and object. The method’s goal is to function and a set of arguments (i.e., words) which
find sequences of pairs of actions by (1) using co- act out the grammatical function in relation to the
occurrence of subjects and objects in neighboring verb. Figure 1 shows the structure of events.
sentences, (2) locating sentences where two verbs e [verb, grammatical function, {set of arguments}]
share the same subject and (3) identifying sentences Figure 1: The structure of an event. An event is a tuple
where two verbs share the same object. Once pairs consisting of a verb, a grammatical function and a set of
of events are extracted, their subject and objects are arguments (i.e., instances of words filling the grammati-
generalized into semantic entities similar to seman- cal function).
tic roles. The set of arguments represents actual instances
(Chambers and Jurafsky, 2008) attempt to identify found during the script extraction process. Figure 2
narrative chains in newspaper corpora. They utilize illustrates an incomplete script for the topic eating
the notion of protagonist overlap or verbs sharing at a restaurant.
co-referring arguments to establish semantic coher- 3.1 The Task
ence in a story. Co-referring arguments are taken We define the task of goal driven script extraction as:
as indicators of a common discourse structure. This (1) given a topic, compile a “relevant” corpus of doc-
assumption is used to find pairwise events in an un-
supervised fashion. Point wise mutual information
(PMI) is used to indicate the relatedness between
35
</bodyText>
<equation confidence="0.99868425">
e1 [ enter, nsubj, {customer, John}) ]
e2 [ enter, dobj, {restaurant} ]
e3 [ order, nsubj, {customer} ]
e4 [ order, dobj, {food} ]
e5 [ eat, nsubj, {customer} ]
e6 [ eat, dobj, {food} ]
e7 [ pay, nsubj, {customer} ]
e8 [ pay, dobj, {bill} ]
e9 [ leave, nsubj, {customer} ]
e10 [ leave, dobj, {restaurant} ]
Temporal Ordering = e1 &lt; e2 &lt; e3 &lt; e4
e4 &lt; e5 &lt; e6 &lt; e7 &lt; e8 &lt; e9 &lt; e10
</equation>
<figureCaption confidence="0.987912">
Figure 2: An excerpt of a script for the topic eating at
a restaurant. The script denotes the stylized actions of a
customer dining at a restaurant. The &lt; relation denotes
event ei happens before event ep
</figureCaption>
<bodyText confidence="0.999972363636363">
uments from a subset of documents on the web, (2)
extract events relevant for the topic, (3) (optional)
refine the topic and restart at 1, and (4) establish a
temporal ordering for the events.
We currently impose restrictions on the form of
acceptable topics. For our purposes, a topic is a short
description of a script, and contains at least a verb
and a noun from the script’s intended domain. For
example, the topic for a passenger’s typical actions
while using public transportation (i.e. a bus) can be
described by the topic riding on a bus.
</bodyText>
<subsectionHeader confidence="0.999877">
3.2 System Architecture
</subsectionHeader>
<bodyText confidence="0.9999285">
The script extraction system consists of a variety of
modules where each module is responsible for a cer-
tain task. Modules are combined in a mixed fash-
ion such that sequential processing is combined with
an iterative improvement procedure. Figure 3 illus-
trates the system architecture and flow of informa-
tion between modules. The following sections de-
scribe each module in detail.
</bodyText>
<subsectionHeader confidence="0.973483">
3.2.1 Document Retrieval
</subsectionHeader>
<bodyText confidence="0.999675285714286">
Our system utilizes the web as its underlying in-
formation source to circumvent domain limitations
of fixed corpora. However, using the entire web to
extract a script for a specific topic is, on one hand,
infeasible due to the size of the web and, on the
other hand, impractical in term of document rele-
vancy. Since only a subset of pages is potentially
</bodyText>
<figureCaption confidence="0.997921">
Figure 3: System Architecture and flow of information.
</figureCaption>
<bodyText confidence="0.9999785">
relevant to a given topic, the web needs to be filtered
such that mostly relevant web pages are retrieved.
The document retrieval module makes use of exist-
ing search engines for this purpose.1
The document retrieval module is presented with
the topic for a script and issues this topic as a query
to the search engines. The search engines produce a
relevancy ranked list of documents/URLs (Brin and
Page, 1998) which, in turn, are downloaded. The
number of downloaded pages depends on the cur-
rent iteration number of the system (i.e., how often
the retrieval-analysis cycle has been executed for a
given topic2).
The document retrieval module is also responsi-
ble for cleaning the documents. The cleaning pro-
cess aims to remove “boilerplate” elements such as
navigational menus and advertising from web pages
while preserving content elements.3 The collection
of cleaned documents for a given topic is considered
to be a topic-specific corpus.
</bodyText>
<subsectionHeader confidence="0.924552">
3.2.2 Latent Semantic Analysis (LSA)
</subsectionHeader>
<bodyText confidence="0.999878">
The aim of the LSA module is to identify words
(verbs, nouns and adjectives) that are closely related
</bodyText>
<footnote confidence="0.984698">
1The Google and Yahoo API’s are used to establish commu-
nication with these search engines.
2At the first iteration, we have arbitrarily choosesn to re-
trieve the first 1000 unduplicated documents.
3The Special Interest Group of the ACL on Web as Cor-
pus (SIGWAC) is interested in web cleaning methods for corpus
construction. Our web page cleaner uses a support vector ma-
chine to classify blocks of a web page as content or non-content.
The cleaner achieves Pz� 85% F1 on a random set of web pages.
</footnote>
<page confidence="0.998005">
36
</page>
<bodyText confidence="0.989839808510638">
to the topic presented to the document retrieval mod-
ule. To find such words, the topic-specific corpus is
(1) part-of-speech tagged and (2) transformed into
a term-document matrix. Each cell represents the
log-entropy for its respective term in a document.
Note that we consider a term to be a tuple consist-
ing of a word and its POS. The advantage of us-
ing word-POS tag combinations over words only is
the ability to query LSA’s concept space for words
by their word class. A concept space is created by
applying SVD to the term-document matrix, reduc-
ing the dimensionality of the scaling matrix and re-
constructing the term-document matrix using the re-
duced scaling matrix.
Once the concept space is constructed, the space
is searched for all terms having a high correlation
with the original topic. Terms from the original topic
are located in the concept space (i.e., term vectors
are located) and other term vectors with high co-
sine similarity are retrieved from the space. A list of
n = 50 terms4 for each word class E {verb, noun,
adjective} is obtained and filtered using a stop list.
The stop list currently contains the 100 most com-
mon words in the English language. The idea behind
the stop list is to remove low content words from the
list. The resulting set of words is deemed to have
high information content with respect to the topic.
This set is used for two purposes: (1) to augment
the original topic and to restart the document col-
lection process using the augmented topic and (2)
identify event pairs constructed by the event finding
module which contain these highly correlated terms
(either as events or event arguments). The first pur-
pose aims to use an iterative process to construct a
higher quality topic-specific corpus. A new corpus
created in this fashion presumably represents docu-
ments that are richer and more relevant to the aug-
mented topic. The second purpose steers the extrac-
tion of events towards events containing those con-
stituents judged most relevant. This fact can be in-
corporated into a maximization calculation based on
pointwise mutual information to find highly corre-
lated events.
4The number was chosen experimentally and is based on
the correlation score (cosine similarity) between word vectors.
After about 50 words, the correlation score begins to drop sig-
nificantly indicating weaker relatedness.
</bodyText>
<subsectionHeader confidence="0.680475">
3.2.3 Event Finding
</subsectionHeader>
<bodyText confidence="0.999309352941176">
The collection of documents (or topic-specific
corpus) is then processed to facilitate finding event
pairs. Finding event pairs involves the notion of
verb argument overlap using the assumption that two
events in a story are related if they share at least
one semantic argument across grammatical func-
tions. This virtue of discourse structure of coherent
stories has been described in (Trabasso et al., 1984)
and applied by (Fujiki et al., 2003) as subject and ob-
ject overlap and by (Chambers and Jurafsky, 2008)
as following a common protagonist in a story. For
example, in the sentences “John ordered a drink. He
enjoyed it very much.” we can establish that events
order and enjoy are part of a common theme because
the arguments (or loosely semantic roles) of order
and enjoy refer to the same entities, that is John =
He and a drink = it.
</bodyText>
<figureCaption confidence="0.99516575">
Figure 4: Example processing of the sentences “Yester-
day, Joe ordered coffee. It was so hot, he couldn’t drink
it right away”. The output after dependency parsing, ref-
erence resolution and event finding is a set of event pairs.
</figureCaption>
<page confidence="0.996268">
37
</page>
<bodyText confidence="0.9998641">
To identify such pairs, the topic specific corpus
is (1) co-reference resolved5 and (2) dependency
parsed6. Sentences containing elements referring
to the same mention of an element are inspected
for verb argument overlap. Figure 4 illustrates this
procedure for the sentences “Yesterday, Joe ordered
coffee. It was so hot, he couldn’t drink it right
away”.
Co-reference resolution tells us that mention he
refers to Joe and mention(s) it refer to coffee. By
our previous assumption of discourse coherence, it
is possible to deduce that events was and drink are
associated with event order. In a similar fashion,
event drink is associated with event was. This is
due to the fact that all events share at least one ar-
gument (in the case of events order and drink two
arguments are shared). For each pair of events shar-
ing arguments in a particular grammatic function, an
event pair is generated indicating where the overlap
occurred.
</bodyText>
<subsectionHeader confidence="0.934906">
3.2.4 Constructing Event Sets
</subsectionHeader>
<bodyText confidence="0.999968833333333">
Sets of events representing script-like structures
are constructed through the use of pointwise mutual
information in combination with the lists of related
words found by Latent Semantic Analysis. We uti-
lize the definition of PMI described in (Chambers
and Jurafsky, 2008). For two events e1 and e2
</bodyText>
<equation confidence="0.999516666666667">
P(e1, e2)
pmi(e1, e2) = log (1)
P(e1)P(e2)
</equation>
<bodyText confidence="0.674532">
where
</bodyText>
<equation confidence="0.996140666666667">
C(e1, e2)
P(e1, e2) = � (2)
i�j C(ei, ej)
</equation>
<bodyText confidence="0.997638111111111">
and C(e1, e2) is the number of times events e1 and
e2 had coreferring arguments.
We extend the definition of PMI between events to
assign more weight to events whose constituents are
contained in the list of words (verbs, nouns and ad-
jectives) judged by Latent Semantic Analysis to be
most relevant to the topic. For notational purposes,
these lists are denoted L. Thus, we can calculate the
weighted LSA PMI LP(e1, e2) as follows:
</bodyText>
<equation confidence="0.951267">
LP(e1, e2) = P(e1, e2) + L5A(e1, e2) (3)
</equation>
<footnote confidence="0.9716035">
5OpenNLP’s Co-Reference Engine is utilized http://
opennlp.sourceforge.net/.
6The Stanford Parser is utilized http://nlp.
stanford.edu/software/lex-parser.shtml
</footnote>
<bodyText confidence="0.948635">
where
</bodyText>
<equation confidence="0.999688428571429">
L5A(e1, e2) = α * (E(e1) + E(e2)) (4)
α = J2 if e1verb E L and e2verb E L 5
1 otherwise ( )
E(e) = (||everb n L ||+ 1)
* ||eArgsnL ||(6)
( +
IleArg.1I 1)
</equation>
<bodyText confidence="0.9592845">
To construct the set of n events related to the
topic, the LP scores are first calculated for each
event pair in the corpus. The set can then be con-
structed by maximizing:
</bodyText>
<equation confidence="0.925355">
LP(ei, ek) (7)
</equation>
<bodyText confidence="0.999915142857143">
Therefore, events that share a larger number of
constituents with the LSA relevancy list are pre-
ferred for inclusion in the event set. This prac-
tice distributes the relatedness weight among the fre-
quency of events and LSA. The noisy nature of our
proposed corpus generation method makes such a
technique essential as we will see in section 4.3.
</bodyText>
<subsectionHeader confidence="0.939862">
3.2.5 Ordering Events
</subsectionHeader>
<bodyText confidence="0.999985857142857">
At this time, we only establish a naive temporal
ordering on the events. The ordering process simply
assumes that an event appearing in the corpus prior
to another event also occurs earlier in time. We re-
alize that this assumption does not always hold, but
delay a more sophisticated ordering process as fu-
ture work.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="introduction">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999847888888889">
This section describes experimental results, obsta-
cles we have encountered, various approaches to
overcome these obstacles and lessons learned from
our work. Unless mentioned otherwise, the results
pertain to the topic eating at a restaurant. This topic
has been chosen for our investigation since previ-
ous work (Schank and Abelson, 1977) establishes a
comprehensive reference as to what a script for this
domain may entail.
</bodyText>
<subsectionHeader confidence="0.986155">
4.1 Domain Richness
</subsectionHeader>
<bodyText confidence="0.999927">
The first step in our work was to confirm the no-
tion that the web can be used as the underlying in-
formation source for topic-specific script extraction.
</bodyText>
<equation confidence="0.64997525">
k−1�
i=0
max
i&lt;k&lt;n
</equation>
<page confidence="0.990612">
38
</page>
<bodyText confidence="0.935729328571428">
The overall goal was to investigate whether a topic-
specific corpus contains sufficiently useful informa-
tion which is conducive to the script extraction task.
Latent Semantic Analysis was performed on the
Part-of-Speech tagged topic specific corpus. The
semantic space was queried using the main con-
stituents of the original topic. Hence, this resulted in
two queries, namely eating and restaurant. For each
query, we identified the most related verbs, nouns
and adjectives/adverbs and placed them in respec-
tive lists. Lists are then combined according to word
class, lemmatized, and pruned. Auxiliary verbs such
as be, do and have consistently rank in the top 10
most similar words in the un-pruned lists. This re-
sult is expected due to the frequency distribution
of auxiliaries in the English language. It is a nat-
ural conclusion to exclude auxiliaries from further
consideration since their information content is rela-
tively low. Furthermore, we extend this notion to ex-
clude the 100 most frequent words in English from
these lists using the same justification. By the in-
verse reasoning, it is desirable to include words in
further processing that occur infrequently in natural
language. We can hypothesize that such words are
significant to a given script because their frequency
appears to be elevated in the corpus. Table 1 (left)
shows the resulting word class lists for both queries.
Duplicates (i.e., words with identical lemma) have
been removed.
The table reveals that some words also appear
in the restaurant script as suggested by (Schank
and Abelson, 1977). In particular, bold verbs re-
semble Schank’s scenes and bold nouns resemble
his props. We can also see that the list of ad-
verbs/adjectives appear to not contribute any signif-
icant information. Note that any bold words have
been hand selected using a human selector’s subjec-
tive experience about the eating at a restaurant do-
main. Furthermore, while some script information
appears to be encoded in these lists, there is a signif-
icant amount of noise, i.e., normal font words that
are seemingly unimportant to the script at hand.
For our purposes, we aim to model this noise so
that it can be reduced or removed to some degree.
Such a model is based on the notion of overlap of
noisy terms in the LSA lists derived from indepen-
dent topic related corpora for the main constituents
of the original topic. For example, for the topic eat-
eating at a restaurant Overlap removed
Verbs Nouns A&amp;A Verbs Nouns A&amp;A
keep home own order home fry
need place still set hand amaze
help table last expect bowl green
dine lot open share plate grill
love part full drink cook chain
order hand off try fish diet
feel reason long cut soup clean
avoid course fat decide service smart
add side right watch break total
let number down process drink relate
stay experience busy save cheese worst
include water fast offer rice black
tend point single provide serve fit
set dish low hear chance light
tell bowl free fill portion exist
found plate white forget body empty
bring bite wrong write party live
locate cook ready follow rest
eat fish true travel cream
leave soup close taste
</bodyText>
<tableCaption confidence="0.9947832">
Table 1: (Left) 20 most relevant terms (after pruning)
for LSA queries eating and restaurant on the eating at
a restaurant corpus. (Right) Terms remaining after noise
modeling and overlap removal. Bold terms in the table
were manually judged by a human to be relevant.
</tableCaption>
<bodyText confidence="0.99278447826087">
ing at a restaurant, we obtain two additional corpora
using the method described in Section 3.2.1, i.e., one
corpus for constituent eating and another for the sec-
ond main constituent of the original topic, restau-
rant. Both corpora are subjected to LSA analysis
from which two (one for each corpus) LSA word
lists are obtained. Each list was created using the
respective corpus query as the LSA query. The as-
sumption is made that words which are shared (pair-
wise) between all three lists (i.e., the two new LSA
lists and the LSA list for topic eating at a restaurant)
are noisy due to the fact that they occur independent
of the original topic.
Table 1 (right) illustrates the LSA list for topic
eating at a restaurant after removing overlapping
terms with the two other LSA lists. Bold words were
judged by a human selector to be relevant to the in-
tended script. From the table we can observe that:
1. A significant amount of words have been re-
moved. The original table contains 50 words
for each word class. The overlap reduced table
contains only 19 verbs, 29 nouns, and 17 adjec-
tives, a reduction of what we consider noise by
</bodyText>
<page confidence="0.997814">
39
</page>
<bodyText confidence="0.594617">
�57%
</bodyText>
<listItem confidence="0.989585375">
2. More words (bold) were judged to be related to
the script (e.g., 6 vs. 5 relevant verbs, 12 vs. 9
nouns, and 3 vs. 0 adjectives/adverbs)
3. More relevant words appear in the top part of
the list (words in the list are ordered by rele-
vancy)
4. Some words judged to be relevant were re-
moved (e.g., dine, bring, eat).
</listItem>
<bodyText confidence="0.93258425">
Using the information from the table (left and
right) and personal knowledge about eating at a
restaurant, a human could re-arrange the verbs and
nouns into a partial script-like format of the form7:
</bodyText>
<figure confidence="0.951367066666667">
e1 [ offer, nsubj, {waiter}) ]
e2 [ offer, dobj, {drink}) ]
Example: waiter offers a drink
e2 [ order, nsubj, {customer} ]
e3 [ order, dobj, {fish, soup, rice} ]
Example: customer orders fish
e4 [ serve/bring, nsubj, {waiter} ]
e5 [ serve/bring, nsubj, {bowl, plate} ]
Example: waiter serves/bings the bowl, plate
es [ eat, nsubj, {customer} ]
e7 [ eat, dobj, {portion, cheese} ]
Example: customer eats the portion, cheese
e8 [ leave, nsubj, {customer} ]
e9 [ leave, dobj, {table} ]
Example: customer leaves the table
</figure>
<bodyText confidence="0.998940875">
Note that this script-like information was not ob-
tained by direct derivation from the information in
the table. It is merely an illustration that some script
information is revealed by LSA. Table 1 neither im-
plies any ordering nor suggest semantic arguments
for a verb. However, the analysis confirms that the
web contains information that can be used in the
script extraction process.
</bodyText>
<subsectionHeader confidence="0.999682">
4.2 Processing Errors
</subsectionHeader>
<bodyText confidence="0.999634">
As mentioned in section 3.2.3, events with co-
referring arguments are extracted in a pairwise fash-
ion. In the following section we describe observa-
tions about the characteristics of events extracted
</bodyText>
<footnote confidence="0.746744">
7Bold terms do not appear in the LSA lists, but were added
for readability.
</footnote>
<bodyText confidence="0.99966580952381">
this way. However, we note that each step in our
system architecture is imperfect, meaning that er-
rors are introduced in each module as the result of
processing. We have already seen such errors in
the form of words with incorrect word class in the
LSA lists as the result of incorrect POS tagging.
Such errors are amplified through imprecise pars-
ing (syntactic and dependency parsing). Other er-
rors, such as omissions, false positives and incor-
rect class detection, are introduced by the named en-
tity recognizer and the co-reference module. With
this in mind, it comes as no surprise that some ex-
tracted events, as seen later, are malformed. For
example, human analysis reveals that the verb slot
of these events are sometimes “littered” with words
from other word classes, or that the arguments of a
verb were incorrectly detected. A majority of these
errors can be attributed to ungrammatical sentences
and phrases in the topic-specific corpus, the remain-
der is due to the current state of the art of the parsers
and reference resolution engine.
</bodyText>
<subsectionHeader confidence="0.999271">
4.3 Observations about events
</subsectionHeader>
<bodyText confidence="0.99995044">
To compare relations between events, we looked at
three different metrics. The first metric M1 simply
observes the frequency counts of pairwise events in
the corpus. The second metric M2 utilizes point
wise mutual information as defined in (Chambers
and Jurafsky, 2008). The third metric M3 is our
LSA based PMI calculation as defined in section
3.2.4.
M1 reveals that uninformative event pairs tend
to have a high number of occurrences. These pairs
are composed of low content, frequently occurring
events. For example, event pair [e [ have, nsubj,{} ],
e [ say, nsubj, {} ]] occurred 123 times in our topic-
specific corpus. More sophisticated metrics, such as
M2, consider the frequency distributions of individ-
ual events and allocate more weight to co-occurring
events with lower frequency counts of their individ-
ual events.
In this fashion, M2 is capable of identifying
strongly related events. For example, Table 2 lists
the five pairwise events with highest PMI for our
topic-specific corpus.
From Table 2, it is apparent that these pairs partic-
ipate in mostly meaningful (in terms of human com-
prehensibility) relationships. For example, it does
</bodyText>
<page confidence="0.990034">
40
</page>
<table confidence="0.981851">
Event Pairs
e[sack, dobj, {the, employees}] e[reassign, dobj, {them}]
e[identify, nsubj, {we, Willett}] e[assert, nsubj, {Willett}]
e[pour, dobj, {a sweet sauce}] e[slide, dobj, {the eggs}]
e[walk, nsubj, {you, his sister}] e[fell, nsubj, {Daniel}]
e[use, nsubj, {the menu}] e[access, dobj, {you}]
</table>
<tableCaption confidence="0.9989805">
Table 2: Pairwise events with highest PMI according to
Equation 1.
</tableCaption>
<bodyText confidence="0.909809">
not require a leap of faith to connect that sacking
employees is related to reassigning them in the con-
text of a corporate environment.
</bodyText>
<figureCaption confidence="0.923405416666667">
e(eat, nsubj), e(gobble, nsubj), e(give, nsubj),
e(live, nsubj), e(know, nsubj), e(go, nsubj),
e(need, nsubj), e(buy, nsubj), e(have, nsubj),
e(make, nsubj), e(say, nsubj), e(work, nsubj),
e(try, nsubj), e(like, nsubj), e(tell, dobj),
e(begin, nsubj), e(think, nsubj), e(tailor, nsubj),
e(take, nsubj), e(open, nsubj),e(be, nsubj)
Figure 5: An event set obtained through metric M2 (us-
ing the PMI between events). Temporal ordering is not
implied. Event arguments are omitted. Bold events indi-
cate subjectively judged strong relatedness to the eating
at a restaurant topic.
</figureCaption>
<bodyText confidence="0.98924325">
Figure 5 shows a set of events for our topic. The
set was created by greedily adding event en such
that for all events e1, e2, ...en_1 already in the set
�n�1
i pmi(ei, en) is largest (see (Chambers and Ju-
rafsky, 2008)). The topic constituent eating (i.e.,
eat) was used as the initial event in the set. If this
set is intended to approximate a script for the eating
at a restaurant domain, then it is easy to see that vir-
tually no information from Schank’s restaurant ref-
erence script is represented. Furthermore, by human
standards, the presented set appears to be incoherent.
From this observation, we can conclude that M2 is
unsuitable for topic-specific script extraction.
Figure 6 illustrates an event set constructed using
metric M3. Note that the sets in Figures 5 and 6
do not imply any ordering on the events. The bold
events indicate events that appear in our reference
script or were judged by a human evaluator to be
logically coherent with the eating at a restaurant do-
main. The evaluation was conducted using the eval-
uators personal experience of the domain. In the fu-
ture, we intend to formalize this evaluation process.
Figure 6 signifies an improvement of the results of
</bodyText>
<reference confidence="0.58087">
e(eat, nsubj), e(wait, dobj), e(total, nsubj)
e(write, dobj), e(place, dobj), e(complete, dobj)
e(exist, nsubj), e(include, dobj), e(top, nsubj)
e(found, dobj), e(keep, dobj), e(open, dobj)
e(offer, dobj), e(average, nsubj), e(fill, dobj)
e(taste, nsubj), e(drink, dobj), e(cook, dobj)
e(read, dobj), e(enjoy, dobj),e(buy, dobj)
</reference>
<figureCaption confidence="0.8216492">
Figure 6: An event set obtained through metric M3
(weighing PMI and LSA between events). Temporal or-
dering is not implied. Event arguments are omitted. Bold
events indicate subjectively judged strong relatedness to
the eating at a restaurant topic.
</figureCaption>
<bodyText confidence="0.99386675">
Figure 5 in terms of the number of events judged to
belong to the restaurant script. This leads us to the
conclusion that a metric based on scaled PMI and
LSA appears more suitable for the web based topic
driven script extraction task. Once a temporal order
is imposed on the events in Figure 6, these events
could, by themselves, serve as a partial event set for
their domain.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="discussions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99992052">
We have presented preliminary work on extracting
script-like information in a topic driven fashion from
the web. Our work shows promise to identify script
knowledge in a topic-specific corpus derived from
an unordered collection of web pages. We have
shown that while web documents contain signifi-
cant amounts of noise (both boilerplate elements and
topic unrelated content), a subset of content can be
identified as script-like knowledge.
Latent Semantic Analysis allows for the filter-
ing and pruning of lists of related words by word
classes. LSA furthermore facilitates noise removal
through overlap detection of word class list elements
between independent corpora of topic constituents.
Our method of weighted LSA and PMI for event re-
latedness produces more promising partial event sets
than existing metrics.
For future work, we leave the automated evalua-
tion of partial sets and the establishing of temporal
relations between events in a set. Our system archi-
tecture features an iterative model to event set im-
provement. We hope that this approach will allow
us to improve upon the quality of event sets by us-
ing extracted sets from one iteration to bootstrap a
new iteration of event extraction.
</bodyText>
<page confidence="0.999217">
41
</page>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999346238095238">
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Comput.
Netw. ISDNSyst., 30(1-7):107–117.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, pages 789–797, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Proceedings of Conference on Empirical
Methods in Natural Language Processing, pages 33–
40, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Scott Deerwester, T. Susan Dumais, W. George Furnas,
K. Thomas Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41:391–
407.
Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Automatic acquisition of script knowl-
edge from a text collection. In EACL ’03: Proceedings
of the tenth conference on European chapter of the As-
sociation for Computational Linguistics, pages 91–94,
Morristown, NJ, USA. Association for Computational
Linguistics.
Dekang Lin and Patrick Pantel. 2001a. DIRT - Discov-
ery of Inference Rules from Text. In KDD ’01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 323–328, New York, NY, USA. ACM.
Dekang Lin and Patrick Pantel. 2001b. Discovery of
Inference Rules for Question-Answering. Nat. Lang.
Eng., 7(4):343–360.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding. Lawrence Erlbaum
Associates, Hillsdale, NJ.
Tom Trabasso, T. Secco, and Paul van den Broek. 1984.
Causal Cohesion and Story Coherence. In Heinz
Mandl, N.L. Stein and Tom Trabasso (eds). Learning
and Comprehension of Text., pages 83–111, Hillsdale,
NJ, USA. Lawrence Erlbaum Associates.
</reference>
<page confidence="0.999294">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.243042">
<title confidence="0.999666">Mining Script-Like Structures from the Web</title>
<author confidence="0.967957">Niels</author>
<affiliation confidence="0.998372">Department of Computer and Electrical University of</affiliation>
<address confidence="0.7542295">Baltimore Baltimore, MD 21250,</address>
<email confidence="0.997627">nkasch1@umbc.edu</email>
<author confidence="0.982382">Tim</author>
<affiliation confidence="0.998486333333333">Department of Computer and Electrical University of</affiliation>
<address confidence="0.7585135">Baltimore Baltimore, MD 21250,</address>
<email confidence="0.999829">oates@cs.umbc.edu</email>
<abstract confidence="0.997255">This paper presents preliminary work to extract script-like structures, called events and event sets, from collections of web documents. Our approach, contrary to existing methods, is topic-driven in the sense that event sets are extracted for a specified topic. We introduce an iterative system architecture and present methods to reduce noise problems with web corpora. Preliminary results show that LSA-based event relatedness yields better event sets from web corpora than previous methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>e</author>
</authors>
<title>e(wait, dobj), e(total, nsubj) e(write, dobj), e(place, dobj), e(complete, dobj) e(exist, nsubj), e(include, dobj), e(top, nsubj) e(found,</title>
<note>dobj), e(keep, dobj), e(open, dobj) e(offer, dobj), e(average, nsubj), e(fill, dobj) e(taste, nsubj), e(drink, dobj), e(cook, dobj) e(read, dobj), e(enjoy, dobj),e(buy, dobj)</note>
<marker>e, </marker>
<rawString>e(eat, nsubj), e(wait, dobj), e(total, nsubj) e(write, dobj), e(place, dobj), e(complete, dobj) e(exist, nsubj), e(include, dobj), e(top, nsubj) e(found, dobj), e(keep, dobj), e(open, dobj) e(offer, dobj), e(average, nsubj), e(fill, dobj) e(taste, nsubj), e(drink, dobj), e(cook, dobj) e(read, dobj), e(enjoy, dobj),e(buy, dobj)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>