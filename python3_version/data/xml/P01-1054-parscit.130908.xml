<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.974334">
Tractability and Structural Closures in Attribute Logic Type Signatures
</title>
<author confidence="0.997828">
Gerald Penn
</author>
<affiliation confidence="0.998472">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.9803895">
10 King’s College Rd.
Toronto M5S 3G4, Canada
</address>
<email confidence="0.999131">
gpenn@cs.toronto.edu
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979153846154">
This paper considers three assumptions
conventionally made about signatures
in typed feature logic that are in po-
tential disagreement with current prac-
tice among grammar developers and
linguists working within feature-based
frameworks such as HPSG: meet-semi-
latticehood, unique feature introduc-
tion, and the absence of subtype cover-
ing. It also discusses the conditions un-
der which each of these can be tractably
restored in realistic grammar signatures
where they do not already exist.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996496">
The logic of typed feature structures (LTFS, Car-
penter 1992) and, in particular, its implementa-
tion in the Attribute Logic Engine (ALE, Car-
penter and Penn 1996), have been widely used
as a means of formalising and developing gram-
mars of natural languages that support computa-
tionally efficient parsing and SLD resolution, no-
tably grammars within the framework of Head-
driven Phrase Structure Grammar (HPSG, Pollard
and Sag 1994). These grammars are formulated
using a vocabulary provided by a finite partially
ordered set of types and a set of features that must
be specified for each grammar, and feature struc-
tures in these grammars must respect certain con-
straints that are also specified. These include ap-
propriateness conditions, which specify, for each
type, all and only the features that take values
in feature structures of that type, and with which
types of values (value restrictions). There are also
more general implicational constraints of the form
, where is a type, and is an expres-
sion from LTFS’s description language. In LTFS
and ALE, these four components, a partial order
of types, a set of features, appropriateness declara-
tions and type-antecedent constraints can be taken
as the signature of a grammar, relative to which
descriptions can be interpreted.
LTFS and ALE also make several assump-
tions about the structure and interpretation of this
partial order of types and about appropriateness,
some for the sake of generality, others for the
sake of efficiency or simplicity. Appropriate-
ness is generally accepted as a good thing, from
the standpoints of both efficiency and representa-
tional accuracy, and while many have advocated
the need for implicational constraints that are even
more general, type-antecedent constraints at the
very least are also accepted as being necessary and
convenient. Not all of the other assumptions are
universally observed by formal linguists or gram-
mar developers, however.
This paper addresses the three most contentious
assumptions that LTFS and ALE make, and how
to deal with their absence in a tractable manner.
They are:
</bodyText>
<listItem confidence="0.984377333333333">
1. Meet-semi-latticehood: every partial order
of types must be a meet semi-lattice. This
implies that every consistent pair of types has
a least upper bound.
2. Unique feature introduction: for every fea-
ture, F, there is a unique most general type to
which F is appropriate.
3. No subtype covering: there can be feature
structures of a non-maximally-specific type
that are not typable as any of its maximally
specific subtypes. When subtype covering
is not assumed, feature structures themselves
</listItem>
<bodyText confidence="0.999430034482759">
can be partially ordered and taken to repre-
sent partial information states about some set
of objects. When subtype covering is as-
sumed, feature structures are discretely or-
dered and totally informative, and can be
taken to represent objects in the (linguistic)
world themselves. The latter interpretation is
subscribed to by Pollard and Sag (1994), for
example.
All three of these conditions have been claimed
elsewhere to be either intractable or impossible
to restore in grammar signatures where they do
not already exist. It will be argued here that: (1)
restoring meet-semi-latticehood is theoretically
intractable, for which the worst case bears a dis-
quieting resemblance to actual practice in current
large-scale grammar signatures, but nevertheless
can be efficiently compilable in practice due to the
sparseness of consistent types; (2) unique feature
introduction can always be restored to a signature
in low-degree polynomial time, and (3) while type
inferencing when subtype covering is assumed is
intractable in the worst case, a very elegant con-
straint logic programming solution combined with
a special compilation method exists that can re-
store tractability in many practical contexts. Some
simple completion algorithms and a corrected NP-
completeness proof for non-disjunctive type infer-
encing with subtype covering are also provided.
</bodyText>
<sectionHeader confidence="0.778102" genericHeader="introduction">
2 Meet-semi-latticehood
</sectionHeader>
<bodyText confidence="0.9706464375">
In LTFS and ALE, partial orders of types are as-
sumed to be meet semi-lattices:
, .
is the binary greatest lower bound, or meet op-
eration, and is the dual of the join operation, ,
which corresponds to unification, or least upper
bounds (in the orientation where corresponds
to the most general type). Figure 1 is not a meet
semi-lattice because and do not have a meet,
nor do and , for example.
In the finite case, the assumption that every pair
of types has a meet is equivalent to the assump-
tion that every consistent set of types, i.e., types
with an upper bound, has a join. It is theoretically
convenient when discussing the unification of fea-
ture structures to assume that the unification of
</bodyText>
<figureCaption confidence="0.931827">
Figure 1: An example of a partial order that is not
a meet semi-lattice.
</figureCaption>
<bodyText confidence="0.995004605263158">
two consistent types always exists. It can also be
more efficient to make this assumption as, in some
representations of types and feature structures,
it avoids a source of non-determinism (selection
among minimal but not least upper bounds) dur-
ing search.
Just because it would be convenient for unifica-
tion to be well-defined, however, does not mean
it would be convenient to think of any empiri-
cal domain’s concepts as a meet semi-lattice, nor
that it would be convenient to add all of the types
necessary to a would-be type hierarchy to ensure
meet-semi-latticehood. The question then natu-
rally arises as to whether it would be possible,
given any finite partial order, to add some extra
elements (types, in this case) to make it a meet
semi-lattice, and if so, how many extra elements
it would take, which also provides a lower bound
on the time complexity of the completion.
It is, in fact, possible to embed any finite partial
order into a smallest lattice that preserves exist-
ing meets and joins by adding extra elements. The
resulting construction is the finite restriction of
the Dedekind-MacNeille completion (Davey and
Priestley, 1990, p. 41).
Definition 2 Given a partially ordered set,
, the Dedekind-MacNeille completion of ,
, is given by:
This route has been considered before in the
context of taxonomical knowledge representation
(A¨ıt-Ka´ci et al., 1989; Fall, 1996). While meet
semi-lattice completions are a practical step
towards providing a semantics for arbitrary
partial orders, they are generally viewed as
an impractical preliminary step to performing
computations over a partial order. Work on
more efficient encoding schemes began with
A¨ıt-Ka´ci et al. (1989), and this seminal paper has
</bodyText>
<figure confidence="0.9941828">
d
c
b
e
a
f
g
Definition 1 A partial order, , is a meet
semi-lattice ifffor any
123 124 134 234
</figure>
<figureCaption confidence="0.999225">
Figure 2: A worst case for the Dedekind-
</figureCaption>
<bodyText confidence="0.989811976190476">
MacNeille completion at .
in turn given rise to several interesting studies
of incremental computations of the Dedekind-
MacNeille completion in which LUBs are added
as they are needed (Habib and Nourine, 1994;
Bertet et al., 1997). This was also the choice
made in the LKB parsing system for HPSG
(Malouf et al., 2000).
There are partial orders of unbounded size for
which . As one family of
worst-case examples, parametrised by , consider
a set , and a partial order de-
fined as all of the size subsets of and all of
the size subsets of , ordered by inclusion. Fig-
ure 2 shows the case where . Although the
maximum subtype and supertype branching fac-
tors in this family increase linearly with size, the
partial orders can grow in depth instead in order to
contain this.
That yields something roughly of the form
shown in Figure 3, which is an example of a recent
trend in using type-intensive encodings of linguis-
tic information into typed feature logic in HPSG,
beginning with Sag (1997). These explicitly iso-
late several dimensions1 of analysis as a means
of classifying complex linguistic objects. In Fig-
ure 3, specific clausal types are selected from
among the possible combinations of CLAUSAL-
ITY and HEADEDNESS subtypes. In this set-
ting, the parameter corresponds roughly to the
number of dimensions used, although an exponen-
tial explosion is obviously not dependent on read-
ing the type hierarchy according to this conven-
tion.
There is a simple algorithm for performing this
completion, which assumes the prior existence of
a most general element ( ), given in Figure 4.
1It should be noted that while the common parlance for
these sections of the type hierarchy is dimension, borrowed
from earlier work by Erbach (1994) on multi-dimensional
inheritance, these are not dimensions in the sense of
Erbach (1994) because not every -tuple of subtypes from
an -dimensional classification is join-compatible.
Most instantiations of the heuristic, “where there
is no meet, add one” (Fall, 1996), do not yield
the Dedekind-MacNeille completion (Bertet et al.,
1997), and other authors have proposed incremen-
tal methods that trade greater efficiency in com-
puting the entire completion at once for their in-
crementality.
Proposition 1 The MSL completion algorithm is
correct on finite partially ordered sets, , i.e.,
upon termination, it has produced .
Proof: Let be the partially ordered set pro-
duced by the algorithm. Clearly, . It
suffices to show that (1) is a complete lattice
(with added), and (2) for all , there
exist subsets such that
.2
Suppose there are such that
. There is a least element, so and have
more than one maximal lower bound, and
others. But then is upper-bounded and
, so the algorithm should not have termi-
nated. Suppose instead that . Again, the
algorithm should not have terminated. So
with added is a complete lattice.
Given , if , then choose
. Otherwise, the algorithm added be-
cause of a bounded set , with minimal up-
per bounds, , which did not have a least
upper bound, i.e., . In this case, choose
and . In ei-
ther case, clearly for
all
In LinGO (Flickinger et al., 1999), the largest
publicly-available LTFS-based grammar, and one
which uses such type-intensive encodings, there
are 3414 types, the largest supertype branching
factor is 19, and although dimensionality is not
distinguished in the source code from other types,
the largest subtype branching factor is 103. Using
supertype branching factor for the most conserva-
tive estimate, this still implies a theoretical maxi-
</bodyText>
<footnote confidence="0.658925333333333">
2These are sometimes called the join density and meet
density, respectively, of in (Davey and Priestley,
1990, p. 42).
</footnote>
<equation confidence="0.754634">
1 2 3 4
.
</equation>
<bodyText confidence="0.969631636363636">
Termination is guaranteed by considering, af-
ter every iteration, the number of sets of meet-
irreducible elements with no meet, since all com-
pletion types added are meet-reducible by defini-
tion.
fin-wh-fill-rel-cl inf-wh-fill-recl-cl red-rel-cl simp-inf-rel-cl wh-subj-rel-cl bare-rel-cl
fin-hd-fill-ph
inf-hd-fill-ph fin-hd-subj-ph
wh-rel-cl non-wh-rel-cl hd-fill-ph hd-comp-ph hd-subj-ph hd-spr-ph
imp-cl decl-cl inter-cl rel-cl hd-adj-ph hd-nexus-ph
clause non-clause hd-ph non-hd-ph
</bodyText>
<figure confidence="0.535595">
CLAUSALITY HEADEDNESS
phrase
</figure>
<figureCaption confidence="0.998158">
Figure 3: A fragment of an English grammar in which supertype branching distinguishes
</figureCaption>
<bodyText confidence="0.998364333333333">
“dimensions” of classification.
mum of approximately 500,000 completion types,
whereas only 893 are necessary, 648 of which are
inferred without reference to previously added
completion types.
Whereas incremental compilation methods rely
on the assumption that the joins of most pairs of
types will never be computed in a corpus before
the signature changes, this method’s efficiency re-
lies on the assumption that most pairs of types
are join-incompatible no matter how the signa-
ture changes. In LinGO, this is indeed the case:
of the 11,655,396 possible pairs, 11,624,866 are
join-incompatible, and there are only 3,306 that
are consistent (with or without joins) and do not
stand in a subtyping or identity relationship. In
fact, the cost of completion is often dominated
by the cost of transitive closure, which, using a
sparse matrix representation, can be completed for
LinGO in about 9 seconds on a 450 MHz Pentium
II with 1GB memory (Penn, 2000a).
While the continued efficiency of compile-time
completion of signatures as they further increase
in size can only be verified empirically, what can
be said at this stage is that the only reason that sig-
natures like LinGO can be tractably compiled at
all is sparseness of consistent types. In other ge-
ometric respects, it bears a close enough resem-
blance to the theoretical worst case to cause con-
cern about scalability. Compilation, if efficient,
is to be preferred from the standpoint of static
error detection, which incremental methods may
elect to skip. In addition, running a new signa-
ture plus grammar over a test corpus is a frequent
task in large-scale grammar development, and in-
cremental methods, even ones that memoise pre-
vious computations, may pay back the savings in
compile-time on a large test corpus. It should also
be noted that another plausible method is compi-
lation into logical terms or bit vectors, in which
some amount of compilation (ranging from linear-
time to exponential) is performed with the remain-
ing cost amortised evenly across all run-time uni-
fications, which often results in a savings during
grammar development.
</bodyText>
<sectionHeader confidence="0.572049" genericHeader="method">
3 Unique Feature Introduction
</sectionHeader>
<bodyText confidence="0.7758889">
LTFS and ALE also assume that appropriateness
guarantees the existence of a unique introducer for
every feature:
Definition 3 Given a type hierarchy, , and
a finite set offeatures, Feat, an appropriateness
specification is a partial function,
such that, for every F :
(Feature Introduction) there is a type
F such that:
– F F ✗, and
Feature introduction has been argued not to be
appropriate for certain empirical domains either,
although Pollard and Sag (1994) do otherwise ob-
serve it. The debate, however, has focussed on
whether to modify some other aspect of type infer-
encing in order to compensate for the lack of fea-
ture introduction, presumably under the assump-
tion that feature introduction was difficult or im-
possible to restore automatically to grammar sig-
natures that did not have it.
</bodyText>
<table confidence="0.889562066666667">
F , and
(Upward Closure / Right Monotonic-
ity) if F and , then
F and F
F .
– for every
, if F ✗, then
1. Find two elements, with minimal upper bounds,
, such that their join is undefined, i.e.,
. If no such pair exists, then stop.
2. Add an element, , such that:
for all ,, and
for all elements, iff for all
.
3. Go to (1).
</table>
<figureCaption confidence="0.99966">
Figure 4: The MSL completion algorithm.
</figureCaption>
<bodyText confidence="0.999974333333333">
Just as with the condition of meet-semi-
latticehood, however, it is possible to take a
would-be signature without feature introduction
and restore this condition through the addition
of extra unique introducing types for certain
appropriate features. The algorithm in Figure 5
achieves this. In practice, the same signature
completion type, , can be used for different
features, provided that their minimal introducers
are the same set, . This clearly produces a
partially ordered set with a unique introducing
type for every feature. It may disturb meet-
semi-latticehood, however, which means that this
completion must precede the meet semi-lattice
completion of Section 2. If generalisation has
already been computed, the signature completion
algorithm runs in , where is the number
of features, and is the number of types.
</bodyText>
<sectionHeader confidence="0.989221" genericHeader="method">
4 Subtype Covering
</sectionHeader>
<bodyText confidence="0.97389415">
In HPSG, it is generally assumed that non-
maximally-specific types are simply a convenient
shorthand for talking about sets of maximally
specific types, sometimes called species, over
which the principles of a grammar are stated. In a
view where feature structures represent discretely
ordered objects in an empirical model, every
feature structure must bear one of these species.
In particular, each non-maximally-specific type
in a description is equivalent to the disjunction of
the maximally specific subtypes that it subsumes.
There are some good reasons not to build this
assumption, called “subtype covering,” into LTFS
or its implementations. Firstly, it is not an ap-
propriate assumption to make for some empiri-
cal domains. Even in HPSG, the denotations of
1. Given candidate signature, , find a feature, F, for
which there is no unique introducing type. Let be
the set of minimal types to which F is appropriate,
where . If there is no such feature, then stop.
</bodyText>
<listItem confidence="0.528621666666667">
2. Add a new type, , to , to which F is appropriate, such
that:
3. Go to (1).
</listItem>
<figureCaption confidence="0.999617">
Figure 5: The introduction completion algorithm.
</figureCaption>
<bodyText confidence="0.999461923076923">
parametrically-typed lists are more naturally in-
terpreted without it. Secondly, not to make the as-
sumption is more general: where it is appropriate,
extra type-antecedent constraints can be added to
the grammar signature of the form:
for each non-maximally-specific type, , and its
maximal subtypes, . These con-
straints become crucial in certain cases where the
possible permutations of appropriate feature val-
ues at a type are not covered by the permutations
of those features on its maximally specific sub-
types. This is the case for the type, verb, in the
signature in Figure 6 (given in ALE syntax, where
sub/2 defines the partial order of types, and
intro/2 defines appropriateness on unique in-
troducers of features). The combination, AUX
INV , is not attested by any of verb’s subtypes.
While there are arguably better ways to represent
this information, the extra type-antecedent con-
straint:
verb aux verb main verb
is necessary in order to decide satisfiability cor-
rectly under the assumption of subtype covering.
We will call types such as verb deranged types.
Types that are not deranged are called normal
types.
</bodyText>
<figure confidence="0.908648142857143">
,
for all , ,
for all types, in ,iff for all ,
, and
F F
F F ,
the generalization of the value restrictions on F
of the elements of .
bot sub [verb,bool].
bool sub [+,-].
verb sub [aux_verb,main_verb]
intro [aux:bool,inv:bool].
aux_verb sub [aux:+,inv:bool].
main_verb sub [aux:-,inv:-].
</figure>
<figureCaption confidence="0.997316">
Figure 6: A signature with a deranged type.
</figureCaption>
<subsectionHeader confidence="0.953838">
4.1 Non-Disjunctive Type Inference under
Subtype Covering is NP-Complete
</subsectionHeader>
<bodyText confidence="0.999991583333333">
Third, although subtype covering is, in the au-
thor’s experience, not a source of inefficiency in
practical LTFS grammars, when subtype cover-
ing is implicitly assumed, determining whether a
non-disjunctive description is satisfiable under ap-
propriateness conditions is an NP-complete prob-
lem, whereas this is known to be polynomial
time without it (and without type-antecedent con-
straints, of course). This was originally proven by
Carpenter and King (1995). The proof, with cor-
rections, is summarised here because it was never
published. Consider the translation of a 3SAT for-
mula into a description relative to the signature
given in Figure 7. The resulting description is al-
ways non-disjunctive, since logical disjunction is
encoded in subtyping. Asking whether a formula
is satisfiable then reduces to asking whether this
description conjoined with trueform is satisfi-
able. Every type is normal except for truedisj,
for which the combination, DISJ1falseform
DISJ2falseform, is not attested in either of its
subtypes. Enforcing subtype covering on this one
deranged type is the sole source of intractability
for this problem.
</bodyText>
<subsectionHeader confidence="0.9939105">
4.2 Practical Enforcement of Subtype
Covering
</subsectionHeader>
<bodyText confidence="0.999902166666666">
Instead of enforcing subtype covering along with
type inferencing, an alternative is to suspend con-
straints on feature structures that encode subtype
covering restrictions, and conduct type inferenc-
ing in their absence. This restores tractability
at the cost of rendering type inferencing sound
but not complete. This can be implemented very
transparently in systems like ALE that are built on
top of another logic programming language with
support for constraint logic programming such as
SICStus Prolog. In the worst case, an answer to a
query to the grammar signature may contain vari-
</bodyText>
<table confidence="0.7193849">
bot sub [bool,formula].
bool sub [true,false].
formula sub [propsymbol,conj,disj,neg,
trueform,falseform].
propsymbol sub [truepropsym,
falsepropsym].
conj sub [trueconj,falseconj1,
falseconj2].
intro [conj1:formula,
conj2:formula].
trueconj intro [conj1:trueform,
conj2:trueform].
falseconj1 intro [conj1:falseform].
falseconj2 intro [conj2:falseform].
disj sub [truedisj,falsedisj]
intro [disj1:formula,
disj2:formula].
truedisj sub [truedisj1,truedisj2].
truedisj1 intro [disj1:trueform].
truedisj2 intro [disj2:trueform].
falsedisj intro [disj1:falseform,
disj2:falseform].
neg sub [trueneg,falseneg]
intro [neg:propsymbol].
trueneg intro [neg:falsepropsym].
falseneg intro [neg:truepropsym].
trueform sub [truepropsym,trueconj,
truedisj,trueneg].
falseform sub [falsepropsym,falseconj1,
falseconj2,falsedisj,falseneg].
</table>
<figureCaption confidence="0.918648">
Figure 7: The signature reducing 3SAT to non-
disjunctive type inferencing.
</figureCaption>
<bodyText confidence="0.997853928571429">
ables with constraints attached to them that must
be exhaustively searched over in order to deter-
mine their satisfiability, and this is still intractable
in the worst case. The advantage of suspending
subtype covering constraints is that other princi-
ples of grammar and proof procedures such as
SLD resolution, parsing or generation can add de-
terministic information that may result in an early
failure or a deterministic set of constraints that can
then be applied immediately and efficiently. The
variables that correspond to feature structures of
a deranged type are precisely those that require
these suspended constraints.
Given a diagnosis of which types in a signature
are deranged (discussed in the next section),
suspended subtype covering constraints can be
implemented for the SICStus Prolog implemen-
tation of ALE by adding relational attachments
to ALE’s type-antecedent universal constraints
that will suspend a goal on candidate feature
structures with deranged types such as verb
or truedisj. The suspended goal unblocks
whenever the deranged type or the type of one
of its appropriate features’ values is updated to
a more specific subtype, and checks the types of
the appropriate features’ values. Of particular use
is the SICStus Constraint Handling Rules (CHR,
Fr¨uhwirth and Abdennadher (1997)) library,
which has the ability not only to suspend, but to
suspend until a particular variable is instantiated
or even bound to another variable. This is the
powerful kind of mechanism required to check
these constraints efficiently, i.e., only when nec-
essary. Re-entrancies in a Prolog term encoding
of feature structures, such as the one ALE uses
(Penn, 1999), may only show up as the binding
of two uninstantiated variables, and re-entrancies
are often an important case where these con-
straints need to be checked. The details of this
reduction to constraint handling rules are given in
Penn (2000b). The relevant complexity-theoretic
issue is the detection of deranged types.
</bodyText>
<subsectionHeader confidence="0.999153">
4.3 Detecting Deranged Types
</subsectionHeader>
<bodyText confidence="0.965265788461538">
The detection of deranged types themselves is
also a potential problem. This is something that
needs to be detected at compile-time when sub-
type covering constraints are generated, and as
small changes in a partial order of types can have
drastic effects on other parts of the signature be-
cause of appropriateness, incremental compila-
tion of the grammar signature itself can be ex-
tremely difficult. This means that the detection of
deranged types must be something that can be per-
formed very quickly, as it will normally be per-
formed repeatedly during development.
A naive algorithm would be, for every type,
to expand the product of its features’ appropriate
value types into the set, , of all possible maxi-
mally specific products, then to do the same for the
products on each of the type’s maximally spe-
cific subtypes, forming sets , and then to re-
move the products in the from . The type is
deranged iff any maximally specific products re-
main in . If the maximum number of
features appropriate to any type is , and there are
types in the signature, then the cost of this is
dominated by the cost of expanding the products,
, since in the worst case all features could have
as their appropriate value.
A less naive algorithm would treat normal (non-
deranged) subtypes as if they were maximally spe-
cific when doing the expansion. This works be-
cause the products of appropriate feature values of
normal types are, by definition, covered by those
of their own maximally specific subtypes. Maxi-
mally specific types, furthermore, are always nor-
mal and do not need to be checked. Atomic types
(types with no appropriate features) are also triv-
ially normal.
It is also possible to avoid doing a great deal of
the remaining expansion, simply by counting the
number of maximally specific products of types
rather than by enumerating them. For exam-
ple, in Figure 6, main verb has one such prod-
uct, AUX INV , and aux verb has two,
AUX INV , and AUX INV . verb,
on the other hand, has all four possible combina-
tions, so it is deranged. The resulting algorithm is
thus given in Figure 8. Using the smallest normal
For each type,, in topological order (from maximally spe-
cific down to ):
if t is maximal or atomic then is normal. Tabulate
normals , a minimal normal subtype cover of
the maximal subtypes of.
Otherwise:
</bodyText>
<listItem confidence="0.931709">
1. Let normals , where is the
set of immediate subtypes of.
2. Let be the number of features appropriate to
, and let
</listItem>
<figure confidence="0.230831818181818">
AppropF AppropF .
3. Given such that (coordinate-
wise):
– if (coordinate-wise), then discard
,
– if , then discard ,
– otherwise replace in with:
Repeat this step until no such exist.
4. Let maximalAppropF
FAppropF
maximal , where
</figure>
<footnote confidence="0.320807333333333">
maximal is the number of maximal subtypes
of .
5. if ,then is deranged; tabulate
normals and continue. Otherwise,
is normal; tabulate normals and con-
tinue.
</footnote>
<figureCaption confidence="0.930383333333333">
Figure 8: The deranged type detection algorithm.
immed. subtype of in
immed. subtype of in
</figureCaption>
<bodyText confidence="0.999913230769231">
subtype cover that we have for the product of ’s
feature values, we iteratively expand the feature
value products for this cover until they partition
their maximal feature products, and then count the
maximal products using multiplication. A similar
trick can be used to calculate maximal efficiently.
The complexity of this approach, in practice,
is much better: , where is the weighted
mean subtype branching factor of a subtype of
a value restriction of a non-maximal non-atomic
type’s feature, and is the weighted mean length
of the longest path from a maximal type to a sub-
type of a value restriction of a non-maximal non-
atomic type’s feature. In the Dedekind-MacNeille
completion of LinGO’s signature, is 1.9, is 2.2,
and the sum of over all non-maximal types
with arity is approximately . The sum of
maximal over every non-maximal type, , on
the other hand, is approximately . Practical
performance is again much better because this al-
gorithm can exploit the empirical observation that
most types in a realistic signature are normal and
that most feature value restrictions on subtypes do
not vary widely. Using branching factor to move
the total number of types to a lower degree term is
crucial for large signatures.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99984125">
Efficient compilation of both meet-semi-
latticehood and subtype covering depends
crucially in practice on sparseness, either of
consistency among types, or of deranged types,
to the extent it is possible at all. Closure for
unique feature introduction runs in linear time in
both the number of features and types. Subtype
covering results in NP-complete non-disjunctive
type inferencing, but the postponement of these
constraints using constraint handling rules can
often hide that complexity in the presence of
other principles of grammar.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913381818182">
H. A¨ıt-Ka´ci, R. Boyer, P. Lincoln, and R. Nasr. 1989.
Efficient implementation of lattice operations. ACM
Transactions on Programming Languages and Sys-
tems, 11(1):115–146.
K. Bertet, M. Morvan, and L. Nourine. 1997. Lazy
completion of a partial order to the smallest lattice.
In Proceedings of the International KRUSE Sympo-
sium: Knowledge Retrieval, Use and Storagefor Ef-
ficiency, pages 72–81.
B. Carpenter and P.J. King. 1995. The complexity
of closed world reasoning in constraint-based gram-
mar theories. In Fourth Meeting on the Mathemat-
ics ofLanguage, University of Pennsylvania.
B. Carpenter and G. Penn. 1996. Compiling typed
attribute-value logic grammars. In H. Bunt and
M. Tomita, editors, Recent Advances in Parsing
Technologies, pages 145–168. Kluwer.
B. Carpenter. 1992. The Logic of Typed Feature Struc-
tures. Cambridge.
B. A. Davey and H. A. Priestley. 1990. Introduction
to Lattices and Order. Cambridge University Press.
G. Erbach. 1994. Multi-dimensional inheritance. In
Proceedings ofKONVENS 94. Springer.
D. Flickinger et al. 1999. The LinGO English
resource grammar. Available on-line from
http://hpsg.stanford.edu/hpsg/
lingo.html.
A. Fall. 1996. Reasoning with Taxonomies. Ph.D. the-
sis, Simon Fraser University.
T. Fr¨uhwirth and S. Abdennadher. 1997. Constraint-
Programmierung. Springer Verlag.
M. Habib and L. Nourine. 1994. Bit-vector encod-
ing for partially ordered sets. In Orders, Algorithms,
Applications: International Workshop ORDAL ’94
Proceedings, pages 1–12. Springer-Verlag.
R. Malouf, J. Carroll, and A. Copestake. 2000. Ef-
ficient feature structure operations without compi-
lation. Journal of Natural Language Engineering,
6(1):29–46.
G. Penn. 1999. An optimized prolog encoding of
typed feature structures. In Proceedings of the
16th International Conference on Logic Program-
ming (ICLP-99), pages 124–138.
G. Penn. 2000a. The Algebraic Structure ofAttributed
Type Signatures. Ph.D. thesis, Carnegie Mellon
University.
G. Penn. 2000b. Applying Constraint Han-
dling Rules to HPSG. In Proceedings of the
First International Conference on Computational
Logic (CL2000), Workshop on Rule-Based Con-
straint Reasoning and Programming, London, UK.
C. Pollard and I. Sag. 1994. Head-driven Phrase
Structure Grammar. Chicago.
I. A. Sag. 1997. English relative clause constructions.
Journal ofLinguistics, 33(2):431–484.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976764">
<title confidence="0.999952">Tractability and Structural Closures in Attribute Logic Type Signatures</title>
<author confidence="0.999883">Gerald Penn</author>
<affiliation confidence="0.9999415">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.994306">10 King’s College Rd. Toronto M5S 3G4, Canada</address>
<email confidence="0.999897">gpenn@cs.toronto.edu</email>
<abstract confidence="0.999107357142857">This paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within feature-based frameworks such as HPSG: meet-semilatticehood, unique feature introduction, and the absence of subtype covering. It also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H A¨ıt-Ka´ci</author>
<author>R Boyer</author>
<author>P Lincoln</author>
<author>R Nasr</author>
</authors>
<title>Efficient implementation of lattice operations.</title>
<date>1989</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<volume>11</volume>
<issue>1</issue>
<marker>A¨ıt-Ka´ci, Boyer, Lincoln, Nasr, 1989</marker>
<rawString>H. A¨ıt-Ka´ci, R. Boyer, P. Lincoln, and R. Nasr. 1989. Efficient implementation of lattice operations. ACM Transactions on Programming Languages and Systems, 11(1):115–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bertet</author>
<author>M Morvan</author>
<author>L Nourine</author>
</authors>
<title>Lazy completion of a partial order to the smallest lattice.</title>
<date>1997</date>
<booktitle>In Proceedings of the International KRUSE Symposium: Knowledge Retrieval, Use and Storagefor Efficiency,</booktitle>
<pages>72--81</pages>
<contexts>
<context position="7521" citStr="Bertet et al., 1997" startWordPosition="1205" endWordPosition="1208">a semantics for arbitrary partial orders, they are generally viewed as an impractical preliminary step to performing computations over a partial order. Work on more efficient encoding schemes began with A¨ıt-Ka´ci et al. (1989), and this seminal paper has d c b e a f g Definition 1 A partial order, , is a meet semi-lattice ifffor any 123 124 134 234 Figure 2: A worst case for the DedekindMacNeille completion at . in turn given rise to several interesting studies of incremental computations of the DedekindMacNeille completion in which LUBs are added as they are needed (Habib and Nourine, 1994; Bertet et al., 1997). This was also the choice made in the LKB parsing system for HPSG (Malouf et al., 2000). There are partial orders of unbounded size for which . As one family of worst-case examples, parametrised by , consider a set , and a partial order defined as all of the size subsets of and all of the size subsets of , ordered by inclusion. Figure 2 shows the case where . Although the maximum subtype and supertype branching factors in this family increase linearly with size, the partial orders can grow in depth instead in order to contain this. That yields something roughly of the form shown in Figure 3, </context>
<context position="9352" citStr="Bertet et al., 1997" startWordPosition="1512" endWordPosition="1515"> simple algorithm for performing this completion, which assumes the prior existence of a most general element ( ), given in Figure 4. 1It should be noted that while the common parlance for these sections of the type hierarchy is dimension, borrowed from earlier work by Erbach (1994) on multi-dimensional inheritance, these are not dimensions in the sense of Erbach (1994) because not every -tuple of subtypes from an -dimensional classification is join-compatible. Most instantiations of the heuristic, “where there is no meet, add one” (Fall, 1996), do not yield the Dedekind-MacNeille completion (Bertet et al., 1997), and other authors have proposed incremental methods that trade greater efficiency in computing the entire completion at once for their incrementality. Proposition 1 The MSL completion algorithm is correct on finite partially ordered sets, , i.e., upon termination, it has produced . Proof: Let be the partially ordered set produced by the algorithm. Clearly, . It suffices to show that (1) is a complete lattice (with added), and (2) for all , there exist subsets such that .2 Suppose there are such that . There is a least element, so and have more than one maximal lower bound, and others. But th</context>
</contexts>
<marker>Bertet, Morvan, Nourine, 1997</marker>
<rawString>K. Bertet, M. Morvan, and L. Nourine. 1997. Lazy completion of a partial order to the smallest lattice. In Proceedings of the International KRUSE Symposium: Knowledge Retrieval, Use and Storagefor Efficiency, pages 72–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
<author>P J King</author>
</authors>
<title>The complexity of closed world reasoning in constraint-based grammar theories.</title>
<date>1995</date>
<booktitle>In Fourth Meeting on the Mathematics ofLanguage,</booktitle>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18738" citStr="Carpenter and King (1995)" startWordPosition="3044" endWordPosition="3047"> sub [aux:+,inv:bool]. main_verb sub [aux:-,inv:-]. Figure 6: A signature with a deranged type. 4.1 Non-Disjunctive Type Inference under Subtype Covering is NP-Complete Third, although subtype covering is, in the author’s experience, not a source of inefficiency in practical LTFS grammars, when subtype covering is implicitly assumed, determining whether a non-disjunctive description is satisfiable under appropriateness conditions is an NP-complete problem, whereas this is known to be polynomial time without it (and without type-antecedent constraints, of course). This was originally proven by Carpenter and King (1995). The proof, with corrections, is summarised here because it was never published. Consider the translation of a 3SAT formula into a description relative to the signature given in Figure 7. The resulting description is always non-disjunctive, since logical disjunction is encoded in subtyping. Asking whether a formula is satisfiable then reduces to asking whether this description conjoined with trueform is satisfiable. Every type is normal except for truedisj, for which the combination, DISJ1falseform DISJ2falseform, is not attested in either of its subtypes. Enforcing subtype covering on this o</context>
</contexts>
<marker>Carpenter, King, 1995</marker>
<rawString>B. Carpenter and P.J. King. 1995. The complexity of closed world reasoning in constraint-based grammar theories. In Fourth Meeting on the Mathematics ofLanguage, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
<author>G Penn</author>
</authors>
<title>Compiling typed attribute-value logic grammars.</title>
<date>1996</date>
<booktitle>Recent Advances in Parsing Technologies,</booktitle>
<pages>145--168</pages>
<editor>In H. Bunt and M. Tomita, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="875" citStr="Carpenter and Penn 1996" startWordPosition="125" endWordPosition="129"> conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within feature-based frameworks such as HPSG: meet-semilatticehood, unique feature introduction, and the absence of subtype covering. It also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist. 1 Introduction The logic of typed feature structures (LTFS, Carpenter 1992) and, in particular, its implementation in the Attribute Logic Engine (ALE, Carpenter and Penn 1996), have been widely used as a means of formalising and developing grammars of natural languages that support computationally efficient parsing and SLD resolution, notably grammars within the framework of Headdriven Phrase Structure Grammar (HPSG, Pollard and Sag 1994). These grammars are formulated using a vocabulary provided by a finite partially ordered set of types and a set of features that must be specified for each grammar, and feature structures in these grammars must respect certain constraints that are also specified. These include appropriateness conditions, which specify, for each ty</context>
</contexts>
<marker>Carpenter, Penn, 1996</marker>
<rawString>B. Carpenter and G. Penn. 1996. Compiling typed attribute-value logic grammars. In H. Bunt and M. Tomita, editors, Recent Advances in Parsing Technologies, pages 145–168. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures.</title>
<date>1992</date>
<location>Cambridge.</location>
<contexts>
<context position="775" citStr="Carpenter 1992" startWordPosition="110" endWordPosition="112">oronto M5S 3G4, Canada gpenn@cs.toronto.edu Abstract This paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within feature-based frameworks such as HPSG: meet-semilatticehood, unique feature introduction, and the absence of subtype covering. It also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist. 1 Introduction The logic of typed feature structures (LTFS, Carpenter 1992) and, in particular, its implementation in the Attribute Logic Engine (ALE, Carpenter and Penn 1996), have been widely used as a means of formalising and developing grammars of natural languages that support computationally efficient parsing and SLD resolution, notably grammars within the framework of Headdriven Phrase Structure Grammar (HPSG, Pollard and Sag 1994). These grammars are formulated using a vocabulary provided by a finite partially ordered set of types and a set of features that must be specified for each grammar, and feature structures in these grammars must respect certain const</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>B. Carpenter. 1992. The Logic of Typed Feature Structures. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A Davey</author>
<author>H A Priestley</author>
</authors>
<title>Introduction to Lattices and Order.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6584" citStr="Davey and Priestley, 1990" startWordPosition="1050" endWordPosition="1053">e type hierarchy to ensure meet-semi-latticehood. The question then naturally arises as to whether it would be possible, given any finite partial order, to add some extra elements (types, in this case) to make it a meet semi-lattice, and if so, how many extra elements it would take, which also provides a lower bound on the time complexity of the completion. It is, in fact, possible to embed any finite partial order into a smallest lattice that preserves existing meets and joins by adding extra elements. The resulting construction is the finite restriction of the Dedekind-MacNeille completion (Davey and Priestley, 1990, p. 41). Definition 2 Given a partially ordered set, , the Dedekind-MacNeille completion of , , is given by: This route has been considered before in the context of taxonomical knowledge representation (A¨ıt-Ka´ci et al., 1989; Fall, 1996). While meet semi-lattice completions are a practical step towards providing a semantics for arbitrary partial orders, they are generally viewed as an impractical preliminary step to performing computations over a partial order. Work on more efficient encoding schemes began with A¨ıt-Ka´ci et al. (1989), and this seminal paper has d c b e a f g Definition 1 </context>
<context position="10904" citStr="Davey and Priestley, 1990" startWordPosition="1777" endWordPosition="1780">bound, i.e., . In this case, choose and . In either case, clearly for all In LinGO (Flickinger et al., 1999), the largest publicly-available LTFS-based grammar, and one which uses such type-intensive encodings, there are 3414 types, the largest supertype branching factor is 19, and although dimensionality is not distinguished in the source code from other types, the largest subtype branching factor is 103. Using supertype branching factor for the most conservative estimate, this still implies a theoretical maxi2These are sometimes called the join density and meet density, respectively, of in (Davey and Priestley, 1990, p. 42). 1 2 3 4 . Termination is guaranteed by considering, after every iteration, the number of sets of meetirreducible elements with no meet, since all completion types added are meet-reducible by definition. fin-wh-fill-rel-cl inf-wh-fill-recl-cl red-rel-cl simp-inf-rel-cl wh-subj-rel-cl bare-rel-cl fin-hd-fill-ph inf-hd-fill-ph fin-hd-subj-ph wh-rel-cl non-wh-rel-cl hd-fill-ph hd-comp-ph hd-subj-ph hd-spr-ph imp-cl decl-cl inter-cl rel-cl hd-adj-ph hd-nexus-ph clause non-clause hd-ph non-hd-ph CLAUSALITY HEADEDNESS phrase Figure 3: A fragment of an English grammar in which supertype bran</context>
</contexts>
<marker>Davey, Priestley, 1990</marker>
<rawString>B. A. Davey and H. A. Priestley. 1990. Introduction to Lattices and Order. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erbach</author>
</authors>
<title>Multi-dimensional inheritance.</title>
<date>1994</date>
<booktitle>In Proceedings ofKONVENS 94.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="9015" citStr="Erbach (1994)" startWordPosition="1465" endWordPosition="1466">, specific clausal types are selected from among the possible combinations of CLAUSALITY and HEADEDNESS subtypes. In this setting, the parameter corresponds roughly to the number of dimensions used, although an exponential explosion is obviously not dependent on reading the type hierarchy according to this convention. There is a simple algorithm for performing this completion, which assumes the prior existence of a most general element ( ), given in Figure 4. 1It should be noted that while the common parlance for these sections of the type hierarchy is dimension, borrowed from earlier work by Erbach (1994) on multi-dimensional inheritance, these are not dimensions in the sense of Erbach (1994) because not every -tuple of subtypes from an -dimensional classification is join-compatible. Most instantiations of the heuristic, “where there is no meet, add one” (Fall, 1996), do not yield the Dedekind-MacNeille completion (Bertet et al., 1997), and other authors have proposed incremental methods that trade greater efficiency in computing the entire completion at once for their incrementality. Proposition 1 The MSL completion algorithm is correct on finite partially ordered sets, , i.e., upon terminati</context>
</contexts>
<marker>Erbach, 1994</marker>
<rawString>G. Erbach. 1994. Multi-dimensional inheritance. In Proceedings ofKONVENS 94. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>The LinGO English resource grammar. Available on-line from http://hpsg.stanford.edu/hpsg/ lingo.html.</title>
<date>1999</date>
<marker>Flickinger, 1999</marker>
<rawString>D. Flickinger et al. 1999. The LinGO English resource grammar. Available on-line from http://hpsg.stanford.edu/hpsg/ lingo.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fall</author>
</authors>
<title>Reasoning with Taxonomies.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Simon Fraser University.</institution>
<contexts>
<context position="6824" citStr="Fall, 1996" startWordPosition="1090" endWordPosition="1091">extra elements it would take, which also provides a lower bound on the time complexity of the completion. It is, in fact, possible to embed any finite partial order into a smallest lattice that preserves existing meets and joins by adding extra elements. The resulting construction is the finite restriction of the Dedekind-MacNeille completion (Davey and Priestley, 1990, p. 41). Definition 2 Given a partially ordered set, , the Dedekind-MacNeille completion of , , is given by: This route has been considered before in the context of taxonomical knowledge representation (A¨ıt-Ka´ci et al., 1989; Fall, 1996). While meet semi-lattice completions are a practical step towards providing a semantics for arbitrary partial orders, they are generally viewed as an impractical preliminary step to performing computations over a partial order. Work on more efficient encoding schemes began with A¨ıt-Ka´ci et al. (1989), and this seminal paper has d c b e a f g Definition 1 A partial order, , is a meet semi-lattice ifffor any 123 124 134 234 Figure 2: A worst case for the DedekindMacNeille completion at . in turn given rise to several interesting studies of incremental computations of the DedekindMacNeille com</context>
<context position="9282" citStr="Fall, 1996" startWordPosition="1504" endWordPosition="1505">g the type hierarchy according to this convention. There is a simple algorithm for performing this completion, which assumes the prior existence of a most general element ( ), given in Figure 4. 1It should be noted that while the common parlance for these sections of the type hierarchy is dimension, borrowed from earlier work by Erbach (1994) on multi-dimensional inheritance, these are not dimensions in the sense of Erbach (1994) because not every -tuple of subtypes from an -dimensional classification is join-compatible. Most instantiations of the heuristic, “where there is no meet, add one” (Fall, 1996), do not yield the Dedekind-MacNeille completion (Bertet et al., 1997), and other authors have proposed incremental methods that trade greater efficiency in computing the entire completion at once for their incrementality. Proposition 1 The MSL completion algorithm is correct on finite partially ordered sets, , i.e., upon termination, it has produced . Proof: Let be the partially ordered set produced by the algorithm. Clearly, . It suffices to show that (1) is a complete lattice (with added), and (2) for all , there exist subsets such that .2 Suppose there are such that . There is a least elem</context>
</contexts>
<marker>Fall, 1996</marker>
<rawString>A. Fall. 1996. Reasoning with Taxonomies. Ph.D. thesis, Simon Fraser University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fr¨uhwirth</author>
<author>S Abdennadher</author>
</authors>
<date>1997</date>
<publisher>ConstraintProgrammierung. Springer Verlag.</publisher>
<marker>Fr¨uhwirth, Abdennadher, 1997</marker>
<rawString>T. Fr¨uhwirth and S. Abdennadher. 1997. ConstraintProgrammierung. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Habib</author>
<author>L Nourine</author>
</authors>
<title>Bit-vector encoding for partially ordered sets.</title>
<date>1994</date>
<booktitle>In Orders, Algorithms, Applications: International Workshop ORDAL ’94 Proceedings,</booktitle>
<pages>1--12</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="7499" citStr="Habib and Nourine, 1994" startWordPosition="1201" endWordPosition="1204">l step towards providing a semantics for arbitrary partial orders, they are generally viewed as an impractical preliminary step to performing computations over a partial order. Work on more efficient encoding schemes began with A¨ıt-Ka´ci et al. (1989), and this seminal paper has d c b e a f g Definition 1 A partial order, , is a meet semi-lattice ifffor any 123 124 134 234 Figure 2: A worst case for the DedekindMacNeille completion at . in turn given rise to several interesting studies of incremental computations of the DedekindMacNeille completion in which LUBs are added as they are needed (Habib and Nourine, 1994; Bertet et al., 1997). This was also the choice made in the LKB parsing system for HPSG (Malouf et al., 2000). There are partial orders of unbounded size for which . As one family of worst-case examples, parametrised by , consider a set , and a partial order defined as all of the size subsets of and all of the size subsets of , ordered by inclusion. Figure 2 shows the case where . Although the maximum subtype and supertype branching factors in this family increase linearly with size, the partial orders can grow in depth instead in order to contain this. That yields something roughly of the fo</context>
</contexts>
<marker>Habib, Nourine, 1994</marker>
<rawString>M. Habib and L. Nourine. 1994. Bit-vector encoding for partially ordered sets. In Orders, Algorithms, Applications: International Workshop ORDAL ’94 Proceedings, pages 1–12. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
<author>J Carroll</author>
<author>A Copestake</author>
</authors>
<title>Efficient feature structure operations without compilation.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="7609" citStr="Malouf et al., 2000" startWordPosition="1222" endWordPosition="1225">eliminary step to performing computations over a partial order. Work on more efficient encoding schemes began with A¨ıt-Ka´ci et al. (1989), and this seminal paper has d c b e a f g Definition 1 A partial order, , is a meet semi-lattice ifffor any 123 124 134 234 Figure 2: A worst case for the DedekindMacNeille completion at . in turn given rise to several interesting studies of incremental computations of the DedekindMacNeille completion in which LUBs are added as they are needed (Habib and Nourine, 1994; Bertet et al., 1997). This was also the choice made in the LKB parsing system for HPSG (Malouf et al., 2000). There are partial orders of unbounded size for which . As one family of worst-case examples, parametrised by , consider a set , and a partial order defined as all of the size subsets of and all of the size subsets of , ordered by inclusion. Figure 2 shows the case where . Although the maximum subtype and supertype branching factors in this family increase linearly with size, the partial orders can grow in depth instead in order to contain this. That yields something roughly of the form shown in Figure 3, which is an example of a recent trend in using type-intensive encodings of linguistic in</context>
</contexts>
<marker>Malouf, Carroll, Copestake, 2000</marker>
<rawString>R. Malouf, J. Carroll, and A. Copestake. 2000. Efficient feature structure operations without compilation. Journal of Natural Language Engineering, 6(1):29–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Penn</author>
</authors>
<title>An optimized prolog encoding of typed feature structures.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Conference on Logic Programming (ICLP-99),</booktitle>
<pages>124--138</pages>
<contexts>
<context position="22617" citStr="Penn, 1999" startWordPosition="3588" endWordPosition="3589"> the type of one of its appropriate features’ values is updated to a more specific subtype, and checks the types of the appropriate features’ values. Of particular use is the SICStus Constraint Handling Rules (CHR, Fr¨uhwirth and Abdennadher (1997)) library, which has the ability not only to suspend, but to suspend until a particular variable is instantiated or even bound to another variable. This is the powerful kind of mechanism required to check these constraints efficiently, i.e., only when necessary. Re-entrancies in a Prolog term encoding of feature structures, such as the one ALE uses (Penn, 1999), may only show up as the binding of two uninstantiated variables, and re-entrancies are often an important case where these constraints need to be checked. The details of this reduction to constraint handling rules are given in Penn (2000b). The relevant complexity-theoretic issue is the detection of deranged types. 4.3 Detecting Deranged Types The detection of deranged types themselves is also a potential problem. This is something that needs to be detected at compile-time when subtype covering constraints are generated, and as small changes in a partial order of types can have drastic effec</context>
</contexts>
<marker>Penn, 1999</marker>
<rawString>G. Penn. 1999. An optimized prolog encoding of typed feature structures. In Proceedings of the 16th International Conference on Logic Programming (ICLP-99), pages 124–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Penn</author>
</authors>
<title>The Algebraic Structure ofAttributed Type Signatures.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="12475" citStr="Penn, 2000" startWordPosition="2011" endWordPosition="2012">ignature changes, this method’s efficiency relies on the assumption that most pairs of types are join-incompatible no matter how the signature changes. In LinGO, this is indeed the case: of the 11,655,396 possible pairs, 11,624,866 are join-incompatible, and there are only 3,306 that are consistent (with or without joins) and do not stand in a subtyping or identity relationship. In fact, the cost of completion is often dominated by the cost of transitive closure, which, using a sparse matrix representation, can be completed for LinGO in about 9 seconds on a 450 MHz Pentium II with 1GB memory (Penn, 2000a). While the continued efficiency of compile-time completion of signatures as they further increase in size can only be verified empirically, what can be said at this stage is that the only reason that signatures like LinGO can be tractably compiled at all is sparseness of consistent types. In other geometric respects, it bears a close enough resemblance to the theoretical worst case to cause concern about scalability. Compilation, if efficient, is to be preferred from the standpoint of static error detection, which incremental methods may elect to skip. In addition, running a new signature p</context>
<context position="22856" citStr="Penn (2000" startWordPosition="3628" endWordPosition="3629">r (1997)) library, which has the ability not only to suspend, but to suspend until a particular variable is instantiated or even bound to another variable. This is the powerful kind of mechanism required to check these constraints efficiently, i.e., only when necessary. Re-entrancies in a Prolog term encoding of feature structures, such as the one ALE uses (Penn, 1999), may only show up as the binding of two uninstantiated variables, and re-entrancies are often an important case where these constraints need to be checked. The details of this reduction to constraint handling rules are given in Penn (2000b). The relevant complexity-theoretic issue is the detection of deranged types. 4.3 Detecting Deranged Types The detection of deranged types themselves is also a potential problem. This is something that needs to be detected at compile-time when subtype covering constraints are generated, and as small changes in a partial order of types can have drastic effects on other parts of the signature because of appropriateness, incremental compilation of the grammar signature itself can be extremely difficult. This means that the detection of deranged types must be something that can be performed very</context>
</contexts>
<marker>Penn, 2000</marker>
<rawString>G. Penn. 2000a. The Algebraic Structure ofAttributed Type Signatures. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Penn</author>
</authors>
<title>Applying Constraint Handling Rules to HPSG.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Conference on Computational Logic (CL2000), Workshop on Rule-Based Constraint Reasoning and Programming,</booktitle>
<location>London, UK.</location>
<contexts>
<context position="12475" citStr="Penn, 2000" startWordPosition="2011" endWordPosition="2012">ignature changes, this method’s efficiency relies on the assumption that most pairs of types are join-incompatible no matter how the signature changes. In LinGO, this is indeed the case: of the 11,655,396 possible pairs, 11,624,866 are join-incompatible, and there are only 3,306 that are consistent (with or without joins) and do not stand in a subtyping or identity relationship. In fact, the cost of completion is often dominated by the cost of transitive closure, which, using a sparse matrix representation, can be completed for LinGO in about 9 seconds on a 450 MHz Pentium II with 1GB memory (Penn, 2000a). While the continued efficiency of compile-time completion of signatures as they further increase in size can only be verified empirically, what can be said at this stage is that the only reason that signatures like LinGO can be tractably compiled at all is sparseness of consistent types. In other geometric respects, it bears a close enough resemblance to the theoretical worst case to cause concern about scalability. Compilation, if efficient, is to be preferred from the standpoint of static error detection, which incremental methods may elect to skip. In addition, running a new signature p</context>
<context position="22856" citStr="Penn (2000" startWordPosition="3628" endWordPosition="3629">r (1997)) library, which has the ability not only to suspend, but to suspend until a particular variable is instantiated or even bound to another variable. This is the powerful kind of mechanism required to check these constraints efficiently, i.e., only when necessary. Re-entrancies in a Prolog term encoding of feature structures, such as the one ALE uses (Penn, 1999), may only show up as the binding of two uninstantiated variables, and re-entrancies are often an important case where these constraints need to be checked. The details of this reduction to constraint handling rules are given in Penn (2000b). The relevant complexity-theoretic issue is the detection of deranged types. 4.3 Detecting Deranged Types The detection of deranged types themselves is also a potential problem. This is something that needs to be detected at compile-time when subtype covering constraints are generated, and as small changes in a partial order of types can have drastic effects on other parts of the signature because of appropriateness, incremental compilation of the grammar signature itself can be extremely difficult. This means that the detection of deranged types must be something that can be performed very</context>
</contexts>
<marker>Penn, 2000</marker>
<rawString>G. Penn. 2000b. Applying Constraint Handling Rules to HPSG. In Proceedings of the First International Conference on Computational Logic (CL2000), Workshop on Rule-Based Constraint Reasoning and Programming, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<date>1994</date>
<booktitle>Head-driven Phrase Structure Grammar.</booktitle>
<location>Chicago.</location>
<contexts>
<context position="1142" citStr="Pollard and Sag 1994" startWordPosition="168" endWordPosition="171">absence of subtype covering. It also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist. 1 Introduction The logic of typed feature structures (LTFS, Carpenter 1992) and, in particular, its implementation in the Attribute Logic Engine (ALE, Carpenter and Penn 1996), have been widely used as a means of formalising and developing grammars of natural languages that support computationally efficient parsing and SLD resolution, notably grammars within the framework of Headdriven Phrase Structure Grammar (HPSG, Pollard and Sag 1994). These grammars are formulated using a vocabulary provided by a finite partially ordered set of types and a set of features that must be specified for each grammar, and feature structures in these grammars must respect certain constraints that are also specified. These include appropriateness conditions, which specify, for each type, all and only the features that take values in feature structures of that type, and with which types of values (value restrictions). There are also more general implicational constraints of the form , where is a type, and is an expression from LTFS’s description l</context>
<context position="3641" citStr="Pollard and Sag (1994)" startWordPosition="567" endWordPosition="570">a unique most general type to which F is appropriate. 3. No subtype covering: there can be feature structures of a non-maximally-specific type that are not typable as any of its maximally specific subtypes. When subtype covering is not assumed, feature structures themselves can be partially ordered and taken to represent partial information states about some set of objects. When subtype covering is assumed, feature structures are discretely ordered and totally informative, and can be taken to represent objects in the (linguistic) world themselves. The latter interpretation is subscribed to by Pollard and Sag (1994), for example. All three of these conditions have been claimed elsewhere to be either intractable or impossible to restore in grammar signatures where they do not already exist. It will be argued here that: (1) restoring meet-semi-latticehood is theoretically intractable, for which the worst case bears a disquieting resemblance to actual practice in current large-scale grammar signatures, but nevertheless can be efficiently compilable in practice due to the sparseness of consistent types; (2) unique feature introduction can always be restored to a signature in low-degree polynomial time, and (</context>
<context position="14118" citStr="Pollard and Sag (1994)" startWordPosition="2278" endWordPosition="2281">the remaining cost amortised evenly across all run-time unifications, which often results in a savings during grammar development. 3 Unique Feature Introduction LTFS and ALE also assume that appropriateness guarantees the existence of a unique introducer for every feature: Definition 3 Given a type hierarchy, , and a finite set offeatures, Feat, an appropriateness specification is a partial function, such that, for every F : (Feature Introduction) there is a type F such that: – F F ✗, and Feature introduction has been argued not to be appropriate for certain empirical domains either, although Pollard and Sag (1994) do otherwise observe it. The debate, however, has focussed on whether to modify some other aspect of type inferencing in order to compensate for the lack of feature introduction, presumably under the assumption that feature introduction was difficult or impossible to restore automatically to grammar signatures that did not have it. F , and (Upward Closure / Right Monotonicity) if F and , then F and F F . – for every , if F ✗, then 1. Find two elements, with minimal upper bounds, , such that their join is undefined, i.e., . If no such pair exists, then stop. 2. Add an element, , such that: for</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. Sag. 1994. Head-driven Phrase Structure Grammar. Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Sag</author>
</authors>
<title>English relative clause constructions.</title>
<date>1997</date>
<journal>Journal ofLinguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="8278" citStr="Sag (1997)" startWordPosition="1345" endWordPosition="1346"> one family of worst-case examples, parametrised by , consider a set , and a partial order defined as all of the size subsets of and all of the size subsets of , ordered by inclusion. Figure 2 shows the case where . Although the maximum subtype and supertype branching factors in this family increase linearly with size, the partial orders can grow in depth instead in order to contain this. That yields something roughly of the form shown in Figure 3, which is an example of a recent trend in using type-intensive encodings of linguistic information into typed feature logic in HPSG, beginning with Sag (1997). These explicitly isolate several dimensions1 of analysis as a means of classifying complex linguistic objects. In Figure 3, specific clausal types are selected from among the possible combinations of CLAUSALITY and HEADEDNESS subtypes. In this setting, the parameter corresponds roughly to the number of dimensions used, although an exponential explosion is obviously not dependent on reading the type hierarchy according to this convention. There is a simple algorithm for performing this completion, which assumes the prior existence of a most general element ( ), given in Figure 4. 1It should b</context>
</contexts>
<marker>Sag, 1997</marker>
<rawString>I. A. Sag. 1997. English relative clause constructions. Journal ofLinguistics, 33(2):431–484.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>