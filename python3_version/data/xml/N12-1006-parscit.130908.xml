<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000110">
<title confidence="0.975576">
Machine Translation of Arabic Dialects
</title>
<author confidence="0.9606715">
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan†, Chris Callison-Burch$
</author>
<affiliation confidence="0.961617">
Raytheon BBN Technologies, Cambridge MA
†Microsoft Research, Redmond WA
$Johns Hopkins University, Baltimore MD
</affiliation>
<sectionHeader confidence="0.976828" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.983714166666667">
Arabic Dialects present many challenges for
machine translation, not least of which is the
lack of data resources. We use crowdsourc-
ing to cheaply and quickly build Levantine-
English and Egyptian-English parallel cor-
pora, consisting of 1.1M words and 380k
words, respectively. The dialectal sentences
are selected from a large corpus of Arabic web
text, and translated using Amazon’s Mechan-
ical Turk. We use this data to build Dialec-
tal Arabic MT systems, and find that small
amounts of dialectal data have a dramatic im-
pact on translation quality. When translating
Egyptian and Levantine test sets, our Dialec-
tal Arabic MT system performs 6.3 and 7.0
BLEU points higher than a Modern Standard
Arabic MT system trained on a 150M-word
Arabic-English parallel corpus.
</bodyText>
<sectionHeader confidence="0.99895" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999892947368421">
The Arabic language is a well-known example of
diglossia (Ferguson, 1959), where the formal vari-
ety of the language, which is taught in schools and
used in written communication and formal speech
(religion, politics, etc.) differs significantly in its
grammatical properties from the informal varieties
that are acquired natively, which are used mostly for
verbal communication. The spoken varieties of the
Arabic language (which we refer to collectively as
Dialectal Arabic) differ widely among themselves,
depending on the geographic distribution and the
socio-economic conditions of the speakers, and they
diverge from the formal variety known as Mod-
ern Standard Arabic (MSA) (Embarki and Ennaji,
2011). Significant differences in the phonology,
morphology, lexicon and even syntax render some
of these varieties mutually incomprehensible.
The use of Dialectal Arabic has traditionally been
confined to informal personal speech, while writ-
</bodyText>
<page confidence="0.991506">
49
</page>
<bodyText confidence="0.99976665">
ing has been done almost exclusively using MSA
(or its ancestor Classical Arabic). This situation is
quickly changing, however, with the rapid prolifer-
ation of social media in the Arabic-speaking part
of the world, where much of the communication
is composed in dialect. The focus of the Arabic
NLP research community, which has been mostly on
MSA, is turning towards dealing with informal com-
munication, with the introduction of the DARPA
BOLT program. This new focus presents new chal-
lenges, the most obvious of which is the lack of di-
alectal linguistic resources. Dialectal text, which is
usually user-generated, is also noisy, and the lack
of standardized orthography means that users often
improvise spelling. Dialectal data also includes a
wider range of topics than formal data genres, such
as newswire, due to its informal nature. These chal-
lenges require innovative solutions if NLP applica-
tions are to deal with Dialectal Arabic effectively.
In this paper:
</bodyText>
<listItem confidence="0.5927252">
• We describe a process for cheaply and quickly
developing parallel corpora for Levantine-
English and Egyptian-English using Amazon’s
Mechanical Turk crowdsourcing service (§3).
• We use the data to perform a variety of machine
</listItem>
<bodyText confidence="0.971532166666667">
translation experiments showing the impact of
morphological analysis, the limited value of
adding MSA parallel data, the usefulness of
cross-dialect training, and the effects of trans-
lating from dialect to MSA to English (§4).
We find that collecting dialect translations has a low
cost ($0.03/word) and that relatively small amounts
of data has a dramatic impact on translation quality.
When trained on 1.5M words of dialectal data, our
system performs 6.3 to 7.0 BLEU points higher than
when it is trained on 100 times more MSA data from
a mismatching domain.
</bodyText>
<note confidence="0.502405">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49–59,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.97845" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999941342465754">
Existing work on natural language processing of Di-
alectal Arabic text, including machine translation, is
somewhat limited. Previous research on Dialectal
Arabic MT has focused on normalizing dialectal in-
put words into MSA equivalents before translating
to English, and they deal with inputs that contain
a limited fraction of dialectal words. Sawaf (2010)
normalized the dialectal words in a hybrid (rule-
based and statistical) MT system, by performing a
combination of character- and morpheme-level map-
pings. They then translated the normalized source
to English using a hybrid MT or alternatively a
Statistical MT system. They tested their method
on proprietary test sets, observing about 1 BLEU
point (Papineni et al., 2002) increase on broadcast
news/conversation and about 2 points on web text.
Salloum and Habash (2011) reduced the proportion
of dialectal out-of-vocabulary (OOV) words also by
mapping their affixed morphemes to MSA equiva-
lents (but did not perform lexical mapping on the
word stems). They allowed for multiple morpho-
logical analyses, passing them on to the MT system
in the form of a lattice. They tested on a subset of
broadcast news and broadcast conversation data sets
consisting of sentences that contain at least one re-
gion marked as non-MSA, with an initial OOV rate
against an MSA training corpus of 1.51%. They
obtained a 0.62 BLEU point gain. Abo Bakr et
al. (2008) suggested another hybrid system to map
Egyptian Arabic to MSA, using morphological anal-
ysis on the input and an Egyptian-MSA lexicon.
Other work that has focused on tasks besides MT
includes that of Chiang et al. (2006), who built a
parser for spoken Levantine Arabic (LA) transcripts
using an MSA treebank. They used an LA-MSA
lexicon in addition to morphological and syntac-
tic rules to map the LA sentences to MSA. Riesa
and Yarowsky (2006) built a statistical morphologi-
cal segmenter for Iraqi and Levantine speech tran-
scripts, and showed that they outperformed rule-
based segmentation with small amounts of training.
Some tools exist for preprocessing and tokenizing
Arabic text with a focus on Dialectal Arabic. For ex-
ample, MAGEAD (Habash and Rambow, 2006) is a
morphological analyzer and generator that can ana-
lyze the surface form of MSA and dialect words into
their root/pattern and affixed morphemes, or gener-
ate the surface form in the opposite direction.
Amazon’s Mechanical Turk (MTurk) is becom-
ing an essential tool for creating annotated resources
for computational linguistics. Callison-Burch and
Dredze (2010) provide an overview of various tasks
for which MTurk has been used, and offer a set of
best practices for ensuring high-quality data.
Zaidan and Callison-Burch (2011a) studied the
quality of crowdsourced translations, by quantifying
the quality of non-professional English translations
of 2,000 Urdu sentences that were originally trans-
lated by the LDC. They demonstrated a variety of
mechanisms that increase the translation quality of
crowdsourced translations to near professional lev-
els, with a total cost that is less than one tenth the
cost of professional translation.
Zaidan and Callison-Burch (2011b) created the
Arabic Online Commentary (AOC) dataset, a 52M-
word monolingual dataset rich in dialectal content.
Over 100k sentences from the AOC were annotated
by native Arabic speakers on MTurk to identify the
dialect level (and dialect itself) in each, and the col-
lected labels were used to train automatic dialect
identification systems. Although a large number
of dialectal sentences were identified (41% of sen-
tences), none were passed on to a translation phase.
</bodyText>
<sectionHeader confidence="0.928456" genericHeader="method">
3 Data Collection and Annotation
</sectionHeader>
<bodyText confidence="0.999732533333333">
Following Zaidan and Callison-Burch (2011a,b), we
use MTurk to identify Dialectal Arabic data and to
create a parallel corpus by hiring non-professional
translators to translate the sentences that were la-
beled as being dialectal. We had Turkers perform
three steps for us: dialect classification, sentence
segmentation, and translation.
Since Dialectal Arabic is much less common in
written form than in spoken form, the first challenge
is to simply find instances of written Dialectal Ara-
bic. We draw from a large corpus of monolingual
Arabic text (approximately 350M words) that was
harvested from the web by the LDC, largely from
weblog and online user groups.1 Before present-
ing our data to annotators, we filter it to identify
</bodyText>
<footnote confidence="0.74302">
1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,
LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,
LDC2008E54, LDC2009E14, LDC2009E93.
</footnote>
<page confidence="0.996256">
50
</page>
<figureCaption confidence="0.943793333333333">
Figure 1: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf and
Iraqi. Habash (2010) gives a breakdown along mostly
the same lines. We used this map as an illustration for
annotators in our dialect classification task (Section 3.1),
with Arabic names for the dialects instead of English.
</figureCaption>
<bodyText confidence="0.999856416666667">
segments most likely to be dialectal (unlike Zaidan
and Callison-Burch (2011b), who did no such pre-
filtering). We eliminate documents with a large per-
centage of non-Arabic or MSA words. We then
retain documents that contain some number of di-
alectal words, using a set of manually selected di-
alectal words that was assembled by culling through
the transcripts of the Levantine Fisher and Egyp-
tian CallHome speech corpora. After filtering, the
dataset contained around 4M words, which we used
as a starting point for creating our Dialectal Arabic-
English parallel corpus.
</bodyText>
<subsectionHeader confidence="0.997109">
3.1 Dialect Classification
</subsectionHeader>
<bodyText confidence="0.999973941176471">
To refine the document set beyond our keyword fil-
tering heuristic and to label which dialect each doc-
ument is written in, we hire Arabic annotators on
MTurk to perform classification similar to Zaidan
and Callison-Burch (2011b). Annotators were asked
to classify the filtered documents for being in MSA
or in one of four regional dialects: Egyptian, Lev-
antine, Gulf/Iraqi or Maghrebi, and were shown the
map in Figure 1 to explain what regions each of the
dialect labels corresponded to. We allowed an addi-
tional “General” dialect option for ambiguous docu-
ments. Unlike Zaidan and Callison-Burch, our clas-
sification was applied to whole documents (corre-
sponding to a user online posting) instead of individ-
ual sentences. To perform quality control, we used
a set of documents for which correct labels were
known. We presented these 20% of the time, and
</bodyText>
<table confidence="0.999141833333333">
Dialect Classification HIT $10,064
Sentence Segmentation HIT $1,940
Translation HIT $32,061
Total cost $44,065
Num words translated 1,516,856
Cost per word 2.9 cents/word
</table>
<tableCaption confidence="0.940518333333333">
Table 1: The total costs for the three MTurk subtasks in-
volved with the creation of our Dialectal Arabic-English
parallel corpus.
</tableCaption>
<bodyText confidence="0.9929465">
eliminated workers who did not correctly classify
them (2% of labels).
Identifying the dialect of a text snippet can be
challenging in the absence of phonetic cues. We
therefore required 3 classifications from different
workers for every document, and accepted a dialect
label if at least two of them agreed. The dialect dis-
tribution of the final output was: 43% Gulf/Iraqi,
28% Levantine, 11% Egyptian, and 16% could not
be classified. MSA and the other labels accounted
for 2%. We decided to translate only the Levantine
and Egyptian documents, since the pool of MTurk
workers contained virtually no workers from Iraq or
the Gulf region.
</bodyText>
<subsectionHeader confidence="0.999744">
3.2 Sentence Segmentation
</subsectionHeader>
<bodyText confidence="0.9999764375">
Since the data we annotated was mostly user-
generated informal web content, the existing punc-
tuation was often insufficient to determine sentence
boundaries. Since sentence boundaries are impor-
tant for correct translation, we segmented passages
into individual sentences using MTurk. We only re-
quired sentences longer than 15 words to be seg-
mented, and allowed Turkers to split and rejoin at
any point between the tokens. The instructions were
simply to “divide the Arabic text into individual sen-
tences, where you believe it would be appropriate
to insert a period.” We also used a set of correctly
segmented passages for quality control, and scored
Turkers using a metric based on the precision and
recall of correct segmentation points. The rejection
rate was 1.2%.
</bodyText>
<subsectionHeader confidence="0.998609">
3.3 Translation to English
</subsectionHeader>
<bodyText confidence="0.999062">
Following Zaidan and Callison-Burch (2011a), we
hired non-professional translators on MTurk to
translate the Levantine and Egyptian sentences into
</bodyText>
<page confidence="0.994189">
51
</page>
<table confidence="0.921302">
Sentence Arabic English
Data Set Pairs Tokens Tokens
MSA-150MW 8.0M 151.4M 204.4M
Dialect-1500KW 180k 1,545,053 2,257,041
MSA-1300KW 71k 1,292,384 1,752,724
MSA-Web-Tune 6,163 145,260 184,185
MSA-Web-Test 5,454 136,396 172,357
Lev-Web-Tune 2,600 20,940 27,399
Lev-Web-Test 2,600 21,092 27,793
Egy-Web-Test 2,600 23,671 33,565
E-Facebook-Tune 3,351 25,130 34,753
E-Facebook-Test 3,188 25,011 34,244
</table>
<tableCaption confidence="0.982135666666667">
Table 2: Statistics about the training/tuning/test datasets
used in our experiments. The token counts are calculated
before MADA segmentation.
</tableCaption>
<bodyText confidence="0.999663">
English. Among several quality control measures,
we rendered the Arabic sentences as images to pre-
vent Turkers from simply copying the Arabic text
into translation software. We still spot checked the
translations against the output of Google Translate
and Bing Translator. We also rejected gobbledygook
garbage translations that have a high percentage of
words not found in an English lexicon.
We quantified the quality of an individual Turker’s
translations in two ways: first by asking native Ara-
bic speaker judges to score a sample of the Turker’s
translations, and second by inserting control sen-
tences for which we have good reference translations
and measuring the Turker’s METEOR (Banerjee and
Lavie, 2005) and BLEU-1 scores (Papineni et al.,
2002).2 The rejection rate of translation assignments
was 5%. We promoted good translators to a re-
stricted access “preferred worker queue”. They were
paid at a higher rate, and were required to translate
control passages only 10% of the time as opposed
to 20% for general Turkers, thus providing us with a
higher translation yield for unseen data.
Worker turnout was initially slow, but increased
quickly as our reputation for being reliable payers
was established; workers started translating larger
volumes and referring their acquaintances. We had
121 workers who each completed 20 or more trans-
lation assignments. We eventually reached and sus-
tained a rate of 200k words of acceptable quality
2BLEU-1 provided a more reliable correlation with human
judgment in this case that the regular BLEU score (which uses
n-gram orders 1, ... ,4), given the limited size of the sample
measured.
translated per week. Unlike Zaidan and Callison-
Burch (2011a), who only translated 2,000 Urdu sen-
tences, we translated sufficient volumes of Dialectal
Arabic to train machine translation systems. In total,
we had 1.1M words of Levantine and 380k words of
Egyptian translated into English, corresponding to
about 2.3M words on the English side.
Table 1 outlines the costs involved with creating
our parallel corpus. The total cost was $44k, or
$0.03/word – an order of magnitude cheaper than
professional translation.
</bodyText>
<sectionHeader confidence="0.864289" genericHeader="method">
4 Experiments in Dialectal Arabic-English
Machine Translation
</sectionHeader>
<bodyText confidence="0.999922363636364">
We performed a set of experiments to contrast sys-
tems trained using our dialectal parallel corpus with
systems trained on a (much larger) MSA-English
parallel corpus. All experiments use the same meth-
ods for training, decoding and parameter tuning, and
we only varied the corpora used for training, tun-
ing and testing. The MT system we used is based
on a phrase-based hierarchical model similar to that
of Shen et al. (2008). We used GIZA++ (Och and
Ney, 2003) to align sentences and extract hierar-
chical rules. The decoder used a log-linear model
that combines the scores of multiple feature scores,
including translation probabilities, smoothed lexi-
cal probabilities, a dependency tree language model,
in addition to a trigram English language model.
Additionally, we used 50,000 sparse, binary-valued
source and target features based on Chiang et al.
(2009). The English language model was trained on
7 billion words from the Gigaword and from a web
crawl. The feature weights were tuned to maximize
the BLEU score on a tuning set using the Expected-
BLEU optimization procedure (Devlin, 2009).
The Dialectal Arabic side of our corpus consisted
of 1.5M words (1.1M Levantine and 380k Egyp-
tian). Table 2 gives statistics about the various
train/tune/test splits we used in our experiments.
Since the Egyptian set was so small, we split it only
to training/test sets, opting not to have a tuning set.
The MSA training data we used consisted of Arabic-
English corpora totaling 150M tokens (Arabic side).
The MSA train/tune/test sets were constructed for
the DARPA GALE program.
We report translation quality in terms of BLEU
</bodyText>
<page confidence="0.98345">
52
</page>
<table confidence="0.999768888888889">
Training Tuning Simple Segment MADA Segment OBLEU DOOV
BLEU OOV BLEU OOV
MSA-Web-Test
MSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%
MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%
Egyptian-Web-Test
Dialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%
Levantine-Web-Test
Dialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%
</table>
<tableCaption confidence="0.998015333333333">
Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal
Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are
more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
</tableCaption>
<table confidence="0.999894222222222">
Training Tuning BLEU OOV BLEU OOV BLEU OOV
Egyptian-Web-Test Levantine-Web-Test MSA-Web-Test
MSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%
MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%
MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%
Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%
Egyptian-360KW 19.04 4.62% 11.21 9.00% - -
Levantine-360KW 14.05 7.11% 16.36 5.24% - -
Levantine-1100KW 17.79 4.83% 19.29 3.31% - -
</table>
<tableCaption confidence="0.941807">
Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora.
The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),
since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the
dialectal data results in marginally worse translations.
</tableCaption>
<bodyText confidence="0.867898666666667">
score.3 In addition, we also report the OOV rate of
the test set relative to the training corpus in each ex-
perimental setups.
</bodyText>
<subsectionHeader confidence="0.985753">
4.1 Morphological Decomposition
</subsectionHeader>
<bodyText confidence="0.999754333333333">
Arabic has a complex morphology compared to En-
glish. Preprocessing the Arabic source by morpho-
logical segmentation has been shown to improve the
performance of Arabic MT (Lee, 2004; Habash and
Sadat, 2006) by decreasing the size of the source vo-
cabulary, and improving the quality of word align-
ments. The morphological analyzers that underlie
most segmenters were developed for MSA, but the
different dialects of Arabic share many of the mor-
phological affixes of MSA, and it is therefore not
unreasonable to expect MSA segmentation to also
improve Dialect Arabic to English MT. To test this,
</bodyText>
<footnote confidence="0.612468">
3We also computed TER (Snover et al., 2006) and METEOR
scores, but omit them because they demonstrated similar trends.
</footnote>
<bodyText confidence="0.999515533333333">
we ran experiments using the MADA morpholog-
ical analyzer (Habash and Rambow, 2005). Table
3 shows the effect of applying segmentation to the
text, for both MSA and Dialectal Arabic. The BLEU
score improves uniformly, although the improve-
ments are most dramatic for smaller datasets, which
is consistent with previous work (Habash and Sadat,
2006). Morphological segmentation gives a smaller
gain on dialectal input, which could be due to two
factors: the segmentation accuracy likely decreases
since we are using an unmodified MSA segmenter,
and there is higher variability in the written form of
dialect compared to MSA. Given the significant, al-
beit smaller gain on dialectal input, we use MADA
segmentation in all our experiments.
</bodyText>
<subsectionHeader confidence="0.997576">
4.2 Effect of Dialectal Training Data Size
</subsectionHeader>
<bodyText confidence="0.999661666666667">
We next examine how the size of the dialectal train-
ing data affects MT performance, and whether it is
useful to combine it with MSA training data. We
</bodyText>
<page confidence="0.988589">
53
</page>
<table confidence="0.999228">
A TL Count English Equivalent
4;- Enjd 31 really/for real — Levantine.
ktyyyr 17 a l000t (corruption of MSA kvyrA).
?_P-D AlnEwm 16 The last name (Al-Na&apos;oom) of a forum admin.
s wH$tyny 14 I miss you (spoken to a female) — Egyptian.
yAzmn 11 oh time (space omitted). Appeared within a poem.
bktyr 11 by much (corruption of MSA bkvyr).
mtlk 10 like you (corruption of MSA mvlk).
</table>
<tableCaption confidence="0.998897">
Table 5: The most frequent OOV’s (with counts &gt; 10) of the dialectal test sets against the MSA training data.
</tableCaption>
<table confidence="0.84859792">
Source (EG1): ! ! ؟&amp;quot;4ا Yy ن�0ا al J• &amp;quot;,;ا
Transliteration: Ant btEml lh AElAn wlA Ayh?!!
MSA-Sys. Output: You are working for a declaration
and not?
Dial-Sys. Output: You are making the advertisement
for him or what?
Reference: Are you promoting it or what?!!
Source (EG1): يد �رy��ا ف34 35 au a6 L�&amp;quot;ا ����
Transliteration: nfsYAtm)n Elyh bEd mA $Af
AlSwrh dy
MSA-Sys. Output: Myself feel to see this image.
Dial-Sys. Output: I wish to check on him after
he saw this picture.
Reference: I wish to be sure that he is fine
after he saw this images
Source (LEI): لyyyآ �����آ y-1ا 441
Transliteration: lhyk Aljw ktyyyr kwwwl
MSA-Sys. Output: God you the atmosphere.
Dial-Sys. Output: this is why the weather is so cool
Reference: This is why the weather is so cool
Source (LEI): Oyu �­ Al لyL
Transliteration: Twl bAlk Em nmzH
MSA-Sys. Output: Do you think about a joke long.
Dial-Sys. Output: Calm down we are kidding
Reference: calm down, we are kidding
</table>
<figureCaption confidence="0.952293666666667">
Figure 2: Examples of improvement in MT output when
training on our Dialectal Arabic-English parallel corpus
instead of an MSA-English parallel corpus.
</figureCaption>
<figure confidence="0.45744928">
Source (EGI): ، W &apos; LW 4L �.hu
Transliteration: gAltlp Tb tEAlY nEd ,
MSA-Sys. Output: Medicine almighty promise.
Dial-Sys. Output: She said, OK, come and then
Reference: She told him, OK, lets count them ,
Source (LEI): vlw&apos;ر 01 �IJ P ! - . U; -أو اmo;
Transliteration: jbgrA w&gt;HyAnA bgDyhA Em
&gt;tslY mE rjgAty
MSA-Sys. Output: I read and sometimes with go
with my uncle.
Dial-Sys. Output: So I read, and sometimes I spend
trying to make my self comfort
with my friends
Reference: So i study and sometimes I spend
the time having fun with my friends
Source (LEI): ن���- Yj &apos;AL -او Jآ vlه L14—L . Aا
سو� Jai
Transliteration: Allh ysAmHkn hlg kl wAHd TAlb
grb bykwn bdw Erws
MSA-Sys. Output: God now each student near the
Bedouin bride.
Dial-Sys. Output: God forgive you, each one is a
close student would want the bride
Reference: God forgive you. Is every one
asking to be close, want a bride!
</figure>
<figureCaption confidence="0.907627">
Figure 3: Examples of ambiguous words that are trans-
lated incorrectly by the MSA-English system, but cor-
rectly by the Dialectal Arabic-English system.
</figureCaption>
<page confidence="0.905798">
54
</page>
<figure confidence="0.984379">
0k 200k 400k 800k 1500k
Dialect Training (No. of Words)
Egyptian web test
0k 200k 400k 800k 1500k
Dialect Training (No. of Words)
Levantine web test
</figure>
<figureCaption confidence="0.9890742">
Figure 4: Learning curves showing the effects of increas-
ing the size of dialectal training data, when combined
with the 150M-word MSA parallel corpus, and when
used alone. Adding the MSA training data is only use-
ful when the dialectal data is scarce (200k words).
</figureCaption>
<bodyText confidence="0.999897230769231">
started with a baseline system trained on the 150M-
word MSA parallel corpus, and added various sized
portions of the dialect parallel corpus to it. Figure 4
shows the resulting learning curve, and compares it
to the learning curve for a system trained solely on
the dialectal parallel corpus. When only 200k words
of dialectal data are available, combining it with the
150M-word MSA corpus results in improved BLEU
scores, adding 0.8–1.5 BLEU points. When 400k
words or more of dialectal data are available, the
MSA training data ceases to provide any gain, and
in fact starts to hurt the performance.
The performance of a system trained on the 1.5M-
word dialectal data is dramatically superior to a sys-
tem that uses only the 150M-word MSA data: +6.32
BLEU points on the Egyptian test set, or 44% rela-
tive gain, and +7.00 BLEU points on the Levantine
test set, or 57% relative gain (fourth line vs. second
line of Table 4). In Section 4.4, we show that those
gains are not an artifact of the similarity between test
and training datasets, or of using the same translator
pool to translate both sets.
Inspecting the difference in the outputs of the Di-
alectal vs. MSA systems, we see that the improve-
ment in score is a reflection of a significant improve-
ment in the quality of translations. Figure 2 shows
a few examples of sentences whose translations im-
prove significantly using the Dialectal system. Fig-
ure 3 shows a particularly interesting category of ex-
amples. Many words are homographs, with different
meanings (and usually different pronunciations) in
MSA vs. one or more dialects. The bolded tokens
in the sentences in Figure 3 are examples of such
words. They are translated incorrectly by the MSA
system, while the dialect system translates them cor-
rectly.4 If we examine the most frequent OOV words
against the MSA training data (Table 5), we find a
number of corrupted MSA words and names, but
that a majority of OOVs are dialect words.
</bodyText>
<subsectionHeader confidence="0.996926">
4.3 Cross-Dialect Training
</subsectionHeader>
<bodyText confidence="0.999964928571429">
Since MSA training data appeared to have little ef-
fect when translating dialectal input, we next inves-
tigated the effect of training data from one dialect on
translating the input of another dialect. We trained a
system with the 360k-word Egyptian training subset
of our dialectal parallel corpus, and another system
with a similar amount of Levantine training data. We
used each system to translate the test set of the other
dialect. As expected, a system performs better when
it translates a test set in the same dialect that it was
trained on (Table 4).
That said, since the Egyptian training set is so
small, adding the (full) Levantine training data im-
proves performance (on the Egyptian test set) by
1.62 BLEU points, compared to using only Egyp-
tian training data. In fact, using the Levantine
training data by itself outperforms the MSA-trained
system on the Egyptian test set by more than 3
BLEU points. (For the Levantine test set, adding
the Egyptian training data has no affect, possibly
due to the small amount of Egyptian data.) This
may suggest that the mismatch between dialects is
less severe than the mismatch between MSA and
dialects. Alternatively, the differences may be due
to the changes in genre from the MSA parallel cor-
pus (which is mainly formal newswire) to the news-
groups and weblogs that mainly comprise the dialec-
tal corpus.
</bodyText>
<footnote confidence="0.981674">
4The word nfsY of Figure 2 (first word of second example)
is also a homograph, as it means myself in MSA and I wish in
Dialectal Arabic.
</footnote>
<figure confidence="0.9979521875">
MSA+Dialect
Dialect
BLEU 22
BLEU 20
18
16
14
12
22
20
18
16
14
12
MSA+Dialect
Dialect
</figure>
<page confidence="0.990704">
55
</page>
<table confidence="0.999863777777778">
Training Tuning BLEU OOV
MSA-150MW Levantine-Web 13.80 4.16%
MSA-150MW+Dialect-1500KW 16.71 2.43%
Dialect-1500KW 15.75 3.79%
MSA-150MW Egyptian-Facebook 15.80 4.16%
MSA-150MW+Dialect-1500KW 18.50 2.43%
Dialect-1500KW 17.90 3.79%
Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%
Dialect-1000KW (no Turker overlap) 17.10 4.60%
</table>
<tableCaption confidence="0.977362333333333">
Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are
entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:
+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
</tableCaption>
<subsectionHeader confidence="0.997633">
4.4 Validation on Independent Test Data
</subsectionHeader>
<bodyText confidence="0.999985805555555">
To eliminate the possibility that the gains are solely
due to similarity between the test/training sets in the
dialectal data, we ran experiments using the same
dialectal training data, but using truly independent
test/tuning data sets selected at random from a larger
set of monolingual data that we collected from pub-
lic Egyptian Facebook pages. This data consists of
a set of original user postings and the subsequent
comments on each, giving the data a more conversa-
tional style than our other test sets. The postings
deal with current Egyptian political affairs, sports
and other topics. The test set we selected consisted
of 25,011 words (3,188 comments and 427 postings
from 86 pages), and the tuning set contained 25,130
words (3,351 comments and 415 conversations from
58 pages). We obtained reference translations for
those using MTurk as well.
Table 6 shows that using the 1.5M-word dialect
parallel corpus for training yields a 2 point BLEU
improvement over using the 150M-word MSA cor-
pus. Adding the MSA training data does yield an
improvement, though of less than a single BLEU
point. It remains true that training on 1.5M words
of dialectal data is better than training on 100 times
more MSA parallel data. The system performance
is sensitive to the tuning set choice, and improves
when it matches the test set in genre and origin.
To eliminate another potential source of artificial
bias, we also performed an experiment where we
removed any training translation contributed by a
Turker who translated any sentence in the Egyptian
Facebook set, to eliminate translator bias. For this,
we were left with 1M words of dialect training data.
This gave the same BLEU score as when training
with a randomly selected subset of the same size
(bottom part of Table 6).
</bodyText>
<subsectionHeader confidence="0.9967625">
4.5 Mapping from Dialectal Arabic to MSA
Before Translating to English
</subsectionHeader>
<bodyText confidence="0.999937172413793">
Given the large amount of linguistic resources that
have been developed for MSA over the past years,
and the extensive research that was conducted on
machine translation from MSA to English and other
languages, an obvious research question is whether
Dialectal Arabic is best translated to English by first
pivoting through MSA, rather than directly. The
proximity of Dialectal Arabic to MSA makes the
mapping in principle easier than general machine
translation, and a number of researchers have ex-
plored this direction (Salloum and Habash, 2011).
In this scenario, the dialectal source would first be
automatically transformed to MSA, using either a
rule-based or statistical mapping module.
The Dialectal Arabic-English parallel corpus we
created presents a unique opportunity to compare
the MSA-pivoting approach against direct transla-
tion. First, we collected equivalent MSA data for
the Levantine Web test and tuning sets, by asking
Turkers to transform dialectal passages to valid and
fluent MSA. Turkers were shown example transfor-
mations, and we encouraged fewer changes where
applicable (e.g. morphological rather than lexical
mapping), but allowed any editing operation in gen-
eral (deletion, substitution, reordering). Sample sub-
missions were independently shown to native Ara-
bic speaking judges, who confirmed they were valid
MSA. A low OOV rate also indicated the correctness
of the mappings. By manually transforming the test
</bodyText>
<page confidence="0.990072">
56
</page>
<table confidence="0.999761857142857">
Training BLEU OOV BLEU OOV DBLEU DOOV
Direct dialect trans Map to MSA then trans
MSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%
MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%
MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%
MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%
MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%
</table>
<tableCaption confidence="0.921442428571429">
Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,
versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it
is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system,
the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
dialectal sentence into MSA, we establish an opti-
mistic estimate of what could be done automatically.
Table 7 compares direct translation versus piv-
</tableCaption>
<bodyText confidence="0.989925352941177">
oting to MSA before translating, using the base-
line MSA-English MT system.5 The performance
of the system improves by 2.3 BLEU points with
dialect-to-MSA pivoting, compared to attempting to
translate the untransformed dialectal input directly.
As we add more dialectal training data, the BLEU
score when translating the untransformed dialect
test set improves rapidly (as seen previously in the
MSA+Dialect learning curve in Figure 4), while the
improvement is less rapid when the text is first trans-
formed to MSA. Direct translation becomes a better
option than mapping to MSA once 400k words of di-
alectal data are added, despite the significantly lower
OOV rate with MSA-mapping. This indicates that
simple vocabulary coverage is not sufficient, and
data domain mismatch, quantified by more complex
matching patterns, is more important.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.98503706122449">
We have described a process for building a Dialec-
tal Arabic-English parallel corpus, by selecting pas-
sages with a relatively high percentage of non-MSA
words from a monolingual Arabic web text corpus,
then using crowdsourcing to classify them by di-
alect, segment them into individual sentences and
translate them to English. The process was success-
fully scaled to the point of reaching and sustaining a
rate of 200k translated words per week, at 1/10 the
cost of professional translation. Our parallel corpus,
consisting of 1.5M words, was produced at a total
5The systems in each column of the table are tuned consis-
tently, using their corresponding tuning sets.
cost of $40k, or roughly $0.03/word.
We used the parallel corpus we constructed to
analyze the behavior of a Dialectal Arabic-English
MT system as a function of the size of the dialec-
tal training corpus. We showed that relatively small
amounts of training data render larger MSA corpora
from different data genres largely ineffective for this
test data. In practice, a system trained on the com-
bined Dialectal-MSA data is likely to give the best
performance, since informal Arabic data is usually
a mixture of Dialectal Arabic and MSA. An area of
future research is using the output of a dialect clas-
sifier, or other features to bias the translation model
towards the Dialectal or the MSA parts of the data.
We also validated the models built from the di-
alectal corpus by using them to translate an inde-
pendent data set collected from Egyptian Facebook
public pages. We finally investigated using MSA
as a “pivot language” for Dialectal Arabic-English
translation, by simulating automatic dialect-to-MSA
mapping using MTurk. We obtained limited gains
from mapping the input to MSA, even when the
mapping is of good quality, and only at lower train-
ing set sizes. This suggests that the mismatch be-
tween training and test data is an important aspect of
the problem, beyond simple vocabulary coverage.
The aim of this paper is to contribute to setting
the direction of future research on Dialectal Arabic
MT. The gains we observed from using MSA mor-
phological segmentation can be further increased
with dialect-specific segmenters. Input preprocess-
ing can also be used to decrease the noise of the
user-generated data. Topic adaptation is another im-
portant problem to tackle if the large MSA linguistic
resources already developed are to be leveraged for
Dialectal Arabic-English MT.
</bodyText>
<page confidence="0.998066">
57
</page>
<sectionHeader confidence="0.998328" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999031777777778">
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program, and in part by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme). The views expressed are those
of the authors and do not reflect the official policy
or position of the Department of Defense or the U.S.
Government. Distribution Statement A (Approved
for Public Release, Distribution Unlimited).
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999890054347826">
Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting writ-
ten Egyptian colloquial dialect into diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008, Cairo, Egypt.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In In Proc. ofACL
2005 Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, Ann Arbor,
Michigan.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon’s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 1–12, Los Angeles,
June.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Trento, Italy.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ’09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master’s thesis, University of Mary-
land, December.
Mohamed Embarki and Moha Ennaji, editors. 2011.
Modern Trends in Arabic Dialectology. The Red Sea
Press.
Charles A. Ferguson. 1959. Diglossia. Word, 15:325–
340.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics (ACL),
Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics, New York,
New York.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan &amp; Claypool.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL ’04:
Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with applica-
tions to machine translation. In Proceedings of the 7th
Conf. of the Association for Machine Translation in the
Americas (AMTA 2006), Cambridge, MA.
Wael Salloum and Nizar Habash. 2011. Dialectal to stan-
dard Arabic paraphrasing to improve Arabic-English
statistical machine translation. In Proceedings of the
2011 Conference of Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Conf. of
the Association for Machine Translation in the Ameri-
cas (AMTA 2010), Denver, Colorado.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577–585, Columbus, Ohio.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
In Proceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA 2006),
pages 223–231, Cambridge, MA.
</reference>
<page confidence="0.983621">
58
</page>
<reference confidence="0.997885307692308">
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37–41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220–
1229, Portland, Oregon, June.
</reference>
<page confidence="0.999263">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.097057">
<title confidence="0.99973">Machine Translation of Arabic Dialects</title>
<author confidence="0.548409">Rabih Zbib</author>
<author confidence="0.548409">Erika Malchiodi</author>
<author confidence="0.548409">Jacob Devlin</author>
<author confidence="0.548409">David Stallard</author>
<author confidence="0.548409">Spyros Schwartz</author>
<author confidence="0.548409">John Makhoul</author>
<author confidence="0.548409">Omar F Chris Raytheon BBN Technologies</author>
<author confidence="0.548409">Cambridge</author>
<affiliation confidence="0.7618535">Research, Redmond Hopkins University, Baltimore MD</affiliation>
<abstract confidence="0.997377533333333">Arabic Dialects present many challenges for machine translation, not least of which is the lack of data resources. We use crowdsourcing to cheaply and quickly build Levantine- English and Egyptian-English parallel corpora, consisting of 1.1M words and 380k words, respectively. The dialectal sentences are selected from a large corpus of Arabic web text, and translated using Amazon’s Mechanical Turk. We use this data to build Dialectal Arabic MT systems, and find that small amounts of dialectal data have a dramatic impact on translation quality. When translating Egyptian and Levantine test sets, our Dialec-</abstract>
<note confidence="0.86239475">tal Arabic MT system performs 6.3 and 7.0 BLEU points higher than a Modern Standard Arabic MT system trained on a 150M-word Arabic-English parallel corpus.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hitham M Abo Bakr</author>
<author>Khaled Shaalan</author>
<author>Ibrahim Ziedan</author>
</authors>
<title>A hybrid approach for converting written Egyptian colloquial dialect into diacritized Arabic.</title>
<date>2008</date>
<booktitle>In The 6th International Conference on Informatics and Systems, INFOS2008,</booktitle>
<location>Cairo, Egypt.</location>
<contexts>
<context position="5424" citStr="Bakr et al. (2008)" startWordPosition="834" endWordPosition="837">points on web text. Salloum and Habash (2011) reduced the proportion of dialectal out-of-vocabulary (OOV) words also by mapping their affixed morphemes to MSA equivalents (but did not perform lexical mapping on the word stems). They allowed for multiple morphological analyses, passing them on to the MT system in the form of a lattice. They tested on a subset of broadcast news and broadcast conversation data sets consisting of sentences that contain at least one region marked as non-MSA, with an initial OOV rate against an MSA training corpus of 1.51%. They obtained a 0.62 BLEU point gain. Abo Bakr et al. (2008) suggested another hybrid system to map Egyptian Arabic to MSA, using morphological analysis on the input and an Egyptian-MSA lexicon. Other work that has focused on tasks besides MT includes that of Chiang et al. (2006), who built a parser for spoken Levantine Arabic (LA) transcripts using an MSA treebank. They used an LA-MSA lexicon in addition to morphological and syntactic rules to map the LA sentences to MSA. Riesa and Yarowsky (2006) built a statistical morphological segmenter for Iraqi and Levantine speech transcripts, and showed that they outperformed rulebased segmentation with small </context>
</contexts>
<marker>Bakr, Shaalan, Ziedan, 2008</marker>
<rawString>Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan. 2008. A hybrid approach for converting written Egyptian colloquial dialect into diacritized Arabic. In The 6th International Conference on Informatics and Systems, INFOS2008, Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgments. In</title>
<date>2005</date>
<booktitle>In Proc. ofACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="13475" citStr="Banerjee and Lavie, 2005" startWordPosition="2076" endWordPosition="2079">ges to prevent Turkers from simply copying the Arabic text into translation software. We still spot checked the translations against the output of Google Translate and Bing Translator. We also rejected gobbledygook garbage translations that have a high percentage of words not found in an English lexicon. We quantified the quality of an individual Turker’s translations in two ways: first by asking native Arabic speaker judges to score a sample of the Turker’s translations, and second by inserting control sentences for which we have good reference translations and measuring the Turker’s METEOR (Banerjee and Lavie, 2005) and BLEU-1 scores (Papineni et al., 2002).2 The rejection rate of translation assignments was 5%. We promoted good translators to a restricted access “preferred worker queue”. They were paid at a higher rate, and were required to translate control passages only 10% of the time as opposed to 20% for general Turkers, thus providing us with a higher translation yield for unseen data. Worker turnout was initially slow, but increased quickly as our reputation for being reliable payers was established; workers started translating larger volumes and referring their acquaintances. We had 121 workers </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments. In In Proc. ofACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<location>Los Angeles,</location>
<contexts>
<context position="6550" citStr="Callison-Burch and Dredze (2010)" startWordPosition="1013" endWordPosition="1016">aqi and Levantine speech transcripts, and showed that they outperformed rulebased segmentation with small amounts of training. Some tools exist for preprocessing and tokenizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon’s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data. Zaidan and Callison-Burch (2011a) studied the quality of crowdsourced translations, by quantifying the quality of non-professional English translations of 2,000 Urdu sentences that were originally translated by the LDC. They demonstrated a variety of mechanisms that increase the translation quality of crowdsourced translations to near professional levels, with a total cost that is less than one tenth the cost of professional translation. Zaidan and Callison-Bur</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12, Los Angeles, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Safiullah Shareef</author>
</authors>
<title>Parsing Arabic dialects.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="5644" citStr="Chiang et al. (2006)" startWordPosition="871" endWordPosition="874">d stems). They allowed for multiple morphological analyses, passing them on to the MT system in the form of a lattice. They tested on a subset of broadcast news and broadcast conversation data sets consisting of sentences that contain at least one region marked as non-MSA, with an initial OOV rate against an MSA training corpus of 1.51%. They obtained a 0.62 BLEU point gain. Abo Bakr et al. (2008) suggested another hybrid system to map Egyptian Arabic to MSA, using morphological analysis on the input and an Egyptian-MSA lexicon. Other work that has focused on tasks besides MT includes that of Chiang et al. (2006), who built a parser for spoken Levantine Arabic (LA) transcripts using an MSA treebank. They used an LA-MSA lexicon in addition to morphological and syntactic rules to map the LA sentences to MSA. Riesa and Yarowsky (2006) built a statistical morphological segmenter for Iraqi and Levantine speech transcripts, and showed that they outperformed rulebased segmentation with small amounts of training. Some tools exist for preprocessing and tokenizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyz</context>
</contexts>
<marker>Chiang, Diab, Habash, Rambow, Shareef, 2006</marker>
<rawString>David Chiang, Mona Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic dialects. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Boulder, Colorado.</location>
<contexts>
<context position="15838" citStr="Chiang et al. (2009)" startWordPosition="2449" endWordPosition="2452"> tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU optimization procedure (Devlin, 2009). The Dialectal Arabic side of our corpus consisted of 1.5M words (1.1M Levantine and 380k Egyptian). Table 2 gives statistics about the various train/tune/test splits we used in our experiments. Since the Egyptian set was so small, we split it only to training/test sets, opting not to have a tuning set. The MSA training data we used consisted of ArabicEnglish cor</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In NAACL ’09: Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
</authors>
<title>Lexical features for statistical machine translation. Master’s thesis,</title>
<date>2009</date>
<institution>University of Maryland,</institution>
<contexts>
<context position="16072" citStr="Devlin, 2009" startWordPosition="2491" endWordPosition="2492">nd extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU optimization procedure (Devlin, 2009). The Dialectal Arabic side of our corpus consisted of 1.5M words (1.1M Levantine and 380k Egyptian). Table 2 gives statistics about the various train/tune/test splits we used in our experiments. Since the Egyptian set was so small, we split it only to training/test sets, opting not to have a tuning set. The MSA training data we used consisted of ArabicEnglish corpora totaling 150M tokens (Arabic side). The MSA train/tune/test sets were constructed for the DARPA GALE program. We report translation quality in terms of BLEU 52 Training Tuning Simple Segment MADA Segment OBLEU DOOV BLEU OOV BLEU </context>
</contexts>
<marker>Devlin, 2009</marker>
<rawString>Jacob Devlin. 2009. Lexical features for statistical machine translation. Master’s thesis, University of Maryland, December.</rawString>
</citation>
<citation valid="true">
<date>2011</date>
<booktitle>Modern Trends in Arabic Dialectology. The</booktitle>
<editor>Mohamed Embarki and Moha Ennaji, editors.</editor>
<publisher>Red Sea Press.</publisher>
<contexts>
<context position="4851" citStr="(2011)" startWordPosition="739" endWordPosition="739">MSA equivalents before translating to English, and they deal with inputs that contain a limited fraction of dialectal words. Sawaf (2010) normalized the dialectal words in a hybrid (rulebased and statistical) MT system, by performing a combination of character- and morpheme-level mappings. They then translated the normalized source to English using a hybrid MT or alternatively a Statistical MT system. They tested their method on proprietary test sets, observing about 1 BLEU point (Papineni et al., 2002) increase on broadcast news/conversation and about 2 points on web text. Salloum and Habash (2011) reduced the proportion of dialectal out-of-vocabulary (OOV) words also by mapping their affixed morphemes to MSA equivalents (but did not perform lexical mapping on the word stems). They allowed for multiple morphological analyses, passing them on to the MT system in the form of a lattice. They tested on a subset of broadcast news and broadcast conversation data sets consisting of sentences that contain at least one region marked as non-MSA, with an initial OOV rate against an MSA training corpus of 1.51%. They obtained a 0.62 BLEU point gain. Abo Bakr et al. (2008) suggested another hybrid s</context>
</contexts>
<marker>2011</marker>
<rawString>Mohamed Embarki and Moha Ennaji, editors. 2011. Modern Trends in Arabic Dialectology. The Red Sea Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles A Ferguson</author>
</authors>
<date>1959</date>
<journal>Diglossia. Word,</journal>
<volume>15</volume>
<pages>340</pages>
<contexts>
<context position="1161" citStr="Ferguson, 1959" startWordPosition="173" endWordPosition="174">M words and 380k words, respectively. The dialectal sentences are selected from a large corpus of Arabic web text, and translated using Amazon’s Mechanical Turk. We use this data to build Dialectal Arabic MT systems, and find that small amounts of dialectal data have a dramatic impact on translation quality. When translating Egyptian and Levantine test sets, our Dialectal Arabic MT system performs 6.3 and 7.0 BLEU points higher than a Modern Standard Arabic MT system trained on a 150M-word Arabic-English parallel corpus. 1 Introduction The Arabic language is a well-known example of diglossia (Ferguson, 1959), where the formal variety of the language, which is taught in schools and used in written communication and formal speech (religion, politics, etc.) differs significantly in its grammatical properties from the informal varieties that are acquired natively, which are used mostly for verbal communication. The spoken varieties of the Arabic language (which we refer to collectively as Dialectal Arabic) differ widely among themselves, depending on the geographic distribution and the socio-economic conditions of the speakers, and they diverge from the formal variety known as Modern Standard Arabic </context>
</contexts>
<marker>Ferguson, 1959</marker>
<rawString>Charles A. Ferguson. 1959. Diglossia. Word, 15:325– 340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="19078" citStr="Habash and Rambow, 2005" startWordPosition="2946" endWordPosition="2949"> (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 shows the effect of applying segmentation to the text, for both MSA and Dialectal Arabic. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). Morphological segmentation gives a smaller gain on dialectal input, which could be due to two factors: the segmentation accuracy likely decreases since we are using an unmodified MSA segmenter, and there is higher variability in the written form of dialect compared to MSA. Given the significant, albeit smaller gain on dialectal inpu</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL), Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>MAGEAD: A morphological analyzer and generator for the Arabic dialects.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="6186" citStr="Habash and Rambow, 2006" startWordPosition="958" endWordPosition="961">er work that has focused on tasks besides MT includes that of Chiang et al. (2006), who built a parser for spoken Levantine Arabic (LA) transcripts using an MSA treebank. They used an LA-MSA lexicon in addition to morphological and syntactic rules to map the LA sentences to MSA. Riesa and Yarowsky (2006) built a statistical morphological segmenter for Iraqi and Levantine speech transcripts, and showed that they outperformed rulebased segmentation with small amounts of training. Some tools exist for preprocessing and tokenizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon’s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data. Zaidan and Callison-Burch (2011a) studied the quality of crowdsourced translations, by quantifying th</context>
</contexts>
<marker>Habash, Rambow, 2006</marker>
<rawString>Nizar Habash and Owen Rambow. 2006. MAGEAD: A morphological analyzer and generator for the Arabic dialects. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>New York, New York.</location>
<contexts>
<context position="18490" citStr="Habash and Sadat, 2006" startWordPosition="2850" endWordPosition="2853"> achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. score.3 In addition, we also report the OOV rate of the test set relative to the training corpus in each experimental setups. 4.1 Morphological Decomposition Arabic has a complex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 sh</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Y Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="8656" citStr="Habash (2010)" startWordPosition="1332" endWordPosition="1333">ritten form than in spoken form, the first challenge is to simply find instances of written Dialectal Arabic. We draw from a large corpus of monolingual Arabic text (approximately 350M words) that was harvested from the web by the LDC, largely from weblog and online user groups.1 Before presenting our data to annotators, we filter it to identify 1Corpora: LDC2006E32, LDC2006E77, LDC2006E90, LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41, LDC2008E54, LDC2009E14, LDC2009E93. 50 Figure 1: One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English. segments most likely to be dialectal (unlike Zaidan and Callison-Burch (2011b), who did no such prefiltering). We eliminate documents with a large percentage of non-Arabic or MSA words. We then retain documents that contain some number of dialectal words, using a set of manually selected dialectal words that was assembled by culling through the transcripts of the Levantine Fisher and Egyptian </context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Y. Habash. 2010. Introduction to Arabic Natural Language Processing. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL ’04: Proceedings of HLT-NAACL 2004,</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="18465" citStr="Lee, 2004" startWordPosition="2848" endWordPosition="2849"> scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. score.3 In addition, we also report the OOV rate of the test set relative to the training corpus in each experimental setups. 4.1 Morphological Decomposition Arabic has a complex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. we ran experiments using the MADA morphological analyzer (Habash and </context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-Suk Lee. 2004. Morphological analysis for statistical machine translation. In HLT-NAACL ’04: Proceedings of HLT-NAACL 2004, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="15438" citStr="Och and Ney, 2003" startWordPosition="2391" endWordPosition="2394">tal cost was $44k, or $0.03/word – an order of magnitude cheaper than professional translation. 4 Experiments in Dialectal Arabic-English Machine Translation We performed a set of experiments to contrast systems trained using our dialectal parallel corpus with systems trained on a (much larger) MSA-English parallel corpus. All experiments use the same methods for training, decoding and parameter tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU opt</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4753" citStr="Papineni et al., 2002" startWordPosition="721" endWordPosition="724"> somewhat limited. Previous research on Dialectal Arabic MT has focused on normalizing dialectal input words into MSA equivalents before translating to English, and they deal with inputs that contain a limited fraction of dialectal words. Sawaf (2010) normalized the dialectal words in a hybrid (rulebased and statistical) MT system, by performing a combination of character- and morpheme-level mappings. They then translated the normalized source to English using a hybrid MT or alternatively a Statistical MT system. They tested their method on proprietary test sets, observing about 1 BLEU point (Papineni et al., 2002) increase on broadcast news/conversation and about 2 points on web text. Salloum and Habash (2011) reduced the proportion of dialectal out-of-vocabulary (OOV) words also by mapping their affixed morphemes to MSA equivalents (but did not perform lexical mapping on the word stems). They allowed for multiple morphological analyses, passing them on to the MT system in the form of a lattice. They tested on a subset of broadcast news and broadcast conversation data sets consisting of sentences that contain at least one region marked as non-MSA, with an initial OOV rate against an MSA training corpus</context>
<context position="13517" citStr="Papineni et al., 2002" startWordPosition="2083" endWordPosition="2086">e Arabic text into translation software. We still spot checked the translations against the output of Google Translate and Bing Translator. We also rejected gobbledygook garbage translations that have a high percentage of words not found in an English lexicon. We quantified the quality of an individual Turker’s translations in two ways: first by asking native Arabic speaker judges to score a sample of the Turker’s translations, and second by inserting control sentences for which we have good reference translations and measuring the Turker’s METEOR (Banerjee and Lavie, 2005) and BLEU-1 scores (Papineni et al., 2002).2 The rejection rate of translation assignments was 5%. We promoted good translators to a restricted access “preferred worker queue”. They were paid at a higher rate, and were required to translate control passages only 10% of the time as opposed to 20% for general Turkers, thus providing us with a higher translation yield for unseen data. Worker turnout was initially slow, but increased quickly as our reputation for being reliable payers was established; workers started translating larger volumes and referring their acquaintances. We had 121 workers who each completed 20 or more translation </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>David Yarowsky</author>
</authors>
<title>Minimally supervised morphological segmentation with applications to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="5867" citStr="Riesa and Yarowsky (2006)" startWordPosition="909" endWordPosition="912">ces that contain at least one region marked as non-MSA, with an initial OOV rate against an MSA training corpus of 1.51%. They obtained a 0.62 BLEU point gain. Abo Bakr et al. (2008) suggested another hybrid system to map Egyptian Arabic to MSA, using morphological analysis on the input and an Egyptian-MSA lexicon. Other work that has focused on tasks besides MT includes that of Chiang et al. (2006), who built a parser for spoken Levantine Arabic (LA) transcripts using an MSA treebank. They used an LA-MSA lexicon in addition to morphological and syntactic rules to map the LA sentences to MSA. Riesa and Yarowsky (2006) built a statistical morphological segmenter for Iraqi and Levantine speech transcripts, and showed that they outperformed rulebased segmentation with small amounts of training. Some tools exist for preprocessing and tokenizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon’s Mechanical Turk (MTurk) is becoming an essential tool for creating </context>
</contexts>
<marker>Riesa, Yarowsky, 2006</marker>
<rawString>Jason Riesa and David Yarowsky. 2006. Minimally supervised morphological segmentation with applications to machine translation. In Proceedings of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA 2006), Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wael Salloum</author>
<author>Nizar Habash</author>
</authors>
<title>Dialectal to standard Arabic paraphrasing to improve Arabic-English statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference of Empirical Methods in Natural Language Processing,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="4851" citStr="Salloum and Habash (2011)" startWordPosition="736" endWordPosition="739">l input words into MSA equivalents before translating to English, and they deal with inputs that contain a limited fraction of dialectal words. Sawaf (2010) normalized the dialectal words in a hybrid (rulebased and statistical) MT system, by performing a combination of character- and morpheme-level mappings. They then translated the normalized source to English using a hybrid MT or alternatively a Statistical MT system. They tested their method on proprietary test sets, observing about 1 BLEU point (Papineni et al., 2002) increase on broadcast news/conversation and about 2 points on web text. Salloum and Habash (2011) reduced the proportion of dialectal out-of-vocabulary (OOV) words also by mapping their affixed morphemes to MSA equivalents (but did not perform lexical mapping on the word stems). They allowed for multiple morphological analyses, passing them on to the MT system in the form of a lattice. They tested on a subset of broadcast news and broadcast conversation data sets consisting of sentences that contain at least one region marked as non-MSA, with an initial OOV rate against an MSA training corpus of 1.51%. They obtained a 0.62 BLEU point gain. Abo Bakr et al. (2008) suggested another hybrid s</context>
<context position="29643" citStr="Salloum and Habash, 2011" startWordPosition="4725" endWordPosition="4728">le 6). 4.5 Mapping from Dialectal Arabic to MSA Before Translating to English Given the large amount of linguistic resources that have been developed for MSA over the past years, and the extensive research that was conducted on machine translation from MSA to English and other languages, an obvious research question is whether Dialectal Arabic is best translated to English by first pivoting through MSA, rather than directly. The proximity of Dialectal Arabic to MSA makes the mapping in principle easier than general machine translation, and a number of researchers have explored this direction (Salloum and Habash, 2011). In this scenario, the dialectal source would first be automatically transformed to MSA, using either a rule-based or statistical mapping module. The Dialectal Arabic-English parallel corpus we created presents a unique opportunity to compare the MSA-pivoting approach against direct translation. First, we collected equivalent MSA data for the Levantine Web test and tuning sets, by asking Turkers to transform dialectal passages to valid and fluent MSA. Turkers were shown example transformations, and we encouraged fewer changes where applicable (e.g. morphological rather than lexical mapping), </context>
</contexts>
<marker>Salloum, Habash, 2011</marker>
<rawString>Wael Salloum and Nizar Habash. 2011. Dialectal to standard Arabic paraphrasing to improve Arabic-English statistical machine translation. In Proceedings of the 2011 Conference of Empirical Methods in Natural Language Processing, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sawaf</author>
</authors>
<title>Arabic dialect handling in hybrid machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th Conf. of the Association for Machine Translation in the Americas (AMTA 2010),</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="4382" citStr="Sawaf (2010)" startWordPosition="665" endWordPosition="666">ismatching domain. 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49–59, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics 2 Previous Work Existing work on natural language processing of Dialectal Arabic text, including machine translation, is somewhat limited. Previous research on Dialectal Arabic MT has focused on normalizing dialectal input words into MSA equivalents before translating to English, and they deal with inputs that contain a limited fraction of dialectal words. Sawaf (2010) normalized the dialectal words in a hybrid (rulebased and statistical) MT system, by performing a combination of character- and morpheme-level mappings. They then translated the normalized source to English using a hybrid MT or alternatively a Statistical MT system. They tested their method on proprietary test sets, observing about 1 BLEU point (Papineni et al., 2002) increase on broadcast news/conversation and about 2 points on web text. Salloum and Habash (2011) reduced the proportion of dialectal out-of-vocabulary (OOV) words also by mapping their affixed morphemes to MSA equivalents (but </context>
</contexts>
<marker>Sawaf, 2010</marker>
<rawString>Hassan Sawaf. 2010. Arabic dialect handling in hybrid machine translation. In Proceedings of the 9th Conf. of the Association for Machine Translation in the Americas (AMTA 2010), Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>577--585</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="15402" citStr="Shen et al. (2008)" startWordPosition="2384" endWordPosition="2387">creating our parallel corpus. The total cost was $44k, or $0.03/word – an order of magnitude cheaper than professional translation. 4 Experiments in Dialectal Arabic-English Machine Translation We performed a set of experiments to contrast systems trained using our dialectal parallel corpus with systems trained on a (much larger) MSA-English parallel corpus. All experiments use the same methods for training, decoding and parameter tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a t</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 577–585, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>Ralph Weischedel</author>
</authors>
<title>A study of translation error rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="18920" citStr="Snover et al., 2006" startWordPosition="2922" endWordPosition="2925">mplex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 shows the effect of applying segmentation to the text, for both MSA and Dialectal Arabic. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). Morphological segmentation gives a smaller gain on dialectal input, which could be due to two factors: the segmentation accuracy likely decreases since we are using an unmodifi</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Weischedel, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and Ralph Weischedel. 2006. A study of translation error rate with targeted human annotation. In Proceedings of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA 2006), pages 223–231, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>The Arabic online commentary dataset: an annotated dataset of informal Arabic with high dialectal content.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>37--41</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="6716" citStr="Zaidan and Callison-Burch (2011" startWordPosition="1040" endWordPosition="1043">okenizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon’s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data. Zaidan and Callison-Burch (2011a) studied the quality of crowdsourced translations, by quantifying the quality of non-professional English translations of 2,000 Urdu sentences that were originally translated by the LDC. They demonstrated a variety of mechanisms that increase the translation quality of crowdsourced translations to near professional levels, with a total cost that is less than one tenth the cost of professional translation. Zaidan and Callison-Burch (2011b) created the Arabic Online Commentary (AOC) dataset, a 52Mword monolingual dataset rich in dialectal content. Over 100k sentences from the AOC were annotate</context>
<context position="8936" citStr="Zaidan and Callison-Burch (2011" startWordPosition="1374" endWordPosition="1377">line user groups.1 Before presenting our data to annotators, we filter it to identify 1Corpora: LDC2006E32, LDC2006E77, LDC2006E90, LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41, LDC2008E54, LDC2009E14, LDC2009E93. 50 Figure 1: One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English. segments most likely to be dialectal (unlike Zaidan and Callison-Burch (2011b), who did no such prefiltering). We eliminate documents with a large percentage of non-Arabic or MSA words. We then retain documents that contain some number of dialectal words, using a set of manually selected dialectal words that was assembled by culling through the transcripts of the Levantine Fisher and Egyptian CallHome speech corpora. After filtering, the dataset contained around 4M words, which we used as a starting point for creating our Dialectal ArabicEnglish parallel corpus. 3.1 Dialect Classification To refine the document set beyond our keyword filtering heuristic and to label w</context>
<context position="12112" citStr="Zaidan and Callison-Burch (2011" startWordPosition="1883" endWordPosition="1886">ect translation, we segmented passages into individual sentences using MTurk. We only required sentences longer than 15 words to be segmented, and allowed Turkers to split and rejoin at any point between the tokens. The instructions were simply to “divide the Arabic text into individual sentences, where you believe it would be appropriate to insert a period.” We also used a set of correctly segmented passages for quality control, and scored Turkers using a metric based on the precision and recall of correct segmentation points. The rejection rate was 1.2%. 3.3 Translation to English Following Zaidan and Callison-Burch (2011a), we hired non-professional translators on MTurk to translate the Levantine and Egyptian sentences into 51 Sentence Arabic English Data Set Pairs Tokens Tokens MSA-150MW 8.0M 151.4M 204.4M Dialect-1500KW 180k 1,545,053 2,257,041 MSA-1300KW 71k 1,292,384 1,752,724 MSA-Web-Tune 6,163 145,260 184,185 MSA-Web-Test 5,454 136,396 172,357 Lev-Web-Tune 2,600 20,940 27,399 Lev-Web-Test 2,600 21,092 27,793 Egy-Web-Test 2,600 23,671 33,565 E-Facebook-Tune 3,351 25,130 34,753 E-Facebook-Test 3,188 25,011 34,244 Table 2: Statistics about the training/tuning/test datasets used in our experiments. The toke</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011a. The Arabic online commentary dataset: an annotated dataset of informal Arabic with high dialectal content. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 37–41, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from non-professionals.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1220--1229</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="6716" citStr="Zaidan and Callison-Burch (2011" startWordPosition="1040" endWordPosition="1043">okenizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon’s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data. Zaidan and Callison-Burch (2011a) studied the quality of crowdsourced translations, by quantifying the quality of non-professional English translations of 2,000 Urdu sentences that were originally translated by the LDC. They demonstrated a variety of mechanisms that increase the translation quality of crowdsourced translations to near professional levels, with a total cost that is less than one tenth the cost of professional translation. Zaidan and Callison-Burch (2011b) created the Arabic Online Commentary (AOC) dataset, a 52Mword monolingual dataset rich in dialectal content. Over 100k sentences from the AOC were annotate</context>
<context position="8936" citStr="Zaidan and Callison-Burch (2011" startWordPosition="1374" endWordPosition="1377">line user groups.1 Before presenting our data to annotators, we filter it to identify 1Corpora: LDC2006E32, LDC2006E77, LDC2006E90, LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41, LDC2008E54, LDC2009E14, LDC2009E93. 50 Figure 1: One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English. segments most likely to be dialectal (unlike Zaidan and Callison-Burch (2011b), who did no such prefiltering). We eliminate documents with a large percentage of non-Arabic or MSA words. We then retain documents that contain some number of dialectal words, using a set of manually selected dialectal words that was assembled by culling through the transcripts of the Levantine Fisher and Egyptian CallHome speech corpora. After filtering, the dataset contained around 4M words, which we used as a starting point for creating our Dialectal ArabicEnglish parallel corpus. 3.1 Dialect Classification To refine the document set beyond our keyword filtering heuristic and to label w</context>
<context position="12112" citStr="Zaidan and Callison-Burch (2011" startWordPosition="1883" endWordPosition="1886">ect translation, we segmented passages into individual sentences using MTurk. We only required sentences longer than 15 words to be segmented, and allowed Turkers to split and rejoin at any point between the tokens. The instructions were simply to “divide the Arabic text into individual sentences, where you believe it would be appropriate to insert a period.” We also used a set of correctly segmented passages for quality control, and scored Turkers using a metric based on the precision and recall of correct segmentation points. The rejection rate was 1.2%. 3.3 Translation to English Following Zaidan and Callison-Burch (2011a), we hired non-professional translators on MTurk to translate the Levantine and Egyptian sentences into 51 Sentence Arabic English Data Set Pairs Tokens Tokens MSA-150MW 8.0M 151.4M 204.4M Dialect-1500KW 180k 1,545,053 2,257,041 MSA-1300KW 71k 1,292,384 1,752,724 MSA-Web-Tune 6,163 145,260 184,185 MSA-Web-Test 5,454 136,396 172,357 Lev-Web-Tune 2,600 20,940 27,399 Lev-Web-Test 2,600 21,092 27,793 Egy-Web-Test 2,600 23,671 33,565 E-Facebook-Tune 3,351 25,130 34,753 E-Facebook-Test 3,188 25,011 34,244 Table 2: Statistics about the training/tuning/test datasets used in our experiments. The toke</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011b. Crowdsourcing translation: Professional quality from non-professionals. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1220– 1229, Portland, Oregon, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>