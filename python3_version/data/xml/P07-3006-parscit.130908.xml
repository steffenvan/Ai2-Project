<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006948">
<title confidence="0.998">
Exploiting Structure for Event Discovery Using the MDI Algorithm
</title>
<author confidence="0.997876">
Martina Naughton
</author>
<affiliation confidence="0.879266">
School of Computer Science &amp; Informatics
University College Dublin
Ireland
</affiliation>
<email confidence="0.993248">
martina.naughton@ucd.ie
</email>
<sectionHeader confidence="0.993735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999806571428571">
Effectively identifying events in unstruc-
tured text is a very difficult task. This is
largely due to the fact that an individual
event can be expressed by several sentences.
In this paper, we investigate the use of clus-
tering methods for the task of grouping the
text spans in a news article that refer to the
same event. The key idea is to cluster the
sentences, using a novel distance metric that
exploits regularities in the sequential struc-
ture of events within a document. When
this approach is compared to a simple bag
of words baseline, a statistically significant
increase in performance is observed.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865777777778">
Accurately identifying events in unstructured text is
an important goal for many applications that require
natural language understanding. There has been an
increased focus on this problem in recent years. The
Automatic Content Extraction (ACE) program1 is
dedicated to developing methods that automatically
infer meaning from language data. Tasks include
the detection and characterisation of Entities, Rela-
tions, and Events. Extensive research has been ded-
icated to entity recognition and binary relation de-
tection with significant results (Bikel et al., 1999).
However, event extraction is still considered as one
of the most challenging tasks because an individual
event can be expressed by several sentences (Xu et
al., 2006).
In this paper, we primarily focus on techniques
for identifying events within a given news article.
Specifically, we describe and evaluate clustering
</bodyText>
<footnote confidence="0.984083">
1http://www.nist.gov/speech/tests/ace/
</footnote>
<page confidence="0.999455">
31
</page>
<bodyText confidence="0.999945636363636">
methods for the task of grouping sentences in a news
article that refer to the same event. We generate
sentence clusters using three variations of the well-
documented Hierarchical Agglomerative Clustering
(HAC) (Manning and Sch¨utze, 1999) as a baseline
for this task. We provide convincing evidence sug-
gesting that inherent structures exist in the manner in
which events appear in documents. In Section 3.1,
we present an algorithm which uses such structures
during the clustering process and as a result a mod-
est increase in accuracy is observed.
Developing methods capable of identifying all
types of events from free text is challenging for sev-
eral reasons. Firstly, different applications consider
different types of events and with different levels of
granularity. A change in state, a horse winning a
race and the race meeting itself can be considered
as events. Secondly, interpretation of events can be
subjective. How people understand an event can de-
pend on their knowledge and perspectives. There-
fore in this current work, the type of event to extract
is known in advance. As a detailed case study, we
investigate event discovery using a corpus of news
articles relating to the recent Iraqi War where the tar-
get event is the “Death” event type. Figure 1 shows
a sample article depicting such events.
The remainder of this paper is organised as fol-
lows: We begin with a brief discussion of related
work in Section 2. We describe our approach to
Event Discovery in Section 3. Our techniques are
experimentally evaluated in Section 4. Finally, we
conclude with a discussion of experimental observa-
tions and opportunities for future work in Section 5.
</bodyText>
<sectionHeader confidence="0.994416" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.85538775">
The aim of Event Extraction is to identify any in-
stance of a particular class of events in a natural
Proceedings of the ACL 2007 Student Research Workshop, pages 31–36,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.572752">
World News
Insurgents Kill 17 in Iraq
</subsectionHeader>
<bodyText confidence="0.942882">
In Tikrit, gunmen killed 17 Iraqis as they were heading to work Sunday at a U.S. military facility.
Capt. Bill Coppernoll, said insurgents fired at several buses of Iraqis from two cars.
Elsewhere, an explosion at a market in Baqubah, about 30 miles north of Baghdad late Thursday.
The market was struck by mortar bombs according to U.S. military spokesman Sgt. Danny Martin.
</bodyText>
<figureCaption confidence="0.998555">
Figure 1: Sample news article that describes multiple events.
</figureCaption>
<bodyText confidence="0.99998759375">
language text, extract the relevant arguments of the
event, and represent the extracted information into
a structured form (Grishman, 1997). The types of
events to extract are known in advance. For exam-
ple, “Attack” and “Death” are possible event types
to be extracted. Previous work in this area focuses
mainly on linguistic and statistical methods to ex-
tract the relevant arguments of a event type. Lin-
guistic methods attempt to capture linguists knowl-
edge in determining constraints for syntax, mor-
phology and the disambiguation of both. Statistical
methods generate models based in the internal struc-
tures of sentences, usually identifying dependency
structures using an already annotated corpus of sen-
tences. However, since an event can be expressed
by several sentences, our approach to event extrac-
tion is as follows: First, identify all the sentences in
a document that refer to the event in question. Sec-
ond, extract event arguments from these sentences
and finally represent the extracted information of the
event in a structured form.
Particularly, in this paper we focus on clustering
methods for grouping sentences in an article that dis-
cuss the same event. The task of clustering simi-
lar sentences is a problem that has been investigated
particularly in the area of text summarisation. In
SimFinder (Hatzivassiloglou et al., 2001), a flexible
clustering tool for summarisation, the task is defined
as finding text units (sentences or paragraphs) that
contain information about a specific subject. How-
ever, the text features used in their similarity metric
are selected using a Machine Learning model.
</bodyText>
<sectionHeader confidence="0.990359" genericHeader="method">
3 Identifying Events within Articles
</sectionHeader>
<bodyText confidence="0.99997024">
We treat the task of grouping together sentences that
refer to the same event(s) as a clustering problem.
As a baseline, we generate sentence clusters us-
ing average-link, single-link and complete-link Hi-
erarchical Agglomerative Clustering. HAC initially
assigns each data point to a singleton cluster, and
repeatedly merges clusters until a specified termi-
nation criteria is satisfied (Manning and Sch¨utze,
1999). These methods require a similarity metric
between two sentences. We use the standard co-
sine metric over a bag-of-words encoding of each
sentence. We remove stopwords and stem each re-
maining term using the Porter stemming algorithm
(Porter, 1997). Our algorithms begin by placing
each sentence in its own cluster. At each itera-
tion we merge the two closest clusters. A fully-
automated approach must use some termination cri-
teria to decide when to stop clustering. In exper-
iments presented here, we adopt two manually su-
pervised methods to set the desired number of clus-
ters (k): “correct” k and “best” k. “Correct” sets k
to be the actual number of events. This value was
obtained during the annotation process (see Section
4.1). “Best” tunes k so as to maximise the quality of
the resulting clusters.
</bodyText>
<subsectionHeader confidence="0.999212">
3.1 Exploiting Article Structure
</subsectionHeader>
<bodyText confidence="0.999982636363636">
Our baseline ignores an important constraint on the
event associated with each sentence: the position
of the sentence within the document. Documents
consist of sentences arranged in a linear order and
nearby sentences in terms of this ordering typically
refer to the same topic (Zha, 2002). Similarly we as-
sume that adjacent sentences are more likely to refer
to the same event, later sentences are likely to intro-
duce new events, etc. In this Section, we describe an
algorithm that exploits this document structure dur-
ing the sentence clustering process.
</bodyText>
<page confidence="0.996049">
32
</page>
<bodyText confidence="0.998943833333333">
The basic idea is to learn a model capable of cap-
turing document structure, i.e. the way events are
reported. Each document is treated as a sequence of
labels (1 label per sentence) where each label repre-
sents the event(s) discussed in that sentence. We de-
fine four generalised event label types: N, represents
a new event sentence; C, represents a continuing
event sentence (i.e. it discusses the same event as the
preceding sentence); B, represents a back-reference
to an earlier event; X, represents a sentence that does
not reference an event. This model takes the form of
a Finite State Automaton (FSA) where:
</bodyText>
<listItem confidence="0.978026666666667">
• States correspond to event labels.
• Transitions correspond to adjacent sentences
that mention the pair of events.
</listItem>
<bodyText confidence="0.999591387096774">
More formally, E = (S, s0, F, L, T) is a model
where S is the set of states, s0 E S is the initial state,
F C S is the set of final states, L is the set of edge
labels and T C (S x L) x S is the set of transitions.
We note that it is the responsibility of the learning
algorithm to discover the correct number of states.
We treat the task of discovering an event model as
that of learning a regular grammar from a set of pos-
itive examples. Following Golds research on learn-
ing regular languages (Gold, 1967), the problem has
received significant attention. In our current experi-
ments, we use Thollard et al’s MDI algorithm (Thol-
lard et al., 2000) for learning the automaton. MDI
has been shown to be effective on a wide range of
tasks, but it must be noted that any grammar infer-
ence algorithm could be substituted.
To estimate how much sequential structure exists
in the sentence labels, the document collection was
randomly split into training and test sets. The au-
tomaton produced by MDI was learned using the
training data, and the probability that each test se-
quence was generated by the automaton was calcu-
lated. These probabilities were compared with those
of a set of random sequences (generated to have the
same distribution of length as the test data). The
probabilities of event sequences from our dataset
and the randomly generated sequences are shown
in Figure 2. The test and random sequences are
sorted by probability. The vertical axis shows the
rank in each sequence and the horizontal axis shows
the negative log probability of the sequence at each
</bodyText>
<figureCaption confidence="0.602062666666667">
Figure 2: Distribution in the probability that actual
and random event sequences are generated by the
automaton produced by MDI.
</figureCaption>
<bodyText confidence="0.997097">
rank. The data suggests that the documents are in-
deed structured, as real document sequences tend to
be much more likely under the trained FSA than ran-
domly generated sequences.
We modify our baseline clustering algorithm to
utilise the structural information omitted by the au-
tomaton as follows: Let L(c1, c2) be a sequence
of labels induced by merging two clusters c1 and
c2. If P(L(c1, c2)) is the probability that sequence
L(c1, c2) is accepted by the automaton, and let
cos(c1, c2) be the cosine distance between c1 and c2.
We can measure the similarity between c1 and c2 as:
</bodyText>
<equation confidence="0.863948">
SIM(c1, c2) = cos(c1, c2) x P(L(c1, c2)) (1)
</equation>
<bodyText confidence="0.970702846153846">
Let r be the number of clusters remaining. Then
there are r(r�1)
2 pairs of clusters. For each pair of
clusters c1,c2 we generate the resulting sequence of
labels that would result if c1 and c2 were merged.
We then input each label sequence to our trained
FSA to obtain the probability that it is generated by
the automaton. At each iteration, the algorithm pro-
ceeds by merging the most similar pair according to
this metric. Figure 3 illustrates this process in more
detail. To terminate the clustering process, we adopt
either the “correct” k or “best” k halting criteria de-
scribed earlier.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.990647">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9949115">
In our experiments, we used a corpus of news arti-
cles which is a subset of the Iraq Body Count (IBC)
</bodyText>
<page confidence="0.997742">
33
</page>
<figureCaption confidence="0.999815">
Figure 3: The sequence-based clustering process.
</figureCaption>
<bodyText confidence="0.999740208333333">
dataset2. This is an independent public database of
media-reported civilian deaths in Iraq resulting di-
rectly from military attack by the U.S. forces. Casu-
alty figures for each event reported are derived solely
from a comprehensive manual survey of online me-
dia reports from various news sources. We obtained
a portion of their corpus which consists of 342 new
articles from 56 news sources. The articles are of
varying size (average sentence length per document
is 25.96). Most of the articles contain references to
multiple events. The average number of events per
document is 5.09. Excess HTML (image captions
etc.) was removed, and sentence boundaries were
identified using the Lingua::EN::Sentence perl mod-
ule available from CPAN3.
To evaluate our clustering methods, we use the
definition of precision and recall proposed by (Hess
and Kushmerick, 2003). We assign each pair of
sentences into one of four categories: (i) clustered
together (and annotated as referring to the same
event); (ii) not clustered together (but annotated as
referring to the same event); (iii) incorrectly clus-
tered together; (iv) correctly not clustered together.
Precision and recall are thus found to be computed
</bodyText>
<equation confidence="0.5090235">
asP= &apos; and a&apos; and F1= 2P
a R
</equation>
<bodyText confidence="0.999783625">
The corpus was annotated by a set of ten vol-
unteers. Within each article, events were uniquely
identified by integers. These values were then
mapped to one of the four label categories, namely
“N”, “C”, “X”, and “B”. For instance, sentences de-
scribing previously unseen events were assigned a
new integer. This value was mapped to the label cat-
egory “N” signifying a new event. Similarly, sen-
</bodyText>
<footnote confidence="0.9995855">
2http://iraqbodycount.org/
3http://cpan.org/
</footnote>
<bodyText confidence="0.9999196875">
tences referring to events in a preceding sentence
were assigned the same integer identifier as that
assigned to the preceding sentence and mapped to
the label category “C”. Sentences that referenced an
event mentioned earlier in the document but not in
the preceding sentence were assigned the same inte-
ger identifier as that sentence but mapped to the label
category “B”. Furthermore, If a sentence did not re-
fer to any event, it was assigned the label 0 and was
mapped to the label category “X”. Finally, each doc-
ument was also annotated with the distinct number
of events reported in it.
In order to approximate the level of inter-
annotation agreement, two annotators were asked to
annotate a disjoint set of 250 documents. Inter-rater
agreements were calculated using the kappa statis-
tic that was first proposed by (Cohen, 1960). This
measure calculates and removes from the agreement
rate the amount of agreement expected by chance.
Therefore, the results are more informative than a
simple agreement average (Cohen, 1960; Carletta,
1996). Some extensions were developed including
(Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et
al., 1991). In this paper the methodology proposed
by (Fleiss, 1981) was implemented. Each sentence
in the document set was rated by the two annotators
and the assigned values were mapped into one of the
four label categories (“N”, “C”, “X”, and “B”). For
complete instructions on how kappa was calculated,
we refer the reader to (Fleiss, 1981). Using the an-
notated data, a kappa score of 0.67 was obtained.
This indicates that the annotations are somewhat in-
consistent, but nonetheless are useful for producing
tentative conclusions.
To determine why the annotators were having dif-
ficulty agreeing, we calculated the kappa score for
each category. For the “N”, “C” and “X” categories,
reasonable scores of 0.69, 0.71 and 0.72 were ob-
tained respectively. For the “B” category a relatively
poor score of 0.52 was achieved indicating that the
raters found it difficult to identify sentences that ref-
erenced events mentioned earlier in the document.
To illustrate the difficulty of the annotation task an
example where the raters disagreed is depicted in
Figure 4. The raters both agreed when assigning
labels to sentence 1 and 2 but disagreed when as-
signing a label to Sentence 23 . In order to correctly
annotate this sentence as referring to the event de-
</bodyText>
<page confidence="0.997582">
34
</page>
<figureCaption confidence="0.990272571428571">
Sentence 1: A suicide attacker set off a bomb that tore through a funeral tent jammed with Shiite mourners Thursday.
Rater 1: label=1. Rater 2: label=1
Sentence 2: The explosion, in a working class neighbourhood of Mosul, destroyed the tent killing nearly 50 people.
Rater 1: label=1. Rater 2: label=1.
Sentence 23: At the hospital of this northern city, doctor Saher Maher said that at least 47 people were killed.
Rater 1: label=1. Rater 2: label=2.
Figure 4: Sample sentences where the raters disagreed.
</figureCaption>
<table confidence="0.9997392">
Algorithm a-link c-link s-link
BL(correct k) 40.5 % 39.2% 39.6%
SEQ(correct k) 47.6%* 45.5%* 44.9%*
BL(best k) 52.0% 48.2% 50.9%
SEQ(best k) 61.0%* 56.9%* 58.6%*
</table>
<tableCaption confidence="0.997833">
Table 1: % F1 achieved using average-link (a-link),
</tableCaption>
<bodyText confidence="0.9575087">
complete-link (c-link) and single-link (s-link) varia-
tions of the baseline and sequence-based algorithms
when the correct and best k halting criteria are used.
Scores marked with * are statistically significant to
a confidence level of 99%.
scribe in sentence 1 and 2, the rater have to resolve
that “the northern city” is referring to “Mosul” and
that “nearly 50” equates to “at least 47”. These and
similar ambiguities in written text make such an an-
notation task very difficult.
</bodyText>
<subsectionHeader confidence="0.681367">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999898269230769">
We evaluated our clustering algorithms using the F1
metric. Results presented in Table 1 were obtained
using 50:50 randomly selected train/test splits aver-
aged over 5 runs. For each run, the automaton pro-
duced by MDI was generated using the training set
and the clustering algorithms were evaluated using
the test set. On average, the sequence-based clus-
tering approach achieves an 8% increase in F1 when
compared to the baseline. Specifically the average-
link variation exhibits the highest F1 score, achiev-
ing 62% when the “best” k termination method is
used.
It is important to note that the inference produced
by the automaton depends on two values: the thresh-
old α of the MDI algorithm and the amount of label
sequences used for learning. The closer α is to 0,
the more general the inferred automaton becomes.
In an attempt to produce a more general automaton,
we chose α = 0.1. Intuitively, as more training data
is used to train the automaton, more accurate infer-
ences are expected. To confirm this we calculated
the %F1 achieved by the average-link variation of
the method for varying levels of training data. Over-
all, an improvement of approx. 5% is observed as
the percentage training data used is increased from
10% to 90%.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999696461538462">
Accurately identifying events in unstructured text is
a very difficult task. This is partly because the de-
scription of an individual event can spread across
several sentences. In this paper, we investigated
the use of clustering for the task of grouping sen-
tences in a document that refer to the same event.
However, there are limitations to this approach that
need to be considered. Firstly, results presented
in Section 4.2 suggest that the performance of the
clusterer depends somewhat on the chosen value
of k (i.e. the number of events in the document).
This information is not readily available. However,
preliminary analysis presented in (Naughton et al.,
2006) indicate that is possible to estimate this value
with reasonable accuracy. Furthermore, promising
results are observed when this estimated value is
used halt the clustering process. Secondly, labelled
data is required to train the automation used by our
novel clustering method. Evidence presented in Sec-
tion 4.1 suggests that reasonable inter-annotation
agreement for such an annotation task is difficult to
achieve. Nevertheless, clustering allows us to take
into account that the manner in which events are de-
scribed is not always linear. To assess exactly how
beneficial this is, we are currently treating this prob-
lem as a text segmentation task. Although this is a
</bodyText>
<page confidence="0.997457">
35
</page>
<bodyText confidence="0.999976875">
crude treatment of the complexity of written text, it
will help us to approximate the benefit (if any) of
applying clustering-based techniques to this task.
In the future, we hope to further evaluate our
methods using a larger dataset containing more
event types. We also hope to examine the inter-
esting possibility that inherent structures learned
from documents originating from one news source
(e.g. Aljazeera) differ from structures learned us-
ing documents originating from another source (e.g.
Reuters). Finally, a single sentence often contains
references to multiple events. For example, consider
the sentence “These two bombings have claimed the
lives of 23 Iraqi soldiers”. Our algorithms assume
that each sentence describes just one event. Future
work will focus on developing methods to automati-
cally recognise such sentences and techniques to in-
corporate them into the clustering process.
Acknowledgements. This research was supported
by the Irish Research Council for Science, Engineer-
ing &amp; Technology (IRCSET) and IBM under grant
RS/2004/IBM/1. The author also wishes to thank
Dr. Joe Carthy and Dr. Nicholas Kushmerick for
their helpful discussions.
</bodyText>
<sectionHeader confidence="0.998492" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999629111111111">
W. Barlow, N. Lai, and S. Azen. 1991. A comparison of
methods for calculating a stratified kappa. Statistics in
Medicine, 10:1465–1472.
Daniel Bikel, Richard Schwartz, and Ralph Weischedel.
1999. An algorithm that learns what’s in a name. Ma-
chine Learning, 34(1-3):211–231.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguis-
tics, 22:249–254.
Jacob Cohen. 1960. A coeficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37–46.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70.
B.S. Everitt. 1968. Moments of the statistics kappa and
the weighted kappa. The British Journal of Mathemat-
ical and Statistical Psychology, 21:97–103.
J.L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76.
J.L. Fleiss, 1981. Statistical methods for rates and pro-
portions, pages 212–36. John Wiley &amp; Sons.
E. Mark Gold. 1967. Grammar identification in the limit.
Information and Control, 10(5):447–474.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In Proceedings of the sev-
enth International Message Understanding Confer-
ence, pages 10–27.
Vasileios Hatzivassiloglou, Judith Klavans, Melissa Hol-
combe, Regina Barzilay, Min-Yen Kan, and Kathleen
McKeown. 2001. SIMFINDER: A flexible clustering
tool for summarisation. In Proceedings of the NAACL
Workshop on Automatic Summarisation, Association
for Computational Linguistics, pages 41–49.
Andreas Hess and Nicholas Kushmerick. 2003. Learn-
ing to attach semantic metadata to web services. In
Proceedings of the International Semantic Web Con-
ference (ISWC 2003), pages 258–273. Springer.
Christopher Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press.
Martina Naughton, Nicholas Kushmerick, and Joseph
Carthy. 2006. Event extraction from heterogeneous
news sources. In Proceedings of the AAAI Workshop
Event Extraction and Synthesis, pages 1–6, Boston.
Martin Porter. 1997. An algorithm for suffix stripping.
Readings in Information Retrieval, pages 313–316.
Franck Thollard, Pierre Dupont, and Colin de la Higuera.
2000. Probabilistic DFA inference using Kullback-
Leibler divergence and minimality. In Proceedings of
the 17th International Conference on Machine Learn-
ing, pages 975–982. Morgan Kaufmann, San Fran-
cisco.
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2006. Auto-
matic event and relation detection with seeds of vary-
ing complexity. In Proceedings of the AAAI Workshop
Event Extraction and Synthesis, pages 12–17, Boston.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement prin-
ciple and sentence clustering. In Proceedings of the
25th annual international ACM SIGIR conference on
Research and development in Information Retrieval,
pages 113–120, New York, NY. ACM Press.
</reference>
<page confidence="0.998935">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.699750">
<title confidence="0.999921">Exploiting Structure for Event Discovery Using the MDI Algorithm</title>
<author confidence="0.999995">Martina Naughton</author>
<affiliation confidence="0.924970666666667">School of Computer Science &amp; Informatics University College Dublin Ireland</affiliation>
<email confidence="0.956241">martina.naughton@ucd.ie</email>
<abstract confidence="0.9965458">Effectively identifying events in unstructured text is a very difficult task. This is largely due to the fact that an individual event can be expressed by several sentences. In this paper, we investigate the use of clustering methods for the task of grouping the text spans in a news article that refer to the same event. The key idea is to cluster the sentences, using a novel distance metric that exploits regularities in the sequential structure of events within a document. When this approach is compared to a simple bag of words baseline, a statistically significant increase in performance is observed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Barlow</author>
<author>N Lai</author>
<author>S Azen</author>
</authors>
<title>A comparison of methods for calculating a stratified kappa. Statistics in Medicine,</title>
<date>1991</date>
<pages>10--1465</pages>
<contexts>
<context position="14237" citStr="Barlow et al., 1991" startWordPosition="2325" endWordPosition="2328">o annotated with the distinct number of events reported in it. In order to approximate the level of interannotation agreement, two annotators were asked to annotate a disjoint set of 250 documents. Inter-rater agreements were calculated using the kappa statistic that was first proposed by (Cohen, 1960). This measure calculates and removes from the agreement rate the amount of agreement expected by chance. Therefore, the results are more informative than a simple agreement average (Cohen, 1960; Carletta, 1996). Some extensions were developed including (Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et al., 1991). In this paper the methodology proposed by (Fleiss, 1981) was implemented. Each sentence in the document set was rated by the two annotators and the assigned values were mapped into one of the four label categories (“N”, “C”, “X”, and “B”). For complete instructions on how kappa was calculated, we refer the reader to (Fleiss, 1981). Using the annotated data, a kappa score of 0.67 was obtained. This indicates that the annotations are somewhat inconsistent, but nonetheless are useful for producing tentative conclusions. To determine why the annotators were having difficulty agreeing, we calcula</context>
</contexts>
<marker>Barlow, Lai, Azen, 1991</marker>
<rawString>W. Barlow, N. Lai, and S. Azen. 1991. A comparison of methods for calculating a stratified kappa. Statistics in Medicine, 10:1465–1472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1376" citStr="Bikel et al., 1999" startWordPosition="206" endWordPosition="209">cant increase in performance is observed. 1 Introduction Accurately identifying events in unstructured text is an important goal for many applications that require natural language understanding. There has been an increased focus on this problem in recent years. The Automatic Content Extraction (ACE) program1 is dedicated to developing methods that automatically infer meaning from language data. Tasks include the detection and characterisation of Entities, Relations, and Events. Extensive research has been dedicated to entity recognition and binary relation detection with significant results (Bikel et al., 1999). However, event extraction is still considered as one of the most challenging tasks because an individual event can be expressed by several sentences (Xu et al., 2006). In this paper, we primarily focus on techniques for identifying events within a given news article. Specifically, we describe and evaluate clustering 1http://www.nist.gov/speech/tests/ace/ 31 methods for the task of grouping sentences in a news article that refer to the same event. We generate sentence clusters using three variations of the welldocumented Hierarchical Agglomerative Clustering (HAC) (Manning and Sch¨utze, 1999)</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel Bikel, Richard Schwartz, and Ralph Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--249</pages>
<contexts>
<context position="14131" citStr="Carletta, 1996" startWordPosition="2312" endWordPosition="2313"> it was assigned the label 0 and was mapped to the label category “X”. Finally, each document was also annotated with the distinct number of events reported in it. In order to approximate the level of interannotation agreement, two annotators were asked to annotate a disjoint set of 250 documents. Inter-rater agreements were calculated using the kappa statistic that was first proposed by (Cohen, 1960). This measure calculates and removes from the agreement rate the amount of agreement expected by chance. Therefore, the results are more informative than a simple agreement average (Cohen, 1960; Carletta, 1996). Some extensions were developed including (Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et al., 1991). In this paper the methodology proposed by (Fleiss, 1981) was implemented. Each sentence in the document set was rated by the two annotators and the assigned values were mapped into one of the four label categories (“N”, “C”, “X”, and “B”). For complete instructions on how kappa was calculated, we refer the reader to (Fleiss, 1981). Using the annotated data, a kappa score of 0.67 was obtained. This indicates that the annotations are somewhat inconsistent, but nonetheless are useful for pr</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22:249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coeficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="13920" citStr="Cohen, 1960" startWordPosition="2281" endWordPosition="2282">arlier in the document but not in the preceding sentence were assigned the same integer identifier as that sentence but mapped to the label category “B”. Furthermore, If a sentence did not refer to any event, it was assigned the label 0 and was mapped to the label category “X”. Finally, each document was also annotated with the distinct number of events reported in it. In order to approximate the level of interannotation agreement, two annotators were asked to annotate a disjoint set of 250 documents. Inter-rater agreements were calculated using the kappa statistic that was first proposed by (Cohen, 1960). This measure calculates and removes from the agreement rate the amount of agreement expected by chance. Therefore, the results are more informative than a simple agreement average (Cohen, 1960; Carletta, 1996). Some extensions were developed including (Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et al., 1991). In this paper the methodology proposed by (Fleiss, 1981) was implemented. Each sentence in the document set was rated by the two annotators and the assigned values were mapped into one of the four label categories (“N”, “C”, “X”, and “B”). For complete instructions on how kappa wa</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coeficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit.</title>
<date>1968</date>
<journal>Psychological Bulletin,</journal>
<volume>70</volume>
<contexts>
<context position="14186" citStr="Cohen, 1968" startWordPosition="2319" endWordPosition="2320">tegory “X”. Finally, each document was also annotated with the distinct number of events reported in it. In order to approximate the level of interannotation agreement, two annotators were asked to annotate a disjoint set of 250 documents. Inter-rater agreements were calculated using the kappa statistic that was first proposed by (Cohen, 1960). This measure calculates and removes from the agreement rate the amount of agreement expected by chance. Therefore, the results are more informative than a simple agreement average (Cohen, 1960; Carletta, 1996). Some extensions were developed including (Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et al., 1991). In this paper the methodology proposed by (Fleiss, 1981) was implemented. Each sentence in the document set was rated by the two annotators and the assigned values were mapped into one of the four label categories (“N”, “C”, “X”, and “B”). For complete instructions on how kappa was calculated, we refer the reader to (Fleiss, 1981). Using the annotated data, a kappa score of 0.67 was obtained. This indicates that the annotations are somewhat inconsistent, but nonetheless are useful for producing tentative conclusions. To determine why the ann</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>Jacob Cohen. 1968. Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. Psychological Bulletin, 70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B S Everitt</author>
</authors>
<title>Moments of the statistics kappa and the weighted kappa.</title>
<date>1968</date>
<journal>The British Journal of Mathematical and Statistical Psychology,</journal>
<pages>21--97</pages>
<contexts>
<context position="14215" citStr="Everitt, 1968" startWordPosition="2323" endWordPosition="2324">ocument was also annotated with the distinct number of events reported in it. In order to approximate the level of interannotation agreement, two annotators were asked to annotate a disjoint set of 250 documents. Inter-rater agreements were calculated using the kappa statistic that was first proposed by (Cohen, 1960). This measure calculates and removes from the agreement rate the amount of agreement expected by chance. Therefore, the results are more informative than a simple agreement average (Cohen, 1960; Carletta, 1996). Some extensions were developed including (Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et al., 1991). In this paper the methodology proposed by (Fleiss, 1981) was implemented. Each sentence in the document set was rated by the two annotators and the assigned values were mapped into one of the four label categories (“N”, “C”, “X”, and “B”). For complete instructions on how kappa was calculated, we refer the reader to (Fleiss, 1981). Using the annotated data, a kappa score of 0.67 was obtained. This indicates that the annotations are somewhat inconsistent, but nonetheless are useful for producing tentative conclusions. To determine why the annotators were having difficult</context>
</contexts>
<marker>Everitt, 1968</marker>
<rawString>B.S. Everitt. 1968. Moments of the statistics kappa and the weighted kappa. The British Journal of Mathematical and Statistical Psychology, 21:97–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<contexts>
<context position="14200" citStr="Fleiss, 1971" startWordPosition="2321" endWordPosition="2322">inally, each document was also annotated with the distinct number of events reported in it. In order to approximate the level of interannotation agreement, two annotators were asked to annotate a disjoint set of 250 documents. Inter-rater agreements were calculated using the kappa statistic that was first proposed by (Cohen, 1960). This measure calculates and removes from the agreement rate the amount of agreement expected by chance. Therefore, the results are more informative than a simple agreement average (Cohen, 1960; Carletta, 1996). Some extensions were developed including (Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et al., 1991). In this paper the methodology proposed by (Fleiss, 1981) was implemented. Each sentence in the document set was rated by the two annotators and the assigned values were mapped into one of the four label categories (“N”, “C”, “X”, and “B”). For complete instructions on how kappa was calculated, we refer the reader to (Fleiss, 1981). Using the annotated data, a kappa score of 0.67 was obtained. This indicates that the annotations are somewhat inconsistent, but nonetheless are useful for producing tentative conclusions. To determine why the annotators were h</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>J.L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Statistical methods for rates and proportions,</title>
<date>1981</date>
<pages>212--36</pages>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="14295" citStr="Fleiss, 1981" startWordPosition="2336" endWordPosition="2337"> order to approximate the level of interannotation agreement, two annotators were asked to annotate a disjoint set of 250 documents. Inter-rater agreements were calculated using the kappa statistic that was first proposed by (Cohen, 1960). This measure calculates and removes from the agreement rate the amount of agreement expected by chance. Therefore, the results are more informative than a simple agreement average (Cohen, 1960; Carletta, 1996). Some extensions were developed including (Cohen, 1968; Fleiss, 1971; Everitt, 1968; Barlow et al., 1991). In this paper the methodology proposed by (Fleiss, 1981) was implemented. Each sentence in the document set was rated by the two annotators and the assigned values were mapped into one of the four label categories (“N”, “C”, “X”, and “B”). For complete instructions on how kappa was calculated, we refer the reader to (Fleiss, 1981). Using the annotated data, a kappa score of 0.67 was obtained. This indicates that the annotations are somewhat inconsistent, but nonetheless are useful for producing tentative conclusions. To determine why the annotators were having difficulty agreeing, we calculated the kappa score for each category. For the “N”, “C” an</context>
</contexts>
<marker>Fleiss, 1981</marker>
<rawString>J.L. Fleiss, 1981. Statistical methods for rates and proportions, pages 212–36. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mark Gold</author>
</authors>
<title>Grammar identification in the limit.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>5</issue>
<contexts>
<context position="8827" citStr="Gold, 1967" startWordPosition="1431" endWordPosition="1432">espond to event labels. • Transitions correspond to adjacent sentences that mention the pair of events. More formally, E = (S, s0, F, L, T) is a model where S is the set of states, s0 E S is the initial state, F C S is the set of final states, L is the set of edge labels and T C (S x L) x S is the set of transitions. We note that it is the responsibility of the learning algorithm to discover the correct number of states. We treat the task of discovering an event model as that of learning a regular grammar from a set of positive examples. Following Golds research on learning regular languages (Gold, 1967), the problem has received significant attention. In our current experiments, we use Thollard et al’s MDI algorithm (Thollard et al., 2000) for learning the automaton. MDI has been shown to be effective on a wide range of tasks, but it must be noted that any grammar inference algorithm could be substituted. To estimate how much sequential structure exists in the sentence labels, the document collection was randomly split into training and test sets. The automaton produced by MDI was learned using the training data, and the probability that each test sequence was generated by the automaton was </context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E. Mark Gold. 1967. Grammar identification in the limit. Information and Control, 10(5):447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Information extraction: Techniques and challenges.</title>
<date>1997</date>
<booktitle>In Proceedings of the seventh International Message Understanding Conference,</booktitle>
<pages>10--27</pages>
<contexts>
<context position="4268" citStr="Grishman, 1997" startWordPosition="672" endWordPosition="673">World News Insurgents Kill 17 in Iraq In Tikrit, gunmen killed 17 Iraqis as they were heading to work Sunday at a U.S. military facility. Capt. Bill Coppernoll, said insurgents fired at several buses of Iraqis from two cars. Elsewhere, an explosion at a market in Baqubah, about 30 miles north of Baghdad late Thursday. The market was struck by mortar bombs according to U.S. military spokesman Sgt. Danny Martin. Figure 1: Sample news article that describes multiple events. language text, extract the relevant arguments of the event, and represent the extracted information into a structured form (Grishman, 1997). The types of events to extract are known in advance. For example, “Attack” and “Death” are possible event types to be extracted. Previous work in this area focuses mainly on linguistic and statistical methods to extract the relevant arguments of a event type. Linguistic methods attempt to capture linguists knowledge in determining constraints for syntax, morphology and the disambiguation of both. Statistical methods generate models based in the internal structures of sentences, usually identifying dependency structures using an already annotated corpus of sentences. However, since an event c</context>
</contexts>
<marker>Grishman, 1997</marker>
<rawString>Ralph Grishman. 1997. Information extraction: Techniques and challenges. In Proceedings of the seventh International Message Understanding Conference, pages 10–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith Klavans</author>
<author>Melissa Holcombe</author>
<author>Regina Barzilay</author>
<author>Min-Yen Kan</author>
<author>Kathleen McKeown</author>
</authors>
<title>SIMFINDER: A flexible clustering tool for summarisation.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on Automatic Summarisation, Association for Computational Linguistics,</booktitle>
<pages>41--49</pages>
<contexts>
<context position="5473" citStr="Hatzivassiloglou et al., 2001" startWordPosition="863" endWordPosition="866">However, since an event can be expressed by several sentences, our approach to event extraction is as follows: First, identify all the sentences in a document that refer to the event in question. Second, extract event arguments from these sentences and finally represent the extracted information of the event in a structured form. Particularly, in this paper we focus on clustering methods for grouping sentences in an article that discuss the same event. The task of clustering similar sentences is a problem that has been investigated particularly in the area of text summarisation. In SimFinder (Hatzivassiloglou et al., 2001), a flexible clustering tool for summarisation, the task is defined as finding text units (sentences or paragraphs) that contain information about a specific subject. However, the text features used in their similarity metric are selected using a Machine Learning model. 3 Identifying Events within Articles We treat the task of grouping together sentences that refer to the same event(s) as a clustering problem. As a baseline, we generate sentence clusters using average-link, single-link and complete-link Hierarchical Agglomerative Clustering. HAC initially assigns each data point to a singleton</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>Vasileios Hatzivassiloglou, Judith Klavans, Melissa Holcombe, Regina Barzilay, Min-Yen Kan, and Kathleen McKeown. 2001. SIMFINDER: A flexible clustering tool for summarisation. In Proceedings of the NAACL Workshop on Automatic Summarisation, Association for Computational Linguistics, pages 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Hess</author>
<author>Nicholas Kushmerick</author>
</authors>
<title>Learning to attach semantic metadata to web services.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Semantic Web Conference (ISWC</booktitle>
<pages>258--273</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="12283" citStr="Hess and Kushmerick, 2003" startWordPosition="2008" endWordPosition="2011">manual survey of online media reports from various news sources. We obtained a portion of their corpus which consists of 342 new articles from 56 news sources. The articles are of varying size (average sentence length per document is 25.96). Most of the articles contain references to multiple events. The average number of events per document is 5.09. Excess HTML (image captions etc.) was removed, and sentence boundaries were identified using the Lingua::EN::Sentence perl module available from CPAN3. To evaluate our clustering methods, we use the definition of precision and recall proposed by (Hess and Kushmerick, 2003). We assign each pair of sentences into one of four categories: (i) clustered together (and annotated as referring to the same event); (ii) not clustered together (but annotated as referring to the same event); (iii) incorrectly clustered together; (iv) correctly not clustered together. Precision and recall are thus found to be computed asP= &apos; and a&apos; and F1= 2P a R The corpus was annotated by a set of ten volunteers. Within each article, events were uniquely identified by integers. These values were then mapped to one of the four label categories, namely “N”, “C”, “X”, and “B”. For instance, s</context>
</contexts>
<marker>Hess, Kushmerick, 2003</marker>
<rawString>Andreas Hess and Nicholas Kushmerick. 2003. Learning to attach semantic metadata to web services. In Proceedings of the International Semantic Web Conference (ISWC 2003), pages 258–273. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martina Naughton</author>
<author>Nicholas Kushmerick</author>
<author>Joseph Carthy</author>
</authors>
<title>Event extraction from heterogeneous news sources.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Workshop Event Extraction and Synthesis,</booktitle>
<pages>1--6</pages>
<location>Boston.</location>
<contexts>
<context position="18614" citStr="Naughton et al., 2006" startWordPosition="3051" endWordPosition="3054">text is a very difficult task. This is partly because the description of an individual event can spread across several sentences. In this paper, we investigated the use of clustering for the task of grouping sentences in a document that refer to the same event. However, there are limitations to this approach that need to be considered. Firstly, results presented in Section 4.2 suggest that the performance of the clusterer depends somewhat on the chosen value of k (i.e. the number of events in the document). This information is not readily available. However, preliminary analysis presented in (Naughton et al., 2006) indicate that is possible to estimate this value with reasonable accuracy. Furthermore, promising results are observed when this estimated value is used halt the clustering process. Secondly, labelled data is required to train the automation used by our novel clustering method. Evidence presented in Section 4.1 suggests that reasonable inter-annotation agreement for such an annotation task is difficult to achieve. Nevertheless, clustering allows us to take into account that the manner in which events are described is not always linear. To assess exactly how beneficial this is, we are currentl</context>
</contexts>
<marker>Naughton, Kushmerick, Carthy, 2006</marker>
<rawString>Martina Naughton, Nicholas Kushmerick, and Joseph Carthy. 2006. Event extraction from heterogeneous news sources. In Proceedings of the AAAI Workshop Event Extraction and Synthesis, pages 1–6, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1997</date>
<booktitle>Readings in Information Retrieval,</booktitle>
<pages>313--316</pages>
<contexts>
<context position="6441" citStr="Porter, 1997" startWordPosition="1011" endWordPosition="1012">s that refer to the same event(s) as a clustering problem. As a baseline, we generate sentence clusters using average-link, single-link and complete-link Hierarchical Agglomerative Clustering. HAC initially assigns each data point to a singleton cluster, and repeatedly merges clusters until a specified termination criteria is satisfied (Manning and Sch¨utze, 1999). These methods require a similarity metric between two sentences. We use the standard cosine metric over a bag-of-words encoding of each sentence. We remove stopwords and stem each remaining term using the Porter stemming algorithm (Porter, 1997). Our algorithms begin by placing each sentence in its own cluster. At each iteration we merge the two closest clusters. A fullyautomated approach must use some termination criteria to decide when to stop clustering. In experiments presented here, we adopt two manually supervised methods to set the desired number of clusters (k): “correct” k and “best” k. “Correct” sets k to be the actual number of events. This value was obtained during the annotation process (see Section 4.1). “Best” tunes k so as to maximise the quality of the resulting clusters. 3.1 Exploiting Article Structure Our baseline</context>
</contexts>
<marker>Porter, 1997</marker>
<rawString>Martin Porter. 1997. An algorithm for suffix stripping. Readings in Information Retrieval, pages 313–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franck Thollard</author>
<author>Pierre Dupont</author>
<author>Colin de la Higuera</author>
</authors>
<title>Probabilistic DFA inference using KullbackLeibler divergence and minimality.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<pages>975--982</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="8966" citStr="Thollard et al., 2000" startWordPosition="1451" endWordPosition="1455">0, F, L, T) is a model where S is the set of states, s0 E S is the initial state, F C S is the set of final states, L is the set of edge labels and T C (S x L) x S is the set of transitions. We note that it is the responsibility of the learning algorithm to discover the correct number of states. We treat the task of discovering an event model as that of learning a regular grammar from a set of positive examples. Following Golds research on learning regular languages (Gold, 1967), the problem has received significant attention. In our current experiments, we use Thollard et al’s MDI algorithm (Thollard et al., 2000) for learning the automaton. MDI has been shown to be effective on a wide range of tasks, but it must be noted that any grammar inference algorithm could be substituted. To estimate how much sequential structure exists in the sentence labels, the document collection was randomly split into training and test sets. The automaton produced by MDI was learned using the training data, and the probability that each test sequence was generated by the automaton was calculated. These probabilities were compared with those of a set of random sequences (generated to have the same distribution of length as</context>
</contexts>
<marker>Thollard, Dupont, Higuera, 2000</marker>
<rawString>Franck Thollard, Pierre Dupont, and Colin de la Higuera. 2000. Probabilistic DFA inference using KullbackLeibler divergence and minimality. In Proceedings of the 17th International Conference on Machine Learning, pages 975–982. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feiyu Xu</author>
<author>Hans Uszkoreit</author>
<author>Hong Li</author>
</authors>
<title>Automatic event and relation detection with seeds of varying complexity.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Workshop Event Extraction and Synthesis,</booktitle>
<pages>12--17</pages>
<location>Boston.</location>
<contexts>
<context position="1544" citStr="Xu et al., 2006" startWordPosition="233" endWordPosition="236"> language understanding. There has been an increased focus on this problem in recent years. The Automatic Content Extraction (ACE) program1 is dedicated to developing methods that automatically infer meaning from language data. Tasks include the detection and characterisation of Entities, Relations, and Events. Extensive research has been dedicated to entity recognition and binary relation detection with significant results (Bikel et al., 1999). However, event extraction is still considered as one of the most challenging tasks because an individual event can be expressed by several sentences (Xu et al., 2006). In this paper, we primarily focus on techniques for identifying events within a given news article. Specifically, we describe and evaluate clustering 1http://www.nist.gov/speech/tests/ace/ 31 methods for the task of grouping sentences in a news article that refer to the same event. We generate sentence clusters using three variations of the welldocumented Hierarchical Agglomerative Clustering (HAC) (Manning and Sch¨utze, 1999) as a baseline for this task. We provide convincing evidence suggesting that inherent structures exist in the manner in which events appear in documents. In Section 3.1</context>
</contexts>
<marker>Xu, Uszkoreit, Li, 2006</marker>
<rawString>Feiyu Xu, Hans Uszkoreit, and Hong Li. 2006. Automatic event and relation detection with seeds of varying complexity. In Proceedings of the AAAI Workshop Event Extraction and Synthesis, pages 12–17, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
</authors>
<title>Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in Information Retrieval,</booktitle>
<pages>113--120</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="7318" citStr="Zha, 2002" startWordPosition="1158" endWordPosition="1159">ly supervised methods to set the desired number of clusters (k): “correct” k and “best” k. “Correct” sets k to be the actual number of events. This value was obtained during the annotation process (see Section 4.1). “Best” tunes k so as to maximise the quality of the resulting clusters. 3.1 Exploiting Article Structure Our baseline ignores an important constraint on the event associated with each sentence: the position of the sentence within the document. Documents consist of sentences arranged in a linear order and nearby sentences in terms of this ordering typically refer to the same topic (Zha, 2002). Similarly we assume that adjacent sentences are more likely to refer to the same event, later sentences are likely to introduce new events, etc. In this Section, we describe an algorithm that exploits this document structure during the sentence clustering process. 32 The basic idea is to learn a model capable of capturing document structure, i.e. the way events are reported. Each document is treated as a sequence of labels (1 label per sentence) where each label represents the event(s) discussed in that sentence. We define four generalised event label types: N, represents a new event sentenc</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>Hongyuan Zha. 2002. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in Information Retrieval, pages 113–120, New York, NY. ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>