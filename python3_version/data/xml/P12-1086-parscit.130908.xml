<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986573">
Named Entity Disambiguation in Streaming Data
</title>
<author confidence="0.995825">
Alexandre Davis1, Adriano Veloso1, Altigran S. da Silva2
Wagner Meira Jr.1, Alberto H. F. Laender1
</author>
<affiliation confidence="0.9457615">
1Computer Science Dept. − Federal University of Minas Gerais
2Computer Science Dept. − Federal University of Amazonas
</affiliation>
<email confidence="0.9902655">
{agdavis,adrianov,meira,laender}@dcc.ufmg.br
alti@dcc.ufam.edu.br
</email>
<sectionHeader confidence="0.99855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999743115384615">
The named entity disambiguation task is to re-
solve the many-to-many correspondence be-
tween ambiguous names and the unique real-
world entity. This task can be modeled as a
classification problem, provided that positive
and negative examples are available for learn-
ing binary classifiers. High-quality sense-
annotated data, however, are hard to be ob-
tained in streaming environments, since the
training corpus would have to be constantly
updated in order to accomodate the fresh data
coming on the stream. On the other hand, few
positive examples plus large amounts of un-
labeled data may be easily acquired. Produc-
ing binary classifiers directly from this data,
however, leads to poor disambiguation per-
formance. Thus, we propose to enhance the
quality of the classifiers using finer-grained
variations of the well-known Expectation-
Maximization (EM) algorithm. We conducted
a systematic evaluation using Twitter stream-
ing data and the results show that our clas-
sifiers are extremely effective, providing im-
provements ranging from 1% to 20%, when
compared to the current state-of-the-art biased
SVMs, being more than 120 times faster.
</bodyText>
<sectionHeader confidence="0.999608" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99625825">
Human language is not exact. For instance, an en-
tity1 may be referred by multiple names (i.e., poly-
semy), and also the same name may refer to different
entities depending on the surrounding context (i.e.,
</bodyText>
<footnote confidence="0.988117">
1The term entity refers to anything that has a distinct, sepa-
rate (materialized or not) existence.
</footnote>
<bodyText confidence="0.999787322580645">
homonymy). The task of named entity disambigua-
tion is to identify which names refer to the same en-
tity in a textual collection (Sarmento et al., 2009;
Yosef et al., 2011; Hoffart et al., 2011). The emer-
gence of new communication technologies, such as
micro-blog platforms, brought a humongous amount
of textual mentions with ambiguous entity names,
raising an urgent need for novel disambiguation ap-
proaches and algorithms.
In this paper we address the named entity disam-
biguation task under a particularly challenging sce-
nario. We are given a stream of messages from a
micro-blog channel such as Twitter2 and a list of
names n1, n2, ... , nN used for mentioning a spe-
cific entity e. Our problem is to monitor the stream
and predict whether an incoming message contain-
ing ni indeed refers to e (positive example) or not
(negative example). This scenario brings challenges
that must be overcome. First, micro-blog messages
are composed of a small amount of words and they
are written in informal, sometimes cryptic style.
These characteristics make hard the identification of
entities and the semantics of their relationships (Liu
et al., 2011). Further, the scarcity of text in the mes-
sages makes it even harder to properly characterize a
common context for the entities. Second, as we need
to monitor messages that keep coming at a fast pace,
we cannot afford to gather information from external
sources on-the-fly. Finally, fresh data coming in the
stream introduces new patterns, quickly invalidating
static disambiguation models.
</bodyText>
<footnote confidence="0.974706666666667">
2Twitter is one of the fastest-growing micro-blog channels,
and an authoritative source for breaking news (Jansen et al.,
2009).
</footnote>
<page confidence="0.934094">
815
</page>
<note confidence="0.9860785">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 815–824,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999981030769231">
We hypothesize that the lack of information in
each individual message and from external sources
can be compensated by using information obtained
from the large and diverse amount of text in a stream
of messages taken as a whole, that is, thousands of
messages per second coming from distinct sources.
The information embedded in such a stream of
messages may be exploited for entity disambigua-
tion through the application of supervised learning
methods, for instance, with the application of bi-
nary classifiers. Such methods, however, suffer from
a data acquisition bottleneck, since they are based
on training datasets that are built by skilled hu-
man annotators who manually inspect the messages.
This annotation process is usually lengthy and la-
borious, being clearly unfeasible to be adopted in
data streaming scenarios. As an alternative to such
manual process, a large amount of unlabeled data,
augmented with a small amount of (likely) posi-
tive examples, can be collected automatically from
the message stream (Liu et al., 2003; Denis, 1998;
Comit´e et al., 1999; Letouzey et al., 2000).
Binary classifiers may be learned from such data
by considering unlabeled data as negative examples.
This strategy, however, leads to classifiers with poor
disambiguation performance, due to a potentially
large number of false-negative examples. In this pa-
per we propose to refine binary classifiers iteratively,
by performing Expectation-Maximization (EM) ap-
proaches (Dempster et al., 1977). Basically, a partial
classifier is used to evaluate the likelihood of an un-
labeled example being a positive example or a nega-
tive example, thus automatically and (continuously)
creating a labeled training corpus. This process con-
tinues iteratively by changing the label of some ex-
amples (an operation we call label-transition), so
that, after some iterations, the combination of la-
bels is expected to converge to the one for which
the observed data is most likely. Based on such an
approach, we introduce novel disambiguation algo-
rithms that differ among themselves on the granu-
larity in which the classifier is updated, and on the
label-transition operations that are allowed.
An important feature of the proposed approach is
that, at each iteration of the EM-process, a new clas-
sifier (an improved one) is produced in order to ac-
count for the current set of labeled examples. We
introduce a novel strategy to maintain the classifiers
up-to-date incrementally after each iteration, or even
after each label-transition operation. Indeed, we the-
oretically show that our classifier needs to be up-
dated just partially and we are able to determine ex-
actly which parts must be updated, making our dis-
ambiguation methods extremely fast.
To evaluate the effectiveness of the proposed al-
gorithms, we performed a systematic set of ex-
periments using large-scale Twitter data containing
messages with ambiguous entity names. In order
to validate our claims, disambiguation performance
is investigated by varying the proportion of false-
negative examples in the unlabeled dataset. Our
algorithms are compared against a state-of-the-art
technique for named entity disambiguation based
on classifiers, providing performance gains ranging
from 1% to 20% and being roughly 120 times faster.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999906142857143">
In the context of databases, traditional entity dis-
ambiguation methods rely on similarity functions
over attributes associated to the entities (de Car-
valho et al., 2012). Obviously, such an approach
is unfeasible for the scenario we consider here.
Still on databases, Bhattacharya and Getoor (2007)
and Dong et. al (2005) propose graph-based dis-
ambiguation methods that generate clusters of co-
referent entities using known relationships between
entities of several types. Methods to disambiguate
person names in e-mail (Minkov et al., 2006) and
Web pages (Bekkerman and McCallum, 2005; Wan
et al., 2005) have employed similar ideas. In e-
mails, information taken from the header of the mes-
sages leads to establish relationships between users
and building a co-reference graph. In Web pages,
reference information come naturally from links.
Such graph-based approach could hardly be applied
to the context we consider, in which the implied re-
lationships between entities mentioned in a given
micro-blog message are not clearly defined.
In the case of textual corpora, traditional disam-
biguation methods represent entity names and their
context (Hasegawa et al., 2004) (i.e., words, phrases
and other names occurring near them) as weighted
vectors (Bagga and Baldwin, 1998; Pedersen et al.,
2005). To evaluate whether two names refer to
the same entity, these methods compute the similar-
</bodyText>
<page confidence="0.997365">
816
</page>
<bodyText confidence="0.999988709677419">
ity between these vectors. Clusters of co-referent
names are then built based on such similarity mea-
sure. Although effective for the tasks considered in
these papers, the simplistic BOW-based approaches
they adopt are not suitable for cases in which the
context is harder to capture due to the small num-
ber of terms available or to informal writing style.
To address these problems, some authors argue that
contextual information may be enriched with knowl-
edge from external sources, such as search results
and the Wikipedia (Cucerzan, 2007; Bunescu and
Pasca, 2006; Han and Zhao, 2009). While such a
strategy is feasible in an off-line setting, two prob-
lems arise when monitoring streams of micro-blog
messages. First, gathering information from exter-
nal sources through the Internet can be costly and,
second, informal mentions to named entities make it
hard to look for related information in such sources.
The disambiguation methods we propose fall into
a learning scenario known as PU (positive and un-
labeled) learning (Liu et al., 2003; Denis, 1998;
Comit´e et al., 1999; Letouzey et al., 2000), in which
a classifier is built from a set of positive examples
plus unlabeled data. Most of the approaches for PU
learning, such as the biased-SVM approach (Li and
Liu, 2003), are based on extracting negative exam-
ples from unlabeled data. We notice that existing ap-
proaches for PU learning are not likely to scale given
the restrictions imposed by streaming data. Thus,
we propose highly incremental approaches, which
are able to process large-scale streaming data.
</bodyText>
<sectionHeader confidence="0.992328" genericHeader="method">
3 Disambiguation in Streaming Data
</sectionHeader>
<bodyText confidence="0.998903822222222">
Consider a stream of messages from a micro-blog
channel such as Twitter and let n1, n2, ... , nN be
names used for mentioning a specific entity e in
these messages. Our problem is to continually moni-
tor the stream and predict whether an incoming mes-
sage containing nz indeed refers to e or not.
This task may be accomplished through the appli-
cation of classification techniques. In this case, we
are given an input data set called the training cor-
pus (denoted as D) which consists of examples of
the form &lt;e, m, c&gt;, where e is the entity, m is a
message containing the entity name (i.e., any nz),
and c C {e, ®} is a binary variable that specifies
whether or not the entity name in m refers to the
desired real-world entity e. The training corpus is
used to produce a classifier that relates textual pat-
terns (i.e., terms and sets of terms) in m to the value
of c. The test set (denoted as T) consists of a set
of records of the form &lt;e, m, ?&gt;, and the classifier
is used to indicate which messages in T refer to (or
not) the desired entity.
Supervised classifiers, however, are subject to a
data acquisition bottleneck, since the creation of a
training corpus requires skilled human annotators to
manually inspect the messages. The cost associ-
ated with this annotation process may render vast
amounts of examples unfeasible. In many cases,
however, the acquisition of some positive examples
is relatively inexpensive. For instance, as we are
dealing with messages collected from micro-blog
channels, we may exploit profiles (or hashtags) that
are known to be strongly associated with the desired
entity. Let us consider, as an illustrative example,
the profile associated with a company (i.e., @bayer).
Although the entity name is ambiguous, the sense of
messages that are posted in this profile is biased to-
wards the entity as being a company. Clearly, other
tricks like this one can be used, but, unfortunately,
they do not guarantee the absence of false-positives,
and they are not complete since the majority of mes-
sages mentioning the entity name may appear out-
side its profile. Thus, the collected examples are
not totally reliable, and disambiguation performance
would be seriously compromised if classifiers were
built upon these uncertain examples directly.
</bodyText>
<subsectionHeader confidence="0.998523">
3.1 Expectation-Maximization Approach
</subsectionHeader>
<bodyText confidence="0.999713125">
In this paper we hypothesize that it is worthwhile
to enhance the reliability of unlabeled examples,
provided that this type of data is inexpensive and
the enhancement effort will be then rewarded with
an improvement in disambiguation performance.
Thus, we propose a new approach based on the
Expectation-Maximization (EM) algorithm (Demp-
ster et al., 1977). We assume two scenarios:
</bodyText>
<listItem confidence="0.999172666666667">
• the training corpus D is composed of a small set
of truly positive examples plus a large amount
of unlabeled examples.
• the training corpus D is composed of a small
set of potentially positive examples plus a large
amount of unlabeled examples.
</listItem>
<page confidence="0.989654">
817
</page>
<bodyText confidence="0.9998182">
In both scenarios, unlabeled examples are ini-
tially treated as negative ones, so that classifiers can
be built from D. Therefore, in both scenarios, D
may contain false-negatives. In the second scenario,
however, D may also contain false-positives.
</bodyText>
<construct confidence="0.968903428571429">
Definition 3.1: The label-transition operation
xe→® turns a negative example xe ∈ D into a
positive one x®. The training corpus D becomes
{(D − xe) ∪ x®}. Similarly, the label-transition
operation x®→e, turns a positive example x® ∈ D
into a negative one xe. The training corpus D be-
comes {(D − x(5) ∪ xe}.
</construct>
<bodyText confidence="0.998947222222222">
Our Expectation Maximization (EM) methods
employ a classifier which assigns to each example
x ∈ D a probability α(x, e) of being negative.
Then, as illustrated in Algorithm 1, label-transition
operations are performed, so that, in the end of the
process, it is expected that the assigned labels con-
verge to the combination for which the data is most
likely. In the first scenario only operations xe→ED
are allowed, while in the second scenario operations
x(D→e are also allowed. In both cases, a crucial issue
that affects the effectiveness of our EM-based meth-
ods concerns the decision of whether or not perform-
ing the label-transition operation. Typically, a tran-
sition threshold αmin is employed, so that a label-
transition operation xE→� is always performed if x
is a negative example and α(x, (D) ≤ αmin. Simi-
larly, operation x®→e is always performed if x is a
positive example and α(x, e) &gt; αmin.
</bodyText>
<figure confidence="0.10209325">
Algorithm 1 Expectation-Maximization Approach.
Given:
D: training corpus
R: a binary classifier learned from D
</figure>
<subsectionHeader confidence="0.89858">
Expectation step:
</subsectionHeader>
<bodyText confidence="0.993107">
perform transition operations on examples in D
</bodyText>
<subsectionHeader confidence="0.892212">
Maximization step:
</subsectionHeader>
<bodyText confidence="0.99914125">
update R and α(x, e) ∀x ∈ D
The optimal value for αmin is not known in ad-
vance. Fortunately, data distribution may provide
hints about proper values for αmin. In our ap-
proach, instead of using a single value for αmin,
which would be applied to all examples indistinctly,
we use a specific α;Lin threshold for each exam-
ple x ∈ D. Based on such an approach, we in-
troduce fine-grained EM-based methods for named
entity disambiguation under streaming data. A spe-
cific challenge is that the proposed methods perform
several transition operations during each EM itera-
tion, and each transition operation may invalidate
parts of the current classifier, which must be prop-
erly updated. We take into consideration two possi-
ble update granularities:
</bodyText>
<listItem confidence="0.985797666666667">
• the classifier is updated after each EM iteration.
• the classifier is updated after each label-
transition operation.
</listItem>
<bodyText confidence="0.905240857142857">
Incremental Classifier: As already discussed, the
classifier must be constantly updated during the EM
process. In this case, well-established classifiers,
such as SVMs (Joachims, 2006), have to be learned
entirely from scratch, replicating work by large.
Thus, we propose as an alternative the use of Lazy
Associative Classifiers (Veloso et al., 2006).
</bodyText>
<construct confidence="0.998039833333333">
Definition 3.2: A classification rule is a specialized
association rule {X →− c} (Agrawal et al., 1993),
where the antecedent X is a set of terms (i.e., a
termset), and the consequent c indicates if the pre-
diction is positive or negative (i.e., c ∈ {(D, e}).
The domain for X is the vocabulary of D. The car-
dinality of rule {X → c} is given by the number of
terms in the antecedent, that is |X |. The support of
X is denoted as Q(X), and is the number of exam-
ples in D having X as a subset. The confidence of
rule {X → c} is denoted as B(X →− c), and is the
conditional probability of c given the termset X, that
</construct>
<equation confidence="0.78268">
is, B(X →− c) = °(X∪c)
U(X) .
</equation>
<bodyText confidence="0.9999106">
In this context, a classifier is denoted as R, and
it is composed of a set of rules {X →− c} ex-
tracted from D. Specifically, R is represented as
a pool of entries with the form &lt;key, properties&gt;,
where key={X, c} and properties={Q(X), Q(X ∪
c), B(X → c)}. Each entry in the pool corresponds
to a rule, and the key is used to facilitate fast access
to rule properties.
Once the classifier R is extracted from D, rules
are collectively used to approximate the likelihood
of an arbitrary example being positive (Q or neg-
ative ((D). Basically, R is interpreted as a poll, in
which each rule {X → c} ∈ R is a vote given by X
for ® or e. Given an example x, a rule {X → c} is
only considered a valid vote if it is applicable to x.
</bodyText>
<page confidence="0.988499">
818
</page>
<construct confidence="0.822309333333333">
Definition 3.3: A rule {X → c} ∈ R is said to be
applicable to example x if X ⊆ x, that is, if all terms
in X are present in example x.
</construct>
<bodyText confidence="0.999961">
We denote as Rx the set of rules in R that are ap-
plicable to example x. Thus, only and all the rules in
Rx are considered as valid votes when classifying x.
Further, we denote as Rcx the subset of Rx contain-
ing only rules predicting c. Votes in Rcx have differ-
ent weights, depending on the confidence of the cor-
responding rules. Weighted votes for c are averaged,
giving the score for c with regard to x (Equation 1).
Finally, the likelihood of x being a negative example
is given by the normalized score (Equation 2).
</bodyText>
<equation confidence="0.9994918">
� s(x, c) = |O(X → |c) , with c ∈ {e, ED} (1)
Rcx
( s(x, e)
a x, e) = (2)
s(x, �) + s(x, �)
</equation>
<bodyText confidence="0.964072960784314">
Training Projection and Demand-Driven Rule
Extraction: Demand-driven rule extraction (Veloso
et al., 2006) is a recent strategy used to avoid the
huge search space for rules, by projecting the train-
ing corpus according to the example being pro-
cessed. More specifically, rule extraction is delayed
until an example x is given for classification. Then,
terms in x are used as a filter that configures the
training corpus D so that just rules that are appli-
cable to x can be extracted. This filtering process
produces a projected training corpus, denoted as Dx,
which contains only terms that are present in x. As
shown in (Silva et al., 2011), the number of rules ex-
tracted using this strategy grows polynomially with
the size of the vocabulary.
Extending the Classifier Dynamically: With
demand-driven rule extraction, the classifier R is ex-
tended dynamically as examples are given for clas-
sification. Initially R is empty; a subset Rxi is ap-
pended to R every time an example xi is processed.
Thus, after processing a sequence of m examples
{x1, x2, ... , xm}, R = {Rx1 ∪ Rx2 ∪ ... ∪ RxJ.
Before extracting rule {X → c}, it is checked
whether this rule is already in R. In this case, while
processing an example x, if an entry is found with
a key matching {X, c}, then the rule in R is used
instead of extracting it from Dx. Otherwise, the rule
is extracted from Dx and then it is inserted into R.
Incremental Updates: Entries in R may become
invalid when D is modified due to a label-transition
operation. Given that D has been modified, the clas-
sifier R must be updated properly. We propose to
maintain R up-to-date incrementally, so that the up-
dated classifier is exactly the same one that would
be obtained by re-constructing it from scratch.
Lemma 3.1: Operation xe→® (or x®→e) does not
change the value of Q(X), for any termset X.
Proof: The operation xe→ED changes only the label
associated with x, but not its terms. Thus, the num-
ber of examples in D having X as a subset is essen-
tially the same as in {(D − xe) ∪ x®. The same
holds for operation x®→e. ■
Lemma 3.2: Operation xE)→® (or x®→e) changes
the value of Q(X ∪ c) iff termset X ⊂ x.
Proof: For operation xe→(), if X ⊂ x, then {X ∪
e} appears once less in {(D − xE)) ∪ x®} than in
D. Similarly, {X ∪ (D} appears once more in {(D −
xe) ∪ x®} than in D. Clearly, if X ⊂6 x, the number
of times {X ∪ e} (and {X ∪ (D}) appears in {(D −
xe)∪x®} remains the same as in D. The same holds
for operation x(D→e. ■
</bodyText>
<construct confidence="0.5082745">
Lemma 3.3: Operation xE→� (or xE→�) changes
the value of O(X → c) iff termset X ⊂ x.
</construct>
<bodyText confidence="0.977196454545455">
Proof: Comes directly from Lemmas 3.1 and 3.2. ■
From Lemmas 3.1 to 3.3, the number of rules that
have to be updated due to a label-transition operation
is bounded by the number of possible termsets in x.
The following theorem states exactly the rules in R
that have to be updated due to a transition operation.
Theorem 3.4: All rules in R that must be updated
due to xE→� (or xE→�) are those in Rx.
Proof: From Lemma 3.3, all rules {X →− c} ∈ R
that have to be updated due to operation xe→ED (or
xE→�) are those for which X ⊆ x. By definition,
Rx contains only and all such rules. ■
Updating O(X → (D) and O(X → (D) is straight-
forward. For operation xE→�, it suffices to iterate
on Rx, incrementing Q(X ∪ (D) and decrementing
Q(X ∪ (D). Similarly, for operation xE→�, it suf-
fices to iterate on Rx, incrementing Q(X ∪ (D) and
decrementing Q(X ∪ (D). The corresponding values
for O(X → (D) and O(X → (D) are simply obtained
by computing σ(X ∪�)
σ(X ) and σ(X ∪�)
σ(X ) , respectively.
</bodyText>
<page confidence="0.996429">
819
</page>
<subsectionHeader confidence="0.999338">
3.2 Best Entropy Cut Method
</subsectionHeader>
<bodyText confidence="0.964344066666667">
In this section we propose a method for finding the
activation threshold, αxmin, which is a fundamental
step of our Expectation-Maximization approach.
Definition 3.4: Let cy ∈ {e, ®} be the label asso-
ciated with an example y ∈ Dx. Consider Ne(Dx)
the number of examples in Dx for which cy=e. Sim-
ilarly, consider N®(Dx) the number of examples in
Dx for which cy=®.
Entropy Minimization: Our method searches for a
threshold αxmin that provides the best entropy cut in
the probability space induced by Dx. Specifically,
given examples {y1, y2,. . . , yk} in Dx, our method
first calculates α(yi, (D) for all yi ∈ Dx. Then, the
values for α(yi, e) are sorted in ascending order. In
an ideal case, there is a cut αxmin such that:
</bodyText>
<equation confidence="0.9647513125">
cyZ r ® if α(yi, e) ≤ αmin
= Sl
8 otherwise
low high
entropy entropy
0.00 1.00
� � � � � �
high low
entropy entropy
0.00 1.00
� � � � �
low low
entropy entropy
0.00 1.00
� � � � � �
best entropy cut
</equation>
<figureCaption confidence="0.934275">
Figure 1: Looking for the minimum entropy cut point.
</figureCaption>
<equation confidence="0.985376769230769">
(N�(O) )
|O |× log N�(O)
E(O) = − |O|
− (Ni��) × log N|��)) (3)
α(y3, e)
α(ys, e)
α(y7, e)
α(y2, e)
α(y1, e)
α(y4, e)
E(Of) = |Of(≤)|
|O |× E(Of(≤)) +
|Of(&gt;) |× E(Of(&gt;)) (4)
</equation>
<bodyText confidence="0.530499">
The basic idea is that any value for
</bodyText>
<figure confidence="0.925187102564103">
induces two
partitions over the space of values for
8) (i.e.,
one partition with values that are lower than
and another partition with values that are higher than
Our method sets
to the value that min-
imizes the average entropy of these two part
αxmin
α(yi,
αxmin,
αxurin).
αxmin
itions.
e)&gt;,
e)&gt;, ...},
that
(D)
8). Also, consider f a candi-
date value for
In this case,
is a sub-list
of
that is,
&lt;cy,
8)&gt;, ...},
such that for all pairs in
(D)
f. Sim-
ilarly,
&lt;cy,
e)&gt;, ...}, such that
for all pairs in Of(&gt;),
0) &gt;
words,
and
are partitions of
induced by
Our method works as follows. Firstly, it calculates
</figure>
<figureCaption confidence="0.304094">
the entropy in
</figureCaption>
<bodyText confidence="0.9175518">
as shown in Equation 3. Then,
it calculates the sum of the entropies in each par-
tition induced by f, according to Equation 4. Fi-
nally, it sets
to the value of f that minimizes
</bodyText>
<figure confidence="0.796359095238095">
{...,&lt;cy&apos;`,α(yi,
&lt;cyj,α(yj,
such
α(yi,
≤α(yj,
αxmin.
Of(≤)
O,
Of(≤)={...,
α(yi,
Of(≤),α(y,
≤
Of(&gt;)={...,
α(y,
α(y,
f. In other
Of(≤)
Of(&gt;)
O
f.
O,
</figure>
<page confidence="0.476804">
αxmin
</page>
<bodyText confidence="0.747923">
E(O)−E(Of), as illustrated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999426">
3.3 Disambiguation Algorithms
</subsectionHeader>
<bodyText confidence="0.999851375">
In this section we discuss four algorithms based
on our incremental EM approach and following our
Best Entropy Cut method. They differ among them-
selves on the granularity in which the classifier is up-
dated and on the label-transition operations allowed:
the classifier is updated incrementally after
each EM iteration (which may comprise sev-
eral label-tran
</bodyText>
<subsectionHeader confidence="0.559624">
|O|
</subsectionHeader>
<bodyText confidence="0.969447">
A1:
sition operations). Only opera-
owed.
</bodyText>
<listItem confidence="0.999312333333333">
• A2: the classifier is updated incrementally after
each EM iteration. Both operations xe—&apos;(D and
x®—&apos;e are allowed.
• A3: the classifier is updated incrementally after
each label-transition operation. Only operation
xe—&apos;® is allowed.
</listItem>
<bodyText confidence="0.96629">
However, there are more difficult cases, for which
itis not possible to obtain a perfect separation in the
probability space. Thus, we propose a more general
method to find the best cut in the probability space.
</bodyText>
<equation confidence="0.721728">
Definition 3.5: Consider a list of pairs
O=
tion xe—&apos;® is all
</equation>
<listItem confidence="0.981685">
• A4: the classifier is updated incrementally af-
ter each label-transition operation. Both opera-
tions xe—&apos;(D and x®—&apos;e are allowed.
</listItem>
<bodyText confidence="0.9183352">
αx
Once
min is calculated, it can be used to activate a
label-transition operation. Next we present the basic
definitions in order to detail this method.
</bodyText>
<page confidence="0.995421">
820
</page>
<sectionHeader confidence="0.903322" genericHeader="method">
4 Experimental Evaluation Table 1: Characteristics of each collection.
</sectionHeader>
<bodyText confidence="0.998952857142857">
In this section we analyze our algorithms using
standard measures such as AUC values. For each
positive+unlabeled (PU) corpus used in our evalu-
ation we randomly selected x% of the positive ex-
amples (P) to become unlabeled ones (U). This pro-
cedure enables us to control the uncertainty level
of the corpus. For each level we have a different
TPR-FPR combination, enabling us to draw ROC
curves.We repeated this procedure five times, so that
five executions were performed for each uncertainty
level. Tables 2–5 show the average for the five
runs. Wilcoxon significance tests were performed
(p&lt;0.05) and best results, including statistical ties,
are shown in bold.
</bodyText>
<subsectionHeader confidence="0.999006">
4.1 Baselines and Collections
</subsectionHeader>
<bodyText confidence="0.999949482758621">
Our baselines include namely SVMs (Joachims,
2006) and Biased SVMs (B-SVM (Liu et al., 2003)).
Although the simple SVM algorithm does not adapt
itself with unlabeled data, we decided to use it in
order to get a sense of the performance achieved
by simple baselines (in this case, unlabeled data is
simply used as negative examples). The B-SVM al-
gorithm uses a soft-margin SVM as the underlying
classifier, which is re-constructed from scratch after
each EM iteration. B-SVM employs a single tran-
sition threshold αmin for the entire corpus, instead
of a different threshold αx min for each x ∈ D. It
is representative of the state-of-the-art for learning
classifiers from PU data.
We employed two different Twitter collections.
The first collection, ORGANIZATIONS, is com-
posed of 10 corpora3 (O1 to O10). Each corpus con-
tains messages in English mentioning the name of
an organization (Bayer, Renault, among others). All
messages were labeled by five annotators. Label
means that the message is associated with the orga-
nization, whereas label e means the opposite.
The other collection, SOCCER TEAMS, contains
6 large-scale PU corpora (ST1 to ST6), taken from a
platform for real time event monitoring (the link to
this platform is omitted due to blind review). Each
corpus contains messages in Portuguese mentioning
the name/mascot of a Brazilian soccer team. Both
collections are summarized in Table 1.
</bodyText>
<footnote confidence="0.941834">
3http://nlp.uned.es/weps/
</footnote>
<table confidence="0.996265090909091">
P U P U
O1 404 10 ST1 216,991 251,198
O2 404 55 ST2 256,027 504,428
O3 349 116 ST3 160,706 509,670
O4 329 119 ST4 147,706 633,357
O5 335 133 ST5 35,021 168,669
O6 314 143 ST6 5,993 351,882
O7 292 148 − − −
O8 295 172 − − −
O9 273 165 − − −
O10 33 425 − − −
</table>
<subsectionHeader confidence="0.570017">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999920967741936">
All experiments were performed on a Linux PC with
an Intel Core 2 Duo 2.20GHz and 4GBytes RAM.
Next we discuss the disambiguation performance
and the computational efficiency of our algorithms.
ORGANIZATIONS Corpora: Table 2 shows av-
erage AUC values for each algorithm. Algorithm
A4 was the best performer in all cases, suggest-
ing the benefits of (i) enabling both types of label-
transition operations and (ii) keeping the classifier
up-to-date after each label-transition operation. Fur-
ther, algorithm A3 performed better than algorithm
A2 in most of the cases, indicating the importance of
keeping the classifier always up-to-date. On average
A1 provides gains of 4% when compared against B-
SVM, while A4 provides gains of more than 20%.
SOCCER TEAMS Corpora: Table 3 shows aver-
age AUC values for each algorithm. Again, algo-
rithm A4 was the best performer, providing gains
that are up to 13% when compared against the base-
line. Also, algorithm A3 performed better than al-
gorithm A2, and the effectiveness of Algorithm A1
is similar to the effectiveness of the baseline.
Since the SOCCER TEAMS collection is com-
posed of large-scale corpora, in addition to high
effectiveness, another important issue to be evalu-
ated is computational performance. Table 4 shows
the results obtained for the evaluation of our algo-
rithms. As it can be seen, algorithm A1 is the fastest
one, since it is the simplest one. Even though being
slower than algorithm A1, algorithm A4 runs, on av-
erage, 120 times faster than B-SVM.
</bodyText>
<page confidence="0.998712">
821
</page>
<tableCaption confidence="0.998229">
Table 2: Average AUC values for each algorithm.
</tableCaption>
<table confidence="0.999079545454546">
A1 A2 A3 A4 SVM B-SVM
O1 0.74 f 0.02 0.76 f 0.02 0.74 f 0.03 0.79 f 0.01 0.71 f 0.03 0.76 f 0.01
O2 0.77 f 0.02 0.78 f 0.02 0.70 f 0.03 0.82 f 0.02 0.73 f 0.03 0.75 f 0.02
O3 0.68 f 0.02 0.70 f 0.01 0.69 f 0.02 0.69 f 0.02 0.64 f 0.03 0.65 f 0.02
O4 0.68 f 0.02 0.68 f 0.02 0.70 f 0.01 0.72 f 0.02 0.63 f 0.02 0.66 f 0.02
O5 0.71 f 0.01 0.72 f 0.01 0.71 f 0.01 0.72 f 0.01 0.69 f 0.01 0.71 f 0.01
O6 0.73 f 0.01 0.73 f 0.01 0.75 f 0.01 0.75 f 0.01 0.68 f 0.02 0.70 f 0.01
O7 0.69 f 0.01 0.72 f 0.01 0.74 f 0.01 0.74 f 0.01 0.66 f 0.02 0.69 f 0.02
O8 0.65 f 0.02 0.68 f 0.02 0.69 f 0.02 0.72 f 0.01 0.61 f 0.03 0.63 f 0.03
O9 0.70 f 0.01 0.70 f 0.01 0.72 f 0.01 0.72 f 0.01 0.65 f 0.01 0.70 f 0.01
O10 0.70 f 0.01 0.74 f 0.02 0.71 f 0.02 0.75 f 0.02 0.61 f 0.03 0.66 f 0.02
</table>
<tableCaption confidence="0.99571">
Table 3: Average AUC values for each algorithm.
</tableCaption>
<table confidence="0.999723428571428">
A1 A2 A3 A4 SVM B-SVM
ST1 0.62 f 0.02 0.63 f 0.02 0.64 f 0.01 0.67 f 0.02 0.59 f 0.03 0.61 f 0.03
ST2 0.55 f 0.01 0.58 f 0.01 0.59 f 0.01 0.59 f 0.01 0.54 f 0.01 0.57 f 0.01
ST3 0.65 f 0.02 0.67 f 0.01 0.67 f 0.01 0.69 f 0.01 0.61 f 0.03 0.64 f 0.03
ST4 0.57 f 0.01 0.59 f 0.01 0.59 f 0.01 0.59 f 0.01 0.50 f 0.04 0.55 f 0.02
ST5 0.74 f 0.01 0.74 f 0.01 0.77 f 0.02 0.77 f 0.01 0.67 f 0.02 0.72 f 0.03
ST6 0.68 f 0.02 0.70 f 0.01 0.71 f 0.01 0.72 f 0.01 0.63 f 0.01 0.68 f 0.02
</table>
<tableCaption confidence="0.98123525">
Table 4: Average execution time (secs) for each algo-
rithm. The time spent by algorithm A1 is similar to the
time spent by algorithm A2. The time spent by algorithm
A3 is similar to the time spent by algorithm A4.
</tableCaption>
<table confidence="0.997913142857143">
A1(pt�A2) A3(pt� A4) SVM B-SVM
ST1 1,565 2,102 9,172 268,216
ST2 2,086 2,488 11,284 297,556
ST3 2,738 3,083 14,917 388,184
ST4 847 1,199 6,188 139,100
ST5 1,304 1,604 9,017 192,576
ST6 1,369 1,658 9,829 196,922
</table>
<sectionHeader confidence="0.999472" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.996775136363636">
In this paper we have introduced a novel EM ap-
proach, which employs a highly incremental un-
derlying classifier based on association rules, com-
pletely avoiding work replication. Further, two
label-transition operations are allowed, enabling the
correction of false-negatives and false-positives. We
proposed four algorithms based on our EM ap-
proach. Our algorithms employ an entropy min-
imization method, which finds the best transition
threshold for each example in D. All these prop-
erties make our algorithms appropriate for named
entity disambiguation under streaming data scenar-
ios. Our experiments involve Twitter data mention-
ing ambiguous named entities. These datasets were
obtained from real application scenarios and from
platforms currently in operation. We have shown
that three of our algorithms achieve significantly
higher disambiguation performance when compared
against a strong baseline (B-SVM), providing gains
ranging from 1% to 20%. Also importantly, for
large-scale streaming data, our algorithms are more
than 120 times faster than the baseline.
</bodyText>
<sectionHeader confidence="0.999496" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.965216857142857">
This research is supported by InWeb − The Brazil-
ian National Institute of Science and Technology for
the Web (CNPq grant no. 573871/2008-6), by UOL
(www.uol.com.br) through its UOL Bolsa Pesquisa
program (process number 20110215172500), and by
the authors’ individual grants from CAPES, CNPq
and Fapemig.
</bodyText>
<page confidence="0.995715">
822
</page>
<sectionHeader confidence="0.996254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836019230769">
R. Agrawal, T. Imielinski and A. Swami. 1993. Min-
ing association rules between sets of items in large
databases. In Proceedings of the 18th ACM SIGMOD
International Conference on Management of Data,
Washington, D.C., pages 207–216.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of the 17th International Conference
on Computational Linguistics, Montreal, Canada,
pages 79–85.
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In
Proceedings of the 14th International Conference on
the World Wide Web, Chiba, Japan, pages 463–470.
I. Bhattacharya and L. Getoor. 2007. Collective entity
resolution in relational data. ACM Transactions on
Knowledge Discovery from Data, 1.
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of the 11st Conference of the European Chap-
ter of the Association for Computational Linguistics,
Proceedings of the Conference, Trento, Italy, pages 9–
16.
F. De Comit´e, F. Denis, R. Gilleron and F. Letouzey.
1999. Positive and unlabeled examples help learning.
In Proceedings of the 10th International Conference
on Algorithmic Learning Theory, Tokyo, Japan, pages
219–230.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
the 4th Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning, Prague, Czech Republic, pages
708–716.
M. G. de Carvalho, A. H. F. Laender, M. A. Gonc¸alves,
and A. S. da Silva. 2006. Learning to deduplicate.
Proceedings of the 6th ACM/IEEE Joint Conference on
Digital Libraries, Chapel Hill, NC, USA. pages 41–50.
A. Dempster, N. Laird, and D. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1–38.
F. Denis. 1998. PAC learning from positive statistical
queries. In Proceedings of the Algorithmic Learning
Theory, 9th International Conference, Otzenhausen,
Germany, pages 112–126.
X. Dong, A. Y. Halevy, and J. Madhavan. 2005. Refer-
ence reconciliation in complex information spaces. In
Proceedings of the 24th ACM SIGMOD International
Conference on Management of Data, Baltimore, USA,
pages 85–96.
X. Han and J. Zhao. 2009. Named entity disambigua-
tion by leveraging wikipedia semantic knowledge. In
Proceedings of the 18th ACM conference on Informa-
tion and knowledge management, Hong Kong, China,
pages 215–224.
T. Hasegawa, S. Sekine and R. Grishman. 2004. Dis-
covering Relations among Named Entities from Large
Corpora. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, pages 415–422.
J. Hoffart, M. Yosef, I. Bordino, H. F¨urstenau, M. Pinkal,
M. Spaniol, B. Taneva, S. Thater and G. Weikum.
2011. Robust Disambiguation of Named Entities in
Text. In Proceedings of the 8th Conference on Empir-
ical Methods in Natural Language Processing, Edin-
burgh, UK, pages 782–792.
B. J. Jansen, M. Zhang, K. Sobel, and A. Chowdury.
2009. Twitter power: Tweets as electronic word of
mouth. JASIST, 60(11):2169–2188.
T. Joachims. 2006. Training linear SVMs in linear time.
In Proceedings of the 12th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, Philadelphia, USA, pages 217–226.
F. Letouzey, F. Denis, and R. Gilleron. 2000. Learning
from positive and unlabeled examples. In Proceedings
of the 11th International Conference on Algorithmic
Learning Theory, Sydney, Australia, pages 71–85.
X. Li and B. Liu. 2003. Learning to classify texts us-
ing positive and unlabeled data. In Proceedings of the
18th International Joint Conference on Artificial Intel-
ligence, Acapulco, Mexico, pages 587–592.
B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. 2003.
Building text classifiers using positive and unlabeled
examples. In Proceedings of the 3rd IEEE Interna-
tional Conference on Data Mining, Melbourne, USA,
pages 179–188.
X. Liu, S. Zhang, F. Wei and M. Zhou 2011. Recogniz-
ing Named Entities in Tweets. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
Portland, Oregon, USA, pages 359–367.
E. Minkov, W. W. Cohen, and A. Y. Ng. 2006. Contex-
tual search and name disambiguation in email using
graphs. In Proceedings of the 29th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, Seattle, USA, pages 27–34.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In Pro-
ceedings of the 6th International Conference on Com-
putational Linguistics and Intelligent Text Processing,
Mexico City, Mexico, pages 226–237.
I. S. Silva, J. Gomide, A. Veloso, W. Meira Jr. and R. Fer-
reira 2011. Effective sentiment stream analysis with
</reference>
<page confidence="0.989934">
823
</page>
<reference confidence="0.999265571428571">
self-augmenting training and demand-driven projec-
tion. In Proceedings of the 34th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, Beijing, China, pages 475–484.
L. Sarmento, A. Kehlenbeck, E. Oliveira, and L. Ungar.
2009. An approach to web-scale named-entity dis-
ambiguation. In Proceedings of the 6th International
Conference on Machine Learning and Data Mining in
Pattern Recognition, Leipzig, Germany, pages 689–
703.
A. Veloso, W. Meira Jr., M. de Carvalho, B. Pˆossas,
S. Parthasarathy, and M. J. Zaki. 2002. Mining fre-
quent itemsets in evolving databases. In Proceedings
of the Second SIAM International Conference on Data
Mining, Arlington, USA.
A. Veloso, W. Meira Jr., and M. J. Zaki. 2006. Lazy
associative classification. In Proceedings of the 6th
IEEE International Conference on Data Mining, Hong
Kong, China, pages 645–654.
X. Wan, J. Gao, M. Li, and B. Ding. 2005. Person reso-
lution in person search results: Webhawk. In Proceed-
ings of the 14th ACM International Conference on In-
formation and Knowledge Management, Bremen, Ger-
many, pages 163–170.
M. Yosef, J. Hoffart, I. Bordino, M. Spaniol and
G. Weikum 2011. AIDA: An Online Tool for Ac-
curate Disambiguation of Named Entities in Text and
Tables. PVLDB, 4(12):1450–1453.
</reference>
<page confidence="0.998732">
824
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.618233">
<title confidence="0.999918">Named Entity Disambiguation in Streaming Data</title>
<author confidence="0.9342745">Adriano Altigran S da Meira Alberto H F</author>
<affiliation confidence="0.980671">Science Dept. University of Minas Science Dept. University of</affiliation>
<email confidence="0.860535">alti@dcc.ufam.edu.br</email>
<abstract confidence="0.990993185185185">The named entity disambiguation task is to resolve the many-to-many correspondence between ambiguous names and the unique realworld entity. This task can be modeled as a classification problem, provided that positive and negative examples are available for learning binary classifiers. High-quality senseannotated data, however, are hard to be obtained in streaming environments, since the training corpus would have to be constantly updated in order to accomodate the fresh data coming on the stream. On the other hand, few positive examples plus large amounts of unlabeled data may be easily acquired. Producing binary classifiers directly from this data, however, leads to poor disambiguation performance. Thus, we propose to enhance the quality of the classifiers using finer-grained variations of the well-known Expectation- Maximization (EM) algorithm. We conducted a systematic evaluation using Twitter streaming data and the results show that our classifiers are extremely effective, providing improvements ranging from 1% to 20%, when compared to the current state-of-the-art biased SVMs, being more than 120 times faster.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>T Imielinski</author>
<author>A Swami</author>
</authors>
<title>Mining association rules between sets of items in large databases.</title>
<date>1993</date>
<booktitle>In Proceedings of the 18th ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>207--216</pages>
<location>Washington, D.C.,</location>
<contexts>
<context position="15824" citStr="Agrawal et al., 1993" startWordPosition="2526" endWordPosition="2529">sideration two possible update granularities: • the classifier is updated after each EM iteration. • the classifier is updated after each labeltransition operation. Incremental Classifier: As already discussed, the classifier must be constantly updated during the EM process. In this case, well-established classifiers, such as SVMs (Joachims, 2006), have to be learned entirely from scratch, replicating work by large. Thus, we propose as an alternative the use of Lazy Associative Classifiers (Veloso et al., 2006). Definition 3.2: A classification rule is a specialized association rule {X →− c} (Agrawal et al., 1993), where the antecedent X is a set of terms (i.e., a termset), and the consequent c indicates if the prediction is positive or negative (i.e., c ∈ {(D, e}). The domain for X is the vocabulary of D. The cardinality of rule {X → c} is given by the number of terms in the antecedent, that is |X |. The support of X is denoted as Q(X), and is the number of examples in D having X as a subset. The confidence of rule {X → c} is denoted as B(X →− c), and is the conditional probability of c given the termset X, that is, B(X →− c) = °(X∪c) U(X) . In this context, a classifier is denoted as R, and it is com</context>
</contexts>
<marker>Agrawal, Imielinski, Swami, 1993</marker>
<rawString>R. Agrawal, T. Imielinski and A. Swami. 1993. Mining association rules between sets of items in large databases. In Proceedings of the 18th ACM SIGMOD International Conference on Management of Data, Washington, D.C., pages 207–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based crossdocument coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>79--85</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="8194" citStr="Bagga and Baldwin, 1998" startWordPosition="1266" endWordPosition="1269">formation taken from the header of the messages leads to establish relationships between users and building a co-reference graph. In Web pages, reference information come naturally from links. Such graph-based approach could hardly be applied to the context we consider, in which the implied relationships between entities mentioned in a given micro-blog message are not clearly defined. In the case of textual corpora, traditional disambiguation methods represent entity names and their context (Hasegawa et al., 2004) (i.e., words, phrases and other names occurring near them) as weighted vectors (Bagga and Baldwin, 1998; Pedersen et al., 2005). To evaluate whether two names refer to the same entity, these methods compute the similar816 ity between these vectors. Clusters of co-referent names are then built based on such similarity measure. Although effective for the tasks considered in these papers, the simplistic BOW-based approaches they adopt are not suitable for cases in which the context is harder to capture due to the small number of terms available or to informal writing style. To address these problems, some authors argue that contextual information may be enriched with knowledge from external source</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-based crossdocument coreferencing using the vector space model. In Proceedings of the 17th International Conference on Computational Linguistics, Montreal, Canada, pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>A McCallum</author>
</authors>
<title>Disambiguating web appearances of people in a social network.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th International Conference on the World Wide Web,</booktitle>
<pages>463--470</pages>
<location>Chiba, Japan,</location>
<contexts>
<context position="7509" citStr="Bekkerman and McCallum, 2005" startWordPosition="1160" endWordPosition="1163">ng roughly 120 times faster. 2 Related Work In the context of databases, traditional entity disambiguation methods rely on similarity functions over attributes associated to the entities (de Carvalho et al., 2012). Obviously, such an approach is unfeasible for the scenario we consider here. Still on databases, Bhattacharya and Getoor (2007) and Dong et. al (2005) propose graph-based disambiguation methods that generate clusters of coreferent entities using known relationships between entities of several types. Methods to disambiguate person names in e-mail (Minkov et al., 2006) and Web pages (Bekkerman and McCallum, 2005; Wan et al., 2005) have employed similar ideas. In emails, information taken from the header of the messages leads to establish relationships between users and building a co-reference graph. In Web pages, reference information come naturally from links. Such graph-based approach could hardly be applied to the context we consider, in which the implied relationships between entities mentioned in a given micro-blog message are not clearly defined. In the case of textual corpora, traditional disambiguation methods represent entity names and their context (Hasegawa et al., 2004) (i.e., words, phra</context>
</contexts>
<marker>Bekkerman, McCallum, 2005</marker>
<rawString>R. Bekkerman and A. McCallum. 2005. Disambiguating web appearances of people in a social network. In Proceedings of the 14th International Conference on the World Wide Web, Chiba, Japan, pages 463–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Bhattacharya</author>
<author>L Getoor</author>
</authors>
<title>Collective entity resolution in relational data.</title>
<date>2007</date>
<journal>ACM Transactions on Knowledge Discovery from Data,</journal>
<volume>1</volume>
<contexts>
<context position="7223" citStr="Bhattacharya and Getoor (2007)" startWordPosition="1117" endWordPosition="1120">tion performance is investigated by varying the proportion of falsenegative examples in the unlabeled dataset. Our algorithms are compared against a state-of-the-art technique for named entity disambiguation based on classifiers, providing performance gains ranging from 1% to 20% and being roughly 120 times faster. 2 Related Work In the context of databases, traditional entity disambiguation methods rely on similarity functions over attributes associated to the entities (de Carvalho et al., 2012). Obviously, such an approach is unfeasible for the scenario we consider here. Still on databases, Bhattacharya and Getoor (2007) and Dong et. al (2005) propose graph-based disambiguation methods that generate clusters of coreferent entities using known relationships between entities of several types. Methods to disambiguate person names in e-mail (Minkov et al., 2006) and Web pages (Bekkerman and McCallum, 2005; Wan et al., 2005) have employed similar ideas. In emails, information taken from the header of the messages leads to establish relationships between users and building a co-reference graph. In Web pages, reference information come naturally from links. Such graph-based approach could hardly be applied to the co</context>
</contexts>
<marker>Bhattacharya, Getoor, 2007</marker>
<rawString>I. Bhattacharya and L. Getoor. 2007. Collective entity resolution in relational data. ACM Transactions on Knowledge Discovery from Data, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>9--16</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="8878" citStr="Bunescu and Pasca, 2006" startWordPosition="1377" endWordPosition="1380"> to the same entity, these methods compute the similar816 ity between these vectors. Clusters of co-referent names are then built based on such similarity measure. Although effective for the tasks considered in these papers, the simplistic BOW-based approaches they adopt are not suitable for cases in which the context is harder to capture due to the small number of terms available or to informal writing style. To address these problems, some authors argue that contextual information may be enriched with knowledge from external sources, such as search results and the Wikipedia (Cucerzan, 2007; Bunescu and Pasca, 2006; Han and Zhao, 2009). While such a strategy is feasible in an off-line setting, two problems arise when monitoring streams of micro-blog messages. First, gathering information from external sources through the Internet can be costly and, second, informal mentions to named entities make it hard to look for related information in such sources. The disambiguation methods we propose fall into a learning scenario known as PU (positive and unlabeled) learning (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000), in which a classifier is built from a set of positive examples </context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>R. Bunescu and M. Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, Trento, Italy, pages 9– 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F De Comit´e</author>
<author>F Denis</author>
<author>R Gilleron</author>
<author>F Letouzey</author>
</authors>
<title>Positive and unlabeled examples help learning.</title>
<date>1999</date>
<booktitle>In Proceedings of the 10th International Conference on Algorithmic Learning Theory,</booktitle>
<pages>219--230</pages>
<location>Tokyo, Japan,</location>
<marker>De Comit´e, Denis, Gilleron, Letouzey, 1999</marker>
<rawString>F. De Comit´e, F. Denis, R. Gilleron and F. Letouzey. 1999. Positive and unlabeled examples help learning. In Proceedings of the 10th International Conference on Algorithmic Learning Theory, Tokyo, Japan, pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>708--716</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8853" citStr="Cucerzan, 2007" startWordPosition="1375" endWordPosition="1376"> two names refer to the same entity, these methods compute the similar816 ity between these vectors. Clusters of co-referent names are then built based on such similarity measure. Although effective for the tasks considered in these papers, the simplistic BOW-based approaches they adopt are not suitable for cases in which the context is harder to capture due to the small number of terms available or to informal writing style. To address these problems, some authors argue that contextual information may be enriched with knowledge from external sources, such as search results and the Wikipedia (Cucerzan, 2007; Bunescu and Pasca, 2006; Han and Zhao, 2009). While such a strategy is feasible in an off-line setting, two problems arise when monitoring streams of micro-blog messages. First, gathering information from external sources through the Internet can be costly and, second, informal mentions to named entities make it hard to look for related information in such sources. The disambiguation methods we propose fall into a learning scenario known as PU (positive and unlabeled) learning (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000), in which a classifier is built from a </context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of the 4th Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Prague, Czech Republic, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G de Carvalho</author>
<author>A H F Laender</author>
<author>M A Gonc¸alves</author>
<author>A S da Silva</author>
</authors>
<title>Learning to deduplicate.</title>
<date>2006</date>
<booktitle>Proceedings of the 6th ACM/IEEE Joint Conference on Digital Libraries,</booktitle>
<pages>41--50</pages>
<location>Chapel Hill, NC, USA.</location>
<marker>de Carvalho, Laender, Gonc¸alves, Silva, 2006</marker>
<rawString>M. G. de Carvalho, A. H. F. Laender, M. A. Gonc¸alves, and A. S. da Silva. 2006. Learning to deduplicate. Proceedings of the 6th ACM/IEEE Joint Conference on Digital Libraries, Chapel Hill, NC, USA. pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="5134" citStr="Dempster et al., 1977" startWordPosition="791" endWordPosition="794">s, a large amount of unlabeled data, augmented with a small amount of (likely) positive examples, can be collected automatically from the message stream (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000). Binary classifiers may be learned from such data by considering unlabeled data as negative examples. This strategy, however, leads to classifiers with poor disambiguation performance, due to a potentially large number of false-negative examples. In this paper we propose to refine binary classifiers iteratively, by performing Expectation-Maximization (EM) approaches (Dempster et al., 1977). Basically, a partial classifier is used to evaluate the likelihood of an unlabeled example being a positive example or a negative example, thus automatically and (continuously) creating a labeled training corpus. This process continues iteratively by changing the label of some examples (an operation we call label-transition), so that, after some iterations, the combination of labels is expected to converge to the one for which the observed data is most likely. Based on such an approach, we introduce novel disambiguation algorithms that differ among themselves on the granularity in which the </context>
<context position="12578" citStr="Dempster et al., 1977" startWordPosition="1990" endWordPosition="1994"> entity name may appear outside its profile. Thus, the collected examples are not totally reliable, and disambiguation performance would be seriously compromised if classifiers were built upon these uncertain examples directly. 3.1 Expectation-Maximization Approach In this paper we hypothesize that it is worthwhile to enhance the reliability of unlabeled examples, provided that this type of data is inexpensive and the enhancement effort will be then rewarded with an improvement in disambiguation performance. Thus, we propose a new approach based on the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). We assume two scenarios: • the training corpus D is composed of a small set of truly positive examples plus a large amount of unlabeled examples. • the training corpus D is composed of a small set of potentially positive examples plus a large amount of unlabeled examples. 817 In both scenarios, unlabeled examples are initially treated as negative ones, so that classifiers can be built from D. Therefore, in both scenarios, D may contain false-negatives. In the second scenario, however, D may also contain false-positives. Definition 3.1: The label-transition operation xe→® turns a negative exa</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Denis</author>
</authors>
<title>PAC learning from positive statistical queries.</title>
<date>1998</date>
<booktitle>In Proceedings of the Algorithmic Learning Theory, 9th International Conference,</booktitle>
<pages>112--126</pages>
<location>Otzenhausen, Germany,</location>
<contexts>
<context position="4695" citStr="Denis, 1998" startWordPosition="730" endWordPosition="731"> learning methods, for instance, with the application of binary classifiers. Such methods, however, suffer from a data acquisition bottleneck, since they are based on training datasets that are built by skilled human annotators who manually inspect the messages. This annotation process is usually lengthy and laborious, being clearly unfeasible to be adopted in data streaming scenarios. As an alternative to such manual process, a large amount of unlabeled data, augmented with a small amount of (likely) positive examples, can be collected automatically from the message stream (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000). Binary classifiers may be learned from such data by considering unlabeled data as negative examples. This strategy, however, leads to classifiers with poor disambiguation performance, due to a potentially large number of false-negative examples. In this paper we propose to refine binary classifiers iteratively, by performing Expectation-Maximization (EM) approaches (Dempster et al., 1977). Basically, a partial classifier is used to evaluate the likelihood of an unlabeled example being a positive example or a negative example, thus automatically a</context>
<context position="9367" citStr="Denis, 1998" startWordPosition="1458" endWordPosition="1459">ed with knowledge from external sources, such as search results and the Wikipedia (Cucerzan, 2007; Bunescu and Pasca, 2006; Han and Zhao, 2009). While such a strategy is feasible in an off-line setting, two problems arise when monitoring streams of micro-blog messages. First, gathering information from external sources through the Internet can be costly and, second, informal mentions to named entities make it hard to look for related information in such sources. The disambiguation methods we propose fall into a learning scenario known as PU (positive and unlabeled) learning (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000), in which a classifier is built from a set of positive examples plus unlabeled data. Most of the approaches for PU learning, such as the biased-SVM approach (Li and Liu, 2003), are based on extracting negative examples from unlabeled data. We notice that existing approaches for PU learning are not likely to scale given the restrictions imposed by streaming data. Thus, we propose highly incremental approaches, which are able to process large-scale streaming data. 3 Disambiguation in Streaming Data Consider a stream of messages from a micro-blog cha</context>
</contexts>
<marker>Denis, 1998</marker>
<rawString>F. Denis. 1998. PAC learning from positive statistical queries. In Proceedings of the Algorithmic Learning Theory, 9th International Conference, Otzenhausen, Germany, pages 112–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Dong</author>
<author>A Y Halevy</author>
<author>J Madhavan</author>
</authors>
<title>Reference reconciliation in complex information spaces.</title>
<date>2005</date>
<booktitle>In Proceedings of the 24th ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>85--96</pages>
<location>Baltimore, USA,</location>
<marker>Dong, Halevy, Madhavan, 2005</marker>
<rawString>X. Dong, A. Y. Halevy, and J. Madhavan. 2005. Reference reconciliation in complex information spaces. In Proceedings of the 24th ACM SIGMOD International Conference on Management of Data, Baltimore, USA, pages 85–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Han</author>
<author>J Zhao</author>
</authors>
<title>Named entity disambiguation by leveraging wikipedia semantic knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management,</booktitle>
<pages>215--224</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="8899" citStr="Han and Zhao, 2009" startWordPosition="1381" endWordPosition="1384">e methods compute the similar816 ity between these vectors. Clusters of co-referent names are then built based on such similarity measure. Although effective for the tasks considered in these papers, the simplistic BOW-based approaches they adopt are not suitable for cases in which the context is harder to capture due to the small number of terms available or to informal writing style. To address these problems, some authors argue that contextual information may be enriched with knowledge from external sources, such as search results and the Wikipedia (Cucerzan, 2007; Bunescu and Pasca, 2006; Han and Zhao, 2009). While such a strategy is feasible in an off-line setting, two problems arise when monitoring streams of micro-blog messages. First, gathering information from external sources through the Internet can be costly and, second, informal mentions to named entities make it hard to look for related information in such sources. The disambiguation methods we propose fall into a learning scenario known as PU (positive and unlabeled) learning (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000), in which a classifier is built from a set of positive examples plus unlabeled data. </context>
</contexts>
<marker>Han, Zhao, 2009</marker>
<rawString>X. Han and J. Zhao. 2009. Named entity disambiguation by leveraging wikipedia semantic knowledge. In Proceedings of the 18th ACM conference on Information and knowledge management, Hong Kong, China, pages 215–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>Discovering Relations among Named Entities from Large Corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>415--422</pages>
<location>Barcelona,</location>
<contexts>
<context position="8090" citStr="Hasegawa et al., 2004" startWordPosition="1250" endWordPosition="1253">d Web pages (Bekkerman and McCallum, 2005; Wan et al., 2005) have employed similar ideas. In emails, information taken from the header of the messages leads to establish relationships between users and building a co-reference graph. In Web pages, reference information come naturally from links. Such graph-based approach could hardly be applied to the context we consider, in which the implied relationships between entities mentioned in a given micro-blog message are not clearly defined. In the case of textual corpora, traditional disambiguation methods represent entity names and their context (Hasegawa et al., 2004) (i.e., words, phrases and other names occurring near them) as weighted vectors (Bagga and Baldwin, 1998; Pedersen et al., 2005). To evaluate whether two names refer to the same entity, these methods compute the similar816 ity between these vectors. Clusters of co-referent names are then built based on such similarity measure. Although effective for the tasks considered in these papers, the simplistic BOW-based approaches they adopt are not suitable for cases in which the context is harder to capture due to the small number of terms available or to informal writing style. To address these prob</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>T. Hasegawa, S. Sekine and R. Grishman. 2004. Discovering Relations among Named Entities from Large Corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain, pages 415–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffart</author>
<author>M Yosef</author>
<author>I Bordino</author>
<author>H F¨urstenau</author>
<author>M Pinkal</author>
<author>M Spaniol</author>
<author>B Taneva</author>
<author>S Thater</author>
<author>G Weikum</author>
</authors>
<title>Robust Disambiguation of Named Entities in Text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 8th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>782--792</pages>
<location>Edinburgh, UK,</location>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>J. Hoffart, M. Yosef, I. Bordino, H. F¨urstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater and G. Weikum. 2011. Robust Disambiguation of Named Entities in Text. In Proceedings of the 8th Conference on Empirical Methods in Natural Language Processing, Edinburgh, UK, pages 782–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>M Zhang</author>
<author>K Sobel</author>
<author>A Chowdury</author>
</authors>
<title>Twitter power: Tweets as electronic word of mouth.</title>
<date>2009</date>
<journal>JASIST,</journal>
<volume>60</volume>
<issue>11</issue>
<contexts>
<context position="3448" citStr="Jansen et al., 2009" startWordPosition="534" endWordPosition="537">s make hard the identification of entities and the semantics of their relationships (Liu et al., 2011). Further, the scarcity of text in the messages makes it even harder to properly characterize a common context for the entities. Second, as we need to monitor messages that keep coming at a fast pace, we cannot afford to gather information from external sources on-the-fly. Finally, fresh data coming in the stream introduces new patterns, quickly invalidating static disambiguation models. 2Twitter is one of the fastest-growing micro-blog channels, and an authoritative source for breaking news (Jansen et al., 2009). 815 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 815–824, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics We hypothesize that the lack of information in each individual message and from external sources can be compensated by using information obtained from the large and diverse amount of text in a stream of messages taken as a whole, that is, thousands of messages per second coming from distinct sources. The information embedded in such a stream of messages may be exploited for entity disambiguation t</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>B. J. Jansen, M. Zhang, K. Sobel, and A. Chowdury. 2009. Twitter power: Tweets as electronic word of mouth. JASIST, 60(11):2169–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>217--226</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="15552" citStr="Joachims, 2006" startWordPosition="2484" endWordPosition="2485">ion under streaming data. A specific challenge is that the proposed methods perform several transition operations during each EM iteration, and each transition operation may invalidate parts of the current classifier, which must be properly updated. We take into consideration two possible update granularities: • the classifier is updated after each EM iteration. • the classifier is updated after each labeltransition operation. Incremental Classifier: As already discussed, the classifier must be constantly updated during the EM process. In this case, well-established classifiers, such as SVMs (Joachims, 2006), have to be learned entirely from scratch, replicating work by large. Thus, we propose as an alternative the use of Lazy Associative Classifiers (Veloso et al., 2006). Definition 3.2: A classification rule is a specialized association rule {X →− c} (Agrawal et al., 1993), where the antecedent X is a set of terms (i.e., a termset), and the consequent c indicates if the prediction is positive or negative (i.e., c ∈ {(D, e}). The domain for X is the vocabulary of D. The cardinality of rule {X → c} is given by the number of terms in the antecedent, that is |X |. The support of X is denoted as Q(X</context>
<context position="25627" citStr="Joachims, 2006" startWordPosition="4367" endWordPosition="4368">ed in our evaluation we randomly selected x% of the positive examples (P) to become unlabeled ones (U). This procedure enables us to control the uncertainty level of the corpus. For each level we have a different TPR-FPR combination, enabling us to draw ROC curves.We repeated this procedure five times, so that five executions were performed for each uncertainty level. Tables 2–5 show the average for the five runs. Wilcoxon significance tests were performed (p&lt;0.05) and best results, including statistical ties, are shown in bold. 4.1 Baselines and Collections Our baselines include namely SVMs (Joachims, 2006) and Biased SVMs (B-SVM (Liu et al., 2003)). Although the simple SVM algorithm does not adapt itself with unlabeled data, we decided to use it in order to get a sense of the performance achieved by simple baselines (in this case, unlabeled data is simply used as negative examples). The B-SVM algorithm uses a soft-margin SVM as the underlying classifier, which is re-constructed from scratch after each EM iteration. B-SVM employs a single transition threshold αmin for the entire corpus, instead of a different threshold αx min for each x ∈ D. It is representative of the state-of-the-art for learn</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>T. Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Philadelphia, USA, pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Letouzey</author>
<author>F Denis</author>
<author>R Gilleron</author>
</authors>
<title>Learning from positive and unlabeled examples.</title>
<date>2000</date>
<booktitle>In Proceedings of the 11th International Conference on Algorithmic Learning Theory,</booktitle>
<pages>71--85</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4741" citStr="Letouzey et al., 2000" startWordPosition="736" endWordPosition="739">th the application of binary classifiers. Such methods, however, suffer from a data acquisition bottleneck, since they are based on training datasets that are built by skilled human annotators who manually inspect the messages. This annotation process is usually lengthy and laborious, being clearly unfeasible to be adopted in data streaming scenarios. As an alternative to such manual process, a large amount of unlabeled data, augmented with a small amount of (likely) positive examples, can be collected automatically from the message stream (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000). Binary classifiers may be learned from such data by considering unlabeled data as negative examples. This strategy, however, leads to classifiers with poor disambiguation performance, due to a potentially large number of false-negative examples. In this paper we propose to refine binary classifiers iteratively, by performing Expectation-Maximization (EM) approaches (Dempster et al., 1977). Basically, a partial classifier is used to evaluate the likelihood of an unlabeled example being a positive example or a negative example, thus automatically and (continuously) creating a labeled training </context>
<context position="9413" citStr="Letouzey et al., 2000" startWordPosition="1464" endWordPosition="1467">rces, such as search results and the Wikipedia (Cucerzan, 2007; Bunescu and Pasca, 2006; Han and Zhao, 2009). While such a strategy is feasible in an off-line setting, two problems arise when monitoring streams of micro-blog messages. First, gathering information from external sources through the Internet can be costly and, second, informal mentions to named entities make it hard to look for related information in such sources. The disambiguation methods we propose fall into a learning scenario known as PU (positive and unlabeled) learning (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000), in which a classifier is built from a set of positive examples plus unlabeled data. Most of the approaches for PU learning, such as the biased-SVM approach (Li and Liu, 2003), are based on extracting negative examples from unlabeled data. We notice that existing approaches for PU learning are not likely to scale given the restrictions imposed by streaming data. Thus, we propose highly incremental approaches, which are able to process large-scale streaming data. 3 Disambiguation in Streaming Data Consider a stream of messages from a micro-blog channel such as Twitter and let n1, n2, ... , nN </context>
</contexts>
<marker>Letouzey, Denis, Gilleron, 2000</marker>
<rawString>F. Letouzey, F. Denis, and R. Gilleron. 2000. Learning from positive and unlabeled examples. In Proceedings of the 11th International Conference on Algorithmic Learning Theory, Sydney, Australia, pages 71–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>B Liu</author>
</authors>
<title>Learning to classify texts using positive and unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>587--592</pages>
<location>Acapulco, Mexico,</location>
<contexts>
<context position="9589" citStr="Li and Liu, 2003" startWordPosition="1495" endWordPosition="1498">arise when monitoring streams of micro-blog messages. First, gathering information from external sources through the Internet can be costly and, second, informal mentions to named entities make it hard to look for related information in such sources. The disambiguation methods we propose fall into a learning scenario known as PU (positive and unlabeled) learning (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000), in which a classifier is built from a set of positive examples plus unlabeled data. Most of the approaches for PU learning, such as the biased-SVM approach (Li and Liu, 2003), are based on extracting negative examples from unlabeled data. We notice that existing approaches for PU learning are not likely to scale given the restrictions imposed by streaming data. Thus, we propose highly incremental approaches, which are able to process large-scale streaming data. 3 Disambiguation in Streaming Data Consider a stream of messages from a micro-blog channel such as Twitter and let n1, n2, ... , nN be names used for mentioning a specific entity e in these messages. Our problem is to continually monitor the stream and predict whether an incoming message containing nz indee</context>
</contexts>
<marker>Li, Liu, 2003</marker>
<rawString>X. Li and B. Liu. 2003. Learning to classify texts using positive and unlabeled data. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco, Mexico, pages 587–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>Y Dai</author>
<author>X Li</author>
<author>W S Lee</author>
<author>P S Yu</author>
</authors>
<title>Building text classifiers using positive and unlabeled examples.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd IEEE International Conference on Data Mining,</booktitle>
<pages>179--188</pages>
<location>Melbourne, USA,</location>
<contexts>
<context position="4682" citStr="Liu et al., 2003" startWordPosition="726" endWordPosition="729">tion of supervised learning methods, for instance, with the application of binary classifiers. Such methods, however, suffer from a data acquisition bottleneck, since they are based on training datasets that are built by skilled human annotators who manually inspect the messages. This annotation process is usually lengthy and laborious, being clearly unfeasible to be adopted in data streaming scenarios. As an alternative to such manual process, a large amount of unlabeled data, augmented with a small amount of (likely) positive examples, can be collected automatically from the message stream (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000). Binary classifiers may be learned from such data by considering unlabeled data as negative examples. This strategy, however, leads to classifiers with poor disambiguation performance, due to a potentially large number of false-negative examples. In this paper we propose to refine binary classifiers iteratively, by performing Expectation-Maximization (EM) approaches (Dempster et al., 1977). Basically, a partial classifier is used to evaluate the likelihood of an unlabeled example being a positive example or a negative example, thus au</context>
<context position="9354" citStr="Liu et al., 2003" startWordPosition="1454" endWordPosition="1457">tion may be enriched with knowledge from external sources, such as search results and the Wikipedia (Cucerzan, 2007; Bunescu and Pasca, 2006; Han and Zhao, 2009). While such a strategy is feasible in an off-line setting, two problems arise when monitoring streams of micro-blog messages. First, gathering information from external sources through the Internet can be costly and, second, informal mentions to named entities make it hard to look for related information in such sources. The disambiguation methods we propose fall into a learning scenario known as PU (positive and unlabeled) learning (Liu et al., 2003; Denis, 1998; Comit´e et al., 1999; Letouzey et al., 2000), in which a classifier is built from a set of positive examples plus unlabeled data. Most of the approaches for PU learning, such as the biased-SVM approach (Li and Liu, 2003), are based on extracting negative examples from unlabeled data. We notice that existing approaches for PU learning are not likely to scale given the restrictions imposed by streaming data. Thus, we propose highly incremental approaches, which are able to process large-scale streaming data. 3 Disambiguation in Streaming Data Consider a stream of messages from a m</context>
<context position="25669" citStr="Liu et al., 2003" startWordPosition="4373" endWordPosition="4376">d x% of the positive examples (P) to become unlabeled ones (U). This procedure enables us to control the uncertainty level of the corpus. For each level we have a different TPR-FPR combination, enabling us to draw ROC curves.We repeated this procedure five times, so that five executions were performed for each uncertainty level. Tables 2–5 show the average for the five runs. Wilcoxon significance tests were performed (p&lt;0.05) and best results, including statistical ties, are shown in bold. 4.1 Baselines and Collections Our baselines include namely SVMs (Joachims, 2006) and Biased SVMs (B-SVM (Liu et al., 2003)). Although the simple SVM algorithm does not adapt itself with unlabeled data, we decided to use it in order to get a sense of the performance achieved by simple baselines (in this case, unlabeled data is simply used as negative examples). The B-SVM algorithm uses a soft-margin SVM as the underlying classifier, which is re-constructed from scratch after each EM iteration. B-SVM employs a single transition threshold αmin for the entire corpus, instead of a different threshold αx min for each x ∈ D. It is representative of the state-of-the-art for learning classifiers from PU data. We employed </context>
</contexts>
<marker>Liu, Dai, Li, Lee, Yu, 2003</marker>
<rawString>B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. 2003. Building text classifiers using positive and unlabeled examples. In Proceedings of the 3rd IEEE International Conference on Data Mining, Melbourne, USA, pages 179–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liu</author>
<author>S Zhang</author>
<author>F Wei</author>
<author>M Zhou</author>
</authors>
<title>Recognizing Named Entities in Tweets.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>359--367</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2930" citStr="Liu et al., 2011" startWordPosition="454" endWordPosition="457">o. We are given a stream of messages from a micro-blog channel such as Twitter2 and a list of names n1, n2, ... , nN used for mentioning a specific entity e. Our problem is to monitor the stream and predict whether an incoming message containing ni indeed refers to e (positive example) or not (negative example). This scenario brings challenges that must be overcome. First, micro-blog messages are composed of a small amount of words and they are written in informal, sometimes cryptic style. These characteristics make hard the identification of entities and the semantics of their relationships (Liu et al., 2011). Further, the scarcity of text in the messages makes it even harder to properly characterize a common context for the entities. Second, as we need to monitor messages that keep coming at a fast pace, we cannot afford to gather information from external sources on-the-fly. Finally, fresh data coming in the stream introduces new patterns, quickly invalidating static disambiguation models. 2Twitter is one of the fastest-growing micro-blog channels, and an authoritative source for breaking news (Jansen et al., 2009). 815 Proceedings of the 50th Annual Meeting of the Association for Computational </context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>X. Liu, S. Zhang, F. Wei and M. Zhou 2011. Recognizing Named Entities in Tweets. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, Oregon, USA, pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Minkov</author>
<author>W W Cohen</author>
<author>A Y Ng</author>
</authors>
<title>Contextual search and name disambiguation in email using graphs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>27--34</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="7465" citStr="Minkov et al., 2006" startWordPosition="1153" endWordPosition="1156">gains ranging from 1% to 20% and being roughly 120 times faster. 2 Related Work In the context of databases, traditional entity disambiguation methods rely on similarity functions over attributes associated to the entities (de Carvalho et al., 2012). Obviously, such an approach is unfeasible for the scenario we consider here. Still on databases, Bhattacharya and Getoor (2007) and Dong et. al (2005) propose graph-based disambiguation methods that generate clusters of coreferent entities using known relationships between entities of several types. Methods to disambiguate person names in e-mail (Minkov et al., 2006) and Web pages (Bekkerman and McCallum, 2005; Wan et al., 2005) have employed similar ideas. In emails, information taken from the header of the messages leads to establish relationships between users and building a co-reference graph. In Web pages, reference information come naturally from links. Such graph-based approach could hardly be applied to the context we consider, in which the implied relationships between entities mentioned in a given micro-blog message are not clearly defined. In the case of textual corpora, traditional disambiguation methods represent entity names and their contex</context>
</contexts>
<marker>Minkov, Cohen, Ng, 2006</marker>
<rawString>E. Minkov, W. W. Cohen, and A. Y. Ng. 2006. Contextual search and name disambiguation in email using graphs. In Proceedings of the 29th International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, USA, pages 27–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>A Purandare</author>
<author>A Kulkarni</author>
</authors>
<title>Name discrimination by clustering similar contexts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>226--237</pages>
<location>Mexico City, Mexico,</location>
<contexts>
<context position="8218" citStr="Pedersen et al., 2005" startWordPosition="1270" endWordPosition="1273">header of the messages leads to establish relationships between users and building a co-reference graph. In Web pages, reference information come naturally from links. Such graph-based approach could hardly be applied to the context we consider, in which the implied relationships between entities mentioned in a given micro-blog message are not clearly defined. In the case of textual corpora, traditional disambiguation methods represent entity names and their context (Hasegawa et al., 2004) (i.e., words, phrases and other names occurring near them) as weighted vectors (Bagga and Baldwin, 1998; Pedersen et al., 2005). To evaluate whether two names refer to the same entity, these methods compute the similar816 ity between these vectors. Clusters of co-referent names are then built based on such similarity measure. Although effective for the tasks considered in these papers, the simplistic BOW-based approaches they adopt are not suitable for cases in which the context is harder to capture due to the small number of terms available or to informal writing style. To address these problems, some authors argue that contextual information may be enriched with knowledge from external sources, such as search result</context>
</contexts>
<marker>Pedersen, Purandare, Kulkarni, 2005</marker>
<rawString>T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name discrimination by clustering similar contexts. In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing, Mexico City, Mexico, pages 226–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ferreira</author>
</authors>
<title>Effective sentiment stream analysis with self-augmenting training and demand-driven projection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>475--484</pages>
<location>Beijing, China,</location>
<marker>Ferreira, 2011</marker>
<rawString>I. S. Silva, J. Gomide, A. Veloso, W. Meira Jr. and R. Ferreira 2011. Effective sentiment stream analysis with self-augmenting training and demand-driven projection. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, Beijing, China, pages 475–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sarmento</author>
<author>A Kehlenbeck</author>
<author>E Oliveira</author>
<author>L Ungar</author>
</authors>
<title>An approach to web-scale named-entity disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition,</booktitle>
<pages>689--703</pages>
<location>Leipzig, Germany,</location>
<contexts>
<context position="1936" citStr="Sarmento et al., 2009" startWordPosition="292" endWordPosition="295">y effective, providing improvements ranging from 1% to 20%, when compared to the current state-of-the-art biased SVMs, being more than 120 times faster. 1 Introduction Human language is not exact. For instance, an entity1 may be referred by multiple names (i.e., polysemy), and also the same name may refer to different entities depending on the surrounding context (i.e., 1The term entity refers to anything that has a distinct, separate (materialized or not) existence. homonymy). The task of named entity disambiguation is to identify which names refer to the same entity in a textual collection (Sarmento et al., 2009; Yosef et al., 2011; Hoffart et al., 2011). The emergence of new communication technologies, such as micro-blog platforms, brought a humongous amount of textual mentions with ambiguous entity names, raising an urgent need for novel disambiguation approaches and algorithms. In this paper we address the named entity disambiguation task under a particularly challenging scenario. We are given a stream of messages from a micro-blog channel such as Twitter2 and a list of names n1, n2, ... , nN used for mentioning a specific entity e. Our problem is to monitor the stream and predict whether an incom</context>
</contexts>
<marker>Sarmento, Kehlenbeck, Oliveira, Ungar, 2009</marker>
<rawString>L. Sarmento, A. Kehlenbeck, E. Oliveira, and L. Ungar. 2009. An approach to web-scale named-entity disambiguation. In Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition, Leipzig, Germany, pages 689– 703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Veloso</author>
<author>W Meira Jr</author>
<author>M de Carvalho</author>
<author>B Pˆossas</author>
<author>S Parthasarathy</author>
<author>M J Zaki</author>
</authors>
<title>Mining frequent itemsets in evolving databases.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second SIAM International Conference on Data Mining,</booktitle>
<location>Arlington, USA.</location>
<marker>Veloso, Jr, de Carvalho, Pˆossas, Parthasarathy, Zaki, 2002</marker>
<rawString>A. Veloso, W. Meira Jr., M. de Carvalho, B. Pˆossas, S. Parthasarathy, and M. J. Zaki. 2002. Mining frequent itemsets in evolving databases. In Proceedings of the Second SIAM International Conference on Data Mining, Arlington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Veloso</author>
<author>W Meira Jr</author>
<author>M J Zaki</author>
</authors>
<title>Lazy associative classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th IEEE International Conference on Data Mining,</booktitle>
<pages>645--654</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="15719" citStr="Veloso et al., 2006" startWordPosition="2509" endWordPosition="2512">eration may invalidate parts of the current classifier, which must be properly updated. We take into consideration two possible update granularities: • the classifier is updated after each EM iteration. • the classifier is updated after each labeltransition operation. Incremental Classifier: As already discussed, the classifier must be constantly updated during the EM process. In this case, well-established classifiers, such as SVMs (Joachims, 2006), have to be learned entirely from scratch, replicating work by large. Thus, we propose as an alternative the use of Lazy Associative Classifiers (Veloso et al., 2006). Definition 3.2: A classification rule is a specialized association rule {X →− c} (Agrawal et al., 1993), where the antecedent X is a set of terms (i.e., a termset), and the consequent c indicates if the prediction is positive or negative (i.e., c ∈ {(D, e}). The domain for X is the vocabulary of D. The cardinality of rule {X → c} is given by the number of terms in the antecedent, that is |X |. The support of X is denoted as Q(X), and is the number of examples in D having X as a subset. The confidence of rule {X → c} is denoted as B(X →− c), and is the conditional probability of c given the t</context>
<context position="17946" citStr="Veloso et al., 2006" startWordPosition="2960" endWordPosition="2963">les in Rx are considered as valid votes when classifying x. Further, we denote as Rcx the subset of Rx containing only rules predicting c. Votes in Rcx have different weights, depending on the confidence of the corresponding rules. Weighted votes for c are averaged, giving the score for c with regard to x (Equation 1). Finally, the likelihood of x being a negative example is given by the normalized score (Equation 2). � s(x, c) = |O(X → |c) , with c ∈ {e, ED} (1) Rcx ( s(x, e) a x, e) = (2) s(x, �) + s(x, �) Training Projection and Demand-Driven Rule Extraction: Demand-driven rule extraction (Veloso et al., 2006) is a recent strategy used to avoid the huge search space for rules, by projecting the training corpus according to the example being processed. More specifically, rule extraction is delayed until an example x is given for classification. Then, terms in x are used as a filter that configures the training corpus D so that just rules that are applicable to x can be extracted. This filtering process produces a projected training corpus, denoted as Dx, which contains only terms that are present in x. As shown in (Silva et al., 2011), the number of rules extracted using this strategy grows polynomi</context>
</contexts>
<marker>Veloso, Jr, Zaki, 2006</marker>
<rawString>A. Veloso, W. Meira Jr., and M. J. Zaki. 2006. Lazy associative classification. In Proceedings of the 6th IEEE International Conference on Data Mining, Hong Kong, China, pages 645–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Gao</author>
<author>M Li</author>
<author>B Ding</author>
</authors>
<title>Person resolution in person search results: Webhawk.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>163--170</pages>
<location>Bremen, Germany,</location>
<contexts>
<context position="7528" citStr="Wan et al., 2005" startWordPosition="1164" endWordPosition="1167"> Related Work In the context of databases, traditional entity disambiguation methods rely on similarity functions over attributes associated to the entities (de Carvalho et al., 2012). Obviously, such an approach is unfeasible for the scenario we consider here. Still on databases, Bhattacharya and Getoor (2007) and Dong et. al (2005) propose graph-based disambiguation methods that generate clusters of coreferent entities using known relationships between entities of several types. Methods to disambiguate person names in e-mail (Minkov et al., 2006) and Web pages (Bekkerman and McCallum, 2005; Wan et al., 2005) have employed similar ideas. In emails, information taken from the header of the messages leads to establish relationships between users and building a co-reference graph. In Web pages, reference information come naturally from links. Such graph-based approach could hardly be applied to the context we consider, in which the implied relationships between entities mentioned in a given micro-blog message are not clearly defined. In the case of textual corpora, traditional disambiguation methods represent entity names and their context (Hasegawa et al., 2004) (i.e., words, phrases and other names</context>
</contexts>
<marker>Wan, Gao, Li, Ding, 2005</marker>
<rawString>X. Wan, J. Gao, M. Li, and B. Ding. 2005. Person resolution in person search results: Webhawk. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, Bremen, Germany, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yosef</author>
<author>J Hoffart</author>
<author>I Bordino</author>
<author>M Spaniol</author>
<author>G Weikum</author>
</authors>
<date>2011</date>
<booktitle>Online Tool for Accurate Disambiguation of Named Entities in Text and Tables. PVLDB,</booktitle>
<pages>4--12</pages>
<publisher>AIDA: An</publisher>
<contexts>
<context position="1956" citStr="Yosef et al., 2011" startWordPosition="296" endWordPosition="299">improvements ranging from 1% to 20%, when compared to the current state-of-the-art biased SVMs, being more than 120 times faster. 1 Introduction Human language is not exact. For instance, an entity1 may be referred by multiple names (i.e., polysemy), and also the same name may refer to different entities depending on the surrounding context (i.e., 1The term entity refers to anything that has a distinct, separate (materialized or not) existence. homonymy). The task of named entity disambiguation is to identify which names refer to the same entity in a textual collection (Sarmento et al., 2009; Yosef et al., 2011; Hoffart et al., 2011). The emergence of new communication technologies, such as micro-blog platforms, brought a humongous amount of textual mentions with ambiguous entity names, raising an urgent need for novel disambiguation approaches and algorithms. In this paper we address the named entity disambiguation task under a particularly challenging scenario. We are given a stream of messages from a micro-blog channel such as Twitter2 and a list of names n1, n2, ... , nN used for mentioning a specific entity e. Our problem is to monitor the stream and predict whether an incoming message containi</context>
</contexts>
<marker>Yosef, Hoffart, Bordino, Spaniol, Weikum, 2011</marker>
<rawString>M. Yosef, J. Hoffart, I. Bordino, M. Spaniol and G. Weikum 2011. AIDA: An Online Tool for Accurate Disambiguation of Named Entities in Text and Tables. PVLDB, 4(12):1450–1453.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>