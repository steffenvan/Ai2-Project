<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001092">
<title confidence="0.999027">
A Statistical Machine Translation Model Based on a Synthetic
Synchronous Grammar
</title>
<author confidence="0.999213">
Hongfei Jiang, Muyun Yang, Tiejun Zhao, Sheng Li and Bo Wang
</author>
<affiliation confidence="0.996853">
School of Computer Science and Technology
Harbin Institute of Technology
</affiliation>
<email confidence="0.987649">
{hfjiang,ymy,tjzhao,lisheng,bowang}@mtlab.hit.edu.cn
</email>
<sectionHeader confidence="0.982301" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999882166666667">
Recently, various synchronous grammars
are proposed for syntax-based machine
translation, e.g. synchronous context-free
grammar and synchronous tree (sequence)
substitution grammar, either purely for-
mal or linguistically motivated. Aim-
ing at combining the strengths of differ-
ent grammars, we describes a synthetic
synchronous grammar (SSG), which ten-
tatively in this paper, integrates a syn-
chronous context-free grammar (SCFG)
and a synchronous tree sequence substitu-
tion grammar (STSSG) for statistical ma-
chine translation. The experimental re-
sults on NIST MT05 Chinese-to-English
test set show that the SSG based transla-
tion system achieves significant improve-
ment over three baseline systems.
</bodyText>
<sectionHeader confidence="0.995085" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971021276596">
The use of various synchronous grammar based
formalisms has been a trend for statistical ma-
chine translation (SMT) (Wu, 1997; Eisner, 2003;
Galley et al., 2006; Chiang, 2007; Zhang et al.,
2008). The grammar formalism determines the in-
trinsic capacities and computational efficiency of
the SMT systems.
To evaluate the capacity of a grammar formal-
ism, two factors, i.e. generative power and expres-
sive power are usually considered (Su and Chang,
1990). The generative power refers to the abil-
ity to generate the strings of the language, and
the expressive power to the ability to describe the
same language with fewer or no extra ambigui-
ties. For the current synchronous grammars based
SMT, to some extent, the generalization ability of
the grammar rules (the usability of the rules for the
new sentences) can be considered as a kind of the
generative power of the grammar and the disam-
biguition ability to the rule candidates can be con-
sidered as an embodiment of expressive power.
However, the generalization ability and the dis-
ambiguition ability often contradict each other in
practice such that various grammar formalisms
in SMT are actually different trade-off be-
tween them. For instance, in our investiga-
tions for SMT (Section 3.1), the Formally SCFG
based hierarchical phrase-based model (here-
inafter FSCFG) (Chiang, 2007) has a better gen-
eralization capability than a Linguistically moti-
vated STSSG based model (hereinafter LSTSSG)
(Zhang et al., 2008), with 5% rules of the former
matched by NIST05 test set while only 3.5% rules
of the latter matched by the same test set. How-
ever, from expressiveness point of view, the for-
mer usually results in more ambiguities than the
latter.
To combine the strengths of different syn-
chronous grammars, this paper proposes a statisti-
cal machine translation model based on a synthetic
synchronous grammar (SSG) which syncretizes
FSCFG and LSTSSG. Moreover, it is noteworthy
that, from the combination point of view, our pro-
posed scheme can be considered as a novel system
combination method which goes beyond the ex-
isting post-decoding style combination of N-best
hypotheses from different systems.
</bodyText>
<sectionHeader confidence="0.80355" genericHeader="method">
2 The Translation Model Based on the
Synthetic Synchronous Grammar
</sectionHeader>
<subsectionHeader confidence="0.9985">
2.1 The Synthetic Synchronous Grammar
</subsectionHeader>
<bodyText confidence="0.584901">
Formally, the proposed Synthetic Synchronous
Grammar (SSG) is a tuple
</bodyText>
<equation confidence="0.991011">
G = (Es, Et, Ns, Nt, X, P)
</equation>
<bodyText confidence="0.999796333333333">
where Es(Et) is the alphabet set of source (target)
terminals, namely the vocabulary; Ns(Nt) is the
alphabet set of source (target) non-terminals, such
</bodyText>
<page confidence="0.634568">
125
</page>
<note confidence="0.9214225">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 125–128,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.990495611111111">
把 钢笔 给 我
Input: A source parse tree T (fJ1 )
Output: A target translation e
for u := 0 to J − 1 do
for v := 1 to J − u do
foreach rule r = (α, -y, ANT, AT , w) spanning
[v, v + u] do
if ANT of r is empty then
Add r into H[v, v + u];
end
else
Substitute the non-terminal leaf node pair
(Nsrc, Ntgt) with the hypotheses in the
hypotheses stack corresponding with Nsrc’s
span iteratively.
end
end
end
</figure>
<figureCaption confidence="0.999647">
Figure 1: A syntax tree pair example. Dotted lines
</figureCaption>
<bodyText confidence="0.9622128">
stands for the word alignments.
as the POS tags and the syntax labels; X repre-
sents the special nonterminal label in FSCFG; and
P is the grammar rule set which is the core part of
a grammar. Every rule r in P is as:
</bodyText>
<equation confidence="0.988373">
r = (a,`/,ANT, AT,W)
</equation>
<bodyText confidence="0.999991375">
where a E [{X}, N3, E3]+ is a sequence of one or
more source words in E3 and nonterminals sym-
bols in [{X}, N3];-y E [{X}, Nt, Et]+ is a se-
quence of one or more target words in Et and non-
terminals symbols in [{X}, Nt]; AT is a many-to-
many corresponding set which includes the align-
ments between the terminal leaf nodes from source
and target side, and ANT is a one-to-one corre-
sponding set which includes the synchronizing re-
lations between the non-terminal leaf nodes from
source and target side; w contains feature values
associated with each rule.
Through this formalization, we can see that
FSCFG rules and LSTSSG rules are both in-
cluded. However, we should point out that the
rules with mixture of X non-terminals and syn-
tactic non-terminals are not included in our cur-
rent implementation despite that they are legal
under the proposed formalism. The rule extrac-
tion in current implementation can be considered
as a combination of the ones in (Chiang, 2007)
and (Zhang et al., 2008). Given the sentence pair
in Figure 1, some SSG rules can be extracted as
illustrated in Figure 2.
</bodyText>
<subsectionHeader confidence="0.998426">
2.2 The SSG-based Translation Model
</subsectionHeader>
<bodyText confidence="0.9996165">
The translation in our SSG-based translation
model can be treated as a SSG derivation. A
derivation consists of a sequence of grammar rule
applications. To model the derivations as a latent
variable, we define the conditional probability dis-
tribution over the target translation a and the cor-
</bodyText>
<figure confidence="0.4975195">
end
Output the 1-best hypothesis in H[1, J] as the final translation.
</figure>
<figureCaption confidence="0.998392">
Figure 3: The pseudocode for the decoding.
</figureCaption>
<bodyText confidence="0.9997175">
responding derivation d of a given source sentence
f as
</bodyText>
<equation confidence="0.98673">
(1) nA(d, e|f) = QA(f)
</equation>
<bodyText confidence="0.99960175">
where Hk is a feature function ,Ak is the corre-
sponding feature weight and QA(f) is a normal-
ization factor for each derivation of f. The main
challenge of SSG-based model is how to distin-
guish and weight the different kinds of derivations
. For a simple illustration, using the rules listed in
Figure 2, three derivations can be produced for the
sentence pair in Figure 1 by the proposed model:
</bodyText>
<equation confidence="0.999873333333333">
d1 = (R4, R1, R2)
d2 = (R6, R7, R8)
d3 = (R4, R7, R2)
</equation>
<bodyText confidence="0.99800825">
All of them are SSG derivations while d1 is also a
FSCFG derivation, d2 is also a LSTSSG deriva-
tion. Ideally, the model is supposed to be able
to weight them differently and to prefer the better
derivation, which deserves intensive study. Some
sophisticated features can be designed for this is-
sue. For example, some features related with
structure richness and grammar consistency1 of a
derivation should be designed to distinguish the
derivations involved various heterogeneous rule
applications. For the page limit and the fair com-
parison, we only adopt the conventional features
as in (Zhang et al., 2008) in our current implemen-
tation.
1This relates with reviewers’ questions: “can a rule ex-
pecting an NN accept an X?” and “... the interaction between
the two typed of rules ...”. In our study in progress, we
would design some features to distinguish the derivation steps
which fulfill the expectation or not, to measure how much
heterogeneous rules are applied in a derivation and so on.
</bodyText>
<figure confidence="0.9553474375">
exp Ek AkHk(d, e, f)
126
Figure 2: Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure
1. R1-R3 are bilingual phrase rules, R4-R5 are FSCFG rules and R6-R8 are LSTSSG rules.
NN
VV VB
to me
1
给
把
1
1
钢笔
BA
PN
1
R1 钢笔 1 the pen 1 R2 我 1 to me 1 R3 钢笔 1 给 2 Give 2 the pen 1
R5 钢笔 1 X[1] 我 2 X[1] the pen 1 to me 2
R4 把 X[1] 给 1 X[2] Give 1 X[1] X[2]
PP
NP
NN[1]
VV[2]
VB[2] NP[1]
TO PRP
RB
R6
R7
1
我
1 Give
DT NN
</figure>
<subsectionHeader confidence="0.988045">
2.3 Decoding
</subsectionHeader>
<bodyText confidence="0.9998205">
For efficiency, our model approximately search for
the single ‘best’ derivation using beam search as
</bodyText>
<equation confidence="0.808767">
(2) (e, d) = argmax
e,d � k Akhk(d, e, f) I-
</equation>
<bodyText confidence="0.999988322580645">
The major challenge for such a SSG-based de-
coder is how to apply the heterogeneous rules in a
derivation. For example, (Chiang, 2007) adopts a
CKY style span-based decoding while (Liu et al.,
2006) applies a linguistically syntax node based
bottom-up decoding, which are difficult to inte-
grate. Fortunately, our current SSG syncretizes
FSCFG and LSTSSG. And the conventional de-
codings of both FSCFG and LSTSSG are span-
based expansion. Thus, it would be a natural way
for our SSG-based decoder to conduct a span-
based beam search. The search procedure is given
by the pseudocode in Figure 3. A hypotheses
stack H[i, j] (similar to the “chart cell” in CKY
parsing) is arranged for each span [i, j] for stor-
ing the translation hypotheses. The hypotheses
stacks are ordered such that every span is trans-
lated after its possible antecedents: smaller spans
before larger spans. For translating each span
[i, j], the decoder traverses each usable rule r =
(α, ry, ANT, AT, )). If there is no nonterminal
leaf node in r, the target side -y will be added into
H[i, j] as the candidate hypothesis. Otherwise, the
nonterminal leaf nodes in r should be substituted
iteratively by the corresponding hypotheses until
all nonterminal leaf nodes are processed. The key
feature of our decoder is that the derivations are
based on synthetic grammar, so that one derivation
may consist of applications of heterogeneous rules
(Please see d3 in Section 2.2 as a simple demon-
stration).
</bodyText>
<sectionHeader confidence="0.714693" genericHeader="method">
3
Experiments and Discussions
</sectionHeader>
<bodyText confidence="0.874727">
Our system, named HITREE, is implemented in
standard C++ and STL. In this section we report
</bodyText>
<table confidence="0.972431">
Extracted(k) Scored(k)(S/E%) Filtered(k)(F/S%)
BP 11,137 4,613(41.4%) 323(0.5%)
LSTSSG the pen 28,497(62.5%) 984(3.5%)
45,580
FSCFG 59,339 25,520(43.0%) 1,266(5.0%)
HITREE 93,782 49,404(52.7%) 1,927(3.9%)
</table>
<tableCaption confidence="0.883838">
Table 1: The statistics of the counts of the rules in
different phases. ‘k’ means one thousand.
</tableCaption>
<bodyText confidence="0.999649875">
on experiments with Chinese-to-English transla-
tion base on it. We used FBIS Chinese-to-English
parallel corpora (7.2M+9.2M words) as the train-
ing data. We also used SRI Language Model-
ing Toolkit to train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus(181M words). NIST MT2002 test set is used
as the development set. The NIST MT2005 test
set is used as the test set. The evaluation met-
ric is case-sensitive BLEU4. For significant test,
we used Zhang’s implementation (Zhang et al.,
2004)(confidence level of 95%). For comparisons,
we used the following three baseline systems:
LSTSSG An in-house implementation of linguis-
tically motivated STSSG based model similar
to (Zhang et al., 2008).
FSCFG An in-house implementation of purely
formally SCFG based model similar to (Chiang,
2007).
MBR We use an in-house combination system
which is an implementation of a classic sentence
level combination method based on the Minimum
Bayes Risk (MBR) decoding (Kumar and Byrne,
2004).
</bodyText>
<subsectionHeader confidence="0.996576">
3.1 Statistics of Rule Numbers in Different
Phases
</subsectionHeader>
<bodyText confidence="0.999794666666667">
Table 1 summarizes the statistics of the rules for
different models in three phases: after extrac-
tion (Extracted), after scoring(Scored), and af-
ter filtering (Filtered) (filtered by NIST05 test
set just, similar to the filtering step in phrase-
based SMT system). In Extracted phase, FSCFG
</bodyText>
<page confidence="0.60701">
127
</page>
<table confidence="0.9998408">
ID System BLEU4 #of used rules(k)
1 LSTSSG 0.2659±0.0043 984
2 FSCFG 0.2613±0.0045 1,266
3 HITREE 0.2730±0.0045 1,927
4 MBR(1,2) 0.2685±0.0044 –
</table>
<tableCaption confidence="0.917525">
Table 2: The Comparison of LSTSSG, FSCFG
,HITREE and the MBR.
</tableCaption>
<bodyText confidence="0.999881588235294">
has obvious more rules than LSTSSG. However,
in Scored phase, this situation reverses. Inter-
estingly, the situation reverses again in Filtered
phase. The reasons for these phenomenons are
that FSCFG abstract rules involves high-degree
generalization. Each FSCFG abstract rule aver-
agely have several duplicates2 in the extracted rule
set. Then, the duplicates will be discarded dur-
ing scoring. However, due to the high-degree gen-
eralization , the FSCFG abstract rules are more
likely to be matched by the test sentences. Con-
trastively, LSTSSG rules have more diversified
structures and thus weaker generalization capabil-
ity than FSCFG rules. From the ratios of two tran-
sition states, Table 1 indicates that HITREE can
be considered as compromise of FSCFG between
LSTSSG.
</bodyText>
<subsectionHeader confidence="0.998305">
3.2 Overall Performances
</subsectionHeader>
<bodyText confidence="0.9999825">
The performance comparison results are presented
in Table 2. The experimental results show that
the SSG-based model (HITREE) achieves signifi-
cant improvements over the models based on the
two isolated grammars: FSCFG and LSTSSG
(both p &lt; 0.001). From combination point of
view, the newly proposed model can be consid-
ered as a novel method going beyond the con-
ventional post-decoding style combination meth-
ods. The baseline Minimum Bayes Risk com-
bination of LSTSSG based model and FSCFG
based model (MBR(1, 2)) obtains significant im-
provements over both candidate models (both p &lt;
0.001). Meanwhile, the experimental results show
that the proposed model outperforms MBR(1, 2)
significantly (p &lt; 0.001). These preliminary re-
sults indicate that the proposed SSG-based model
is rather promising and it may serve as an alterna-
tive, if not superior, to current combination meth-
ods.
</bodyText>
<sectionHeader confidence="0.999398" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.986109684210526">
To combine the strengths of different gram-
mars, this paper proposes a statistical machine
2Rules with identical source side and target side are du-
plicated.
translation model based on a synthetic syn-
chronous grammar (SSG) which syncretizes a
purely formal synchronous context-free gram-
mar (FSCFG) and a linguistically motivated syn-
chronous tree sequence substitution grammar
(LSTSSG). Experimental results show that SSG-
based model achieves significant improvements
over the FSCFG-based model and LSTSSG-based
model.
In the future work, we would like to verify
the effectiveness of the proposed model on vari-
ous datasets and to design more sophisticated fea-
tures. Furthermore, the integrations of more dif-
ferent kinds of synchronous grammars for statisti-
cal machine translation will be investigated.
</bodyText>
<sectionHeader confidence="0.99657" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9957588">
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
</bodyText>
<sectionHeader confidence="0.995541" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808633333333">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In computational linguistics, 33(2).
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
ofACL 2003.
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-
rich syntactic translation models In Proceedings of
ACL-COLING.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
04.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine transla-
tion. In Proceedings of ACL-COLING.
Keh-Yin Su and Jing-Shin Chang. 1990. Some key
Issues in Designing Machine Translation Systems.
Machine Translation, 5(4):265-300.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of LREC 2004, pages 2051-2054.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li,
Chew Lim Tan and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings ofACL-HLT.
</reference>
<page confidence="0.901812">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962703">
<title confidence="0.990069">A Statistical Machine Translation Model Based on a Synthetic Synchronous Grammar</title>
<author confidence="0.999789">Hongfei Jiang</author>
<author confidence="0.999789">Muyun Yang</author>
<author confidence="0.999789">Tiejun Zhao</author>
<author confidence="0.999789">Sheng Li</author>
<author confidence="0.999789">Bo Wang</author>
<affiliation confidence="0.999839">School of Computer Science and Technology Harbin Institute of Technology</affiliation>
<abstract confidence="0.999075315789474">Recently, various synchronous grammars are proposed for syntax-based machine translation, e.g. synchronous context-free grammar and synchronous tree (sequence) substitution grammar, either purely formal or linguistically motivated. Aiming at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<booktitle>In computational linguistics,</booktitle>
<pages>33--2</pages>
<contexts>
<context position="1161" citStr="Chiang, 2007" startWordPosition="159" endWordPosition="160">ferent grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the usability of </context>
<context position="5270" citStr="Chiang, 2007" startWordPosition="870" endWordPosition="871">nd ANT is a one-to-one corresponding set which includes the synchronizing relations between the non-terminal leaf nodes from source and target side; w contains feature values associated with each rule. Through this formalization, we can see that FSCFG rules and LSTSSG rules are both included. However, we should point out that the rules with mixture of X non-terminals and syntactic non-terminals are not included in our current implementation despite that they are legal under the proposed formalism. The rule extraction in current implementation can be considered as a combination of the ones in (Chiang, 2007) and (Zhang et al., 2008). Given the sentence pair in Figure 1, some SSG rules can be extracted as illustrated in Figure 2. 2.2 The SSG-based Translation Model The translation in our SSG-based translation model can be treated as a SSG derivation. A derivation consists of a sequence of grammar rule applications. To model the derivations as a latent variable, we define the conditional probability distribution over the target translation a and the corend Output the 1-best hypothesis in H[1, J] as the final translation. Figure 3: The pseudocode for the decoding. responding derivation d of a given </context>
<context position="8098" citStr="Chiang, 2007" startWordPosition="1395" endWordPosition="1396">. R1-R3 are bilingual phrase rules, R4-R5 are FSCFG rules and R6-R8 are LSTSSG rules. NN VV VB to me 1 给 把 1 1 钢笔 BA PN 1 R1 钢笔 1 the pen 1 R2 我 1 to me 1 R3 钢笔 1 给 2 Give 2 the pen 1 R5 钢笔 1 X[1] 我 2 X[1] the pen 1 to me 2 R4 把 X[1] 给 1 X[2] Give 1 X[1] X[2] PP NP NN[1] VV[2] VB[2] NP[1] TO PRP RB R6 R7 1 我 1 Give DT NN 2.3 Decoding For efficiency, our model approximately search for the single ‘best’ derivation using beam search as (2) (e, d) = argmax e,d � k Akhk(d, e, f) IThe major challenge for such a SSG-based decoder is how to apply the heterogeneous rules in a derivation. For example, (Chiang, 2007) adopts a CKY style span-based decoding while (Liu et al., 2006) applies a linguistically syntax node based bottom-up decoding, which are difficult to integrate. Fortunately, our current SSG syncretizes FSCFG and LSTSSG. And the conventional decodings of both FSCFG and LSTSSG are spanbased expansion. Thus, it would be a natural way for our SSG-based decoder to conduct a spanbased beam search. The search procedure is given by the pseudocode in Figure 3. A hypotheses stack H[i, j] (similar to the “chart cell” in CKY parsing) is arranged for each span [i, j] for storing the translation hypotheses</context>
<context position="10665" citStr="Chiang, 2007" startWordPosition="1806" endWordPosition="1807">in a 4-gram language model on the Xinhua portion of the English Gigaword corpus(181M words). NIST MT2002 test set is used as the development set. The NIST MT2005 test set is used as the test set. The evaluation metric is case-sensitive BLEU4. For significant test, we used Zhang’s implementation (Zhang et al., 2004)(confidence level of 95%). For comparisons, we used the following three baseline systems: LSTSSG An in-house implementation of linguistically motivated STSSG based model similar to (Zhang et al., 2008). FSCFG An in-house implementation of purely formally SCFG based model similar to (Chiang, 2007). MBR We use an in-house combination system which is an implementation of a classic sentence level combination method based on the Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004). 3.1 Statistics of Rule Numbers in Different Phases Table 1 summarizes the statistics of the rules for different models in three phases: after extraction (Extracted), after scoring(Scored), and after filtering (Filtered) (filtered by NIST05 test set just, similar to the filtering step in phrasebased SMT system). In Extracted phase, FSCFG 127 ID System BLEU4 #of used rules(k) 1 LSTSSG 0.2659±0.0043 984 2 FSCF</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. In computational linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="1126" citStr="Eisner, 2003" startWordPosition="153" endWordPosition="154">g at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalization ability of t</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings ofACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of contextrich syntactic translation models</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING.</booktitle>
<contexts>
<context position="1147" citStr="Galley et al., 2006" startWordPosition="155" endWordPosition="158"> the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Galley, M. and Graehl, J. and Knight, K. and Marcu, D. and DeNeefe, S. and Wang, W. and Thayer, I. 2006. Scalable inference and training of contextrich syntactic translation models In Proceedings of ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT04.</booktitle>
<contexts>
<context position="10853" citStr="Kumar and Byrne, 2004" startWordPosition="1834" endWordPosition="1837">as the test set. The evaluation metric is case-sensitive BLEU4. For significant test, we used Zhang’s implementation (Zhang et al., 2004)(confidence level of 95%). For comparisons, we used the following three baseline systems: LSTSSG An in-house implementation of linguistically motivated STSSG based model similar to (Zhang et al., 2008). FSCFG An in-house implementation of purely formally SCFG based model similar to (Chiang, 2007). MBR We use an in-house combination system which is an implementation of a classic sentence level combination method based on the Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004). 3.1 Statistics of Rule Numbers in Different Phases Table 1 summarizes the statistics of the rules for different models in three phases: after extraction (Extracted), after scoring(Scored), and after filtering (Filtered) (filtered by NIST05 test set just, similar to the filtering step in phrasebased SMT system). In Extracted phase, FSCFG 127 ID System BLEU4 #of used rules(k) 1 LSTSSG 0.2659±0.0043 984 2 FSCFG 0.2613±0.0045 1,266 3 HITREE 0.2730±0.0045 1,927 4 MBR(1,2) 0.2685±0.0044 – Table 2: The Comparison of LSTSSG, FSCFG ,HITREE and the MBR. has obvious more rules than LSTSSG. However, in </context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In HLT04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-to-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING.</booktitle>
<contexts>
<context position="8162" citStr="Liu et al., 2006" startWordPosition="1404" endWordPosition="1407">d R6-R8 are LSTSSG rules. NN VV VB to me 1 给 把 1 1 钢笔 BA PN 1 R1 钢笔 1 the pen 1 R2 我 1 to me 1 R3 钢笔 1 给 2 Give 2 the pen 1 R5 钢笔 1 X[1] 我 2 X[1] the pen 1 to me 2 R4 把 X[1] 给 1 X[2] Give 1 X[1] X[2] PP NP NN[1] VV[2] VB[2] NP[1] TO PRP RB R6 R7 1 我 1 Give DT NN 2.3 Decoding For efficiency, our model approximately search for the single ‘best’ derivation using beam search as (2) (e, d) = argmax e,d � k Akhk(d, e, f) IThe major challenge for such a SSG-based decoder is how to apply the heterogeneous rules in a derivation. For example, (Chiang, 2007) adopts a CKY style span-based decoding while (Liu et al., 2006) applies a linguistically syntax node based bottom-up decoding, which are difficult to integrate. Fortunately, our current SSG syncretizes FSCFG and LSTSSG. And the conventional decodings of both FSCFG and LSTSSG are spanbased expansion. Thus, it would be a natural way for our SSG-based decoder to conduct a spanbased beam search. The search procedure is given by the pseudocode in Figure 3. A hypotheses stack H[i, j] (similar to the “chart cell” in CKY parsing) is arranged for each span [i, j] for storing the translation hypotheses. The hypotheses stacks are ordered such that every span is tran</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proceedings of ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yin Su</author>
<author>Jing-Shin Chang</author>
</authors>
<title>Some key Issues in Designing Machine Translation Systems.</title>
<date>1990</date>
<booktitle>Machine Translation,</booktitle>
<pages>5--4</pages>
<contexts>
<context position="1439" citStr="Su and Chang, 1990" startWordPosition="201" endWordPosition="204">esults on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the usability of the rules for the new sentences) can be considered as a kind of the generative power of the grammar and the disambiguition ability to the rule candidates can be considered as an embodiment of expressive power. However, the generalization ability and the disambiguition ability o</context>
</contexts>
<marker>Su, Chang, 1990</marker>
<rawString>Keh-Yin Su and Jing-Shin Chang. 1990. Some key Issues in Designing Machine Translation Systems. Machine Translation, 5(4):265-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="1112" citStr="Wu, 1997" startWordPosition="151" endWordPosition="152">ted. Aiming at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalizatio</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>2051--2054</pages>
<contexts>
<context position="10368" citStr="Zhang et al., 2004" startWordPosition="1761" endWordPosition="1764">3.9%) Table 1: The statistics of the counts of the rules in different phases. ‘k’ means one thousand. on experiments with Chinese-to-English translation base on it. We used FBIS Chinese-to-English parallel corpora (7.2M+9.2M words) as the training data. We also used SRI Language Modeling Toolkit to train a 4-gram language model on the Xinhua portion of the English Gigaword corpus(181M words). NIST MT2002 test set is used as the development set. The NIST MT2005 test set is used as the test set. The evaluation metric is case-sensitive BLEU4. For significant test, we used Zhang’s implementation (Zhang et al., 2004)(confidence level of 95%). For comparisons, we used the following three baseline systems: LSTSSG An in-house implementation of linguistically motivated STSSG based model similar to (Zhang et al., 2008). FSCFG An in-house implementation of purely formally SCFG based model similar to (Chiang, 2007). MBR We use an in-house combination system which is an implementation of a classic sentence level combination method based on the Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004). 3.1 Statistics of Rule Numbers in Different Phases Table 1 summarizes the statistics of the rules for different m</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of LREC 2004, pages 2051-2054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Ai Ti AW</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT.</booktitle>
<contexts>
<context position="1182" citStr="Zhang et al., 2008" startWordPosition="161" endWordPosition="164">s, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the usability of the rules for the new</context>
<context position="2449" citStr="Zhang et al., 2008" startWordPosition="364" endWordPosition="367">enerative power of the grammar and the disambiguition ability to the rule candidates can be considered as an embodiment of expressive power. However, the generalization ability and the disambiguition ability often contradict each other in practice such that various grammar formalisms in SMT are actually different trade-off between them. For instance, in our investigations for SMT (Section 3.1), the Formally SCFG based hierarchical phrase-based model (hereinafter FSCFG) (Chiang, 2007) has a better generalization capability than a Linguistically motivated STSSG based model (hereinafter LSTSSG) (Zhang et al., 2008), with 5% rules of the former matched by NIST05 test set while only 3.5% rules of the latter matched by the same test set. However, from expressiveness point of view, the former usually results in more ambiguities than the latter. To combine the strengths of different synchronous grammars, this paper proposes a statistical machine translation model based on a synthetic synchronous grammar (SSG) which syncretizes FSCFG and LSTSSG. Moreover, it is noteworthy that, from the combination point of view, our proposed scheme can be considered as a novel system combination method which goes beyond the </context>
<context position="5295" citStr="Zhang et al., 2008" startWordPosition="873" endWordPosition="876">one corresponding set which includes the synchronizing relations between the non-terminal leaf nodes from source and target side; w contains feature values associated with each rule. Through this formalization, we can see that FSCFG rules and LSTSSG rules are both included. However, we should point out that the rules with mixture of X non-terminals and syntactic non-terminals are not included in our current implementation despite that they are legal under the proposed formalism. The rule extraction in current implementation can be considered as a combination of the ones in (Chiang, 2007) and (Zhang et al., 2008). Given the sentence pair in Figure 1, some SSG rules can be extracted as illustrated in Figure 2. 2.2 The SSG-based Translation Model The translation in our SSG-based translation model can be treated as a SSG derivation. A derivation consists of a sequence of grammar rule applications. To model the derivations as a latent variable, we define the conditional probability distribution over the target translation a and the corend Output the 1-best hypothesis in H[1, J] as the final translation. Figure 3: The pseudocode for the decoding. responding derivation d of a given source sentence f as (1) </context>
<context position="6972" citStr="Zhang et al., 2008" startWordPosition="1162" endWordPosition="1165">, R7, R2) All of them are SSG derivations while d1 is also a FSCFG derivation, d2 is also a LSTSSG derivation. Ideally, the model is supposed to be able to weight them differently and to prefer the better derivation, which deserves intensive study. Some sophisticated features can be designed for this issue. For example, some features related with structure richness and grammar consistency1 of a derivation should be designed to distinguish the derivations involved various heterogeneous rule applications. For the page limit and the fair comparison, we only adopt the conventional features as in (Zhang et al., 2008) in our current implementation. 1This relates with reviewers’ questions: “can a rule expecting an NN accept an X?” and “... the interaction between the two typed of rules ...”. In our study in progress, we would design some features to distinguish the derivation steps which fulfill the expectation or not, to measure how much heterogeneous rules are applied in a derivation and so on. exp Ek AkHk(d, e, f) 126 Figure 2: Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure 1. R1-R3 are bilingual phrase rules, R4-R5 are FSCFG rules and R6-R8 are LSTSSG rules. N</context>
<context position="10569" citStr="Zhang et al., 2008" startWordPosition="1790" endWordPosition="1793">llel corpora (7.2M+9.2M words) as the training data. We also used SRI Language Modeling Toolkit to train a 4-gram language model on the Xinhua portion of the English Gigaword corpus(181M words). NIST MT2002 test set is used as the development set. The NIST MT2005 test set is used as the test set. The evaluation metric is case-sensitive BLEU4. For significant test, we used Zhang’s implementation (Zhang et al., 2004)(confidence level of 95%). For comparisons, we used the following three baseline systems: LSTSSG An in-house implementation of linguistically motivated STSSG based model similar to (Zhang et al., 2008). FSCFG An in-house implementation of purely formally SCFG based model similar to (Chiang, 2007). MBR We use an in-house combination system which is an implementation of a classic sentence level combination method based on the Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004). 3.1 Statistics of Rule Numbers in Different Phases Table 1 summarizes the statistics of the rules for different models in three phases: after extraction (Extracted), after scoring(Scored), and after filtering (Filtered) (filtered by NIST05 test set just, similar to the filtering step in phrasebased SMT system). I</context>
</contexts>
<marker>Zhang, Jiang, AW, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li, Chew Lim Tan and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proceedings ofACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>