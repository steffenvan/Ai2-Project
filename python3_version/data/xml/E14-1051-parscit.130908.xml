<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.973202">
Word Embeddings through Hellinger PCA
</title>
<author confidence="0.964871">
R´emi Lebret
</author>
<affiliation confidence="0.957397">
Idiap Research Institute
</affiliation>
<address confidence="0.636383">
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
</address>
<email confidence="0.993159">
remi@lebret.ch
</email>
<author confidence="0.923839">
Ronan Collobert
</author>
<affiliation confidence="0.926389">
Idiap Research Institute
</affiliation>
<note confidence="0.5843055">
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
</note>
<email confidence="0.996291">
ronan@collobert.com
</email>
<sectionHeader confidence="0.997358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999769111111111">
Word embeddings resulting from neural
language models have been shown to be
a great asset for a large variety of NLP
tasks. However, such architecture might
be difficult and time-consuming to train.
Instead, we propose to drastically sim-
plify the word embeddings computation
through a Hellinger PCA of the word co-
occurence matrix. We compare those new
word embeddings with some well-known
embeddings on named entity recognition
and movie review tasks and show that we
can reach similar or even better perfor-
mance. Although deep learning is not re-
ally necessary for generating good word
embeddings, we show that it can provide
an easy way to adapt embeddings to spe-
cific tasks.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910846153847">
Building word embeddings has always generated
much interest for linguists. Popular approaches
such as Brown clustering algorithm (Brown et al.,
1992) have been used with success in a wide vari-
ety of NLP tasks (Sch¨utze, 1995; Koo et al., 2008;
Ratinov and Roth, 2009). Those word embed-
dings are often seen as a low dimensional-vector
space where the dimensions are features poten-
tially describing syntactic or semantic properties.
Recently, distributed approaches based on neural
network language models (NNLM) have revived
the field of learning word embeddings (Collobert
and Weston, 2008; Huang and Yates, 2009; Turian
et al., 2010; Collobert et al., 2011). However, a
neural network architecture can be hard to train.
Finding the right parameters to tune the model is
often a challenging task and the training phase is
in general computationally expensive.
This paper aims to show that such good word
embeddings can be obtained using simple (mostly
linear) operations. We show that similar word
embeddings can be computed using the word co-
occurrence statistics and a well-known dimension-
ality reduction operation such as Principal Com-
ponent Analysis (PCA). We then compare our em-
beddings with the CW (Collobert and Weston,
2008), Turian (Turian et al., 2010), HLBL (Mnih
and Hinton, 2008) embeddings, which come from
deep architectures and the LR-MVL (Dhillon et
al., 2011) embeddings, which also come from a
spectral method on several NLP tasks.
We claim that, assuming an appropriate met-
ric, a simple spectral method as PCA can generate
word embeddings as good as with deep-learning
architectures. On the other hand, deep-learning
architectures have shown their potential in sev-
eral supervised NLP tasks, by using these word
embeddings. As they are usually generated over
large corpora of unlabeled data, words are repre-
sented in a generic manner. Having generic em-
beddings, good performance can be achieved on
NLP tasks where the syntactic aspect is domi-
nant such as Part-Of-Speech, chunking and NER
(Turian et al., 2010; Collobert et al., 2011; Dhillon
et al., 2011). For supervised tasks relying more
on the semantic aspect as sentiment classification,
it is usually helpful to adapt the existing embed-
dings to improve performance (Labutov and Lip-
son, 2013). We show in this paper that such em-
bedding specialization can be easily done via neu-
ral network architectures and that helps to increase
general performance.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999840428571429">
As 80% of the meaning of English text comes
from word choice and the remaining 20% comes
from word order (Landauer, 2002), it seems quite
important to leverage word order to capture all the
semantic information. Connectionist approaches
have therefore been proposed to develop dis-
tributed representations which encode the struc-
</bodyText>
<page confidence="0.97371">
482
</page>
<note confidence="0.993049">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–490,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999803861538462">
tural relationships between words (Hinton, 1986;
Pollack, 1990; Elman, 1991). More recently, a
neural network language model was proposed in
Bengio et al. (2003) where word vector representa-
tions are simultaneously learned along with a sta-
tistical language model. This architecture inspired
other authors: Collobert and Weston (2008) de-
signed a neural language model which eliminates
the linear dependency on vocabulary size, Mnih
and Hinton (2008) proposed a hierarchical linear
neural model, Mikolov et al. (2010) investigated
a recurrent neural network architecture for lan-
guage modeling. Such architectures being trained
over large corpora of unlabeled text with the aim
to predict correct scores end up learning the co-
occurence statistics.
Linguists assumed long ago that words occur-
ring in similar contexts tend to have similar mean-
ings (Wittgenstein, 1953). Using the word co-
occurrence statistics is thus a natural choice to em-
bed similar words into a common vector space
(Turney and Pantel, 2010). Common approaches
calculate the frequencies, apply some transforma-
tions (tf-idf, PPMI), reduce the dimensionality and
calculate the similarities (Lowe, 2001). Consid-
ering a fixed-sized word vocabulary D and a set
of words W to embed, the co-occurence matrix C
is of size JWJxJDJ. C is then vocabulary size-
dependent. One can apply a dimensionality reduc-
tion operation to C leading to C¯ E R|W|xd, where
d « JDJ. Dimensionality reduction techniques
such as Singular Valued Decomposition (SVD)
are widely used (e.g. LSA (Landauer and Du-
mais, 1997), ICA (V¨ayrynen and Honkela, 2004)).
However, word co-occurence statistics are dis-
crete distributions. An information theory mea-
sure such as the Hellinger distance seems to be
more appropriate than the Euclidean distance over
a discrete distribution space. In this paper we
will compare the Hellinger PCA against the clas-
sical Euclidean PCA and the Low Rank Multi-
View Learning (LR-MVL) method, which is an-
other spectral method based on Canonical Corre-
lation Analysis (CCA) to learn word embeddings
(Dhillon et al., 2011).
It has been shown that using word embeddings
as features helps to improve general performance
on many NLP tasks (Turian et al., 2010). How-
ever these embeddings can be too generic to per-
form well on other tasks such as sentiment clas-
sification. For such task, word embeddings must
capture the sentiment information. Maas et al.
(2011) proposed a model for jointly capturing se-
mantic and sentiment components of words into
vector spaces. More recently, Labutov and Lip-
son (2013) presented a method which takes exist-
ing embeddings and, by using some labeled data,
re-embed them in the same space. They showed
that these new embeddings can be better predic-
tors in a supervised task. In this paper, we con-
sider word embedding-based linear and non-linear
models for two NLP supervised tasks: Named En-
tity Recognition and IMDB movie review. We an-
alyze the effect of fine-tuning existing embeddings
over each task of interest.
</bodyText>
<sectionHeader confidence="0.983658" genericHeader="method">
3 Spectral Method for Word
Embeddings
</sectionHeader>
<bodyText confidence="0.999942857142857">
A NNLM learns which words among the vocab-
ulary are likely to appear after a given sequence
of words. More formally, it learns the next word
probability distribution. Instead, simply counting
words on a large corpus of unlabeled text can be
performed to retrieve those word distributions and
to represent words (Turney and Pantel, 2010).
</bodyText>
<subsectionHeader confidence="0.99978">
3.1 Word co-occurence statistics
</subsectionHeader>
<bodyText confidence="0.999823142857143">
”You shall know a word by the company it keeps”
(Firth, 1957). It is a natural choice to use the word
co-occurence statistics to acquire representations
of word meanings. Raw word co-occurence fre-
quencies are computed by counting the number of
times each context word w E D occurs after a se-
quence of words T:
</bodyText>
<equation confidence="0.9996635">
p(wJT) = p(w, T ) n(w,T) (1)
p(T) Ew n(w, T)
</equation>
<bodyText confidence="0.999788777777778">
where n(w, T) is the number of times each context
word w occurs after the sequence T. The size of
T can go from 1 to t words. The next word prob-
ability distribution p for each word or sequence of
words is thus obtained. It is a multinomial dis-
tribution of JDJ classes (words). A co-occurence
matrix of size N x JDJ is finally built by com-
puting those frequencies over all the N possible
sequences of words.
</bodyText>
<subsectionHeader confidence="0.999699">
3.2 Hellinger distance
</subsectionHeader>
<bodyText confidence="0.99834325">
Similarities between words can be derived by
computing a distance between their correspond-
ing word distributions. Several distances (or met-
rics) over discrete distributions exist, such as the
</bodyText>
<page confidence="0.998857">
483
</page>
<bodyText confidence="0.993493285714286">
Bhattacharyya distance, the Hellinger distance or
Kullback-Leibler divergence. We chose here the
Hellinger distance for its simplicity and symme-
try property (as it is a true distance). Consid-
ering two discrete probability distributions P =
(p1, ... , pk) and Q = (q1, ... , qk), the Hellinger
distance is formally defined as:
</bodyText>
<equation confidence="0.986549">
1
H(P, Q) = − ✓2
</equation>
<bodyText confidence="0.906962666666667">
which is directly related to the Euclidean norm of
the difference of the square root vectors:
Note that it makes more sense to take the Hellinger
distance rather than the Euclidean distance for
comparing discrete distributions, as P and Q are
unit vectors according to the Hellinger distance
✓ P and ✓Q are units vector according to the E2
(
norm).
</bodyText>
<subsectionHeader confidence="0.998784">
3.3 Dimensionality Reduction
</subsectionHeader>
<bodyText confidence="0.999938444444444">
As discrete distributions are vocabulary size-
dependent, using directly the distribution as a
word embedding is not really tractable for large
vocabulary. We propose to perform a princi-
pal component analysis (PCA) of the word co-
occurence probability matrix to represent words
in a lower dimensional space while minimizing
the reconstruction error according to the Hellinger
distance.
</bodyText>
<sectionHeader confidence="0.999178" genericHeader="method">
4 Architectures for NLP tasks
</sectionHeader>
<bodyText confidence="0.999976666666667">
Traditional NLP approaches extract from docu-
ments a rich set of hand-designed features which
are then fed to a standard classification algorithm.
The choice of features is a task-specific empirical
process. In contrast, we want to pre-process our
features as little as possible. In that respect, a mul-
tilayer neural network architecture seems appro-
priate as it can be trained in an end-to-end fashion
on the task of interest.
</bodyText>
<subsectionHeader confidence="0.995172">
4.1 Sentence-level Approach
</subsectionHeader>
<bodyText confidence="0.9999891">
The sentence-level approach aims at tagging with
a label each word in a given sentence. Embed-
dings of each word in a sentence are fed to linear
and non-linear classification models followed by a
CRF-type sentence tag inference. We chose here
neural networks as classifiers.
Sliding window Context is crucial to character-
ize word meanings. We thus consider n context
words around each word xt to be tagged, lead-
ing to a window of N = (2n + 1) words [x]t =
(xt−n, . . . , xt, . . . , xt+n). As each word is em-
bedded into a dwrd-dimensional vector, it results
a dwrd x N vector representing a window of N
words, which aims at characterizing the middle
word xt in this window. Given a complete sen-
tence of T words, we can obtain for each word a
context-dependent representation by sliding over
all the possible windows in the sentence. A same
linear transformation is then applied on each win-
dow for each word to tag:
</bodyText>
<equation confidence="0.992725">
g([x]t) = W [x]t + b, (4)
</equation>
<bodyText confidence="0.99655725">
where W E RM×dwrdN and b E RM are the pa-
rameters, with M the number of classes. Alterna-
tively, a one hidden layer non-linear network can
be considered:
</bodyText>
<equation confidence="0.999347">
g([x]t) = Wh(U[x]t) + b, (5)
</equation>
<bodyText confidence="0.999993625">
where U E Rnhu×dwrdN, with nhu the number of
hidden units and h(.) a transfer function.
CRF-type inference There exists strong depen-
dencies between tags in a sentence: some tags
cannot follow other tags. To take the sentence
structure into account, we want to encourage valid
paths of tags during training, while discourag-
ing all other paths. Considering the matrix of
scores outputs by the network, we train a sim-
ple conditional random field (CRF). At inference
time, given a sentence to tag, the best path which
minimizes the sentence score is inferred with the
Viterbi algorithm. More formally, we denote B
all the trainable parameters of the network and
fθ([x]T1 ) the matrix of scores. The element [fθ]i,t
of the matrix is the score output by the network for
the sentence [x]T1 and the ith tag, at the tth word.
We introduce a transition score [A]i,j for jumping
from i to j tags in successive words, and an initial
score [A]i,0 for starting from the ith tag. As the
transition scores are going to be trained, we define
B˜ = B U{[A]i,jbi, j}. The score of a sentence [x]T1
along a path of tags [i]T1 is then given by the sum
of transition scores and networks scores:
</bodyText>
<equation confidence="0.988863571428571">
T
s([x]T1 , [i]T1,˜B) = (A[i]t−1,[i]t + [fθ][i]t,t) .
t=1
(6)
(✓pi − ✓qi)2 , (2)
J �
k
i=1
1
H(P, Q) =
✓ �
I I P − QI I
2 . (3)
✓2
</equation>
<page confidence="0.988402">
484
</page>
<bodyText confidence="0.9863082">
We normalize this score over all possible tag paths
[j]T1 using a softmax, and we interpret the resulting
ratio as a conditional tag path probability. Taking
the log, the conditional probability of the true path
[y]T1 is therefore given by:
</bodyText>
<equation confidence="0.959886">
log p([y]T 1 , [x]T 1 , ˜θ) = s([x]T 1 , [y]T 1 , ˜θ)
− logadd s([x]T1 ,[j]T1 ,˜θ),
d[j]T1
(7)
</equation>
<bodyText confidence="0.992161">
where we adopt the notation
</bodyText>
<equation confidence="0.986414">
logadd
i �zi = log (
i ezi) . (8)
</equation>
<bodyText confidence="0.9996674">
Computing the log-likelihood efficiently is not
straightforward, as the number of terms in the
logadd grows exponentially with the length of
the sentence. It can be computed in linear time
with the Forward algorithm, which derives a recur-
sion similar to the Viterbi algorithm (see Rabiner
(1989)). We can thus maximize the log-likelihood
over all the training pairs ([x]T1 , [y]T1 ) to find, given
a sentence [x]T1 , the best tag path which minimizes
the sentence score (6):
</bodyText>
<equation confidence="0.9313785">
argmax s([x]T1 , [j]T1 ,˜θ) . (9)
[j]T1
</equation>
<bodyText confidence="0.999937">
In contrast to classical CRF, all parameters θ are
trained in a end-to-end manner, by backpropa-
gation through the Forward recursion, following
Collobert et al. (2011).
</bodyText>
<subsectionHeader confidence="0.980604">
4.2 Document-level Approach
</subsectionHeader>
<bodyText confidence="0.999632666666667">
The document-level approach is a document bi-
nary classifier, with classes y E {−1,1}. For each
document, a set of (trained) filters is applied to
the sliding window described in section 4.1. The
maximum value obtained by the ith filter over the
whole document is:
</bodyText>
<equation confidence="0.953558333333333">
max
t [ ]
wi[x]t + bi i,t 1 G i G nfilter . (10)
</equation>
<bodyText confidence="0.9727784">
It can be seen as a way to measure if the infor-
mation represented by the filter has been captured
in the document or not. We feed all these inter-
mediate scores to a linear classifier, leading to the
following simple model:
</bodyText>
<equation confidence="0.996852666666667">
[ ]
fθ(x) = α max W[x]t + b . (11)
t
</equation>
<bodyText confidence="0.999962555555556">
In the case of movie reviews, the ith filter might
capture positive or negative sentiment depending
on the sign of αi. As in section 4.1, we will also
consider a non-linear classifier in the experiments.
Training The neural network is trained using
stochastic gradient ascent. We denote θ all the
trainable parameters of the network. Using a train-
ing set T, we minimize the following soft margin
loss function with respect to θ:
</bodyText>
<equation confidence="0.991024666666667">
�θ +— log . (12)
(1 + e−yfθ(x))
(x,y)ET
</equation>
<subsectionHeader confidence="0.992006">
4.3 Embedding Fine-Tuning
</subsectionHeader>
<bodyText confidence="0.919724944444444">
As seen in section 3, the process to compute
generic word embedding is quite straightforward.
These embeddings can then be used as features
for supervised NLP systems and help to improve
the general performance (Turian et al., 2010; Col-
lobert et al., 2011; Chen et al., 2013). However,
most of these systems cannot tune these embed-
dings as they are not structurally able to. By lever-
aging the deep architecture of our system, we can
define a lookup-table layer initialized with exist-
ing embeddings as the first layer of the network.
Lookup-Table Layer We consider a fixed-sized
word dictionary D. Given a sequence of N words
w1, w2, ... , wN, each word wn E W is first em-
bedded into a dwrd-dimensional vector space, by
applying a lookup-table operation:
at in
d
</bodyText>
<equation confidence="0.814354">
1 1 , ... ,0
ex wn
= (W)wn ,
</equation>
<bodyText confidence="0.991475461538462">
where the matrix W E Rdwrdx|D |represents
the embeddings to be tuned in this lookup layer.
(W)wn E Rdwrd is the wth column of W and dwrd
is the word vector size. Given any sequence of N
words [w]N1in D, the lookup table layer applies
the same operation for each word in the sequence,
producing the following output matrix:
LTW([w]N1 ) = ((W)1[w]1 ... (W)[w]N) .
Training Given a task of interest, a relevant rep-
resentation of each word is then given by the cor-
responding lookup table feature vector, which is
trained by backpropagation. Word representations
are initialized with existing embeddings.
</bodyText>
<equation confidence="0.96165">
LTW(wn) = W C0, ...,
</equation>
<page confidence="0.997491">
485
</page>
<sectionHeader confidence="0.999397" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999963">
We evaluate the quality of our embeddings ob-
tained on a large corpora of unlabeled text by com-
paring their performance against the CW (Col-
lobert and Weston, 2008), Turian (Turian et al.,
2010), HLBL (Mnih and Hinton, 2008), and LR-
MVL (Dhillon et al., 2011) embeddings on NER
and movie review tasks. We also show that the
general performance can be improved for these
tasks by fine-tuning the word embeddings.
</bodyText>
<subsectionHeader confidence="0.9978245">
5.1 Building Word Representation over
Large Corpora
</subsectionHeader>
<bodyText confidence="0.999441314285714">
Our English corpus is composed of the entire En-
glish Wikipedia1 (where all MediaWiki markups
have been removed), the Reuters corpus and the
Wall Street Journal (WSJ) corpus. We consider
lower case words to limit the number of words
in the vocabulary. Additionally, all occurrences
of sequences of numbers within a word are re-
placed with the string “NUMBER”. The result-
ing text was tokenized using the Stanford tok-
enizer2. The data set contains about 1,652 million
words. As vocabulary, we considered all the words
within our corpus which appear at least one hun-
dred times. This results in a 178,080 words vocab-
ulary. To build the co-occurence matrix, we used
only the 10,000 most frequent words within our
vocabulary as context words. To get embeddings
for words, we needed to only consider sequences
T of t = 1 word. After PCA, each word can
be represented in any n-dimensional vector (with
n E {1, ... ,10000}). We chose to embed words
in a 50-dimensional vector, which is the common
dimension among the other embeddings in the lit-
erature. The resulting embeddings will be referred
as H-PCA in the following sections. To highlight
the importance of the Hellinger distance, we also
computed the PCA of the co-occurence probabil-
ity matrix with respect to the Euclidean metric.
The resulting embeddings are denoted E-PCA.
Computational cost The Hellinger PCA is very
fast to compute. We report in Table 1 the time
needed to compute the embeddings described
above. For this benchmark we used Intel i7 3770K
3.5GHz CPUs. As the computation of the covari-
ance matrix is highly parallelizable, we report re-
sults with 1, 100 and 500 CPUs. The Eigende-
</bodyText>
<footnote confidence="0.999943">
1Available at http://download.wikimedia.org. We took the
May 2012 version.
2Available at http://nlp.stanford.edu/software/tokenizer.shtml
</footnote>
<bodyText confidence="0.972789666666667">
composition of the C matrix has been computed
with the SSYEVR LAPACK subroutine on one
CPU. We compare completion times for 1,000 and
10,000 eigenvectors. Finally, we report comple-
tion times to generate the emdeddings by linear
projection using 50, 100 and 200 eigenvectors. Al-
though the linear projection is already quite fast
on only one CPU, this operation can also be com-
puted in parallel. Those results show that the
Hellinger PCA can generate about 200,000 em-
beddings in about three minutes with a cluster of
100 CPUs.
</bodyText>
<table confidence="0.999584555555556">
time (s)
# of CPUs 1 100 500
Covariance matrix 9930 99 20
1,000 Eigenvectors 72 - -
10,000 Eigenvectors 110 - -
50D Embeddings 20 0.2 0.04
100D Embeddings 29 0.29 0.058
200D Embeddings 67 0.67 0.134
Total for 50D 10,022 171.2 92.04
</table>
<tableCaption confidence="0.994367">
Table 1: Benchmark of the experiment. Times are
reported in seconds.
</tableCaption>
<subsectionHeader confidence="0.998149">
5.2 Existing Available Word Embeddings
</subsectionHeader>
<bodyText confidence="0.9998975">
We compare our H-PCA’s embeddings with the
following publicly available embeddings:
</bodyText>
<listItem confidence="0.7856398125">
• LR-MVL3: it covers 300,000 words with 50
dimensions for each word. They were trained
on the RCV1 corpus using the Low Rank
Multi-View Learning method. We only used
their context oblivious embeddings coming
from the eigenfeature dictionary.
• CW4: it covers 130,000 words with 50 di-
mensions for each word. They were trained
for about two months, over Wikipedia, using
a neural network language model approach.
• Turian5: it covers 268,810 words with 25,
50, 100 or 200 dimensions for each word.
They were trained on the RCV1 corpus us-
ing the same system as the CW embeddings
but with different parameters. We used only
the 50 dimensions.
</listItem>
<footnote confidence="0.99934325">
3Available at http://www.cis.upenn.edu/ un-
gar/eigenwords/
4From SENNA: http://ml.nec-labs.com/senna/
5Available at http://metaoptimize.com/projects/wordreprs/
</footnote>
<page confidence="0.996871">
486
</page>
<listItem confidence="0.9996306">
• HLBL5 : it covers 246,122 words with 50 or
100 dimensions for each word. They were
trained on the RCV1 corpus using a Hierar-
chical Log-Bilinear Model. We used only the
50 dimensions.
</listItem>
<subsectionHeader confidence="0.977087">
5.3 Supervised Evaluation Tasks
</subsectionHeader>
<bodyText confidence="0.995037682926829">
Using word embeddings as feature proved that it
can improve the generalization performance on
several NLP tasks (Turian et al., 2010; Collobert
et al., 2011; Chen et al., 2013). Using our word
embeddings, we thus trained the sentence-level ar-
chitecture described in section 4.1 on a NER task.
Named Entity Recognition (NER) It labels
atomic elements in the sentence into categories
such as “PERSON” or “LOCATION”. The
CoNLL 2003 setup6 is a NER benchmark data
set based on Reuters data. The contest provides
training, validation and testing sets. The networks
are fed with two raw features: word embeddings
and a capital letter feature. The “caps” feature
tells if each word was in lowercase, was all up-
percase, had first letter capital, or had at least
one non-initial capital letter. No other feature has
been used to tune the models. This is a main
difference with other systems which usually use
more features as POS tags, prefixes and suffixes
or gazetteers. Hyper-parameters were tuned on
the validation set. We selected n = 2 context
words leading to a window of 5 words. We used a
special “PADDING” word for context at the be-
ginning and the end of each sentence. For the
non-linear model, the number of hidden units was
300. As benchmark system, we report the system
of Ando et al. (2005), which reached 89.31% F1
with a semi-supervised approach and less special-
ized features than CoNLL 2003 challengers.
The NER evaluation task is mainly syntactic.
As we wish to evaluate whether our word embed-
dings can also capture semantic, we trained the
document-level architecture described in section
4.2 over a movie review task.
IMDB Review Dataset We used a collection of
50,000 reviews from IMDB7. It allows no more
than 30 reviews per movie. It contains an even
number of positive and negative reviews, so ran-
domly guessing yields 50% accuracy. Only highly
polarized reviews have been considered. A nega-
</bodyText>
<footnote confidence="0.9996545">
6http://www.cnts.ua.ac.be/conll2003/ner/
7Available at http://www.andrew-maas.net/data/sentiment
</footnote>
<bodyText confidence="0.997172428571429">
tive review has a score ≤ 4 out of 10, and a posi-
tive review has a score ≥ 7 out of 10. It has been
evenly divided into training and test sets (25,000
reviews each). For this task, we only used the
word embeddings as features. We perform a sim-
ple cross-validation on the training set to choose
the optimal hyper-parameters. The network had a
window of 5 words and nfilter = 1000 filters. As
benchmark system, we report the system of Maas
et al. (2011), which reached 88.90% accuracy with
a mix of unsupervised and supervised techniques
to learn word vectors capturing semantic term-
document information, as well as rich sentiment
content.
</bodyText>
<figure confidence="0.9989965">
(a) NER validation set.
(b) IMDB review dataset.
</figure>
<figureCaption confidence="0.839682">
Figure 1: Effect of varying the normalization fac-
tor A with a non-linear approach and fine-tuning.
</figureCaption>
<subsectionHeader confidence="0.991084">
5.4 Embeddings Normalization
</subsectionHeader>
<bodyText confidence="0.9889845">
Word embeddings are continuous vector spaces
that are not necessarily in a bounded range. To
avoid saturation issues in the network architec-
tures, embeddings need to be properly normalized.
Considering the matrix of word embeddings E,
the normalized embeddings are:
</bodyText>
<equation confidence="0.711051">
E˜ = A(E − E) (15)
σ(E)
</equation>
<figure confidence="0.99379572972973">
LRMVL
Turian
CW
H-PCA
HLBL
91.5
91
90.5
90
89.5
89
88.5
88
87.5
0.001 0.01 0.1 1
lambda
F1 score
93
92.5
92
0.001 0.01 0.1 1
lambda
accuracy
91
90
89
88
87
86
85
84
83
LRMVL
Turian
CW
H-PCA
HLBL
</figure>
<page confidence="0.996272">
487
</page>
<bodyText confidence="0.999981">
where E is the mean of the embeddings, Q(E) is
the standard deviation of the embeddings and A is
a normalization factor. Figure 1 shows the effect
of A on both supervised tasks. The embeddings
normalization depends on the type of the network
architecture. In the document-level approach, best
results are obtained with A = 0.1 for all embed-
dings, while a normalization factor set to 1 is bet-
ter for H-PCA’s embeddings in the sentence-level
approach. These results show the importance of
applying the right normalization for word embed-
dings.
</bodyText>
<subsectionHeader confidence="0.556841">
5.5 Results
</subsectionHeader>
<bodyText confidence="0.995956432432432">
H-PCA’s embeddings Results summarized in
Table 2 reveal that performance on NER task can
be as good with word embeddings from a word co-
occurence matrix decomposition as with a neural
network language model trained for weeks. The
best F1 scores are indeed obtained using the H-
PCA tuned embeddings. Results for the movie re-
view task in Table 3 show that H-PCA’s embed-
dings also perform as well as all the other embed-
dings on the movie review task. It is worth men-
tioning that on both tasks, H-PCA’s embeddings
outperform the E-PCA’s embeddings, demonstrat-
ing the value of the Hellinger distance. When the
embeddings are not tuned, the CW’s embeddings
slightly outperform the H-PCA’s embeddings on
NER task. The performance difference between
both fixed embeddings on the movie review task is
about 3%. Embeddings from the CW neural lan-
guage model seems to capture more semantic in-
formation but we showed that this lack of semantic
information can be offset by fine-tuning.
Embeddings fine-tuning We note that tuning
the embeddings by backpropagation increases the
general performance on both NER and movie re-
view tasks. The increase is, in general, higher for
the movie review task, which reveals the impor-
tance of embedding fine-tuning for NLP tasks with
a high semantic component. We show in Table 4
that the embeddings after fine-tuning give a higher
rank to words that are related to the task of interest
which is movie-sentiment-based relations in this
case.
Linear vs nonlinear model We also report re-
sults with a linear version of our neural networks.
Having non-linearity helps for NER. It seems im-
portant to extract non-linear features for such a
task. However, we note that the linear approach
</bodyText>
<table confidence="0.9998485625">
Approach Fixed Tuned
Benchmark 89.31
Non-Linear Approach
H-PCA 87.91 ± 0.17 89.16 ± 0.09
E-PCA 84.28 ± 0.15 87.09 ± 0.12
LR-MVL 86.83 ± 0.20 87.38 ± 0.07
CW 88.14 ± 0.21 88.69 ± 0.16
Turian 86.26 ± 0.13 87.35 ± 0.12
HLBL 83.87 ± 0.25 85.91 ± 0.17
Linear Approach
H-PCA 84.64 ± 0.11 87.97 ± 0.09
E-PCA 78.15 ± 0.15 85.99 ± 0.09
LR-MVL 82.27 ± 0.14 86.83 ± 0.17
CW 84.50 ± 0.19 86.84 ± 0.08
Turian 83.33 ± 0.07 86.79 ± 0.11
HLBL 80.31± 0.11 85.06 ± 0.13
</table>
<tableCaption confidence="0.994888">
Table 2: Performance comparison on NER task
</tableCaption>
<bodyText confidence="0.969275833333333">
with different embeddings. The first column is
results with the original embeddings. The sec-
ond column is results with embeddings after fine-
tuning for this task. Results are reported in F1
score (mean ± standard deviation of ten training
runs with different initialization).
</bodyText>
<table confidence="0.99928325">
Approach Fixed Tuned
Benchmark 88.90
Non-Linear Approach
H-PCA 84.20 ± 0.16 89.89 ± 0.09
E-PCA 74.85 ± 0.12 89.70 ± 0.06
LR-MVL 85.33 ± 0.14 90.06 ± 0.09
CW 87.54 ± 0.27 89.77 ± 0.05
Turian 85.33 ± 0.10 89.99 ± 0.05
HLBL 85.51 ± 0.14 89.58 ± 0.06
Linear Approach
H-PCA 84.11 ± 0.05 89.90 ± 0.10
E-PCA 73.27 ± 0.16 89.62 ± 0.05
LR-MVL 84.37 ± 0.16 89.77 ± 0.09
CW 87.62 ± 0.24 89.92 ± 0.07
Turian 84.44 ± 0.13 89.66 ± 0.10
HLBL 85.34 ± 0.10 89.64 ± 0.05
</table>
<tableCaption confidence="0.998319">
Table 3: Performance comparison on movie re-
</tableCaption>
<bodyText confidence="0.995471714285714">
view task with different embeddings. The first
column is results with the original embeddings.
The second column is results with embeddings af-
ter fine-tuning for this task. Results are reported
in classification accuracy (mean ± standard devi-
ation of ten training runs with different initializa-
tion).
</bodyText>
<page confidence="0.995499">
488
</page>
<table confidence="0.967139">
BORING BAD AWESOME
before after before after before after
SAD CRAP HORRIBLE TERRIBLE SPOOKY TERRIFIC
SILLY LAME TERRIBLE STUPID AWFUL TIMELESS
SUBLIME MESS DREADFUL BORING SILLY FANTASTIC
FANCY STUPID UNFORTUNATE DULL SUMMERTIME LOVELY
SOBER DULL AMAZING CRAP NASTY FLAWLESS
TRASH HORRIBLE AWFUL WRONG MACABRE MARVELOUS
LOUD RUBBISH MARVELOUS TRASH CRAZY EERIE
RIDICULOUS SHAME WONDERFUL SHAME ROTTEN LIVELY
RUDE AWFUL GOOD KINDA OUTRAGEOUS FANTASY
MAGIC ANNOYING FANTASTIC JOKE SCARY SURREAL
</table>
<tableCaption confidence="0.7549455">
Table 4: Set of words with their 10 nearest neighbors before and after fine-tuning for the movie review
task (using the Euclidean metric in the embedding space). H-PCA’s embeddings are used here.
</tableCaption>
<bodyText confidence="0.9999373125">
performs as well as the non-linear approach for
the movie review task. Our linear approach cap-
tures all the necessary sentiment features to pre-
dict whether a review is positive or negative. It
is thus not surprising that a bag-of-words based
method can perform well on this task (Wang and
Manning, 2012). However, as our method takes
the whole review as input, we can extract windows
of words having the most discriminative power:
it is a major advantage of our method compared
to conventional bag-of-words based methods. We
report in Table 5 some examples of windows of
words extracted from the most discriminative fil-
ters αi (positive and negative). Note that there is
about the same number of positive and negative
filters after learning.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999992529411765">
We have demonstrated that appealing word
embeddings can be obtained by computing a
Hellinger PCA of the word co-occurence ma-
trix. While a neural network language model
can be painful and long to train, we can get
a word co-occurence matrix by simply counting
words over a large corpus. The resulting em-
beddings give similar results on NLP tasks, even
from a N × 10,000 word co-occurence matrix
computed with only one word of context. It re-
veals that having a significant, but not too large
set of common words, seems sufficient for cap-
turing most of the syntactic and semantic char-
acteristics of words. As PCA of a N × 10, 000
matrix is really fast and not memory consuming,
our method gives an interesting and practical al-
ternative to neural language models for generat-
</bodyText>
<table confidence="0.900202105263158">
αi [x]t
- the worst film this year
very worst film i ’ve
very worst movie i ’ve
- watch this unfunny stinker.
, extremely unfunny drivel come
, this ludicrous script gets
- it was pointless and boring
it is unfunny. unfunny
film are awful and embarrassing
+ both really just wonderful.
. a truly excellent film
. a really great film
+ excellent film with great performances
excellent film with a great
excellent movie with a stellar
+ incredible . just incredible .
performances and just amazing .
one was really great.
</table>
<tableCaption confidence="0.974026">
Table 5: The top 3 positive and negative filters
</tableCaption>
<bodyText confidence="0.990721571428571">
αiwi and their respective top 3 windows of words
[x]t within the whole IMDB review dataset.
ing word embeddings. However, we showed that
deep-learning is an interesting framework to fine-
tune embeddings over specific NLP tasks. Our
H-PCA’s embeddings are available online, here:
http://www.lebret.ch/words/.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995265">
This work was supported by the HASLER foun-
dation through the grant “Information and Com-
munication Technology for a Better World 2020”
(SmartWorld).
</bodyText>
<page confidence="0.997085">
489
</page>
<bodyText confidence="0.997326">
T. K. Landauer. 2002. On the computational basis
of learning and cognition: Arguments from lsa. In
N. Ross, editor, The psychology of learning and mo-
tivation, volume 41, pages 43–84. Academic Press,
San Francisco, CA.
</bodyText>
<sectionHeader confidence="0.88951" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.866756">
R. K. Ando, T. Zhang, and P. Bartlett. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research, 6:1817–1853.
</bodyText>
<reference confidence="0.999896528735632">
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. J.
Mach. Learn. Res., 3:1137–1155, March.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,
and J C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467–479.
Y. Chen, B. Perozzi, R. Al-Rfou’, and S. Skiena. 2013.
The expressive power of word embeddings. CoRR,
abs/1301.3226.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493–2537.
P. S. Dhillon, D. Foster, and L. Ungar. 2011. Multi-
view learning of word embeddings via CCA. In
Advances in Neural Information Processing Systems
(NIPS), volume 24.
J. L. Elman. 1991. Distributed representations, sim-
ple recurrent networks, and grammatical structure.
Machine Learning, 7:195–225.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
55. 1952-59:1–32.
G. E. Hinton. 1986. Learning distributed representa-
tions of concepts. In Proceedings of the Eighth An-
nual Conference of the Cognitive Science Society,
pages 1–12. Hillsdale, NJ: Erlbaum.
F. Huang and A. Yates. 2009. Distributional represen-
tations for handling sparsity in supervised sequence-
labeling. In Proceedings of the Association for
Computational Linguistics (ACL), pages 495–503.
Association for Computational Linguistics.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL), pages 595–603.
I. Labutov and H. Lipson. 2013. Re-embedding words.
In ACL.
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato’s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
W. Lowe, 2001. Towards a theory of semantic space,
pages 576–581.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y.
Ng, and C. Potts. 2011. Learning word vectors for
sentiment analysis. In ACL, pages 142–150.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and
Sanjeev Khudanpur. 2010. Recurrent neural net-
work based language model.
A. Mnih and G. Hinton. 2008. A Scalable Hierarchical
Distributed Language Model. In Advances in Neural
Information Processing Systems, volume 21.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46:77–105.
L. R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. In Proceedings of the IEEE, pages 257–286.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL), pages
147–155. Association for Computational Linguis-
tics.
H. Sch¨utze. 1995. Distributional part-of-speech tag-
ging. In Proceedings of the Association for Compu-
tational Linguistics (ACL), pages 141–148. Morgan
Kaufmann Publishers Inc.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In ACL.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. J. Artif.
Int. Res., 37(1):141–188, January.
J. J. V¨ayrynen and T. Honkela. 2004. Word category
maps based on emergent features created by ICA.
In Proceedings of the STeP’2004 Cognition + Cy-
bernetics Symposium.
S Wang and C. D. Manning. 2012. Baselines and bi-
grams: Simple, good sentiment and topic classifica-
tion. ACL ’12.
L. Wittgenstein. 1953. Philosophical Investigations.
Blackwell.
</reference>
<page confidence="0.998263">
490
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.782966">
<title confidence="0.990173">Word Embeddings through Hellinger PCA</title>
<author confidence="0.885578">R´emi</author>
<affiliation confidence="0.991436">Idiap Research</affiliation>
<address confidence="0.976878">Rue Marconi 19, CP 1920 Martigny,</address>
<email confidence="0.991949">remi@lebret.ch</email>
<author confidence="0.974393">Ronan Collobert</author>
<affiliation confidence="0.99996">Idiap Research Institute</affiliation>
<address confidence="0.998932">Rue Marconi 19, CP 592 1920 Martigny, Switzerland</address>
<email confidence="0.999902">ronan@collobert.com</email>
<abstract confidence="0.997678684210526">Word embeddings resulting from neural language models have been shown to be a great asset for a large variety of NLP tasks. However, such architecture might be difficult and time-consuming to train. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word cooccurence matrix. We compare those new word embeddings with some well-known embeddings on named entity recognition and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="4095" citStr="Bengio et al. (2003)" startWordPosition="636" endWordPosition="639">comes from word order (Landauer, 2002), it seems quite important to leverage word order to capture all the semantic information. Connectionist approaches have therefore been proposed to develop distributed representations which encode the struc482 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–490, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tural relationships between words (Hinton, 1986; Pollack, 1990; Elman, 1991). More recently, a neural network language model was proposed in Bengio et al. (2003) where word vector representations are simultaneously learned along with a statistical language model. This architecture inspired other authors: Collobert and Weston (2008) designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim to predict correct scores end up learning the cooccurence statistics. Linguists assume</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>V J D Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="1102" citStr="Brown et al., 1992" startWordPosition="165" endWordPosition="168">drastically simplify the word embeddings computation through a Hellinger PCA of the word cooccurence matrix. We compare those new word embeddings with some well-known embeddings on named entity recognition and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks. 1 Introduction Building word embeddings has always generated much interest for linguists. Popular approaches such as Brown clustering algorithm (Brown et al., 1992) have been used with success in a wide variety of NLP tasks (Sch¨utze, 1995; Koo et al., 2008; Ratinov and Roth, 2009). Those word embeddings are often seen as a low dimensional-vector space where the dimensions are features potentially describing syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings (Collobert and Weston, 2008; Huang and Yates, 2009; Turian et al., 2010; Collobert et al., 2011). However, a neural network architecture can be hard to train. Finding the right parameters</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>B Perozzi</author>
<author>R Al-Rfou’</author>
<author>S Skiena</author>
</authors>
<title>The expressive power of word embeddings.</title>
<date>2013</date>
<location>CoRR, abs/1301.3226.</location>
<marker>Chen, Perozzi, Al-Rfou’, Skiena, 2013</marker>
<rawString>Y. Chen, B. Perozzi, R. Al-Rfou’, and S. Skiena. 2013. The expressive power of word embeddings. CoRR, abs/1301.3226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning, ICML.</booktitle>
<contexts>
<context position="1542" citStr="Collobert and Weston, 2008" startWordPosition="234" endWordPosition="237">gs to specific tasks. 1 Introduction Building word embeddings has always generated much interest for linguists. Popular approaches such as Brown clustering algorithm (Brown et al., 1992) have been used with success in a wide variety of NLP tasks (Sch¨utze, 1995; Koo et al., 2008; Ratinov and Roth, 2009). Those word embeddings are often seen as a low dimensional-vector space where the dimensions are features potentially describing syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings (Collobert and Weston, 2008; Huang and Yates, 2009; Turian et al., 2010; Collobert et al., 2011). However, a neural network architecture can be hard to train. Finding the right parameters to tune the model is often a challenging task and the training phase is in general computationally expensive. This paper aims to show that such good word embeddings can be obtained using simple (mostly linear) operations. We show that similar word embeddings can be computed using the word cooccurrence statistics and a well-known dimensionality reduction operation such as Principal Component Analysis (PCA). We then compare our embedding</context>
<context position="4267" citStr="Collobert and Weston (2008)" startWordPosition="660" endWordPosition="663">ore been proposed to develop distributed representations which encode the struc482 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–490, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tural relationships between words (Hinton, 1986; Pollack, 1990; Elman, 1991). More recently, a neural network language model was proposed in Bengio et al. (2003) where word vector representations are simultaneously learned along with a statistical language model. This architecture inspired other authors: Collobert and Weston (2008) designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim to predict correct scores end up learning the cooccurence statistics. Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings (Wittgenstein, 1953). Using the word cooccurrence statistics is thus a natural choice to e</context>
<context position="16107" citStr="Collobert and Weston, 2008" startWordPosition="2721" endWordPosition="2725">of N words [w]N1in D, the lookup table layer applies the same operation for each word in the sequence, producing the following output matrix: LTW([w]N1 ) = ((W)1[w]1 ... (W)[w]N) . Training Given a task of interest, a relevant representation of each word is then given by the corresponding lookup table feature vector, which is trained by backpropagation. Word representations are initialized with existing embeddings. LTW(wn) = W C0, ..., 485 5 Experimental Setup We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW (Collobert and Weston, 2008), Turian (Turian et al., 2010), HLBL (Mnih and Hinton, 2008), and LRMVL (Dhillon et al., 2011) embeddings on NER and movie review tasks. We also show that the general performance can be improved for these tasks by fine-tuning the word embeddings. 5.1 Building Word Representation over Large Corpora Our English corpus is composed of the entire English Wikipedia1 (where all MediaWiki markups have been removed), the Reuters corpus and the Wall Street Journal (WSJ) corpus. We consider lower case words to limit the number of words in the vocabulary. Additionally, all occurrences of sequences of numb</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1611" citStr="Collobert et al., 2011" startWordPosition="246" endWordPosition="249"> generated much interest for linguists. Popular approaches such as Brown clustering algorithm (Brown et al., 1992) have been used with success in a wide variety of NLP tasks (Sch¨utze, 1995; Koo et al., 2008; Ratinov and Roth, 2009). Those word embeddings are often seen as a low dimensional-vector space where the dimensions are features potentially describing syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings (Collobert and Weston, 2008; Huang and Yates, 2009; Turian et al., 2010; Collobert et al., 2011). However, a neural network architecture can be hard to train. Finding the right parameters to tune the model is often a challenging task and the training phase is in general computationally expensive. This paper aims to show that such good word embeddings can be obtained using simple (mostly linear) operations. We show that similar word embeddings can be computed using the word cooccurrence statistics and a well-known dimensionality reduction operation such as Principal Component Analysis (PCA). We then compare our embeddings with the CW (Collobert and Weston, 2008), Turian (Turian et al., 20</context>
<context position="3003" citStr="Collobert et al., 2011" startWordPosition="470" endWordPosition="473">on several NLP tasks. We claim that, assuming an appropriate metric, a simple spectral method as PCA can generate word embeddings as good as with deep-learning architectures. On the other hand, deep-learning architectures have shown their potential in several supervised NLP tasks, by using these word embeddings. As they are usually generated over large corpora of unlabeled data, words are represented in a generic manner. Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2011). For supervised tasks relying more on the semantic aspect as sentiment classification, it is usually helpful to adapt the existing embeddings to improve performance (Labutov and Lipson, 2013). We show in this paper that such embedding specialization can be easily done via neural network architectures and that helps to increase general performance. 2 Related Work As 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order (Landauer, 2002), it seems quite important to leverage word order to capture all the semantic information.</context>
<context position="13410" citStr="Collobert et al. (2011)" startWordPosition="2237" endWordPosition="2240">rward, as the number of terms in the logadd grows exponentially with the length of the sentence. It can be computed in linear time with the Forward algorithm, which derives a recursion similar to the Viterbi algorithm (see Rabiner (1989)). We can thus maximize the log-likelihood over all the training pairs ([x]T1 , [y]T1 ) to find, given a sentence [x]T1 , the best tag path which minimizes the sentence score (6): argmax s([x]T1 , [j]T1 ,˜θ) . (9) [j]T1 In contrast to classical CRF, all parameters θ are trained in a end-to-end manner, by backpropagation through the Forward recursion, following Collobert et al. (2011). 4.2 Document-level Approach The document-level approach is a document binary classifier, with classes y E {−1,1}. For each document, a set of (trained) filters is applied to the sliding window described in section 4.1. The maximum value obtained by the ith filter over the whole document is: max t [ ] wi[x]t + bi i,t 1 G i G nfilter . (10) It can be seen as a way to measure if the information represented by the filter has been captured in the document or not. We feed all these intermediate scores to a linear classifier, leading to the following simple model: [ ] fθ(x) = α max W[x]t + b . (11)</context>
<context position="14763" citStr="Collobert et al., 2011" startWordPosition="2481" endWordPosition="2485">tion 4.1, we will also consider a non-linear classifier in the experiments. Training The neural network is trained using stochastic gradient ascent. We denote θ all the trainable parameters of the network. Using a training set T, we minimize the following soft margin loss function with respect to θ: �θ +— log . (12) (1 + e−yfθ(x)) (x,y)ET 4.3 Embedding Fine-Tuning As seen in section 3, the process to compute generic word embedding is quite straightforward. These embeddings can then be used as features for supervised NLP systems and help to improve the general performance (Turian et al., 2010; Collobert et al., 2011; Chen et al., 2013). However, most of these systems cannot tune these embeddings as they are not structurally able to. By leveraging the deep architecture of our system, we can define a lookup-table layer initialized with existing embeddings as the first layer of the network. Lookup-Table Layer We consider a fixed-sized word dictionary D. Given a sequence of N words w1, w2, ... , wN, each word wn E W is first embedded into a dwrd-dimensional vector space, by applying a lookup-table operation: at in d 1 1 , ... ,0 ex wn = (W)wn , where the matrix W E Rdwrdx|D |represents the embeddings to be t</context>
<context position="20311" citStr="Collobert et al., 2011" startWordPosition="3406" endWordPosition="3409">m as the CW embeddings but with different parameters. We used only the 50 dimensions. 3Available at http://www.cis.upenn.edu/ ungar/eigenwords/ 4From SENNA: http://ml.nec-labs.com/senna/ 5Available at http://metaoptimize.com/projects/wordreprs/ 486 • HLBL5 : it covers 246,122 words with 50 or 100 dimensions for each word. They were trained on the RCV1 corpus using a Hierarchical Log-Bilinear Model. We used only the 50 dimensions. 5.3 Supervised Evaluation Tasks Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks (Turian et al., 2010; Collobert et al., 2011; Chen et al., 2013). Using our word embeddings, we thus trained the sentence-level architecture described in section 4.1 on a NER task. Named Entity Recognition (NER) It labels atomic elements in the sentence into categories such as “PERSON” or “LOCATION”. The CoNLL 2003 setup6 is a NER benchmark data set based on Reuters data. The contest provides training, validation and testing sets. The networks are fed with two raw features: word embeddings and a capital letter feature. The “caps” feature tells if each word was in lowercase, was all uppercase, had first letter capital, or had at least on</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Dhillon</author>
<author>D Foster</author>
<author>L Ungar</author>
</authors>
<title>Multiview learning of word embeddings via CCA.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>24</volume>
<contexts>
<context position="2329" citStr="Dhillon et al., 2011" startWordPosition="361" endWordPosition="364">ne the model is often a challenging task and the training phase is in general computationally expensive. This paper aims to show that such good word embeddings can be obtained using simple (mostly linear) operations. We show that similar word embeddings can be computed using the word cooccurrence statistics and a well-known dimensionality reduction operation such as Principal Component Analysis (PCA). We then compare our embeddings with the CW (Collobert and Weston, 2008), Turian (Turian et al., 2010), HLBL (Mnih and Hinton, 2008) embeddings, which come from deep architectures and the LR-MVL (Dhillon et al., 2011) embeddings, which also come from a spectral method on several NLP tasks. We claim that, assuming an appropriate metric, a simple spectral method as PCA can generate word embeddings as good as with deep-learning architectures. On the other hand, deep-learning architectures have shown their potential in several supervised NLP tasks, by using these word embeddings. As they are usually generated over large corpora of unlabeled data, words are represented in a generic manner. Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Par</context>
<context position="6005" citStr="Dhillon et al., 2011" startWordPosition="935" endWordPosition="938">es such as Singular Valued Decomposition (SVD) are widely used (e.g. LSA (Landauer and Dumais, 1997), ICA (V¨ayrynen and Honkela, 2004)). However, word co-occurence statistics are discrete distributions. An information theory measure such as the Hellinger distance seems to be more appropriate than the Euclidean distance over a discrete distribution space. In this paper we will compare the Hellinger PCA against the classical Euclidean PCA and the Low Rank MultiView Learning (LR-MVL) method, which is another spectral method based on Canonical Correlation Analysis (CCA) to learn word embeddings (Dhillon et al., 2011). It has been shown that using word embeddings as features helps to improve general performance on many NLP tasks (Turian et al., 2010). However these embeddings can be too generic to perform well on other tasks such as sentiment classification. For such task, word embeddings must capture the sentiment information. Maas et al. (2011) proposed a model for jointly capturing semantic and sentiment components of words into vector spaces. More recently, Labutov and Lipson (2013) presented a method which takes existing embeddings and, by using some labeled data, re-embed them in the same space. They</context>
<context position="16201" citStr="Dhillon et al., 2011" startWordPosition="2739" endWordPosition="2742">e, producing the following output matrix: LTW([w]N1 ) = ((W)1[w]1 ... (W)[w]N) . Training Given a task of interest, a relevant representation of each word is then given by the corresponding lookup table feature vector, which is trained by backpropagation. Word representations are initialized with existing embeddings. LTW(wn) = W C0, ..., 485 5 Experimental Setup We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW (Collobert and Weston, 2008), Turian (Turian et al., 2010), HLBL (Mnih and Hinton, 2008), and LRMVL (Dhillon et al., 2011) embeddings on NER and movie review tasks. We also show that the general performance can be improved for these tasks by fine-tuning the word embeddings. 5.1 Building Word Representation over Large Corpora Our English corpus is composed of the entire English Wikipedia1 (where all MediaWiki markups have been removed), the Reuters corpus and the Wall Street Journal (WSJ) corpus. We consider lower case words to limit the number of words in the vocabulary. Additionally, all occurrences of sequences of numbers within a word are replaced with the string “NUMBER”. The resulting text was tokenized usin</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>P. S. Dhillon, D. Foster, and L. Ungar. 2011. Multiview learning of word embeddings via CCA. In Advances in Neural Information Processing Systems (NIPS), volume 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Distributed representations, simple recurrent networks, and grammatical structure.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>7--195</pages>
<contexts>
<context position="4010" citStr="Elman, 1991" startWordPosition="624" endWordPosition="625"> of the meaning of English text comes from word choice and the remaining 20% comes from word order (Landauer, 2002), it seems quite important to leverage word order to capture all the semantic information. Connectionist approaches have therefore been proposed to develop distributed representations which encode the struc482 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–490, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tural relationships between words (Hinton, 1986; Pollack, 1990; Elman, 1991). More recently, a neural network language model was proposed in Bengio et al. (2003) where word vector representations are simultaneously learned along with a statistical language model. This architecture inspired other authors: Collobert and Weston (2008) designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim t</context>
</contexts>
<marker>Elman, 1991</marker>
<rawString>J. L. Elman. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine Learning, 7:195–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory</title>
<date>1957</date>
<pages>1930--55</pages>
<contexts>
<context position="7395" citStr="Firth, 1957" startWordPosition="1168" endWordPosition="1169">ed tasks: Named Entity Recognition and IMDB movie review. We analyze the effect of fine-tuning existing embeddings over each task of interest. 3 Spectral Method for Word Embeddings A NNLM learns which words among the vocabulary are likely to appear after a given sequence of words. More formally, it learns the next word probability distribution. Instead, simply counting words on a large corpus of unlabeled text can be performed to retrieve those word distributions and to represent words (Turney and Pantel, 2010). 3.1 Word co-occurence statistics ”You shall know a word by the company it keeps” (Firth, 1957). It is a natural choice to use the word co-occurence statistics to acquire representations of word meanings. Raw word co-occurence frequencies are computed by counting the number of times each context word w E D occurs after a sequence of words T: p(wJT) = p(w, T ) n(w,T) (1) p(T) Ew n(w, T) where n(w, T) is the number of times each context word w occurs after the sequence T. The size of T can go from 1 to t words. The next word probability distribution p for each word or sequence of words is thus obtained. It is a multinomial distribution of JDJ classes (words). A co-occurence matrix of size</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. 1957. A synopsis of linguistic theory 1930-55. 1952-59:1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Learning distributed representations of concepts.</title>
<date>1986</date>
<booktitle>In Proceedings of the Eighth Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1--12</pages>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="3981" citStr="Hinton, 1986" startWordPosition="620" endWordPosition="621">rmance. 2 Related Work As 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order (Landauer, 2002), it seems quite important to leverage word order to capture all the semantic information. Connectionist approaches have therefore been proposed to develop distributed representations which encode the struc482 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–490, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tural relationships between words (Hinton, 1986; Pollack, 1990; Elman, 1991). More recently, a neural network language model was proposed in Bengio et al. (2003) where word vector representations are simultaneously learned along with a statistical language model. This architecture inspired other authors: Collobert and Weston (2008) designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of </context>
</contexts>
<marker>Hinton, 1986</marker>
<rawString>G. E. Hinton. 1986. Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, pages 1–12. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>A Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequencelabeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>495--503</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1565" citStr="Huang and Yates, 2009" startWordPosition="238" endWordPosition="241">oduction Building word embeddings has always generated much interest for linguists. Popular approaches such as Brown clustering algorithm (Brown et al., 1992) have been used with success in a wide variety of NLP tasks (Sch¨utze, 1995; Koo et al., 2008; Ratinov and Roth, 2009). Those word embeddings are often seen as a low dimensional-vector space where the dimensions are features potentially describing syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings (Collobert and Weston, 2008; Huang and Yates, 2009; Turian et al., 2010; Collobert et al., 2011). However, a neural network architecture can be hard to train. Finding the right parameters to tune the model is often a challenging task and the training phase is in general computationally expensive. This paper aims to show that such good word embeddings can be obtained using simple (mostly linear) operations. We show that similar word embeddings can be computed using the word cooccurrence statistics and a well-known dimensionality reduction operation such as Principal Component Analysis (PCA). We then compare our embeddings with the CW (Collober</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>F. Huang and A. Yates. 2009. Distributional representations for handling sparsity in supervised sequencelabeling. In Proceedings of the Association for Computational Linguistics (ACL), pages 495–503. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>595--603</pages>
<contexts>
<context position="1195" citStr="Koo et al., 2008" startWordPosition="184" endWordPosition="187">ence matrix. We compare those new word embeddings with some well-known embeddings on named entity recognition and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks. 1 Introduction Building word embeddings has always generated much interest for linguists. Popular approaches such as Brown clustering algorithm (Brown et al., 1992) have been used with success in a wide variety of NLP tasks (Sch¨utze, 1995; Koo et al., 2008; Ratinov and Roth, 2009). Those word embeddings are often seen as a low dimensional-vector space where the dimensions are features potentially describing syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings (Collobert and Weston, 2008; Huang and Yates, 2009; Turian et al., 2010; Collobert et al., 2011). However, a neural network architecture can be hard to train. Finding the right parameters to tune the model is often a challenging task and the training phase is in general computati</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the Association for Computational Linguistics (ACL), pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Labutov</author>
<author>H Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3218" citStr="Labutov and Lipson, 2013" startWordPosition="503" endWordPosition="507">itectures have shown their potential in several supervised NLP tasks, by using these word embeddings. As they are usually generated over large corpora of unlabeled data, words are represented in a generic manner. Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2011). For supervised tasks relying more on the semantic aspect as sentiment classification, it is usually helpful to adapt the existing embeddings to improve performance (Labutov and Lipson, 2013). We show in this paper that such embedding specialization can be easily done via neural network architectures and that helps to increase general performance. 2 Related Work As 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order (Landauer, 2002), it seems quite important to leverage word order to capture all the semantic information. Connectionist approaches have therefore been proposed to develop distributed representations which encode the struc482 Proceedings of the 14th Conference of the European Chapter of the Association for Computational</context>
<context position="6483" citStr="Labutov and Lipson (2013)" startWordPosition="1014" endWordPosition="1018">ing (LR-MVL) method, which is another spectral method based on Canonical Correlation Analysis (CCA) to learn word embeddings (Dhillon et al., 2011). It has been shown that using word embeddings as features helps to improve general performance on many NLP tasks (Turian et al., 2010). However these embeddings can be too generic to perform well on other tasks such as sentiment classification. For such task, word embeddings must capture the sentiment information. Maas et al. (2011) proposed a model for jointly capturing semantic and sentiment components of words into vector spaces. More recently, Labutov and Lipson (2013) presented a method which takes existing embeddings and, by using some labeled data, re-embed them in the same space. They showed that these new embeddings can be better predictors in a supervised task. In this paper, we consider word embedding-based linear and non-linear models for two NLP supervised tasks: Named Entity Recognition and IMDB movie review. We analyze the effect of fine-tuning existing embeddings over each task of interest. 3 Spectral Method for Word Embeddings A NNLM learns which words among the vocabulary are likely to appear after a given sequence of words. More formally, it </context>
</contexts>
<marker>Labutov, Lipson, 2013</marker>
<rawString>I. Labutov and H. Lipson. 2013. Re-embedding words. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review.</journal>
<contexts>
<context position="5484" citStr="Landauer and Dumais, 1997" startWordPosition="852" endWordPosition="856">oice to embed similar words into a common vector space (Turney and Pantel, 2010). Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities (Lowe, 2001). Considering a fixed-sized word vocabulary D and a set of words W to embed, the co-occurence matrix C is of size JWJxJDJ. C is then vocabulary sizedependent. One can apply a dimensionality reduction operation to C leading to C¯ E R|W|xd, where d « JDJ. Dimensionality reduction techniques such as Singular Valued Decomposition (SVD) are widely used (e.g. LSA (Landauer and Dumais, 1997), ICA (V¨ayrynen and Honkela, 2004)). However, word co-occurence statistics are discrete distributions. An information theory measure such as the Hellinger distance seems to be more appropriate than the Euclidean distance over a discrete distribution space. In this paper we will compare the Hellinger PCA against the classical Euclidean PCA and the Low Rank MultiView Learning (LR-MVL) method, which is another spectral method based on Canonical Correlation Analysis (CCA) to learn word embeddings (Dhillon et al., 2011). It has been shown that using word embeddings as features helps to improve gen</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lowe</author>
</authors>
<title>Towards a theory of semantic space,</title>
<date>2001</date>
<pages>576--581</pages>
<contexts>
<context position="5097" citStr="Lowe, 2001" startWordPosition="788" endWordPosition="789">ork architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim to predict correct scores end up learning the cooccurence statistics. Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings (Wittgenstein, 1953). Using the word cooccurrence statistics is thus a natural choice to embed similar words into a common vector space (Turney and Pantel, 2010). Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities (Lowe, 2001). Considering a fixed-sized word vocabulary D and a set of words W to embed, the co-occurence matrix C is of size JWJxJDJ. C is then vocabulary sizedependent. One can apply a dimensionality reduction operation to C leading to C¯ E R|W|xd, where d « JDJ. Dimensionality reduction techniques such as Singular Valued Decomposition (SVD) are widely used (e.g. LSA (Landauer and Dumais, 1997), ICA (V¨ayrynen and Honkela, 2004)). However, word co-occurence statistics are discrete distributions. An information theory measure such as the Hellinger distance seems to be more appropriate than the Euclidean </context>
</contexts>
<marker>Lowe, 2001</marker>
<rawString>W. Lowe, 2001. Towards a theory of semantic space, pages 576–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Maas</author>
<author>R E Daly</author>
<author>P T Pham</author>
<author>D Huang</author>
<author>A Y Ng</author>
<author>C Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>142--150</pages>
<contexts>
<context position="6340" citStr="Maas et al. (2011)" startWordPosition="992" endWordPosition="995">distribution space. In this paper we will compare the Hellinger PCA against the classical Euclidean PCA and the Low Rank MultiView Learning (LR-MVL) method, which is another spectral method based on Canonical Correlation Analysis (CCA) to learn word embeddings (Dhillon et al., 2011). It has been shown that using word embeddings as features helps to improve general performance on many NLP tasks (Turian et al., 2010). However these embeddings can be too generic to perform well on other tasks such as sentiment classification. For such task, word embeddings must capture the sentiment information. Maas et al. (2011) proposed a model for jointly capturing semantic and sentiment components of words into vector spaces. More recently, Labutov and Lipson (2013) presented a method which takes existing embeddings and, by using some labeled data, re-embed them in the same space. They showed that these new embeddings can be better predictors in a supervised task. In this paper, we consider word embedding-based linear and non-linear models for two NLP supervised tasks: Named Entity Recognition and IMDB movie review. We analyze the effect of fine-tuning existing embeddings over each task of interest. 3 Spectral Met</context>
<context position="22610" citStr="Maas et al. (2011)" startWordPosition="3793" endWordPosition="3796">% accuracy. Only highly polarized reviews have been considered. A nega6http://www.cnts.ua.ac.be/conll2003/ner/ 7Available at http://www.andrew-maas.net/data/sentiment tive review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. It has been evenly divided into training and test sets (25,000 reviews each). For this task, we only used the word embeddings as features. We perform a simple cross-validation on the training set to choose the optimal hyper-parameters. The network had a window of 5 words and nfilter = 1000 filters. As benchmark system, we report the system of Maas et al. (2011), which reached 88.90% accuracy with a mix of unsupervised and supervised techniques to learn word vectors capturing semantic termdocument information, as well as rich sentiment content. (a) NER validation set. (b) IMDB review dataset. Figure 1: Effect of varying the normalization factor A with a non-linear approach and fine-tuning. 5.4 Embeddings Normalization Word embeddings are continuous vector spaces that are not necessarily in a bounded range. To avoid saturation issues in the network architectures, embeddings need to be properly normalized. Considering the matrix of word embeddings E, t</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. 2011. Learning word vectors for sentiment analysis. In ACL, pages 142–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>M Karafiat</author>
<author>L Burget</author>
<author>J Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<contexts>
<context position="4449" citStr="Mikolov et al. (2010)" startWordPosition="688" endWordPosition="691">cs, pages 482–490, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tural relationships between words (Hinton, 1986; Pollack, 1990; Elman, 1991). More recently, a neural network language model was proposed in Bengio et al. (2003) where word vector representations are simultaneously learned along with a statistical language model. This architecture inspired other authors: Collobert and Weston (2008) designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim to predict correct scores end up learning the cooccurence statistics. Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings (Wittgenstein, 1953). Using the word cooccurrence statistics is thus a natural choice to embed similar words into a common vector space (Turney and Pantel, 2010). Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensiona</context>
</contexts>
<marker>Mikolov, Karafiat, Burget, Cernocky, Khudanpur, 2010</marker>
<rawString>T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>A Scalable Hierarchical Distributed Language Model.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>21</volume>
<contexts>
<context position="2244" citStr="Mnih and Hinton, 2008" startWordPosition="348" endWordPosition="351">a neural network architecture can be hard to train. Finding the right parameters to tune the model is often a challenging task and the training phase is in general computationally expensive. This paper aims to show that such good word embeddings can be obtained using simple (mostly linear) operations. We show that similar word embeddings can be computed using the word cooccurrence statistics and a well-known dimensionality reduction operation such as Principal Component Analysis (PCA). We then compare our embeddings with the CW (Collobert and Weston, 2008), Turian (Turian et al., 2010), HLBL (Mnih and Hinton, 2008) embeddings, which come from deep architectures and the LR-MVL (Dhillon et al., 2011) embeddings, which also come from a spectral method on several NLP tasks. We claim that, assuming an appropriate metric, a simple spectral method as PCA can generate word embeddings as good as with deep-learning architectures. On the other hand, deep-learning architectures have shown their potential in several supervised NLP tasks, by using these word embeddings. As they are usually generated over large corpora of unlabeled data, words are represented in a generic manner. Having generic embeddings, good perfor</context>
<context position="4382" citStr="Mnih and Hinton (2008)" startWordPosition="678" endWordPosition="681"> the European Chapter of the Association for Computational Linguistics, pages 482–490, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tural relationships between words (Hinton, 1986; Pollack, 1990; Elman, 1991). More recently, a neural network language model was proposed in Bengio et al. (2003) where word vector representations are simultaneously learned along with a statistical language model. This architecture inspired other authors: Collobert and Weston (2008) designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim to predict correct scores end up learning the cooccurence statistics. Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings (Wittgenstein, 1953). Using the word cooccurrence statistics is thus a natural choice to embed similar words into a common vector space (Turney and Pantel, 2010). Common approaches calculate the frequencie</context>
<context position="16167" citStr="Mnih and Hinton, 2008" startWordPosition="2732" endWordPosition="2735">ration for each word in the sequence, producing the following output matrix: LTW([w]N1 ) = ((W)1[w]1 ... (W)[w]N) . Training Given a task of interest, a relevant representation of each word is then given by the corresponding lookup table feature vector, which is trained by backpropagation. Word representations are initialized with existing embeddings. LTW(wn) = W C0, ..., 485 5 Experimental Setup We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW (Collobert and Weston, 2008), Turian (Turian et al., 2010), HLBL (Mnih and Hinton, 2008), and LRMVL (Dhillon et al., 2011) embeddings on NER and movie review tasks. We also show that the general performance can be improved for these tasks by fine-tuning the word embeddings. 5.1 Building Word Representation over Large Corpora Our English corpus is composed of the entire English Wikipedia1 (where all MediaWiki markups have been removed), the Reuters corpus and the Wall Street Journal (WSJ) corpus. We consider lower case words to limit the number of words in the vocabulary. Additionally, all occurrences of sequences of numbers within a word are replaced with the string “NUMBER”. The</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>A. Mnih and G. Hinton. 2008. A Scalable Hierarchical Distributed Language Model. In Advances in Neural Information Processing Systems, volume 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--77</pages>
<contexts>
<context position="3996" citStr="Pollack, 1990" startWordPosition="622" endWordPosition="623">ted Work As 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order (Landauer, 2002), it seems quite important to leverage word order to capture all the semantic information. Connectionist approaches have therefore been proposed to develop distributed representations which encode the struc482 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–490, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tural relationships between words (Hinton, 1986; Pollack, 1990; Elman, 1991). More recently, a neural network language model was proposed in Bengio et al. (2003) where word vector representations are simultaneously learned along with a statistical language model. This architecture inspired other authors: Collobert and Weston (2008) designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text </context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>J. B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46:77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="13024" citStr="Rabiner (1989)" startWordPosition="2173" endWordPosition="2174">we interpret the resulting ratio as a conditional tag path probability. Taking the log, the conditional probability of the true path [y]T1 is therefore given by: log p([y]T 1 , [x]T 1 , ˜θ) = s([x]T 1 , [y]T 1 , ˜θ) − logadd s([x]T1 ,[j]T1 ,˜θ), d[j]T1 (7) where we adopt the notation logadd i �zi = log ( i ezi) . (8) Computing the log-likelihood efficiently is not straightforward, as the number of terms in the logadd grows exponentially with the length of the sentence. It can be computed in linear time with the Forward algorithm, which derives a recursion similar to the Viterbi algorithm (see Rabiner (1989)). We can thus maximize the log-likelihood over all the training pairs ([x]T1 , [y]T1 ) to find, given a sentence [x]T1 , the best tag path which minimizes the sentence score (6): argmax s([x]T1 , [j]T1 ,˜θ) . (9) [j]T1 In contrast to classical CRF, all parameters θ are trained in a end-to-end manner, by backpropagation through the Forward recursion, following Collobert et al. (2011). 4.2 Document-level Approach The document-level approach is a document binary classifier, with classes y E {−1,1}. For each document, a set of (trained) filters is applied to the sliding window described in sectio</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>147--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1220" citStr="Ratinov and Roth, 2009" startWordPosition="188" endWordPosition="191">mpare those new word embeddings with some well-known embeddings on named entity recognition and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks. 1 Introduction Building word embeddings has always generated much interest for linguists. Popular approaches such as Brown clustering algorithm (Brown et al., 1992) have been used with success in a wide variety of NLP tasks (Sch¨utze, 1995; Koo et al., 2008; Ratinov and Roth, 2009). Those word embeddings are often seen as a low dimensional-vector space where the dimensions are features potentially describing syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings (Collobert and Weston, 2008; Huang and Yates, 2009; Turian et al., 2010; Collobert et al., 2011). However, a neural network architecture can be hard to train. Finding the right parameters to tune the model is often a challenging task and the training phase is in general computationally expensive. This pa</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147–155. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>141--148</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<marker>Sch¨utze, 1995</marker>
<rawString>H. Sch¨utze. 1995. Distributional part-of-speech tagging. In Proceedings of the Association for Computational Linguistics (ACL), pages 141–148. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1586" citStr="Turian et al., 2010" startWordPosition="242" endWordPosition="245">embeddings has always generated much interest for linguists. Popular approaches such as Brown clustering algorithm (Brown et al., 1992) have been used with success in a wide variety of NLP tasks (Sch¨utze, 1995; Koo et al., 2008; Ratinov and Roth, 2009). Those word embeddings are often seen as a low dimensional-vector space where the dimensions are features potentially describing syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings (Collobert and Weston, 2008; Huang and Yates, 2009; Turian et al., 2010; Collobert et al., 2011). However, a neural network architecture can be hard to train. Finding the right parameters to tune the model is often a challenging task and the training phase is in general computationally expensive. This paper aims to show that such good word embeddings can be obtained using simple (mostly linear) operations. We show that similar word embeddings can be computed using the word cooccurrence statistics and a well-known dimensionality reduction operation such as Principal Component Analysis (PCA). We then compare our embeddings with the CW (Collobert and Weston, 2008), </context>
<context position="2979" citStr="Turian et al., 2010" startWordPosition="466" endWordPosition="469">om a spectral method on several NLP tasks. We claim that, assuming an appropriate metric, a simple spectral method as PCA can generate word embeddings as good as with deep-learning architectures. On the other hand, deep-learning architectures have shown their potential in several supervised NLP tasks, by using these word embeddings. As they are usually generated over large corpora of unlabeled data, words are represented in a generic manner. Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2011). For supervised tasks relying more on the semantic aspect as sentiment classification, it is usually helpful to adapt the existing embeddings to improve performance (Labutov and Lipson, 2013). We show in this paper that such embedding specialization can be easily done via neural network architectures and that helps to increase general performance. 2 Related Work As 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order (Landauer, 2002), it seems quite important to leverage word order to capture all t</context>
<context position="6140" citStr="Turian et al., 2010" startWordPosition="958" endWordPosition="961">. However, word co-occurence statistics are discrete distributions. An information theory measure such as the Hellinger distance seems to be more appropriate than the Euclidean distance over a discrete distribution space. In this paper we will compare the Hellinger PCA against the classical Euclidean PCA and the Low Rank MultiView Learning (LR-MVL) method, which is another spectral method based on Canonical Correlation Analysis (CCA) to learn word embeddings (Dhillon et al., 2011). It has been shown that using word embeddings as features helps to improve general performance on many NLP tasks (Turian et al., 2010). However these embeddings can be too generic to perform well on other tasks such as sentiment classification. For such task, word embeddings must capture the sentiment information. Maas et al. (2011) proposed a model for jointly capturing semantic and sentiment components of words into vector spaces. More recently, Labutov and Lipson (2013) presented a method which takes existing embeddings and, by using some labeled data, re-embed them in the same space. They showed that these new embeddings can be better predictors in a supervised task. In this paper, we consider word embedding-based linear</context>
<context position="14739" citStr="Turian et al., 2010" startWordPosition="2477" endWordPosition="2480">sign of αi. As in section 4.1, we will also consider a non-linear classifier in the experiments. Training The neural network is trained using stochastic gradient ascent. We denote θ all the trainable parameters of the network. Using a training set T, we minimize the following soft margin loss function with respect to θ: �θ +— log . (12) (1 + e−yfθ(x)) (x,y)ET 4.3 Embedding Fine-Tuning As seen in section 3, the process to compute generic word embedding is quite straightforward. These embeddings can then be used as features for supervised NLP systems and help to improve the general performance (Turian et al., 2010; Collobert et al., 2011; Chen et al., 2013). However, most of these systems cannot tune these embeddings as they are not structurally able to. By leveraging the deep architecture of our system, we can define a lookup-table layer initialized with existing embeddings as the first layer of the network. Lookup-Table Layer We consider a fixed-sized word dictionary D. Given a sequence of N words w1, w2, ... , wN, each word wn E W is first embedded into a dwrd-dimensional vector space, by applying a lookup-table operation: at in d 1 1 , ... ,0 ex wn = (W)wn , where the matrix W E Rdwrdx|D |represent</context>
<context position="16137" citStr="Turian et al., 2010" startWordPosition="2727" endWordPosition="2730">e layer applies the same operation for each word in the sequence, producing the following output matrix: LTW([w]N1 ) = ((W)1[w]1 ... (W)[w]N) . Training Given a task of interest, a relevant representation of each word is then given by the corresponding lookup table feature vector, which is trained by backpropagation. Word representations are initialized with existing embeddings. LTW(wn) = W C0, ..., 485 5 Experimental Setup We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW (Collobert and Weston, 2008), Turian (Turian et al., 2010), HLBL (Mnih and Hinton, 2008), and LRMVL (Dhillon et al., 2011) embeddings on NER and movie review tasks. We also show that the general performance can be improved for these tasks by fine-tuning the word embeddings. 5.1 Building Word Representation over Large Corpora Our English corpus is composed of the entire English Wikipedia1 (where all MediaWiki markups have been removed), the Reuters corpus and the Wall Street Journal (WSJ) corpus. We consider lower case words to limit the number of words in the vocabulary. Additionally, all occurrences of sequences of numbers within a word are replaced</context>
<context position="20287" citStr="Turian et al., 2010" startWordPosition="3402" endWordPosition="3405"> using the same system as the CW embeddings but with different parameters. We used only the 50 dimensions. 3Available at http://www.cis.upenn.edu/ ungar/eigenwords/ 4From SENNA: http://ml.nec-labs.com/senna/ 5Available at http://metaoptimize.com/projects/wordreprs/ 486 • HLBL5 : it covers 246,122 words with 50 or 100 dimensions for each word. They were trained on the RCV1 corpus using a Hierarchical Log-Bilinear Model. We used only the 50 dimensions. 5.3 Supervised Evaluation Tasks Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks (Turian et al., 2010; Collobert et al., 2011; Chen et al., 2013). Using our word embeddings, we thus trained the sentence-level architecture described in section 4.1 on a NER task. Named Entity Recognition (NER) It labels atomic elements in the sentence into categories such as “PERSON” or “LOCATION”. The CoNLL 2003 setup6 is a NER benchmark data set based on Reuters data. The contest provides training, validation and testing sets. The networks are fed with two raw features: word embeddings and a capital letter feature. The “caps” feature tells if each word was in lowercase, was all uppercase, had first letter cap</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semisupervised learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="4938" citStr="Turney and Pantel, 2010" startWordPosition="766" endWordPosition="769">tes the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim to predict correct scores end up learning the cooccurence statistics. Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings (Wittgenstein, 1953). Using the word cooccurrence statistics is thus a natural choice to embed similar words into a common vector space (Turney and Pantel, 2010). Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities (Lowe, 2001). Considering a fixed-sized word vocabulary D and a set of words W to embed, the co-occurence matrix C is of size JWJxJDJ. C is then vocabulary sizedependent. One can apply a dimensionality reduction operation to C leading to C¯ E R|W|xd, where d « JDJ. Dimensionality reduction techniques such as Singular Valued Decomposition (SVD) are widely used (e.g. LSA (Landauer and Dumais, 1997), ICA (V¨ayrynen and Honkela, 2004)). However, word co</context>
<context position="7299" citStr="Turney and Pantel, 2010" startWordPosition="1150" endWordPosition="1153">ised task. In this paper, we consider word embedding-based linear and non-linear models for two NLP supervised tasks: Named Entity Recognition and IMDB movie review. We analyze the effect of fine-tuning existing embeddings over each task of interest. 3 Spectral Method for Word Embeddings A NNLM learns which words among the vocabulary are likely to appear after a given sequence of words. More formally, it learns the next word probability distribution. Instead, simply counting words on a large corpus of unlabeled text can be performed to retrieve those word distributions and to represent words (Turney and Pantel, 2010). 3.1 Word co-occurence statistics ”You shall know a word by the company it keeps” (Firth, 1957). It is a natural choice to use the word co-occurence statistics to acquire representations of word meanings. Raw word co-occurence frequencies are computed by counting the number of times each context word w E D occurs after a sequence of words T: p(wJT) = p(w, T ) n(w,T) (1) p(T) Ew n(w, T) where n(w, T) is the number of times each context word w occurs after the sequence T. The size of T can go from 1 to t words. The next word probability distribution p for each word or sequence of words is thus </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. J. Artif. Int. Res., 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J V¨ayrynen</author>
<author>T Honkela</author>
</authors>
<title>Word category maps based on emergent features created by ICA.</title>
<date>2004</date>
<booktitle>In Proceedings of the STeP’2004 Cognition + Cybernetics Symposium.</booktitle>
<marker>V¨ayrynen, Honkela, 2004</marker>
<rawString>J. J. V¨ayrynen and T. Honkela. 2004. Word category maps based on emergent features created by ICA. In Proceedings of the STeP’2004 Cognition + Cybernetics Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>C D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<journal>ACL</journal>
<volume>12</volume>
<contexts>
<context position="28282" citStr="Wang and Manning, 2012" startWordPosition="4759" endWordPosition="4762">US SHAME WONDERFUL SHAME ROTTEN LIVELY RUDE AWFUL GOOD KINDA OUTRAGEOUS FANTASY MAGIC ANNOYING FANTASTIC JOKE SCARY SURREAL Table 4: Set of words with their 10 nearest neighbors before and after fine-tuning for the movie review task (using the Euclidean metric in the embedding space). H-PCA’s embeddings are used here. performs as well as the non-linear approach for the movie review task. Our linear approach captures all the necessary sentiment features to predict whether a review is positive or negative. It is thus not surprising that a bag-of-words based method can perform well on this task (Wang and Manning, 2012). However, as our method takes the whole review as input, we can extract windows of words having the most discriminative power: it is a major advantage of our method compared to conventional bag-of-words based methods. We report in Table 5 some examples of windows of words extracted from the most discriminative filters αi (positive and negative). Note that there is about the same number of positive and negative filters after learning. 6 Conclusion We have demonstrated that appealing word embeddings can be obtained by computing a Hellinger PCA of the word co-occurence matrix. While a neural net</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>S Wang and C. D. Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. ACL ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wittgenstein</author>
</authors>
<title>Philosophical Investigations.</title>
<date>1953</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="4797" citStr="Wittgenstein, 1953" startWordPosition="743" endWordPosition="744">cal language model. This architecture inspired other authors: Collobert and Weston (2008) designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton (2008) proposed a hierarchical linear neural model, Mikolov et al. (2010) investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim to predict correct scores end up learning the cooccurence statistics. Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings (Wittgenstein, 1953). Using the word cooccurrence statistics is thus a natural choice to embed similar words into a common vector space (Turney and Pantel, 2010). Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities (Lowe, 2001). Considering a fixed-sized word vocabulary D and a set of words W to embed, the co-occurence matrix C is of size JWJxJDJ. C is then vocabulary sizedependent. One can apply a dimensionality reduction operation to C leading to C¯ E R|W|xd, where d « JDJ. Dimensionality reduction techniques such as Si</context>
</contexts>
<marker>Wittgenstein, 1953</marker>
<rawString>L. Wittgenstein. 1953. Philosophical Investigations. Blackwell.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>