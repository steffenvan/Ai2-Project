<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.975116">
Combining Speech Retrieval Results with Generalized Additive Models
</title>
<author confidence="0.942306">
J. Scott Olsson* and Douglas W. Oard†
</author>
<affiliation confidence="0.982834">
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742
Human Language Technology Center of Excellence
John Hopkins University, Baltimore, MD 21211
</affiliation>
<email confidence="0.998412">
olsson@math.umd.edu,oard@umd.edu
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999074">
Rapid and inexpensive techniques for auto-
matic transcription of speech have the po-
tential to dramatically expand the types of
content to which information retrieval tech-
niques can be productively applied, but lim-
itations in accuracy and robustness must be
overcome before that promise can be fully
realized. Combining retrieval results from
systems built on various errorful representa-
tions of the same collection offers some po-
tential to address these challenges. This pa-
per explores that potential by applying Gener-
alized Additive Models to optimize the combi-
nation of ranked retrieval results obtained us-
ing transcripts produced automatically for the
same spoken content by substantially differ-
ent recognition systems. Topic-averaged re-
trieval effectiveness better than any previously
reported for the same collection was obtained,
and even larger gains are apparent when using
an alternative measure emphasizing results on
the most difficult topics.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999767">
Speech retrieval, like other tasks that require trans-
forming the representation of language, suffers from
both random and systematic errors that are intro-
duced by the speech-to-text transducer. Limita-
tions in signal processing, acoustic modeling, pro-
nunciation, vocabulary, and language modeling can
be accommodated in several ways, each of which
make different trade-offs and thus induce different
</bodyText>
<note confidence="0.8056515">
* Dept. of Mathematics/AMSC, UMD
† College of Information Studies, UMD
</note>
<bodyText confidence="0.999928176470588">
error characteristics. Moreover, different applica-
tions produce different types of challenges and dif-
ferent opportunities. As a result, optimizing a sin-
gle recognition system for all transcription tasks is
well beyond the reach of present technology, and
even systems that are apparently similar on average
can make different mistakes on different sources. A
natural response to this challenge is to combine re-
trieval results from multiple systems, each imper-
fect, to achieve reasonably robust behavior over a
broader range of tasks. In this paper, we compare
alternative ways of combining these ranked lists.
Note, we do not assume access to the internal work-
ings of the recognition systems, or even to the tran-
scripts produced by those systems.
System combination has a long history in infor-
mation retrieval. Most often, the goal is to combine
results from systems that search different content
(“collection fusion”) or to combine results from dif-
ferent systems on the same content (“data fusion”).
When working with multiple transcriptions of the
same content, we are again presented with new op-
portunities. In this paper we compare some well
known techniques for combination of retrieval re-
sults with a new evidence combination technique
based on a general framework known as Gener-
alized Additive Models (GAMs). We show that
this new technique significantly outperforms sev-
eral well known information retrieval fusion tech-
niques, and we present evidence that it is the ability
of GAMs to combine inputs non-linearly that at least
partly explains our improvements.
The remainder of this paper is organized as fol-
lows. We first review prior work on evidence com-
</bodyText>
<page confidence="0.992355">
461
</page>
<note confidence="0.695944">
Proceedings ofACL-08: HLT, pages 461–469,
</note>
<page confidence="0.494652">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.9999083">
bination in information retrieval in Section 2, and
then introduce Generalized Additive Models in Sec-
tion 3. Section 4 describes the design of our ex-
periments with a 589 hour collection of conversa-
tional speech for which information retrieval queries
and relevance judgments are available. Section 5
presents the results of our experiments, and we con-
clude in Section 6 with a brief discussion of implica-
tions of our results and the potential for future work
on this important problem.
</bodyText>
<sectionHeader confidence="0.996524" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99997992">
One approach for combining ranked retrieval results
is to simply linearly combine the multiple system
scores for each topic and document. This approach
has been extensively applied in the literature (Bartell
et al., 1994; Callan et al., 1995; Powell et al., 2000;
Vogt and Cottrell, 1999), with varying degrees of
success, owing in part to the potential difficulty of
normalizing scores across retrieval systems. In this
study, we partially abstract away from this poten-
tial difficulty by using the same retrieval system on
both representations of the collection documents (so
that we don’t expect score distributions to be signif-
icantly different for the combination inputs).
Of course, many fusion techniques using more ad-
vanced score normalization methods have been pro-
posed. Shaw and Fox (1994) proposed a number
of such techniques, perhaps the most successful of
which is known as CombMNZ. CombMNZ has been
shown to achieve strong performance and has been
used in many subsequent studies (Lee, 1997; Mon-
tague and Aslam, 2002; Beitzel et al., 2004; Lillis et
al., 2006). In this study, we also use CombMNZ
as a baseline for comparison, and following Lil-
lis et al. (2006) and Lee (1997), compute it in the
following way. First, we normalize each score si
</bodyText>
<equation confidence="0.631642">
as norm(si) = si−min(s) where max(s) and
max(s)−min(s)
</equation>
<bodyText confidence="0.999115">
min(s) are the maximum and minimum scores seen
in the input result list. After normalization, the
CombMNZ score for a document d is computed as
</bodyText>
<equation confidence="0.9950065">
G
CombMNZd = Ns,d x |Nd &gt; 0|.
</equation>
<bodyText confidence="0.992767423076923">
Here, L is the number of ranked lists to be com-
bined, Nt,d is the normalized score of document d
in ranked list E, and |Nd &gt; 0 |is the number of non-
zero normalized scores given to d by any result set.
Manmatha et al. (2001) showed that retrieval
scores from IR systems could be modeled using a
Normal distribution for relevant documents and ex-
ponential distribution for non-relevant documents.
However, in their study, fusion results using these
comparatively complex normalization approaches
achieved performance no better than the much sim-
pler CombMNZ.
A simple rank-based fusion technique is inter-
leaving (Voorhees et al., 1994). In this approach,
the highest ranked document from each list is taken
in turn (ignoring duplicates) and placed at the top of
the new, combined list.
Many probabilistic combination approaches have
also been developed, a recent example being Lillis
et al. (2006). Perhaps the most closely related pro-
posal, using logistic regression, was made first by
Savoy et al. (1988). Logistic regression is one exam-
ple from the broad class of models which GAMs en-
compass. Unlike GAMs in their full generality how-
ever, logistic regression imposes a comparatively
high degree of linearity in the model structure.
</bodyText>
<subsectionHeader confidence="0.998775">
2.1 Combining speech retrieval results
</subsectionHeader>
<bodyText confidence="0.999966636363636">
Previous work on single-collection result fusion has
naturally focused on combining results from multi-
ple retrieval systems. In this case, the potential for
performance improvements depends critically on the
uniqueness of the different input systems being com-
bined. Accordingly, small variations in the same
system often do not combine to produce results bet-
ter than the best of their inputs (Beitzel et al., 2004).
Errorful document collections such as conversa-
tional speech introduce new difficulties and oppor-
tunities for data fusion. This is so, in particular,
because even the same system can produce drasti-
cally different retrieval results when multiple repre-
sentations of the documents (e.g., multiple transcript
hypotheses) are available. Consider, for example,
Figure 1 which shows, for each term in each of our
title queries, the proportion of relevant documents
containing that term in only one of our two tran-
script hypotheses. Critically, by plotting this propor-
tion against the term’s inverse document frequency,
we observe that the most discriminative query terms
are often not available in both document represen-
</bodyText>
<page confidence="0.99888">
462
</page>
<figure confidence="0.98462">
1 2
Inverse
</figure>
<figureCaption confidence="0.995054">
Figure 1: For each term in each query, the proportion of
</figureCaption>
<bodyText confidence="0.948328375">
relevant documents containing the term vs. inverse doc-
ument frequency. For increasingly discriminative terms
(higher idf), we observe that the probability of only one
transcript containing the term increases dramatically.
tations. As these high-idf terms make large contri-
butions to retrieval scores, this suggests that even an
identical retrieval system may return a large score
using one transcript hypothesis, and yet a very low
score using another. Accordingly, a linear combina-
tion of scores is unlikely to be optimal.
A second example illustrates the difficulty. Sup-
pose recognition system A can recognize a particu-
lar high-idf query term, but system B never can. In
the extreme case, the term may simply be out of vo-
cabulary, although this may occur for various other
reasons (e.g., poor language modeling or pronuncia-
tion dictionaries). Here again, a linear combination
of scores will fail, as will rank-based interleaving.
In the latter case, we will alternate between taking a
plausible document from system A and an inevitably
worse result from the crippled system B.
As a potential solution for these difficulties, we
consider the use of generalized additive models for
retrieval fusion.
</bodyText>
<sectionHeader confidence="0.991217" genericHeader="method">
3 Generalized Additive Models
</sectionHeader>
<bodyText confidence="0.961476571428571">
Generalized Additive Models (GAMs) are a gen-
eralization of Generalized Linear Models (GLMs),
while GLMs are a generalization of the well known
linear model. In a GLM, the distribution of an ob-
served random variable Yi is related to the linear pre-
dictor ηi through a smooth monotonic link function
g,
</bodyText>
<equation confidence="0.98855">
g(µi) = ηi = Xiβ.
</equation>
<bodyText confidence="0.999615285714286">
Here, Xi is the ith row of the model matrix X (one
set of observations corresponding to one observed
yi) and β is a vector of unknown parameters to be
learned from the data. If we constrain our link func-
tion g to be the identity transformation, and assume
Yi is Normal, then our GLM reduces to a simple lin-
ear model.
But GLMs are considerably more versatile than
linear models. First, rather than only the Normal dis-
tribution, the response Yi is free to have any distribu-
tion belonging to the exponential family of distribu-
tions. This family includes many useful distributions
such as the Binomial, Normal, Gamma, and Poisson.
Secondly, by allowing non-identity link functions g,
some degree of non-linearity may be incorporated in
the model structure.
A well known GLM in the NLP communityaisnlo-
gistic regression (which may alternatively be derived
as a maximum entropy classifier). In logistic regres-
sion, the response is assumed to be Binomial and the
chosen link function is the logit transformation,
</bodyText>
<equation confidence="0.917196">
n
-
g(µi) = logit(µi) = log µi1 − µi
.
</equation>
<bodyText confidence="0.99969025">
Generalized additive models allow for additional
model flexibility by allowing the linear predictor to
now also contain learned smooth functions fj of the
covariates xk. For example,
</bodyText>
<equation confidence="0.978755">
g(µi) = X*iθ + f1(x1i) + f2(x2i) + f3(x3i, x4i).
</equation>
<bodyText confidence="0.998343222222222">
As in a GLM, µi - E(Yi) and Yi belongs to the
exponential family. Strictly parametric model com-
ponents are still permitted, which we represent as a
row of the model matrix XZ (with associated param-
eters θ).
GAMs may be thought of as GLMs where one
or more covariate has been transformed by a basis
expansion, f(x) = Eqj bj(x)βj. Given a set of q
basis functions bj spanning a q-dimensional space
</bodyText>
<figure confidence="0.994621714285714">
c
n c
o n l
ou
s
0.. , 08
po
</figure>
<page confidence="0.99959">
463
</page>
<bodyText confidence="0.9999764375">
of smooth transformations, we are back to the lin-
ear problem of learning coefficients Oj which “opti-
mally” fit the data. If we knew the appropriate trans-
formation of our covariates (say the logarithm), we
could simply apply it ourselves. GAMs allow us to
learn these transformations from the data, when we
expect some transformation to be useful but don’t
know it’s form a priori. In practice, these smooth
functions may be represented and the model pa-
rameters may be learned in various ways. In this
work, we use the excellent open source package
mgcv (Wood, 2006), which uses penalized likeli-
hood maximization to prevent arbitrarily “wiggly”
smooth functions (i.e., overfitting). Smooths (in-
cluding multidimensional smooths) are represented
by thin plate regression splines (Wood, 2003).
</bodyText>
<subsectionHeader confidence="0.948987">
3.1 Combining speech retrieval results with
GAMs
</subsectionHeader>
<bodyText confidence="0.999927166666667">
The chief difficulty introduced in combining ranked
speech retrieval results is the severe disagreement in-
troduced by differing document hypotheses. As we
saw in Figure 1, it is often the case that the most dis-
criminative query terms occur in only one transcript
source.
</bodyText>
<subsectionHeader confidence="0.846577">
3.1.1 GLM with factors
</subsectionHeader>
<bodyText confidence="0.999865333333333">
Our first new approach for handling differences in
transcripts is an extension of the logistic regression
model previously used in data fusion work, (Savoy
et al., 1988). Specifically, we augment the model
with the first-order interaction of scores x1x2 and
the factor αi, so that
</bodyText>
<equation confidence="0.989549">
logit{E(Ri)} = O0+αi+x1O1+x2O2+x1x2O3,
</equation>
<bodyText confidence="0.981727142857143">
where the relevance Ri ∼ Binomial. A factor is
essentially a learned intercept for different subsets
of the response. In this case,
OBOTH if both representations matched qi
OIBM only di,IBM matched qi
OBBN only di,BBN matched qi
where αi corresponds to data row i, with associ-
ated document representations di,source and query
qi. The intuition is simply that we’d like our model
to have different biases for or against relevance
based on which transcript source retrieved the doc-
ument. This is a small-dimensional way of damp-
ening the effects of significant disagreements in the
document representations.
</bodyText>
<subsectionHeader confidence="0.851288">
3.1.2 GAM with multidimensional smooth
</subsectionHeader>
<bodyText confidence="0.996279411764706">
If a document’s score is large in both systems, we
expect it to have high probability of relevance. How-
ever, as a document’s score increases linearly in one
source, we have no reason to expect its probability
of relevance to also increase linearly. Moreover, be-
cause the most discriminative terms are likely to be
found in only one transcript source, even an absent
score for a document does not ensure a document
is not relevant. It is clear then that the mapping
from document scores to probability of relevance is
in general a complex nonlinear surface. The limited
degree of nonlinear structure afforded to GLMs by
non-identity link functions is unlikely to sufficiently
capture this intuition.
Instead, we can model this non-linearity using a
generalized additive model with multidimensional
smooth f(xIBM, xBBN), so that
</bodyText>
<equation confidence="0.998851">
logit{E(Ri)} = O0 + f(xIBM, xBBN)-
</equation>
<bodyText confidence="0.999392631578947">
Again, Ri ∼ Binomial and O0 is a learned inter-
cept (which, alternatively, may be absorbed by the
smooth f).
Figure 2 shows the smoothing transformation f
learned during our evaluation. Note the small de-
crease in predicted probability of relevance as the
retrieval score from one system decreases, while the
probability curves upward again as the disagreement
increases. This captures our intuition that systems
often disagree strongly because discriminative terms
are often not recognized in all transcript sources.
We can think of the probability of relevance map-
ping learned by the factor model of Section 3.1.1 as
also being a surface defined over the space of input
document scores. That model, however, was con-
strained to be linear. It may be visualized as a col-
lection of affine planes (with common normal vec-
tors, but each shifted upwards by their factor level’s
weight and the common intercept).
</bodyText>
<equation confidence="0.976856">
αi = {
</equation>
<page confidence="0.996073">
464
</page>
<sectionHeader confidence="0.683658" genericHeader="method">
4 Experiments
4.1 Dataset
</sectionHeader>
<bodyText confidence="0.957205833333333">
Our dataset is a collection of 272 oral history inter-
views from the MALACH collection. The task is
to retrieve short speech segments which were man-
ually designated as being topically coherent by pro-
fessional indexers. There are 8,104 such segments
(corresponding to roughly 589 hours of conversa-
tional speech) and 96 assessed topics. We follow the
topic partition used for the 2007 evaluation by the
Cross Language Evaluation Forum’s cross-language
speech retrieval track (Pecina et al., 2007). This
gives us 63 topics on which to train our combination
systems and 33 topics for evaluation.
</bodyText>
<subsectionHeader confidence="0.375622">
4.2 Evaluation
</subsectionHeader>
<subsubsectionHeader confidence="0.762261">
4.2.1 Geometric Mean Average Precision
</subsubsectionHeader>
<bodyText confidence="0.911638210526316">
Average precision (AP) is the average of the pre-
cision values obtained after each document relevant
to a particular query is retrieved. To assess the
effectiveness of a system across multiple queries,
a commonly used measure is mean average preci-
sion (MAP). Mean average precision is defined as
the arithmetic mean of per-topic average precision,
�
MAP = 1 n APn. A consequence of the arith-
n
metic mean is that, if a system improvement dou-
bles AP for one topic from 0.02 to 0.04, while si-
multaneously decreasing AP on another from 0.4 to
0.38, the MAP will be unchanged. If we prefer to
highlight performance differences on the lowest per-
forming topics, a widely used alternative is the geo-
metric mean of average precision (GMAP), first in-
troduced in the TREC 2004 robust track (Voorhees,
2006).
</bodyText>
<equation confidence="0.430413444444444">
�HGMAP = � APn
n
computed as an arithmetic mean
of logs,
1 � log APn.
GMAP = exp
n
n
es
</equation>
<bodyText confidence="0.984511291666667">
465 using the Wilcoxon signed-rank test. This test, while
not requiring a particular distribution on the mea-
surements, does assume that they belong to an in-
terval scale. Similarly, the arithmetic mean of MAP
assumes AP has interval scale. As Robertson (2006)
has pointed out, it is in no sense clear that AP
(prior to any transformation) satisfies this assump-
tion. This becomes an argument for GMAP, since it
may also be defined using an arithmetic mean of log-
transformed average precisions. That is to say, the
logarithm is simply one possible monotonic trans-
formation which is arguably as good as any other,
including the identify transform, in terms of whether
the transformed value satisfies the interval assump-
tion. This log transform (and hence GMAP) is use-
We apply the same reasoning to test for statistical
itberg &amp;
with AP = 0, we follow the Robust Track conven-
R¨ohmel
ed
on GMAP.
ful simply because it highlights improvements on
the most difficult queries.
retrieval score for query
</bodyText>
<equation confidence="0.645988">
D’s
Q as
</equation>
<bodyText confidence="0.999317666666667">
tion and add E = 0.00001. The authors are not aware
of significance tests having been previously report
where the inverse document fr
</bodyText>
<equation confidence="0.826379">
equency (idf) is de-
</equation>
<bodyText confidence="0.917747">
fined as
Robertson (2006) presents a justification and analy-
sis of GMAP and notes that it may alternatively be
</bodyText>
<subsubsectionHeader confidence="0.860939">
4.2.2 Significance Testing for GMAP
</subsubsectionHeader>
<bodyText confidence="0.996949">
A standard way of measuring the significance of
system improvements in MAP is to compare aver-
age precision (AP) on each of the evaluation queri
</bodyText>
<subsectionHeader confidence="0.995327">
4.3 Retrieval System
</subsectionHeader>
<bodyText confidence="0.969227416666667">
We use Okapi BM25 (Robertson et al., 1996) as
our basic retrieval system, which defines a document
n
i=1
N is the size of the collection, n(qi) is the docu-
ment frequency for term qi,
is the frequency of
term qi in query Q,
D) is the term frequency
of query term qi in document D,
is the length
of the matching document, an
</bodyText>
<equation confidence="0.846972375">
n(qi) + 0.5 ,
qfi
f(qi,
|D|
d avgdl is the average
length of a document in the collection. We set the
N − n(qi) + 0.5
idf(qi) = log
</equation>
<bodyText confidence="0.998959333333333">
significance in GMAP improvements. That is, we
test for significant improvements in GMAP by ap-
plying the Wilcoxon signed rank test to the paired,
</bodyText>
<equation confidence="0.9973955">
(k3+1)qf�
k3+qf�)f(qi,D)(k1 + 1)
</equation>
<bodyText confidence="0.983490333333333">
transformed average precisions, log AP. We handle
tied pairs and compute exact p-values using the Stre-
Shift-Algorithm (1990). For topics
</bodyText>
<equation confidence="0.99048125">
idf(qi) ,
f(qi, D) + k1(1 − b + b |D|
avgdl)
s(D, Q) =
</equation>
<figureCaption confidence="0.999253">
Figure 2: The two dimensional smooth f(sIBM, sBBN)
learned to predict relevance given input scores from IBM
and BBN transcripts.
</figureCaption>
<bodyText confidence="0.998119">
parameters to k1 = 1, k3 = 1, b = .5, which gave
good results on a single transcript.
</bodyText>
<subsectionHeader confidence="0.996067">
4.4 Speech Recognition Transcripts
</subsectionHeader>
<bodyText confidence="0.9999899375">
Our first set of speech recognition transcripts was
produced by IBM for the MALACH project, and
used for several years in the CLEF cross-language
speech retrieval (CL-SR) track (Pecina et al., 2007).
The IBM recognizer was built using a manually
produced pronunciation dictionary and 200 hours
of transcribed audio. The resulting interview tran-
scripts have a reported mean word error rate (WER)
of approximately 25% on held out data, which was
obtained by priming the language model with meta-
data available from pre-interview questionnaires.
This represents significant improvements over IBM
transcripts used in earlier CL-SR evaluations, which
had a best reported WER of 39.6% (Byrne et al.,
2004). This system is reported to have run at ap-
proximately 10 times real time.
</bodyText>
<subsectionHeader confidence="0.536588">
4.4.1 New Transcripts for MALACH
</subsectionHeader>
<bodyText confidence="0.99998888">
We were graciously permitted to use BBN Tech-
nology’s speech recognition system to produce a
second set of ASR transcripts for our experiments
(Prasad et al., 2005; Matsoukas et al., 2005). We se-
lected the one side of the audio having largest RMS
amplitude for training and decoding. This channel
was down-sampled to 8kHz and segmented using an
available broadcast news segmenter. Because we did
not have a pronunciation dictionary which covered
the transcribed audio, we automatically generated
pronunciations for roughly 14k words using a rule-
based transliterator and the CMU lexicon. Using
the same 200 hours of transcribed audio, we trained
acoustic models as described in (Prasad et al., 2005).
We use a mixture of the training transcripts and var-
ious newswire sources for our language model train-
ing. We did not attempt to prime the language model
for particular interviewees or otherwise utilize any
interview metadata. For decoding, we ran a fast (ap-
proximately 1 times real time) system, as described
in (Matsoukas et al., 2005). Unfortunately, as we do
not have the same development set used by IBM, a
direct comparison of WER is not possible. Testing
on a small held out set of 4.3 hours, we observed our
system had a WER of 32.4%.
</bodyText>
<subsectionHeader confidence="0.907531">
4.5 Combination Methods
</subsectionHeader>
<bodyText confidence="0.999962714285714">
For baseline comparisons, we ran our evaluation on
each of the two transcript sources (IBM and our new
transcripts), the linear combination chosen to opti-
mize MAP (LC-MAP), the linear combination cho-
sen to optimize GMAP (LC-GMAP), interleaving
(IL), and CombMNZ. We denote our additive fac-
tor model as Factor GLM, and our multidimensional
smooth GAM model as MD-GAM.
Linear combination parameters were chosen to
optimize performance on the training set, sweeping
the weight for each source at intervals of 0.01. For
the generalized additive models, we maximized the
penalized likelihood of the training examples under
our model, as described in Section 3.
</bodyText>
<sectionHeader confidence="0.999821" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.997158666666667">
Table 1 shows our complete set of results. This
includes baseline scores from our new set of
transcripts, each of our baseline combination ap-
proaches, and results from our proposed combina-
tion models. Although we are chiefly interested in
improvements on difficult topics (i.e., GMAP), we
present MAP for comparison. Results in bold in-
dicate the largest mean value of the measure (ei-
ther AP or log AP), while daggers (†) indicate the
</bodyText>
<page confidence="0.9987">
466
</page>
<table confidence="0.993980153846154">
Type Model MAP GMAP
T IBM 0.0531 (-.2) 0.0134 (-11.8)
- BBN 0.0532 0.0152
- LC-MAP 0.0564 (+6.0) 0.0158 (+3.9)
- LC-GMAP 0.0587 (+10.3) 0.0154 (+1.3)
- IL 0.0592 (+11.3) 0.0165 (+8.6)
- CombMNZ 0.0550 (+3.4) 0.0150 (-1.3)
- Factor GLM 0.0611 (+14.9)† 0.0161 (+5.9)
- MD-GAM 0.0561 (+5.5)† 0.0180 (+18.4)†
TD IBM 0.0415 (-15.1) 0.0173 (-9.9)
- BBN 0.0489 0.0192
- LC-MAP 0.0519 (+6.1)† 0.0201 (+4.7)†
- LC-GMAP 0.0531 (+8.6)† 0.0200 (+4.2)
- IL 0.0507 (+3.7) 0.0210 (+9.4)
- CombMNZ 0.0495 (+1.2)† 0.0196 (+2.1)
- Factor GLM 0.0526 (+7.6)† 0.0198 (+3.1)
- MD-GAM 0.0529 (+8.2)† 0.0223 (+16.2)†
zionism
impact guilt
previou
assembl
attitud
0.001 0.002 0.005 0.010 0.020 0.050 0.100 0.200
Term recall in IBM transcripts
0.001 0.002 0.005 0.010 0.020 0.050
Term recall in BBN transcripts
</table>
<tableCaption confidence="0.85786">
Table 1: MAP and GMAP for each combination ap-
proach, using the evaluation query set from the CLEF-
2007 CL-SR (MALACH) collection. Shown in paren-
</tableCaption>
<figureCaption confidence="0.955895090909091">
theses is the relative improvement in score over the best
single transcripts results (i.e., using our new set of tran-
scripts). The best (mean) score for each condition is in
bold.
Figure 3: The proportion of relevant documents returned
in IBM and BBN transcripts for discriminative title words
(title words occurring in less than .01 of the collection).
Point size is proportional to the improvement in average
precision using (1) the best linear combination chosen to
optimize GMAP (A) and (2) the combination using MD-
GAM (0).
</figureCaption>
<bodyText confidence="0.999905075">
combination is a statistically significant improve-
ment (α = 0.05) over our new transcript set (that
is, over the best single transcript result). Tests for
statistically significant improvements in GMAP are
computed using our paired log AP test, as discussed
in Section 4.2.2.
First, we note that the GAM model with multi-
dimensional smooth gives the largest GMAP im-
provement for both title and title-description runs.
Secondly, it is the only combination approach able
to produce statistically significant relative improve-
ments on both measures for both conditions. For
GMAP, our measure of interest, these improve-
ments are 18.4% and 16.2% respectively.
One surprising observation from Table 1 is that
the mean improvement in log AP for interleaving is
fairly large and yet not statistically significant (it is
in fact a larger mean improvement than several other
baseline combination approaches which are signifi-
cant improvements. This may suggest that interleav-
ing suffers from a large disparity between its best
and worst performance on the query set.
Figure 3 examines whether our improvements
come systematically from only one of the transcript
sources. It shows the proportion of relevant docu-
ments in each transcript source containing the most
discriminative title words (words occurring in less
than .01 of the collection). Each point represents
one term for one topic. The size of the point is pro-
portional to the difference in AP observed on that
topic by using MD-GAM and by using LC-GMAP.
If the difference is positive (MD-GAM wins), we
plot 0, otherwise A. First, we observe that, when
it wins, MD-GAM tends to increase AP much more
than when LC-GMAP wins. While there are many
wins also for LC-GMAP, the effects of the larger
MD-GAM improvements will dominate for many of
the most difficult queries. Secondly, there does not
appear to be any evidence that one transcript source
has much higher term-recall than the other.
</bodyText>
<subsectionHeader confidence="0.99151">
5.1 Oracle linear combination
</subsectionHeader>
<bodyText confidence="0.999206333333333">
A chief advantage of our MD-GAM combination
model is that it is able to map input scores non-
linearly onto a probability of document relevance.
</bodyText>
<page confidence="0.997254">
467
</page>
<table confidence="0.9997954">
Type Model GMAP
T Oracle-LC-GMAP 0.0168
- MD-GAM 0.0180 (+7.1)
TD Oracle-LC-GMAP 0.0222
- MD-GAM 0.0223 (+0.5)
</table>
<tableCaption confidence="0.925855666666667">
Table 2: GMAP results for an oracle experiment in
which MD-GAM was fairly trained and LC-GMAP was
unfairly optimized on the test queries.
</tableCaption>
<bodyText confidence="0.999926545454545">
To make an assessment of how much this capabil-
ity helps the system, we performed an oracle exper-
iment where we again constrained MD-GAM to be
fairly trained but allowed LC-GMAP to cheat and
choose the combination optimizing GMAP on the
test data. Table 2 lists the results. While the im-
provement with MD-GAM is now not statistically
significant (primarily because of our small query
set), we found it still out-performed the oracle linear
combination. For title-only queries, this improve-
ment was surprisingly large at 7.1% relative.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999902782608695">
While speech retrieval is one example of retrieval
under errorful document representations, other sim-
ilar tasks may also benefit from these combination
models. This includes the task of cross-language re-
trieval, as well as the retrieval of documents obtained
by optical character recognition.
Within speech retrieval, further work also remains
to be done. For example, various other features are
likely to be useful in predicting optimal system com-
bination. These might include, for example, confi-
dence scores, acoustic confusability, or other strong
cues that one recognition system is unlikely to have
properly recognized a query term. We look forward
to investigating these possibilities in future work.
The question of how much a system should ex-
pose its internal workings (e.g., its document rep-
resentations) to external systems is a long standing
problem in meta-search. We’ve taken the rather nar-
row view that systems might only expose the list of
scores they assigned to retrieved documents, a plau-
sible scenario considering the many systems now
emerging which are effectively doing this already.
Some examples include EveryZing,1 the MIT Lec-
</bodyText>
<footnote confidence="0.944015">
1http://www.everyzing.com/
</footnote>
<bodyText confidence="0.999643846153846">
ture Browser,2 and Comcast’s video search.3 This
trend is likely to continue as the underlying repre-
sentations of the content are themselves becoming
increasingly complex (e.g., word and subword level
lattices or confusion networks). The cost of expos-
ing such a vast quantity of such complex data rapidly
becomes difficult to justify.
But if the various representations of the con-
tent are available, there are almost certainly other
combination approaches worth investigating. Some
possible approaches include simple linear combi-
nations of the putative term frequencies, combina-
tions of one best transcript hypotheses (e.g., us-
ing ROVER (Fiscus, 1997)), or methods exploiting
word-lattice information (Evermann and Woodland,
2000).
Our planet’s 6.6 billion people speak many more
words every day than even the largest Web search
engines presently index. While much of this is
surely not worth hearing again (or even once!), some
of it is surely precious beyond measure. Separating
the wheat from the chaff in this cacophony is the rai-
son d’etre for information retrieval, and it is hard to
conceive of an information retrieval challenge with
greater scope or greater potential to impact our soci-
ety than improving our access to the spoken word.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999948666666667">
The authors are grateful to BBN Technologies, who
generously provided access to their speech recogni-
tion system for this research.
</bodyText>
<sectionHeader confidence="0.998488" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998332384615385">
Brian T. Bartell, Garrison W. Cottrell, and Richard K.
Belew. 1994. Automatic combination of multi-
ple ranked retrieval systems. In Proceedings of the
17th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 173–181.
Steven M. Beitzel, Eric C. Jensen, Abdur Chowdhury,
David Grossman, Ophir Frieder, and Nazli Goharian.
2004. Fusion of effective retrieval strategies in the
same information retrieval system. J. Am. Soc. Inf. Sci.
Technol., 55(10):859–868.
W. Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajic,
D.W. Oard, M. Picheny, J. Psutka, B. Ramabhadran,
</reference>
<footnote confidence="0.9949815">
2http://web.sls.csail.mit.edu/lectures/
3http://videosearch.comcast.net
</footnote>
<page confidence="0.997056">
468
</page>
<reference confidence="0.999795635416667">
D. Soergel, T. Ward, and Wei-Jing Zhu. 2004. Au-
tomatic recognition of spontaneous speech for access
to multilingual oral history archives. IEEE Transac-
tions on Speech and Audio Processing, Special Issue
on Spontaneous Speech Processing, 12(4):420–435,
July.
J. P. Callan, Z. Lu, and W. Bruce Croft. 1995. Search-
ing Distributed Collections with Inference Networks .
In E. A. Fox, P. Ingwersen, and R. Fidel, editors, Pro-
ceedings of the 18th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 21–28, Seattle, Washington.
ACM Press.
G. Evermann and P.C. Woodland. 2000. Posterior prob-
ability decoding, confidence estimation and system
combination. In Proceedings of the Speech Transcrip-
tion Workshop, May.
Jonathan G. Fiscus. 1997. A Post-Processing System to
Yield Reduced Word Error Rates: Recogniser Output
Voting Error Reduction (ROVER). In Proceedings of
the IEEE ASRU Workshop, pages 347–352.
Jong-Hak Lee. 1997. Analyses of multiple evidence
combination. In SIGIR Forum, pages 267–276.
David Lillis, Fergus Toolan, Rem Collier, and John Dun-
nion. 2006. Probfuse: a probabilistic approach to data
fusion. In SIGIR ’06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 139–146,
New York, NY, USA. ACM.
R. Manmatha, T. Rath, and F. Feng. 2001. Modeling
score distributions for combining the outputs of search
engines. In SIGIR ’01: Proceedings of the 24th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 267–275,
New York, NY, USA. ACM.
Spyros Matsoukas, Rohit Prasad, Srinivas Laxminarayan,
Bing Xiang, Long Nguyen, and Richard Schwartz.
2005. The 2004 BBN 1xRT Recognition Systems
for English Broadcast News and Conversational Tele-
phone Speech. In Interspeech 2005, pages 1641–1644.
Mark Montague and Javed A. Aslam. 2002. Condorcet
fusion for improved retrieval. In CIKM ’02: Proceed-
ings of the eleventh international conference on Infor-
mation and knowledge management, pages 538–548,
New York, NY, USA. ACM.
Pavel Pecina, Petra Hoffmannova, Gareth J.F. Jones, Jian-
qiang Wang, and Douglas W. Oard. 2007. Overview
of the CLEF-2007 Cross-Language Speech Retrieval
Track. In Proceedings of the CLEF 2007 Workshop
on Cross-Language Information Retrieval and Evalu-
ation, September.
Allison L. Powell, James C. French, James P. Callan,
Margaret E. Connell, and Charles L. Viles. 2000.
The impact of database selection on distributed search-
ing. In Research and Development in Information Re-
trieval, pages 232–239.
R. Prasad, S. Matsoukas, C.L. Kao, J. Ma, D.X. Xu,
T. Colthurst, O. Kimball, R. Schwartz, J.L. Gauvain,
L. Lamel, H. Schwenk, G. Adda, and F. Lefevre.
2005. The 2004 BBN/LIMSI 20xRT English Conver-
sational Telephone Speech Recognition System. In In-
terspeech 2005.
S. Robertson, S. Walker, S. Jones, and M. Hancock-
Beaulieu M. Gatford. 1996. Okapi at TREC-3. In
Text REtrieval Conference, pages 21–30.
Stephen Robertson. 2006. On GMAP: and other trans-
formations. In CIKM ’06: Proceedings of the 15th
ACM international conference on Information and
knowledge management, pages 78–83, New York, NY,
USA. ACM.
J. Savoy, A. Le Calv´e, and D. Vrajitoru. 1988. Report on
the TREC-5 experiment: Data fusion and collection
fusion.
Joseph A. Shaw and Edward A. Fox. 1994. Combination
of multiple searches. In Proceedings of the 2nd Text
REtrieval Conference (TREC-2).
Bernd Streitberg and Joachim R¨ohmel. 1990. On tests
that are uniformly more powerful than the Wilcoxon-
Mann-Whitney test. Biometrics, 46(2):481–484.
Christopher C. Vogt and Garrison W. Cottrell. 1999. Fu-
sion via a linear combination of scores. Information
Retrieval, 1(3):151–173.
Ellen M. Voorhees, Narendra Kumar Gupta, and Ben
Johnson-Laird. 1994. The collection fusion problem.
In D. K. Harman, editor, The Third Text REtrieval Con-
ference (TREC-3), pages 500–225. National Institute
of Standards and Technology.
Ellen M. Voorhees. 2006. Overview of the TREC 2005
robust retrieval track. In Ellem M. Voorhees and L.P.
Buckland, editors, The Fourteenth Text REtrieval Con-
ference, (TREC 2005), Gaithersburg, MD: NIST.
Simon N. Wood. 2003. Thin plate regression splines.
Journal Of The Royal Statistical Society Series B,
65(1):95–114.
Simon Wood. 2006. Generalized Additive Models: An
Introduction with R. Chapman and Hall/CRC.
</reference>
<page confidence="0.999626">
469
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.534971">
<title confidence="0.998887">Combining Speech Retrieval Results with Generalized Additive Models</title>
<author confidence="0.991549">Scott</author>
<author confidence="0.991549">W Douglas</author>
<affiliation confidence="0.785049">UMIACS Laboratory for Computational Linguistics and Information Processing University of Maryland, College Park, MD 20742 Human Language Technology Center of Excellence</affiliation>
<address confidence="0.931462">John Hopkins University, Baltimore, MD 21211</address>
<abstract confidence="0.999380913043478">Rapid and inexpensive techniques for automatic transcription of speech have the potential to dramatically expand the types of content to which information retrieval techniques can be productively applied, but limitations in accuracy and robustness must be overcome before that promise can be fully realized. Combining retrieval results from systems built on various errorful representations of the same collection offers some potential to address these challenges. This paper explores that potential by applying Generalized Additive Models to optimize the combination of ranked retrieval results obtained using transcripts produced automatically for the same spoken content by substantially different recognition systems. Topic-averaged retrieval effectiveness better than any previously reported for the same collection was obtained, and even larger gains are apparent when using an alternative measure emphasizing results on the most difficult topics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brian T Bartell</author>
<author>Garrison W Cottrell</author>
<author>Richard K Belew</author>
</authors>
<title>Automatic combination of multiple ranked retrieval systems.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>173--181</pages>
<contexts>
<context position="4313" citStr="Bartell et al., 1994" startWordPosition="650" endWordPosition="653">ion 3. Section 4 describes the design of our experiments with a 589 hour collection of conversational speech for which information retrieval queries and relevance judgments are available. Section 5 presents the results of our experiments, and we conclude in Section 6 with a brief discussion of implications of our results and the potential for future work on this important problem. 2 Previous Work One approach for combining ranked retrieval results is to simply linearly combine the multiple system scores for each topic and document. This approach has been extensively applied in the literature (Bartell et al., 1994; Callan et al., 1995; Powell et al., 2000; Vogt and Cottrell, 1999), with varying degrees of success, owing in part to the potential difficulty of normalizing scores across retrieval systems. In this study, we partially abstract away from this potential difficulty by using the same retrieval system on both representations of the collection documents (so that we don’t expect score distributions to be significantly different for the combination inputs). Of course, many fusion techniques using more advanced score normalization methods have been proposed. Shaw and Fox (1994) proposed a number of </context>
</contexts>
<marker>Bartell, Cottrell, Belew, 1994</marker>
<rawString>Brian T. Bartell, Garrison W. Cottrell, and Richard K. Belew. 1994. Automatic combination of multiple ranked retrieval systems. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 173–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven M Beitzel</author>
<author>Eric C Jensen</author>
<author>Abdur Chowdhury</author>
<author>David Grossman</author>
<author>Ophir Frieder</author>
<author>Nazli Goharian</author>
</authors>
<title>Fusion of effective retrieval strategies in the same information retrieval system.</title>
<date>2004</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>55</volume>
<issue>10</issue>
<contexts>
<context position="5144" citStr="Beitzel et al., 2004" startWordPosition="784" endWordPosition="787">partially abstract away from this potential difficulty by using the same retrieval system on both representations of the collection documents (so that we don’t expect score distributions to be significantly different for the combination inputs). Of course, many fusion techniques using more advanced score normalization methods have been proposed. Shaw and Fox (1994) proposed a number of such techniques, perhaps the most successful of which is known as CombMNZ. CombMNZ has been shown to achieve strong performance and has been used in many subsequent studies (Lee, 1997; Montague and Aslam, 2002; Beitzel et al., 2004; Lillis et al., 2006). In this study, we also use CombMNZ as a baseline for comparison, and following Lillis et al. (2006) and Lee (1997), compute it in the following way. First, we normalize each score si as norm(si) = si−min(s) where max(s) and max(s)−min(s) min(s) are the maximum and minimum scores seen in the input result list. After normalization, the CombMNZ score for a document d is computed as G CombMNZd = Ns,d x |Nd &gt; 0|. Here, L is the number of ranked lists to be combined, Nt,d is the normalized score of document d in ranked list E, and |Nd &gt; 0 |is the number of nonzero normalized </context>
<context position="7264" citStr="Beitzel et al., 2004" startWordPosition="1134" endWordPosition="1137">models which GAMs encompass. Unlike GAMs in their full generality however, logistic regression imposes a comparatively high degree of linearity in the model structure. 2.1 Combining speech retrieval results Previous work on single-collection result fusion has naturally focused on combining results from multiple retrieval systems. In this case, the potential for performance improvements depends critically on the uniqueness of the different input systems being combined. Accordingly, small variations in the same system often do not combine to produce results better than the best of their inputs (Beitzel et al., 2004). Errorful document collections such as conversational speech introduce new difficulties and opportunities for data fusion. This is so, in particular, because even the same system can produce drastically different retrieval results when multiple representations of the documents (e.g., multiple transcript hypotheses) are available. Consider, for example, Figure 1 which shows, for each term in each of our title queries, the proportion of relevant documents containing that term in only one of our two transcript hypotheses. Critically, by plotting this proportion against the term’s inverse documen</context>
</contexts>
<marker>Beitzel, Jensen, Chowdhury, Grossman, Frieder, Goharian, 2004</marker>
<rawString>Steven M. Beitzel, Eric C. Jensen, Abdur Chowdhury, David Grossman, Ophir Frieder, and Nazli Goharian. 2004. Fusion of effective retrieval strategies in the same information retrieval system. J. Am. Soc. Inf. Sci. Technol., 55(10):859–868.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Byrne</author>
<author>D Doermann</author>
<author>M Franz</author>
<author>S Gustman</author>
<author>J Hajic</author>
<author>D W Oard</author>
<author>M Picheny</author>
<author>J Psutka</author>
<author>B Ramabhadran</author>
<author>D Soergel</author>
<author>T Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Automatic recognition of spontaneous speech for access to multilingual oral history archives.</title>
<date>2004</date>
<booktitle>IEEE Transactions on Speech and Audio Processing, Special Issue on Spontaneous Speech Processing,</booktitle>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="19974" citStr="Byrne et al., 2004" startWordPosition="3253" endWordPosition="3256">ACH project, and used for several years in the CLEF cross-language speech retrieval (CL-SR) track (Pecina et al., 2007). The IBM recognizer was built using a manually produced pronunciation dictionary and 200 hours of transcribed audio. The resulting interview transcripts have a reported mean word error rate (WER) of approximately 25% on held out data, which was obtained by priming the language model with metadata available from pre-interview questionnaires. This represents significant improvements over IBM transcripts used in earlier CL-SR evaluations, which had a best reported WER of 39.6% (Byrne et al., 2004). This system is reported to have run at approximately 10 times real time. 4.4.1 New Transcripts for MALACH We were graciously permitted to use BBN Technology’s speech recognition system to produce a second set of ASR transcripts for our experiments (Prasad et al., 2005; Matsoukas et al., 2005). We selected the one side of the audio having largest RMS amplitude for training and decoding. This channel was down-sampled to 8kHz and segmented using an available broadcast news segmenter. Because we did not have a pronunciation dictionary which covered the transcribed audio, we automatically generat</context>
</contexts>
<marker>Byrne, Doermann, Franz, Gustman, Hajic, Oard, Picheny, Psutka, Ramabhadran, Soergel, Ward, Zhu, 2004</marker>
<rawString>W. Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajic, D.W. Oard, M. Picheny, J. Psutka, B. Ramabhadran, D. Soergel, T. Ward, and Wei-Jing Zhu. 2004. Automatic recognition of spontaneous speech for access to multilingual oral history archives. IEEE Transactions on Speech and Audio Processing, Special Issue on Spontaneous Speech Processing, 12(4):420–435, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Callan</author>
<author>Z Lu</author>
<author>W Bruce Croft</author>
</authors>
<title>Searching Distributed Collections with Inference Networks .</title>
<date>1995</date>
<booktitle>Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>21--28</pages>
<editor>In E. A. Fox, P. Ingwersen, and R. Fidel, editors,</editor>
<location>Seattle, Washington.</location>
<contexts>
<context position="4334" citStr="Callan et al., 1995" startWordPosition="654" endWordPosition="657">ibes the design of our experiments with a 589 hour collection of conversational speech for which information retrieval queries and relevance judgments are available. Section 5 presents the results of our experiments, and we conclude in Section 6 with a brief discussion of implications of our results and the potential for future work on this important problem. 2 Previous Work One approach for combining ranked retrieval results is to simply linearly combine the multiple system scores for each topic and document. This approach has been extensively applied in the literature (Bartell et al., 1994; Callan et al., 1995; Powell et al., 2000; Vogt and Cottrell, 1999), with varying degrees of success, owing in part to the potential difficulty of normalizing scores across retrieval systems. In this study, we partially abstract away from this potential difficulty by using the same retrieval system on both representations of the collection documents (so that we don’t expect score distributions to be significantly different for the combination inputs). Of course, many fusion techniques using more advanced score normalization methods have been proposed. Shaw and Fox (1994) proposed a number of such techniques, perh</context>
</contexts>
<marker>Callan, Lu, Croft, 1995</marker>
<rawString>J. P. Callan, Z. Lu, and W. Bruce Croft. 1995. Searching Distributed Collections with Inference Networks . In E. A. Fox, P. Ingwersen, and R. Fidel, editors, Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 21–28, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Evermann</author>
<author>P C Woodland</author>
</authors>
<title>Posterior probability decoding, confidence estimation and system combination.</title>
<date>2000</date>
<booktitle>In Proceedings of the Speech Transcription Workshop,</booktitle>
<contexts>
<context position="28722" citStr="Evermann and Woodland, 2000" startWordPosition="4660" endWordPosition="4663">ntations of the content are themselves becoming increasingly complex (e.g., word and subword level lattices or confusion networks). The cost of exposing such a vast quantity of such complex data rapidly becomes difficult to justify. But if the various representations of the content are available, there are almost certainly other combination approaches worth investigating. Some possible approaches include simple linear combinations of the putative term frequencies, combinations of one best transcript hypotheses (e.g., using ROVER (Fiscus, 1997)), or methods exploiting word-lattice information (Evermann and Woodland, 2000). Our planet’s 6.6 billion people speak many more words every day than even the largest Web search engines presently index. While much of this is surely not worth hearing again (or even once!), some of it is surely precious beyond measure. Separating the wheat from the chaff in this cacophony is the raison d’etre for information retrieval, and it is hard to conceive of an information retrieval challenge with greater scope or greater potential to impact our society than improving our access to the spoken word. Acknowledgements The authors are grateful to BBN Technologies, who generously provide</context>
</contexts>
<marker>Evermann, Woodland, 2000</marker>
<rawString>ACM Press. G. Evermann and P.C. Woodland. 2000. Posterior probability decoding, confidence estimation and system combination. In Proceedings of the Speech Transcription Workshop, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A Post-Processing System to Yield Reduced Word Error Rates: Recogniser Output Voting Error Reduction (ROVER).</title>
<date>1997</date>
<booktitle>In Proceedings of the IEEE ASRU Workshop,</booktitle>
<pages>347--352</pages>
<contexts>
<context position="28643" citStr="Fiscus, 1997" startWordPosition="4653" endWordPosition="4654">rch.3 This trend is likely to continue as the underlying representations of the content are themselves becoming increasingly complex (e.g., word and subword level lattices or confusion networks). The cost of exposing such a vast quantity of such complex data rapidly becomes difficult to justify. But if the various representations of the content are available, there are almost certainly other combination approaches worth investigating. Some possible approaches include simple linear combinations of the putative term frequencies, combinations of one best transcript hypotheses (e.g., using ROVER (Fiscus, 1997)), or methods exploiting word-lattice information (Evermann and Woodland, 2000). Our planet’s 6.6 billion people speak many more words every day than even the largest Web search engines presently index. While much of this is surely not worth hearing again (or even once!), some of it is surely precious beyond measure. Separating the wheat from the chaff in this cacophony is the raison d’etre for information retrieval, and it is hard to conceive of an information retrieval challenge with greater scope or greater potential to impact our society than improving our access to the spoken word. Acknow</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G. Fiscus. 1997. A Post-Processing System to Yield Reduced Word Error Rates: Recogniser Output Voting Error Reduction (ROVER). In Proceedings of the IEEE ASRU Workshop, pages 347–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hak Lee</author>
</authors>
<title>Analyses of multiple evidence combination.</title>
<date>1997</date>
<booktitle>In SIGIR Forum,</booktitle>
<pages>267--276</pages>
<contexts>
<context position="5096" citStr="Lee, 1997" startWordPosition="777" endWordPosition="778">retrieval systems. In this study, we partially abstract away from this potential difficulty by using the same retrieval system on both representations of the collection documents (so that we don’t expect score distributions to be significantly different for the combination inputs). Of course, many fusion techniques using more advanced score normalization methods have been proposed. Shaw and Fox (1994) proposed a number of such techniques, perhaps the most successful of which is known as CombMNZ. CombMNZ has been shown to achieve strong performance and has been used in many subsequent studies (Lee, 1997; Montague and Aslam, 2002; Beitzel et al., 2004; Lillis et al., 2006). In this study, we also use CombMNZ as a baseline for comparison, and following Lillis et al. (2006) and Lee (1997), compute it in the following way. First, we normalize each score si as norm(si) = si−min(s) where max(s) and max(s)−min(s) min(s) are the maximum and minimum scores seen in the input result list. After normalization, the CombMNZ score for a document d is computed as G CombMNZd = Ns,d x |Nd &gt; 0|. Here, L is the number of ranked lists to be combined, Nt,d is the normalized score of document d in ranked list E, a</context>
</contexts>
<marker>Lee, 1997</marker>
<rawString>Jong-Hak Lee. 1997. Analyses of multiple evidence combination. In SIGIR Forum, pages 267–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lillis</author>
<author>Fergus Toolan</author>
<author>Rem Collier</author>
<author>John Dunnion</author>
</authors>
<title>Probfuse: a probabilistic approach to data fusion.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="5166" citStr="Lillis et al., 2006" startWordPosition="788" endWordPosition="791">y from this potential difficulty by using the same retrieval system on both representations of the collection documents (so that we don’t expect score distributions to be significantly different for the combination inputs). Of course, many fusion techniques using more advanced score normalization methods have been proposed. Shaw and Fox (1994) proposed a number of such techniques, perhaps the most successful of which is known as CombMNZ. CombMNZ has been shown to achieve strong performance and has been used in many subsequent studies (Lee, 1997; Montague and Aslam, 2002; Beitzel et al., 2004; Lillis et al., 2006). In this study, we also use CombMNZ as a baseline for comparison, and following Lillis et al. (2006) and Lee (1997), compute it in the following way. First, we normalize each score si as norm(si) = si−min(s) where max(s) and max(s)−min(s) min(s) are the maximum and minimum scores seen in the input result list. After normalization, the CombMNZ score for a document d is computed as G CombMNZd = Ns,d x |Nd &gt; 0|. Here, L is the number of ranked lists to be combined, Nt,d is the normalized score of document d in ranked list E, and |Nd &gt; 0 |is the number of nonzero normalized scores given to d by a</context>
<context position="6473" citStr="Lillis et al. (2006)" startWordPosition="1010" endWordPosition="1013">d be modeled using a Normal distribution for relevant documents and exponential distribution for non-relevant documents. However, in their study, fusion results using these comparatively complex normalization approaches achieved performance no better than the much simpler CombMNZ. A simple rank-based fusion technique is interleaving (Voorhees et al., 1994). In this approach, the highest ranked document from each list is taken in turn (ignoring duplicates) and placed at the top of the new, combined list. Many probabilistic combination approaches have also been developed, a recent example being Lillis et al. (2006). Perhaps the most closely related proposal, using logistic regression, was made first by Savoy et al. (1988). Logistic regression is one example from the broad class of models which GAMs encompass. Unlike GAMs in their full generality however, logistic regression imposes a comparatively high degree of linearity in the model structure. 2.1 Combining speech retrieval results Previous work on single-collection result fusion has naturally focused on combining results from multiple retrieval systems. In this case, the potential for performance improvements depends critically on the uniqueness of t</context>
</contexts>
<marker>Lillis, Toolan, Collier, Dunnion, 2006</marker>
<rawString>David Lillis, Fergus Toolan, Rem Collier, and John Dunnion. 2006. Probfuse: a probabilistic approach to data fusion. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 139–146,</rawString>
</citation>
<citation valid="true">
<authors>
<author>ACM R Manmatha</author>
<author>T Rath</author>
<author>F Feng</author>
</authors>
<title>Modeling score distributions for combining the outputs of search engines.</title>
<date>2001</date>
<booktitle>In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>267--275</pages>
<contexts>
<context position="5803" citStr="Manmatha et al. (2001)" startWordPosition="910" endWordPosition="913">, we also use CombMNZ as a baseline for comparison, and following Lillis et al. (2006) and Lee (1997), compute it in the following way. First, we normalize each score si as norm(si) = si−min(s) where max(s) and max(s)−min(s) min(s) are the maximum and minimum scores seen in the input result list. After normalization, the CombMNZ score for a document d is computed as G CombMNZd = Ns,d x |Nd &gt; 0|. Here, L is the number of ranked lists to be combined, Nt,d is the normalized score of document d in ranked list E, and |Nd &gt; 0 |is the number of nonzero normalized scores given to d by any result set. Manmatha et al. (2001) showed that retrieval scores from IR systems could be modeled using a Normal distribution for relevant documents and exponential distribution for non-relevant documents. However, in their study, fusion results using these comparatively complex normalization approaches achieved performance no better than the much simpler CombMNZ. A simple rank-based fusion technique is interleaving (Voorhees et al., 1994). In this approach, the highest ranked document from each list is taken in turn (ignoring duplicates) and placed at the top of the new, combined list. Many probabilistic combination approaches</context>
</contexts>
<marker>Manmatha, Rath, Feng, 2001</marker>
<rawString>New York, NY, USA. ACM. R. Manmatha, T. Rath, and F. Feng. 2001. Modeling score distributions for combining the outputs of search engines. In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 267–275,</rawString>
</citation>
<citation valid="true">
<authors>
<author>ACM Spyros Matsoukas</author>
<author>Rohit Prasad</author>
<author>Srinivas Laxminarayan</author>
<author>Bing Xiang</author>
<author>Long Nguyen</author>
<author>Richard Schwartz</author>
</authors>
<date>2005</date>
<booktitle>The 2004 BBN 1xRT Recognition Systems for English Broadcast News and Conversational Telephone Speech. In Interspeech</booktitle>
<pages>1641--1644</pages>
<contexts>
<context position="20269" citStr="Matsoukas et al., 2005" startWordPosition="3303" endWordPosition="3306">an word error rate (WER) of approximately 25% on held out data, which was obtained by priming the language model with metadata available from pre-interview questionnaires. This represents significant improvements over IBM transcripts used in earlier CL-SR evaluations, which had a best reported WER of 39.6% (Byrne et al., 2004). This system is reported to have run at approximately 10 times real time. 4.4.1 New Transcripts for MALACH We were graciously permitted to use BBN Technology’s speech recognition system to produce a second set of ASR transcripts for our experiments (Prasad et al., 2005; Matsoukas et al., 2005). We selected the one side of the audio having largest RMS amplitude for training and decoding. This channel was down-sampled to 8kHz and segmented using an available broadcast news segmenter. Because we did not have a pronunciation dictionary which covered the transcribed audio, we automatically generated pronunciations for roughly 14k words using a rulebased transliterator and the CMU lexicon. Using the same 200 hours of transcribed audio, we trained acoustic models as described in (Prasad et al., 2005). We use a mixture of the training transcripts and various newswire sources for our langua</context>
</contexts>
<marker>Matsoukas, Prasad, Laxminarayan, Xiang, Nguyen, Schwartz, 2005</marker>
<rawString>New York, NY, USA. ACM. Spyros Matsoukas, Rohit Prasad, Srinivas Laxminarayan, Bing Xiang, Long Nguyen, and Richard Schwartz. 2005. The 2004 BBN 1xRT Recognition Systems for English Broadcast News and Conversational Telephone Speech. In Interspeech 2005, pages 1641–1644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Montague</author>
<author>Javed A Aslam</author>
</authors>
<title>Condorcet fusion for improved retrieval.</title>
<date>2002</date>
<booktitle>In CIKM ’02: Proceedings of the eleventh international conference on Information and knowledge management,</booktitle>
<pages>538--548</pages>
<contexts>
<context position="5122" citStr="Montague and Aslam, 2002" startWordPosition="779" endWordPosition="783">ystems. In this study, we partially abstract away from this potential difficulty by using the same retrieval system on both representations of the collection documents (so that we don’t expect score distributions to be significantly different for the combination inputs). Of course, many fusion techniques using more advanced score normalization methods have been proposed. Shaw and Fox (1994) proposed a number of such techniques, perhaps the most successful of which is known as CombMNZ. CombMNZ has been shown to achieve strong performance and has been used in many subsequent studies (Lee, 1997; Montague and Aslam, 2002; Beitzel et al., 2004; Lillis et al., 2006). In this study, we also use CombMNZ as a baseline for comparison, and following Lillis et al. (2006) and Lee (1997), compute it in the following way. First, we normalize each score si as norm(si) = si−min(s) where max(s) and max(s)−min(s) min(s) are the maximum and minimum scores seen in the input result list. After normalization, the CombMNZ score for a document d is computed as G CombMNZd = Ns,d x |Nd &gt; 0|. Here, L is the number of ranked lists to be combined, Nt,d is the normalized score of document d in ranked list E, and |Nd &gt; 0 |is the number </context>
</contexts>
<marker>Montague, Aslam, 2002</marker>
<rawString>Mark Montague and Javed A. Aslam. 2002. Condorcet fusion for improved retrieval. In CIKM ’02: Proceedings of the eleventh international conference on Information and knowledge management, pages 538–548,</rawString>
</citation>
<citation valid="true">
<authors>
<author>ACM Pavel Pecina</author>
<author>Petra Hoffmannova</author>
<author>Gareth J F Jones</author>
<author>Jianqiang Wang</author>
<author>Douglas W Oard</author>
</authors>
<title>Overview of the CLEF-2007 Cross-Language Speech Retrieval Track.</title>
<date>2007</date>
<booktitle>In Proceedings of the CLEF 2007 Workshop on Cross-Language Information Retrieval and Evaluation,</booktitle>
<contexts>
<context position="15697" citStr="Pecina et al., 2007" startWordPosition="2512" endWordPosition="2515">ctors, but each shifted upwards by their factor level’s weight and the common intercept). αi = { 464 4 Experiments 4.1 Dataset Our dataset is a collection of 272 oral history interviews from the MALACH collection. The task is to retrieve short speech segments which were manually designated as being topically coherent by professional indexers. There are 8,104 such segments (corresponding to roughly 589 hours of conversational speech) and 96 assessed topics. We follow the topic partition used for the 2007 evaluation by the Cross Language Evaluation Forum’s cross-language speech retrieval track (Pecina et al., 2007). This gives us 63 topics on which to train our combination systems and 33 topics for evaluation. 4.2 Evaluation 4.2.1 Geometric Mean Average Precision Average precision (AP) is the average of the precision values obtained after each document relevant to a particular query is retrieved. To assess the effectiveness of a system across multiple queries, a commonly used measure is mean average precision (MAP). Mean average precision is defined as the arithmetic mean of per-topic average precision, � MAP = 1 n APn. A consequence of the arithn metic mean is that, if a system improvement doubles AP f</context>
<context position="19474" citStr="Pecina et al., 2007" startWordPosition="3177" endWordPosition="3180">precisions, log AP. We handle tied pairs and compute exact p-values using the StreShift-Algorithm (1990). For topics idf(qi) , f(qi, D) + k1(1 − b + b |D| avgdl) s(D, Q) = Figure 2: The two dimensional smooth f(sIBM, sBBN) learned to predict relevance given input scores from IBM and BBN transcripts. parameters to k1 = 1, k3 = 1, b = .5, which gave good results on a single transcript. 4.4 Speech Recognition Transcripts Our first set of speech recognition transcripts was produced by IBM for the MALACH project, and used for several years in the CLEF cross-language speech retrieval (CL-SR) track (Pecina et al., 2007). The IBM recognizer was built using a manually produced pronunciation dictionary and 200 hours of transcribed audio. The resulting interview transcripts have a reported mean word error rate (WER) of approximately 25% on held out data, which was obtained by priming the language model with metadata available from pre-interview questionnaires. This represents significant improvements over IBM transcripts used in earlier CL-SR evaluations, which had a best reported WER of 39.6% (Byrne et al., 2004). This system is reported to have run at approximately 10 times real time. 4.4.1 New Transcripts for</context>
</contexts>
<marker>Pecina, Hoffmannova, Jones, Wang, Oard, 2007</marker>
<rawString>New York, NY, USA. ACM. Pavel Pecina, Petra Hoffmannova, Gareth J.F. Jones, Jianqiang Wang, and Douglas W. Oard. 2007. Overview of the CLEF-2007 Cross-Language Speech Retrieval Track. In Proceedings of the CLEF 2007 Workshop on Cross-Language Information Retrieval and Evaluation, September.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Allison L Powell</author>
<author>James C French</author>
<author>James P Callan</author>
</authors>
<marker>Powell, French, Callan, </marker>
<rawString>Allison L. Powell, James C. French, James P. Callan,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret E Connell</author>
<author>Charles L Viles</author>
</authors>
<title>The impact of database selection on distributed searching.</title>
<date>2000</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>232--239</pages>
<marker>Connell, Viles, 2000</marker>
<rawString>Margaret E. Connell, and Charles L. Viles. 2000. The impact of database selection on distributed searching. In Research and Development in Information Retrieval, pages 232–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>S Matsoukas</author>
<author>C L Kao</author>
<author>J Ma</author>
<author>D X Xu</author>
<author>T Colthurst</author>
<author>O Kimball</author>
<author>R Schwartz</author>
<author>J L Gauvain</author>
<author>L Lamel</author>
<author>H Schwenk</author>
<author>G Adda</author>
<author>F Lefevre</author>
</authors>
<title>BBN/LIMSI 20xRT English Conversational Telephone Speech Recognition System. In Interspeech</title>
<date>2005</date>
<booktitle>The</booktitle>
<contexts>
<context position="20244" citStr="Prasad et al., 2005" startWordPosition="3299" endWordPosition="3302">ts have a reported mean word error rate (WER) of approximately 25% on held out data, which was obtained by priming the language model with metadata available from pre-interview questionnaires. This represents significant improvements over IBM transcripts used in earlier CL-SR evaluations, which had a best reported WER of 39.6% (Byrne et al., 2004). This system is reported to have run at approximately 10 times real time. 4.4.1 New Transcripts for MALACH We were graciously permitted to use BBN Technology’s speech recognition system to produce a second set of ASR transcripts for our experiments (Prasad et al., 2005; Matsoukas et al., 2005). We selected the one side of the audio having largest RMS amplitude for training and decoding. This channel was down-sampled to 8kHz and segmented using an available broadcast news segmenter. Because we did not have a pronunciation dictionary which covered the transcribed audio, we automatically generated pronunciations for roughly 14k words using a rulebased transliterator and the CMU lexicon. Using the same 200 hours of transcribed audio, we trained acoustic models as described in (Prasad et al., 2005). We use a mixture of the training transcripts and various newswi</context>
</contexts>
<marker>Prasad, Matsoukas, Kao, Ma, Xu, Colthurst, Kimball, Schwartz, Gauvain, Lamel, Schwenk, Adda, Lefevre, 2005</marker>
<rawString>R. Prasad, S. Matsoukas, C.L. Kao, J. Ma, D.X. Xu, T. Colthurst, O. Kimball, R. Schwartz, J.L. Gauvain, L. Lamel, H. Schwenk, G. Adda, and F. Lefevre. 2005. The 2004 BBN/LIMSI 20xRT English Conversational Telephone Speech Recognition System. In Interspeech 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
<author>S Walker</author>
<author>S Jones</author>
<author>M HancockBeaulieu M Gatford</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1996</date>
<booktitle>In Text REtrieval Conference,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="18242" citStr="Robertson et al., 1996" startWordPosition="2953" endWordPosition="2956">AP. ful simply because it highlights improvements on the most difficult queries. retrieval score for query D’s Q as tion and add E = 0.00001. The authors are not aware of significance tests having been previously report where the inverse document fr equency (idf) is defined as Robertson (2006) presents a justification and analysis of GMAP and notes that it may alternatively be 4.2.2 Significance Testing for GMAP A standard way of measuring the significance of system improvements in MAP is to compare average precision (AP) on each of the evaluation queri 4.3 Retrieval System We use Okapi BM25 (Robertson et al., 1996) as our basic retrieval system, which defines a document n i=1 N is the size of the collection, n(qi) is the document frequency for term qi, is the frequency of term qi in query Q, D) is the term frequency of query term qi in document D, is the length of the matching document, an n(qi) + 0.5 , qfi f(qi, |D| d avgdl is the average length of a document in the collection. We set the N − n(qi) + 0.5 idf(qi) = log significance in GMAP improvements. That is, we test for significant improvements in GMAP by applying the Wilcoxon signed rank test to the paired, (k3+1)qf� k3+qf�)f(qi,D)(k1 + 1) transfor</context>
</contexts>
<marker>Robertson, Walker, Jones, Gatford, 1996</marker>
<rawString>S. Robertson, S. Walker, S. Jones, and M. HancockBeaulieu M. Gatford. 1996. Okapi at TREC-3. In Text REtrieval Conference, pages 21–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
</authors>
<title>On GMAP: and other transformations.</title>
<date>2006</date>
<booktitle>In CIKM ’06: Proceedings of the 15th ACM international conference on Information and knowledge management,</booktitle>
<pages>78--83</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="16991" citStr="Robertson (2006)" startWordPosition="2740" endWordPosition="2741">from 0.4 to 0.38, the MAP will be unchanged. If we prefer to highlight performance differences on the lowest performing topics, a widely used alternative is the geometric mean of average precision (GMAP), first introduced in the TREC 2004 robust track (Voorhees, 2006). �HGMAP = � APn n computed as an arithmetic mean of logs, 1 � log APn. GMAP = exp n n es 465 using the Wilcoxon signed-rank test. This test, while not requiring a particular distribution on the measurements, does assume that they belong to an interval scale. Similarly, the arithmetic mean of MAP assumes AP has interval scale. As Robertson (2006) has pointed out, it is in no sense clear that AP (prior to any transformation) satisfies this assumption. This becomes an argument for GMAP, since it may also be defined using an arithmetic mean of logtransformed average precisions. That is to say, the logarithm is simply one possible monotonic transformation which is arguably as good as any other, including the identify transform, in terms of whether the transformed value satisfies the interval assumption. This log transform (and hence GMAP) is useWe apply the same reasoning to test for statistical itberg &amp; with AP = 0, we follow the Robust </context>
</contexts>
<marker>Robertson, 2006</marker>
<rawString>Stephen Robertson. 2006. On GMAP: and other transformations. In CIKM ’06: Proceedings of the 15th ACM international conference on Information and knowledge management, pages 78–83, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Savoy</author>
<author>A Le Calv´e</author>
<author>D Vrajitoru</author>
</authors>
<title>Report on the TREC-5 experiment: Data fusion and collection fusion.</title>
<date>1988</date>
<marker>Savoy, Le Calv´e, Vrajitoru, 1988</marker>
<rawString>J. Savoy, A. Le Calv´e, and D. Vrajitoru. 1988. Report on the TREC-5 experiment: Data fusion and collection fusion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph A Shaw</author>
<author>Edward A Fox</author>
</authors>
<title>Combination of multiple searches.</title>
<date>1994</date>
<booktitle>In Proceedings of the 2nd Text REtrieval Conference (TREC-2).</booktitle>
<contexts>
<context position="4891" citStr="Shaw and Fox (1994)" startWordPosition="741" endWordPosition="744">d in the literature (Bartell et al., 1994; Callan et al., 1995; Powell et al., 2000; Vogt and Cottrell, 1999), with varying degrees of success, owing in part to the potential difficulty of normalizing scores across retrieval systems. In this study, we partially abstract away from this potential difficulty by using the same retrieval system on both representations of the collection documents (so that we don’t expect score distributions to be significantly different for the combination inputs). Of course, many fusion techniques using more advanced score normalization methods have been proposed. Shaw and Fox (1994) proposed a number of such techniques, perhaps the most successful of which is known as CombMNZ. CombMNZ has been shown to achieve strong performance and has been used in many subsequent studies (Lee, 1997; Montague and Aslam, 2002; Beitzel et al., 2004; Lillis et al., 2006). In this study, we also use CombMNZ as a baseline for comparison, and following Lillis et al. (2006) and Lee (1997), compute it in the following way. First, we normalize each score si as norm(si) = si−min(s) where max(s) and max(s)−min(s) min(s) are the maximum and minimum scores seen in the input result list. After normal</context>
</contexts>
<marker>Shaw, Fox, 1994</marker>
<rawString>Joseph A. Shaw and Edward A. Fox. 1994. Combination of multiple searches. In Proceedings of the 2nd Text REtrieval Conference (TREC-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Streitberg</author>
<author>Joachim R¨ohmel</author>
</authors>
<title>On tests that are uniformly more powerful than the WilcoxonMann-Whitney test.</title>
<date>1990</date>
<journal>Biometrics,</journal>
<volume>46</volume>
<issue>2</issue>
<marker>Streitberg, R¨ohmel, 1990</marker>
<rawString>Bernd Streitberg and Joachim R¨ohmel. 1990. On tests that are uniformly more powerful than the WilcoxonMann-Whitney test. Biometrics, 46(2):481–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher C Vogt</author>
<author>Garrison W Cottrell</author>
</authors>
<title>Fusion via a linear combination of scores.</title>
<date>1999</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="4381" citStr="Vogt and Cottrell, 1999" startWordPosition="662" endWordPosition="665">589 hour collection of conversational speech for which information retrieval queries and relevance judgments are available. Section 5 presents the results of our experiments, and we conclude in Section 6 with a brief discussion of implications of our results and the potential for future work on this important problem. 2 Previous Work One approach for combining ranked retrieval results is to simply linearly combine the multiple system scores for each topic and document. This approach has been extensively applied in the literature (Bartell et al., 1994; Callan et al., 1995; Powell et al., 2000; Vogt and Cottrell, 1999), with varying degrees of success, owing in part to the potential difficulty of normalizing scores across retrieval systems. In this study, we partially abstract away from this potential difficulty by using the same retrieval system on both representations of the collection documents (so that we don’t expect score distributions to be significantly different for the combination inputs). Of course, many fusion techniques using more advanced score normalization methods have been proposed. Shaw and Fox (1994) proposed a number of such techniques, perhaps the most successful of which is known as Co</context>
</contexts>
<marker>Vogt, Cottrell, 1999</marker>
<rawString>Christopher C. Vogt and Garrison W. Cottrell. 1999. Fusion via a linear combination of scores. Information Retrieval, 1(3):151–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Narendra Kumar Gupta</author>
<author>Ben Johnson-Laird</author>
</authors>
<title>The collection fusion problem.</title>
<date>1994</date>
<booktitle>The Third Text REtrieval Conference (TREC-3),</booktitle>
<pages>500--225</pages>
<editor>In D. K. Harman, editor,</editor>
<contexts>
<context position="6211" citStr="Voorhees et al., 1994" startWordPosition="968" endWordPosition="971">s the number of ranked lists to be combined, Nt,d is the normalized score of document d in ranked list E, and |Nd &gt; 0 |is the number of nonzero normalized scores given to d by any result set. Manmatha et al. (2001) showed that retrieval scores from IR systems could be modeled using a Normal distribution for relevant documents and exponential distribution for non-relevant documents. However, in their study, fusion results using these comparatively complex normalization approaches achieved performance no better than the much simpler CombMNZ. A simple rank-based fusion technique is interleaving (Voorhees et al., 1994). In this approach, the highest ranked document from each list is taken in turn (ignoring duplicates) and placed at the top of the new, combined list. Many probabilistic combination approaches have also been developed, a recent example being Lillis et al. (2006). Perhaps the most closely related proposal, using logistic regression, was made first by Savoy et al. (1988). Logistic regression is one example from the broad class of models which GAMs encompass. Unlike GAMs in their full generality however, logistic regression imposes a comparatively high degree of linearity in the model structure. </context>
</contexts>
<marker>Voorhees, Gupta, Johnson-Laird, 1994</marker>
<rawString>Ellen M. Voorhees, Narendra Kumar Gupta, and Ben Johnson-Laird. 1994. The collection fusion problem. In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 500–225. National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC 2005 robust retrieval track.</title>
<date>2006</date>
<booktitle>The Fourteenth Text REtrieval Conference, (TREC 2005),</booktitle>
<editor>In Ellem M. Voorhees and L.P. Buckland, editors,</editor>
<publisher>NIST.</publisher>
<location>Gaithersburg, MD:</location>
<contexts>
<context position="16643" citStr="Voorhees, 2006" startWordPosition="2676" endWordPosition="2677">oss multiple queries, a commonly used measure is mean average precision (MAP). Mean average precision is defined as the arithmetic mean of per-topic average precision, � MAP = 1 n APn. A consequence of the arithn metic mean is that, if a system improvement doubles AP for one topic from 0.02 to 0.04, while simultaneously decreasing AP on another from 0.4 to 0.38, the MAP will be unchanged. If we prefer to highlight performance differences on the lowest performing topics, a widely used alternative is the geometric mean of average precision (GMAP), first introduced in the TREC 2004 robust track (Voorhees, 2006). �HGMAP = � APn n computed as an arithmetic mean of logs, 1 � log APn. GMAP = exp n n es 465 using the Wilcoxon signed-rank test. This test, while not requiring a particular distribution on the measurements, does assume that they belong to an interval scale. Similarly, the arithmetic mean of MAP assumes AP has interval scale. As Robertson (2006) has pointed out, it is in no sense clear that AP (prior to any transformation) satisfies this assumption. This becomes an argument for GMAP, since it may also be defined using an arithmetic mean of logtransformed average precisions. That is to say, th</context>
</contexts>
<marker>Voorhees, 2006</marker>
<rawString>Ellen M. Voorhees. 2006. Overview of the TREC 2005 robust retrieval track. In Ellem M. Voorhees and L.P. Buckland, editors, The Fourteenth Text REtrieval Conference, (TREC 2005), Gaithersburg, MD: NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon N Wood</author>
</authors>
<title>Thin plate regression splines.</title>
<date>2003</date>
<journal>Journal Of The Royal Statistical Society Series B,</journal>
<volume>65</volume>
<issue>1</issue>
<contexts>
<context position="12092" citStr="Wood, 2003" startWordPosition="1935" endWordPosition="1936">tes (say the logarithm), we could simply apply it ourselves. GAMs allow us to learn these transformations from the data, when we expect some transformation to be useful but don’t know it’s form a priori. In practice, these smooth functions may be represented and the model parameters may be learned in various ways. In this work, we use the excellent open source package mgcv (Wood, 2006), which uses penalized likelihood maximization to prevent arbitrarily “wiggly” smooth functions (i.e., overfitting). Smooths (including multidimensional smooths) are represented by thin plate regression splines (Wood, 2003). 3.1 Combining speech retrieval results with GAMs The chief difficulty introduced in combining ranked speech retrieval results is the severe disagreement introduced by differing document hypotheses. As we saw in Figure 1, it is often the case that the most discriminative query terms occur in only one transcript source. 3.1.1 GLM with factors Our first new approach for handling differences in transcripts is an extension of the logistic regression model previously used in data fusion work, (Savoy et al., 1988). Specifically, we augment the model with the first-order interaction of scores x1x2 a</context>
</contexts>
<marker>Wood, 2003</marker>
<rawString>Simon N. Wood. 2003. Thin plate regression splines. Journal Of The Royal Statistical Society Series B, 65(1):95–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Wood</author>
</authors>
<title>Generalized Additive Models: An Introduction with R. Chapman and Hall/CRC.</title>
<date>2006</date>
<contexts>
<context position="11869" citStr="Wood, 2006" startWordPosition="1907" endWordPosition="1908">sional space c n c o n l ou s 0.. , 08 po 463 of smooth transformations, we are back to the linear problem of learning coefficients Oj which “optimally” fit the data. If we knew the appropriate transformation of our covariates (say the logarithm), we could simply apply it ourselves. GAMs allow us to learn these transformations from the data, when we expect some transformation to be useful but don’t know it’s form a priori. In practice, these smooth functions may be represented and the model parameters may be learned in various ways. In this work, we use the excellent open source package mgcv (Wood, 2006), which uses penalized likelihood maximization to prevent arbitrarily “wiggly” smooth functions (i.e., overfitting). Smooths (including multidimensional smooths) are represented by thin plate regression splines (Wood, 2003). 3.1 Combining speech retrieval results with GAMs The chief difficulty introduced in combining ranked speech retrieval results is the severe disagreement introduced by differing document hypotheses. As we saw in Figure 1, it is often the case that the most discriminative query terms occur in only one transcript source. 3.1.1 GLM with factors Our first new approach for handl</context>
</contexts>
<marker>Wood, 2006</marker>
<rawString>Simon Wood. 2006. Generalized Additive Models: An Introduction with R. Chapman and Hall/CRC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>