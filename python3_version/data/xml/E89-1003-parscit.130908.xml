<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.9944" genericHeader="abstract">
EFFICIENT PROCESSING OF
FLEXIBLE CATEGORIAL GRAMMAR
</sectionHeader>
<bodyText confidence="0.7947854">
Gosse Bouma
Research Institute for Knowledge Systems
Postbus 463, 6200 AL Maastricht
The Netherlands
e-mail(earn): exriksgb@hmar15
</bodyText>
<sectionHeader confidence="0.857888" genericHeader="introduction">
ABSTRACT*
</sectionHeader>
<bodyText confidence="0.999622769230769">
From a processing point of view, however,
flexible categorial systems are problematic,
since they introduce spurious ambiguity. In
this paper, we present a flexible categorial
grammar which makes extensive use of the
product-operator, first introduced by
Lambek (1958). The grammar has the prop-
erty that for every reading of a sentence, a
strictly left-branching derivation can be
given. This leads to the definition of a subset
of the grammar, for which the spurious ambi-
guity problem does not arise and efficient
processing is possible.
</bodyText>
<figure confidence="0.934613">
1. Flexibility vs. Ambiguity
(1) application : A/B B ==&gt; A
B B\A ==&gt; A
composition : A/B B/C ==&gt; A/C
C\B B\A ==&gt; c\A
raising A ==&gt; (B/A)\B
A ==&gt; B/(A\B)
</figure>
<bodyText confidence="0.962228859649123">
With this grammar many alternative con-
stituent structures for a sentence can be
generated, even where this does not corre-
spond to semantic ambiguities. From a lin-
guistic point of view, this has many advan-
tages. Various kind of arguments for giving
up traditional conceptions of constituent
structure can be given, but the most con-
vincing and well-documented case in favour
of flexible constituent structure is coordi-
nation (see Steedman (1985), Dowty (1988),
and Zwarts (1986)).
Categorial Grammars owe much of their
popularity to the fact that they allow for
various degrees of flexibility with respect to
constituent structure. From a processing
point of view, however, flexible categorial
systems are problematic, since they intro-
duce spurious ambiguity.
The best known example of a flexible
categorial grammar is a grammar containing
the reduction rules application and compo-
sition, and the category changing rule rais-
ing 1 :
*
I would like to thank Esther Konig,
Erik-Jan van der Linden, Michael Moortgat,
Adriaan van Paassen and the participants of
the Edinburgh Categorial Grammar Weekend,
who made useful comments to earlier
presentations of this material. All remaining
errors and misconceptions are of course my
own.
1 Throughout this paper we will be us-
ing the notation of Lambek (1958), in which
A/B and B\A are a right-directional and a
The standard assumption in generative
grammar is that coordination always takes
place between between constituents. Right-
node raising constructions and other in-
stances of non-constituent conjunction are
problematic, because it is not clear what the
status of the coordinated elements in these
constructions is. Flexible categorial gram-
mar presents an elegant solution for such
cases, since, next to canonical constituent
structures, it also admits various other con-
stituent structures. Therefore, the sentences
in (2) can be considered to be ordinary in-
stances of coordination (of two categories
s/np and (vp/np)\vp, respectively).
(2) a. John sold and Mary bought a book
s/vp vp/np s/vp vp/np np
s/np s/np
left-directional functor respectively, looking
for an argument of category B.
- 19 -
</bodyText>
<figure confidence="0.821672166666667">
b. J. loves Mary madly and Sue wildly
vp/np np vp\vp np vp\vp
Mary madly
np vp\vp
(vp/np)\vp
(vp/np)\vp
</figure>
<bodyText confidence="0.999609392857143">
A somewhat different type of argument
for flexible phrase structure is based on the
way humans process natural language. In
Ades &amp; Steedman (1982) it is pointed out
that humans process natural language in a
left-to-right, incremental, manner. This pro-
cessing aspect is accounted for in a flexible
categorial system, where constituents can be
built for any part of a sentence. Since syn-
tactic rules operate in parallel with semantic
interpretation rules, building a syntactic
structure for an initial part of a sentence,
implies that a corresponding semantic struc-
ture can also be constructed.
These and other arguments suggest that
there is no such thing as a fixed constituent
structure, but that the order in which ele-
ments combine with eachother is rather free.
From a parsing point of view, however,
flexibility appears to be a disadvantage.
Flexible categorial grammars produce large
numbers of, often semantically equivalent,
derivations for a given phrase. This spurious
ambiguity problem (Wittenburg (1986))
makes efficient processing of flexible catego-
rial grammar problematic, since quite often
there is an exponential growth of the number
of possible derivations, relative to the length
of the string to be parsed.
There have been two proposals for
eliminating spurious ambiguity from the
grammar. The first is Wittenburg (1987). In
this paper, a categorial grammar with compo-
sition and heavily restricted versions of
raising (for subject n p &apos;s only) is considered.
Wittenburg proposes to eliminate spurious
ambiguity by redefining composition. His
predictive composition rules apply only in
those cases where they are really needed to
make a derivation possible. A disadvantage
of this method, noticed by Wittenburg, is
that one may have to add special predictive
composition rules for all general combina-
tory rules in the grammar. Some careful
rewriting of the original grammar has to take
place, before things work as desired.
Pareschi &amp; Steedman (1987) propose an
efficient chart-parsing algorithm for catego-
rial grammars with spurious ambiguity. In-
stead of the usual strategy, in which all pos-
sible subconstituents are added to the chart,
Pareschi &amp; Steedman restrict themselves to
adding only those constituents that may lead
to a difference in semantics. Thus, in (3)
only the underlined constituents are in the
chart. The &amp;quot;---&amp;quot; constituent is not.
</bodyText>
<listItem confidence="0.6550515">
(3) John loves Mary madly
s/vp vp/np np vp\vp
</listItem>
<bodyText confidence="0.991649025">
Combining &apos;madly&apos; with the rest would be
impossible or lead to backtracking in the
normal case. Here, the Pareschi &amp; Steedman
algorithm starts looking for a constituent
left adjacent of madly, which contains an el-
ement X / vp as a leftmost category. If such a
constituent can be found, it can be concluded
that the rest of that constituent must
(implicitly) be a v p , and thus the validity of
combining vp\vp with this constituent has
been established. Therefore, Pareschi &amp;
Steedman are able to work with only a mini-
mal amount of items in the chart.
Both Wittenburg and Pareschi &amp; Steed-
man work with categorial grammars, which
contain restricted versions of composition
and raising. Although they can be processed
efficiently, there is linguistic evidence that
they are not fully adequate for analysis of
such phenomena as coordination. Since
atomic categories can in general not be
raised in these grammars, sentence (2b) (in
which the category np has to be raised)
cannot be derived. Furthermore, since
composition is not generalized, as in Ades &amp;
Steedman (1982), a sentence such as John
sold but Mary donated a book to the library
would not be derivable. The possibilities for
left-to-right, incremental, processing are
also limited. Therefore, there is reason to
look for a more flexible system, for which
efficient parsing is still possible.
- 20 -
Strong Structural Complete-
ness
If a sequence of categories Xi .. Xn
reduces to Y, with semantics Y&apos;,
there is a reduction to Y, with se-
mantics Y&apos;, for any bracketing of
X1 ..Xn into constituents.
</bodyText>
<sectionHeader confidence="0.588478" genericHeader="method">
2. Structural Completeness (5)
</sectionHeader>
<bodyText confidence="0.99990265">
In the next section we present a gram-
matical calculus, which is more flexible than
the systems considered by Wittenburg
(1987) and Pareschi &amp; Steedman (1987), and
therefore is attractive for linguistic pur-
poses. At the same time, it offers a solution
to the spurious ambiguity problem.
Spurious ambiguity causes problems for
parsing in the systems mentioned above, be-
cause there is no systematic relationship
between syntactic structures and semantic
representations. That is, there is no way to
identify in advance, for a given sentence S. a
proper subset of the set of all possible syn-
tactic structures and associated semantic
representations, for which it holds that it
will contain all possible semantic represen-
tations of S.
Consider now a grammar for which the
following property holds:
</bodyText>
<sectionHeader confidence="0.435273" genericHeader="method">
(4) Structural Completeness
</sectionHeader>
<bodyText confidence="0.991050914285714">
If a sequence of categories X1 Xn
reduces to Y, there is a reduction to
Y for any bracketing of X1 ..Xn into
constituents. (Moortgat, 1987:5)2
Structural complete grammars are interest-
ing linguistically, since they are able to
handle, for instance, all kinds of non-con-
stituent conjunction, and also because they
allow for strict left-to-right processing (see
Moortgat, 1988).
The latter observation has consequences
for parsing as well,, since, if we can parse
every sentence in a strict left-to-right man-
ner (that is, we produce only strictly left-
branching syntax trees), the parsing algo-
rithm can be greatly simplified. Notice,
however, that such a parsing strategy is only
valid, if we also guarantee that all possible
readings of a sentence can be found in this
way. Thus, instead of (4), we are looking for
grammars with the following, slightly
stronger, property:
2 Buszkowski (1988) provides a
slightly different definition in terms of
functor-argument structures.
Grammars with this property, can poten-
tially circumvent the spurious ambiguity
problem, since for these grammars, we only
have to inspect all left-branching syntax
trees, to find all possible readings. This
method will only fail if the set of left-
branching trees itself would contain spuri-
ous ambiguous derivations. In section 4 we
will show that these can be eliminated from
the calculus presented below.
</bodyText>
<sectionHeader confidence="0.993874" genericHeader="method">
3. The P-calculus
</sectionHeader>
<bodyText confidence="0.999715870967742">
The P(roduct)-calculus is a categorial
grammar, based on Lambek (1958), which
has the property of strong structural com-
pleteness.
In Lambek (1958), the foundations of
flexible categorial grammar are formulated
in the form of a calculus for syntactic cate-
gories. Well-known categorial rules, such as
application, composition and category-rais-
ing, are theorems of this calculus. A largely
neglected aspect of this calculus, is the use
of the product-operator.
The calculus we present below, was
developed as an alternative for Moortgat&apos;s
(1988) M-system. The M-system is a subset
of the Lambek-calculus, which uses, next to
application, only a very general form of
composition. Since it has no raising, it seems
to be an attractive candidate for investigat-
ing the possibilities of left-associative
parsing for categorial grammar. It is not
completely satisfactory, however, since
structural completeness is not fully guaran-
teed, and also, since it is unknown whether
the strong structural completeness property
holds for this system. In our calculus, we
hope to overcome these problems, by using
product-introduction and -elimination rules
instead of composition.
The kernel of the P-calculus is right-
and left-application, as usual. Next to these,
</bodyText>
<equation confidence="0.640821">
- 21 -
S --&gt; NP VP), or (in CO) with a reduction
</equation>
<bodyText confidence="0.993856714285714">
rule (application or composition, for in-
stance), we now have the freedom to
concatenate arbitrary categories, completely
irrespective of their internal structure.
we use a rule for introducing the product-
operator, and two inference rules for elimi-
nating products :
</bodyText>
<equation confidence="0.802511444444444">
(6) RA: A/B B =&gt; A
LA: B B\A =&gt; A
(product) introduction:
I : AB =&gt; A*B
inference rules :
P : A B =&gt; C, D C =&gt; E
D*A B =&gt; E
P&apos;: AB=&gt;C, CD=&gt;E
A B*D =&gt; E
</equation>
<bodyText confidence="0.901700333333333">
We can use this calculus to produce left-
branching syntax trees for any given
(grammatical) sentence. (7) is a simple ex-
</bodyText>
<figure confidence="0.980947909090909">
ample3 .
(7) John loves Mary madly
s/vp vp/np np vp\vp
s/vp*vp/np
(a)
s/vp*vp
(b)
(a) vp/np np =&gt; vp,s/vp vp =&gt; s/vp*vp
s/vp*vp/np np =&gt; s/vp*vp
(b) vp vp\vp =&gt; vp , s/vp vp =&gt; s
s/vp*vp vp =&gt; s
</figure>
<bodyText confidence="0.91529147368421">
The P-calculus is structurally complete.
To prove this, we prove that for any four
categories A,B,C,D, it holds that :
(AB)C --&gt; D &lt;==&gt; A(BC) --&gt; D, where --&gt;
is the derivability relation. From this,
structural completeness may be concluded,
since any bracketing (or branching of syntax
trees) can be obtained by applying this
equivalence an arbitrary number of times.
Proof : From (AB)C --&gt; D it follows that
there exists a category E such that AB --&gt;
E and EC --&gt; D. BC --&gt; B*C, by I. Now
A(B*C) --&gt; D, by P&apos;, since AB --&gt; E and
EC --&gt; D. Therefore, by transitivity of --&gt;,
A(BC) --&gt; D. To prove that A(BC) --&gt; D
==&gt; (AB)C --&gt; D use P instead of P.
Semantics can be added to the grammar,
by giving a semantic counterpart (in lower
case) for each of the rules in (6):
</bodyText>
<table confidence="0.809966">
(8) RA: A/B:a B:b =&gt; A:a(b)
LA: B:b B\A:a =&gt; A:a(b)
(product) introduction:
I : A:a B:b =&gt; A*B:a`b
inference rules :
P : A:a B:b =&gt; C:c, D:d C:c =&gt; E:e
D*A:d*a B:b =&gt; E:e
P&apos; : A:a B:b =&gt; C:c, C:c D:d =&gt; E:e
A:a B*D:b*d =&gt; E:e
</table>
<bodyText confidence="0.9986445">
The first step in the derivation of (7) is the
application of rule I. The other two reduc-
tions ((a) and (b)) are instantiations of the
inference rule P. As the example shows, the
*-operator (more in particular its use in I)
does something like concatenation, but
whereas such operations are normally asso-
ciated with particular grammatical rules (i.e.
you may concatenate two elements of category
NP and V P, respectively, if there is a rule
</bodyText>
<page confidence="0.357809">
3 To improve readability, we assume
</page>
<bodyText confidence="0.998904333333333">
that the operators / and \ take precedence
over * (X*Y/Z should be read as X*(Y/Z) ).
We can now include semantics in the
proof given above, and from this, we may con-
clude that strong structural completeness
holds for the P-calculus as well.
</bodyText>
<sectionHeader confidence="0.9947495" genericHeader="method">
4. Eliminating Spurious
Ambiguity
</sectionHeader>
<bodyText confidence="0.979632863636364">
In this section we outline a subset of the
P-calculus, for which efficient processing is
possible. As was noted above, in the P-cal-
culus there is always a strictly left-
branching derivation for any reading of a
- 22 -
sentence S. The restrictions we add in this
section are needed to eliminate spurious am-
biguities from these left-branching deriva-
tions.
Restricting a parser so that it will only
accept left-branching derivations will not
directly lead to an efficient parsing proce-
dure for the P-calculus. The reason is
twofold.
First, nothing in the P-calculus excludes
spurious ambiguity which occurs within the
set of left-branching analysis trees. Con-
sider again example (7). This sentence is
unambiguous, but nevertheless we can give a
left-branching derivation for it which dif-
fers from the one given earlier :
</bodyText>
<listItem confidence="0.944972">
(9) John loves Mary madly
</listItem>
<bodyText confidence="0.914643142857143">
s/vp vp/np np vp\vp
s/vp*vp/np
(s/vp*vp/np)*np
The inference step (**) can be proven to be
valid, if we use P&apos; as well as P.
An even more serious problem is caused
by the interaction between I and P,P&apos;.
</bodyText>
<figure confidence="0.918071">
(10) A B =&gt; A*B A*B C =&gt; D
P.
B C =&gt; B*C A B*C =&gt; D
A*B C =&gt; D
</figure>
<bodyText confidence="0.999836217391304">
If we try to prove that A*B and C can be re-
duced to a category D, we could use P, with I
in the left premise. To prove the second
premise, we could use P&apos;, also with using I in
the left premise. But now the right premise
of P&apos; is identical to our initial problem, and
thus we have made a useless loop, which
could even lead to an infinite regress.
These problems can be eliminated, if we
restrict the grammar in two ways. First of
all, we consider only derivations of the form
==&gt; S, where C ,...,Cn ,S do not
contain the product-operator. This means we
require that the start symbol of the grammar,
and the set lexical categories must be prod-
uct-free. Notice that this restriction can be
easily made, since most categorial lexicons
do not contain the product-operator anyway.
Given this restriction, the inference rule
P can be restricted: we require that the left
premise of this rules always is an instance of
either left- or right-application. Consider
what would happen if we used I here :
</bodyText>
<figure confidence="0.5828855">
B C =&gt; B*C A B*C =&gt; D
A*B C =&gt; D
</figure>
<bodyText confidence="0.998965111111111">
Since the lexicon is product-free, and we are
interested in strictly left-branching deriva-
tions only, we know that C must be a prod-
uct-free category. If we combine B and C
through I, we are faced with the problem in
***. At this point we could use I again for in-
stance, thereby instantiating D as A*(B*C).
But this will lead to a spurious ambiguity,
since we know that:
</bodyText>
<sectionHeader confidence="0.42026" genericHeader="method">
A*(B*C) E =&gt; F iff (A*B)*C E =&gt; F4.
</sectionHeader>
<bodyText confidence="0.9999411">
A category (A*B)*C can be obtained by ap-
plying I directly to A*B and C.
If we apply P&apos; at point ***, we find our-
selves trying to find a solution for A B =&gt;
E, and then E C =&gt; D. But this is nothing
else than trying to find a left-branching
derivation for A,B,C =&gt; D, and therefore,
the inference step in (11) has not led to
anything new.
In fact, given that the lexicon is product
free and only application may be used in the
left premise of P. P&apos; is never needed to derive
a left-branching tree.
As a result, we get (12), where we have
made a distinction between reduction rules
(right and left-application) and other rules.
This enables us to restrict the left premise
of P. The fact that every reduction rule is
also a general rule of the grammar, is ex-
pressed by R. P&apos; has been eliminated.
</bodyText>
<sectionHeader confidence="0.246889" genericHeader="method">
4 In the P-calculus, this follows from
</sectionHeader>
<bodyText confidence="0.601991">
the fact that E must be product-free. It is a
theorem of the Lambek-calculus as well.
</bodyText>
<figure confidence="0.717135727272727">
CP—D
- 23 -
(12) RA: A/B B-&gt; A
LA: B -&gt; A
(product) introduction:
I : AB =&gt; A*B
inference rules :
P : A B -&gt; C, D C =&gt; E
D*A B =&gt; E
R : A B -&gt; C
A B =&gt; C
</figure>
<bodyText confidence="0.998564">
The system in (12) is a subset of the P-
calculus, which is able to generate a strictly
left-branching derivation for every reading
of a given sentence of the grammar.
The Prolog fragment in (13) shows how
the restricted system in (12) can be used to
define a simple left-associative parsing algo-
rithm.
</bodyText>
<listItem confidence="0.384348">
(13) parse([C] ==&gt; C) !.
</listItem>
<table confidence="0.782158125">
parse([C1,C2IRest] ==&gt; S) :-
rule([C1,C2] ==&gt; C3),
parse([C3IRest] ==&gt; S).
% R :
rule(X ==&gt; Y) :-
reduction_rule(X ==&gt; Y).
% P :
% &apos;+&apos; is used instead of 1&amp;quot; to avoid
% unnecessary bracketing
rule([X+Y,Z] ==&gt; W) :-
reduction_rule([Y,Z] ==&gt; V),
rule([X,V] ==&gt; W).
rule([X,Y] ==&gt; X+Y).
% application :
reduction_rule([X/Y,Y] ==&gt; X).
reduction_rule([Y,Y\X] ==&gt; X).
</table>
<sectionHeader confidence="0.948539" genericHeader="method">
5. Shift-reduce parsing
</sectionHeader>
<bodyText confidence="0.999896454545455">
It has sometimes been noted that a
derivation tree in categorial grammar (such
as (7)) does not really reflect constituent
structure in the traditional sense, but that it
reflects a particular parse process. This may
be true for categorial systems in general, but
it is particularly clear for the P-calculus.
Consider for instance how one would parse a
sentence like (7) using a shift-reduce pars-
ing technique, and having only right- and
left-application as syntax rules.
</bodyText>
<table confidence="0.807289125">
(remaining) input stack
s/vp,vp/np,np,vp\vp
vp/np,np,vp\vp s/vp
np,vp\vp s/vp,vp/np
vp \ vp s/vp,vp/np,np
v p \ v p s/vp,vp
s/vp,vp,vp\vp
s/vp,vp
</table>
<bodyText confidence="0.978518256410256">
Shifting an element onto the stack (apart
form the first one maybe) seems to be equiv-
alent to combining elements by means of I.
The stack is after all nothing but a somewhat
different representation of the product types
we used earlier. The fact that adding one el-
ement to the stack (vp\vp) induces two re-
duction steps, is comparable to the fact that
the inference rule P may have the effect of
eliminating more than one slash at time.
The similarity between shift-reduce
parsing and the derivations in P brings in
another interesting aspect. The shift-reduce
algorithm is a correct parsing strategy, be-
cause it will produce all (syntactic) ambigu-
ities for a given input string. This means
that in the example above, a shift-reduce
parser would only produce one syntax tree
(assuming that the grammar has only appli-
cation).
If the input was potentially ambiguous,
as in (15), there are two different deriva-
tions.
(15) a/a a a\a
It is after shifting a on the stack that a
difference arises. Here, one can either re-
duce or shift one more step. The first choice
leads to the left-branching derivation, the
second to the right-branching one.
The choice between shifting or reducing
has a categorial equivalent. In the P-calcu-
lus, one can either produce a left-branching
derivation tree for (15) by using application
only, or as indicated in (16).
- 24 -
to the grammar (since X can be instantiated,
for instance as s/vp*vp/np). This means
that left-associative derivations are not
always possible for coordinated sentences.
</bodyText>
<figure confidence="0.9736788">
(16) a/a a a \ a
I
a/a*a
P
a
</figure>
<bodyText confidence="0.999842272727273">
Note that the P-calculus thus is able to
find genuine syntactic (or potentially se-
mantic) ambiguities, without producing a
different branching phrase structure. The
correspondence to shift-reduce parsing al-
ready suggests this of course, since we
should consider the phrase structure pro-
duced by a structurally complete grammar
much more as a record of the parse process
than as a constituent structure in the tradi-
tional sense.
</bodyText>
<sectionHeader confidence="0.997355" genericHeader="method">
6. Coordination
</sectionHeader>
<bodyText confidence="0.999842333333333">
The P-calculus is structurally complete,
and therefore, all the arguments that have
been presented in favour of a categorial
analysis of coordination, hold for the P-cal-
culus as well. Coordination introduces poly-
morphism in the grammar, however, and this
leads to some complications for the re-
stricted P-calculus presented in (12).
Adding a category X \ (X/X) for coordina-
tors to the P-calculus, enables us to handle
non-constituent conjunction, as is exempli-
fied in (17) and (18).
</bodyText>
<figure confidence="0.910790153846154">
(17) John loves and Peter adores Sue
s/vp vp/np XVX/X) s/vp vp/np np
I
s/vp*vp/np s/vp*vp/np
s/vp*vp/np
P
s
(18) J. loves Mary madly and Sue wildly
vp/np np vp\vp M(X/X) np vp\vp
np*vp\vp np*vp\vp
np*vp\vp
P.
vp
</figure>
<bodyText confidence="0.99817">
The restricted-calculus of (12) was de-
signed to enable efficient left-associative
parsing. We assumed that lexical categories
would always be product-free, but this as-
sumption no longer holds, if we add X \( X /X )
Our solution to this problem, is to add
rules such as (19) to the grammar, which can
transform certain product-categories into
product-free categories.
</bodyText>
<listItem confidence="0.386582">
(19) A/(B *C) =----&gt; (A/C)/B
</listItem>
<bodyText confidence="0.999183125">
A number of such rules are needed to restore
left-associativity.
Next to syntactical additions, some
modifications to the semantic part of the
inference rule P had to be made, in order to
cope with the polymorphic semantics
proposed for coordination by Partee &amp; Rooth
(1983).
</bodyText>
<sectionHeader confidence="0.70525" genericHeader="conclusions">
7. Concluding remarks.
</sectionHeader>
<bodyText confidence="0.991873675675676">
The spurious ambiguity problem has
been solved in this paper in a rather para-
doxical manner. Whereas Wittenburg (1987)
tries to do away with ambiguous phrase
structure as much as possible (it only arises
where you need it) and Pareschi &amp; Steedman
(1987) use a chart parsing technique to re-
cover implicit constituents efficiently, the
strategy in this paper has been to go for
complete ambiguity. It is in fact this
massive ambiguity, which trivializes
constituent structure to such an extent that
one might as well ignore it, and choose a con-
stituent structure that fits ones purposes
best (left-branching in this case). It seems
that as far as processing is concerned, the
half-way flexible systems of Steedman
(having generalized composition, and heavily
restricted forms of raising) are in fact the
hardest case. Simple AB-grammars are in all
respects similar to CF-grammars, and can di-
rectly be parsed by any bottom-up
algorithm. For strong structurally complete
systems such as P, spurious ambiguity can
be eliminated by inspecting left-branching
trees only. For flexible but not structurally
complete systems, it is much harder to pre-
dict which derivations are interesting and
which ones are not, and therefore the only
solution is often to inspect all possibilities.
- 25 -
Wittenburg, Kent (1987), Predictive
Combinators: A Method for Efficient Pro-
cessing of Combinatory Categorial Grammars.
Proceedings of the 25th Annual Meeting of
the Association for Computational Linguis-
tics, Stanford University, 73-80.
</bodyText>
<sectionHeader confidence="0.909544" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981577239130435">
Ades, Antony &amp; Mark Steedman (1982), On
the Order of Words, Linguistics &amp; Phi-
losophy 4, 517-518.
Buszkowski, Wojciech (1988). Generative
Power of Categorial Grammars. In R. Oehrle,
E. Bach, and D. Wheeler (eds.), Categorial
Grammars and Natural Language Structures,
Reidel, Dordrecht, 69-94.
Zwarts, Frans (1986), Categoriale Grammatica
en Algebraische Semantiek, Dissertation,
State University Groningen.
Dowty, David (1988), Type Raising, Func-
tional Composition, and Non-constituent
Conjunction. In R. Oehrle, E. Bach, and D.
Wheeler (eds.), Categorial Grammars and
Natural Language Structures, Reidel, Dor-
drecht, 153-197.
Lambek, Joachim (1958), The mathematics of
sentence structure. American Mathematical
Monthly 65, 154-170.
Moortgat, Michael (1987), Lambek Categorial
Grammar and the Autonomy Thesis. INL
working papers 87-03.
Moortgat, Michael (1988), Categorial Inves-
tigations : Logical and Linguistic Aspects of
the Lambek Calculus. Dissertation, Univer-
sity of Amsterdam.
Pareschi, Remo and Mark Steedman (1987), A
Lazy Way to Chart-Parse with Categorial
Grammars. Proceedings of the 25th Annual
Meeting of the Association for Computational
Linguistics, Stanford University, 81-88.
Partee, Barbara and Mats Rooth (1983), Gen-
eralized Conjunction and Type Ambiguity. In
R. }Merle, C. Schwarze and A. von Stechow
(eds.) Meaning, Use, and Interpretation of
Language, de Gruyter, Berlin, 361-383.
Steedman, Mark (1985), Dependency and
Coordination in the Grammar of Dutch and
English. Language 61, 523-568.
Wittenburg, Kent (1986), Natural Language
Parsing with Combinatory Categorial Gram-
mars in a Graph-Unification-Based Formal-
ism. Ph. D. dissertation, University of Texas
at Austin.
- 26 -
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.089852">
<title confidence="0.994136">EFFICIENT PROCESSING OF FLEXIBLE CATEGORIAL GRAMMAR</title>
<author confidence="0.964431">Gosse Bouma</author>
<affiliation confidence="0.940402">Research Institute for Knowledge Systems</affiliation>
<address confidence="0.8635995">Postbus 463, 6200 AL Maastricht The Netherlands</address>
<email confidence="0.305188">e-mail(earn):exriksgb@hmar15</email>
<abstract confidence="0.991582350943396">From a processing point of view, however, flexible categorial systems are problematic, they introduce ambiguity. this paper, we present a flexible categorial grammar which makes extensive use of the product-operator, first introduced by Lambek (1958). The grammar has the property that for every reading of a sentence, a strictly left-branching derivation can be given. This leads to the definition of a subset of the grammar, for which the spurious ambiguity problem does not arise and efficient processing is possible. 1. Flexibility vs. Ambiguity application : B ==&gt; A B B\A ==&gt; A : B/C ==&gt; A/C C\B B\A ==&gt; c\A ==&gt; (B/A)\B A ==&gt; B/(A\B) With this grammar many alternative constituent structures for a sentence can be generated, even where this does not correspond to semantic ambiguities. From a linguistic point of view, this has many advantages. Various kind of arguments for giving up traditional conceptions of constituent structure can be given, but the most convincing and well-documented case in favour of flexible constituent structure is coordination (see Steedman (1985), Dowty (1988), and Zwarts (1986)). Categorial Grammars owe much of their popularity to the fact that they allow for various degrees of flexibility with respect to constituent structure. From a processing point of view, however, flexible categorial systems are problematic, since they introambiguity. The best known example of a flexible categorial grammar is a grammar containing reduction rules compothe category changing rule rais- 1: * I would like to thank Esther Konig, Erik-Jan van der Linden, Michael Moortgat, Adriaan van Paassen and the participants of the Edinburgh Categorial Grammar Weekend, who made useful comments to earlier presentations of this material. All remaining errors and misconceptions are of course my own. Throughout this paper we will be ing the notation of Lambek (1958), in which B\A are a right-directional and a The standard assumption in generative grammar is that coordination always takes place between between constituents. Rightnode raising constructions and other instances of non-constituent conjunction are problematic, because it is not clear what the status of the coordinated elements in these constructions is. Flexible categorial grammar presents an elegant solution for such cases, since, next to canonical constituent structures, it also admits various other constituent structures. Therefore, the sentences in (2) can be considered to be ordinary instances of coordination (of two categories John sold and Mary bought book s/vp vp/np s/vp vp/np np s/np s/np left-directional functor respectively, looking an argument of category - 19 - J. madly wildly vp/np np vp\vp np vp\vp Mary madly np vp\vp (vp/np)\vp (vp/np)\vp A somewhat different type of argument for flexible phrase structure is based on the way humans process natural language. In Ades &amp; Steedman (1982) it is pointed out that humans process natural language in a left-to-right, incremental, manner. This processing aspect is accounted for in a flexible categorial system, where constituents can be built for any part of a sentence. Since syntactic rules operate in parallel with semantic interpretation rules, building a syntactic structure for an initial part of a sentence, implies that a corresponding semantic structure can also be constructed. These and other arguments suggest that there is no such thing as a fixed constituent structure, but that the order in which elements combine with eachother is rather free. From a parsing point of view, however, flexibility appears to be a disadvantage. Flexible categorial grammars produce large numbers of, often semantically equivalent, for a given phrase. This problem (1986)) makes efficient processing of flexible categorial grammar problematic, since quite often there is an exponential growth of the number of possible derivations, relative to the length of the string to be parsed. There have been two proposals for eliminating spurious ambiguity from the grammar. The first is Wittenburg (1987). In this paper, a categorial grammar with composition and heavily restricted versions of raising (for subject n p &apos;s only) is considered. Wittenburg proposes to eliminate spurious ambiguity by redefining composition. His rules apply only in those cases where they are really needed to make a derivation possible. A disadvantage of this method, noticed by Wittenburg, is one may have to add special composition rules for all general combinatory rules in the grammar. Some careful rewriting of the original grammar has to take place, before things work as desired. Pareschi &amp; Steedman (1987) propose an efficient chart-parsing algorithm for categorial grammars with spurious ambiguity. Instead of the usual strategy, in which all possible subconstituents are added to the chart, Pareschi &amp; Steedman restrict themselves to adding only those constituents that may lead to a difference in semantics. Thus, in (3) only the underlined constituents are in the chart. The &amp;quot;---&amp;quot; constituent is not. John loves Mary s/vp vp/np np vp\vp Combining &apos;madly&apos; with the rest would be impossible or lead to backtracking in the normal case. Here, the Pareschi &amp; Steedman algorithm starts looking for a constituent adjacent of contains an element X / vp as a leftmost category. If such a constituent can be found, it can be concluded that the rest of that constituent must (implicitly) be a v p , and thus the validity of combining vp\vp with this constituent has been established. Therefore, Pareschi &amp; Steedman are able to work with only a minimal amount of items in the chart. Both Wittenburg and Pareschi &amp; Steedman work with categorial grammars, which contain restricted versions of composition and raising. Although they can be processed efficiently, there is linguistic evidence that they are not fully adequate for analysis of such phenomena as coordination. Since atomic categories can in general not be raised in these grammars, sentence (2b) (in the category to be raised) cannot be derived. Furthermore, since composition is not generalized, as in Ades &amp; (1982), a sentence such as sold but Mary donated a book to the library would not be derivable. The possibilities for left-to-right, incremental, processing are also limited. Therefore, there is reason to look for a more flexible system, for which efficient parsing is still possible. - 20 - Strong Structural Completeness a sequence categories Xi .. reduces to Y, with semantics Y&apos;, a to Y, with semantics Y&apos;, for any bracketing of into constituents. 2. Structural Completeness (5) In the next section we present a grammatical calculus, which is more flexible than the systems considered by Wittenburg (1987) and Pareschi &amp; Steedman (1987), and therefore is attractive for linguistic purposes. At the same time, it offers a solution to the spurious ambiguity problem. Spurious ambiguity causes problems for parsing in the systems mentioned above, because there is no systematic relationship between syntactic structures and semantic representations. That is, there is no way to identify in advance, for a given sentence S. a proper subset of the set of all possible syntactic structures and associated semantic representations, for which it holds that it will contain all possible semantic representations of S. Consider now a grammar for which the following property holds: (4) Structural Completeness a sequence categories X1 to Y, there a to for any bracketing of X1 into (Moortgat, Structural complete grammars are interesting linguistically, since they are able to handle, for instance, all kinds of non-constituent conjunction, and also because they allow for strict left-to-right processing (see Moortgat, 1988). The latter observation has consequences for parsing as well,, since, if we can parse every sentence in a strict left-to-right manner (that is, we produce only strictly leftbranching syntax trees), the parsing algorithm can be greatly simplified. Notice, however, that such a parsing strategy is only valid, if we also guarantee that all possible readings of a sentence can be found in this way. Thus, instead of (4), we are looking for grammars with the following, slightly stronger, property: 2 Buszkowski (1988) provides a slightly different definition in terms of functor-argument structures. Grammars with this property, can potentially circumvent the spurious ambiguity problem, since for these grammars, we only have to inspect all left-branching syntax trees, to find all possible readings. This method will only fail if the set of leftbranching trees itself would contain spurious ambiguous derivations. In section 4 we will show that these can be eliminated from the calculus presented below. P-calculus The P(roduct)-calculus is a categorial grammar, based on Lambek (1958), which has the property of strong structural completeness. In Lambek (1958), the foundations of flexible categorial grammar are formulated in the form of a calculus for syntactic categories. Well-known categorial rules, such as application, composition and category-raising, are theorems of this calculus. A largely neglected aspect of this calculus, is the use of the product-operator. The calculus we present below, was developed as an alternative for Moortgat&apos;s (1988) M-system. The M-system is a subset of the Lambek-calculus, which uses, next to application, only a very general form of composition. Since it has no raising, it seems to be an attractive candidate for investigating the possibilities of left-associative parsing for categorial grammar. It is not completely satisfactory, however, since structural completeness is not fully guaranteed, and also, since it is unknown whether the strong structural completeness property holds for this system. In our calculus, we hope to overcome these problems, by using</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Antony Ades</author>
<author>Mark Steedman</author>
</authors>
<date>1982</date>
<journal>On the Order of Words, Linguistics &amp; Philosophy</journal>
<volume>4</volume>
<pages>517--518</pages>
<contexts>
<context position="3277" citStr="Ades &amp; Steedman (1982)" startWordPosition="514" endWordPosition="517"> structures, it also admits various other constituent structures. Therefore, the sentences in (2) can be considered to be ordinary instances of coordination (of two categories s/np and (vp/np)\vp, respectively). (2) a. John sold and Mary bought a book s/vp vp/np s/vp vp/np np s/np s/np left-directional functor respectively, looking for an argument of category B. - 19 - b. J. loves Mary madly and Sue wildly vp/np np vp\vp np vp\vp Mary madly np vp\vp (vp/np)\vp (vp/np)\vp A somewhat different type of argument for flexible phrase structure is based on the way humans process natural language. In Ades &amp; Steedman (1982) it is pointed out that humans process natural language in a left-to-right, incremental, manner. This processing aspect is accounted for in a flexible categorial system, where constituents can be built for any part of a sentence. Since syntactic rules operate in parallel with semantic interpretation rules, building a syntactic structure for an initial part of a sentence, implies that a corresponding semantic structure can also be constructed. These and other arguments suggest that there is no such thing as a fixed constituent structure, but that the order in which elements combine with eachoth</context>
<context position="6612" citStr="Ades &amp; Steedman (1982)" startWordPosition="1048" endWordPosition="1051">blished. Therefore, Pareschi &amp; Steedman are able to work with only a minimal amount of items in the chart. Both Wittenburg and Pareschi &amp; Steedman work with categorial grammars, which contain restricted versions of composition and raising. Although they can be processed efficiently, there is linguistic evidence that they are not fully adequate for analysis of such phenomena as coordination. Since atomic categories can in general not be raised in these grammars, sentence (2b) (in which the category np has to be raised) cannot be derived. Furthermore, since composition is not generalized, as in Ades &amp; Steedman (1982), a sentence such as John sold but Mary donated a book to the library would not be derivable. The possibilities for left-to-right, incremental, processing are also limited. Therefore, there is reason to look for a more flexible system, for which efficient parsing is still possible. - 20 - Strong Structural Completeness If a sequence of categories Xi .. Xn reduces to Y, with semantics Y&apos;, there is a reduction to Y, with semantics Y&apos;, for any bracketing of X1 ..Xn into constituents. 2. Structural Completeness (5) In the next section we present a grammatical calculus, which is more flexible than </context>
</contexts>
<marker>Ades, Steedman, 1982</marker>
<rawString>Ades, Antony &amp; Mark Steedman (1982), On the Order of Words, Linguistics &amp; Philosophy 4, 517-518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Buszkowski</author>
</authors>
<title>Generative Power of Categorial Grammars. In</title>
<date>1988</date>
<booktitle>Categorial Grammars and Natural Language Structures,</booktitle>
<pages>69--94</pages>
<editor>R. Oehrle, E. Bach, and D. Wheeler (eds.),</editor>
<location>Reidel, Dordrecht,</location>
<contexts>
<context position="8840" citStr="Buszkowski (1988)" startWordPosition="1412" endWordPosition="1413">uent conjunction, and also because they allow for strict left-to-right processing (see Moortgat, 1988). The latter observation has consequences for parsing as well,, since, if we can parse every sentence in a strict left-to-right manner (that is, we produce only strictly leftbranching syntax trees), the parsing algorithm can be greatly simplified. Notice, however, that such a parsing strategy is only valid, if we also guarantee that all possible readings of a sentence can be found in this way. Thus, instead of (4), we are looking for grammars with the following, slightly stronger, property: 2 Buszkowski (1988) provides a slightly different definition in terms of functor-argument structures. Grammars with this property, can potentially circumvent the spurious ambiguity problem, since for these grammars, we only have to inspect all left-branching syntax trees, to find all possible readings. This method will only fail if the set of leftbranching trees itself would contain spurious ambiguous derivations. In section 4 we will show that these can be eliminated from the calculus presented below. 3. The P-calculus The P(roduct)-calculus is a categorial grammar, based on Lambek (1958), which has the propert</context>
</contexts>
<marker>Buszkowski, 1988</marker>
<rawString>Buszkowski, Wojciech (1988). Generative Power of Categorial Grammars. In R. Oehrle, E. Bach, and D. Wheeler (eds.), Categorial Grammars and Natural Language Structures, Reidel, Dordrecht, 69-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans Zwarts</author>
</authors>
<title>Categoriale Grammatica en Algebraische Semantiek, Dissertation,</title>
<date>1986</date>
<institution>State University Groningen.</institution>
<contexts>
<context position="1361" citStr="Zwarts (1986)" startWordPosition="213" endWordPosition="214">s. Ambiguity (1) application : A/B B ==&gt; A B B\A ==&gt; A composition : A/B B/C ==&gt; A/C C\B B\A ==&gt; c\A raising A ==&gt; (B/A)\B A ==&gt; B/(A\B) With this grammar many alternative constituent structures for a sentence can be generated, even where this does not correspond to semantic ambiguities. From a linguistic point of view, this has many advantages. Various kind of arguments for giving up traditional conceptions of constituent structure can be given, but the most convincing and well-documented case in favour of flexible constituent structure is coordination (see Steedman (1985), Dowty (1988), and Zwarts (1986)). Categorial Grammars owe much of their popularity to the fact that they allow for various degrees of flexibility with respect to constituent structure. From a processing point of view, however, flexible categorial systems are problematic, since they introduce spurious ambiguity. The best known example of a flexible categorial grammar is a grammar containing the reduction rules application and composition, and the category changing rule raising 1 : * I would like to thank Esther Konig, Erik-Jan van der Linden, Michael Moortgat, Adriaan van Paassen and the participants of the Edinburgh Categor</context>
</contexts>
<marker>Zwarts, 1986</marker>
<rawString>Zwarts, Frans (1986), Categoriale Grammatica en Algebraische Semantiek, Dissertation, State University Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Type Raising, Functional Composition, and Non-constituent Conjunction. In</title>
<date>1988</date>
<booktitle>Categorial Grammars and Natural Language Structures,</booktitle>
<pages>153--197</pages>
<editor>R. Oehrle, E. Bach, and D. Wheeler (eds.),</editor>
<location>Reidel, Dordrecht,</location>
<contexts>
<context position="1342" citStr="Dowty (1988)" startWordPosition="210" endWordPosition="211">. 1. Flexibility vs. Ambiguity (1) application : A/B B ==&gt; A B B\A ==&gt; A composition : A/B B/C ==&gt; A/C C\B B\A ==&gt; c\A raising A ==&gt; (B/A)\B A ==&gt; B/(A\B) With this grammar many alternative constituent structures for a sentence can be generated, even where this does not correspond to semantic ambiguities. From a linguistic point of view, this has many advantages. Various kind of arguments for giving up traditional conceptions of constituent structure can be given, but the most convincing and well-documented case in favour of flexible constituent structure is coordination (see Steedman (1985), Dowty (1988), and Zwarts (1986)). Categorial Grammars owe much of their popularity to the fact that they allow for various degrees of flexibility with respect to constituent structure. From a processing point of view, however, flexible categorial systems are problematic, since they introduce spurious ambiguity. The best known example of a flexible categorial grammar is a grammar containing the reduction rules application and composition, and the category changing rule raising 1 : * I would like to thank Esther Konig, Erik-Jan van der Linden, Michael Moortgat, Adriaan van Paassen and the participants of th</context>
</contexts>
<marker>Dowty, 1988</marker>
<rawString>Dowty, David (1988), Type Raising, Functional Composition, and Non-constituent Conjunction. In R. Oehrle, E. Bach, and D. Wheeler (eds.), Categorial Grammars and Natural Language Structures, Reidel, Dordrecht, 153-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<title>The mathematics of sentence structure.</title>
<date>1958</date>
<journal>American Mathematical Monthly</journal>
<volume>65</volume>
<pages>154--170</pages>
<contexts>
<context position="2182" citStr="Lambek (1958)" startWordPosition="343" endWordPosition="344">egorial systems are problematic, since they introduce spurious ambiguity. The best known example of a flexible categorial grammar is a grammar containing the reduction rules application and composition, and the category changing rule raising 1 : * I would like to thank Esther Konig, Erik-Jan van der Linden, Michael Moortgat, Adriaan van Paassen and the participants of the Edinburgh Categorial Grammar Weekend, who made useful comments to earlier presentations of this material. All remaining errors and misconceptions are of course my own. 1 Throughout this paper we will be using the notation of Lambek (1958), in which A/B and B\A are a right-directional and a The standard assumption in generative grammar is that coordination always takes place between between constituents. Rightnode raising constructions and other instances of non-constituent conjunction are problematic, because it is not clear what the status of the coordinated elements in these constructions is. Flexible categorial grammar presents an elegant solution for such cases, since, next to canonical constituent structures, it also admits various other constituent structures. Therefore, the sentences in (2) can be considered to be ordin</context>
<context position="9417" citStr="Lambek (1958)" startWordPosition="1500" endWordPosition="1501">onger, property: 2 Buszkowski (1988) provides a slightly different definition in terms of functor-argument structures. Grammars with this property, can potentially circumvent the spurious ambiguity problem, since for these grammars, we only have to inspect all left-branching syntax trees, to find all possible readings. This method will only fail if the set of leftbranching trees itself would contain spurious ambiguous derivations. In section 4 we will show that these can be eliminated from the calculus presented below. 3. The P-calculus The P(roduct)-calculus is a categorial grammar, based on Lambek (1958), which has the property of strong structural completeness. In Lambek (1958), the foundations of flexible categorial grammar are formulated in the form of a calculus for syntactic categories. Well-known categorial rules, such as application, composition and category-raising, are theorems of this calculus. A largely neglected aspect of this calculus, is the use of the product-operator. The calculus we present below, was developed as an alternative for Moortgat&apos;s (1988) M-system. The M-system is a subset of the Lambek-calculus, which uses, next to application, only a very general form of composi</context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>Lambek, Joachim (1958), The mathematics of sentence structure. American Mathematical Monthly 65, 154-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Moortgat</author>
</authors>
<title>Lambek Categorial Grammar and the Autonomy Thesis. INL working papers 87-03.</title>
<date>1987</date>
<contexts>
<context position="8088" citStr="Moortgat, 1987" startWordPosition="1294" endWordPosition="1295">ms mentioned above, because there is no systematic relationship between syntactic structures and semantic representations. That is, there is no way to identify in advance, for a given sentence S. a proper subset of the set of all possible syntactic structures and associated semantic representations, for which it holds that it will contain all possible semantic representations of S. Consider now a grammar for which the following property holds: (4) Structural Completeness If a sequence of categories X1 Xn reduces to Y, there is a reduction to Y for any bracketing of X1 ..Xn into constituents. (Moortgat, 1987:5)2 Structural complete grammars are interesting linguistically, since they are able to handle, for instance, all kinds of non-constituent conjunction, and also because they allow for strict left-to-right processing (see Moortgat, 1988). The latter observation has consequences for parsing as well,, since, if we can parse every sentence in a strict left-to-right manner (that is, we produce only strictly leftbranching syntax trees), the parsing algorithm can be greatly simplified. Notice, however, that such a parsing strategy is only valid, if we also guarantee that all possible readings of a s</context>
</contexts>
<marker>Moortgat, 1987</marker>
<rawString>Moortgat, Michael (1987), Lambek Categorial Grammar and the Autonomy Thesis. INL working papers 87-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Moortgat</author>
</authors>
<title>Categorial Investigations : Logical and Linguistic Aspects of the Lambek Calculus. Dissertation,</title>
<date>1988</date>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="8325" citStr="Moortgat, 1988" startWordPosition="1327" endWordPosition="1328">le syntactic structures and associated semantic representations, for which it holds that it will contain all possible semantic representations of S. Consider now a grammar for which the following property holds: (4) Structural Completeness If a sequence of categories X1 Xn reduces to Y, there is a reduction to Y for any bracketing of X1 ..Xn into constituents. (Moortgat, 1987:5)2 Structural complete grammars are interesting linguistically, since they are able to handle, for instance, all kinds of non-constituent conjunction, and also because they allow for strict left-to-right processing (see Moortgat, 1988). The latter observation has consequences for parsing as well,, since, if we can parse every sentence in a strict left-to-right manner (that is, we produce only strictly leftbranching syntax trees), the parsing algorithm can be greatly simplified. Notice, however, that such a parsing strategy is only valid, if we also guarantee that all possible readings of a sentence can be found in this way. Thus, instead of (4), we are looking for grammars with the following, slightly stronger, property: 2 Buszkowski (1988) provides a slightly different definition in terms of functor-argument structures. Gr</context>
</contexts>
<marker>Moortgat, 1988</marker>
<rawString>Moortgat, Michael (1988), Categorial Investigations : Logical and Linguistic Aspects of the Lambek Calculus. Dissertation, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remo Pareschi</author>
<author>Mark Steedman</author>
</authors>
<title>A Lazy Way to Chart-Parse with Categorial Grammars.</title>
<date>1987</date>
<booktitle>Proceedings of the 25th Annual Meeting of the Association</booktitle>
<pages>81--88</pages>
<institution>for Computational Linguistics, Stanford University,</institution>
<contexts>
<context position="5099" citStr="Pareschi &amp; Steedman (1987)" startWordPosition="797" endWordPosition="800">n this paper, a categorial grammar with composition and heavily restricted versions of raising (for subject n p &apos;s only) is considered. Wittenburg proposes to eliminate spurious ambiguity by redefining composition. His predictive composition rules apply only in those cases where they are really needed to make a derivation possible. A disadvantage of this method, noticed by Wittenburg, is that one may have to add special predictive composition rules for all general combinatory rules in the grammar. Some careful rewriting of the original grammar has to take place, before things work as desired. Pareschi &amp; Steedman (1987) propose an efficient chart-parsing algorithm for categorial grammars with spurious ambiguity. Instead of the usual strategy, in which all possible subconstituents are added to the chart, Pareschi &amp; Steedman restrict themselves to adding only those constituents that may lead to a difference in semantics. Thus, in (3) only the underlined constituents are in the chart. The &amp;quot;---&amp;quot; constituent is not. (3) John loves Mary madly s/vp vp/np np vp\vp Combining &apos;madly&apos; with the rest would be impossible or lead to backtracking in the normal case. Here, the Pareschi &amp; Steedman algorithm starts looking for</context>
<context position="7286" citStr="Pareschi &amp; Steedman (1987)" startWordPosition="1162" endWordPosition="1165">a book to the library would not be derivable. The possibilities for left-to-right, incremental, processing are also limited. Therefore, there is reason to look for a more flexible system, for which efficient parsing is still possible. - 20 - Strong Structural Completeness If a sequence of categories Xi .. Xn reduces to Y, with semantics Y&apos;, there is a reduction to Y, with semantics Y&apos;, for any bracketing of X1 ..Xn into constituents. 2. Structural Completeness (5) In the next section we present a grammatical calculus, which is more flexible than the systems considered by Wittenburg (1987) and Pareschi &amp; Steedman (1987), and therefore is attractive for linguistic purposes. At the same time, it offers a solution to the spurious ambiguity problem. Spurious ambiguity causes problems for parsing in the systems mentioned above, because there is no systematic relationship between syntactic structures and semantic representations. That is, there is no way to identify in advance, for a given sentence S. a proper subset of the set of all possible syntactic structures and associated semantic representations, for which it holds that it will contain all possible semantic representations of S. Consider now a grammar for </context>
<context position="21701" citStr="Pareschi &amp; Steedman (1987)" startWordPosition="3676" endWordPosition="3679">uct-categories into product-free categories. (19) A/(B *C) =----&gt; (A/C)/B A number of such rules are needed to restore left-associativity. Next to syntactical additions, some modifications to the semantic part of the inference rule P had to be made, in order to cope with the polymorphic semantics proposed for coordination by Partee &amp; Rooth (1983). 7. Concluding remarks. The spurious ambiguity problem has been solved in this paper in a rather paradoxical manner. Whereas Wittenburg (1987) tries to do away with ambiguous phrase structure as much as possible (it only arises where you need it) and Pareschi &amp; Steedman (1987) use a chart parsing technique to recover implicit constituents efficiently, the strategy in this paper has been to go for complete ambiguity. It is in fact this massive ambiguity, which trivializes constituent structure to such an extent that one might as well ignore it, and choose a constituent structure that fits ones purposes best (left-branching in this case). It seems that as far as processing is concerned, the half-way flexible systems of Steedman (having generalized composition, and heavily restricted forms of raising) are in fact the hardest case. Simple AB-grammars are in all respect</context>
</contexts>
<marker>Pareschi, Steedman, 1987</marker>
<rawString>Pareschi, Remo and Mark Steedman (1987), A Lazy Way to Chart-Parse with Categorial Grammars. Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, Stanford University, 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Partee</author>
<author>Mats Rooth</author>
</authors>
<title>Generalized Conjunction and Type Ambiguity. In</title>
<date>1983</date>
<booktitle>Meaning, Use, and Interpretation of Language, de Gruyter,</booktitle>
<pages>361--383</pages>
<editor>R. }Merle, C. Schwarze and A. von Stechow (eds.)</editor>
<location>Berlin,</location>
<contexts>
<context position="21423" citStr="Partee &amp; Rooth (1983)" startWordPosition="3630" endWordPosition="3633">nable efficient left-associative parsing. We assumed that lexical categories would always be product-free, but this assumption no longer holds, if we add X \( X /X ) Our solution to this problem, is to add rules such as (19) to the grammar, which can transform certain product-categories into product-free categories. (19) A/(B *C) =----&gt; (A/C)/B A number of such rules are needed to restore left-associativity. Next to syntactical additions, some modifications to the semantic part of the inference rule P had to be made, in order to cope with the polymorphic semantics proposed for coordination by Partee &amp; Rooth (1983). 7. Concluding remarks. The spurious ambiguity problem has been solved in this paper in a rather paradoxical manner. Whereas Wittenburg (1987) tries to do away with ambiguous phrase structure as much as possible (it only arises where you need it) and Pareschi &amp; Steedman (1987) use a chart parsing technique to recover implicit constituents efficiently, the strategy in this paper has been to go for complete ambiguity. It is in fact this massive ambiguity, which trivializes constituent structure to such an extent that one might as well ignore it, and choose a constituent structure that fits ones</context>
</contexts>
<marker>Partee, Rooth, 1983</marker>
<rawString>Partee, Barbara and Mats Rooth (1983), Generalized Conjunction and Type Ambiguity. In R. }Merle, C. Schwarze and A. von Stechow (eds.) Meaning, Use, and Interpretation of Language, de Gruyter, Berlin, 361-383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Dependency and Coordination in the Grammar of Dutch and English.</title>
<date>1985</date>
<journal>Language</journal>
<volume>61</volume>
<pages>523--568</pages>
<contexts>
<context position="1328" citStr="Steedman (1985)" startWordPosition="208" endWordPosition="209">ssing is possible. 1. Flexibility vs. Ambiguity (1) application : A/B B ==&gt; A B B\A ==&gt; A composition : A/B B/C ==&gt; A/C C\B B\A ==&gt; c\A raising A ==&gt; (B/A)\B A ==&gt; B/(A\B) With this grammar many alternative constituent structures for a sentence can be generated, even where this does not correspond to semantic ambiguities. From a linguistic point of view, this has many advantages. Various kind of arguments for giving up traditional conceptions of constituent structure can be given, but the most convincing and well-documented case in favour of flexible constituent structure is coordination (see Steedman (1985), Dowty (1988), and Zwarts (1986)). Categorial Grammars owe much of their popularity to the fact that they allow for various degrees of flexibility with respect to constituent structure. From a processing point of view, however, flexible categorial systems are problematic, since they introduce spurious ambiguity. The best known example of a flexible categorial grammar is a grammar containing the reduction rules application and composition, and the category changing rule raising 1 : * I would like to thank Esther Konig, Erik-Jan van der Linden, Michael Moortgat, Adriaan van Paassen and the part</context>
</contexts>
<marker>Steedman, 1985</marker>
<rawString>Steedman, Mark (1985), Dependency and Coordination in the Grammar of Dutch and English. Language 61, 523-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kent Wittenburg</author>
</authors>
<title>Natural Language Parsing with Combinatory Categorial Grammars in a Graph-Unification-Based Formalism.</title>
<date>1986</date>
<institution>University of Texas at Austin.</institution>
<note>Ph. D. dissertation,</note>
<contexts>
<context position="4145" citStr="Wittenburg (1986)" startWordPosition="650" endWordPosition="651"> operate in parallel with semantic interpretation rules, building a syntactic structure for an initial part of a sentence, implies that a corresponding semantic structure can also be constructed. These and other arguments suggest that there is no such thing as a fixed constituent structure, but that the order in which elements combine with eachother is rather free. From a parsing point of view, however, flexibility appears to be a disadvantage. Flexible categorial grammars produce large numbers of, often semantically equivalent, derivations for a given phrase. This spurious ambiguity problem (Wittenburg (1986)) makes efficient processing of flexible categorial grammar problematic, since quite often there is an exponential growth of the number of possible derivations, relative to the length of the string to be parsed. There have been two proposals for eliminating spurious ambiguity from the grammar. The first is Wittenburg (1987). In this paper, a categorial grammar with composition and heavily restricted versions of raising (for subject n p &apos;s only) is considered. Wittenburg proposes to eliminate spurious ambiguity by redefining composition. His predictive composition rules apply only in those case</context>
</contexts>
<marker>Wittenburg, 1986</marker>
<rawString>Wittenburg, Kent (1986), Natural Language Parsing with Combinatory Categorial Grammars in a Graph-Unification-Based Formalism. Ph. D. dissertation, University of Texas at Austin.</rawString>
</citation>
<citation valid="false">
<pages>26</pages>
<marker></marker>
<rawString>- 26 -</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>