<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002995">
<title confidence="0.983693">
Realistic Grammar Error Simulation using Markov Logic
</title>
<author confidence="0.98546">
Sungjin Lee
</author>
<affiliation confidence="0.8327995">
Pohang University of Science and
Technology
</affiliation>
<address confidence="0.636379">
Pohang, Korea
</address>
<email confidence="0.995703">
junion@postech.ac.kr
</email>
<author confidence="0.868032">
Gary Geunbae Lee
</author>
<affiliation confidence="0.7430515">
Pohang University of Science and
Technology
</affiliation>
<address confidence="0.596352">
Pohang, Korea
</address>
<email confidence="0.996228">
gblee@postech.ac.kr
</email>
<sectionHeader confidence="0.997759" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9714251875">
The development of Dialog-Based Computer-
Assisted Language Learning (DB-CALL) sys-
tems requires research on the simulation of
language learners. This paper presents a new
method for generation of grammar errors, an
important part of the language learner simula-
tor. Realistic errors are generated via Markov
Logic, which provides an effective way to
merge a statistical approach with expert know-
ledge about the grammar error characteristics
of language learners. Results suggest that the
distribution of simulated grammar errors gen-
erated by the proposed model is similar to that
of real learners. Human judges also gave con-
sistently close judgments on the quality of the
real and simulated grammar errors.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999844126984127">
Second Language Acquisition (SLA) researchers
have claimed that feedback provided during con-
versational interaction facilitates the acquisition
process. Thus, interest in developing Dialog-
Based Computer Assisted Language Learning
(DB-CALL) systems is rapidly increasing. How-
ever, developing DB-CALL systems takes a long
time and entails a high cost in collecting learners’
data. Also, evaluating the systems is not a trivial
task because it requires numerous language
learners with a wide range of proficiency levels
as subjects.
While previous studies have considered user
simulation in the development and evaluation of
spoken dialog systems (Schatzmann et al., 2006),
they have not yet simulated grammar errors be-
cause those systems were assumed to be used by
native speakers, who normally produce few
grammar errors in utterances. However, as tele-
phone-based information access systems become
more commonly available to the general public,
the inability to deal with non-native speakers is
becoming a serious limitation since, at least for
some applications, (e.g. tourist information, le-
gal/social advice) non-native speakers represent
a significant portion of the everyday user popula-
tion. Thus, (Raux and Eskenazi, 2004) conducted
a study on adaptation of spoken dialog systems
to non-native users. In particular, DB-CALL sys-
tems should obviously deal with grammar errors
because language learners naturally commit nu-
merous grammar errors. Thus grammar error si-
mulation should be embedded in the user simula-
tion for the development and evaluation of such
systems.
In Foster’s (2007) pioneering work, she de-
scribed a procedure which automatically intro-
duces frequently occurring grammatical errors
into sentences to make ungrammatical training
data for a robust parser. However the algorithm
cannot be directly applied to grammar error gen-
eration for language learner simulation for sever-
al reasons. First, it either introduces one error per
sentence or none, regardless of how many words
of the sentence are likely to generate errors.
Second, it determines which type of error it will
create only by relying on the relative frequencies
of error types and their relevant parts of speech.
This, however, can result in unrealistic errors. As
exemplified in Table 1, when the algorithm tries
to create an error by deleting a word, it would
probably omit the word ‘go’ because verb is one
of the most frequent parts of speech omitted re-
sulting in an unrealistic error like the first simu-
lated output. However, Korean/Japanese lan-
guage learners of English tend to make subject-
verb agreement errors, omission errors of the
preposition of prepositional verbs, and omission
errors of articles because their first language
does not have similar grammar rules so that they
may be slow on the uptake of such constructs.
Thus, they often commit errors like the second
simulated output.
</bodyText>
<page confidence="0.970932">
81
</page>
<note confidence="0.939391">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 81–84,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<subsectionHeader confidence="0.934385">
Input sentence
</subsectionHeader>
<bodyText confidence="0.9117876">
He wants to go to a movie theater
Unrealistic simulated output
He wants to to a movie theater
Realistic simulated output
He want go to movie theater
</bodyText>
<tableCaption confidence="0.994045">
Table 1: Examples of simulated outputs
</tableCaption>
<bodyText confidence="0.999988611111111">
This paper develops an approach to statistical
grammar error simulation that can incorporate
this type of knowledge about language learners’
error characteristics and shows that it does in-
deed result in realistic grammar errors. The ap-
proach is based on Markov logic, a representa-
tion language that combines probabilistic graphi-
cal models and first-order logic (Richardson and
Domingos, 2006). Markov logic enables concise
specification of very complex models. Efficient
open-source Markov logic learning and inference
algorithms were used to implement our solution.
We begin by describing the overall process of
grammar error simulation and then briefly re-
viewing the necessary background in Markov
logic. We then describe our Markov Logic Net-
work (MLN) for grammar error simulation. Fi-
nally, we present our experiments and results.
</bodyText>
<sectionHeader confidence="0.9825435" genericHeader="introduction">
2 Overall process of grammar error si-
mulation
</sectionHeader>
<bodyText confidence="0.9995484">
The task of grammar error simulation is to gen-
erate an ill-formed sentence when given a well-
formed input sentence. The generation procedure
involves three steps: 1) Generating probability
over error types for each word of the well-
formed input sentence through MLN inference 2)
Determining an error type by sampling the gen-
erated probability for each word 3) Creating an
ill-formed output sentence by realizing the cho-
sen error types (Figure 1).
</bodyText>
<sectionHeader confidence="0.977142" genericHeader="method">
3 Markov Logic
</sectionHeader>
<bodyText confidence="0.999383238095238">
Markov logic is a probabilistic extension of finite
first-order logic (Richardson and Domingos,
2006). An MLN is a set of weighted first-order
clauses. Together with a set of constants, it de-
fines a Markov network with one node per
ground atom and one feature per ground clause.
The weight of a feature is the weight of the first-
order clause that originated it. The probability of
a state x in such a network is given by P(x) =
(1/Z) exp (Zi wifi(x)), where Z is a normali-
zation constant, wi is the weight of the ith clause,
fi = 1 if the ith clause is true, and fi = 0 oth-
erwise.
Markov logic makes it possible to compactly
specify probability distributions over complex
relational domains. We used the learning and
inference algorithms provided in the open-source
Alchemy package (Kok et al., 2006). In particu-
lar, we performed inference using the belief
propagation algorithm (Pearl, 1988), and genera-
tive weight learning.
</bodyText>
<sectionHeader confidence="0.99823" genericHeader="method">
4 An MLN for Grammar Error Simula-
tion
</sectionHeader>
<bodyText confidence="0.99994225">
This section presents our MLN implementation
which consists of three components: 1) Basic
formulas based on parts of speech, which are
comparable to Foster’s method 2) Analytic for-
mulas drawn from expert knowledge obtained by
error analysis on a learner corpus 3) Error limit-
ing formulas that penalize statistical model’s
over-generation of nonsense errors.
</bodyText>
<subsectionHeader confidence="0.994747">
4.1 Basic formulas
</subsectionHeader>
<bodyText confidence="0.993564214285714">
Error patterns obtained by error analysis, which
might capture a lack or an over-generalization of
knowledge of a particular construction, cannot
explain every error that learners commit. Be-
cause an error can take the form of a perfor-
mance slip which can randomly occur due to
carelessness or tiredness, more general formulas
are needed as a default case. The basic formulas
are represented by the simple rule:
 PosTag(s, i, +pt)  ErrorType(s, i, +et)
where all free variables are implicitly universally
quantified. The “+pt, +et ” notation signifies
that the MLN contains an instance of this rule for
each (part of speech, error type) pair. The evi-
</bodyText>
<figureCaption confidence="0.988824">
Figure 1: An example process of grammar error simulation
</figureCaption>
<page confidence="0.991728">
82
</page>
<bodyText confidence="0.999843">
dence predicate in this case is PosTag(s, i, pt),
which is true iff the ith position of the sentence s
has the part of speech pt. The query predicate is
ErrorType(s, i, et). It is true iff the ith position
of the sentence s has the error type et, and infer-
ring it returns the probability that the word at
position i would commit an error of type et.
</bodyText>
<subsectionHeader confidence="0.989583">
4.2 Analytic formulas
</subsectionHeader>
<bodyText confidence="0.999931055555555">
On top of the basic formulas, analytic formulas
add concrete knowledge of realistic error charac-
teristics of language learners. Error analysis and
linguistic differences between the first language
and the second language can identify various
error sources for each error type. We roughly
categorize the error sources into three groups for
explanation: 1) Over-generalization of the rules
of the second language 2) Lack of knowledge of
some rules of the second language 3) Applying
rules and forms of the first language into the
second language.
Often, English learners commit pluralization
error with irregular nouns. This is because they
over-generalize the pluralization rule, i.e. attach-
ing ‘s/es’, so that they apply the rule even to ir-
regular nouns such as ‘fish’ and ‘feet’ etc. This
characteristic is captured by the simple formula:
</bodyText>
<listItem confidence="0.9642865">
• IrregularPluralNoun(s, i) n PosTag(s, i, NNS)
 ErrorType(s, i, N_NUM_SUB)
</listItem>
<bodyText confidence="0.999984571428571">
where IrregularPluralNoun(s, i) is true iff the
ith word of the sentence s is an irregular plural
and N_NUM_SUB is the abbreviation for substi-
tution by noun number error.
One trivial error caused by a lack of know-
ledge of the second language is using the singu-
lar noun form for weekly events:
</bodyText>
<listItem confidence="0.964891">
• Word(s, i — 1, on) n DayNoun(s, i)
</listItem>
<bodyText confidence="0.985498857142857">
A PosTag(s, i, NNS) ErrorType(s, i, N_NUM_SUB)
where Word(s, i — 1, on) is true iff the i — 1th
word is ‘on’ and DayNoun(s, i) is true iff the
ith word of the sentence s is a noun describing
day like Sunday(s). Another example is use of
plurals behind ‘every’ due to the ignorance that a
noun modified by ‘every’ should be singular:
</bodyText>
<listItem confidence="0.990859">
• Word(s, di, every) n DeterminerRel(s, di, ni)
 ErrorType(s, ni, N_NUM_SUB)
</listItem>
<bodyText confidence="0.999965571428571">
where DeterminerRel(s, di, ni) is true iff the
dith word is the determiner of the nith word.
An example of errors by applying the rules of
the first language is that Korean/Japanese often
allows omission of the subject of a sentence; thus,
they easily commit the subject omission error.
The following formula is for the case:
</bodyText>
<listItem confidence="0.843602">
• Subject(s, i) ErrorType(s, i, N_LXC_DEL)
</listItem>
<bodyText confidence="0.998767">
where Subject(s, i) is true iff the ith word is the
subject and N_LXC_DEL is the abbreviation for
deletion by noun lexis error.1
</bodyText>
<subsectionHeader confidence="0.990093">
4.3 Error limiting formulas
</subsectionHeader>
<bodyText confidence="0.999943428571428">
A number of elementary formulas explicitly
stated as hard formulas prevent the MLN from
generating improbable errors that might result
from over-generations of the statistical model.
For example, a verb complement error should not
have a probability at the words that are not com-
plements of a verb:
</bodyText>
<listItem confidence="0.90661">
• ! VerbComplement(s, vi, ci)
</listItem>
<equation confidence="0.492722">
 ! ErrorType(s, ci,V_CMP_SUB).
</equation>
<bodyText confidence="0.999969285714286">
where “!” denotes logically ‘not’ and “.” at the
end signifies that it is a hard formula. Hard formu-
las are given maximum weight during inference.
VerbComplement(s, vi, ci) is true iff the ci th
word is a complement of the verb at the vith po-
sition and V_CMP_SUB is the abbreviation for
substitution by verb complement error.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999893379310345">
Experiments used the NICT JLE Corpus, which
is speech samples from an English oral profi-
ciency interview test, the ACTFL-ALC Standard
Speaking Test (SST). 167 of the files are error
annotated. The error tagset consists of 47 tags
that are described in Izumi (2005). We appended
structural type of errors (substitution, addition,
deletion) to the original error types because
structural type should be determined when creat-
ing an error. For example, V_TNS_SUB consists
of the original error type V_TNS (verb tense) and
structural type SUB (substitution). Level-
specific language learner simulation was accom-
plished by dividing the 167 error annotated files
into 3 level groups: Beginner(level1-4), Interme-
diate(level5-6), Advanced(level7-9).
The grammar error simulation was compared
with real learners’ errors and the baseline model
using only basic formulas comparable to Foster’s
algorithm, with 10-fold cross validations per-
formed for each group. The validation results
were added together across the rounds to com-
pare the number of simulated errors with the
number of real errors. Error types that occurred
less than 20 times were excluded to improve re-
liability. Result graphs suggest that the distribu-
tion of simulated grammar errors generated by
the proposed model using all formulas is similar
to that of real learners for all level groups and the
</bodyText>
<footnote confidence="0.467689">
1 Because space is limited, all formulas can be found at
http://isoft.postech.ac.kr/ges/grm_err_sim.mln
</footnote>
<page confidence="0.993358">
83
</page>
<figure confidence="0.8043475">
Beginner Level:
DK,(Real  ||Proposed)=0.075, DK,(Real  ||Baseline)=0.092
</figure>
<figureCaption confidence="0.9970995">
Figure 2: Comparison between the distributions of the
real and simulated data
</figureCaption>
<bodyText confidence="0.999843904761905">
proposed model outperforms the baseline model
using only the basic formulas. The Kullback-
Leibler divergences, a measure of the difference
between two probability distributions, were also
measured for quantitative comparison. For all
level groups, the Kullback-Leibler divergence of
the proposed model from the real is less than that
of the baseline model (Figure 2).
Two human judges verified the overall realism
of the simulated errors. They evaluated 100 ran-
domly chosen sentences consisting of 50 sen-
tences each from the real and simulated data. The
sequence of the test sentences was mixed so that
the human judges did not know whether the
source of the sentence was real or simulated.
They evaluated sentences with a two-level scale
(0: Unrealistic, 1: Realistic). The result shows
that the inter evaluator agreement (kappa) is
moderate and that both judges gave relatively
close judgments on the quality of the real and
simulated data (Table 2).
</bodyText>
<note confidence="0.609577">
Human 1 Human 2 Average Kappa
</note>
<table confidence="0.9866885">
Real 0.84 0.8 0.82 0.46
Simulated 0.8 0.8 0.8 0.5
</table>
<tableCaption confidence="0.995106">
Table 2: Human evaluation results
</tableCaption>
<sectionHeader confidence="0.932168" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999963842105263">
This paper introduced a somewhat new research
topic, grammar error simulation. Expert know-
ledge of error characteristics was imported to
statistical modeling using Markov logic, which
provides a theoretically sound way of encoding
knowledge into probabilistic first order logic.
Results indicate that our method can make an
error distribution more similar to the real error
distribution than the baseline and that the quality
of simulated sentences is relatively close to that
of real sentences in the judgment of human eva-
luators. Our future work includes adding more
expert knowledge through error analysis to in-
crementally improve the performance. Further-
more, actual development and evaluation of a
DB-CALL system will be arranged so that we
may investigate how much the cost of collecting
data and evaluation would be reduced by using
language learner simulation.
</bodyText>
<sectionHeader confidence="0.977402" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.974629666666667">
This research was supported by the MKE (Ministry of
Knowledge Economy), Korea, under the ITRC (In-
formation Technology Research Center) support pro-
gram supervised by the IITA (Institute for Informa-
tion Technology Advancement) (IITA-2009-C1090-
0902-0045).
</bodyText>
<sectionHeader confidence="0.997273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998618210526316">
Foster, J. 2007. Treebanks Gone Bad: Parser evalua-
tion and retraining using a treebank of ungrammat-
ical sentences. IJDAR, 10(3-4), 129-145.
Izumi, E et al. 2005. Error Annotation for Corpus of
Japanese Learner English. In Proc. International
Workshop on Linguistically Interpreted Corpora
Kok, S. et al. 2006. The Alchemy system for statistic-
al relational AI. http://alchemy.cs.washington.edu/.
Pearl, J. 1988. Probabilistic Reasoning in Intelligent
Systems Morgan Kaufmann.
Raux, A. and Eskenazi, M. 2004. Non-Native Users in
the Let&apos;s Go!! Spoken Dialogue System: Dealing
with Linguistic Mismatch, HLT/NAACL.
Richardson, M. and Domingos, P. 2006. Markov logic
networks. Machine Learning, 62(1):107-136.
Schatzmann, J. et al. 2006. A survey of statistical user
simulation techniques for reinforcement-learning
of dialogue management strategies, The Know-
ledge Engineering Review, Vol. 21:2, 97–126
</reference>
<figure confidence="0.98568675">
Intermediate Level:
DK,(Real  ||Proposed)=0.075, DK,(Real  ||Baseline)=0.142
Advanced Level:
DK,(Real  ||Proposed)=0.068, DK,(Real  ||Baseline)=0.122
</figure>
<page confidence="0.957398">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.665263">
<title confidence="0.988644">Realistic Grammar Error Simulation using Markov Logic</title>
<author confidence="0.940684">Sungjin Lee</author>
<affiliation confidence="0.986967">Pohang University of Science and Technology</affiliation>
<address confidence="0.989746">Pohang, Korea</address>
<email confidence="0.987125">junion@postech.ac.kr</email>
<author confidence="0.790571">Gary Geunbae Lee</author>
<affiliation confidence="0.9870575">Pohang University of Science and Technology</affiliation>
<address confidence="0.986453">Pohang, Korea</address>
<email confidence="0.992161">gblee@postech.ac.kr</email>
<abstract confidence="0.999084823529412">The development of Dialog-Based Computer- Assisted Language Learning (DB-CALL) systems requires research on the simulation of language learners. This paper presents a new method for generation of grammar errors, an important part of the language learner simulator. Realistic errors are generated via Markov Logic, which provides an effective way to merge a statistical approach with expert knowledge about the grammar error characteristics of language learners. Results suggest that the distribution of simulated grammar errors generated by the proposed model is similar to that of real learners. Human judges also gave consistently close judgments on the quality of the real and simulated grammar errors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Foster</author>
</authors>
<title>Treebanks Gone Bad: Parser evaluation and retraining using a treebank of ungrammatical sentences.</title>
<date>2007</date>
<journal>IJDAR,</journal>
<volume>10</volume>
<issue>3</issue>
<pages>129--145</pages>
<marker>Foster, 2007</marker>
<rawString>Foster, J. 2007. Treebanks Gone Bad: Parser evaluation and retraining using a treebank of ungrammatical sentences. IJDAR, 10(3-4), 129-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
</authors>
<title>Error Annotation for Corpus of Japanese Learner English.</title>
<date>2005</date>
<booktitle>In Proc. International Workshop on Linguistically Interpreted Corpora</booktitle>
<contexts>
<context position="11097" citStr="Izumi (2005)" startWordPosition="1792" endWordPosition="1793">P_SUB). where “!” denotes logically ‘not’ and “.” at the end signifies that it is a hard formula. Hard formulas are given maximum weight during inference. VerbComplement(s, vi, ci) is true iff the ci th word is a complement of the verb at the vith position and V_CMP_SUB is the abbreviation for substitution by verb complement error. 5 Experiments Experiments used the NICT JLE Corpus, which is speech samples from an English oral proficiency interview test, the ACTFL-ALC Standard Speaking Test (SST). 167 of the files are error annotated. The error tagset consists of 47 tags that are described in Izumi (2005). We appended structural type of errors (substitution, addition, deletion) to the original error types because structural type should be determined when creating an error. For example, V_TNS_SUB consists of the original error type V_TNS (verb tense) and structural type SUB (substitution). Levelspecific language learner simulation was accomplished by dividing the 167 error annotated files into 3 level groups: Beginner(level1-4), Intermediate(level5-6), Advanced(level7-9). The grammar error simulation was compared with real learners’ errors and the baseline model using only basic formulas compar</context>
</contexts>
<marker>Izumi, 2005</marker>
<rawString>Izumi, E et al. 2005. Error Annotation for Corpus of Japanese Learner English. In Proc. International Workshop on Linguistically Interpreted Corpora</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kok</author>
</authors>
<title>The Alchemy system for statistical relational AI.</title>
<date>2006</date>
<note>http://alchemy.cs.washington.edu/.</note>
<marker>Kok, 2006</marker>
<rawString>Kok, S. et al. 2006. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<date>1988</date>
<booktitle>Probabilistic Reasoning in Intelligent Systems</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="6414" citStr="Pearl, 1988" startWordPosition="1002" endWordPosition="1003">he weight of a feature is the weight of the firstorder clause that originated it. The probability of a state x in such a network is given by P(x) = (1/Z) exp (Zi wifi(x)), where Z is a normalization constant, wi is the weight of the ith clause, fi = 1 if the ith clause is true, and fi = 0 otherwise. Markov logic makes it possible to compactly specify probability distributions over complex relational domains. We used the learning and inference algorithms provided in the open-source Alchemy package (Kok et al., 2006). In particular, we performed inference using the belief propagation algorithm (Pearl, 1988), and generative weight learning. 4 An MLN for Grammar Error Simulation This section presents our MLN implementation which consists of three components: 1) Basic formulas based on parts of speech, which are comparable to Foster’s method 2) Analytic formulas drawn from expert knowledge obtained by error analysis on a learner corpus 3) Error limiting formulas that penalize statistical model’s over-generation of nonsense errors. 4.1 Basic formulas Error patterns obtained by error analysis, which might capture a lack or an over-generalization of knowledge of a particular construction, cannot expla</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>M Eskenazi</author>
</authors>
<title>Non-Native Users in the Let&apos;s Go!! Spoken Dialogue System: Dealing with Linguistic Mismatch,</title>
<date>2004</date>
<location>HLT/NAACL.</location>
<contexts>
<context position="2195" citStr="Raux and Eskenazi, 2004" startWordPosition="315" endWordPosition="318"> and evaluation of spoken dialog systems (Schatzmann et al., 2006), they have not yet simulated grammar errors because those systems were assumed to be used by native speakers, who normally produce few grammar errors in utterances. However, as telephone-based information access systems become more commonly available to the general public, the inability to deal with non-native speakers is becoming a serious limitation since, at least for some applications, (e.g. tourist information, legal/social advice) non-native speakers represent a significant portion of the everyday user population. Thus, (Raux and Eskenazi, 2004) conducted a study on adaptation of spoken dialog systems to non-native users. In particular, DB-CALL systems should obviously deal with grammar errors because language learners naturally commit numerous grammar errors. Thus grammar error simulation should be embedded in the user simulation for the development and evaluation of such systems. In Foster’s (2007) pioneering work, she described a procedure which automatically introduces frequently occurring grammatical errors into sentences to make ungrammatical training data for a robust parser. However the algorithm cannot be directly applied to</context>
</contexts>
<marker>Raux, Eskenazi, 2004</marker>
<rawString>Raux, A. and Eskenazi, M. 2004. Non-Native Users in the Let&apos;s Go!! Spoken Dialogue System: Dealing with Linguistic Mismatch, HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="4574" citStr="Richardson and Domingos, 2006" startWordPosition="694" endWordPosition="697"> August 2009. c�2009 ACL and AFNLP Input sentence He wants to go to a movie theater Unrealistic simulated output He wants to to a movie theater Realistic simulated output He want go to movie theater Table 1: Examples of simulated outputs This paper develops an approach to statistical grammar error simulation that can incorporate this type of knowledge about language learners’ error characteristics and shows that it does indeed result in realistic grammar errors. The approach is based on Markov logic, a representation language that combines probabilistic graphical models and first-order logic (Richardson and Domingos, 2006). Markov logic enables concise specification of very complex models. Efficient open-source Markov logic learning and inference algorithms were used to implement our solution. We begin by describing the overall process of grammar error simulation and then briefly reviewing the necessary background in Markov logic. We then describe our Markov Logic Network (MLN) for grammar error simulation. Finally, we present our experiments and results. 2 Overall process of grammar error simulation The task of grammar error simulation is to generate an ill-formed sentence when given a wellformed input sentenc</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Richardson, M. and Domingos, P. 2006. Markov logic networks. Machine Learning, 62(1):107-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
</authors>
<title>A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies,</title>
<date>2006</date>
<journal>The Knowledge Engineering Review,</journal>
<volume>21</volume>
<pages>97--126</pages>
<marker>Schatzmann, 2006</marker>
<rawString>Schatzmann, J. et al. 2006. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies, The Knowledge Engineering Review, Vol. 21:2, 97–126</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>