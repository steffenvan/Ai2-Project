<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000091">
<title confidence="0.994977">
Characterizing and Recognizing Spoken Corrections in
Human-Computer Dialogue
</title>
<author confidence="0.84474">
Gina-Anne Levow
</author>
<affiliation confidence="0.775612">
MIT Al Laboratory
</affiliation>
<address confidence="0.859707">
Room 769, 545 Technology Sq
Cambridge, MA 02139
</address>
<email confidence="0.918543">
gina©ai.mit.edu
</email>
<sectionHeader confidence="0.99591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992308">
Miscommunication in speech recognition sys-
tems is unavoidable, but a detailed character-
ization of user corrections will enable speech
systems to identify when a correction is taking
place and to more accurately recognize the con-
tent of correction utterances. In this paper we
investigate the adaptations of users when they
encounter recognition errors in interactions with
a voice-in/voice-out spoken language system. In
analyzing more than 300 pairs of original and re-
peat correction utterances, matched on speaker
and lexical content, we found overall increases
in both utterance and pause duration from orig-
inal to correction. Interestingly, corrections of
misrecognition errors (CME) exhibited signifi-
cantly heightened pitch variability, while cor-
rections of rejection errors (CRE) showed only a
small but significant decrease in pitch minimum.
CME&apos;s demonstrated much greater increases in
measures of duration and pitch variability than
CRE&apos;s. These contrasts allow the development
of decision trees which distinguish CME&apos;s from
CRE&apos;s and from original inputs at 70-75% ac-
curacy based on duration, pitch, and amplitude
features.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932826086957">
The frequent recognition errors which plague
speech recognition systems present a signifi-
cant barrier to widespread acceptance of this
technology. The difficulty of correcting sys-
tem misrecognitions is directly correlated with
user assessments of system quality. The in-
creased probability of recognition errors imme-
diately after an error compounds this prob-
lem. Thus, it becomes crucially important
to characterize the differences between origi-
nal utterances and user corrections of system
recognition failures both in order to recognize
when a user attempts a correction, indicating a
prior recognition error, and to improve recogni-
tion accuracy on these problematic utterances.
Analysis of data drawn from a field trial of
a telephone-based voice-in/voice-out conversa-
tional system demonstrates significant differ-
ences between original inputs and corrections in
measures of duration, pause, and pitch. These
differences in turn aid in the development of de-
cision trees which distinguish between new in-
put and user corrections.
</bodyText>
<sectionHeader confidence="0.998314" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.999880647058824">
We begin by describing the framework in which
the utterances used for these experiments were
collected. We then describe the acoustic fea-
tures used in the data analysis and decision tree
design and motivate this choice by considering
some related work. A report of the results of the
descriptive analysis follows and demonstrates
the significant differences among original inputs
and different types of corrections. We then turn
to the issues involved in the actual construction
and assessment of decision trees which identify
spoken corrections. The best of these classi-
fiers achieves 75% success rates. Finally, we
conclude with a plan to use these results to
improve recognition accuracy on spoken correc-
tions and ease recovery from human-computer
miscommunication.
</bodyText>
<sectionHeader confidence="0.971856" genericHeader="method">
3 System, Subjects, and Errors
</sectionHeader>
<bodyText confidence="0.999803125">
The utterances used in the design and analy-
sis of the decision tree classifiers were drawn
from approximately 60 hours of user interac-
tions in a field trial of the Sun Microsystems
SpeechActs system (Yankelovich et al., 1995).
SpeechActs provides a voice-only interface to
common desktop applications such as e-mail,
calendar, weather reports, stock quotes, and
</bodyText>
<page confidence="0.997121">
736
</page>
<bodyText confidence="0.746803578947369">
time and currency conversions. The system
allows conversational interactions with speech
recognition provided by BBN&apos;s HARK recog-
nizer and speech synthesis through Centigram&apos;s
TruVoice system. A sample interaction appears
below.
SpeechActs: You have 14 new messages and
47 old messages. The first new
message is from Eric Baatz
regarding &amp;quot;report completed.&amp;quot;
User: Let me hear it.
SpeechActs: &amp;quot;The first draft is ready for
your comments. Eric.&amp;quot;
User: Reply and include current message.
SpeechActs: Preparing message to Eric Baatz.
Begin recording after the tone.
When finished, pause several seconds
User: Eric, I&apos;ll get those to you
this afternoon.
</bodyText>
<listItem confidence="0.90356975">
SpeechActs: Say cancel, send, or review.
User: Send.
SpeechActs: Message sent. What now?
User: Next message.
SpeechActs: New message two from Stuart
Adams, subject &amp;quot;Can we meet at
3:00 today?&amp;quot;
User: Switch to calendar... 1
</listItem>
<bodyText confidence="0.962956097560976">
The field trial involved a group of nineteen
subjects. Four of the participants were members
of the system development staff, fourteen were
volunteers drawn from Sun Microsystems&apos; staff,
and a final class of subjects consisted of one-
time guest users There were three female and
sixteen male subjects.
All interactions with the system were
recorded and digitized in standard telephone
audio quality format at 8kHz sampling in 8-bit
mu-law encoding during the conversation. In
addition, speech recognition results, parser re-
sults, and synthesized responses were logged. A
paid assistant then produced a correct verbatim
transcript of all user utterances and, by compar-
ing the transcription to the recognition results,
labeled each utterance with one of four accuracy
codes as described below.
OK: recognition correct; action correct
Error Minor: recognition not exact; action correct
Error: recognition incorrect; action incorrect
Rejection: no recognition result; no action
Overall there were 7752 user utterances
recorded, of which 1961 resulted in a label of ei-
ther &apos;Error&apos; or &apos;Rejection&apos;, giving an error rate
of 25%. 1250 utterances, almost two-thirds of
the errors, produced outright rejections, while
706 errors were substitution misrecognitions.
The remainder of the errors were due to sys-
tem crashes or parser errors. The probability
of experiencing a recognition failure after a cor-
rect recognition was 16%, but immediately after
an incorrect recognition it was 44%, 2.75 times
greater. This increase in error likelihood sug-
gests a change in speaking style which diverges
from the recognizer&apos;s model. The remainder
•of this paper will identify common acoustic
changes which characterize this error correction
speaking style. This description leads to the de-
velopment of a decision tree classifier which can
label utterances as corrections or original input.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999928714285715">
Since full voice-in/voice-out spoken language
systems have only recently been developed, lit-
tle work has been done on error correction di-
alogs in this context. Two areas of related re-
search that have been investigated are the iden-
tification of self-repairs and disfluencies, where
the speaker self-interrupts to change an utter-
ance in progress, and some preliminary efforts
in the study of corrections in speech input.
In analyzing and identifying self-repairs,
(Bear et al., 1992) and (Heeman and Allen,
1994) found that the most effective methods
relied on identifying shared textual regions be-
tween the reparandum and the repair. However,
these techniques are limited to those instances
where a reliable recognition string is available;
in general, that is not the case for most speech
recognition systems currently available. Alter-
native approaches described in (Nakatani and
Hirschberg, 1994) and (Shriberg et al., 1997),
have emphasized acoustic-prosodic cues, includ-
ing duration, pitch, and amplitude as discrimi-
nating features.
The few studies that have focussed on spoken
corrections of computer misrecognitions, (Ovi-
att et al., 1996) and (Swerts and Ostendorf,
1995), also found significant effects of duration,
and in Oviatt et al., pause insertion and length-
</bodyText>
<page confidence="0.872836">
737
</page>
<bodyText confidence="0.953253428571429">
i.3000Rr
ening played a role. However, in only one of
these studies was input &amp;quot;conversational&amp;quot;, the
other was a form-filling application, and nei-
ther involved spoken system responses, relying
instead on visual displays for feedback, with po-
tential impact on speaking style.
</bodyText>
<sectionHeader confidence="0.863556" genericHeader="method">
5 Error Data, Features, and
Examples
</sectionHeader>
<bodyText confidence="0.999986571428572">
For these experiments, we selected pairs of ut-
terances: the first (original) utterance is the
first attempt by the user to enter an input or
a query; the second (repeat) follows a system
recognition error, either misrecognition or re-
jection, and tries to correct the mistake in the
same words as the original. For example,
</bodyText>
<listItem confidence="0.866555818181818">
SYSTEM SAID: Please say mail, calendar,
weather, stock quotes or
start over to begin again.
USER SAID:MAIL
SYSTEM HEARD: MAIL
CODE: OK
SYSTEM SAID: Switching to mail.
Your first message is ..
USER SAID:Read message four eight nine
SYSTEM HEARD: &amp;quot;nothing&amp;quot;
CODE: Rejection
SYSTEM SAID: Sorry?
USER SAID:Read message four eight nine
SYSTEM HEARD: &amp;quot;nothing&amp;quot;
CODE: Rejection
SYSTEM SAID: Still no luck. Speak clearly,
but don&apos;t over emphasize .
USER SAID:Go to message four eight nine
SYSTEM HEARD: Go to message four
please umm
CODE: ERROR
SYSTEM SAID: I heard goto new message 4.
</listItem>
<bodyText confidence="0.985211222222222">
In total, there were 302 of these original-repeat
pairs: 214 resulting from rejections, and 88 from
misrecognit ions .
Following (Oviatt et al., 1996), (Shriberg et
al., 1997), and (Ostendorf et al., 1996), we
coded a set of acoustic-prosodic features to de-
scribe the utterances. These features fall into
four main groups: durational, pause, pitch, and
amplitude. We further selected variants of these
feature classes that could be scored automati-
cally, or at least mostly automatically with some
.
ead— far eight Mint
,500fts 0.000ft ,1500as &apos;,2000ra — psoo. i3000.r
...
.
Ir../ nennege far 191. .
;fro
</bodyText>
<figureCaption confidence="0.778127">
Figure 1: A lexically matched pair where the
repeat (bottom) has an 18% increase in total
duration and a 400% increase in pause duration.
</figureCaption>
<bodyText confidence="0.9993656">
minor hand-adjustment. We hoped that these
features would be available during the recog-
nition process so that ultimately the original-
repeat correction contrasts would be identified
automatically.
</bodyText>
<subsectionHeader confidence="0.971636">
5.1 Duration
</subsectionHeader>
<bodyText confidence="0.999996117647059">
The basic duration measure is total utterance
duration. This value is obtained through a two-
step procedure. First we perform an automatic
forced alignment of the utterance to the ver-
batim transcription text using the OGI CSLU
CSLUsh Toolkit (Colton, 1995). Then the
alignment is inspected and, if necessary, ad-
justed by hand to correct for any errors, such
as those caused by extraneous background noise
or non-speech sounds. A typical alignment ap-
pears in Figure 1. In addition to the sim-
ple measure of total duration in milliseconds,
a number of derived measures also prove useful.
Some examples of such measures are speaking
rate in terms of syllables per second and a ra-
tio of the actual utterance duration to the mean
duration for that type of utterance.
</bodyText>
<subsectionHeader confidence="0.991524">
5.2 Pause
</subsectionHeader>
<bodyText confidence="0.999887">
A pause is any region of silence internal to an
utterance and longer than 10 milliseconds in du-
ration. Silences preceding unvoiced stops and
affricates were not coded as pauses due to the
difficulty of identifying the onset of consonants
of these classes. Pause-based features include
number of pauses, average pause duration, total
pause duration, and silence as a percentage of
total utterance duration. An example of pause
</bodyText>
<equation confidence="0.257036">
4000ms 4500ea ,2000•3
</equation>
<page confidence="0.901717">
738
</page>
<figureCaption confidence="0.9985945">
Figure 2: Contrasting Falling (top) and Rising
(bottom) Pitch Contours
</figureCaption>
<bodyText confidence="0.90185">
insertion and lengthening appear in Figure 1.
</bodyText>
<subsectionHeader confidence="0.975968">
5.3 Pitch
</subsectionHeader>
<bodyText confidence="0.999940909090909">
To derive pitch features, we first apply the
FO (fundamental frequency) analysis function
from the Entropic ESPS Waves+ system (Se-
crest and Doddington. 1993) to produce a basic
pitch track. Most of the related work reported
above had found relationships between the mag-
nitude of pitch features and discourse function
rather than presence of accent type, used more
heavily by (Pierrehumbert and Hirschberg,
1990), (Hirschberg and Litman, 1993). Thus,
we chose to concentrate on pitch features of the
former type. A trained analyst examines the
pitch track to remove any points of doubling or
halving due to pitch tracker error, non-speech
sounds, and excessive glottalization of &gt; 5 sam-
ple points. We compute several derived mea-
sures using simple algorithms to obtain FO max-
imum, FO minimum, FO range, final FO contour,
slope of maximum pitch rise, slope of maximum
pitch fall, and sum of the slopes of the steep-
est rise and fall. Figure 2 depicts a basic pitch
contour.
</bodyText>
<subsectionHeader confidence="0.972025">
5.4 Amplitude
</subsectionHeader>
<bodyText confidence="0.999971333333333">
Amplitude, measuring the loudness of an utter-
ance, is also computed using the ESPS Waves+
system. Mean amplitudes are computed over
all voiced regions with amplitude &gt; 30dB. Am-
plitude features include utterance mean ampli-
tude, mean amplitude of last voiced region, am-
plitude of loudest region, standard deviation,
and difference from mean to last and maximum
to last.
</bodyText>
<sectionHeader confidence="0.993683" genericHeader="method">
6 Descriptive Acoustic Analysis
</sectionHeader>
<bodyText confidence="0.999824">
Using the features described above, we per-
formed some initial simple statistical analyses
to identify those features which would be most
useful in distinguishing original inputs from re-
peat corrections, and corrections of rejection er-
rors (CRE) from corrections of misrecognition
errors (CME). The results for the most inter-
esting features, duration, pause, and pitch, are
described below.
</bodyText>
<subsectionHeader confidence="0.994867">
6.1 Duration
</subsectionHeader>
<bodyText confidence="0.99991">
Total utterance duration is significantly greater
for corrections than for original inputs. In ad-
dition, increases in correction duration relative
to mean duration for the utterance prove signif-
icantly greater for CME&apos;s than for CRE&apos;s.
</bodyText>
<subsectionHeader confidence="0.994563">
6.2 Pause
</subsectionHeader>
<bodyText confidence="0.9999285">
Similarly to utterance duration, total pause
length increases from original to repeat. For
original-repeat pairs where at least one pause
appears, paired t-test on log-transformed data
reveal significantly greater pause durations for
corrections than for original inputs.
</bodyText>
<subsectionHeader confidence="0.997962">
6.3 Pitch
</subsectionHeader>
<bodyText confidence="0.9998833125">
While no overall trends reached significance for
pitch measures, CRE&apos;s and CME&apos;s, when con-
sidered separately, did reveal some interesting
contrasts between corrections and original in-
puts within each subset and between the two
types of corrections. Specifically, male speakers
showed a small but significant decrease in pitch
minimum for CRE&apos;s.
CME&apos;s produced two unexpected results.
First they displayed a large and significant in-
crease in pitch variability from original to re-
peat as measured the slope of the steepest rise,
while CRE&apos;s exhibited a corresponding decrease
rising slopes. In addition, they also showed sig-
nificant increases in steepest rise measures when
compared with CRE&apos;s.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999941857142857">
The acoustic-prosodic measures we have exam-
ined indicate substantial differences not only be-
tween original inputs and repeat corrections,
but also between the two correction classes,
those in response to rejections and those in re-
sponse to misrecognitions. Let us consider the
relation of these results to those of related work
</bodyText>
<figure confidence="0.992929857142857">
reply
message
to
&apos;reply
on•
message
to
</figure>
<page confidence="0.995157">
739
</page>
<bodyText confidence="0.99966">
and produce a more clear overall picture of spo-
ken correction behavior in human-computer di-
alogue.
</bodyText>
<subsectionHeader confidence="0.6730365">
7.1 Duration and Pause:
Conversational to Clear Speech
</subsectionHeader>
<bodyText confidence="0.9999671">
Durational measures, particularly increases in
duration, appear as a common phenomenon
among several analyses of speaking style
[ (Oviatt et al., 1996), (Ostendorf et al.,
1996), (Shriberg et al., 1997)1. Similarly, in-
creases in number and duration of silence re-
gions are associated with disfluencies (Shriberg
et al., 1997), self-repairs (Nakatani and
Hirschberg, 1994), and more careful speech
(Ostendorf et al., 1996) as well as with spo-
ken corrections (Oviatt et al., 1996). These
changes in our correction data fit smoothly into
an analysis of error corrections as invoking shifts
from conversational to more &amp;quot;clear&amp;quot; or &amp;quot;careful&amp;quot;
speaking styles. Thus, we observe a parallel be-
tween the changes in duration and pause from
original to repeat correction, described as con-
versational to clear in (Oviatt et al., 1996),
and from casual conversation to carefully read
speech in (Ostendorf et al., 1996).
</bodyText>
<subsectionHeader confidence="0.995196">
7.2 Pitch
</subsectionHeader>
<bodyText confidence="0.999979541666667">
Pitch, on the other hand, does not fit smoothly
into this picture of corrections taking on clear
speech characteristics similar to those found in
carefully read speech. First of all, (Ostendorf
et al., 1996) did not find any pitch measures
to be useful in distinguishing speaking mode
on the continuum from a rapid conversational
style to a carefully read style. Second, pitch
features seem to play little role in corrections of
rejections. Only a small decrease in pitch min-
imum was found, and this difference can easily
be explained by the combination of two simple
trends. First, there was a decrease in the num-
ber of final rising contours, and second, there
were increases in utterance length, that, even
under constant rates of declination, will yield
lower pitch minima. Third, this feature pro-
duces a divergence in behavior of CME&apos;s from
CRE&apos;s.
While CRE&apos;s exhibited only the change in
pitch minimum described above, corrections of
misrecognition errors displayed some dramatic
changes in pitch behavior. Since we observed
that simple measures of pitch maximum, min-
imum, and range failed to capture even the
basic contrast of rising versus falling contour,
we extended our feature set with measures of
slope of rise and slope of fall. These mea-
sures may be viewed both as an attempt to
create a simplified form of Taylor&apos;s rise-fall-
continuation model (Taylor, 1995) and as an
attempt to provide quantitative measures of
pitch accent. Measures of pitch accent and con-
tour had shown some utility in identifying cer-
tain discourse relations [ (Pierrehumbert and
Hirschberg, 1990), (Hirschberg and Litman,
1993). Although changes in pitch maxima and
minima were not significant in themselves, the
increases in rise slopes for CME&apos;s in contrast to
flattening of rise slopes in CRE&apos;s combined to
form a highly significant measure. While not
defining a specific overall contour as in (Tay-
lor, 1995), this trend clearly indicates increased
pitch accentuation. Future work will seek to de-
scribe not only the magnitude, but also the form
of these pitch accents and their relation to those
outlined in (Pierrehumbert and Hirschberg,
1990).
</bodyText>
<subsectionHeader confidence="0.997329">
7.3 Summary
</subsectionHeader>
<bodyText confidence="0.999934636363636">
It is clear that many of the adaptations asso-
ciated with error corrections can be attributed
to a general shift from conversational to clear
speech articulation. However, while this model
may adequately describe corrections of rejection
errors, corrections of misrecognition errors ob-
viously incorporate additional pitch accent fea-
tures to indicate their discourse function. These
contrasts will be shown to ease the identification
of these utterances as corrections and to high-
light their contrastive intent.
</bodyText>
<sectionHeader confidence="0.934324" genericHeader="method">
8 Decision Tree Experiments
</sectionHeader>
<bodyText confidence="0.999765846153846">
The next step was to develop predictive classi-
fiers of original vs repeat corrections and CME&apos;s
vs CRE&apos;s informed by the descriptive analysis
above. We chose to implement these classifiers
with decision trees (using Quinlan&apos;s (Quinlan,
1992) C4.5) trained on a subset of the original-
repeat pair data. Decision trees have two fea-
tures which make them desirable for this task.
First, since they can ignore irrelevant attributes,
they will not be misled by meaningless noise in
one or more of the 38 duration, pause, pitch,
and amplitude features coded. Since these fea-
tures are probably not all important, it is desir-
</bodyText>
<page confidence="0.988247">
740
</page>
<bodyText confidence="0.999972333333333">
able to use a technique which can identify those
which are most relevant. Second, decision trees
are highly intelligible; simple inspection of trees
can identify which rules use which attributes
to arrive at a classification, unlike more opaque
machine learning techniques such as neural nets.
</bodyText>
<sectionHeader confidence="0.9680255" genericHeader="method">
8.1 Decision Trees: Results &amp;
Discussion
</sectionHeader>
<bodyText confidence="0.999996064516129">
The first set of decision tree trials attempted
to classify original and repeat correction utter-
ances, for both correction types. We used a set
of 38 attributes: 18 based on duration and pause
measures, 6 on amplitude, five on pitch height
and range, and 13 on pitch contour. Trials were
made with each of the possible subsets of these
four feature classes on over 600 instances with
seven-way cross-validation. The best results,
33% error, were obtained using attributes from
all sets. Duration measures were most impor-
tant, providing an improvement of at least 10%
in accuracy over all trees without duration fea-
tures.
The next set of trials dealt with the two er-
ror correction classes separately. One focussed
on distinguishing CME&apos;s from CRE&apos;s, while
the other concentrated on differentiating CNIEs
alone from original inputs. The test attributes
and trial structure were the same as above. The
best error rate for the CME vs. CRE classi-
fier was 30.7%, again achieved with attributes
from all classes, but depending most heavily on
durational features. Finally the most success-
ful decision trees were those separating original
inputs from CME&apos;s. These trees obtained an
accuracy rate of 75% (25% error) using simi-
lar attributes to the previous trials. The most
important splits were based on pitch slope and
durational features. An exemplar of this type
of decision tree in shown below.
</bodyText>
<figure confidence="0.72254645">
normdurationl &gt; 0.2335 : r (39.0/4.9)
normduration1 &lt;= 0.2335 :
normduration2 &lt;= 20.471 :
normduration3 &lt;= 1.0116 :
normduration1 &gt; -0.0023 : 0 (51/3)
normdurationl &lt;= -0.0023 :
I pitchslope &gt; 0.265 : o (19/4))
I pitchslope &lt;= 0.265 :
II pitchlastmin &lt;= 25.2214:r(11/2)
II pitchlastmin &gt; 25.2214:
III minslope &lt;= -0.221:r(18/3)
I III minslope &gt; -0.221:o(15/3)
normduration3 &gt; 1.0116 :
Inormduration4 &gt; 0.0615 : r (7.0/1.3)
Inormduration4 &lt;= 0.0615 :
IInormduration3 &lt;= 1.0277 : r (8.0/3.3)
Ilnormduration3 &gt; 1.0277 : o (19.0/8.0)
normduration2 &gt; 20.471 :
I pitchslope &lt;= 0.281 : r (24.0/3.7)
I pitchslope &gt; 0.281 : o (7.0/2.4)
</figure>
<bodyText confidence="0.999974181818182">
These decision tree results in conjunction
with the earlier descriptive analysis provide ev-
idence of strong contrasts between original in-
puts and repeat corrections, as well as between
the two classes of corrections. They suggest that
different error rates after correct and after erro-
neous recognitions are due to a change in speak-
ing style that we have begun to model.
In addition, the results on corrections of mis-
recognition errors are particularly encouraging.
In current systems, all recognition results are
treated as new input unless a rejection occurs.
User corrections of system misrecognitions can
currently only be identified by complex reason-
ing requiring an accurate transcription. In con-
trast, the method described here provides a way
to use acoustic features such as duration, pause,
and pitch variability to identify these particu-
larly challenging error corrections without strict
dependence on a perfect textual transcription
of the input and with relatively little computa-
tional effort.
</bodyText>
<sectionHeader confidence="0.987806" genericHeader="conclusions">
9 Conclusions &amp;Future Work
</sectionHeader>
<bodyText confidence="0.999879214285714">
Using acoustic-prosodic features such as dura-
tion, pause, and pitch variability to identify er-
ror corrections in spoken dialog systems shows
promise for resolving this knotty problem. We
further plan to explore the use of more accu-
rate characterization of the contrasts between
original and correction inputs to adapt standard
recognition procedures to improve recognition
accuracy in error correction interactions. Help-
ing to identify and successfully recognize spoken
corrections will improve the ease of recovering
from human-computer miscommunication and
will lower this hurdle to widespread acceptance
of spoken language systems.
</bodyText>
<page confidence="0.996664">
741
</page>
<sectionHeader confidence="0.988912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997865753846154">
J. Bear, J. Dowding, and E. Shriberg. 1992. In-
tegrating multiple knowledge sources for de-
tection and correction of repairs in human-
computer dialog. In Proceedings of the A CL,
pages 56-63, University of Delaware, Newark,
DE.
D. Colton. 1995. Course manual for CSE 553
speech recognition laboratory. Technical Re-
port CSLU-007-95, Center for Spoken Lan-
guage Understanding, Oregon Graduate In-
stitute, July.
P.A. Heeman and J. Allen. 1994. Detecting and
correcting speech repairs. In Proceedings of
the ACL, pages 295-302, New Mexico State
University, Las Cruces, NM.
Julia Hirschberg and Diane Litman. 1993.
Empirical studies on the disambiguation
of cue phrases. Computational linguistics,
19(3):501-530.
C.H. Nakatani and J. Hirschberg. 1994. A
corpus-based study of repair cues in sponta-
neous speech. Journal of the Acoustic Society
of America, 95(3):1603-1616.
M. Ostendorf, B. Byrne, M. Bacchiani,
M. Finke, A. Gunawardana, K. Ross,
S. Roweis, E. Shribergand D. Talkin,
A. Waibel, B. Wheatley, and T. Zeppenfeld.
1996. Modeling systematic variations in pro-
nunciation via a language-dependent hidden
speaking mode. In Proceedings of the In-
ternational Conference on Spoken Language
Processing. supplementary paper.
S.L. Oviatt, G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate
speech during human-computer error resolu-
tion. In Proceedings of the International Con-
ference on Spoken Language Processing, vol-
ume 2, pages 801-804.
Janet Pierrehumbert and Julia Hirschberg.
1990. The meaning of intonational contours
in the interpretation of discourse. In P. Co-
hen, J. Morgan, and M. Pollack, editors, In-
tentions in Communication, pages 271-312.
MIT Press, Cambridge, MA.
J.R. Quinlan. 1992. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann.
B. G. Secrest and G. R. Doddington. 1993. An
integrated pitch tracking algorithm for speech
systems. In ICASSP 1993.
E. Shriberg, R. Bates, and A. Stolcke. 1997.
A prosody-only decision-tree model for dis-
fluency detection. In Eurospeech &apos;97.
M. Swerts and M. Ostendorf. 1995. Discourse
prosody in human-machine interactions. In
Proceedings of the ECSA Tutorial and Re-
search Workshop on Spoken Dialog Systems
- Theories and Applications.
Paul Taylor. 1995. The rise/fall/continuation
model of intonation. Speech Communication,
15:169-186.
N. Yankelovich, G. Levow, and M. Marx. 1995.
Designing SpeechActs: Issues in speech user
interfaces. In CHI &apos;95 Conference on Human
Factors in Computing Systems, Denver, CO,
May.
</reference>
<page confidence="0.997283">
742
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.629995">
<title confidence="0.922931">Characterizing and Recognizing Spoken Corrections in Human-Computer Dialogue</title>
<author confidence="0.806678">Gina-Anne Levow</author>
<affiliation confidence="0.995064">Laboratory</affiliation>
<address confidence="0.9774155">Room 769, 545 Technology Sq Cambridge, MA 02139</address>
<email confidence="0.999902">gina©ai.mit.edu</email>
<abstract confidence="0.995714807692308">Miscommunication in speech recognition systems is unavoidable, but a detailed characterization of user corrections will enable speech systems to identify when a correction is taking place and to more accurately recognize the content of correction utterances. In this paper we investigate the adaptations of users when they encounter recognition errors in interactions with a voice-in/voice-out spoken language system. In analyzing more than 300 pairs of original and repeat correction utterances, matched on speaker and lexical content, we found overall increases in both utterance and pause duration from original to correction. Interestingly, corrections of misrecognition errors (CME) exhibited significantly heightened pitch variability, while corrections of rejection errors (CRE) showed only a small but significant decrease in pitch minimum. CME&apos;s demonstrated much greater increases in measures of duration and pitch variability than CRE&apos;s. These contrasts allow the development of decision trees which distinguish CME&apos;s from CRE&apos;s and from original inputs at 70-75% accuracy based on duration, pitch, and amplitude features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bear</author>
<author>J Dowding</author>
<author>E Shriberg</author>
</authors>
<title>Integrating multiple knowledge sources for detection and correction of repairs in humancomputer dialog.</title>
<date>1992</date>
<booktitle>In Proceedings of the A CL,</booktitle>
<pages>56--63</pages>
<institution>University of Delaware,</institution>
<location>Newark, DE.</location>
<contexts>
<context position="6773" citStr="Bear et al., 1992" startWordPosition="1011" endWordPosition="1014">ption leads to the development of a decision tree classifier which can label utterances as corrections or original input. 4 Related Work Since full voice-in/voice-out spoken language systems have only recently been developed, little work has been done on error correction dialogs in this context. Two areas of related research that have been investigated are the identification of self-repairs and disfluencies, where the speaker self-interrupts to change an utterance in progress, and some preliminary efforts in the study of corrections in speech input. In analyzing and identifying self-repairs, (Bear et al., 1992) and (Heeman and Allen, 1994) found that the most effective methods relied on identifying shared textual regions between the reparandum and the repair. However, these techniques are limited to those instances where a reliable recognition string is available; in general, that is not the case for most speech recognition systems currently available. Alternative approaches described in (Nakatani and Hirschberg, 1994) and (Shriberg et al., 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. The few studies that have focussed on spoken </context>
</contexts>
<marker>Bear, Dowding, Shriberg, 1992</marker>
<rawString>J. Bear, J. Dowding, and E. Shriberg. 1992. Integrating multiple knowledge sources for detection and correction of repairs in humancomputer dialog. In Proceedings of the A CL, pages 56-63, University of Delaware, Newark, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Colton</author>
</authors>
<title>Course manual for CSE 553 speech recognition laboratory.</title>
<date>1995</date>
<tech>Technical Report CSLU-007-95,</tech>
<institution>Center for Spoken Language Understanding, Oregon Graduate Institute,</institution>
<contexts>
<context position="9983" citStr="Colton, 1995" startWordPosition="1521" endWordPosition="1522">91. . ;fro Figure 1: A lexically matched pair where the repeat (bottom) has an 18% increase in total duration and a 400% increase in pause duration. minor hand-adjustment. We hoped that these features would be available during the recognition process so that ultimately the originalrepeat correction contrasts would be identified automatically. 5.1 Duration The basic duration measure is total utterance duration. This value is obtained through a twostep procedure. First we perform an automatic forced alignment of the utterance to the verbatim transcription text using the OGI CSLU CSLUsh Toolkit (Colton, 1995). Then the alignment is inspected and, if necessary, adjusted by hand to correct for any errors, such as those caused by extraneous background noise or non-speech sounds. A typical alignment appears in Figure 1. In addition to the simple measure of total duration in milliseconds, a number of derived measures also prove useful. Some examples of such measures are speaking rate in terms of syllables per second and a ratio of the actual utterance duration to the mean duration for that type of utterance. 5.2 Pause A pause is any region of silence internal to an utterance and longer than 10 millisec</context>
</contexts>
<marker>Colton, 1995</marker>
<rawString>D. Colton. 1995. Course manual for CSE 553 speech recognition laboratory. Technical Report CSLU-007-95, Center for Spoken Language Understanding, Oregon Graduate Institute, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
<author>J Allen</author>
</authors>
<title>Detecting and correcting speech repairs.</title>
<date>1994</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>295--302</pages>
<institution>Mexico State University,</institution>
<location>New</location>
<contexts>
<context position="6802" citStr="Heeman and Allen, 1994" startWordPosition="1016" endWordPosition="1019">opment of a decision tree classifier which can label utterances as corrections or original input. 4 Related Work Since full voice-in/voice-out spoken language systems have only recently been developed, little work has been done on error correction dialogs in this context. Two areas of related research that have been investigated are the identification of self-repairs and disfluencies, where the speaker self-interrupts to change an utterance in progress, and some preliminary efforts in the study of corrections in speech input. In analyzing and identifying self-repairs, (Bear et al., 1992) and (Heeman and Allen, 1994) found that the most effective methods relied on identifying shared textual regions between the reparandum and the repair. However, these techniques are limited to those instances where a reliable recognition string is available; in general, that is not the case for most speech recognition systems currently available. Alternative approaches described in (Nakatani and Hirschberg, 1994) and (Shriberg et al., 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. The few studies that have focussed on spoken corrections of computer misre</context>
</contexts>
<marker>Heeman, Allen, 1994</marker>
<rawString>P.A. Heeman and J. Allen. 1994. Detecting and correcting speech repairs. In Proceedings of the ACL, pages 295-302, New Mexico State University, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<pages>19--3</pages>
<contexts>
<context position="11519" citStr="Hirschberg and Litman, 1993" startWordPosition="1768" endWordPosition="1771">ce duration. An example of pause 4000ms 4500ea ,2000•3 738 Figure 2: Contrasting Falling (top) and Rising (bottom) Pitch Contours insertion and lengthening appear in Figure 1. 5.3 Pitch To derive pitch features, we first apply the FO (fundamental frequency) analysis function from the Entropic ESPS Waves+ system (Secrest and Doddington. 1993) to produce a basic pitch track. Most of the related work reported above had found relationships between the magnitude of pitch features and discourse function rather than presence of accent type, used more heavily by (Pierrehumbert and Hirschberg, 1990), (Hirschberg and Litman, 1993). Thus, we chose to concentrate on pitch features of the former type. A trained analyst examines the pitch track to remove any points of doubling or halving due to pitch tracker error, non-speech sounds, and excessive glottalization of &gt; 5 sample points. We compute several derived measures using simple algorithms to obtain FO maximum, FO minimum, FO range, final FO contour, slope of maximum pitch rise, slope of maximum pitch fall, and sum of the slopes of the steepest rise and fall. Figure 2 depicts a basic pitch contour. 5.4 Amplitude Amplitude, measuring the loudness of an utterance, is also</context>
<context position="17150" citStr="Hirschberg and Litman, 1993" startWordPosition="2658" endWordPosition="2661">changes in pitch behavior. Since we observed that simple measures of pitch maximum, minimum, and range failed to capture even the basic contrast of rising versus falling contour, we extended our feature set with measures of slope of rise and slope of fall. These measures may be viewed both as an attempt to create a simplified form of Taylor&apos;s rise-fallcontinuation model (Taylor, 1995) and as an attempt to provide quantitative measures of pitch accent. Measures of pitch accent and contour had shown some utility in identifying certain discourse relations [ (Pierrehumbert and Hirschberg, 1990), (Hirschberg and Litman, 1993). Although changes in pitch maxima and minima were not significant in themselves, the increases in rise slopes for CME&apos;s in contrast to flattening of rise slopes in CRE&apos;s combined to form a highly significant measure. While not defining a specific overall contour as in (Taylor, 1995), this trend clearly indicates increased pitch accentuation. Future work will seek to describe not only the magnitude, but also the form of these pitch accents and their relation to those outlined in (Pierrehumbert and Hirschberg, 1990). 7.3 Summary It is clear that many of the adaptations associated with error cor</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational linguistics, 19(3):501-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Nakatani</author>
<author>J Hirschberg</author>
</authors>
<title>A corpus-based study of repair cues in spontaneous speech.</title>
<date>1994</date>
<journal>Journal of the Acoustic Society of America,</journal>
<pages>95--3</pages>
<contexts>
<context position="7189" citStr="Nakatani and Hirschberg, 1994" startWordPosition="1073" endWordPosition="1076">encies, where the speaker self-interrupts to change an utterance in progress, and some preliminary efforts in the study of corrections in speech input. In analyzing and identifying self-repairs, (Bear et al., 1992) and (Heeman and Allen, 1994) found that the most effective methods relied on identifying shared textual regions between the reparandum and the repair. However, these techniques are limited to those instances where a reliable recognition string is available; in general, that is not the case for most speech recognition systems currently available. Alternative approaches described in (Nakatani and Hirschberg, 1994) and (Shriberg et al., 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. The few studies that have focussed on spoken corrections of computer misrecognitions, (Oviatt et al., 1996) and (Swerts and Ostendorf, 1995), also found significant effects of duration, and in Oviatt et al., pause insertion and length737 i.3000Rr ening played a role. However, in only one of these studies was input &amp;quot;conversational&amp;quot;, the other was a form-filling application, and neither involved spoken system responses, relying instead on visual displays for </context>
<context position="14993" citStr="Nakatani and Hirschberg, 1994" startWordPosition="2306" endWordPosition="2309">onsider the relation of these results to those of related work reply message to &apos;reply on• message to 739 and produce a more clear overall picture of spoken correction behavior in human-computer dialogue. 7.1 Duration and Pause: Conversational to Clear Speech Durational measures, particularly increases in duration, appear as a common phenomenon among several analyses of speaking style [ (Oviatt et al., 1996), (Ostendorf et al., 1996), (Shriberg et al., 1997)1. Similarly, increases in number and duration of silence regions are associated with disfluencies (Shriberg et al., 1997), self-repairs (Nakatani and Hirschberg, 1994), and more careful speech (Ostendorf et al., 1996) as well as with spoken corrections (Oviatt et al., 1996). These changes in our correction data fit smoothly into an analysis of error corrections as invoking shifts from conversational to more &amp;quot;clear&amp;quot; or &amp;quot;careful&amp;quot; speaking styles. Thus, we observe a parallel between the changes in duration and pause from original to repeat correction, described as conversational to clear in (Oviatt et al., 1996), and from casual conversation to carefully read speech in (Ostendorf et al., 1996). 7.2 Pitch Pitch, on the other hand, does not fit smoothly into thi</context>
</contexts>
<marker>Nakatani, Hirschberg, 1994</marker>
<rawString>C.H. Nakatani and J. Hirschberg. 1994. A corpus-based study of repair cues in spontaneous speech. Journal of the Acoustic Society of America, 95(3):1603-1616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>B Byrne</author>
<author>M Bacchiani</author>
<author>M Finke</author>
<author>A Gunawardana</author>
<author>K Ross</author>
<author>S Roweis</author>
<author>E Shribergand D Talkin</author>
<author>A Waibel</author>
<author>B Wheatley</author>
<author>T Zeppenfeld</author>
</authors>
<title>Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing. supplementary</booktitle>
<pages>paper.</pages>
<contexts>
<context position="8982" citStr="Ostendorf et al., 1996" startWordPosition="1360" endWordPosition="1363">o mail. Your first message is .. USER SAID:Read message four eight nine SYSTEM HEARD: &amp;quot;nothing&amp;quot; CODE: Rejection SYSTEM SAID: Sorry? USER SAID:Read message four eight nine SYSTEM HEARD: &amp;quot;nothing&amp;quot; CODE: Rejection SYSTEM SAID: Still no luck. Speak clearly, but don&apos;t over emphasize . USER SAID:Go to message four eight nine SYSTEM HEARD: Go to message four please umm CODE: ERROR SYSTEM SAID: I heard goto new message 4. In total, there were 302 of these original-repeat pairs: 214 resulting from rejections, and 88 from misrecognit ions . Following (Oviatt et al., 1996), (Shriberg et al., 1997), and (Ostendorf et al., 1996), we coded a set of acoustic-prosodic features to describe the utterances. These features fall into four main groups: durational, pause, pitch, and amplitude. We further selected variants of these feature classes that could be scored automatically, or at least mostly automatically with some . ead— far eight Mint ,500fts 0.000ft ,1500as &apos;,2000ra — psoo. i3000.r ... . Ir../ nennege far 191. . ;fro Figure 1: A lexically matched pair where the repeat (bottom) has an 18% increase in total duration and a 400% increase in pause duration. minor hand-adjustment. We hoped that these features would be av</context>
<context position="14800" citStr="Ostendorf et al., 1996" startWordPosition="2278" endWordPosition="2281">s not only between original inputs and repeat corrections, but also between the two correction classes, those in response to rejections and those in response to misrecognitions. Let us consider the relation of these results to those of related work reply message to &apos;reply on• message to 739 and produce a more clear overall picture of spoken correction behavior in human-computer dialogue. 7.1 Duration and Pause: Conversational to Clear Speech Durational measures, particularly increases in duration, appear as a common phenomenon among several analyses of speaking style [ (Oviatt et al., 1996), (Ostendorf et al., 1996), (Shriberg et al., 1997)1. Similarly, increases in number and duration of silence regions are associated with disfluencies (Shriberg et al., 1997), self-repairs (Nakatani and Hirschberg, 1994), and more careful speech (Ostendorf et al., 1996) as well as with spoken corrections (Oviatt et al., 1996). These changes in our correction data fit smoothly into an analysis of error corrections as invoking shifts from conversational to more &amp;quot;clear&amp;quot; or &amp;quot;careful&amp;quot; speaking styles. Thus, we observe a parallel between the changes in duration and pause from original to repeat correction, described as conver</context>
</contexts>
<marker>Ostendorf, Byrne, Bacchiani, Finke, Gunawardana, Ross, Roweis, Talkin, Waibel, Wheatley, Zeppenfeld, 1996</marker>
<rawString>M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke, A. Gunawardana, K. Ross, S. Roweis, E. Shribergand D. Talkin, A. Waibel, B. Wheatley, and T. Zeppenfeld. 1996. Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode. In Proceedings of the International Conference on Spoken Language Processing. supplementary paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>G Levow</author>
<author>M MacEarchern</author>
<author>K Kuhn</author>
</authors>
<title>Modeling hyperarticulate speech during human-computer error resolution.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>801--804</pages>
<contexts>
<context position="7435" citStr="Oviatt et al., 1996" startWordPosition="1108" endWordPosition="1112">he most effective methods relied on identifying shared textual regions between the reparandum and the repair. However, these techniques are limited to those instances where a reliable recognition string is available; in general, that is not the case for most speech recognition systems currently available. Alternative approaches described in (Nakatani and Hirschberg, 1994) and (Shriberg et al., 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. The few studies that have focussed on spoken corrections of computer misrecognitions, (Oviatt et al., 1996) and (Swerts and Ostendorf, 1995), also found significant effects of duration, and in Oviatt et al., pause insertion and length737 i.3000Rr ening played a role. However, in only one of these studies was input &amp;quot;conversational&amp;quot;, the other was a form-filling application, and neither involved spoken system responses, relying instead on visual displays for feedback, with potential impact on speaking style. 5 Error Data, Features, and Examples For these experiments, we selected pairs of utterances: the first (original) utterance is the first attempt by the user to enter an input or a query; the seco</context>
<context position="8927" citStr="Oviatt et al., 1996" startWordPosition="1351" endWordPosition="1354">SYSTEM HEARD: MAIL CODE: OK SYSTEM SAID: Switching to mail. Your first message is .. USER SAID:Read message four eight nine SYSTEM HEARD: &amp;quot;nothing&amp;quot; CODE: Rejection SYSTEM SAID: Sorry? USER SAID:Read message four eight nine SYSTEM HEARD: &amp;quot;nothing&amp;quot; CODE: Rejection SYSTEM SAID: Still no luck. Speak clearly, but don&apos;t over emphasize . USER SAID:Go to message four eight nine SYSTEM HEARD: Go to message four please umm CODE: ERROR SYSTEM SAID: I heard goto new message 4. In total, there were 302 of these original-repeat pairs: 214 resulting from rejections, and 88 from misrecognit ions . Following (Oviatt et al., 1996), (Shriberg et al., 1997), and (Ostendorf et al., 1996), we coded a set of acoustic-prosodic features to describe the utterances. These features fall into four main groups: durational, pause, pitch, and amplitude. We further selected variants of these feature classes that could be scored automatically, or at least mostly automatically with some . ead— far eight Mint ,500fts 0.000ft ,1500as &apos;,2000ra — psoo. i3000.r ... . Ir../ nennege far 191. . ;fro Figure 1: A lexically matched pair where the repeat (bottom) has an 18% increase in total duration and a 400% increase in pause duration. minor ha</context>
<context position="14774" citStr="Oviatt et al., 1996" startWordPosition="2274" endWordPosition="2277"> substantial differences not only between original inputs and repeat corrections, but also between the two correction classes, those in response to rejections and those in response to misrecognitions. Let us consider the relation of these results to those of related work reply message to &apos;reply on• message to 739 and produce a more clear overall picture of spoken correction behavior in human-computer dialogue. 7.1 Duration and Pause: Conversational to Clear Speech Durational measures, particularly increases in duration, appear as a common phenomenon among several analyses of speaking style [ (Oviatt et al., 1996), (Ostendorf et al., 1996), (Shriberg et al., 1997)1. Similarly, increases in number and duration of silence regions are associated with disfluencies (Shriberg et al., 1997), self-repairs (Nakatani and Hirschberg, 1994), and more careful speech (Ostendorf et al., 1996) as well as with spoken corrections (Oviatt et al., 1996). These changes in our correction data fit smoothly into an analysis of error corrections as invoking shifts from conversational to more &amp;quot;clear&amp;quot; or &amp;quot;careful&amp;quot; speaking styles. Thus, we observe a parallel between the changes in duration and pause from original to repeat corre</context>
</contexts>
<marker>Oviatt, Levow, MacEarchern, Kuhn, 1996</marker>
<rawString>S.L. Oviatt, G. Levow, M. MacEarchern, and K. Kuhn. 1996. Modeling hyperarticulate speech during human-computer error resolution. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 801-804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Pierrehumbert</author>
<author>Julia Hirschberg</author>
</authors>
<title>The meaning of intonational contours in the interpretation of discourse. In</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>271--312</pages>
<editor>P. Cohen, J. Morgan, and M. Pollack, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11488" citStr="Pierrehumbert and Hirschberg, 1990" startWordPosition="1764" endWordPosition="1767">lence as a percentage of total utterance duration. An example of pause 4000ms 4500ea ,2000•3 738 Figure 2: Contrasting Falling (top) and Rising (bottom) Pitch Contours insertion and lengthening appear in Figure 1. 5.3 Pitch To derive pitch features, we first apply the FO (fundamental frequency) analysis function from the Entropic ESPS Waves+ system (Secrest and Doddington. 1993) to produce a basic pitch track. Most of the related work reported above had found relationships between the magnitude of pitch features and discourse function rather than presence of accent type, used more heavily by (Pierrehumbert and Hirschberg, 1990), (Hirschberg and Litman, 1993). Thus, we chose to concentrate on pitch features of the former type. A trained analyst examines the pitch track to remove any points of doubling or halving due to pitch tracker error, non-speech sounds, and excessive glottalization of &gt; 5 sample points. We compute several derived measures using simple algorithms to obtain FO maximum, FO minimum, FO range, final FO contour, slope of maximum pitch rise, slope of maximum pitch fall, and sum of the slopes of the steepest rise and fall. Figure 2 depicts a basic pitch contour. 5.4 Amplitude Amplitude, measuring the lo</context>
<context position="17119" citStr="Pierrehumbert and Hirschberg, 1990" startWordPosition="2654" endWordPosition="2657">nition errors displayed some dramatic changes in pitch behavior. Since we observed that simple measures of pitch maximum, minimum, and range failed to capture even the basic contrast of rising versus falling contour, we extended our feature set with measures of slope of rise and slope of fall. These measures may be viewed both as an attempt to create a simplified form of Taylor&apos;s rise-fallcontinuation model (Taylor, 1995) and as an attempt to provide quantitative measures of pitch accent. Measures of pitch accent and contour had shown some utility in identifying certain discourse relations [ (Pierrehumbert and Hirschberg, 1990), (Hirschberg and Litman, 1993). Although changes in pitch maxima and minima were not significant in themselves, the increases in rise slopes for CME&apos;s in contrast to flattening of rise slopes in CRE&apos;s combined to form a highly significant measure. While not defining a specific overall contour as in (Taylor, 1995), this trend clearly indicates increased pitch accentuation. Future work will seek to describe not only the magnitude, but also the form of these pitch accents and their relation to those outlined in (Pierrehumbert and Hirschberg, 1990). 7.3 Summary It is clear that many of the adapta</context>
</contexts>
<marker>Pierrehumbert, Hirschberg, 1990</marker>
<rawString>Janet Pierrehumbert and Julia Hirschberg. 1990. The meaning of intonational contours in the interpretation of discourse. In P. Cohen, J. Morgan, and M. Pollack, editors, Intentions in Communication, pages 271-312. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1992</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="18463" citStr="Quinlan, 1992" startWordPosition="2862" endWordPosition="2863">ever, while this model may adequately describe corrections of rejection errors, corrections of misrecognition errors obviously incorporate additional pitch accent features to indicate their discourse function. These contrasts will be shown to ease the identification of these utterances as corrections and to highlight their contrastive intent. 8 Decision Tree Experiments The next step was to develop predictive classifiers of original vs repeat corrections and CME&apos;s vs CRE&apos;s informed by the descriptive analysis above. We chose to implement these classifiers with decision trees (using Quinlan&apos;s (Quinlan, 1992) C4.5) trained on a subset of the originalrepeat pair data. Decision trees have two features which make them desirable for this task. First, since they can ignore irrelevant attributes, they will not be misled by meaningless noise in one or more of the 38 duration, pause, pitch, and amplitude features coded. Since these features are probably not all important, it is desir740 able to use a technique which can identify those which are most relevant. Second, decision trees are highly intelligible; simple inspection of trees can identify which rules use which attributes to arrive at a classificati</context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>J.R. Quinlan. 1992. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B G Secrest</author>
<author>G R Doddington</author>
</authors>
<title>An integrated pitch tracking algorithm for speech systems.</title>
<date>1993</date>
<booktitle>In ICASSP</booktitle>
<marker>Secrest, Doddington, 1993</marker>
<rawString>B. G. Secrest and G. R. Doddington. 1993. An integrated pitch tracking algorithm for speech systems. In ICASSP 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Bates</author>
<author>A Stolcke</author>
</authors>
<title>A prosody-only decision-tree model for disfluency detection.</title>
<date>1997</date>
<booktitle>In Eurospeech &apos;97.</booktitle>
<contexts>
<context position="7217" citStr="Shriberg et al., 1997" startWordPosition="1078" endWordPosition="1081">rupts to change an utterance in progress, and some preliminary efforts in the study of corrections in speech input. In analyzing and identifying self-repairs, (Bear et al., 1992) and (Heeman and Allen, 1994) found that the most effective methods relied on identifying shared textual regions between the reparandum and the repair. However, these techniques are limited to those instances where a reliable recognition string is available; in general, that is not the case for most speech recognition systems currently available. Alternative approaches described in (Nakatani and Hirschberg, 1994) and (Shriberg et al., 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. The few studies that have focussed on spoken corrections of computer misrecognitions, (Oviatt et al., 1996) and (Swerts and Ostendorf, 1995), also found significant effects of duration, and in Oviatt et al., pause insertion and length737 i.3000Rr ening played a role. However, in only one of these studies was input &amp;quot;conversational&amp;quot;, the other was a form-filling application, and neither involved spoken system responses, relying instead on visual displays for feedback, with potential imp</context>
<context position="8952" citStr="Shriberg et al., 1997" startWordPosition="1355" endWordPosition="1358">: OK SYSTEM SAID: Switching to mail. Your first message is .. USER SAID:Read message four eight nine SYSTEM HEARD: &amp;quot;nothing&amp;quot; CODE: Rejection SYSTEM SAID: Sorry? USER SAID:Read message four eight nine SYSTEM HEARD: &amp;quot;nothing&amp;quot; CODE: Rejection SYSTEM SAID: Still no luck. Speak clearly, but don&apos;t over emphasize . USER SAID:Go to message four eight nine SYSTEM HEARD: Go to message four please umm CODE: ERROR SYSTEM SAID: I heard goto new message 4. In total, there were 302 of these original-repeat pairs: 214 resulting from rejections, and 88 from misrecognit ions . Following (Oviatt et al., 1996), (Shriberg et al., 1997), and (Ostendorf et al., 1996), we coded a set of acoustic-prosodic features to describe the utterances. These features fall into four main groups: durational, pause, pitch, and amplitude. We further selected variants of these feature classes that could be scored automatically, or at least mostly automatically with some . ead— far eight Mint ,500fts 0.000ft ,1500as &apos;,2000ra — psoo. i3000.r ... . Ir../ nennege far 191. . ;fro Figure 1: A lexically matched pair where the repeat (bottom) has an 18% increase in total duration and a 400% increase in pause duration. minor hand-adjustment. We hoped t</context>
<context position="14825" citStr="Shriberg et al., 1997" startWordPosition="2282" endWordPosition="2285">l inputs and repeat corrections, but also between the two correction classes, those in response to rejections and those in response to misrecognitions. Let us consider the relation of these results to those of related work reply message to &apos;reply on• message to 739 and produce a more clear overall picture of spoken correction behavior in human-computer dialogue. 7.1 Duration and Pause: Conversational to Clear Speech Durational measures, particularly increases in duration, appear as a common phenomenon among several analyses of speaking style [ (Oviatt et al., 1996), (Ostendorf et al., 1996), (Shriberg et al., 1997)1. Similarly, increases in number and duration of silence regions are associated with disfluencies (Shriberg et al., 1997), self-repairs (Nakatani and Hirschberg, 1994), and more careful speech (Ostendorf et al., 1996) as well as with spoken corrections (Oviatt et al., 1996). These changes in our correction data fit smoothly into an analysis of error corrections as invoking shifts from conversational to more &amp;quot;clear&amp;quot; or &amp;quot;careful&amp;quot; speaking styles. Thus, we observe a parallel between the changes in duration and pause from original to repeat correction, described as conversational to clear in (Ovi</context>
</contexts>
<marker>Shriberg, Bates, Stolcke, 1997</marker>
<rawString>E. Shriberg, R. Bates, and A. Stolcke. 1997. A prosody-only decision-tree model for disfluency detection. In Eurospeech &apos;97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swerts</author>
<author>M Ostendorf</author>
</authors>
<title>Discourse prosody in human-machine interactions.</title>
<date>1995</date>
<booktitle>In Proceedings of the ECSA Tutorial and Research Workshop on Spoken Dialog Systems - Theories and Applications.</booktitle>
<contexts>
<context position="7468" citStr="Swerts and Ostendorf, 1995" startWordPosition="1114" endWordPosition="1117">relied on identifying shared textual regions between the reparandum and the repair. However, these techniques are limited to those instances where a reliable recognition string is available; in general, that is not the case for most speech recognition systems currently available. Alternative approaches described in (Nakatani and Hirschberg, 1994) and (Shriberg et al., 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. The few studies that have focussed on spoken corrections of computer misrecognitions, (Oviatt et al., 1996) and (Swerts and Ostendorf, 1995), also found significant effects of duration, and in Oviatt et al., pause insertion and length737 i.3000Rr ening played a role. However, in only one of these studies was input &amp;quot;conversational&amp;quot;, the other was a form-filling application, and neither involved spoken system responses, relying instead on visual displays for feedback, with potential impact on speaking style. 5 Error Data, Features, and Examples For these experiments, we selected pairs of utterances: the first (original) utterance is the first attempt by the user to enter an input or a query; the second (repeat) follows a system reco</context>
</contexts>
<marker>Swerts, Ostendorf, 1995</marker>
<rawString>M. Swerts and M. Ostendorf. 1995. Discourse prosody in human-machine interactions. In Proceedings of the ECSA Tutorial and Research Workshop on Spoken Dialog Systems - Theories and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Taylor</author>
</authors>
<title>The rise/fall/continuation model of intonation.</title>
<date>1995</date>
<journal>Speech Communication,</journal>
<pages>15--169</pages>
<contexts>
<context position="16909" citStr="Taylor, 1995" startWordPosition="2623" endWordPosition="2624">wer pitch minima. Third, this feature produces a divergence in behavior of CME&apos;s from CRE&apos;s. While CRE&apos;s exhibited only the change in pitch minimum described above, corrections of misrecognition errors displayed some dramatic changes in pitch behavior. Since we observed that simple measures of pitch maximum, minimum, and range failed to capture even the basic contrast of rising versus falling contour, we extended our feature set with measures of slope of rise and slope of fall. These measures may be viewed both as an attempt to create a simplified form of Taylor&apos;s rise-fallcontinuation model (Taylor, 1995) and as an attempt to provide quantitative measures of pitch accent. Measures of pitch accent and contour had shown some utility in identifying certain discourse relations [ (Pierrehumbert and Hirschberg, 1990), (Hirschberg and Litman, 1993). Although changes in pitch maxima and minima were not significant in themselves, the increases in rise slopes for CME&apos;s in contrast to flattening of rise slopes in CRE&apos;s combined to form a highly significant measure. While not defining a specific overall contour as in (Taylor, 1995), this trend clearly indicates increased pitch accentuation. Future work wi</context>
</contexts>
<marker>Taylor, 1995</marker>
<rawString>Paul Taylor. 1995. The rise/fall/continuation model of intonation. Speech Communication, 15:169-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Yankelovich</author>
<author>G Levow</author>
<author>M Marx</author>
</authors>
<title>Designing SpeechActs: Issues in speech user interfaces.</title>
<date>1995</date>
<booktitle>In CHI &apos;95 Conference on Human Factors in Computing Systems,</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="3398" citStr="Yankelovich et al., 1995" startWordPosition="500" endWordPosition="503">of corrections. We then turn to the issues involved in the actual construction and assessment of decision trees which identify spoken corrections. The best of these classifiers achieves 75% success rates. Finally, we conclude with a plan to use these results to improve recognition accuracy on spoken corrections and ease recovery from human-computer miscommunication. 3 System, Subjects, and Errors The utterances used in the design and analysis of the decision tree classifiers were drawn from approximately 60 hours of user interactions in a field trial of the Sun Microsystems SpeechActs system (Yankelovich et al., 1995). SpeechActs provides a voice-only interface to common desktop applications such as e-mail, calendar, weather reports, stock quotes, and 736 time and currency conversions. The system allows conversational interactions with speech recognition provided by BBN&apos;s HARK recognizer and speech synthesis through Centigram&apos;s TruVoice system. A sample interaction appears below. SpeechActs: You have 14 new messages and 47 old messages. The first new message is from Eric Baatz regarding &amp;quot;report completed.&amp;quot; User: Let me hear it. SpeechActs: &amp;quot;The first draft is ready for your comments. Eric.&amp;quot; User: Reply and</context>
</contexts>
<marker>Yankelovich, Levow, Marx, 1995</marker>
<rawString>N. Yankelovich, G. Levow, and M. Marx. 1995. Designing SpeechActs: Issues in speech user interfaces. In CHI &apos;95 Conference on Human Factors in Computing Systems, Denver, CO, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>