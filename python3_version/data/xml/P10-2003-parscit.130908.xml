<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029708">
<title confidence="0.993878">
Learning Lexicalized Reordering Models from Reordering Graphs
</title>
<author confidence="0.998093">
Jinsong Su, Yang Liu, Yajuan L¨u, Haitao Mi, Qun Liu
</author>
<affiliation confidence="0.981486333333333">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.86957">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.998456">
{sujinsong,yliu,lvyajuan,htmi,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988864375">
Lexicalized reordering models play a crucial
role in phrase-based translation systems. They
are usually learned from the word-aligned
bilingual corpus by examining the reordering
relations of adjacent phrases. Instead of just
checking whether there is one phrase adjacent
to a given phrase, we argue that it is important
to take the number of adjacent phrases into
account for better estimations of reordering
models. We propose to use a structure named
reordering graph, which represents all phrase
segmentations of a sentence pair, to learn lex-
icalized reordering models efficiently. Exper-
imental results on the NIST Chinese-English
test sets show that our approach significantly
outperforms the baseline method.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99664375">
Phrase-based translation systems (Koehn et al.,
2003; Och and Ney, 2004) prove to be the state-
of-the-art as they have delivered translation perfor-
mance in recent machine translation evaluations.
While excelling at memorizing local translation and
reordering, phrase-based systems have difficulties in
modeling permutations among phrases. As a result,
it is important to develop effective reordering mod-
els to capture such non-local reordering.
The early phrase-based paradigm (Koehn et al.,
2003) applies a simple distance-based distortion
penalty to model the phrase movements. More re-
cently, many researchers have presented lexicalized
reordering models that take advantage of lexical
information to predict reordering (Tillmann, 2004;
Xiong et al., 2006; Zens and Ney, 2006; Koehn et
</bodyText>
<figureCaption confidence="0.957099285714286">
Figure 1: Occurrence of a swap with different numbers
of adjacent bilingual phrases: only one phrase in (a) and
three phrases in (b). Black squares denote word align-
ments and gray rectangles denote bilingual phrases. [s,t]
indicates the target-side span of bilingual phrase bp and
[u,v] represents the source-side span of bilingual phrase
bp.
</figureCaption>
<bodyText confidence="0.989917136363637">
al., 2007; Galley and Manning, 2008). These mod-
els are learned from a word-aligned corpus to pre-
dict three orientations of a phrase pair with respect
to the previous bilingual phrase: monotone (M),
swap (S), and discontinuous (D). Take the bilingual
phrase bp in Figure 1(a) for example. The word-
based reordering model (Koehn et al., 2007) ana-
lyzes the word alignments at positions (s −1, u −1)
and (s − 1, v + 1). The orientation of bp is set
to D because the position (s − 1, v + 1) contains
no word alignment. The phrase-based reordering
model (Tillmann, 2004) determines the presence
of the adjacent bilingual phrase located in position
(s −1, v + 1) and then treats the orientation of bp as
S. Given no constraint on maximum phrase length,
the hierarchical phrase reordering model (Galley and
Manning, 2008) also analyzes the adjacent bilingual
phrases for bp and identifies its orientation as S.
However, given a bilingual phrase, the above-
mentioned models just consider the presence of an
adjacent bilingual phrase rather than the number of
adjacent bilingual phrases. See the examples in Fig-
</bodyText>
<page confidence="0.986898">
12
</page>
<note confidence="0.843308">
Proceedings of the ACL 2010 Conference Short Papers, pages 12–16,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.997086">
Figure 2: (a) A parallel Chinese-English sentence pair and (b) its corresponding reordering graph. In (b), we denote
each bilingual phrase with a rectangle, where the upper and bottom numbers in the brackets represent the source
and target spans of this bilingual phrase respectively. M = monotone (solid lines), S = swap (dotted line), and D =
discontinuous (segmented lines). The bilingual phrases marked in the gray constitute a reordering example.
</figureCaption>
<bodyText confidence="0.999942875">
ure 1 for illustration. In Figure 1(a), bp is in a swap
order with only one bilingual phrase. In Figure 1(b),
bp swaps with three bilingual phrases. Lexicalized
reordering models do not distinguish different num-
bers of adjacent phrase pairs, and just give bp the
same count in the swap orientation.
In this paper, we propose a novel method to better
estimate the reordering probabilities with the con-
sideration of varying numbers of adjacent bilingual
phrases. Our method uses reordering graphs to rep-
resent all phrase segmentations of parallel sentence
pairs, and then gets the fractional counts of bilin-
gual phrases for orientations from reordering graphs
in an inside-outside fashion. Experimental results
indicate that our method achieves significant im-
provements over the traditional lexicalized reorder-
ing model (Koehn et al., 2007).
This paper is organized as follows: in Section 2,
we first give a brief introduction to the traditional
lexicalized reordering model. Then we introduce
our method to estimate the reordering probabilities
from reordering graphs. The experimental results
are reported in Section 3. Finally, we end with a
conclusion and future work in Section 4.
</bodyText>
<sectionHeader confidence="0.5154555" genericHeader="method">
2 Estimation of Reordering Probabilities
Based on Reordering Graph
</sectionHeader>
<bodyText confidence="0.9952245">
In this section, we first describe the traditional lexi-
calized reordering model, and then illustrate how to
construct reordering graphs to estimate the reorder-
ing probabilities.
</bodyText>
<subsectionHeader confidence="0.964292">
2.1 Lexicalized Reordering Model
</subsectionHeader>
<bodyText confidence="0.997271444444444">
Given a phrase pair bp = (ei, fai), where ai de-
fines that the source phrase fai is aligned to the
target phrase ei, the traditional lexicalized reorder-
ing model computes the reordering count of bp in
the orientation o based on the word alignments of
boundary words. Specifically, the model collects
bilingual phrases and distinguishes their orientations
with respect to the previous bilingual phrase into
three categories:
</bodyText>
<equation confidence="0.599114333333333">
o = { M ai − ai_1 = 1 (1)
5 ai − ai_1 = −1
D |ai − ai_1 |=6 1
</equation>
<bodyText confidence="0.9331825">
Using the relative-frequency approach, the re-
ordering probability regarding bp is
</bodyText>
<listItem confidence="0.967101">
o b Count(o, bp) (2)
p(I p) _ Eo, Count(ol, bp)
</listItem>
<subsectionHeader confidence="0.999726">
2.2 Reordering Graph
</subsectionHeader>
<bodyText confidence="0.999787">
For a parallel sentence pair, its reordering graph in-
dicates all possible translation derivations consisting
of the extracted bilingual phrases. To construct a
reordering graph, we first extract bilingual phrases
using the way of (Och, 2003). Then, the adjacent
</bodyText>
<page confidence="0.994505">
13
</page>
<bodyText confidence="0.999113857142857">
bilingual phrases are linked according to the target-
side order. Some bilingual phrases, which have
no adjacent bilingual phrases because of maximum
length limitation, are linked to the nearest bilingual
phrases in the target-side order.
Shown in Figure 2(b), the reordering graph for
the parallel sentence pair (Figure 2(a)) can be rep-
resented as an undirected graph, where each rect-
angle corresponds to a phrase pair, each link is the
orientation relationship between adjacent bilingual
phrases, and two distinguished rectangles b, and b,
indicate the beginning and ending of the parallel sen-
tence pair, respectively. With the reordering graph,
we can obtain all reordering examples containing
the given bilingual phrase. For example, the bilin-
gual phrase (zhengshi huitan, formal meetings) (see
Figure 2(a)), corresponding to the rectangle labeled
with the source span [6,7] and the target span [4,5],
is in a monotone order with one previous phrase
and in a discontinuous order with two subsequent
phrases (see Figure 2(b)).
</bodyText>
<subsectionHeader confidence="0.999399">
2.3 Estimation of Reordering Probabilities
</subsectionHeader>
<bodyText confidence="0.99891508">
We estimate the reordering probabilities from re-
ordering graphs. Given a parallel sentence pair,
there are many translation derivations correspond-
ing to different paths in its reordering graph. As-
suming all derivations have a uniform probability,
the fractional counts of bilingual phrases for orien-
tations can be calculated by utilizing an algorithm in
the inside-outside fashion.
Given a phrase pair bp in the reordering graph,
we denote the number of paths from bs to bp with
α(bp). It can be computed in an iterative way
α(bp) = Ebp, α(bp0), where bp0 is one of the pre-
vious bilingual phrases of by and α(bs)=1. In a sim-
ilar way, the number of paths from be to bp, notated
as Q(bp), is simply Q(bp) = Ebp,, Q(bp00), where
bp00 is one of the subsequent bilingual phrases of bp
and Q(be)=1. Here, we show the α and Q values of
all bilingual phrases of Figure 2 in Table 1. Espe-
cially, for the reordering example consisting of the
bilingual phrases bp1=(jiang juxing, will hold) and
bp2=(zhengshi huitan, formal meetings), marked in
the gray color in Figure 2, the α and Q values can be
calculated: α(bp1) = 1, Q(bp2) = 1+1 = 2, Q(bs) =
8+1 = 9.
Inspired by the parsing literature on pruning
</bodyText>
<table confidence="0.998679647058824">
src span trg span α P
[0, 0] [0, 0] 1 9
[1, 1] [1, 1] 1 8
[1, 7] [1, 7] 1 1
[4, 4] [2, 2] 1 1
[4, 5] [2, 3] 1 3
[4, 6] [2, 4] 1 1
[4, 7] [2, 5] 1 2
[2, 7] [2, 7] 1 1
[5, 5] [3, 3] 1 1
[6, 6] [4, 4] 2 1
[6, 7] [4, 5] 1 2
[7, 7] [5, 5] 3 1
[2, 2] [6, 6] 5 1
[2, 3] [6, 7] 2 1
[3, 3] [7, 7] 5 1
[8, 8] [8, 8] 9 1
</table>
<tableCaption confidence="0.9891325">
Table 1: The α and Q values of the bilingual phrases
shown in Figure 2.
</tableCaption>
<bodyText confidence="0.863407">
(Charniak and Johnson, 2005; Huang, 2008), the
fractional count of (o, bp0, bp) is
</bodyText>
<equation confidence="0.9989415">
Count(o, bp0, bp) = α(bp0) - Q(bp) (3)
Q(bs)
</equation>
<bodyText confidence="0.999886">
where the numerator indicates the number of paths
containing the reordering example (o, bp0, bp) and
the denominator is the total number of paths in the
reordering graph. Continuing with the reordering
example described above, we obtain its fractional
count using the formula (3): Count(M, bp1, bp2) =
</bodyText>
<equation confidence="0.690471">
(1 x 2)/9 = 2/9.
</equation>
<bodyText confidence="0.9778995">
Then, the fractional count of bp in the orientation
o is calculated as described below:
</bodyText>
<equation confidence="0.949691">
Count(o, bp) = � Count(o, bp0, bp) (4)
bp&apos;
</equation>
<bodyText confidence="0.998628666666667">
For example, we compute the fractional count of
bp2 in the monotone orientation by the formula (4):
Count(M, bp2) = 2/9.
As described in the lexicalized reordering model
(Section 2.1), we apply the formula (2) to calculate
the final reordering probabilities.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99999525">
We conduct experiments to investigate the effec-
tiveness of our method on the msd-fe reorder-
ing model and the msd-bidirectional-fe reordering
model. These two models are widely applied in
</bodyText>
<page confidence="0.997885">
14
</page>
<bodyText confidence="0.999610833333333">
phrase-based system (Koehn et al., 2007). The msd-
fe reordering model has three features, which rep-
resent the probabilities of bilingual phrases in three
orientations: monotone, swap, or discontinuous. If a
msd-bidirectional-fe model is used, then the number
of features doubles: one for each direction.
</bodyText>
<subsectionHeader confidence="0.99819">
3.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.99998908">
Two different sizes of training corpora are used in
our experiments: one is a small-scale corpus that
comes from FBIS corpus consisting of 239K bilin-
gual sentence pairs, the other is a large-scale corpus
that includes 1.55M bilingual sentence pairs from
LDC. The 2002 NIST MT evaluation test data is
used as the development set and the 2003, 2004,
2005 NIST MT test data are the test sets. We
choose the MOSES1 (Koehn et al., 2007) as the ex-
perimental decoder. GIZA++ (Och and Ney, 2003)
and the heuristics “grow-diag-final-and” are used to
generate a word-aligned corpus, where we extract
bilingual phrases with maximum length 7. We use
SRILM Toolkits (Stolcke, 2002) to train a 4-gram
language model on the Xinhua portion of Gigaword
corpus.
In exception to the reordering probabilities, we
use the same features in the comparative experi-
ments. During decoding, we set ttable-limit = 20,
stack = 100, and perform minimum-error-rate train-
ing (Och, 2003) to tune various feature weights. The
translation quality is evaluated by case-insensitive
BLEU-4 metric (Papineni et al., 2002). Finally, we
conduct paired bootstrap sampling (Koehn, 2004) to
test the significance in BLEU scores differences.
</bodyText>
<subsectionHeader confidence="0.999528">
3.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999958111111111">
Table 2 shows the results of experiments with the
small training corpus. For the msd-fe model, the
BLEU scores by our method are 30.51 32.78 and
29.50, achieving absolute improvements of 0.89,
0.66 and 0.62 on the three test sets, respectively. For
the msd-bidirectional-fe model, our method obtains
BLEU scores of 30.49 32.73 and 29.24, with abso-
lute improvements of 1.11, 0.73 and 0.60 over the
baseline method.
</bodyText>
<footnote confidence="0.97140775">
1The phrase-based lexical reordering model (Tillmann,
2004) is also closely related to our model. However, due to
the limit of time and space, we only use Moses-style reordering
model (Koehn et al., 2007) as our baseline.
</footnote>
<table confidence="0.998442">
model method MT-03 MT-04 MT-05
m-f baseline 29.62 32.12 28.88
RG 30.51** 32.78** 29.50*
m-b-f baseline 29.38 32.00 28.64
RG 30.49** 32.73** 29.24*
</table>
<tableCaption confidence="0.9641422">
Table 2: Experimental results with the small-scale cor-
pus. m-f: msd-fe reordering model. m-b-f: msd-
bidirectional-fe reordering model. RG: probabilities esti-
mation based on Reordering Graph. * or **: significantly
better than baseline (p &lt; 0.05 or p &lt; 0.01).
</tableCaption>
<table confidence="0.9969608">
model method MT-03 MT-04 MT-05
m-f baseline 31.58 32.39 31.49
RG 32.44** 33.24** 31.64
m-b-f baseline 32.43 33.07 31.69
RG 33.29** 34.49** 32.79**
</table>
<tableCaption confidence="0.9865055">
Table 3: Experimental results with the large-scale cor-
pus.
</tableCaption>
<bodyText confidence="0.998501454545455">
Table 3 shows the results of experiments with
the large training corpus. In the experiments of
the msd-fe model, in exception to the MT-05 test
set, our method is superior to the baseline method.
The BLEU scores by our method are 32.44, 33.24
and 31.64, which obtain 0.86, 0.85 and 0.15 gains
on three test set, respectively. For the msd-
bidirectional-fe model, the BLEU scores produced
by our approach are 33.29, 34.49 and 32.79 on the
three test sets, with 0.86, 1.42 and 1.1 points higher
than the baseline method, respectively.
</bodyText>
<sectionHeader confidence="0.995608" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999957466666667">
In this paper, we propose a method to improve the
reordering model by considering the effect of the
number of adjacent bilingual phrases on the reorder-
ing probabilities estimation. Experimental results on
NIST Chinese-to-English tasks demonstrate the ef-
fectiveness of our method.
Our method is also general to other lexicalized
reordering models. We plan to apply our method
to the complex lexicalized reordering models, for
example, the hierarchical reordering model (Galley
and Manning, 2008) and the MEBTG reordering
model (Xiong et al., 2006). In addition, how to fur-
ther improve the reordering model by distinguishing
the derivations with different probabilities will be-
come another study emphasis in further research.
</bodyText>
<page confidence="0.996855">
15
</page>
<sectionHeader confidence="0.981895" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999791333333333">
The authors were supported by National Natural Sci-
ence Foundation of China, Contracts 60873167 and
60903138. We thank the anonymous reviewers for
their insightful comments. We are also grateful to
Hongmei Zhao and Shu Cai for their helpful feed-
back.
</bodyText>
<sectionHeader confidence="0.998446" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999625571428571">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL 2005, pages 173–180.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP 2008, pages 848–856.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ofACL 2008,
pages 586–594.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177–180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388–395.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, pages 417–449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311–318.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP 2002, pages 901–
904.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
ACL 2004, Short Papers, pages 101–104.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for statis-
tical machine translation. In Proc. ofACL 2006, pages
521–528.
Richard Zens and Hermann Ney. 2006. Discriminvative
reordering models for statistical machine translation.
In Proc. of Workshop on Statistical Machine Transla-
tion 2006, pages 521–528.
</reference>
<page confidence="0.998693">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.800852">
<title confidence="0.999976">Learning Lexicalized Reordering Models from Reordering Graphs</title>
<author confidence="0.948915">Jinsong Su</author>
<author confidence="0.948915">Yang Liu</author>
<author confidence="0.948915">Yajuan L¨u</author>
<author confidence="0.948915">Haitao Mi</author>
<author confidence="0.948915">Qun Liu</author>
<affiliation confidence="0.945861">Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.994543">P.O. Box 2704, Beijing 100190, China</address>
<abstract confidence="0.999507823529412">Lexicalized reordering models play a crucial role in phrase-based translation systems. They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>173--180</pages>
<contexts>
<context position="8923" citStr="Charniak and Johnson, 2005" startWordPosition="1466" endWordPosition="1469">ld) and bp2=(zhengshi huitan, formal meetings), marked in the gray color in Figure 2, the α and Q values can be calculated: α(bp1) = 1, Q(bp2) = 1+1 = 2, Q(bs) = 8+1 = 9. Inspired by the parsing literature on pruning src span trg span α P [0, 0] [0, 0] 1 9 [1, 1] [1, 1] 1 8 [1, 7] [1, 7] 1 1 [4, 4] [2, 2] 1 1 [4, 5] [2, 3] 1 3 [4, 6] [2, 4] 1 1 [4, 7] [2, 5] 1 2 [2, 7] [2, 7] 1 1 [5, 5] [3, 3] 1 1 [6, 6] [4, 4] 2 1 [6, 7] [4, 5] 1 2 [7, 7] [5, 5] 3 1 [2, 2] [6, 6] 5 1 [2, 3] [6, 7] 2 1 [3, 3] [7, 7] 5 1 [8, 8] [8, 8] 9 1 Table 1: The α and Q values of the bilingual phrases shown in Figure 2. (Charniak and Johnson, 2005; Huang, 2008), the fractional count of (o, bp0, bp) is Count(o, bp0, bp) = α(bp0) - Q(bp) (3) Q(bs) where the numerator indicates the number of paths containing the reordering example (o, bp0, bp) and the denominator is the total number of paths in the reordering graph. Continuing with the reordering example described above, we obtain its fractional count using the formula (3): Count(M, bp1, bp2) = (1 x 2)/9 = 2/9. Then, the fractional count of bp in the orientation o is calculated as described below: Count(o, bp) = � Count(o, bp0, bp) (4) bp&apos; For example, we compute the fractional count of b</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proc. of ACL 2005, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>848--856</pages>
<contexts>
<context position="2220" citStr="Galley and Manning, 2008" startWordPosition="316" endWordPosition="319">enalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s −1, u −1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bilingual phrase located in posi</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proc. of EMNLP 2008, pages 848–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>586--594</pages>
<contexts>
<context position="8937" citStr="Huang, 2008" startWordPosition="1470" endWordPosition="1471">, formal meetings), marked in the gray color in Figure 2, the α and Q values can be calculated: α(bp1) = 1, Q(bp2) = 1+1 = 2, Q(bs) = 8+1 = 9. Inspired by the parsing literature on pruning src span trg span α P [0, 0] [0, 0] 1 9 [1, 1] [1, 1] 1 8 [1, 7] [1, 7] 1 1 [4, 4] [2, 2] 1 1 [4, 5] [2, 3] 1 3 [4, 6] [2, 4] 1 1 [4, 7] [2, 5] 1 2 [2, 7] [2, 7] 1 1 [5, 5] [3, 3] 1 1 [6, 6] [4, 4] 2 1 [6, 7] [4, 5] 1 2 [7, 7] [5, 5] 3 1 [2, 2] [6, 6] 5 1 [2, 3] [6, 7] 2 1 [3, 3] [7, 7] 5 1 [8, 8] [8, 8] 9 1 Table 1: The α and Q values of the bilingual phrases shown in Figure 2. (Charniak and Johnson, 2005; Huang, 2008), the fractional count of (o, bp0, bp) is Count(o, bp0, bp) = α(bp0) - Q(bp) (3) Q(bs) where the numerator indicates the number of paths containing the reordering example (o, bp0, bp) and the denominator is the total number of paths in the reordering graph. Continuing with the reordering example described above, we obtain its fractional count using the formula (3): Count(M, bp1, bp2) = (1 x 2)/9 = 2/9. Then, the fractional count of bp in the orientation o is calculated as described below: Count(o, bp) = � Count(o, bp0, bp) (4) bp&apos; For example, we compute the fractional count of bp2 in the mono</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. ofACL 2008, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1106" citStr="Koehn et al., 2003" startWordPosition="150" endWordPosition="153">the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering mod</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL 2003, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL 2007, Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="2523" citStr="Koehn et al., 2007" startWordPosition="367" endWordPosition="370">nt bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s −1, u −1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bilingual phrase located in position (s −1, v + 1) and then treats the orientation of bp as S. Given no constraint on maximum phrase length, the hierarchical phrase reordering model (Galley and Manning, 2008) also analyzes the adjacent bilingual phrases for bp and identifies its orientation as S. However, given a bilingual phrase, th</context>
<context position="4725" citStr="Koehn et al., 2007" startWordPosition="721" endWordPosition="724">adjacent phrase pairs, and just give bp the same count in the swap orientation. In this paper, we propose a novel method to better estimate the reordering probabilities with the consideration of varying numbers of adjacent bilingual phrases. Our method uses reordering graphs to represent all phrase segmentations of parallel sentence pairs, and then gets the fractional counts of bilingual phrases for orientations from reordering graphs in an inside-outside fashion. Experimental results indicate that our method achieves significant improvements over the traditional lexicalized reordering model (Koehn et al., 2007). This paper is organized as follows: in Section 2, we first give a brief introduction to the traditional lexicalized reordering model. Then we introduce our method to estimate the reordering probabilities from reordering graphs. The experimental results are reported in Section 3. Finally, we end with a conclusion and future work in Section 4. 2 Estimation of Reordering Probabilities Based on Reordering Graph In this section, we first describe the traditional lexicalized reordering model, and then illustrate how to construct reordering graphs to estimate the reordering probabilities. 2.1 Lexic</context>
<context position="9977" citStr="Koehn et al., 2007" startWordPosition="1641" endWordPosition="1644">ional count of bp in the orientation o is calculated as described below: Count(o, bp) = � Count(o, bp0, bp) (4) bp&apos; For example, we compute the fractional count of bp2 in the monotone orientation by the formula (4): Count(M, bp2) = 2/9. As described in the lexicalized reordering model (Section 2.1), we apply the formula (2) to calculate the final reordering probabilities. 3 Experiments We conduct experiments to investigate the effectiveness of our method on the msd-fe reordering model and the msd-bidirectional-fe reordering model. These two models are widely applied in 14 phrase-based system (Koehn et al., 2007). The msdfe reordering model has three features, which represent the probabilities of bilingual phrases in three orientations: monotone, swap, or discontinuous. If a msd-bidirectional-fe model is used, then the number of features doubles: one for each direction. 3.1 Experiment Setup Two different sizes of training corpora are used in our experiments: one is a small-scale corpus that comes from FBIS corpus consisting of 239K bilingual sentence pairs, the other is a large-scale corpus that includes 1.55M bilingual sentence pairs from LDC. The 2002 NIST MT evaluation test data is used as the deve</context>
<context position="12101" citStr="Koehn et al., 2007" startWordPosition="1979" endWordPosition="1982"> Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. However, due to the limit of time and space, we only use Moses-style reordering model (Koehn et al., 2007) as our baseline. model method MT-03 MT-04 MT-05 m-f baseline 29.62 32.12 28.88 RG 30.51** 32.78** 29.50* m-b-f baseline 29.38 32.00 28.64 RG 30.49** 32.73** 29.24* Table 2: Experimental results with the small-scale corpus. m-f: msd-fe reordering model. m-b-f: msdbidirectional-fe reordering model. RG: probabilities estimation based on Reordering Graph. * or **: significantly better than baseline (p &lt; 0.05 or p &lt; 0.01). model method MT-03 MT-04 MT-05 m-f baseline 31.58 32.39 31.49 RG 32.44** 33.24** 31.64 m-b-f baseline 32.43 33.07 31.69 RG 33.29** 34.49** 32.79** Table 3: Experimental results </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL 2007, Demonstration Session, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="11404" citStr="Koehn, 2004" startWordPosition="1869" endWordPosition="1870">e used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. However, </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10748" citStr="Och and Ney, 2003" startWordPosition="1769" endWordPosition="1772">inuous. If a msd-bidirectional-fe model is used, then the number of features doubles: one for each direction. 3.1 Experiment Setup Two different sizes of training corpora are used in our experiments: one is a small-scale corpus that comes from FBIS corpus consisting of 239K bilingual sentence pairs, the other is a large-scale corpus that includes 1.55M bilingual sentence pairs from LDC. The 2002 NIST MT evaluation test data is used as the development set and the 2003, 2004, 2005 NIST MT test data are the test sets. We choose the MOSES1 (Koehn et al., 2007) as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Fin</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation. Computational Linguistics,</title>
<date>2004</date>
<pages>417--449</pages>
<contexts>
<context position="1126" citStr="Och and Ney, 2004" startWordPosition="154" endWordPosition="157">ions of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advant</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, pages 417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6226" citStr="Och, 2003" startWordPosition="967" endWordPosition="968">pecifically, the model collects bilingual phrases and distinguishes their orientations with respect to the previous bilingual phrase into three categories: o = { M ai − ai_1 = 1 (1) 5 ai − ai_1 = −1 D |ai − ai_1 |=6 1 Using the relative-frequency approach, the reordering probability regarding bp is o b Count(o, bp) (2) p(I p) _ Eo, Count(ol, bp) 2.2 Reordering Graph For a parallel sentence pair, its reordering graph indicates all possible translation derivations consisting of the extracted bilingual phrases. To construct a reordering graph, we first extract bilingual phrases using the way of (Och, 2003). Then, the adjacent 13 bilingual phrases are linked according to the targetside order. Some bilingual phrases, which have no adjacent bilingual phrases because of maximum length limitation, are linked to the nearest bilingual phrases in the target-side order. Shown in Figure 2(b), the reordering graph for the parallel sentence pair (Figure 2(a)) can be represented as an undirected graph, where each rectangle corresponds to a phrase pair, each link is the orientation relationship between adjacent bilingual phrases, and two distinguished rectangles b, and b, indicate the beginning and ending of</context>
<context position="11215" citStr="Och, 2003" startWordPosition="1843" endWordPosition="1844">04, 2005 NIST MT test data are the test sets. We choose the MOSES1 (Koehn et al., 2007) as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 2</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="11343" citStr="Papineni et al., 2002" startWordPosition="1859" endWordPosition="1862"> GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1The phrase-based lexical reordering model (Ti</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL 2002, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP 2002,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="10929" citStr="Stolcke, 2002" startWordPosition="1797" endWordPosition="1798">r experiments: one is a small-scale corpus that comes from FBIS corpus consisting of 239K bilingual sentence pairs, the other is a large-scale corpus that includes 1.55M bilingual sentence pairs from LDC. The 2002 NIST MT evaluation test data is used as the development set and the 2003, 2004, 2005 NIST MT test data are the test sets. We choose the MOSES1 (Koehn et al., 2007) as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proc. of ICSLP 2002, pages 901– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of HLTACL 2004, Short Papers,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="1790" citStr="Tillmann, 2004" startWordPosition="248" endWordPosition="249">vered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M), swap (S)</context>
<context position="11956" citStr="Tillmann, 2004" startWordPosition="1955" endWordPosition="1956">2). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. However, due to the limit of time and space, we only use Moses-style reordering model (Koehn et al., 2007) as our baseline. model method MT-03 MT-04 MT-05 m-f baseline 29.62 32.12 28.88 RG 30.51** 32.78** 29.50* m-b-f baseline 29.38 32.00 28.64 RG 30.49** 32.73** 29.24* Table 2: Experimental results with the small-scale corpus. m-f: msd-fe reordering model. m-b-f: msdbidirectional-fe reordering model. RG: probabilities estimation based on Reordering Graph. * or **: significantly better than baseline (p &lt; 0.05 or p &lt; 0.01). model method MT-03 MT-04 MT-05 m</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proc. of HLTACL 2004, Short Papers, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>521--528</pages>
<contexts>
<context position="1810" citStr="Xiong et al., 2006" startWordPosition="250" endWordPosition="253">n performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M), swap (S), and discontinuous </context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proc. ofACL 2006, pages 521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminvative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of Workshop on Statistical Machine Translation</booktitle>
<pages>521--528</pages>
<contexts>
<context position="1830" citStr="Zens and Ney, 2006" startWordPosition="254" endWordPosition="257">ent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M), swap (S), and discontinuous (D). Take the biling</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminvative reordering models for statistical machine translation. In Proc. of Workshop on Statistical Machine Translation 2006, pages 521–528.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>