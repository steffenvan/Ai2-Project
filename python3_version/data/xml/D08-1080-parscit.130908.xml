<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.9978975">
Topic-Driven Multi-Document Summarization
with Encyclopedic Knowledge and Spreading Activation
</title>
<author confidence="0.918389">
Vivi Nastase
</author>
<affiliation confidence="0.681467">
EML Research gGmbH
</affiliation>
<address confidence="0.540273">
Heidelberg, Germany
</address>
<email confidence="0.726848">
nastase@eml-research.de
</email>
<sectionHeader confidence="0.990421" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.978596409090909">
Information of interest to users is often dis-
tributed over a set of documents. Users
can specify their request for information as a
query/topic – a set of one or more sentences
or questions. Producing a good summary of
the relevant information relies on understand-
ing the query and linking it with the associ-
ated set of documents. To “understand” the
query we expand it using encyclopedic knowl-
edge in Wikipedia. The expanded query is
linked with its associated documents through
spreading activation in a graph that represents
words and their grammatical connections in
these documents. The topic expanded words
and activated nodes in the graph are used to
produce an extractive summary. The method
proposed is tested on the DUC summariza-
tion data. The system implemented ranks high
compared to the participating systems in the
DUC competitions, confirming our hypothesis
that encyclopedic knowledge is a useful addi-
tion to a summarization system.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999793954545455">
Topic-driven summarization reflects a user-based
summarization task: from a set of documents de-
rive a summary that contains information on a spe-
cific topic of interest to a user. Producing a good
summary relies on “understanding” the user’s infor-
mation request, and the documents to be summa-
rized. It is commonly agreed that the verbal part
of a text provides pointers to a much larger body of
knowledge we assume the listener has. An Amer-
ican citizen, for example, when told There will be
fireworks on July 4th, understands that there will
be a celebration involving fireworks on the occasion
of the U.S. Independence Day. Understanding an
utterance implies lexical, common-sense and ency-
clopedic knowledge. Lexical knowledge is usually
incorporated in systems through machine readable
dictionaries, wordnets or thesauri. Common-sense
and encyclopedic knowledge were harder to capture,
but recently Wikipedia has opened the possibility of
accessing such knowledge on a large scale, and in
numerous languages.
To “understand” a user’s information request –
one or more sentences or questions (the topic of
the summary) – summarization systems try to ex-
pand it. This will provide later stages of process-
ing with more keywords/keyphrases for retrieving
from the documents relevant fragments. In this pa-
per we experiment with Wikipedia for topic expan-
sion. The body of research involving Wikipedia
as a source of knowledge is growing fast, as the
NLP community finds more and more applications
of this useful resource: it is used to acquire knowl-
edge (Suchanek et al., 2007; Auer et al., 2007);
to induce taxonomies and compute semantic relat-
edness (Ponzetto &amp; Strube, 2007b; 2007a); as a
source of features for text classification (Gabrilovich
&amp; Markovitch, 2006) and for answering questions
(Ahn et al., 2004; Katz et al., 2005). The work pre-
sented here uses hyperlinks in Wikipedia articles to
expand keywords and keyphrases extracted from the
query. Ambiguous words are disambiguated using
the context provided by the query.
“Understanding” the documents to be summa-
rized implies identifying the entities mentioned, how
</bodyText>
<page confidence="0.978164">
763
</page>
<note confidence="0.962072">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 763–772,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999822869565217">
they are connected, and how they are related to the
entities in the topic. For this, we start again from the
topic, and spread an activation signal in a large graph
that covers all documents for this topic – nodes are
words/named entities in the texts, links are gram-
matical relations. This way we cross from the topic
to the documents, and combine information which
is important in the topic with information which is
important and relevant in the documents. We take
the most highly activated nodes as additional topic
expansions, and produce an extractive summary by
choosing from the sentences that connect the topic
expansion words in the large document graph.
The experiments confirm that Wikipedia is a
source of useful knowledge for summarization, and
that further expanding the topic within the associ-
ated set of documents improves the summarization
results even more. We compare the performance
of the summarization system to that of participating
systems in the DUC competitions. The system we
describe ranks 2nd, 9th and 5th in terms of ROUGE-
SU4 on the DUC 2005, DUC 2006 and DUC 2007
data respectively.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9987789">
While the recent exponential increase in the amount
of information with which we must cope makes
summarization a very desirable tool in the present,
summarization is not a novel task. Rath et al. (1961)
and Edmundson (1969) have explored extractive
summary formation, and have raised important eval-
uation issues for extractive summaries when com-
pared to several human produced gold standards.
Nowadays, summarization methods try to incorpo-
rate tools, methodologies and resources developed
over the past decades. The NIST organized com-
petitions under the Document Understanding Con-
ferences – DUC (since 2008, Text Analysis Confer-
ence (TAC))1 events provide a forum for the compar-
ison of a variety of approaches, ranging from knowl-
edge poor – Gotti et al. (2007) rely exclusively on
a parser, without any additional sources of informa-
tion – to knowledge rich and complex – GISTexter
(Hickl et al., 2007) combines question answering,
textual entailment, topic signature modules and a va-
</bodyText>
<footnote confidence="0.9314895">
1http://duc.nist.gov/, http://www.nist.
gov/tac.
</footnote>
<bodyText confidence="0.997531583333333">
riety of knowledge sources for summarization.
The most frequently used knowledge source in
NLP in general, and also for summarization, is
WordNet (Fellbaum, 1998). Barzilay &amp; Elhadad
(1999) use WordNet to model a text’s content rel-
ative to a topic based on lexical chains. The sen-
tences intersected by the most and strongest chains
are chosen for the extractive summary. Alterna-
tive sources for query expansion and document pro-
cessing have also been explored. Amini &amp; Usunier
(2007) use the documents to be summarized them-
selves to cluster terms, and thus expanding the query
“internally”. More advanced methods for query ex-
pansion use “topic signatures” – words and gram-
matically related pairs of words that model the query
and even the expected answer from sets of docu-
ments marked as relevant or not (Lin &amp; Hovy, 2000;
Harabagiu, 2004).
Graph-based methods for text summarization
work usually at the level of sentences (Erkan &amp;
Radev, 2004; Mihalcea &amp; Tarau, 2004). Edge
weights between sentences represent a similarity
measure, and a PageRank algorithm is used to deter-
mine the sentences that are the most salient from a
collection of documents and closest to a given topic.
At the word level, Leskovec et al. (2004) build
a document graph using subject-verb-object triples,
semantic normalization and coreference resolution.
They use several methods (node degree, PageRank,
Hubs, etc.) to compute statistics for the nodes in
the network, and use these as attribute values in
a machine learning algorithm, where the attribute
that is learned is whether the node should appear
in the final summary or not. Annotations for train-
ing come from human produced summaries. Mo-
hamed &amp; Rajasekaran (2006) incrementally build
a graph for a document collection by combining
graph-representations of sentences. Links between
entities in a sentence can be isa (within an NP)
or related to (between different phrases in a sen-
tence). Nodes and relations are weighted according
to their connectivity, and sentence selection for the
final summary is based on the most highly connected
nodes. Ye &amp; Chua (2006) build an extractive sum-
mary based on a concept lattice, which captures in
a hierarchical structure co-occurrences of concepts
among sentences. Nodes higher in this structure cor-
respond to frequently co-occurring terms, and are
</bodyText>
<page confidence="0.994077">
764
</page>
<figure confidence="0.98217372">
&lt;topic&gt;
&lt;num&gt; D0704A &lt; /num&gt;
&lt;title&gt; Amnesty International &lt; /title&gt;
&lt;narr&gt;
What is the scope of operations of Amnesty International
and what are the international reactions to its activities?
Give examples of charges lodged by the organization and
complaints against it.
&lt; /narr&gt;
&lt;docs&gt;
...
&lt; /docs&gt;
&lt; /topic&gt;
&lt;topic&gt;
&lt;num&gt; D0740I &lt; /num&gt;
&lt;title&gt; round-the-world balloon flight &lt; /title&gt;
&lt;narr&gt;
Report on the planning, attempts and first success-
ful balloon circumnavigation of the earth by Bertrand
Piccard and his crew.
&lt; /narr&gt;
&lt;docs&gt;
...
&lt; /docs&gt;
&lt; /topic&gt;
</figure>
<figureCaption confidence="0.99997">
Figure 1: Sample topics from DUC 2007
</figureCaption>
<bodyText confidence="0.99734440625">
assumed to be more representative with respect to
the document topic.
Mani &amp; Bloedorn (1999) build a “chronologi-
cal” graph, in which sentence order is respected and
each occurrence of a concept is a separate node.
Edges between nodes cover several types of rela-
tions: adjacency (ADJ); identity – instance of the
same word (SAME); other semantic links, in par-
ticular synonymy and hypernymy; PHRASE links
connect components of a phrase; NAME indicate
named entities; COREF link coreferential name in-
stances. Among other things, they identify regions
of the text salient to a user’s query, based on spread-
ing activation starting from query words in this doc-
ument graph. Spreading activation was introduced
in the 60s and 70s to model psychological processes
of memory activation in humans (Quillian, 1967;
Collins &amp; Loftus, 1975).
In this approach we use Wikipedia as a source of
knowledge for related concepts – the texts of hyper-
links in an article describing a concept are taken as
its related concepts. The query is further expanded
by using spreading activation to move away from the
topic in a large graph that covers all documents for
a given topic. From the nodes thus reached we se-
lect using a PageRank algorithm the ones that are
most important in the documents. We study the im-
pact of a decay parameter which controls how far
to move from the topic, and the number of highest
ranked nodes to be added to the expanded topic. The
summary is built based on word associations in the
documents’ graph.
</bodyText>
<sectionHeader confidence="0.9044905" genericHeader="method">
3 Topic Expansion with Encyclopedic
Knowledge or WordNet
</sectionHeader>
<bodyText confidence="0.9949582">
In DUC topic-driven multi-document summariza-
tion, the topic has a title, an ID that links it to a set of
documents, and one or more sentences and/or ques-
tions, as illustrated in Figure 1.
Topic processing is done in several steps:
</bodyText>
<listItem confidence="0.978303">
1. Preprocessing: Produce the dependency pair
</listItem>
<bodyText confidence="0.923066941176471">
representation of the topics using the Stanford
Parser2. Pairs that have closed-class words are fil-
tered out, and the remaining words are lemmatized3.
We extract named entities (NEs), as the parser
works at the word level. In the dependency pairs
we replace an NE’s fragments with the complete NE.
2a. Query expansion with Wikipedia: Extract
all open-class words and NEs from the topic, and
expand them using Wikipedia articles whose titles
are these words or phrases.
For each Wikipedia article we extract as related
concepts the texts of the hyperlinks in the first para-
graph (see Figure 24). The reason for not including
links from the entire article body is that apart from
the first paragraph, which is more focused, often
times hyperlinks are included whenever the under-
lying concept appears in Wikipedia, without it being
</bodyText>
<footnote confidence="0.9766805">
2http://nlp.stanford.edu/software/
lex-parser.shtml
3Using XTAG morphological database ftp:
//ftp.cis.upenn.edu/pub/xtag/morph-1.5/
morph-1.5.tar.gz.
4The left side shows the first paragraph as it appears on the
page, the right side shows the corresponding fragment from the
source file, with the annotations specific to Wikipedia.
</footnote>
<page confidence="0.988582">
765
</page>
<bodyText confidence="0.976787926829269">
Mining
Mining is the extraction of valuable
minerals or other geological materi-
als from the earth, usually (but not al-
ways) from an ore body, vein or (coal)
seam. Materials recovered by min-
ing include bauxite, coal, copper, gold,
silver, diamonds, iron, precious met-
als, lead, limestone, magnesite, nickel,
phosphate, oil shale, rock salt, tin, ura-
nium and molybdenum . Any material
that cannot be grown from agricultural
processes, or created artificially in a
laboratory or factory, is usually mined.
Mining in a wider sense comprises ex-
traction of any non-renewable resource
(e.g. petroleum, natural gas ,or even
water ).
’’’Mining’’’ is the extraction of [[value
(economics)|valuable]] [[mineral]]s or other
[[geology|geological]] materials from the
earth, usually (but not always) from an
[[ore]] body, [[vein (geology)|vein]] or (coal)
seam. Materials recovered by mining include
[[bauxite]], [[coal]], [[copper]], [[gold]],
[[silver]], [[diamond]]s, [[iron]], [[precious
metal]]s, [[lead]], [[limestone]], [[magnesite]],
[[nickel]], [[phosphate]], [[oil shale]], [[Sodium
chloride|rock salt]], [[tin]], [[uranium]] and
[[molybdenum]]. Any material that cannot be grown
from [[agriculture|agricultural]] processes, or
created [[Chemical synthesis|artificially]] in a
[[laboratory]] or [[factory]], is usually mined.
Mining in a wider sense comprises extraction of any
[[non-renewable resource]] (e.g., [[petroleum]],
[[natural gas]], or even [[fossil water|water]]).
Extracted related concepts for mining:
value (economics), valuable, mineral, geology, geological, ore, vein (geology), vein, coal, bauxite,
copper, gold, silver, diamond, iron, precious metal, lead, limestone, magnesite, nickel, phosphate,
oil shale, Sodium chloride, rock salt, agriculture, agricultural, Chemical synthesis, artificially,
laboratory, factory, non-renewable resource, petroleum, natural gas, fossil water, water.
</bodyText>
<figureCaption confidence="0.991242">
Figure 2: First paragraph for article Mining in the English Wikipedia, and the extracted related concepts.
</figureCaption>
<bodyText confidence="0.992086846153846">
Word Wikipedia expansion WordNet expansion
mining lead, agricultural, mineral, gold, ore, production
petroleum, nickel, iron, coal, tin, value,
copper, water, bauxite, silver, diamond
flight lift, air pass, trip, lam, overflight, ballooning,
nonstop flight, aviation, soaring, air,
flying, solo, break, escape
status registered way, situation, mode, position, place,
par, need, light, danger, health, state,
standing, face, rank, demand,
command, control
Southern Poverty racism, American, United States, –
Law Center research, civil rights, litigation
</bodyText>
<tableCaption confidence="0.997897">
Table 1: Expanded concepts from DUC 2007 topics, after filtering based on the documents to be summarized.
</tableCaption>
<bodyText confidence="0.991946">
particularly relevant to the current article.
To expand a word (or NE) W from the query, we
search for an article having W as the title, or part of
the title.
</bodyText>
<listItem confidence="0.8107514">
1. If one exact match is found (e.g. Southern
Poverty Law Center), extract the related con-
cepts for this article.
2. If several exact or partial matches are found,
use the larger context of the query to narrow
</listItem>
<bodyText confidence="0.87909552631579">
down to the intended meaning. For example,
Turkey – referring to the country – appears in
several topics in the DUC 2007 data. There
are multiple entries for “Turkey” in Wikipedia
– for the country, the bird, cities with this name
in the U.S. among others. We use a Lesk-like
measure, and compute the overlap between the
topic query and the set of hyperlinks in the first
paragraph (Lesk, 1986). We choose the ex-
pansion for the entry with the highest overlap.
If the query context does not help in disam-
biguation, we use the expansions for all partial
matches that tie for the highest overlap.
3. If an article with the required name does not
exist, the word will not be expanded.
2b. Query expansion with WordNet: Extract all
nouns and NEs from the topic, and expand them
with hypernyms, hyponyms and antonyms in Word-
Net 2.0:
</bodyText>
<page confidence="0.908055">
766
</page>
<listItem confidence="0.975637">
1. If an word (or NE) W from the query corre-
sponds to an unambiguous entry in WordNet,
expand that entry.
2. If W has multiple senses, choose the sense(s)
which have the highest overlap with the query.
To compute overlap, for a sense we take its ex-
pansions (one step hypernyms, hyponyms and
antonyms) and the words from the definition.
3. If W has no senses in WordNet, the word will
not be expanded.
3. Expansion filtering: Filter the list of related
concepts: keep only terms that appear in the docu-
ment collection for the current topic.
</listItem>
<bodyText confidence="0.999539827586207">
Table 1 includes the expansions obtained from
Wikipedia and from WordNet respectively for a
number of words in topics from the DUC 2007 col-
lection. mining is a specific activity, involving a lim-
ited set of materials. While such connections cannot
be retrieved through hypernym, meronym or other
semantic relations in WordNet, they are part of ency-
clopedic knowledge, and can be found in Wikipedia.
flight is a more general concept – there are spe-
cific types of flight, which appear as hyponyms
in WordNet, while in Wikipedia it is more gener-
ally described as the motion of an object through
air, which does not provide us with interesting re-
lated concepts. status is a very general concept,
and rather vague, for which neither WordNet nor
Wikipedia can provide very useful information. Fi-
nally, Wikipedia is rich in named entities, which are
not in the scope of a semantic lexicon. WordNet
does contain named entities, but not on the scale on
which Wikipedia does.
For the 45 topics from DUC 2007, the expansion
with Wikipedia generated 1054 additional words,
while with WordNet 2510. This difference comes
from the fact that with Wikipedia it is mostly the
NEs that are expanded, whereas with WordNet the
common nouns, which are more numerous in the
topics. The overlap between the two sets of expan-
sions is 48 words (0.046 relative to Wikipedia ex-
pansions, 0.019 relative to WordNet).
</bodyText>
<sectionHeader confidence="0.9986425" genericHeader="method">
4 Topic Expansion with Spreading
Activation and PageRank
</sectionHeader>
<bodyText confidence="0.999982818181818">
Concepts related to the ones in the topic provide a
good handle on the documents to summarize – they
indicate parts of the document that should be in-
cluded in the summary. It is however obvious that
the summary should contain more than that, and
this information comes from the documents to be
summarized. Amini &amp; Usunier (2007) have shown
that expanding the query within the set of docu-
ments leads to good results. Following this idea, to
find more relevant concepts we look for words/NEs
which are related to the topic, and at the same time
important in the collection of documents for the
given topic. The methods described in this section
are applied on a large graph that covers the entire
document collection for one topic. The documents
are processed in a similar way to the query – parsed
with the Stanford Parser, output in dependency rela-
tion format, lemmatized using XTag’s morpholog-
ical data file. The graph consists of nodes corre-
sponding to lemmatized words and NEs in the doc-
uments, and edges correspoding to grammatical de-
pendency relations.
</bodyText>
<subsectionHeader confidence="0.997379">
4.1 Spreading Activation
</subsectionHeader>
<bodyText confidence="0.999935090909091">
To find words/NEs related to the topic we spread an
activation signal starting from the topic words and
their expansions (in a manner similar to (Mani &amp;
Bloedorn, 1999), and using an algorithm inspired by
(Anderson, 1983)), which are given a node weight
of 1. As we traverse the graph starting from these
nodes, the signal is propagated by assigning a weight
to each edge and each node traversed based on the
signal strength. The signal strength diminishes with
the distance from the node of origin depending on a
signal decay parameter, according to the formula:
</bodyText>
<equation confidence="0.9969996">
wn(N0) = 1;
st = (1 − d W, (Nt)
ecay) *
wn(Nt+1) = st;
we(Nt, Nt+1)t+1 = we(Nt, Nt+1)t + st;
</equation>
<bodyText confidence="0.978391666666667">
where Nt is the current node; Nt+1 is the node we
are moving towards; wn(Nt) is the weight of node
Nt; st is the signal strength at step t; Out(Nt)
</bodyText>
<table confidence="0.911053888888889">
Out(Nt);
767
Topic Topic expanded words Top ranked nodes
D0738 status, registered, South America, cen- company, dollar, project, sector, iron,
What is the status of mining tral, 1998, obstacle, mining, lead, agri- mine, silver, percent, big, value, indus-
in central and South Amer- cultural, mineral, gold, ore, petroleum, try, source, overturn, regulate, link, of-
ica? Include obstacles en- nickel, iron, coal, tin, value, copper, ficial, decree, financing, expert, firm,
countered. water, bauxite, silver, diamond, in- activity, estimate, state, For Peru, Peru,
D0717 clude, encounter third, already, top, 12th, creation, ton
Describe the various law- combination, set, half, American Home drug, market, company, settle, re-
suits against American Products, know, fenfluramine, phen- dux, claim, American Home Products,
Home Products which termine, obesity, release, dexfenflu- make, cause, seek, cover, people, al-
resulted from the use of ramine, use, United States, Wal-Mart, low, agree, dismiss, other, sue, case,
fenfluramine, also known fen, describe, diet, call, drug, drugs, Pondimin, state, link, million, award,
as Pondimin, and half of medication, patients, medicine, law- user, estimate, thousand, file, think,
the diet drug combination suit, right, court, damages, defendant, note, damages, Harris County
called ”fen-phen”. plaintiff, also, various, Pondimin, re-
sult
</table>
<tableCaption confidence="0.999722">
Table 2: Top ranked nodes after expanding the topic with spreading activation and PageRank
</tableCaption>
<bodyText confidence="0.99993625">
is the number of outgoing edges from node Nt;
we(Nt, Nt+1)t is the weight of the edge between
Nt and Nt+1 at time t (i.e., before actually travers-
ing the edge and spreading the activation from Nt);
we(Nt, Nt+1)t+1 is the weight of the edge after
spreading activation. The weight of the edges is cu-
mulative, to gather strength from all signals that pass
through the edge. Activation is spread sequentially
from each node in the (expanded) topic.
The decay parameter is used to control how far
the influence of the starting nodes should reach – the
lower the decay, the farther the signal can reach.
</bodyText>
<subsectionHeader confidence="0.942769">
4.2 PageRank
</subsectionHeader>
<bodyText confidence="0.999975258064516">
The previous step has assigned weights to edges in
the graph, such that higher weights are closer to
topic and/or topic expanded words. After this ini-
tialization of the graph, we run a PageRank algo-
rithm (Brin &amp; Page, 1998) to determine more impor-
tant nodes. By running this algorithm after initializ-
ing the graph edge weights, from the nodes that are
closer to topic and topic expanded words we boost
those that are more important in the documents.
The starting point of the PageRank algorithm is
the graph with weighted edges obtained in the pre-
vious step. The node weights are initialized with
1 (the starting value does not matter). Analysis of
the documents graph for several topics has revealed
that there is a large highly interconnected structure,
and many disconnected small (2-3 nodes) fragments.
Page Rank will run on this dense core structure.
The PageRank algorithm is guaranteed to converge
if the graph is aperiodic and irreducible (Grimmett
&amp; Stirzaker, 1989). Aperiodicity implies that the
greatest common divisor of the graph’s cycles is 1
– this condition is met. Irreducibility of the graph
means that it has no leaves, and there are no two
nodes with the same set of neighbours. The rem-
edy in such cases is to connect each leaf to all other
nodes in the graph, and conflate nodes with the same
set of neighbours.
Once the graph topology meets the PageRank
convergence conditions, we run the algorithm. The
original formula for computing the rank of a node at
each iteration step is:
</bodyText>
<equation confidence="0.99989">
1 � d �
PR(nz) = N + d
</equation>
<bodyText confidence="0.999980666666667">
where nz is a node, d is the damping factor (usually
d = 0.85 and this is the value we use as well), N
is the number of nodes in the graph, PR(nz) is the
rank of node nz, Adjni is the set of nodes adjacent
to nz, and Out(nj) is the number of outgoing edges
from nj (our graph is non-directed, so this number
is the total number of edges with one end in nj).
We adjust this formula to reflect the weights of the
edges, and the version used is the following:
</bodyText>
<equation confidence="0.973779142857143">
1 � d �
PR(nz) = N + d
njEAdjni
PR(nj)
Out(nj)
PR(nj)wont(nj)�
njEAdjni
</equation>
<page confidence="0.990632">
768
</page>
<table confidence="0.9998515">
Expansion ROUGE-2 ROUGE-SU4 BE
none 0.09270 (0.08785 - 0.09762) 0.14587 (0.14019 - 0.1514) 0.04958 (0.04559 - 0.05413)
WNwith WSD 0.09494 (0.09086 - 0.09900) 0.15295 (0.14897 - 0.15681) 0.04985 (0.04606 - 0.05350)
WNno WSD 0.09596 (0.09189 - 0.09990) 0.15357 (0.14947 - 0.15741) 0.05173 (0.04794 - 0.05550)
Wiki 0.10173 (0.09721 - 0.10608) 0.15725 (0.15345 - 0.16130) 0.05542 (0.05125 - 0.05967)
WNno WSD + Wiki 0.09604 (0.09228 - 0.09980) 0.15315 (0.14923 - 0.15694) 0.05292 (0.04912 - 0.05647)
</table>
<tableCaption confidence="0.999917">
Table 3: Comparison of topic expansion methods with 95% confidence intervals.
</tableCaption>
<equation confidence="0.857455">
�wout(nj) = we(nk, nj)
nkEAdjnj
</equation>
<bodyText confidence="0.9998882">
In Table 2 we show examples of top ranked nodes
for several topics, extracted with this algorithm. The
words in italics are keywords/phrases from the topic
query, and the top ranked nodes are listed in decreas-
ing order of their rank.
</bodyText>
<sectionHeader confidence="0.997039" genericHeader="method">
5 Summarization
</sectionHeader>
<bodyText confidence="0.99916725">
The summarization method implemented is based
on the idea that the entities or events mentioned in
the query are somehow connected to each other, and
the documents to be summarized contain informa-
tion that allows us to make these connections. We
use again the graph for all the documents in the col-
lection related to one topic, built using the depen-
dency relation representation of the texts. The nodes
in this graph are words/NEs, and the links are gram-
matical relations.
We extract from this graph the subgraph that cov-
ers connections between all open class words/NEs
in the topic or expanded topic query. Each edge in
the extracted subgraph corresponds to a grammati-
cal relation in a sentence of a document. We col-
lect all sentences thus represented in the subgraph,
and rerank them based on the number of edges they
cover, and the occurrence of topic or expanded topic
terms. We use the following formula to compute a
sentence score:
</bodyText>
<equation confidence="0.787868">
Score(S) = topicWords * wword
</equation>
<table confidence="0.70643225">
+ expandedWords * wexpandedWord
+ topRankedWords * wtopRankedWord
+ edgesCovered * wsubgraphEdge
+ depRelation * wdepRelation
</table>
<bodyText confidence="0.997480285714286">
wword, wexpandedWord, wtopRankedWord,
wsubgraphEdge and wdepRelation are weight pa-
rameters that give different importance to exact
words from the topic, expanded words, top ranked
words and edges covered in the extracted subgraph.
During all experiments these parameters are fixed.5
To form the summary we traverse the ranked list
of sentences starting with the highest ranked one,
and add sentences to a summary, or delete from the
existing summary, based on a simple lexical overlap
measure. We stop when the desired summary length
is reached – for DUC 2005–2007, 250 words (last
sentence may be truncated to fill the summary up to
the allowed word limit).
</bodyText>
<sectionHeader confidence="0.996397" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999844047619048">
Experiments are run on DUC 2007 main summa-
rization task data, for the last experiment we used
the DUC 2005 and DUC 2006 data as well. Perfor-
mance is evaluated in terms of ROUGE-2, ROUGE-
SU4 and BE recall, following the methodology and
using the same parameters as in the DUC summa-
rization events.
We analyze several types of topic expansion: no
expansion, WordNet, Wikipedia, and within doc-
ument collection expansion using spreading acti-
vation and Page Rank. The spreading activation
method has several parameters whose values must
be determined.
We first compare the summaries produced with
no topic expansion, WordNet (WN) and Wikipedia
(Wiki) respectively. Table 3 shows the results in
terms of ROUGE and BE recall on the DUC 2007
(main) data. Word sense disambiguation (WSD) for
expansion with WordNet did not work very well,
as evidenced by the lower results for disambiguated
expansion (WN with WSD) compared to the non-
</bodyText>
<footnote confidence="0.924619571428572">
5The values used were set following a small number of ex-
periments on DUC 2007 data, as the purpose was not to tune
the system for best performance, but rather to study the impact
of more interesting parameters, in particular expansion type,
decay and node ranking. The values used are the following:
wword = 5, wexpandedWord = 2.5, wtopRankedWord = 0.5,
wsubgraphEdge = 2, wdepRelation = 0.
</footnote>
<page confidence="0.996728">
769
</page>
<bodyText confidence="0.999232181818182">
disambiguated one. A better disambiguation algo-
rithm may reverse the situation. Expanding a topic
only with Wikipedia hyperlinks gives the best re-
sults. At the document level, the results are not as
clear cut. Figure 3 shows a comparison in terms of
ROUGE-SU4 recall scores at the document level of
the Wikipedia and WN (no WSD) expansion meth-
ods, sorted in increasing order of the Wikipedia-
based expansion scores. The points are connected
to allow the reader to follow the results for each
method.
</bodyText>
<note confidence="0.592203">
DUC 2007 documents
</note>
<figureCaption confidence="0.984823">
Figure 3: Comparison of Wikipedia and WN ROUGE-
SU4 per-document recall results.
</figureCaption>
<bodyText confidence="0.999841476190476">
Because the overlap between Wikipedia and
WordNet expanded queries was very low, we ex-
pected the two types of expansion to be complemen-
tary, and the combination to give better results than
either expansion by itself. An analysis of results
for each document with the three expansion meth-
ods – Wikipedia, WordNet, and their combination –
showed that the simple combination of the expanded
words cannot take advantage of the situations when
one of the two methods performs better. In future
work we will explore how to detect, based on the
words in the query, which type of expansion is best,
and how to combine them using a weighting scheme.
We choose the best configuration from above
(Wikipedia expansion), and further expand the query
through spreading activation and PageRank. This
new type of expansion has two main parameters
which influence the summarization outcome: num-
ber of top ranked nodes to add to the topic expan-
sion, and the decay of the spreading activation algo-
rithm.
</bodyText>
<figure confidence="0.994858444444444">
0.1
0.108
0.106
0.104
0.102
0.1
0.098
0 0.2 0.4 0.6 0.8 1
decay
</figure>
<figureCaption confidence="0.993693">
Figure 4: Impact of signal decay in spreading activation
on summarization performance.
</figureCaption>
<bodyText confidence="0.999978842105263">
The decay parameter determines how far the in-
fluence of the starting nodes (words from query or
Wikipedia-expanded query) should be felt. The re-
sults in Figure 4 – for decay values 0.1, 0.5, 0.95,
0.99, 0.999, 0.9999, 1 – indicate that faster decay
(reflected through a higher decay value) keeps the
summary more focused around the given topic, and
leads to better results.6 For a high enough decay
– and eventually a decay of 1 – the weights of the
edges become extremely small, and due to real num-
ber representation in memory, practically 0. In this
situation PageRank has no effect, and all nodes have
the same rank.
We fix the decay parameter to 0.9999, and we
study the impact of the number of top nodes chosen
after ranking with PageRank. Figure 5 shows the re-
sults when the number of top ranked nodes chosen
varies. Adding highly ranked nodes benefits the per-
formance of the system only up to a certain limit.
</bodyText>
<footnote confidence="0.945959666666667">
6During this set of experiments all other parameters are
fixed, the number of top ranked nodes added to the topic ex-
pansion is 30.
</footnote>
<figure confidence="0.999708452380952">
0 5 10 15 20 25 30 35 40 45
ROUGE-SU4 Recall
0.22
0.18
0.16
0.14
0.12
0.08
0.2
0.1
Wiki
WN
0.165
0.164
0.163
0.162
0.161
0.16
0.159
0.158
0.157
0.156
0.155
0.154
0 0.2 0.4 0.6 0.8 1
decay
BE
0.062
0.061
0.06
0.059
0.058
0.057
0.056
0.055
0.054
0.053
0 0.2 0.4 0.6 0.8 1
decay
ROUGE-SU4
1
ROUGE-2
</figure>
<page confidence="0.590027">
770
</page>
<figureCaption confidence="0.993003666666667">
Figure 5: Impact of the number of top ranked nodes
added to the expanded topic on summarization perfor-
mance.
</figureCaption>
<bodyText confidence="0.999643571428572">
From the values we tested, the best results were ob-
tained when adding 40 nodes to the expanded topic.
The best system configuration from the ones ex-
plored7 is run on the DUC 2005, 2006 and 2007
(main) data. The performance and rank (in parenthe-
ses) compared to participating systems is presented
in Table 4.
</bodyText>
<table confidence="0.99951675">
DUC ROUGE-2 ROUGE-SU4 BE
2005 (32) 0.07074 (3) 0.13002 (2) –
2006 (35) 0.08091 (11) 0.14022 (9) 0.04223 (11)
2007 (32) 0.11048 (6) 0.16479 (5) 0.06250 (5)
</table>
<tableCaption confidence="0.7341065">
Table 4: System performance (and rank) on the DUC
2005, 2006 and 2007 (main) data. The number in paren-
thesis after the DUC year indicates the number of com-
peting systems.
</tableCaption>
<sectionHeader confidence="0.99577" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99996895">
The experiments conducted within the summa-
rization framework of the Document Understand-
ing Conference have confirmed that encyclopedic
knowledge extracted from Wikipedia can benefit the
summarization task. Wikipedia articles are a source
of relevant related concepts, that are useful for ex-
panding a summarization query. Furthermore, in-
cluding information from the documents to be sum-
marized by choosing relevant concepts – based on
closeness to topic keywords and relative importance
– improves even more the quality of the summaries,
judged through ROUGE-2, ROUGE-SU4 and BE
recall scores, as it is commonly done in the DUC
competitions. The topic expansion methods ex-
plored lead to high summarization performance –
ranked 2nd, 91h and 51h on DUC 2005, 2006 and
2007 respectively according to ROUGE-SU4 scores
– compared to (more than 30) DUC participating
systems.
The graph representation of the documents is cen-
tral to the summarization method we described. Be-
cause of this, we plan to improve this representation
by collapsing together coreferential nodes and clus-
tering together related concepts, and verify whether
such changes impact the summarization results, as
we expect they would.
Being able to move away from the topic within
the set of documents and discover new relevant
nodes is an important issue, especially from the
point of view of a new summarization style –
updates. In update summaries the starting point is
a topic, which a summarization system must track
in consecutive sets of documents. We can adjust
the spreading activation parameters to how far a
new set of documents is from the topic. Future
work includes testing the spreading activation and
page ranking method in the context of the update
summarization task and exploring methods of
extracting related concepts from the full text of
Wikipedia articles.
</bodyText>
<figure confidence="0.999016351351351">
1
0.1
0.109
0.108
0.107
0.106
0.105
0.104
0.103
0.102
0 10 20 30 40 50
number of nodes
0.1645
0.164
0.1635
0.163
0.1625
0.162
0.1615
0.161
0.1605
0 10 20 30 40 50
number of nodes
BE
0 10 20 30 40 50
number of nodes
ROUGE-2
ROUGE-SU4
0.063
0.062
0.061
0.06
0.059
0.058
0.057
0.056
0.055
</figure>
<footnote confidence="0.42173725">
7Wikipedia expansion + 40 top nodes after spreading acti-
vation and PageRank, decay = 0.9999, weypandedWord = 3.5,
wdepRelation = 1, the other parameters have the same values
as before.
</footnote>
<bodyText confidence="0.9165255">
Acknowledgments This work was funded by the
Klaus Tschira Foundation, Heidelberg, Germany.
We thank the anonymous reviewers for insightful
comments and suggestions.
</bodyText>
<page confidence="0.99679">
771
</page>
<sectionHeader confidence="0.995886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999912707865169">
Ahn, D., V. Jijkoun, G. Mishne, K. M¨uller, M. de Rijke
&amp; S. Schlobach (2004). Using Wikipedia at the TREC
QA track. In Proc. of TREC-13.
Amini, M. R. &amp; N. Usunier (2007). A contextual query
expansion approach by term clustering for robust text
summarization. In Proc. ofDUC-07.
Anderson, J. R. (1983). A spreading activation theory
of memory. Journal of Verbal Learning and Verbal
Behaviour, 22:261–295.
Auer, S., C. Bizer, J. Lehmann, G. Kobilarov, R. Cyga-
niak &amp; Z. Ives (2007). DBpedia: A nucleus for a Web
of open data. In Proc. of ISWC 2007 + ASWC 2007,
pp. 722–735.
Barzilay, R. &amp; M. Elhadad (1999). Using lexical chains
for text summarization. In I. Mani &amp; M. T. May-
bury (Eds.), Advances in Automatic Text Summariza-
tion, pp. 111–121. Cambridge, Mass.: MIT Press.
Brin, S. &amp; L. Page (1998). The anatomy of a large-scale
hypertextual web search engine. Computer Networks
andISDNSystems, 30(1–7):107–117.
Collins, A. M. &amp; E. F. Loftus (1975). A spreading-
activation theory of semantic processing. Psychologi-
cal Review, (82):407–428.
Edmundson, H. (1969). New methods in automatic ex-
tracting. Journal of the Association for Computing
Machinery, 16(2):264–285.
Erkan, G. &amp; D. R. Radev (2004). LexRank: Graph-based
lexical centrality as salience in text summarization.
Journal of Artificial Intelligence Research, 22:457–
479.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lex-
ical Database. Cambridge, Mass.: MIT Press.
Gabrilovich, E. &amp; S. Markovitch (2006). Overcoming
the brittleness bottleneck using Wikipedia: Enhancing
text categorization with encyclopedic knowledge. In
Proc. ofAAAI-06, pp. 1301–1306.
Gotti, F., G. Lapalme, L. Nerima &amp; E. Wehrli (2007).
GOFAIsum: a symbolic summarizer for DUC. In
Proc. ofDUC-07.
Grimmett, G. &amp; D. Stirzaker (1989). Probability and
Random Processes. Oxford University Press.
Harabagiu, S. (2004). Incremental topic representations.
In Proc. of COLING-04, pp. 583–589.
Hickl, A., K. Roberts &amp; F. L. C. C. Lacatusu (2007).
LCC’s GISTexter at DUC 2007: Machine reading for
update summarization. In Proc. ofDUC-07.
Katz, B., G. Marton, G. Borchardt, A. Brownell,
S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,
F. Mora, S. Stiller, O. Uzuner &amp; A. Wilcox (2005).
External knowledge sources for Question Answering.
In Proc. of TREC-14.
Lesk, M. (1986). Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from an ice cream cone. In Proc. ofACSD-86, pp. 24–
26.
Leskovec, J., M. Grobelnik &amp; N. Milic-Frayling (2004).
Learning sub-structures of document semantic graphs
for document summarization. In Proc. of LinkKDD-
04.
Lin, C.-Y. &amp; E. Hovy (2000). The automated acquisition
of topic signatures for automatic summarization. In
Proc. of COLING-00, pp. 495–501.
Mani, I. &amp; E. Bloedorn (1999). Summarizing similarities
and differences among related documents. Information
Retrieval, 1(1):35–67.
Mihalcea, R. &amp; P. Tarau (2004). TextRank: Bringing or-
der into texts. In Proc. EMNLP-04, pp. 404–411.
Mohamed, A. A. &amp; S. Rajasekaran (2006). Query-based
summarization based on document graphs. In Proc. of
DUC-06.
Ponzetto, S. P. &amp; M. Strube (2007a). Deriving a large
scale taxonomy from Wikipedia. In Proc. ofAAAI-07,
pp. 1440–1445.
Ponzetto, S. P. &amp; M. Strube (2007b). Knowledge derived
from Wikipedia for computing semantic relatedness.
Journal of Artificial Intelligence Research, 30:181–
212.
Quillian, M. R. (1967). Word concepts: A theory and
simulation of some basic semantic capabilities. Be-
havioural Science, 12(5):410–430.
Rath, G., A. Resnick &amp; T. Savage (1961). The formation
of abstracts by the selection of sentences. American
Documentation, 12(2):139–143.
Suchanek, F. M., G. Kasneci &amp; G. Weikum (2007).
YAGO: A core of semantic knowledge. In Proc. of
WWW-07, pp. 697–706.
Ye, S. &amp; T.-S. Chua (2006). NUS at DUC 2006: Doc-
ument concept lattice for summarization. In Proc. of
DUC-06.
</reference>
<page confidence="0.997394">
772
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.701477">
<title confidence="0.9992235">Topic-Driven Multi-Document with Encyclopedic Knowledge and Spreading Activation</title>
<author confidence="0.993269">Vivi</author>
<affiliation confidence="0.997817">EML Research</affiliation>
<address confidence="0.729558">Heidelberg,</address>
<email confidence="0.997718">nastase@eml-research.de</email>
<abstract confidence="0.998620826086956">Information of interest to users is often distributed over a set of documents. Users can specify their request for information as a query/topic – a set of one or more sentences or questions. Producing a good summary of the relevant information relies on understanding the query and linking it with the associated set of documents. To “understand” the query we expand it using encyclopedic knowledge in Wikipedia. The expanded query is linked with its associated documents through spreading activation in a graph that represents words and their grammatical connections in these documents. The topic expanded words and activated nodes in the graph are used to produce an extractive summary. The method proposed is tested on the DUC summarization data. The system implemented ranks high compared to the participating systems in the DUC competitions, confirming our hypothesis that encyclopedic knowledge is a useful addition to a summarization system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Ahn</author>
<author>V Jijkoun</author>
<author>G Mishne</author>
<author>K M¨uller</author>
<author>M de Rijke</author>
<author>S Schlobach</author>
</authors>
<title>Using Wikipedia at the TREC QA track.</title>
<date>2004</date>
<booktitle>In Proc. of TREC-13.</booktitle>
<marker>Ahn, Jijkoun, Mishne, M¨uller, de Rijke, Schlobach, 2004</marker>
<rawString>Ahn, D., V. Jijkoun, G. Mishne, K. M¨uller, M. de Rijke &amp; S. Schlobach (2004). Using Wikipedia at the TREC QA track. In Proc. of TREC-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Amini</author>
<author>N Usunier</author>
</authors>
<title>A contextual query expansion approach by term clustering for robust text summarization.</title>
<date>2007</date>
<booktitle>In Proc. ofDUC-07.</booktitle>
<contexts>
<context position="6088" citStr="Amini &amp; Usunier (2007)" startWordPosition="950" endWordPosition="953">l., 2007) combines question answering, textual entailment, topic signature modules and a va1http://duc.nist.gov/, http://www.nist. gov/tac. riety of knowledge sources for summarization. The most frequently used knowledge source in NLP in general, and also for summarization, is WordNet (Fellbaum, 1998). Barzilay &amp; Elhadad (1999) use WordNet to model a text’s content relative to a topic based on lexical chains. The sentences intersected by the most and strongest chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini &amp; Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin &amp; Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan &amp; Radev, 2004; Mihalcea &amp; Tarau, 2004). Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to determine the </context>
<context position="17716" citStr="Amini &amp; Usunier (2007)" startWordPosition="2797" endWordPosition="2800">tly the NEs that are expanded, whereas with WordNet the common nouns, which are more numerous in the topics. The overlap between the two sets of expansions is 48 words (0.046 relative to Wikipedia expansions, 0.019 relative to WordNet). 4 Topic Expansion with Spreading Activation and PageRank Concepts related to the ones in the topic provide a good handle on the documents to summarize – they indicate parts of the document that should be included in the summary. It is however obvious that the summary should contain more than that, and this information comes from the documents to be summarized. Amini &amp; Usunier (2007) have shown that expanding the query within the set of documents leads to good results. Following this idea, to find more relevant concepts we look for words/NEs which are related to the topic, and at the same time important in the collection of documents for the given topic. The methods described in this section are applied on a large graph that covers the entire document collection for one topic. The documents are processed in a similar way to the query – parsed with the Stanford Parser, output in dependency relation format, lemmatized using XTag’s morphological data file. The graph consists</context>
</contexts>
<marker>Amini, Usunier, 2007</marker>
<rawString>Amini, M. R. &amp; N. Usunier (2007). A contextual query expansion approach by term clustering for robust text summarization. In Proc. ofDUC-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Anderson</author>
</authors>
<title>A spreading activation theory of memory.</title>
<date>1983</date>
<journal>Journal of Verbal Learning and Verbal Behaviour,</journal>
<pages>22--261</pages>
<contexts>
<context position="18692" citStr="Anderson, 1983" startWordPosition="2963" endWordPosition="2964">ntire document collection for one topic. The documents are processed in a similar way to the query – parsed with the Stanford Parser, output in dependency relation format, lemmatized using XTag’s morphological data file. The graph consists of nodes corresponding to lemmatized words and NEs in the documents, and edges correspoding to grammatical dependency relations. 4.1 Spreading Activation To find words/NEs related to the topic we spread an activation signal starting from the topic words and their expansions (in a manner similar to (Mani &amp; Bloedorn, 1999), and using an algorithm inspired by (Anderson, 1983)), which are given a node weight of 1. As we traverse the graph starting from these nodes, the signal is propagated by assigning a weight to each edge and each node traversed based on the signal strength. The signal strength diminishes with the distance from the node of origin depending on a signal decay parameter, according to the formula: wn(N0) = 1; st = (1 − d W, (Nt) ecay) * wn(Nt+1) = st; we(Nt, Nt+1)t+1 = we(Nt, Nt+1)t + st; where Nt is the current node; Nt+1 is the node we are moving towards; wn(Nt) is the weight of node Nt; st is the signal strength at step t; Out(Nt) Out(Nt); 767 Top</context>
</contexts>
<marker>Anderson, 1983</marker>
<rawString>Anderson, J. R. (1983). A spreading activation theory of memory. Journal of Verbal Learning and Verbal Behaviour, 22:261–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Auer</author>
<author>C Bizer</author>
<author>J Lehmann</author>
<author>G Kobilarov</author>
<author>R Cyganiak</author>
<author>Z Ives</author>
</authors>
<title>DBpedia: A nucleus for a Web of open data.</title>
<date>2007</date>
<journal>ASWC</journal>
<booktitle>In Proc. of ISWC</booktitle>
<pages>722--735</pages>
<contexts>
<context position="2734" citStr="Auer et al., 2007" startWordPosition="423" endWordPosition="426">rge scale, and in numerous languages. To “understand” a user’s information request – one or more sentences or questions (the topic of the summary) – summarization systems try to expand it. This will provide later stages of processing with more keywords/keyphrases for retrieving from the documents relevant fragments. In this paper we experiment with Wikipedia for topic expansion. The body of research involving Wikipedia as a source of knowledge is growing fast, as the NLP community finds more and more applications of this useful resource: it is used to acquire knowledge (Suchanek et al., 2007; Auer et al., 2007); to induce taxonomies and compute semantic relatedness (Ponzetto &amp; Strube, 2007b; 2007a); as a source of features for text classification (Gabrilovich &amp; Markovitch, 2006) and for answering questions (Ahn et al., 2004; Katz et al., 2005). The work presented here uses hyperlinks in Wikipedia articles to expand keywords and keyphrases extracted from the query. Ambiguous words are disambiguated using the context provided by the query. “Understanding” the documents to be summarized implies identifying the entities mentioned, how 763 Proceedings of the 2008 Conference on Empirical Methods in Natura</context>
</contexts>
<marker>Auer, Bizer, Lehmann, Kobilarov, Cyganiak, Ives, 2007</marker>
<rawString>Auer, S., C. Bizer, J. Lehmann, G. Kobilarov, R. Cyganiak &amp; Z. Ives (2007). DBpedia: A nucleus for a Web of open data. In Proc. of ISWC 2007 + ASWC 2007, pp. 722–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for text summarization. In</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>111--121</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="5795" citStr="Barzilay &amp; Elhadad (1999)" startWordPosition="900" endWordPosition="903">since 2008, Text Analysis Conference (TAC))1 events provide a forum for the comparison of a variety of approaches, ranging from knowledge poor – Gotti et al. (2007) rely exclusively on a parser, without any additional sources of information – to knowledge rich and complex – GISTexter (Hickl et al., 2007) combines question answering, textual entailment, topic signature modules and a va1http://duc.nist.gov/, http://www.nist. gov/tac. riety of knowledge sources for summarization. The most frequently used knowledge source in NLP in general, and also for summarization, is WordNet (Fellbaum, 1998). Barzilay &amp; Elhadad (1999) use WordNet to model a text’s content relative to a topic based on lexical chains. The sentences intersected by the most and strongest chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini &amp; Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as re</context>
</contexts>
<marker>Barzilay, Elhadad, 1999</marker>
<rawString>Barzilay, R. &amp; M. Elhadad (1999). Using lexical chains for text summarization. In I. Mani &amp; M. T. Maybury (Eds.), Advances in Automatic Text Summarization, pp. 111–121. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer Networks andISDNSystems,</journal>
<pages>30--1</pages>
<contexts>
<context position="21577" citStr="Brin &amp; Page, 1998" startWordPosition="3428" endWordPosition="3431"> the weight of the edge after spreading activation. The weight of the edges is cumulative, to gather strength from all signals that pass through the edge. Activation is spread sequentially from each node in the (expanded) topic. The decay parameter is used to control how far the influence of the starting nodes should reach – the lower the decay, the farther the signal can reach. 4.2 PageRank The previous step has assigned weights to edges in the graph, such that higher weights are closer to topic and/or topic expanded words. After this initialization of the graph, we run a PageRank algorithm (Brin &amp; Page, 1998) to determine more important nodes. By running this algorithm after initializing the graph edge weights, from the nodes that are closer to topic and topic expanded words we boost those that are more important in the documents. The starting point of the PageRank algorithm is the graph with weighted edges obtained in the previous step. The node weights are initialized with 1 (the starting value does not matter). Analysis of the documents graph for several topics has revealed that there is a large highly interconnected structure, and many disconnected small (2-3 nodes) fragments. Page Rank will r</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Brin, S. &amp; L. Page (1998). The anatomy of a large-scale hypertextual web search engine. Computer Networks andISDNSystems, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>E F Loftus</author>
</authors>
<title>A spreadingactivation theory of semantic processing.</title>
<date>1975</date>
<journal>Psychological Review,</journal>
<pages>82--407</pages>
<contexts>
<context position="9357" citStr="Collins &amp; Loftus, 1975" startWordPosition="1474" endWordPosition="1477">rate node. Edges between nodes cover several types of relations: adjacency (ADJ); identity – instance of the same word (SAME); other semantic links, in particular synonymy and hypernymy; PHRASE links connect components of a phrase; NAME indicate named entities; COREF link coreferential name instances. Among other things, they identify regions of the text salient to a user’s query, based on spreading activation starting from query words in this document graph. Spreading activation was introduced in the 60s and 70s to model psychological processes of memory activation in humans (Quillian, 1967; Collins &amp; Loftus, 1975). In this approach we use Wikipedia as a source of knowledge for related concepts – the texts of hyperlinks in an article describing a concept are taken as its related concepts. The query is further expanded by using spreading activation to move away from the topic in a large graph that covers all documents for a given topic. From the nodes thus reached we select using a PageRank algorithm the ones that are most important in the documents. We study the impact of a decay parameter which controls how far to move from the topic, and the number of highest ranked nodes to be added to the expanded t</context>
</contexts>
<marker>Collins, Loftus, 1975</marker>
<rawString>Collins, A. M. &amp; E. F. Loftus (1975). A spreadingactivation theory of semantic processing. Psychological Review, (82):407–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="4797" citStr="Edmundson (1969)" startWordPosition="753" endWordPosition="754">or summarization, and that further expanding the topic within the associated set of documents improves the summarization results even more. We compare the performance of the summarization system to that of participating systems in the DUC competitions. The system we describe ranks 2nd, 9th and 5th in terms of ROUGESU4 on the DUC 2005, DUC 2006 and DUC 2007 data respectively. 2 Related Work While the recent exponential increase in the amount of information with which we must cope makes summarization a very desirable tool in the present, summarization is not a novel task. Rath et al. (1961) and Edmundson (1969) have explored extractive summary formation, and have raised important evaluation issues for extractive summaries when compared to several human produced gold standards. Nowadays, summarization methods try to incorporate tools, methodologies and resources developed over the past decades. The NIST organized competitions under the Document Understanding Conferences – DUC (since 2008, Text Analysis Conference (TAC))1 events provide a forum for the comparison of a variety of approaches, ranging from knowledge poor – Gotti et al. (2007) rely exclusively on a parser, without any additional sources o</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H. (1969). New methods in automatic extracting. Journal of the Association for Computing Machinery, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>LexRank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<pages>479</pages>
<contexts>
<context position="6548" citStr="Erkan &amp; Radev, 2004" startWordPosition="1027" endWordPosition="1030">t chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini &amp; Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin &amp; Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan &amp; Radev, 2004; Mihalcea &amp; Tarau, 2004). Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to determine the sentences that are the most salient from a collection of documents and closest to a given topic. At the word level, Leskovec et al. (2004) build a document graph using subject-verb-object triples, semantic normalization and coreference resolution. They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the nodes in the network, and use these as attribute values in a machine learning algorithm, where the attribute that is lear</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Erkan, G. &amp; D. R. Radev (2004). LexRank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22:457– 479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="5768" citStr="Fellbaum, 1998" startWordPosition="898" endWordPosition="899">nferences – DUC (since 2008, Text Analysis Conference (TAC))1 events provide a forum for the comparison of a variety of approaches, ranging from knowledge poor – Gotti et al. (2007) rely exclusively on a parser, without any additional sources of information – to knowledge rich and complex – GISTexter (Hickl et al., 2007) combines question answering, textual entailment, topic signature modules and a va1http://duc.nist.gov/, http://www.nist. gov/tac. riety of knowledge sources for summarization. The most frequently used knowledge source in NLP in general, and also for summarization, is WordNet (Fellbaum, 1998). Barzilay &amp; Elhadad (1999) use WordNet to model a text’s content relative to a topic based on lexical chains. The sentences intersected by the most and strongest chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini &amp; Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from set</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge.</title>
<date>2006</date>
<booktitle>In Proc. ofAAAI-06,</booktitle>
<pages>1301--1306</pages>
<contexts>
<context position="2905" citStr="Gabrilovich &amp; Markovitch, 2006" startWordPosition="448" endWordPosition="451">tion systems try to expand it. This will provide later stages of processing with more keywords/keyphrases for retrieving from the documents relevant fragments. In this paper we experiment with Wikipedia for topic expansion. The body of research involving Wikipedia as a source of knowledge is growing fast, as the NLP community finds more and more applications of this useful resource: it is used to acquire knowledge (Suchanek et al., 2007; Auer et al., 2007); to induce taxonomies and compute semantic relatedness (Ponzetto &amp; Strube, 2007b; 2007a); as a source of features for text classification (Gabrilovich &amp; Markovitch, 2006) and for answering questions (Ahn et al., 2004; Katz et al., 2005). The work presented here uses hyperlinks in Wikipedia articles to expand keywords and keyphrases extracted from the query. Ambiguous words are disambiguated using the context provided by the query. “Understanding” the documents to be summarized implies identifying the entities mentioned, how 763 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 763–772, Honolulu, October 2008.c�2008 Association for Computational Linguistics they are connected, and how they are related to the entities </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2006</marker>
<rawString>Gabrilovich, E. &amp; S. Markovitch (2006). Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge. In Proc. ofAAAI-06, pp. 1301–1306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gotti</author>
<author>G Lapalme</author>
<author>L Nerima</author>
<author>E Wehrli</author>
</authors>
<title>GOFAIsum: a symbolic summarizer for DUC.</title>
<date>2007</date>
<booktitle>In Proc. ofDUC-07.</booktitle>
<contexts>
<context position="5334" citStr="Gotti et al. (2007)" startWordPosition="834" endWordPosition="837">resent, summarization is not a novel task. Rath et al. (1961) and Edmundson (1969) have explored extractive summary formation, and have raised important evaluation issues for extractive summaries when compared to several human produced gold standards. Nowadays, summarization methods try to incorporate tools, methodologies and resources developed over the past decades. The NIST organized competitions under the Document Understanding Conferences – DUC (since 2008, Text Analysis Conference (TAC))1 events provide a forum for the comparison of a variety of approaches, ranging from knowledge poor – Gotti et al. (2007) rely exclusively on a parser, without any additional sources of information – to knowledge rich and complex – GISTexter (Hickl et al., 2007) combines question answering, textual entailment, topic signature modules and a va1http://duc.nist.gov/, http://www.nist. gov/tac. riety of knowledge sources for summarization. The most frequently used knowledge source in NLP in general, and also for summarization, is WordNet (Fellbaum, 1998). Barzilay &amp; Elhadad (1999) use WordNet to model a text’s content relative to a topic based on lexical chains. The sentences intersected by the most and strongest cha</context>
</contexts>
<marker>Gotti, Lapalme, Nerima, Wehrli, 2007</marker>
<rawString>Gotti, F., G. Lapalme, L. Nerima &amp; E. Wehrli (2007). GOFAIsum: a symbolic summarizer for DUC. In Proc. ofDUC-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grimmett</author>
<author>D Stirzaker</author>
</authors>
<title>Probability and Random Processes.</title>
<date>1989</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="22329" citStr="Grimmett &amp; Stirzaker, 1989" startWordPosition="3551" endWordPosition="3554">are closer to topic and topic expanded words we boost those that are more important in the documents. The starting point of the PageRank algorithm is the graph with weighted edges obtained in the previous step. The node weights are initialized with 1 (the starting value does not matter). Analysis of the documents graph for several topics has revealed that there is a large highly interconnected structure, and many disconnected small (2-3 nodes) fragments. Page Rank will run on this dense core structure. The PageRank algorithm is guaranteed to converge if the graph is aperiodic and irreducible (Grimmett &amp; Stirzaker, 1989). Aperiodicity implies that the greatest common divisor of the graph’s cycles is 1 – this condition is met. Irreducibility of the graph means that it has no leaves, and there are no two nodes with the same set of neighbours. The remedy in such cases is to connect each leaf to all other nodes in the graph, and conflate nodes with the same set of neighbours. Once the graph topology meets the PageRank convergence conditions, we run the algorithm. The original formula for computing the rank of a node at each iteration step is: 1 � d � PR(nz) = N + d where nz is a node, d is the damping factor (usu</context>
</contexts>
<marker>Grimmett, Stirzaker, 1989</marker>
<rawString>Grimmett, G. &amp; D. Stirzaker (1989). Probability and Random Processes. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
</authors>
<title>Incremental topic representations.</title>
<date>2004</date>
<booktitle>In Proc. of COLING-04,</booktitle>
<pages>583--589</pages>
<contexts>
<context position="6444" citStr="Harabagiu, 2004" startWordPosition="1013" endWordPosition="1014">ntent relative to a topic based on lexical chains. The sentences intersected by the most and strongest chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini &amp; Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin &amp; Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan &amp; Radev, 2004; Mihalcea &amp; Tarau, 2004). Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to determine the sentences that are the most salient from a collection of documents and closest to a given topic. At the word level, Leskovec et al. (2004) build a document graph using subject-verb-object triples, semantic normalization and coreference resolution. They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the nodes in the netw</context>
</contexts>
<marker>Harabagiu, 2004</marker>
<rawString>Harabagiu, S. (2004). Incremental topic representations. In Proc. of COLING-04, pp. 583–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>K Roberts</author>
<author>F L C C Lacatusu</author>
</authors>
<title>LCC’s GISTexter at DUC 2007: Machine reading for update summarization. In</title>
<date>2007</date>
<booktitle>Proc. ofDUC-07.</booktitle>
<contexts>
<context position="5475" citStr="Hickl et al., 2007" startWordPosition="858" endWordPosition="861">d important evaluation issues for extractive summaries when compared to several human produced gold standards. Nowadays, summarization methods try to incorporate tools, methodologies and resources developed over the past decades. The NIST organized competitions under the Document Understanding Conferences – DUC (since 2008, Text Analysis Conference (TAC))1 events provide a forum for the comparison of a variety of approaches, ranging from knowledge poor – Gotti et al. (2007) rely exclusively on a parser, without any additional sources of information – to knowledge rich and complex – GISTexter (Hickl et al., 2007) combines question answering, textual entailment, topic signature modules and a va1http://duc.nist.gov/, http://www.nist. gov/tac. riety of knowledge sources for summarization. The most frequently used knowledge source in NLP in general, and also for summarization, is WordNet (Fellbaum, 1998). Barzilay &amp; Elhadad (1999) use WordNet to model a text’s content relative to a topic based on lexical chains. The sentences intersected by the most and strongest chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini &amp; U</context>
</contexts>
<marker>Hickl, Roberts, Lacatusu, 2007</marker>
<rawString>Hickl, A., K. Roberts &amp; F. L. C. C. Lacatusu (2007). LCC’s GISTexter at DUC 2007: Machine reading for update summarization. In Proc. ofDUC-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Katz</author>
<author>G Marton</author>
<author>G Borchardt</author>
<author>A Brownell</author>
<author>S Felshin</author>
<author>D Loreto</author>
<author>J Louis-Rosenberg</author>
<author>B Lu</author>
<author>F Mora</author>
<author>S Stiller</author>
<author>O Uzuner</author>
<author>A Wilcox</author>
</authors>
<title>External knowledge sources for Question Answering.</title>
<date>2005</date>
<booktitle>In Proc. of TREC-14.</booktitle>
<contexts>
<context position="2971" citStr="Katz et al., 2005" startWordPosition="460" endWordPosition="463">h more keywords/keyphrases for retrieving from the documents relevant fragments. In this paper we experiment with Wikipedia for topic expansion. The body of research involving Wikipedia as a source of knowledge is growing fast, as the NLP community finds more and more applications of this useful resource: it is used to acquire knowledge (Suchanek et al., 2007; Auer et al., 2007); to induce taxonomies and compute semantic relatedness (Ponzetto &amp; Strube, 2007b; 2007a); as a source of features for text classification (Gabrilovich &amp; Markovitch, 2006) and for answering questions (Ahn et al., 2004; Katz et al., 2005). The work presented here uses hyperlinks in Wikipedia articles to expand keywords and keyphrases extracted from the query. Ambiguous words are disambiguated using the context provided by the query. “Understanding” the documents to be summarized implies identifying the entities mentioned, how 763 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 763–772, Honolulu, October 2008.c�2008 Association for Computational Linguistics they are connected, and how they are related to the entities in the topic. For this, we start again from the topic, and spread </context>
</contexts>
<marker>Katz, Marton, Borchardt, Brownell, Felshin, Loreto, Louis-Rosenberg, Lu, Mora, Stiller, Uzuner, Wilcox, 2005</marker>
<rawString>Katz, B., G. Marton, G. Borchardt, A. Brownell, S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu, F. Mora, S. Stiller, O. Uzuner &amp; A. Wilcox (2005). External knowledge sources for Question Answering. In Proc. of TREC-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proc. ofACSD-86,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="14967" citStr="Lesk, 1986" startWordPosition="2321" endWordPosition="2322">title. 1. If one exact match is found (e.g. Southern Poverty Law Center), extract the related concepts for this article. 2. If several exact or partial matches are found, use the larger context of the query to narrow down to the intended meaning. For example, Turkey – referring to the country – appears in several topics in the DUC 2007 data. There are multiple entries for “Turkey” in Wikipedia – for the country, the bird, cities with this name in the U.S. among others. We use a Lesk-like measure, and compute the overlap between the topic query and the set of hyperlinks in the first paragraph (Lesk, 1986). We choose the expansion for the entry with the highest overlap. If the query context does not help in disambiguation, we use the expansions for all partial matches that tie for the highest overlap. 3. If an article with the required name does not exist, the word will not be expanded. 2b. Query expansion with WordNet: Extract all nouns and NEs from the topic, and expand them with hypernyms, hyponyms and antonyms in WordNet 2.0: 766 1. If an word (or NE) W from the query corresponds to an unambiguous entry in WordNet, expand that entry. 2. If W has multiple senses, choose the sense(s) which ha</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk, M. (1986). Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proc. ofACSD-86, pp. 24– 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Leskovec</author>
<author>M Grobelnik</author>
<author>N Milic-Frayling</author>
</authors>
<title>Learning sub-structures of document semantic graphs for document summarization.</title>
<date>2004</date>
<booktitle>In Proc. of LinkKDD04.</booktitle>
<contexts>
<context position="6826" citStr="Leskovec et al. (2004)" startWordPosition="1074" endWordPosition="1077">anced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin &amp; Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan &amp; Radev, 2004; Mihalcea &amp; Tarau, 2004). Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to determine the sentences that are the most salient from a collection of documents and closest to a given topic. At the word level, Leskovec et al. (2004) build a document graph using subject-verb-object triples, semantic normalization and coreference resolution. They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the nodes in the network, and use these as attribute values in a machine learning algorithm, where the attribute that is learned is whether the node should appear in the final summary or not. Annotations for training come from human produced summaries. Mohamed &amp; Rajasekaran (2006) incrementally build a graph for a document collection by combining graph-representations of sentences. Links between enti</context>
</contexts>
<marker>Leskovec, Grobelnik, Milic-Frayling, 2004</marker>
<rawString>Leskovec, J., M. Grobelnik &amp; N. Milic-Frayling (2004). Learning sub-structures of document semantic graphs for document summarization. In Proc. of LinkKDD04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for automatic summarization.</title>
<date>2000</date>
<booktitle>In Proc. of COLING-00,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="6426" citStr="Lin &amp; Hovy, 2000" startWordPosition="1009" endWordPosition="1012"> model a text’s content relative to a topic based on lexical chains. The sentences intersected by the most and strongest chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini &amp; Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin &amp; Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan &amp; Radev, 2004; Mihalcea &amp; Tarau, 2004). Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to determine the sentences that are the most salient from a collection of documents and closest to a given topic. At the word level, Leskovec et al. (2004) build a document graph using subject-verb-object triples, semantic normalization and coreference resolution. They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Lin, C.-Y. &amp; E. Hovy (2000). The automated acquisition of topic signatures for automatic summarization. In Proc. of COLING-00, pp. 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedorn</author>
</authors>
<title>Summarizing similarities and differences among related documents.</title>
<date>1999</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="8623" citStr="Mani &amp; Bloedorn (1999)" startWordPosition="1356" endWordPosition="1359">&lt;narr&gt; What is the scope of operations of Amnesty International and what are the international reactions to its activities? Give examples of charges lodged by the organization and complaints against it. &lt; /narr&gt; &lt;docs&gt; ... &lt; /docs&gt; &lt; /topic&gt; &lt;topic&gt; &lt;num&gt; D0740I &lt; /num&gt; &lt;title&gt; round-the-world balloon flight &lt; /title&gt; &lt;narr&gt; Report on the planning, attempts and first successful balloon circumnavigation of the earth by Bertrand Piccard and his crew. &lt; /narr&gt; &lt;docs&gt; ... &lt; /docs&gt; &lt; /topic&gt; Figure 1: Sample topics from DUC 2007 assumed to be more representative with respect to the document topic. Mani &amp; Bloedorn (1999) build a “chronological” graph, in which sentence order is respected and each occurrence of a concept is a separate node. Edges between nodes cover several types of relations: adjacency (ADJ); identity – instance of the same word (SAME); other semantic links, in particular synonymy and hypernymy; PHRASE links connect components of a phrase; NAME indicate named entities; COREF link coreferential name instances. Among other things, they identify regions of the text salient to a user’s query, based on spreading activation starting from query words in this document graph. Spreading activation was </context>
<context position="18639" citStr="Mani &amp; Bloedorn, 1999" startWordPosition="2953" endWordPosition="2956"> this section are applied on a large graph that covers the entire document collection for one topic. The documents are processed in a similar way to the query – parsed with the Stanford Parser, output in dependency relation format, lemmatized using XTag’s morphological data file. The graph consists of nodes corresponding to lemmatized words and NEs in the documents, and edges correspoding to grammatical dependency relations. 4.1 Spreading Activation To find words/NEs related to the topic we spread an activation signal starting from the topic words and their expansions (in a manner similar to (Mani &amp; Bloedorn, 1999), and using an algorithm inspired by (Anderson, 1983)), which are given a node weight of 1. As we traverse the graph starting from these nodes, the signal is propagated by assigning a weight to each edge and each node traversed based on the signal strength. The signal strength diminishes with the distance from the node of origin depending on a signal decay parameter, according to the formula: wn(N0) = 1; st = (1 − d W, (Nt) ecay) * wn(Nt+1) = st; we(Nt, Nt+1)t+1 = we(Nt, Nt+1)t + st; where Nt is the current node; Nt+1 is the node we are moving towards; wn(Nt) is the weight of node Nt; st is th</context>
</contexts>
<marker>Mani, Bloedorn, 1999</marker>
<rawString>Mani, I. &amp; E. Bloedorn (1999). Summarizing similarities and differences among related documents. Information Retrieval, 1(1):35–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP-04,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="6573" citStr="Mihalcea &amp; Tarau, 2004" startWordPosition="1031" endWordPosition="1034">or the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini &amp; Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin &amp; Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan &amp; Radev, 2004; Mihalcea &amp; Tarau, 2004). Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to determine the sentences that are the most salient from a collection of documents and closest to a given topic. At the word level, Leskovec et al. (2004) build a document graph using subject-verb-object triples, semantic normalization and coreference resolution. They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the nodes in the network, and use these as attribute values in a machine learning algorithm, where the attribute that is learned is whether the node s</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Mihalcea, R. &amp; P. Tarau (2004). TextRank: Bringing order into texts. In Proc. EMNLP-04, pp. 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A A Mohamed</author>
<author>S Rajasekaran</author>
</authors>
<title>Query-based summarization based on document graphs.</title>
<date>2006</date>
<booktitle>In Proc. of DUC-06.</booktitle>
<contexts>
<context position="7304" citStr="Mohamed &amp; Rajasekaran (2006)" startWordPosition="1146" endWordPosition="1150">termine the sentences that are the most salient from a collection of documents and closest to a given topic. At the word level, Leskovec et al. (2004) build a document graph using subject-verb-object triples, semantic normalization and coreference resolution. They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the nodes in the network, and use these as attribute values in a machine learning algorithm, where the attribute that is learned is whether the node should appear in the final summary or not. Annotations for training come from human produced summaries. Mohamed &amp; Rajasekaran (2006) incrementally build a graph for a document collection by combining graph-representations of sentences. Links between entities in a sentence can be isa (within an NP) or related to (between different phrases in a sentence). Nodes and relations are weighted according to their connectivity, and sentence selection for the final summary is based on the most highly connected nodes. Ye &amp; Chua (2006) build an extractive summary based on a concept lattice, which captures in a hierarchical structure co-occurrences of concepts among sentences. Nodes higher in this structure correspond to frequently co-o</context>
</contexts>
<marker>Mohamed, Rajasekaran, 2006</marker>
<rawString>Mohamed, A. A. &amp; S. Rajasekaran (2006). Query-based summarization based on document graphs. In Proc. of DUC-06.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S P Ponzetto</author>
<author>M</author>
</authors>
<title>Strube (2007a). Deriving a large scale taxonomy from Wikipedia.</title>
<booktitle>In Proc. ofAAAI-07,</booktitle>
<pages>1440--1445</pages>
<marker>Ponzetto, M, </marker>
<rawString>Ponzetto, S. P. &amp; M. Strube (2007a). Deriving a large scale taxonomy from Wikipedia. In Proc. ofAAAI-07, pp. 1440–1445.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S P Ponzetto</author>
<author>M</author>
</authors>
<title>Strube (2007b). Knowledge derived from Wikipedia for computing semantic relatedness.</title>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>30</volume>
<pages>212</pages>
<marker>Ponzetto, M, </marker>
<rawString>Ponzetto, S. P. &amp; M. Strube (2007b). Knowledge derived from Wikipedia for computing semantic relatedness. Journal of Artificial Intelligence Research, 30:181– 212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Quillian</author>
</authors>
<title>Word concepts: A theory and simulation of some basic semantic capabilities.</title>
<date>1967</date>
<journal>Behavioural Science,</journal>
<volume>12</volume>
<issue>5</issue>
<contexts>
<context position="9332" citStr="Quillian, 1967" startWordPosition="1472" endWordPosition="1473">oncept is a separate node. Edges between nodes cover several types of relations: adjacency (ADJ); identity – instance of the same word (SAME); other semantic links, in particular synonymy and hypernymy; PHRASE links connect components of a phrase; NAME indicate named entities; COREF link coreferential name instances. Among other things, they identify regions of the text salient to a user’s query, based on spreading activation starting from query words in this document graph. Spreading activation was introduced in the 60s and 70s to model psychological processes of memory activation in humans (Quillian, 1967; Collins &amp; Loftus, 1975). In this approach we use Wikipedia as a source of knowledge for related concepts – the texts of hyperlinks in an article describing a concept are taken as its related concepts. The query is further expanded by using spreading activation to move away from the topic in a large graph that covers all documents for a given topic. From the nodes thus reached we select using a PageRank algorithm the ones that are most important in the documents. We study the impact of a decay parameter which controls how far to move from the topic, and the number of highest ranked nodes to b</context>
</contexts>
<marker>Quillian, 1967</marker>
<rawString>Quillian, M. R. (1967). Word concepts: A theory and simulation of some basic semantic capabilities. Behavioural Science, 12(5):410–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rath</author>
<author>A Resnick</author>
<author>T Savage</author>
</authors>
<title>The formation of abstracts by the selection of sentences.</title>
<date>1961</date>
<journal>American Documentation,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="4776" citStr="Rath et al. (1961)" startWordPosition="748" endWordPosition="751">e of useful knowledge for summarization, and that further expanding the topic within the associated set of documents improves the summarization results even more. We compare the performance of the summarization system to that of participating systems in the DUC competitions. The system we describe ranks 2nd, 9th and 5th in terms of ROUGESU4 on the DUC 2005, DUC 2006 and DUC 2007 data respectively. 2 Related Work While the recent exponential increase in the amount of information with which we must cope makes summarization a very desirable tool in the present, summarization is not a novel task. Rath et al. (1961) and Edmundson (1969) have explored extractive summary formation, and have raised important evaluation issues for extractive summaries when compared to several human produced gold standards. Nowadays, summarization methods try to incorporate tools, methodologies and resources developed over the past decades. The NIST organized competitions under the Document Understanding Conferences – DUC (since 2008, Text Analysis Conference (TAC))1 events provide a forum for the comparison of a variety of approaches, ranging from knowledge poor – Gotti et al. (2007) rely exclusively on a parser, without any</context>
</contexts>
<marker>Rath, Resnick, Savage, 1961</marker>
<rawString>Rath, G., A. Resnick &amp; T. Savage (1961). The formation of abstracts by the selection of sentences. American Documentation, 12(2):139–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Suchanek</author>
<author>G Kasneci</author>
<author>G Weikum</author>
</authors>
<title>YAGO: A core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proc. of WWW-07,</booktitle>
<pages>697--706</pages>
<contexts>
<context position="2714" citStr="Suchanek et al., 2007" startWordPosition="419" endWordPosition="422"> such knowledge on a large scale, and in numerous languages. To “understand” a user’s information request – one or more sentences or questions (the topic of the summary) – summarization systems try to expand it. This will provide later stages of processing with more keywords/keyphrases for retrieving from the documents relevant fragments. In this paper we experiment with Wikipedia for topic expansion. The body of research involving Wikipedia as a source of knowledge is growing fast, as the NLP community finds more and more applications of this useful resource: it is used to acquire knowledge (Suchanek et al., 2007; Auer et al., 2007); to induce taxonomies and compute semantic relatedness (Ponzetto &amp; Strube, 2007b; 2007a); as a source of features for text classification (Gabrilovich &amp; Markovitch, 2006) and for answering questions (Ahn et al., 2004; Katz et al., 2005). The work presented here uses hyperlinks in Wikipedia articles to expand keywords and keyphrases extracted from the query. Ambiguous words are disambiguated using the context provided by the query. “Understanding” the documents to be summarized implies identifying the entities mentioned, how 763 Proceedings of the 2008 Conference on Empiric</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Suchanek, F. M., G. Kasneci &amp; G. Weikum (2007). YAGO: A core of semantic knowledge. In Proc. of WWW-07, pp. 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ye</author>
<author>T-S Chua</author>
</authors>
<title>NUS at DUC 2006: Document concept lattice for summarization.</title>
<date>2006</date>
<booktitle>In Proc. of DUC-06.</booktitle>
<contexts>
<context position="7700" citStr="Ye &amp; Chua (2006)" startWordPosition="1210" endWordPosition="1213">n a machine learning algorithm, where the attribute that is learned is whether the node should appear in the final summary or not. Annotations for training come from human produced summaries. Mohamed &amp; Rajasekaran (2006) incrementally build a graph for a document collection by combining graph-representations of sentences. Links between entities in a sentence can be isa (within an NP) or related to (between different phrases in a sentence). Nodes and relations are weighted according to their connectivity, and sentence selection for the final summary is based on the most highly connected nodes. Ye &amp; Chua (2006) build an extractive summary based on a concept lattice, which captures in a hierarchical structure co-occurrences of concepts among sentences. Nodes higher in this structure correspond to frequently co-occurring terms, and are 764 &lt;topic&gt; &lt;num&gt; D0704A &lt; /num&gt; &lt;title&gt; Amnesty International &lt; /title&gt; &lt;narr&gt; What is the scope of operations of Amnesty International and what are the international reactions to its activities? Give examples of charges lodged by the organization and complaints against it. &lt; /narr&gt; &lt;docs&gt; ... &lt; /docs&gt; &lt; /topic&gt; &lt;topic&gt; &lt;num&gt; D0740I &lt; /num&gt; &lt;title&gt; round-the-world ball</context>
</contexts>
<marker>Ye, Chua, 2006</marker>
<rawString>Ye, S. &amp; T.-S. Chua (2006). NUS at DUC 2006: Document concept lattice for summarization. In Proc. of DUC-06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>