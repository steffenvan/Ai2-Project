<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003561">
<title confidence="0.89985">
Yet Another Chart-Based Technique for Parsing Ill-Formed Input
Tsuneaki Kato
</title>
<note confidence="0.3239445">
NTT Information and Communication Systems Laboratories
1-2356 Take, Yokosuka-shi, Kanagawa, 238-03 JAPAN
</note>
<email confidence="0.99832">
kato@nttnly.ntt.jp
</email>
<sectionHeader confidence="0.993978" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992307692308">
A new chart-based technique for parsing ill-formed
input is proposed. This can process sentences
with unknown/misspelled words, omitted words
or extraneous words. This generalized parsing
strategy is, similar to Mellish&apos;s, based on an
active chart parser, and shares the many
advantages of Mellish&apos;s technique. It is based on
pure syntactic knowledge, it is independent of all
grammars, and it does not slow down the original
parsing operation if there is no ill-formedness.
However, unlike Mellish&apos;s technique, it doesn&apos;t
employ any complicated heuristic parameters.
There are two key points. First, instead of using
a unified or interleaved process for finding errors
and correcting them, we separate the initial error
detection stage from the other stages and adopt a
version of bi-directional parsing. This effectively
prunes the search space. Second, it employs
normal top-down parsing, in which each parsing
state reflects the global context, instead of top-
down chart parsing. This enables the technique to
determine the global plausibility of candidates
easily, based on an admissible A* search. The
proposed strategy could enumerate all possible
minimal-penalty solutions in just 4 times the
time taken to parse the correct sentences.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964507692308">
It is important that natural language interface systems
have the capability of composing the globally most
plausible explanation if a given input can not be
syntactically parsed. This would be useful for handling
erroneous inputs from the user and for offsetting
grammar and lexicon insufficiency. Also, such a
capability could be applied to deal with the
ungrammatical sentences and sentence fragments that
frequently appear in spoken dialogs (Bear, Dowding and
Shriberg, 1992). Several efforts have been conducted to
achieve this objective ((Lang, 1988; Saito and Tomita,
1988), for example.) One major decision to be made in
designing this capability is whether knowledge other
than purely syntactic knowledge is to be used. Other-
than syntactic knowledge includes grammar specific
recovery rules such as meta-rules (Weishedel and
Sondheimer, 1983), semantic or pragmatic knowledge
which may depend on a particular domain (Carbonell and
Hayes, 1983) or the characteristics of the ill-formed
utterances observed in human discourse (Hindle, 1983).
Although it is obvious that the utilizing such knowledge
allows us to devise more powerful strategies, we should
first determine the effectiveness of using only syntactic
knowledge. Moreover, the result can be applied widely,
as using syntactic knowledge is a base of the most of
strategies.
One significant advance in the usage of syntactic
knowledge was contained in the technique proposed by
Mellish (1989). It can handle not only
unknown/misspelled words, but also omitted words and
extraneous words in sentences. It can deal with such
problems, and develop plausible explanations quickly
since it utilizes the full syntactic context by using an
active chart parser (Kay, 1980; Gazdar and Mellish,
1989). One problem with his technique is that its
performance heavily depends on how the search
heuristics, which is implemented as a score calculated
from six parameters, is set. The heuristics complicates
the algorithm significantly. This must be one of reasons
why the performance of the method, as Mellish himself
noted, dropped dramatically when the input contains
multiple errors.
This paper proposes a new technique for parsing
inputs that contain simple kinds of ill-formedness. This
generalized parsing strategy is, similar to Mellish&apos;s,
based on an active chart parser, and so shares the many
advantages of Mellish&apos;s technique. It is based on pure
syntactics, it is independent of all grammars, and it does
not slow down the original parsing operation if there is
no ill-formedness. However, unlike Mellish&apos;s technique,
it doesn&apos;t employ any complicated heuristic parameters.
There are two key points. First, instead of using a
unified or interleaved process for finding errors and
correcting them, we separate the initial error detection
stage from the other stages and adopt a version of bi-
directional parsing, which has been pointed out to be a
useful strategy for fragment parsing by itself (Satta and
Stock, 1989). This effectively prunes the search space
and allows the new technique to take full account of the
right-side context. Second, it employs normal top-down
parsing, in which each parsing state reflects the global
context, instead of top-down chart parsing. This enables
the technique to determine the global plausibility of
candidates easily. The results of preliminary experiments
are encouraging. The proposed strategy could enumerate
</bodyText>
<page confidence="0.998157">
107
</page>
<bodyText confidence="0.997357">
all possible minimal-penalty solutions in just 4 times
the time taken to parse the correct sentences. That is, it
is almost twice as fast as Mellish&apos;s strategy.
</bodyText>
<sectionHeader confidence="0.742897" genericHeader="method">
2 Mellish&apos;s Technique And Its Problems
</sectionHeader>
<bodyText confidence="0.999016651685393">
The basic strategy of Mellish&apos;s technique is to run a
bottom-up parser over the input and then, if this fails to
find a complete parse, to run a generalized top-down
parser over the resulting chart to hypothesize complete
parse candidates. When the input is well-formed, the
bottom-up parser, precisely speaking, a left corner parser
without top-down filtering, would generate the parse
without any overhead. Even if it failed, it is guaranteed
to find all complete constituents of all possible parses.
Reference to these constituents, enables us to avoid
repeating existing works and to exploit the full syntactic
context rather just the left-side context of error
candidates. The generalized top-down parser attempts to
find out minimal errors by refining the set of &amp;quot;needs&amp;quot;
that originates with bottom-up parsing. Each need
indicates the absence of an expected constituent. The
generalized parser hypothesizes, and so remedies an error,
when it was sufficiently focused on. Next, the parser
tries to construct a complete parse by taking account of
the hypothesis. In the case of multiple errors, the
location and recovery phases are repeated until a complete
parse is obtained.
The data structure introduced for representing
information about local needs is called the generalized
edge. It is an extension of active and inactive edges, and
is described as
&lt;C from S to E needs csi from sl to el,
cs2 from s2 to e2, , csn from sn to en &gt;
where C is category, csi are sequences of categories
(which will be shown inside square brackets), S, E, si,
and ei are positions in the input. The special symbol
&amp;quot;*&amp;quot; denotes the position that remains to be determined.
The presence of an edge of this kind in the chart indicates
that the parser is attempting to find a phrase of category
C that covers the input from position S to E but that in
order to succeed it must still satisfy all the needs listed.
Each need satisfies a sequence of categories csi that must
be found contiguously to occupy the portion from si to
ei. An edge with an empty need, which corresponds to
an inactive edge is represented as
&lt;C from S to E needs nothing&gt;.
The generalized top-down parser that uses the
generalized edge as the data structure is governed by six
rules: three for finding out errors and the other three for
recovering from the three kinds of error. The three error
locating rules are the top-down rule, the fundamental rule
and the simplification rule. The first one is also used in
the ordinary top-down chart parser, and the third one is
just for house keeping. The second rule, the fundamental
rule, directs to combine an active edge with a inactive
edge. It was extended from the ordinary rule so that
found constituents could be incorporated from either
direction. However, the constituents that can be
absorbed are limited to those in the first category
sequence; that is, one of the categories belonging to csi.
The application of the six rules is mainly controlled by
the scores given to edges, that is, agenda control is
employed. The score of a particular edge reflects its
global plausibility and is calculated from six parameters,
one of which, for example, says that edges that arise
from the fundamental rule are preferable to those that
arise from the top-down rule.
Although Mellish&apos;s technique has a lot of advantages
such as the ability to utilize the right-side context of
errors and independence of a specific grammar, it can
create a huge number of edges, as it mainly uses the top-
down rule for finding errors. That is, refining a set of
error candidates toward a pre-terminal category by
applying only the top-down rule may create too many
alternatives. In addition, since the generalized edges
represent just local needs and don&apos;t reflect the global
needs that created them, it is hard to decide if they should
be expanded. In particular, these problems become
critical when parsing ill-formed inputs, since the top-
down rule may be applied without any anchoring; pre-
terminals can not be considered as anchors, as pre-
terminals may be freely created by error recovery rules.
This argument also applies to the start symbol, as that
symbol may be created depending the constituent
hypothesized by error recovery rules and the fundamental
rule. Mellish uses agenda control to prevent the
generation of potentially useless edges. For this
purpose, the agenda control needs complicated heuristic
scoring, which complicates the whole algorithm.
Moreover, so that the scoring reflects global plausibility,
it must employs a sort of dependency analysis, a
mechanism for the propagation of changes and an easily
reordered agenda, which clearly contradicts his original
idea in which edges must be reflected only local needs.
</bodyText>
<sectionHeader confidence="0.990035" genericHeader="method">
3 Proposed Algorithm
</sectionHeader>
<bodyText confidence="0.999898470588235">
The technique proposed here resolves the above problems
as follows. First, some portion of the error location
process is separated from and precedes the processes that
are governed by agenda control, and is archived by using
a version of bi-directional parsing. Second, so that the
search process can be anchored by the start symbol, a
data structure is created that can represent global
plausibility. Third, in order to reduce the dependency on
the top-down rule, a rule is developed that uses two
active edges to locate errors. This process is closer to
ordinary top-down parsing than chart parsing and global
plausibility scoring is accurate and easily calculated. For
simplicity of explanation, simple CF-PSG grammar
formalism is assumed throughout this paper, although
there are obvious generalizations to other formalism such
as DCG (Pereira and Warren, 1980) or unification based
grammars (Shieber, 1986).
</bodyText>
<page confidence="0.992805">
108
</page>
<table confidence="0.914855066666667">
Bottom-up rule:
&lt;C from S to E needs nothing&gt;
Cl --&gt; ...Cs&apos; C Cs2... where Csi is not empty (in the grammar)
-------------------------------------
&lt;C1 from * to E2 needs Cs! from * to S. Cs2 from E to E2&gt;
where if Cs2 is empty then E2 E else E2=*.
Fundamental rule:
&lt;C from S to E needs ... , [...Csl 1, Cl, Cs12...] from s to e , ...&gt;
&lt;C1 from S1 to El needs nothing&gt;
&lt;C from S to E needs ... , Csii from si to S1 ,Cs i2 from El to el ,...&gt;
where si S1 or si=*, El el ore =*.
Simplification rule:
&lt;C from S to E needs
from Si_i to s, [] from s to s, Csi+i from s to ei+1, ...&gt;
&lt;C from S to E needs ..., Csi_i from 5i_1 to s, Cs1+1 from s to , ...&gt;
</table>
<figureCaption confidence="0.99915">
Figure 1. The Bi-Directional Parsing Rules
</figureCaption>
<bodyText confidence="0.985564818181818">
The first phase of the process is invoked after the
failure of left corner parsing. The bottom-up parsing
leaves behind all complete constituents of every possible
parse and unsatisfied active edges for all error points that
are to the immediate right of sequences of constituents
corresponding to the RHS. Since parsing proceeds left
to right, an active edge is generated only when an error
point exists to the right of the found constituents. In the
first phase, bi-directional bottom-up parsing generates all
generalized edges that represent unsatisfied expectations
to the right and left of constituents. From some
perspectives, the role this phase plays is similar to that
of the covered bi-directional phase of the Picky parser
(Magerman and Weir, 1992), though the method
proposed herein does not employ stochastic information
at all. This process can be described in three rules as
shown in Figure 1. As can be seen, this is bi-directional
bottom-up parsing that uses generalized edges as the data
structure. For simplicity, the details for avoiding
duplicated edge generation have been omitted. It is worth
noting that after this process, the needs listed in each
generalized edge indicate that the expected constituents
did not exist, while, before this process, a need may exist
just because an expectation has not been checked.
The second phase finds out errors and corrects them.
The location operation proceeds by refining a need into
more precise one, and it starts from the global need that
refers to the start symbol, S, from 0 to n, where n is the
length of the given input. In the notion of generalized
edges, that need can be represented as,
&lt;GOAL from 0 to n needs [S] from 0 to n&gt;.
The data structure reflecting global needs directly is used
in this phase, so the left part of each generalized edge is
redundant and can be omitted. In addition, two values, g
and h, are introduced. g denotes how much cost has been
expended for the recovery so far, and h is the estimation
of how much cost will be needed to reach a solution.
Cost involves solution plausibility; solutions with low
plausibility have high costs. Thus, the data structure
used in this phase is,
&lt;needs Cs! from si to el, c52 from s2 toe, ,
csn from sn to en, g, h&gt;.
Here, the number of errors corrected so far is taken as g,
and the total number of categories in the needs is used as
h. As mentioned above, since the needs listed indicate
only the existence of errors as detected by the preceding
process and to be refined, the value of h is always less
than or equal to the number of the errors that must be
corrected to get a solution. That is, the best first search
using g+h as the cost functions is an admissible A*
search (Rich and Knight, 1991). Needless to say, more
sophisticated cost functions can also be used, in which,
for example, the cost depends on the kind of error.
The rules governing the second phase, which
correspond to the search state transition operators in the
context of search problems, are shown in Figure 2. The
top-down rule and the refining rule locate errors and the
other three rules are for correcting them. Most important
is the refining rule, which tries to find out errors by
using generalized edges in a top-down manner toward pre-
terminals. This reduces the frequency of using the top-
down rule and prevents an explosion in the number of
alternatives.
This process starts from
&lt;needs [S] from 0 to n, g: 0, h: 1&gt;.
To reach the following need means to get one solution.
</bodyText>
<note confidence="0.432029">
&lt;needs nothing, g: h: 0&gt;.
</note>
<page confidence="0.987436">
109
</page>
<figure confidence="0.983226047619047">
Top-down rule:
&lt;needs [Ci...Csi] from si to El, g: G, h: H&gt;
C1 ...RHS (in grammar)
&lt;needs [...RHS ...Csi ] form si to E1 , g: G, h: H+(length of RHS)-1&gt;
Refining rule:
&lt;needs [...Csi 1, C1, Cs12...] from si to el , , g: G, h: H&gt;
&lt;C1 from S to E needs Csi from S1 to El , ,Csn from Sn to En &gt;
&lt;needs Csii from si to S , Csi from Si to El , , Csn from Sn to En ,
Csi2 from E to el , g: G, h: H+E(length of Csn)-1&gt;
The result must be well-formed, that is sl &lt;Si or sl—* or S1—* and so on.
Garbage rule:
&lt;needs [CI ...Csi] from si to ei , g: G, h: H&gt; where C1 is a pre-terminal
&lt;Ci from S1 to E1 needs nothing&gt; where sl 5- Si
&lt;needs Csi from El to ei , g: G-F(Si-s1), h: H-1&gt;
Unknown word rule:
&lt;needs [Ci...Csi] from si to ei , g: G, h: H&gt; where Ci is a pre-terminal
&lt;needs Csi from si+1 to el, g: G+1, h: H-1&gt;
where the edge, &lt;Ci from si to si+1 needs nothing&gt; does not exist in the chart
Empty category rule:
&lt;needs Csi from s to s , Cs2 from s2 to e2 ,...,g: G, h: H&gt;
&lt;needs Cs2 from s2 to e2 , g: G+(length of Csi), h: H-(length of Cs1)&gt;
</figure>
<figureCaption confidence="0.920414222222222">
Figure 2. The Error Locating and Recovery Rules
incidental. In reality, application of the top-down rule
may be meaningful only when all the constituents listed
in the RHS of a grammar rule contain errors. In every
other case, generalized edges derived from that rule must
have been generated already by the first phase. The
application of the top-down rule can be restricted to cases
involving unary rules, if one assumes at most one error
may exist.
</figureCaption>
<sectionHeader confidence="0.988508" genericHeader="method">
4 Preliminary Experiments
</sectionHeader>
<bodyText confidence="0.943568533333333">
In order to evaluate the technique described above, some
preliminary experiments were conducted. The
experiments employed the same framework as used by
Mellish, and used a similar sized grammar, the small e-
free CF-PSG for a fragment of English with 141 rules
and 72 categories. Random sentences (10 for each length
considered) were generated from the grammar, and then
random occurrences of specific types of errors were
introduced into these sentences. The errors considered
were none, deletion of one word, adding one known or
unknown word, and substituting one unknown or known
word for one word of the sentence. The amount of work
done by the parser was calculated using the concept of
&amp;quot;cycle&amp;quot;. The parser consumes one cycle for processing
each edge. The results are shown in Table 1. The
</bodyText>
<page confidence="0.951611">
110
</page>
<bodyText confidence="0.99892325">
The need with the smallest value of g+h is processed
first. If two needs have the same value of g-i-h, the one
with the smaller h values dominates. This control
strategy guarantees to find the solution with minimal
cost first; that is, the solution with the minimum
number of recoveries.
Figure 3 shows an example of this technique in
operation. (a) shows the sample grammar adopted, (b)
shows the input to be processed, and (c) shows some of
the edges left behind after the failure of the original
bottom-up parsing. As shown in (d), the first phase
generates several edges that indicate unsatisfied
expectations to the left of found constituents. The
second phase begins with need (e-1). Among the others,
(e-2) and (e-3) are realized by applying the refining rule
and the top-down rule, respectively. Since (e-2) has the
smallest value of g+h, it takes precedence to be
expanded. The refining rule processes (e-2) and generates
(e-4) and (e-7), among others. The solution indicated by
(e-6), which says that the fifth word of the input must be
a preposition, is generated from (e-4). Another solution
indicated by (e-9), which says that the fifth word of the
input must be a conjunctive is derived from (e-7). That
the top-down rule played no role in this example was not
</bodyText>
<figureCaption confidence="0.988667">
Figure 3. An Example of the Error Recovery Process
</figureCaption>
<figure confidence="0.9224">
(a) The grammar: NP —*N NP —&gt; Det N
S --&gt; NP VP ...(a-1) VP—) Vt NP VP—Vi
NP —&gt; NP C NP PP P NP ...(a-2)
VP --) VP PP
(b) The input:
The lady bought cakes an the shop
0 1 2 3 4 5 6 7
(c) Examples of the edges left behind:
</figure>
<table confidence="0.956627826086957">
&lt;NP from 0 to 2 needs nothing&gt;
&lt;Vi from 2 to 3 needs nothing&gt;
&lt;NP from 3 to 4 needs nothing&gt;
&lt;NP from 5 to 7 needs nothing&gt;
&lt;S from 0 to * needs [VP] from 2 to *&gt;
&lt;VP from 2 to * needs [PP] from 4 to *&gt;
&lt;VP from 2 to * needs [NP] from 3 to *&gt;
&lt;/NIP from 3 to * needs [C NP] from 4 to *&gt;
(d) Examples edges generated in the bi-directional parsing:
&lt;PP from * to 7 needs [P] from * to 5&gt; ...(d-1) Bottom up rule, (c-1), (a-2)
&lt;NP from 3 to 7 needs [C] from 4 to 5&gt; ...(d-2) Fundamental rule, (c-1), (c-5)
(e) Focusing on and recovering from errors:
&lt;needs [S] from 0 to 7, g:0, h:1&gt; ...(e-1) Initial needs
&lt;needs [VP] from 2 to 7, g:0, h:1&gt; ...(e-2) Refining rule, (e-1), (c-2)
&lt;needs [NP VP] from 0 to 7, g:0, h:2&gt; ...(e-3) Top-down rule, (e-1), (a-1)
&lt;needs [PP] from 4 to 7, g:0, h:1&gt; ...(e-4) Refining rule, (e-2), (c-3)
&lt;needs [P] from 4 to 5, g:0, h:1&gt; ...(e-5) Refining rule, (e-4), (d-1)
&lt;needs nothing, g:1, h:0&gt; ...(e-6) Unknown word rule, (e-5)
The fifth word, &amp;quot;an&amp;quot;, is hypothesized to be an unknown preposition (P)
&lt;needs [NP] from 3 to 7, g:0, h:1&gt; ...(e-7) Refining rule, (e-2), (c-4)
&lt;needs [C] from 4 to 5, g:0, h:1&gt; ...(e-8) Refining rule, (e-7), (d-2)
&lt;needs nothing, g:1, h:0&gt; ...(e-9) Unknown word rule, (e-8)
The fifth word, &amp;quot;an&amp;quot;, hypothesized to be an unknown conjunctive (C)
</table>
<bodyText confidence="0.999954894736842">
statistics in the table are described as follows. BU cycles
is the number of cycles taken to exhaust the chart in the
initial bottom-up parsing. BD cycles is the number of
cycles required for bi-directional bottom-up parsing in the
first phase. #solns is the number of different solutions
and represents descriptions of possible errors. First/Last
is the number of cycles required for error location and
recovery to find the first / last solution. LR cycles is the
number of cycles in the error locating and recovery phase
required to exhaust all possibilities of sets of errors with
the same penalty as the first solution.
The preliminary results show that, for short sentences
with one error, enumerating all possible minimum-
penalty errors takes about 4 times as long as parsing the
correct sentences. This is almost twice the speed of
Mellish&apos;s strategy. As 75% of the process are occupied
by the first bi-directional parsing operation, more cycles
are needed to get the first solution with the proposed
technique than with Mellish&apos;s strategy.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9994925">
The second phase of the proposed technique is based on
ordinary top-down parsing or tree search rather than chart
parsing. As a consequence, some error location
operations may be redundant, as Mellish pointed out.
For example, suppose a new grammar rule, N ---&gt; N PP
is added to the grammar given in Figure 3. In that case,
the following edge, located in the first phase, may cause
a redundant error locating process, as the same search is
triggered by (e-4).
&lt;N from 3 to * needs [PP] from 4 to *&gt;.
One way for avoiding such redundancies is to use a data
structure that reflects just local needs. However, it is
true that an effective error location process must take
into account global needs. There is a tradeoff between
simplicity and the avoidance of duplicated efforts. The
technique proposed here employs a data structure that
</bodyText>
<page confidence="0.999203">
111
</page>
<tableCaption confidence="0.999334">
Table 1. Preliminary Experimental Results
</tableCaption>
<table confidence="0.998369555555556">
Error Length of BU cycles BD cycles #solns - Last LR cycles
original First
None 6 70 1.3
9 114 1.4
12 170 2.0
Delete 6 42 132 6.0 8 24 31
one
word
9 79 255 4.5 19 32 43
12 111 378 6.2 25 43 57
, ,
Ackl 6 60 191 3.0 14 20 28
unknown
word
9 99 322 3.7 25 37 46
12 147 534 2.6 46 61 72
,
Add 6 69 221 5.7 14 25 33
known
word
9 94 292 4.7 24 38 55
12 159 578 6.9 51 70 80
.
Substitute 6 50 153 2.7 9 14 20
unknown
word
9 76 239 4.2 21 34 43
.
12 109 363 3.2 34 46 58
.
Substitute 6 51 151 3.8 9 18 27
known
word
9 82 256 3.2 15 25 33
-
12 116 384 3.8 33 58 71
</table>
<bodyText confidence="0.999573666666667">
directly reflects the global needs. Mellish, on the other
hand, utilized a structure that reflected just local needs
and tried to put global needs into the heuristic function.
The result, at least so far as confirmed by tests, was that
pruning allowed the simple method to overcome the
drawback of duplicated effort. Moreover, Mellish&apos;s
dependency control mechanism, introduced to maintain
the plausibility scores, means that edges are no longer
local. In addition, it can be expected that a standard
graph search strategy for avoiding duplicated search is
applicable to the technique proposed.
Theoretical investigation is needed to confirm how
the number of grammar rules and the length of input will
affect the amount of computation needed. Furthermore,
the algorithm has to be extended in order to incorporate
the high level knowledge that comes from semantics and
pragmatics. Stochastic information such as statistics on
category trigrams must be useful for effective control.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999711090909091">
Bear, John, Dowding, John and Shriberg, Elizabeth
(1992). Integrating Multiple Knowledge Sources
for Detection and Correction of Repairs in
Human-Computer Dialog. Proceedings of 30th
ACL, 56 - 63.
Carbonell, Jaime G. and Hayes, Philip J. (1983).
Recovery Strategies for Parsing Extra
grammatical Language. JACL, 9 (3-4), 123 - 146.
Gazdar, Gerald and Mellish, Chris (1989). Natural
Language Processing in LISP. Workingham:
Addison-Wesley.
Hindle, Donald (1983). Deterministic Parsing of
Syntactic Non-fluencies. Proceedings of 21st
ACL, 123 - 128.
Kay, Martin (1980). Algorithm Schemata and Data
Structures in Syntactic Processing. Research
Report CSL-80-12 Xerox PARC.
Lang, Bernard (1988). Parsing Incomplete Sentences.
Proceedings of COLING 88, 365 - 371.
Magerman, David M. and Weir, Carl (1992). Efficiency,
Robustness and Accuracy in Picky Chart Parsing.
Proceedings of 30th ACL, 40- 47.
Mellish, Chris S. (1989). Some Chart-Based Techniques
for Parsing Ill-Formed Input. Proceedings of 27th
ACL, 102 - 109.
Pereira, Fernando C.N. and Warren, David, H.D. (1980).
Definite Clause Grammars for Language Analysis
- A Survey of the Formalism and a Comparison
with Augmented Transition Networks. Artificial
Intelligence, 13 (3), 231 - 278.
Rich, Elaine and Knight, Kevin (1991). Artificial
Intelligence (2nd ed.). New York: McGraw-Hill.
Saito, Hiroaki and Tomita, Masaru (1988). Parsing
Noisy Sentences. Proceedings of COLING 88,
561 - 566.
Satta, Giorgio and Stock, Oliviero (1989). Formal
Properties and Implementation of Bidirectional
Charts. Proceedings of IJCAI 89, 1480 - 1485.
Shieber, Stuart M. (1986). An Introduction to
Unification-Based Approaches to Grammar.
Stanford: CSLI Lecture Notes 4.
Weishedel, Ralph M. and Sondheimer, Norman K.
(1983). Meta-Rules as a Basis for Processing III-
Formed Input. JACL, 9 (3-4), 161 - 177.
</reference>
<page confidence="0.998277">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964999">
<title confidence="0.998011">Yet Another Chart-Based Technique for Parsing Ill-Formed Input</title>
<author confidence="0.999028">Tsuneaki Kato</author>
<affiliation confidence="0.999697">NTT Information and Communication Systems Laboratories</affiliation>
<address confidence="0.985632">1-2356 Take, Yokosuka-shi, Kanagawa, 238-03 JAPAN</address>
<email confidence="0.984516">kato@nttnly.ntt.jp</email>
<abstract confidence="0.999871185185185">A new chart-based technique for parsing ill-formed input is proposed. This can process sentences with unknown/misspelled words, omitted words or extraneous words. This generalized parsing strategy is, similar to Mellish&apos;s, based on an active chart parser, and shares the many advantages of Mellish&apos;s technique. It is based on pure syntactic knowledge, it is independent of all grammars, and it does not slow down the original parsing operation if there is no ill-formedness. However, unlike Mellish&apos;s technique, it doesn&apos;t employ any complicated heuristic parameters. There are two key points. First, instead of using a unified or interleaved process for finding errors and correcting them, we separate the initial error detection stage from the other stages and adopt a version of bi-directional parsing. This effectively prunes the search space. Second, it employs normal top-down parsing, in which each parsing state reflects the global context, instead of topdown chart parsing. This enables the technique to determine the global plausibility of candidates easily, based on an admissible A* search. The proposed strategy could enumerate all possible minimal-penalty solutions in just 4 times the time taken to parse the correct sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Bear</author>
<author>John Dowding</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Integrating Multiple Knowledge Sources for Detection and Correction of Repairs in Human-Computer Dialog.</title>
<date>1992</date>
<booktitle>Proceedings of 30th ACL,</booktitle>
<pages>56--63</pages>
<marker>Bear, Dowding, Shriberg, 1992</marker>
<rawString>Bear, John, Dowding, John and Shriberg, Elizabeth (1992). Integrating Multiple Knowledge Sources for Detection and Correction of Repairs in Human-Computer Dialog. Proceedings of 30th ACL, 56 - 63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Philip J Hayes</author>
</authors>
<title>Recovery Strategies for Parsing Extra grammatical Language.</title>
<date>1983</date>
<journal>JACL,</journal>
<volume>9</volume>
<pages>3--4</pages>
<contexts>
<context position="2419" citStr="Carbonell and Hayes, 1983" startWordPosition="351" endWordPosition="354">ld be applied to deal with the ungrammatical sentences and sentence fragments that frequently appear in spoken dialogs (Bear, Dowding and Shriberg, 1992). Several efforts have been conducted to achieve this objective ((Lang, 1988; Saito and Tomita, 1988), for example.) One major decision to be made in designing this capability is whether knowledge other than purely syntactic knowledge is to be used. Otherthan syntactic knowledge includes grammar specific recovery rules such as meta-rules (Weishedel and Sondheimer, 1983), semantic or pragmatic knowledge which may depend on a particular domain (Carbonell and Hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (Hindle, 1983). Although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge. Moreover, the result can be applied widely, as using syntactic knowledge is a base of the most of strategies. One significant advance in the usage of syntactic knowledge was contained in the technique proposed by Mellish (1989). It can handle not only unknown/misspelled words, but also omitted words and extraneous words</context>
</contexts>
<marker>Carbonell, Hayes, 1983</marker>
<rawString>Carbonell, Jaime G. and Hayes, Philip J. (1983). Recovery Strategies for Parsing Extra grammatical Language. JACL, 9 (3-4), 123 - 146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Chris Mellish</author>
</authors>
<date>1989</date>
<booktitle>Natural Language Processing in LISP. Workingham:</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="3223" citStr="Gazdar and Mellish, 1989" startWordPosition="474" endWordPosition="477">powerful strategies, we should first determine the effectiveness of using only syntactic knowledge. Moreover, the result can be applied widely, as using syntactic knowledge is a base of the most of strategies. One significant advance in the usage of syntactic knowledge was contained in the technique proposed by Mellish (1989). It can handle not only unknown/misspelled words, but also omitted words and extraneous words in sentences. It can deal with such problems, and develop plausible explanations quickly since it utilizes the full syntactic context by using an active chart parser (Kay, 1980; Gazdar and Mellish, 1989). One problem with his technique is that its performance heavily depends on how the search heuristics, which is implemented as a score calculated from six parameters, is set. The heuristics complicates the algorithm significantly. This must be one of reasons why the performance of the method, as Mellish himself noted, dropped dramatically when the input contains multiple errors. This paper proposes a new technique for parsing inputs that contain simple kinds of ill-formedness. This generalized parsing strategy is, similar to Mellish&apos;s, based on an active chart parser, and so shares the many ad</context>
</contexts>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>Gazdar, Gerald and Mellish, Chris (1989). Natural Language Processing in LISP. Workingham: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Deterministic Parsing of Syntactic Non-fluencies.</title>
<date>1983</date>
<booktitle>Proceedings of 21st ACL,</booktitle>
<pages>123--128</pages>
<contexts>
<context position="2514" citStr="Hindle, 1983" startWordPosition="366" endWordPosition="367">en dialogs (Bear, Dowding and Shriberg, 1992). Several efforts have been conducted to achieve this objective ((Lang, 1988; Saito and Tomita, 1988), for example.) One major decision to be made in designing this capability is whether knowledge other than purely syntactic knowledge is to be used. Otherthan syntactic knowledge includes grammar specific recovery rules such as meta-rules (Weishedel and Sondheimer, 1983), semantic or pragmatic knowledge which may depend on a particular domain (Carbonell and Hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (Hindle, 1983). Although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge. Moreover, the result can be applied widely, as using syntactic knowledge is a base of the most of strategies. One significant advance in the usage of syntactic knowledge was contained in the technique proposed by Mellish (1989). It can handle not only unknown/misspelled words, but also omitted words and extraneous words in sentences. It can deal with such problems, and develop plausible explanations quickly since</context>
</contexts>
<marker>Hindle, 1983</marker>
<rawString>Hindle, Donald (1983). Deterministic Parsing of Syntactic Non-fluencies. Proceedings of 21st ACL, 123 - 128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Algorithm Schemata and Data Structures in Syntactic Processing.</title>
<date>1980</date>
<tech>Research Report CSL-80-12 Xerox PARC.</tech>
<contexts>
<context position="3196" citStr="Kay, 1980" startWordPosition="472" endWordPosition="473">evise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge. Moreover, the result can be applied widely, as using syntactic knowledge is a base of the most of strategies. One significant advance in the usage of syntactic knowledge was contained in the technique proposed by Mellish (1989). It can handle not only unknown/misspelled words, but also omitted words and extraneous words in sentences. It can deal with such problems, and develop plausible explanations quickly since it utilizes the full syntactic context by using an active chart parser (Kay, 1980; Gazdar and Mellish, 1989). One problem with his technique is that its performance heavily depends on how the search heuristics, which is implemented as a score calculated from six parameters, is set. The heuristics complicates the algorithm significantly. This must be one of reasons why the performance of the method, as Mellish himself noted, dropped dramatically when the input contains multiple errors. This paper proposes a new technique for parsing inputs that contain simple kinds of ill-formedness. This generalized parsing strategy is, similar to Mellish&apos;s, based on an active chart parser</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Kay, Martin (1980). Algorithm Schemata and Data Structures in Syntactic Processing. Research Report CSL-80-12 Xerox PARC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Parsing Incomplete Sentences.</title>
<date>1988</date>
<booktitle>Proceedings of COLING 88,</booktitle>
<pages>365--371</pages>
<contexts>
<context position="2022" citStr="Lang, 1988" startWordPosition="294" endWordPosition="295">me taken to parse the correct sentences. 1 Introduction It is important that natural language interface systems have the capability of composing the globally most plausible explanation if a given input can not be syntactically parsed. This would be useful for handling erroneous inputs from the user and for offsetting grammar and lexicon insufficiency. Also, such a capability could be applied to deal with the ungrammatical sentences and sentence fragments that frequently appear in spoken dialogs (Bear, Dowding and Shriberg, 1992). Several efforts have been conducted to achieve this objective ((Lang, 1988; Saito and Tomita, 1988), for example.) One major decision to be made in designing this capability is whether knowledge other than purely syntactic knowledge is to be used. Otherthan syntactic knowledge includes grammar specific recovery rules such as meta-rules (Weishedel and Sondheimer, 1983), semantic or pragmatic knowledge which may depend on a particular domain (Carbonell and Hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (Hindle, 1983). Although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we </context>
</contexts>
<marker>Lang, 1988</marker>
<rawString>Lang, Bernard (1988). Parsing Incomplete Sentences. Proceedings of COLING 88, 365 - 371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
<author>Carl Weir</author>
</authors>
<title>Efficiency, Robustness and Accuracy in Picky Chart Parsing.</title>
<date>1992</date>
<booktitle>Proceedings of 30th ACL,</booktitle>
<volume>40</volume>
<pages>47</pages>
<contexts>
<context position="12198" citStr="Magerman and Weir, 1992" startWordPosition="1967" endWordPosition="1970">nstituents of every possible parse and unsatisfied active edges for all error points that are to the immediate right of sequences of constituents corresponding to the RHS. Since parsing proceeds left to right, an active edge is generated only when an error point exists to the right of the found constituents. In the first phase, bi-directional bottom-up parsing generates all generalized edges that represent unsatisfied expectations to the right and left of constituents. From some perspectives, the role this phase plays is similar to that of the covered bi-directional phase of the Picky parser (Magerman and Weir, 1992), though the method proposed herein does not employ stochastic information at all. This process can be described in three rules as shown in Figure 1. As can be seen, this is bi-directional bottom-up parsing that uses generalized edges as the data structure. For simplicity, the details for avoiding duplicated edge generation have been omitted. It is worth noting that after this process, the needs listed in each generalized edge indicate that the expected constituents did not exist, while, before this process, a need may exist just because an expectation has not been checked. The second phase fi</context>
</contexts>
<marker>Magerman, Weir, 1992</marker>
<rawString>Magerman, David M. and Weir, Carl (1992). Efficiency, Robustness and Accuracy in Picky Chart Parsing. Proceedings of 30th ACL, 40- 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris S Mellish</author>
</authors>
<title>Some Chart-Based Techniques for Parsing Ill-Formed Input.</title>
<date>1989</date>
<booktitle>Proceedings of 27th ACL,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="2925" citStr="Mellish (1989)" startWordPosition="430" endWordPosition="431">, 1983), semantic or pragmatic knowledge which may depend on a particular domain (Carbonell and Hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (Hindle, 1983). Although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge. Moreover, the result can be applied widely, as using syntactic knowledge is a base of the most of strategies. One significant advance in the usage of syntactic knowledge was contained in the technique proposed by Mellish (1989). It can handle not only unknown/misspelled words, but also omitted words and extraneous words in sentences. It can deal with such problems, and develop plausible explanations quickly since it utilizes the full syntactic context by using an active chart parser (Kay, 1980; Gazdar and Mellish, 1989). One problem with his technique is that its performance heavily depends on how the search heuristics, which is implemented as a score calculated from six parameters, is set. The heuristics complicates the algorithm significantly. This must be one of reasons why the performance of the method, as Melli</context>
</contexts>
<marker>Mellish, 1989</marker>
<rawString>Mellish, Chris S. (1989). Some Chart-Based Techniques for Parsing Ill-Formed Input. Proceedings of 27th ACL, 102 - 109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David Warren</author>
<author>H D</author>
</authors>
<title>Definite Clause Grammars for Language Analysis - A Survey of the Formalism and a Comparison with Augmented Transition Networks.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<issue>3</issue>
<pages>231--278</pages>
<marker>Pereira, Warren, D, 1980</marker>
<rawString>Pereira, Fernando C.N. and Warren, David, H.D. (1980). Definite Clause Grammars for Language Analysis - A Survey of the Formalism and a Comparison with Augmented Transition Networks. Artificial Intelligence, 13 (3), 231 - 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elaine Rich</author>
<author>Kevin Knight</author>
</authors>
<date>1991</date>
<journal>Artificial Intelligence</journal>
<volume>2</volume>
<editor>ed.).</editor>
<publisher>McGraw-Hill.</publisher>
<location>New York:</location>
<contexts>
<context position="14177" citStr="Rich and Knight, 1991" startWordPosition="2326" endWordPosition="2329"> have high costs. Thus, the data structure used in this phase is, &lt;needs Cs! from si to el, c52 from s2 toe, , csn from sn to en, g, h&gt;. Here, the number of errors corrected so far is taken as g, and the total number of categories in the needs is used as h. As mentioned above, since the needs listed indicate only the existence of errors as detected by the preceding process and to be refined, the value of h is always less than or equal to the number of the errors that must be corrected to get a solution. That is, the best first search using g+h as the cost functions is an admissible A* search (Rich and Knight, 1991). Needless to say, more sophisticated cost functions can also be used, in which, for example, the cost depends on the kind of error. The rules governing the second phase, which correspond to the search state transition operators in the context of search problems, are shown in Figure 2. The top-down rule and the refining rule locate errors and the other three rules are for correcting them. Most important is the refining rule, which tries to find out errors by using generalized edges in a top-down manner toward preterminals. This reduces the frequency of using the topdown rule and prevents an ex</context>
</contexts>
<marker>Rich, Knight, 1991</marker>
<rawString>Rich, Elaine and Knight, Kevin (1991). Artificial Intelligence (2nd ed.). New York: McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroaki Saito</author>
<author>Masaru Tomita</author>
</authors>
<title>Parsing Noisy Sentences.</title>
<date>1988</date>
<booktitle>Proceedings of COLING 88,</booktitle>
<pages>561--566</pages>
<contexts>
<context position="2047" citStr="Saito and Tomita, 1988" startWordPosition="296" endWordPosition="299">parse the correct sentences. 1 Introduction It is important that natural language interface systems have the capability of composing the globally most plausible explanation if a given input can not be syntactically parsed. This would be useful for handling erroneous inputs from the user and for offsetting grammar and lexicon insufficiency. Also, such a capability could be applied to deal with the ungrammatical sentences and sentence fragments that frequently appear in spoken dialogs (Bear, Dowding and Shriberg, 1992). Several efforts have been conducted to achieve this objective ((Lang, 1988; Saito and Tomita, 1988), for example.) One major decision to be made in designing this capability is whether knowledge other than purely syntactic knowledge is to be used. Otherthan syntactic knowledge includes grammar specific recovery rules such as meta-rules (Weishedel and Sondheimer, 1983), semantic or pragmatic knowledge which may depend on a particular domain (Carbonell and Hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (Hindle, 1983). Although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine th</context>
</contexts>
<marker>Saito, Tomita, 1988</marker>
<rawString>Saito, Hiroaki and Tomita, Masaru (1988). Parsing Noisy Sentences. Proceedings of COLING 88, 561 - 566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
<author>Oliviero Stock</author>
</authors>
<title>Formal Properties and Implementation of Bidirectional Charts.</title>
<date>1989</date>
<booktitle>Proceedings of IJCAI 89,</booktitle>
<pages>1480--1485</pages>
<contexts>
<context position="4446" citStr="Satta and Stock, 1989" startWordPosition="666" endWordPosition="669">antages of Mellish&apos;s technique. It is based on pure syntactics, it is independent of all grammars, and it does not slow down the original parsing operation if there is no ill-formedness. However, unlike Mellish&apos;s technique, it doesn&apos;t employ any complicated heuristic parameters. There are two key points. First, instead of using a unified or interleaved process for finding errors and correcting them, we separate the initial error detection stage from the other stages and adopt a version of bidirectional parsing, which has been pointed out to be a useful strategy for fragment parsing by itself (Satta and Stock, 1989). This effectively prunes the search space and allows the new technique to take full account of the right-side context. Second, it employs normal top-down parsing, in which each parsing state reflects the global context, instead of top-down chart parsing. This enables the technique to determine the global plausibility of candidates easily. The results of preliminary experiments are encouraging. The proposed strategy could enumerate 107 all possible minimal-penalty solutions in just 4 times the time taken to parse the correct sentences. That is, it is almost twice as fast as Mellish&apos;s strategy.</context>
</contexts>
<marker>Satta, Stock, 1989</marker>
<rawString>Satta, Giorgio and Stock, Oliviero (1989). Formal Properties and Implementation of Bidirectional Charts. Proceedings of IJCAI 89, 1480 - 1485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar. Stanford:</title>
<date>1986</date>
<journal>CSLI Lecture Notes</journal>
<volume>4</volume>
<contexts>
<context position="10741" citStr="Shieber, 1986" startWordPosition="1696" endWordPosition="1697">can be anchored by the start symbol, a data structure is created that can represent global plausibility. Third, in order to reduce the dependency on the top-down rule, a rule is developed that uses two active edges to locate errors. This process is closer to ordinary top-down parsing than chart parsing and global plausibility scoring is accurate and easily calculated. For simplicity of explanation, simple CF-PSG grammar formalism is assumed throughout this paper, although there are obvious generalizations to other formalism such as DCG (Pereira and Warren, 1980) or unification based grammars (Shieber, 1986). 108 Bottom-up rule: &lt;C from S to E needs nothing&gt; Cl --&gt; ...Cs&apos; C Cs2... where Csi is not empty (in the grammar) ------------------------------------- &lt;C1 from * to E2 needs Cs! from * to S. Cs2 from E to E2&gt; where if Cs2 is empty then E2 E else E2=*. Fundamental rule: &lt;C from S to E needs ... , [...Csl 1, Cl, Cs12...] from s to e , ...&gt; &lt;C1 from S1 to El needs nothing&gt; &lt;C from S to E needs ... , Csii from si to S1 ,Cs i2 from El to el ,...&gt; where si S1 or si=*, El el ore =*. Simplification rule: &lt;C from S to E needs from Si_i to s, [] from s to s, Csi+i from s to ei+1, ...&gt; &lt;C from S to E n</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart M. (1986). An Introduction to Unification-Based Approaches to Grammar. Stanford: CSLI Lecture Notes 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph M Weishedel</author>
<author>Norman K Sondheimer</author>
</authors>
<title>Meta-Rules as a Basis for Processing IIIFormed Input.</title>
<date>1983</date>
<journal>JACL,</journal>
<volume>9</volume>
<pages>3--4</pages>
<contexts>
<context position="2318" citStr="Weishedel and Sondheimer, 1983" startWordPosition="336" endWordPosition="339">ous inputs from the user and for offsetting grammar and lexicon insufficiency. Also, such a capability could be applied to deal with the ungrammatical sentences and sentence fragments that frequently appear in spoken dialogs (Bear, Dowding and Shriberg, 1992). Several efforts have been conducted to achieve this objective ((Lang, 1988; Saito and Tomita, 1988), for example.) One major decision to be made in designing this capability is whether knowledge other than purely syntactic knowledge is to be used. Otherthan syntactic knowledge includes grammar specific recovery rules such as meta-rules (Weishedel and Sondheimer, 1983), semantic or pragmatic knowledge which may depend on a particular domain (Carbonell and Hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (Hindle, 1983). Although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge. Moreover, the result can be applied widely, as using syntactic knowledge is a base of the most of strategies. One significant advance in the usage of syntactic knowledge was contained in the technique proposed by Mellish</context>
</contexts>
<marker>Weishedel, Sondheimer, 1983</marker>
<rawString>Weishedel, Ralph M. and Sondheimer, Norman K. (1983). Meta-Rules as a Basis for Processing IIIFormed Input. JACL, 9 (3-4), 161 - 177.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>