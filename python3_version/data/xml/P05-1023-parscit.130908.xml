<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007548">
<title confidence="0.9994485">
Data-Defined Kernels for Parse Reranking
Derived from Probabilistic Models
</title>
<author confidence="0.996637">
James Henderson
</author>
<affiliation confidence="0.9984235">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.80171">
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
</address>
<email confidence="0.998663">
james.henderson@ed.ac.uk
</email>
<author confidence="0.995816">
Ivan Titov
</author>
<affiliation confidence="0.998935">
Department of Computer Science
University of Geneva
</affiliation>
<address confidence="0.9646115">
24, rue G´en´eral Dufour
CH-1211 Gen`eve 4, Switzerland
</address>
<email confidence="0.998378">
ivan.titov@cui.unige.ch
</email>
<sectionHeader confidence="0.995625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999864238095238">
Previous research applying kernel meth-
ods to natural language parsing have fo-
cussed on proposing kernels over parse
trees, which are hand-crafted based on do-
main knowledge and computational con-
siderations. In this paper we propose a
method for defining kernels in terms of
a probabilistic model of parsing. This
model is then trained, so that the param-
eters of the probabilistic model reflect the
generalizations in the training data. The
method we propose then uses these trained
parameters to define a kernel for rerank-
ing parse trees. In experiments, we use
a neural network based statistical parser
as the probabilistic model, and use the
resulting kernel with the Voted Percep-
tron algorithm to rerank the top 20 parses
from the probabilistic model. This method
achieves a significant improvement over
the accuracy of the probabilistic model.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972076923077">
Kernel methods have been shown to be very ef-
fective in many machine learning problems. They
have the advantage that learning can try to optimize
measures related directly to expected testing perfor-
mance (i.e. “large margin” methods), rather than
the probabilistic measures used in statistical models,
which are only indirectly related to expected test-
ing performance. Work on kernel methods in natural
language has focussed on the definition of appropri-
ate kernels for natural language tasks. In particu-
lar, most of the work on parsing with kernel meth-
ods has focussed on kernels over parse trees (Collins
and Duffy, 2002; Shen and Joshi, 2003; Shen et
al., 2003; Collins and Roark, 2004). These kernels
have all been hand-crafted to try reflect properties
of parse trees which are relevant to discriminating
correct parse trees from incorrect ones, while at the
same time maintaining the tractability of learning.
Some work in machine learning has taken an al-
ternative approach to defining kernels, where the
kernel is derived from a probabilistic model of the
task (Jaakkola and Haussler, 1998; Tsuda et al.,
2002). This way of defining kernels has two ad-
vantages. First, linguistic knowledge about parsing
is reflected in the design of the probabilistic model,
not directly in the kernel. Designing probabilistic
models to reflect linguistic knowledge is a process
which is currently well understood, both in terms of
reflecting generalizations and controlling computa-
tional cost. Because many NLP problems are un-
bounded in size and complexity, it is hard to specify
all possible relevant kernel features without having
so many features that the computations become in-
tractable and/or the data becomes too sparse.1 Sec-
ond, the kernel is defined using the trained param-
eters of the probabilistic model. Thus the kernel is
in part determined by the training data, and is auto-
matically tailored to reflect properties of parse trees
which are relevant to parsing.
</bodyText>
<footnote confidence="0.998418333333333">
1For example, see (Henderson, 2004) for a discussion of
why generative models are better than models parameterized to
estimate the a posteriori probability directly.
</footnote>
<page confidence="0.921187">
181
</page>
<note confidence="0.9915795">
Proceedings of the 43rd Annual Meeting of the ACL, pages 181–188,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999833833333333">
In this paper, we propose a new method for de-
riving a kernel from a probabilistic model which is
specifically tailored to reranking tasks, and we ap-
ply this method to natural language parsing. For the
probabilistic model, we use a state-of-the-art neural
network based statistical parser (Henderson, 2003).
The resulting kernel is then used with the Voted Per-
ceptron algorithm (Freund and Schapire, 1998) to
reranking the top 20 parses from the probabilistic
model. This method achieves a significant improve-
ment over the accuracy of the probabilistic model
alone.
</bodyText>
<sectionHeader confidence="0.970882" genericHeader="method">
2 Kernels Derived from Probabilistic
Models
</sectionHeader>
<bodyText confidence="0.999980483870968">
In recent years, several methods have been proposed
for constructing kernels from trained probabilistic
models. As usual, these kernels are then used with
linear classifiers to learn the desired task. As well as
some empirical successes, these methods are moti-
vated by theoretical results which suggest we should
expect some improvement with these classifiers over
the classifier which chooses the most probable an-
swer according to the probabilistic model (i.e. the
maximum a posteriori (MAP) classifier). There is
guaranteed to be a linear classifier for the derived
kernel which performs at least as well as the MAP
classifier for the probabilistic model. So, assuming
a large-margin classifier can optimize a more ap-
propriate criteria than the posterior probability, we
should expect the derived kernel’s classifier to per-
form better than the probabilistic model’s classifier,
although empirical results on a given task are never
guaranteed.
In this section, we first present two previous ker-
nels and then propose a new kernel specifically for
reranking tasks. In each of these discussions we
need to characterize the parsing problem as a classi-
fication task. Parsing can be regarded as a mapping
from an input space of sentences xEX to a struc-
tured output space of parse trees yEY. On the basis
of training sentences, we learn a discriminant func-
tion F : X x Y —* R. The parse tree y with the
largest value for this discriminant function F(x, y)
is the output parse tree for the sentence x. We focus
on the linear discriminant functions:
</bodyText>
<equation confidence="0.866444">
Fw(x, y) = &lt;w, φ(x, y)&gt;,
</equation>
<bodyText confidence="0.999963333333333">
where φ(x, y) is a feature vector for the sentence-
tree pair, w is a parameter vector for the discrim-
inant function, and &lt;a, b&gt; is the inner product of
vectors a and b. In the remainder of this section, we
will characterize the kernel methods we consider in
terms of the feature extractor φ(x, y).
</bodyText>
<subsectionHeader confidence="0.985958">
2.1 Fisher Kernels
</subsectionHeader>
<bodyText confidence="0.999978142857143">
The Fisher kernel (Jaakkola and Haussler, 1998) is
one of the best known kernels belonging to the class
of probability model based kernels. Given a genera-
tive model of P(z|�θ) with smooth parameterization,
the Fisher score of an example z is a vector of partial
derivatives of the log-likelihood of the example with
respect to the model parameters:
</bodyText>
<equation confidence="0.997105666666667">
φˆθ(z) = (∂log P(z|ˆθ)
∂θ1 , . . . , ∂log P(z|ˆθ)
∂θl ).
</equation>
<bodyText confidence="0.998422111111111">
This score can be regarded as specifying how the
model should be changed in order to maximize the
likelihood of the example z. Then we can define the
similarity between data points as the inner product
of the corresponding Fisher scores. This kernel is
often referred to as the practical Fisher kernel. The
theoretical Fisher kernel depends on the Fisher in-
formation matrix, which is not feasible to compute
for most practical tasks and is usually omitted.
The Fisher kernel is only directly applicable to
binary classification tasks. We can apply it to our
task by considering an example z to be a sentence-
tree pair (x, y), and classifying the pairs into cor-
rect parses versus incorrect parses. When we use the
Fisher score φˆθ(x, y) in the discriminant function F,
we can interpret the value as the confidence that the
tree y is correct, and choose the y in which we are
the most confident.
</bodyText>
<subsectionHeader confidence="0.99801">
2.2 TOP Kernels
</subsectionHeader>
<bodyText confidence="0.9992964">
Tsuda (2002) proposed another kernel constructed
from a probabilistic model, called the Tangent vec-
tors Of Posterior log-odds (TOP) kernel. Their TOP
kernel is also only for binary classification tasks, so,
as above, we treat the input z as a sentence-tree pair
and the output category c E {−1, +1} as incor-
rect/correct. It is assumed that the true probability
distribution is included in the class of probabilis-
tic models and that the true parameter vector θ? is
unique. The feature extractor of the TOP kernel for
</bodyText>
<page confidence="0.949147">
182
</page>
<equation confidence="0.873297">
the input z is defined by:
φˆθ(z) = (v(z, ˆθ), ∂v aeiθ)...,∂v a0l ),
where v(z, ˆθ) = log P(c=+1|z, ˆθ) −
log P(c=−1|z, ˆθ).
</equation>
<bodyText confidence="0.999957285714286">
In addition to being at least as good as the
MAP classifier, the choice of the TOP kernel fea-
ture extractor is motivated by the minimization of
the binary classification error of a linear classifier
&lt;w, φˆθ(z)&gt; + b. Tsuda (2002) demonstrates that
this error is closely related to the estimation error of
the posterior probability P(c=+1|z, θ?) by the esti-
mator g(&lt;w, φˆθ(z)&gt; + b), where g is the sigmoid
function g(t) = 1/(1 + exp (−t)).
The TOP kernel isn’t quite appropriate for struc-
tured classification tasks because φˆθ(z) is motivated
by binary classificaton error minimization. In the
next subsection, we will adapt it to structured classi-
fication.
</bodyText>
<subsectionHeader confidence="0.995934">
2.3 A TOP Kernel for Reranking
</subsectionHeader>
<bodyText confidence="0.999993666666667">
We define the reranking task as selecting a parse tree
from the list of candidate trees suggested by a proba-
bilistic model. Furthermore, we only consider learn-
ing to rerank the output of a particular probabilistic
model, without requiring the classifier to have good
performance when applied to a candidate list pro-
vided by a different model. In this case, it is natural
to model the probability that a parse tree is the best
candidate given the list of candidate trees:
</bodyText>
<equation confidence="0.950567">
P(yk|x, y1, ... , ys) = P(x,yk)
Et P(x,yt),
</equation>
<bodyText confidence="0.979503583333333">
where y1, ... , ys is the list of candidate parse trees.
To construct a new TOP kernel for reranking, we
apply an approach similar to that used for the TOP
kernel (Tsuda et al., 2002), but we consider the prob-
ability P(yk|x, y1, ... , ys, θ?) instead of the proba-
bility P(c=+1|z, θ?) considered by Tsuda. The re-
sulting feature extractor is given by:
φˆθ(x, yk) = (v(x, yk, ˆθ) ∂v(aelk,ˆθ) ∂v(aekA),
,
where v(x, yk, ˆθ) = log P (yk|y1, . . . , ys, ˆθ) −
log Et=,�k P(yt|y1, ... , ys, ˆθ). We will call this ker-
nel the TOP reranking kernel.
</bodyText>
<sectionHeader confidence="0.998541" genericHeader="method">
3 The Probabilistic Model
</sectionHeader>
<bodyText confidence="0.999973411764706">
To complete the definition of the kernel, we need
to choose a probabilistic model of parsing. For
this we use a statistical parser which has previously
been shown to achieve state-of-the-art performance,
namely that proposed in (Henderson, 2003). This
parser has two levels of parameterization. The first
level of parameterization is in terms of a history-
based generative probability model, but this level is
not appropriate for our purposes because it defines
an infinite number of parameters (one for every pos-
sible partial parse history). When parsing a given
sentence, the bounded set of parameters which are
relevant to a given parse are estimated using a neural
network. The weights of this neural network form
the second level of parameterization. There is a fi-
nite number of these parameters. Neural network
training is applied to determine the values of these
parameters, which in turn determine the values of
the probability model’s parameters, which in turn
determine the probabilistic model of parse trees.
We do not use the complete set of neural network
weights to define our kernels, but instead we define a
third level of parameterization which only includes
the network’s output layer weights. These weights
define a normalized exponential model, with the net-
work’s hidden layer as the input features. When we
tried using the complete set of weights in some small
scale experiments, training the classifier was more
computationally expensive, and actually performed
slightly worse than just using the output weights.
Using just the output weights also allows us to make
some approximations in the TOP reranking kernel
which makes the classifier learning algorithm more
efficient.
</bodyText>
<subsectionHeader confidence="0.999189">
3.1 A History-Based Probability Model
</subsectionHeader>
<bodyText confidence="0.999621357142857">
As with many other statistical parsers (Ratnaparkhi,
1999; Collins, 1999; Charniak, 2000), Henderson
(2003) uses a history-based model of parsing. He
defines the mapping from phrase structure trees to
parse sequences using a form of left-corner parsing
strategy (see (Henderson, 2003) for more details).
The parser actions include: introducing a new con-
stituent with a specified label, attaching one con-
stituent to another, and predicting the next word of
the sentence. A complete parse consists of a se-
quence of these actions, d1,..., dm, such that per-
forming d1,..., dm results in a complete phrase struc-
ture tree.
Because this mapping to parse sequences is
</bodyText>
<page confidence="0.995558">
183
</page>
<bodyText confidence="0.99992675">
one-to-one, and the word prediction actions in
a complete parse d1,..., dm specify the sentence,
P(d1,..., dm) is equivalent to the joint probability of
the output phrase structure tree and the input sen-
tence. This probability can be then be decomposed
into the multiplication of the probabilities of each
action decision di conditioned on that decision’s
prior parse history d1,..., di−1.
</bodyText>
<equation confidence="0.566415">
P(d1,..., dm) = HiP(di|d1,..., di−1)
</equation>
<subsectionHeader confidence="0.995537">
3.2 Estimating Decision Probabilities with a
Neural Network
</subsectionHeader>
<bodyText confidence="0.999987125">
The parameters of the above probability model are
the P(di|d1,..., di−1). There are an infinite num-
ber of these parameters, since the parse history
d1,..., di−1 grows with the length of the sentence. In
other work on history-based parsing, independence
assumptions are applied so that only a finite amount
of information from the parse history can be treated
as relevant to each parameter, thereby reducing the
number of parameters to a finite set which can be
estimated directly. Instead, Henderson (2003) uses
a neural network to induce a finite representation
of this unbounded history, which we will denote
h(d1,..., di−1). Neural network training tries to find
such a history representation which preserves all the
information about the history which is relevant to es-
timating the desired probability.
</bodyText>
<equation confidence="0.561">
P(di|d1,..., di−1) ≈ P(di|h(d1,..., di−1))
</equation>
<bodyText confidence="0.999857421052631">
Using a neural network architecture called Simple
Synchrony Networks (SSNs), the history representa-
tion h(d1,..., di−1) is incrementally computed from
features of the previous decision di−1 plus a finite
set of previous history representations h(d1,..., dj),
j &lt; i − 1. Each history representation is a finite
vector of real numbers, called the network’s hidden
layer. As long as the history representation for po-
sition i − 1 is always included in the inputs to the
history representation for position i, any information
about the entire sequence could be passed from his-
tory representation to history representation and be
used to estimate the desired probability. However,
learning is biased towards paying more attention to
information which passes through fewer history rep-
resentations.
To exploit this learning bias, structural locality is
used to determine which history representations are
input to which others. First, each history representa-
tion is assigned to the constituent which is on the top
of the parser’s stack when it is computed. Then ear-
lier history representations whose constituents are
structurally local to the current representation’s con-
stituent are input to the computation of the correct
representation. In this way, the number of represen-
tations which information needs to pass through in
order to flow from history representation i to his-
tory representation j is determined by the structural
distance between i’s constituent and j’s constituent,
and not just the distance between i and j in the
parse sequence. This provides the neural network
with a linguistically appropriate inductive bias when
it learns the history representations, as explained in
more detail in (Henderson, 2003).
Once it has computed h(d1,..., di−1), the SSN
uses a normalized exponential to estimate a proba-
bility distribution over the set of possible next deci-
sions di given the history:
</bodyText>
<equation confidence="0.999057666666667">
P(di|d1,..., di−1, 0) ≈
exp(&lt;Bdi,h(d1,...,di−1)&gt;)
EtEN(di−1) exp(&lt;0t,h(d1,...,di−1)&gt;),
</equation>
<bodyText confidence="0.999989866666667">
where by 0t we denote the set of output layer
weights, corresponding to the parser action t,
N(di−1) defines a set of possible next parser actions
after the step di−1 and 0 denotes the full set of model
parameters.
We trained SSN parsing models, using the on-line
version of Backpropagation to perform the gradient
descent with a maximum likelihood objective func-
tion. This learning simultaneously tries to optimize
the parameters of the output computation and the pa-
rameters of the mappings h(d1,..., di−1). With multi-
layered networks such as SSNs, this training is not
guaranteed to converge to a global optimum, but in
practice a network whose criteria value is close to
the optimum can be found.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="method">
4 Large-Margin Optimization
</sectionHeader>
<bodyText confidence="0.999820142857143">
Once we have defined a kernel over parse trees, gen-
eral techniques for linear classifier optimization can
be used to learn the given task. The most sophis-
ticated of these techniques (such as Support Vec-
tor Machines) are unfortunately too computationally
expensive to be used on large datasets like the Penn
Treebank (Marcus et al., 1993). Instead we use a
</bodyText>
<page confidence="0.994815">
184
</page>
<bodyText confidence="0.999961361702128">
method which has often been shown to be virtu-
ally as good, the Voted Perceptron (VP) (Freund and
Schapire, 1998) algorithm. The VP algorithm was
originally applied to parse reranking in (Collins and
Duffy, 2002) with the Tree kernel. We modify the
perceptron training algorithm to make it more suit-
able for parsing, where zero-one classification loss
is not the evaluation measure usually employed. We
also develop a variant of the kernel defined in sec-
tion 2.3, which is more efficient when used with the
VP algorithm.
Given a list of candidate trees, we train the clas-
sifier to select the tree with largest constituent F1
score. The F1 score is a measure of the similarity
between the tree in question and the gold standard
parse, and is the standard way to evaluate the accu-
racy of a parser. We denote the k’th candidate tree
for the j’th sentence xj by yjk. Without loss of gener-
ality, let us assume that yj1 is the candidate tree with
the largest F1 score.
The Voted Perceptron algorithm is an ensem-
ble method for combining the various intermediate
models which are produced during training a per-
ceptron. It demonstrates more stable generalization
performance than the normal perceptron algorithm
when the problem is not linearly separable (Freund
and Schapire, 1998), as is usually the case.
We modify the perceptron algorithm by introduc-
ing a new classification loss function. This modifi-
cation enables us to treat differently the cases where
the perceptron predicts a tree with an F1 score much
smaller than that of the top candidate and the cases
where the predicted and the top candidates have sim-
ilar score values. The natural choice for the loss
function would be Δ(yjk, yj1) = F1(yj1) − F1(yjk),
where F1(yjk) denotes the F1 score value for the
parse tree yjk. This approach is very similar to slack
variable rescaling for Support Vector Machines pro-
posed in (Tsochantaridis et al., 2004). The learning
algorithm we employed is presented in figure 1.
When applying kernels with a large training cor-
pus, we face efficiency issues because of the large
number of the neural network weights. Even though
we use only the output layer weights, this vector
grows with the size of the vocabulary, and thus can
be large. The kernels presented in section 2 all lead
to feature vectors without many zero values. This
</bodyText>
<equation confidence="0.8265574">
w = 0
for j = 1 .. n
for k = 2 .. s
if &lt;w,φ(xj, yjk)&gt; &gt; &lt;w,φ(xj, yj1)&gt;
w = w + Δ(yjk, yj1)(φ(xj, yj1) − φ(xj, yjk))
</equation>
<figureCaption confidence="0.999841">
Figure 1: The modified perceptron algorithm
</figureCaption>
<bodyText confidence="0.998767636363636">
happens because we compute the derivative of the
normalization factor used in the network’s estima-
tion of P (di|d1,..., di−1). This normalization factor
depends on the output layer weights corresponding
to all the possible next decisions (see section 3.2).
This makes an application of the VP algorithm in-
feasible in the case of a large vocabulary.
We can address this problem by freezing the
normalization factor when computing the feature
vector. Note that we can rewrite the model log-
probability of the tree as:
</bodyText>
<equation confidence="0.999313833333333">
Elog P(y|θ) =
j
to r exp(&lt;θdi,h(d1,...,di−1)&gt;)
Lam g (EIEN(di−1) exp(&lt;θ,,h(d1,...,di−1)&gt;)) =
&amp;(&lt;θdi, h(d1,..., di−1)&gt;)−
&amp; log EtEN(di−1) exp(&lt;θt, h(d1,..., di−1)&gt;).
</equation>
<bodyText confidence="0.999983461538462">
We treat the parameters used to compute the first
term as different from the parameters used to com-
pute the second term, and we define our kernel only
using the parameters in the first term. This means
that the second term does not effect the derivatives
in the formula for the feature vector φ(x, y). Thus
the feature vector for the kernel will contain non-
zero entries only in the components corresponding
to the parser actions which are present in the candi-
date derivation for the sentence, and thus in the first
vector component. We have applied this technique
to the TOP reranking kernel, the result of which we
will call the efficient TOP reranking kernel.
</bodyText>
<sectionHeader confidence="0.9923" genericHeader="method">
5 The Experimental Results
</sectionHeader>
<bodyText confidence="0.999924333333333">
We used the Penn Treebank WSJ corpus (Marcus et
al., 1993) to perform empirical experiments on the
proposed parsing models. In each case the input to
the network is a sequence of tag-word pairs.2 We re-
port results for two different vocabulary sizes, vary-
ing in the frequency with which tag-word pairs must
</bodyText>
<footnote confidence="0.9957205">
2We used a publicly available tagger (Ratnaparkhi, 1996) to
provide the tags.
</footnote>
<page confidence="0.998718">
185
</page>
<bodyText confidence="0.999984720930232">
occur in the training set in order to be included ex-
plicitly in the vocabulary. A frequency threshold of
200 resulted in a vocabulary of 508 tag-word pairs
(including tag-unknown word pairs) and a threshold
of 20 resulted in 4215 tag-word pairs. We denote
the probabilistic model trained with the vocabulary
of 508 by the SSN-Freq&gt;200, the model trained with
the vocabulary of 4215 by the SSN-Freq&gt;20.
Testing the probabilistic parser requires using a
beam search through the space of possible parses.
We used a form of beam search which prunes the
search after the prediction of each word. We set the
width of this post-word beam to 40 for both testing
of the probabilistic model and generating the candi-
date list for reranking. For training and testing of
the kernel models, we provided a candidate list con-
sisting of the top 20 parses found by the generative
probabilistic model. When using the Fisher kernel,
we added the log-probability of the tree given by the
probabilistic model as the feature. This was not nec-
essary for the TOP kernels because they already con-
tain a feature corresponding to the probability esti-
mated by the probabilistic model (see section 2.3).
We trained the VP model with all three kernels
using the 508 word vocabulary (Fisher-Freq&gt;200,
TOP-Freq&gt;200, TOP-Eff-Freq&gt;200) but only the ef-
ficient TOP reranking kernel model was trained with
the vocabulary of 4215 words (TOP-Eff-Freq&gt;20).
The non-sparsity of the feature vectors for other ker-
nels led to the excessive memory requirements and
larger testing time. In each case, the VP model was
run for only one epoch. We would expect some im-
provement if running it for more epochs, as has been
empirically demonstrated in other domains (Freund
and Schapire, 1998).
To avoid repeated testing on the standard testing
set, we first compare the different models with their
performance on the validation set. Note that the val-
idation set wasn’t used during learning of the kernel
models or for adjustment of any parameters.
Standard measures of accuracy are shown in ta-
ble 1.3 Both the Fisher kernel and the TOP kernels
show better accuracy than the baseline probabilistic
</bodyText>
<tableCaption confidence="0.497796333333333">
3All our results are computed with the evalb program fol-
lowing the standard criteria in (Collins, 1999), and using the
standard training (sections 2–22, 39,832 sentences, 910,196
words), validation (section 24, 1346 sentence, 31507 words),
and testing (section 23, 2416 sentences, 54268 words) sets
(Collins, 1999).
</tableCaption>
<table confidence="0.999799571428571">
LR LP F,3=1
SSN-Freq&gt;200 87.2 88.5 87.8
Fisher-Freq&gt;200 87.2 88.8 87.9
TOP-Freq&gt;200 87.3 88.9 88.1
TOP-Eff-Freq&gt;200 87.3 88.9 88.1
SSN-Freq&gt;20 88.1 89.2 88.6
TOP-Eff-Freq&gt;20 88.2 89.7 88.9
</table>
<tableCaption confidence="0.999632">
Table 1: Percentage labeled constituent recall (LR),
</tableCaption>
<bodyText confidence="0.995405038461538">
precision (LP), and a combination of both (F,a=1) on
validation set sentences of length at most 100.
model, but only the improvement of the TOP kernels
is statistically significant.4 For the TOP kernel, the
improvement over baseline is about the same with
both vocabulary sizes. Also note that the perfor-
mance of the efficient TOP reranking kernel is the
same as that of the original TOP reranking kernel,
for the smaller vocabulary.
For comparison to previous results, table 2 lists
the results on the testing set for our best model
(TOP-Efficient-Freq&gt;20) and several other statisti-
cal parsers (Collins, 1999; Collins and Duffy, 2002;
Collins and Roark, 2004; Henderson, 2003; Char-
niak, 2000; Collins, 2000; Shen and Joshi, 2004;
Shen et al., 2003; Henderson, 2004; Bod, 2003).
First note that the parser based on the TOP efficient
kernel has better accuracy than (Henderson, 2003),
which used the same parsing method as our base-
line model, although the trained network parameters
were not the same. When compared to other kernel
methods, our approach performs better than those
based on the Tree kernel (Collins and Duffy, 2002;
Collins and Roark, 2004), and is only 0.2% worse
than the best results achieved by a kernel method for
parsing (Shen et al., 2003; Shen and Joshi, 2004).
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999893857142857">
The first application of kernel methods to parsing
was proposed by Collins and Duffy (2002). They
used the Tree kernel, where the features of a tree are
all its connected tree fragments. The VP algorithm
was applied to rerank the output of a probabilistic
model and demonstrated an improvement over the
baseline.
</bodyText>
<footnote confidence="0.866188">
4We measured significance with the randomized signifi-
cance test of (Yeh, 2000).
</footnote>
<page confidence="0.988688">
186
</page>
<table confidence="0.993202846153846">
LR LP F,a=1*
Collins99 88.1 88.3 88.2
Collins&amp;Duffy02 88.6 88.9 88.7
Collins&amp;Roark04 88.4 89.1 88.8
Henderson03 88.8 89.5 89.1
Charniak00 89.6 89.5 89.5
TOP-Eff-Freq&gt;20 89.1 90.1 89.6
Collins00 89.6 89.9 89.7
Shen&amp;Joshi04 89.5 90.0 89.8
Shen et al.03 89.7 90.0 89.8
Henderson04 89.8 90.4 90.1
Bod03 90.7 90.8 90.7
* Fp=1 for previous models may have rounding errors.
</table>
<tableCaption confidence="0.996579">
Table 2: Percentage labeled constituent recall (LR),
</tableCaption>
<bodyText confidence="0.980881425925926">
precision (LP), and a combination of both (Fo=1) on
the entire testing set.
Shen and Joshi (2003) applied an SVM based
voting algorithm with the Preference kernel defined
over pairs for reranking. To define the Preference
kernel they used the Tree kernel and the Linear ker-
nel as its underlying kernels and achieved state-of-
the-art results with the Linear kernel.
In (Shen et al., 2003) it was pointed out that
most of the arbitrary tree fragments allowed by the
Tree kernel are linguistically meaningless. The au-
thors suggested the use of Lexical Tree Adjoining
Grammar (LTAG) based features as a more linguis-
tically appropriate set of features. They empiri-
cally demonstrated that incorporation of these fea-
tures helps to improve reranking performance.
Shen and Joshi (2004) proposed to improve mar-
gin based methods for reranking by defining the
margin not only between the top tree and all the
other trees in the candidate list but between all the
pairs of parses in the ordered candidate list for the
given sentence. They achieved the best results when
training with an uneven margin scaled by the heuris-
tic function of the candidates positions in the list.
One potential drawback of this method is that it
doesn’t take into account the actual F1 score of the
candidate and considers only the position in the list
ordered by the F1 score. We expect that an im-
provement could be achieved by combining our ap-
proach of scaling updates by the F1 loss with the
all pairs approach of (Shen and Joshi, 2004). Use
of the F1 loss function during training demonstrated
better performance comparing to the 0-1 loss func-
tion when applied to a structured classification task
(Tsochantaridis et al., 2004).
All the described kernel methods are limited to
the reranking of candidates from an existing parser
due to the complexity of finding the best parse given
a kernel (i.e. the decoding problem). (Taskar et
al., 2004) suggested a method for maximal mar-
gin parsing which employs the dynamic program-
ming approach to decoding and parameter estima-
tion problems. The efficiency of dynamic program-
ming means that the entire space of parses can be
considered, not just a candidate list. However, not
all kernels are suitable for this method. The dy-
namic programming approach requires the feature
vector of a tree to be decomposable into a sum over
parts of the tree. In particular, this is impossible with
the TOP and Fisher kernels derived from the SSN
model. Also, it isn’t clear whether the algorithm
remains tractable for a large training set with long
sentences, since the authors only present results for
sentences of length less than or equal to 15.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99997947826087">
This paper proposes a method for deriving a ker-
nel for reranking from a probabilistic model, and
demonstrates state-of-the-art accuracy when this
method is applied to parse reranking. Contrary to
most of the previous research on kernel methods in
parsing, linguistic knowledge does not have to be ex-
pressed through a list of features, but instead can be
expressed through the design of a probability model.
The parameters of this probability model are then
trained, so that they reflect what features of trees are
relevant to parsing. The kernel is then derived from
this trained model in such a way as to maximize its
usefulness for reranking.
We performed experiments on parse reranking us-
ing a neural network based statistical parser as both
the probabilistic model and the source of the list
of candidate parses. We used a modification of
the Voted Perceptron algorithm to perform reranking
with the kernel. The results were amongst the best
current statistical parsers, and only 0.2% worse than
the best current parsing methods which use kernels.
We would expect further improvement if we used
different models to derive the kernel and to gener-
</bodyText>
<page confidence="0.993588">
187
</page>
<bodyText confidence="0.9999228">
ate the candidates, thereby exploiting the advantages
of combining multiple models, as do the better per-
forming methods using kernels.
In recent years, probabilistic models have become
commonplace in natural language processing. We
believe that this approach to defining kernels would
simplify the problem of defining kernels for these
tasks, and could be very useful for many of them.
In particular, maximum entropy models also use a
normalized exponential function to estimate proba-
bilities, so all the methods discussed in this paper
would be applicable to maximum entropy models.
This approach would be particularly useful for tasks
where there is less data available than in parsing, for
which large-margin methods work particularly well.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999924790123457">
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proc. 10th Conf. of European Chap-
ter of the Association for Computational Linguistics,
Budapest, Hungary.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. 1st Meeting of North American
Chapter ofAssociation for Computational Linguistics,
pages 132–139, Seattle, Washington.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures and the voted perceptron. In Proc.
40th Meeting of Association for Computational Lin-
guistics, pages 263–270.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proc. 42th
Meeting ofAssociation for Computational Linguistics,
Barcelona, Spain.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. 17th Int. Conf. on Ma-
chine Learning, pages 175–182, Stanford, CA.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In Proc. of the 11th Annual Conf. on Computational
Learning Theory, pages 209–217, Madisson WI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the Asso-
ciation for Computational Linguistics and the Human
Language Technology Conf., pages 103–110, Edmon-
ton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting ofAssociation for Computational Linguistics,
Barcelona, Spain.
Tommi S. Jaakkola and David Haussler. 1998. Ex-
ploiting generative models in discriminative classi-
fiers. Advances in Neural Information Processes Sys-
tems 11.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proc. Conf. on Empir-
ical Methods in Natural Language Processing, pages
133–142, Univ. of Pennsylvania, PA.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151–175.
Libin Shen and Aravind K. Joshi. 2003. An SVM based
voting algorithm with application to parse reranking.
In Proc. of the 7th Conf. on Computational Natural
Language Learning, pages 9–16, Edmonton, Canada.
Libin Shen and Aravind K. Joshi. 2004. Flexible margin
selection for reranking with full pairwise samples. In
Proc. of the 1st Int. Joint Conf. on Natural Language
Processing, Hainan Island, China.
Libin Shen, Anoop Sarkar, and Aravind K. Joshi. 2003.
Using LTAG based features in parse reranking. In
Proc. of Conf. on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. Conf. on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proc. 21st Int. Conf. on Machine
Learning, pages 823–830, Banff, Alberta, Canada.
K. Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg,
and K. Muller. 2002. A new discriminative ker-
nel from probabilistic models. Neural Computation,
14(10):2397–2414.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of the result differences. In Proc.
17th International Conf. on Computational Linguis-
tics, pages 947–953, Saarbruken, Germany.
</reference>
<page confidence="0.997524">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.772994">
<title confidence="0.9928225">Data-Defined Kernels for Parse Reranking Derived from Probabilistic Models</title>
<author confidence="0.999985">James Henderson</author>
<affiliation confidence="0.939008666666667">School of Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.997458">Edinburgh EH8 9LW, United Kingdom</address>
<email confidence="0.997941">james.henderson@ed.ac.uk</email>
<author confidence="0.999428">Ivan Titov</author>
<affiliation confidence="0.99997">Department of Computer Science University of Geneva</affiliation>
<address confidence="0.994596">24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland</address>
<email confidence="0.980656">ivan.titov@cui.unige.ch</email>
<abstract confidence="0.999612681818182">Previous research applying kernel methods to natural language parsing have focussed on proposing kernels over parse trees, which are hand-crafted based on domain knowledge and computational considerations. In this paper we propose a method for defining kernels in terms of a probabilistic model of parsing. This model is then trained, so that the parameters of the probabilistic model reflect the generalizations in the training data. The method we propose then uses these trained parameters to define a kernel for reranking parse trees. In experiments, we use a neural network based statistical parser as the probabilistic model, and use the resulting kernel with the Voted Perceptron algorithm to rerank the top 20 parses from the probabilistic model. This method achieves a significant improvement over the accuracy of the probabilistic model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An efficient implementation of a new DOP model.</title>
<date>2003</date>
<booktitle>In Proc. 10th Conf. of European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="24240" citStr="Bod, 2003" startWordPosition="3973" endWordPosition="3974"> significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi, 2004). 6 Related Work The first application of kernel methods to parsing was proposed by Collins an</context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>Rens Bod. 2003. An efficient implementation of a new DOP model. In Proc. 10th Conf. of European Chapter of the Association for Computational Linguistics, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. 1st Meeting of North American Chapter ofAssociation for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="11542" citStr="Charniak, 2000" startWordPosition="1902" endWordPosition="1903">. These weights define a normalized exponential model, with the network’s hidden layer as the input features. When we tried using the complete set of weights in some small scale experiments, training the classifier was more computationally expensive, and actually performed slightly worse than just using the output weights. Using just the output weights also allows us to make some approximations in the TOP reranking kernel which makes the classifier learning algorithm more efficient. 3.1 A History-Based Probability Model As with many other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), Henderson (2003) uses a history-based model of parsing. He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see (Henderson, 2003) for more details). The parser actions include: introducing a new constituent with a specified label, attaching one constituent to another, and predicting the next word of the sentence. A complete parse consists of a sequence of these actions, d1,..., dm, such that performing d1,..., dm results in a complete phrase structure tree. Because this mapping to parse sequences is 183 one-to-one, and the word </context>
<context position="24155" citStr="Charniak, 2000" startWordPosition="3958" endWordPosition="3960">f length at most 100. model, but only the improvement of the TOP kernels is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi, 2004). 6 Relat</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. 1st Meeting of North American Chapter ofAssociation for Computational Linguistics, pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. 40th Meeting of Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1849" citStr="Collins and Duffy, 2002" startWordPosition="279" endWordPosition="282">n Kernel methods have been shown to be very effective in many machine learning problems. They have the advantage that learning can try to optimize measures related directly to expected testing performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected</context>
<context position="16734" citStr="Collins and Duffy, 2002" startWordPosition="2719" endWordPosition="2722">he optimum can be found. 4 Large-Margin Optimization Once we have defined a kernel over parse trees, general techniques for linear classifier optimization can be used to learn the given task. The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank (Marcus et al., 1993). Instead we use a 184 method which has often been shown to be virtually as good, the Voted Perceptron (VP) (Freund and Schapire, 1998) algorithm. The VP algorithm was originally applied to parse reranking in (Collins and Duffy, 2002) with the Tree kernel. We modify the perceptron training algorithm to make it more suitable for parsing, where zero-one classification loss is not the evaluation measure usually employed. We also develop a variant of the kernel defined in section 2.3, which is more efficient when used with the VP algorithm. Given a list of candidate trees, we train the classifier to select the tree with largest constituent F1 score. The F1 score is a measure of the similarity between the tree in question and the gold standard parse, and is the standard way to evaluate the accuracy of a parser. We denote the k’</context>
<context position="24097" citStr="Collins and Duffy, 2002" startWordPosition="3948" endWordPosition="3951">P), and a combination of both (F,a=1) on validation set sentences of length at most 100. model, but only the improvement of the TOP kernels is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures and the voted perceptron. In Proc. 40th Meeting of Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. 42th Meeting ofAssociation for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1916" citStr="Collins and Roark, 2004" startWordPosition="291" endWordPosition="294">ine learning problems. They have the advantage that learning can try to optimize measures related directly to expected testing performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the probabilistic model, not directly in the kern</context>
<context position="24122" citStr="Collins and Roark, 2004" startWordPosition="3952" endWordPosition="3955">both (F,a=1) on validation set sentences of length at most 100. model, but only the improvement of the TOP kernels is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 200</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. 42th Meeting ofAssociation for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="11525" citStr="Collins, 1999" startWordPosition="1900" endWordPosition="1901">t layer weights. These weights define a normalized exponential model, with the network’s hidden layer as the input features. When we tried using the complete set of weights in some small scale experiments, training the classifier was more computationally expensive, and actually performed slightly worse than just using the output weights. Using just the output weights also allows us to make some approximations in the TOP reranking kernel which makes the classifier learning algorithm more efficient. 3.1 A History-Based Probability Model As with many other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), Henderson (2003) uses a history-based model of parsing. He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see (Henderson, 2003) for more details). The parser actions include: introducing a new constituent with a specified label, attaching one constituent to another, and predicting the next word of the sentence. A complete parse consists of a sequence of these actions, d1,..., dm, such that performing d1,..., dm results in a complete phrase structure tree. Because this mapping to parse sequences is 183 one-to-o</context>
<context position="23007" citStr="Collins, 1999" startWordPosition="3784" endWordPosition="3785">ning it for more epochs, as has been empirically demonstrated in other domains (Freund and Schapire, 1998). To avoid repeated testing on the standard testing set, we first compare the different models with their performance on the validation set. Note that the validation set wasn’t used during learning of the kernel models or for adjustment of any parameters. Standard measures of accuracy are shown in table 1.3 Both the Fisher kernel and the TOP kernels show better accuracy than the baseline probabilistic 3All our results are computed with the evalb program following the standard criteria in (Collins, 1999), and using the standard training (sections 2–22, 39,832 sentences, 910,196 words), validation (section 24, 1346 sentence, 31507 words), and testing (section 23, 2416 sentences, 54268 words) sets (Collins, 1999). LR LP F,3=1 SSN-Freq&gt;200 87.2 88.5 87.8 Fisher-Freq&gt;200 87.2 88.8 87.9 TOP-Freq&gt;200 87.3 88.9 88.1 TOP-Eff-Freq&gt;200 87.3 88.9 88.1 SSN-Freq&gt;20 88.1 89.2 88.6 TOP-Eff-Freq&gt;20 88.2 89.7 88.9 Table 1: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (F,a=1) on validation set sentences of length at most 100. model, but only the improvement of the TOP k</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. 17th Int. Conf. on Machine Learning,</booktitle>
<pages>175--182</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="24170" citStr="Collins, 2000" startWordPosition="3961" endWordPosition="3962"> 100. model, but only the improvement of the TOP kernels is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi, 2004). 6 Related Work The fir</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. 17th Int. Conf. on Machine Learning, pages 175–182, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1998</date>
<booktitle>In Proc. of the 11th Annual Conf. on Computational Learning Theory,</booktitle>
<pages>209--217</pages>
<location>Madisson WI.</location>
<contexts>
<context position="3902" citStr="Freund and Schapire, 1998" startWordPosition="603" endWordPosition="606">ter than models parameterized to estimate the a posteriori probability directly. 181 Proceedings of the 43rd Annual Meeting of the ACL, pages 181–188, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics In this paper, we propose a new method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing. For the probabilistic model, we use a state-of-the-art neural network based statistical parser (Henderson, 2003). The resulting kernel is then used with the Voted Perceptron algorithm (Freund and Schapire, 1998) to reranking the top 20 parses from the probabilistic model. This method achieves a significant improvement over the accuracy of the probabilistic model alone. 2 Kernels Derived from Probabilistic Models In recent years, several methods have been proposed for constructing kernels from trained probabilistic models. As usual, these kernels are then used with linear classifiers to learn the desired task. As well as some empirical successes, these methods are motivated by theoretical results which suggest we should expect some improvement with these classifiers over the classifier which chooses t</context>
<context position="16635" citStr="Freund and Schapire, 1998" startWordPosition="2704" endWordPosition="2707">ranteed to converge to a global optimum, but in practice a network whose criteria value is close to the optimum can be found. 4 Large-Margin Optimization Once we have defined a kernel over parse trees, general techniques for linear classifier optimization can be used to learn the given task. The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank (Marcus et al., 1993). Instead we use a 184 method which has often been shown to be virtually as good, the Voted Perceptron (VP) (Freund and Schapire, 1998) algorithm. The VP algorithm was originally applied to parse reranking in (Collins and Duffy, 2002) with the Tree kernel. We modify the perceptron training algorithm to make it more suitable for parsing, where zero-one classification loss is not the evaluation measure usually employed. We also develop a variant of the kernel defined in section 2.3, which is more efficient when used with the VP algorithm. Given a list of candidate trees, we train the classifier to select the tree with largest constituent F1 score. The F1 score is a measure of the similarity between the tree in question and the </context>
<context position="22499" citStr="Freund and Schapire, 1998" startWordPosition="3698" endWordPosition="3701">ity estimated by the probabilistic model (see section 2.3). We trained the VP model with all three kernels using the 508 word vocabulary (Fisher-Freq&gt;200, TOP-Freq&gt;200, TOP-Eff-Freq&gt;200) but only the efficient TOP reranking kernel model was trained with the vocabulary of 4215 words (TOP-Eff-Freq&gt;20). The non-sparsity of the feature vectors for other kernels led to the excessive memory requirements and larger testing time. In each case, the VP model was run for only one epoch. We would expect some improvement if running it for more epochs, as has been empirically demonstrated in other domains (Freund and Schapire, 1998). To avoid repeated testing on the standard testing set, we first compare the different models with their performance on the validation set. Note that the validation set wasn’t used during learning of the kernel models or for adjustment of any parameters. Standard measures of accuracy are shown in table 1.3 Both the Fisher kernel and the TOP kernels show better accuracy than the baseline probabilistic 3All our results are computed with the evalb program following the standard criteria in (Collins, 1999), and using the standard training (sections 2–22, 39,832 sentences, 910,196 words), validati</context>
</contexts>
<marker>Freund, Schapire, 1998</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1998. Large margin classification using the perceptron algorithm. In Proc. of the 11th Annual Conf. on Computational Learning Theory, pages 209–217, Madisson WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proc. joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf.,</booktitle>
<pages>103--110</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="3803" citStr="Henderson, 2003" startWordPosition="589" endWordPosition="590">ng. 1For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. 181 Proceedings of the 43rd Annual Meeting of the ACL, pages 181–188, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics In this paper, we propose a new method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing. For the probabilistic model, we use a state-of-the-art neural network based statistical parser (Henderson, 2003). The resulting kernel is then used with the Voted Perceptron algorithm (Freund and Schapire, 1998) to reranking the top 20 parses from the probabilistic model. This method achieves a significant improvement over the accuracy of the probabilistic model alone. 2 Kernels Derived from Probabilistic Models In recent years, several methods have been proposed for constructing kernels from trained probabilistic models. As usual, these kernels are then used with linear classifiers to learn the desired task. As well as some empirical successes, these methods are motivated by theoretical results which s</context>
<context position="9963" citStr="Henderson, 2003" startWordPosition="1655" endWordPosition="1656">sider the probability P(yk|x, y1, ... , ys, θ?) instead of the probability P(c=+1|z, θ?) considered by Tsuda. The resulting feature extractor is given by: φˆθ(x, yk) = (v(x, yk, ˆθ) ∂v(aelk,ˆθ) ∂v(aekA), , where v(x, yk, ˆθ) = log P (yk|y1, . . . , ys, ˆθ) − log Et=,�k P(yt|y1, ... , ys, ˆθ). We will call this kernel the TOP reranking kernel. 3 The Probabilistic Model To complete the definition of the kernel, we need to choose a probabilistic model of parsing. For this we use a statistical parser which has previously been shown to achieve state-of-the-art performance, namely that proposed in (Henderson, 2003). This parser has two levels of parameterization. The first level of parameterization is in terms of a historybased generative probability model, but this level is not appropriate for our purposes because it defines an infinite number of parameters (one for every possible partial parse history). When parsing a given sentence, the bounded set of parameters which are relevant to a given parse are estimated using a neural network. The weights of this neural network form the second level of parameterization. There is a finite number of these parameters. Neural network training is applied to determ</context>
<context position="11560" citStr="Henderson (2003)" startWordPosition="1904" endWordPosition="1905">efine a normalized exponential model, with the network’s hidden layer as the input features. When we tried using the complete set of weights in some small scale experiments, training the classifier was more computationally expensive, and actually performed slightly worse than just using the output weights. Using just the output weights also allows us to make some approximations in the TOP reranking kernel which makes the classifier learning algorithm more efficient. 3.1 A History-Based Probability Model As with many other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), Henderson (2003) uses a history-based model of parsing. He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see (Henderson, 2003) for more details). The parser actions include: introducing a new constituent with a specified label, attaching one constituent to another, and predicting the next word of the sentence. A complete parse consists of a sequence of these actions, d1,..., dm, such that performing d1,..., dm results in a complete phrase structure tree. Because this mapping to parse sequences is 183 one-to-one, and the word prediction actions</context>
<context position="13110" citStr="Henderson (2003)" startWordPosition="2151" endWordPosition="2152">1,..., di−1. P(d1,..., dm) = HiP(di|d1,..., di−1) 3.2 Estimating Decision Probabilities with a Neural Network The parameters of the above probability model are the P(di|d1,..., di−1). There are an infinite number of these parameters, since the parse history d1,..., di−1 grows with the length of the sentence. In other work on history-based parsing, independence assumptions are applied so that only a finite amount of information from the parse history can be treated as relevant to each parameter, thereby reducing the number of parameters to a finite set which can be estimated directly. Instead, Henderson (2003) uses a neural network to induce a finite representation of this unbounded history, which we will denote h(d1,..., di−1). Neural network training tries to find such a history representation which preserves all the information about the history which is relevant to estimating the desired probability. P(di|d1,..., di−1) ≈ P(di|h(d1,..., di−1)) Using a neural network architecture called Simple Synchrony Networks (SSNs), the history representation h(d1,..., di−1) is incrementally computed from features of the previous decision di−1 plus a finite set of previous history representations h(d1,..., dj</context>
<context position="15169" citStr="Henderson, 2003" startWordPosition="2468" endWordPosition="2469">uents are structurally local to the current representation’s constituent are input to the computation of the correct representation. In this way, the number of representations which information needs to pass through in order to flow from history representation i to history representation j is determined by the structural distance between i’s constituent and j’s constituent, and not just the distance between i and j in the parse sequence. This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). Once it has computed h(d1,..., di−1), the SSN uses a normalized exponential to estimate a probability distribution over the set of possible next decisions di given the history: P(di|d1,..., di−1, 0) ≈ exp(&lt;Bdi,h(d1,...,di−1)&gt;) EtEN(di−1) exp(&lt;0t,h(d1,...,di−1)&gt;), where by 0t we denote the set of output layer weights, corresponding to the parser action t, N(di−1) defines a set of possible next parser actions after the step di−1 and 0 denotes the full set of model parameters. We trained SSN parsing models, using the on-line version of Backpropagation to perform the gradient descent with a maxi</context>
<context position="24139" citStr="Henderson, 2003" startWordPosition="3956" endWordPosition="3957">n set sentences of length at most 100. model, but only the improvement of the TOP kernels is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Inducing history representations for broad coverage statistical parsing. In Proc. joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf., pages 103–110, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proc. 42nd Meeting ofAssociation for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="3226" citStr="Henderson, 2004" startWordPosition="500" endWordPosition="501">ntly well understood, both in terms of reflecting generalizations and controlling computational cost. Because many NLP problems are unbounded in size and complexity, it is hard to specify all possible relevant kernel features without having so many features that the computations become intractable and/or the data becomes too sparse.1 Second, the kernel is defined using the trained parameters of the probabilistic model. Thus the kernel is in part determined by the training data, and is automatically tailored to reflect properties of parse trees which are relevant to parsing. 1For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. 181 Proceedings of the 43rd Annual Meeting of the ACL, pages 181–188, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics In this paper, we propose a new method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing. For the probabilistic model, we use a state-of-the-art neural network based statistical parser (Henderson, 2003). The resulting kernel </context>
<context position="24228" citStr="Henderson, 2004" startWordPosition="3971" endWordPosition="3972"> is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi, 2004). 6 Related Work The first application of kernel methods to parsing was proposed b</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proc. 42nd Meeting ofAssociation for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi S Jaakkola</author>
<author>David Haussler</author>
</authors>
<title>Exploiting generative models in discriminative classifiers.</title>
<date>1998</date>
<booktitle>Advances in Neural Information Processes Systems 11.</booktitle>
<contexts>
<context position="2323" citStr="Jaakkola and Haussler, 1998" startWordPosition="355" endWordPosition="358"> natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the probabilistic model, not directly in the kernel. Designing probabilistic models to reflect linguistic knowledge is a process which is currently well understood, both in terms of reflecting generalizations and controlling computational cost. Because many NLP problems are unbounded in size and complexity, it is hard to specify all possible relevant kernel features without having so many features that the computations become intractable and/or the dat</context>
<context position="6037" citStr="Jaakkola and Haussler, 1998" startWordPosition="960" endWordPosition="963">training sentences, we learn a discriminant function F : X x Y —* R. The parse tree y with the largest value for this discriminant function F(x, y) is the output parse tree for the sentence x. We focus on the linear discriminant functions: Fw(x, y) = &lt;w, φ(x, y)&gt;, where φ(x, y) is a feature vector for the sentencetree pair, w is a parameter vector for the discriminant function, and &lt;a, b&gt; is the inner product of vectors a and b. In the remainder of this section, we will characterize the kernel methods we consider in terms of the feature extractor φ(x, y). 2.1 Fisher Kernels The Fisher kernel (Jaakkola and Haussler, 1998) is one of the best known kernels belonging to the class of probability model based kernels. Given a generative model of P(z|�θ) with smooth parameterization, the Fisher score of an example z is a vector of partial derivatives of the log-likelihood of the example with respect to the model parameters: φˆθ(z) = (∂log P(z|ˆθ) ∂θ1 , . . . , ∂log P(z|ˆθ) ∂θl ). This score can be regarded as specifying how the model should be changed in order to maximize the likelihood of the example z. Then we can define the similarity between data points as the inner product of the corresponding Fisher scores. Thi</context>
</contexts>
<marker>Jaakkola, Haussler, 1998</marker>
<rawString>Tommi S. Jaakkola and David Haussler. 1998. Exploiting generative models in discriminative classifiers. Advances in Neural Information Processes Systems 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="16500" citStr="Marcus et al., 1993" startWordPosition="2679" endWordPosition="2682">computation and the parameters of the mappings h(d1,..., di−1). With multilayered networks such as SSNs, this training is not guaranteed to converge to a global optimum, but in practice a network whose criteria value is close to the optimum can be found. 4 Large-Margin Optimization Once we have defined a kernel over parse trees, general techniques for linear classifier optimization can be used to learn the given task. The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank (Marcus et al., 1993). Instead we use a 184 method which has often been shown to be virtually as good, the Voted Perceptron (VP) (Freund and Schapire, 1998) algorithm. The VP algorithm was originally applied to parse reranking in (Collins and Duffy, 2002) with the Tree kernel. We modify the perceptron training algorithm to make it more suitable for parsing, where zero-one classification loss is not the evaluation measure usually employed. We also develop a variant of the kernel defined in section 2.3, which is more efficient when used with the VP algorithm. Given a list of candidate trees, we train the classifier </context>
<context position="20429" citStr="Marcus et al., 1993" startWordPosition="3353" endWordPosition="3356"> define our kernel only using the parameters in the first term. This means that the second term does not effect the derivatives in the formula for the feature vector φ(x, y). Thus the feature vector for the kernel will contain nonzero entries only in the components corresponding to the parser actions which are present in the candidate derivation for the sentence, and thus in the first vector component. We have applied this technique to the TOP reranking kernel, the result of which we will call the efficient TOP reranking kernel. 5 The Experimental Results We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform empirical experiments on the proposed parsing models. In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags. 185 occur in the training set in order to be included explicitly in the vocabulary. A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs (including tag-unknown word pairs) and a threshold of 20 resulted in 4215 tag-word pairs. We denote the probabilis</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<location>Univ. of Pennsylvania, PA.</location>
<contexts>
<context position="20733" citStr="Ratnaparkhi, 1996" startWordPosition="3405" endWordPosition="3406">h are present in the candidate derivation for the sentence, and thus in the first vector component. We have applied this technique to the TOP reranking kernel, the result of which we will call the efficient TOP reranking kernel. 5 The Experimental Results We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform empirical experiments on the proposed parsing models. In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags. 185 occur in the training set in order to be included explicitly in the vocabulary. A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs (including tag-unknown word pairs) and a threshold of 20 resulted in 4215 tag-word pairs. We denote the probabilistic model trained with the vocabulary of 508 by the SSN-Freq&gt;200, the model trained with the vocabulary of 4215 by the SSN-Freq&gt;20. Testing the probabilistic parser requires using a beam search through the space of possible parses. We used a form of beam search which prunes the search after the predicti</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. Conf. on Empirical Methods in Natural Language Processing, pages 133–142, Univ. of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--151</pages>
<contexts>
<context position="11510" citStr="Ratnaparkhi, 1999" startWordPosition="1898" endWordPosition="1899">the network’s output layer weights. These weights define a normalized exponential model, with the network’s hidden layer as the input features. When we tried using the complete set of weights in some small scale experiments, training the classifier was more computationally expensive, and actually performed slightly worse than just using the output weights. Using just the output weights also allows us to make some approximations in the TOP reranking kernel which makes the classifier learning algorithm more efficient. 3.1 A History-Based Probability Model As with many other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), Henderson (2003) uses a history-based model of parsing. He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see (Henderson, 2003) for more details). The parser actions include: introducing a new constituent with a specified label, attaching one constituent to another, and predicting the next word of the sentence. A complete parse consists of a sequence of these actions, d1,..., dm, such that performing d1,..., dm results in a complete phrase structure tree. Because this mapping to parse sequences </context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34:151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>An SVM based voting algorithm with application to parse reranking.</title>
<date>2003</date>
<booktitle>In Proc. of the 7th Conf. on Computational Natural Language Learning,</booktitle>
<pages>9--16</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1871" citStr="Shen and Joshi, 2003" startWordPosition="283" endWordPosition="286">n shown to be very effective in many machine learning problems. They have the advantage that learning can try to optimize measures related directly to expected testing performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the </context>
<context position="25677" citStr="Shen and Joshi (2003)" startWordPosition="4207" endWordPosition="4210"> over the baseline. 4We measured significance with the randomized significance test of (Yeh, 2000). 186 LR LP F,a=1* Collins99 88.1 88.3 88.2 Collins&amp;Duffy02 88.6 88.9 88.7 Collins&amp;Roark04 88.4 89.1 88.8 Henderson03 88.8 89.5 89.1 Charniak00 89.6 89.5 89.5 TOP-Eff-Freq&gt;20 89.1 90.1 89.6 Collins00 89.6 89.9 89.7 Shen&amp;Joshi04 89.5 90.0 89.8 Shen et al.03 89.7 90.0 89.8 Henderson04 89.8 90.4 90.1 Bod03 90.7 90.8 90.7 * Fp=1 for previous models may have rounding errors. Table 2: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (Fo=1) on the entire testing set. Shen and Joshi (2003) applied an SVM based voting algorithm with the Preference kernel defined over pairs for reranking. To define the Preference kernel they used the Tree kernel and the Linear kernel as its underlying kernels and achieved state-ofthe-art results with the Linear kernel. In (Shen et al., 2003) it was pointed out that most of the arbitrary tree fragments allowed by the Tree kernel are linguistically meaningless. The authors suggested the use of Lexical Tree Adjoining Grammar (LTAG) based features as a more linguistically appropriate set of features. They empirically demonstrated that incorporation o</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. An SVM based voting algorithm with application to parse reranking. In Proc. of the 7th Conf. on Computational Natural Language Learning, pages 9–16, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>Flexible margin selection for reranking with full pairwise samples.</title>
<date>2004</date>
<booktitle>In Proc. of the 1st Int. Joint Conf. on Natural Language Processing,</booktitle>
<location>Hainan Island, China.</location>
<contexts>
<context position="24192" citStr="Shen and Joshi, 2004" startWordPosition="3963" endWordPosition="3966">t only the improvement of the TOP kernels is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi, 2004). 6 Related Work The first application of kern</context>
<context position="26355" citStr="Shen and Joshi (2004)" startWordPosition="4316" endWordPosition="4319">e kernel defined over pairs for reranking. To define the Preference kernel they used the Tree kernel and the Linear kernel as its underlying kernels and achieved state-ofthe-art results with the Linear kernel. In (Shen et al., 2003) it was pointed out that most of the arbitrary tree fragments allowed by the Tree kernel are linguistically meaningless. The authors suggested the use of Lexical Tree Adjoining Grammar (LTAG) based features as a more linguistically appropriate set of features. They empirically demonstrated that incorporation of these features helps to improve reranking performance. Shen and Joshi (2004) proposed to improve margin based methods for reranking by defining the margin not only between the top tree and all the other trees in the candidate list but between all the pairs of parses in the ordered candidate list for the given sentence. They achieved the best results when training with an uneven margin scaled by the heuristic function of the candidates positions in the list. One potential drawback of this method is that it doesn’t take into account the actual F1 score of the candidate and considers only the position in the list ordered by the F1 score. We expect that an improvement cou</context>
</contexts>
<marker>Shen, Joshi, 2004</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2004. Flexible margin selection for reranking with full pairwise samples. In Proc. of the 1st Int. Joint Conf. on Natural Language Processing, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using LTAG based features in parse reranking.</title>
<date>2003</date>
<booktitle>In Proc. of Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1890" citStr="Shen et al., 2003" startWordPosition="287" endWordPosition="290">ective in many machine learning problems. They have the advantage that learning can try to optimize measures related directly to expected testing performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the probabilistic model</context>
<context position="24211" citStr="Shen et al., 2003" startWordPosition="3967" endWordPosition="3970"> of the TOP kernels is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes. Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary. For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq&gt;20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same. When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi, 2004). 6 Related Work The first application of kernel methods to parsi</context>
<context position="25966" citStr="Shen et al., 2003" startWordPosition="4255" endWordPosition="4258">ins00 89.6 89.9 89.7 Shen&amp;Joshi04 89.5 90.0 89.8 Shen et al.03 89.7 90.0 89.8 Henderson04 89.8 90.4 90.1 Bod03 90.7 90.8 90.7 * Fp=1 for previous models may have rounding errors. Table 2: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (Fo=1) on the entire testing set. Shen and Joshi (2003) applied an SVM based voting algorithm with the Preference kernel defined over pairs for reranking. To define the Preference kernel they used the Tree kernel and the Linear kernel as its underlying kernels and achieved state-ofthe-art results with the Linear kernel. In (Shen et al., 2003) it was pointed out that most of the arbitrary tree fragments allowed by the Tree kernel are linguistically meaningless. The authors suggested the use of Lexical Tree Adjoining Grammar (LTAG) based features as a more linguistically appropriate set of features. They empirically demonstrated that incorporation of these features helps to improve reranking performance. Shen and Joshi (2004) proposed to improve margin based methods for reranking by defining the margin not only between the top tree and all the other trees in the candidate list but between all the pairs of parses in the ordered candi</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Libin Shen, Anoop Sarkar, and Aravind K. Joshi. 2003. Using LTAG based features in parse reranking. In Proc. of Conf. on Empirical Methods in Natural Language Processing, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proc. Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="27489" citStr="Taskar et al., 2004" startWordPosition="4513" endWordPosition="4516">ly the position in the list ordered by the F1 score. We expect that an improvement could be achieved by combining our approach of scaling updates by the F1 loss with the all pairs approach of (Shen and Joshi, 2004). Use of the F1 loss function during training demonstrated better performance comparing to the 0-1 loss function when applied to a structured classification task (Tsochantaridis et al., 2004). All the described kernel methods are limited to the reranking of candidates from an existing parser due to the complexity of finding the best parse given a kernel (i.e. the decoding problem). (Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. The efficiency of dynamic programming means that the entire space of parses can be considered, not just a candidate list. However, not all kernels are suitable for this method. The dynamic programming approach requires the feature vector of a tree to be decomposable into a sum over parts of the tree. In particular, this is impossible with the TOP and Fisher kernels derived from the SSN model. Also, it isn’t clear whether the algorithm remains tractable fo</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In Proc. Conf. on Empirical Methods in Natural Language Processing, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proc. 21st Int. Conf. on Machine Learning,</booktitle>
<pages>823--830</pages>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="18424" citStr="Tsochantaridis et al., 2004" startWordPosition="3008" endWordPosition="3011"> as is usually the case. We modify the perceptron algorithm by introducing a new classification loss function. This modification enables us to treat differently the cases where the perceptron predicts a tree with an F1 score much smaller than that of the top candidate and the cases where the predicted and the top candidates have similar score values. The natural choice for the loss function would be Δ(yjk, yj1) = F1(yj1) − F1(yjk), where F1(yjk) denotes the F1 score value for the parse tree yjk. This approach is very similar to slack variable rescaling for Support Vector Machines proposed in (Tsochantaridis et al., 2004). The learning algorithm we employed is presented in figure 1. When applying kernels with a large training corpus, we face efficiency issues because of the large number of the neural network weights. Even though we use only the output layer weights, this vector grows with the size of the vocabulary, and thus can be large. The kernels presented in section 2 all lead to feature vectors without many zero values. This w = 0 for j = 1 .. n for k = 2 .. s if &lt;w,φ(xj, yjk)&gt; &gt; &lt;w,φ(xj, yj1)&gt; w = w + Δ(yjk, yj1)(φ(xj, yj1) − φ(xj, yjk)) Figure 1: The modified perceptron algorithm happens because we com</context>
<context position="27274" citStr="Tsochantaridis et al., 2004" startWordPosition="4477" endWordPosition="4480">ith an uneven margin scaled by the heuristic function of the candidates positions in the list. One potential drawback of this method is that it doesn’t take into account the actual F1 score of the candidate and considers only the position in the list ordered by the F1 score. We expect that an improvement could be achieved by combining our approach of scaling updates by the F1 loss with the all pairs approach of (Shen and Joshi, 2004). Use of the F1 loss function during training demonstrated better performance comparing to the 0-1 loss function when applied to a structured classification task (Tsochantaridis et al., 2004). All the described kernel methods are limited to the reranking of candidates from an existing parser due to the complexity of finding the best parse given a kernel (i.e. the decoding problem). (Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. The efficiency of dynamic programming means that the entire space of parses can be considered, not just a candidate list. However, not all kernels are suitable for this method. The dynamic programming approach requires the feature vector of a t</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proc. 21st Int. Conf. on Machine Learning, pages 823–830, Banff, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tsuda</author>
<author>M Kawanabe</author>
<author>G Ratsch</author>
<author>S Sonnenburg</author>
<author>K Muller</author>
</authors>
<title>A new discriminative kernel from probabilistic models.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>10</issue>
<contexts>
<context position="2344" citStr="Tsuda et al., 2002" startWordPosition="359" endWordPosition="362">articular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the probabilistic model, not directly in the kernel. Designing probabilistic models to reflect linguistic knowledge is a process which is currently well understood, both in terms of reflecting generalizations and controlling computational cost. Because many NLP problems are unbounded in size and complexity, it is hard to specify all possible relevant kernel features without having so many features that the computations become intractable and/or the data becomes too sparse.</context>
<context position="9335" citStr="Tsuda et al., 2002" startWordPosition="1540" endWordPosition="1543">te trees suggested by a probabilistic model. Furthermore, we only consider learning to rerank the output of a particular probabilistic model, without requiring the classifier to have good performance when applied to a candidate list provided by a different model. In this case, it is natural to model the probability that a parse tree is the best candidate given the list of candidate trees: P(yk|x, y1, ... , ys) = P(x,yk) Et P(x,yt), where y1, ... , ys is the list of candidate parse trees. To construct a new TOP kernel for reranking, we apply an approach similar to that used for the TOP kernel (Tsuda et al., 2002), but we consider the probability P(yk|x, y1, ... , ys, θ?) instead of the probability P(c=+1|z, θ?) considered by Tsuda. The resulting feature extractor is given by: φˆθ(x, yk) = (v(x, yk, ˆθ) ∂v(aelk,ˆθ) ∂v(aekA), , where v(x, yk, ˆθ) = log P (yk|y1, . . . , ys, ˆθ) − log Et=,�k P(yt|y1, ... , ys, ˆθ). We will call this kernel the TOP reranking kernel. 3 The Probabilistic Model To complete the definition of the kernel, we need to choose a probabilistic model of parsing. For this we use a statistical parser which has previously been shown to achieve state-of-the-art performance, namely that p</context>
</contexts>
<marker>Tsuda, Kawanabe, Ratsch, Sonnenburg, Muller, 2002</marker>
<rawString>K. Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg, and K. Muller. 2002. A new discriminative kernel from probabilistic models. Neural Computation, 14(10):2397–2414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of the result differences.</title>
<date>2000</date>
<booktitle>In Proc. 17th International Conf. on Computational Linguistics,</booktitle>
<pages>947--953</pages>
<location>Saarbruken, Germany.</location>
<contexts>
<context position="25154" citStr="Yeh, 2000" startWordPosition="4126" endWordPosition="4127">ased on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi, 2004). 6 Related Work The first application of kernel methods to parsing was proposed by Collins and Duffy (2002). They used the Tree kernel, where the features of a tree are all its connected tree fragments. The VP algorithm was applied to rerank the output of a probabilistic model and demonstrated an improvement over the baseline. 4We measured significance with the randomized significance test of (Yeh, 2000). 186 LR LP F,a=1* Collins99 88.1 88.3 88.2 Collins&amp;Duffy02 88.6 88.9 88.7 Collins&amp;Roark04 88.4 89.1 88.8 Henderson03 88.8 89.5 89.1 Charniak00 89.6 89.5 89.5 TOP-Eff-Freq&gt;20 89.1 90.1 89.6 Collins00 89.6 89.9 89.7 Shen&amp;Joshi04 89.5 90.0 89.8 Shen et al.03 89.7 90.0 89.8 Henderson04 89.8 90.4 90.1 Bod03 90.7 90.8 90.7 * Fp=1 for previous models may have rounding errors. Table 2: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (Fo=1) on the entire testing set. Shen and Joshi (2003) applied an SVM based voting algorithm with the Preference kernel defined ove</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of the result differences. In Proc. 17th International Conf. on Computational Linguistics, pages 947–953, Saarbruken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>