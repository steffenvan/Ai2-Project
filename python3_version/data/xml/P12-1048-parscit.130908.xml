<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000140">
<title confidence="0.9990085">
Translation Model Adaptation for Statistical Machine Translation with
Monolingual Topic Information*
</title>
<author confidence="0.953593">
Jinsong Su&apos;,&apos;, Hua Wu&apos;, Haifeng Wang&apos;, Yidong Chen&apos;, Xiaodong Shi&apos;,
Huailin Dong&apos;, and Qun Liu&apos;
</author>
<affiliation confidence="0.949439666666667">
Xiamen University, Xiamen, China&apos;
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China&apos;
Baidu Inc., Beijing, China&apos;
</affiliation>
<email confidence="0.822926333333333">
{jssu, ydchen, mandel, hldong}@xmu.edu.cn
{wu hua, wanghaifeng}@baicu.com
liuqun@ict.ac.cn
</email>
<sectionHeader confidence="0.995504" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.90229055">
To adapt a translation model trained from
the data in one domain to another, previous
works paid more attention to the studies of
parallel corpus while ignoring the in-domain
monolingual corpora which can be obtained
more easily. In this paper, we propose a
novel approach for translation model adapta-
tion by utilizing in-domain monolingual top-
ic information instead of the in-domain bilin-
gual corpora, which incorporates the topic in-
formation into translation probability estima-
tion. Our method establishes the relationship
between the out-of-domain bilingual corpus
and the in-domain monolingual corpora vi-
a topic mapping and phrase-topic distribution
probability estimation from in-domain mono-
lingual corpora. Experimental result on the
NIST Chinese-English translation task shows
that our approach significantly outperforms
the baseline system.
</bodyText>
<sectionHeader confidence="0.998868" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991917130434783">
In recent years, statistical machine translation(SMT)
has been rapidly developing with more and more
novel translation models being proposed and put in-
to practice (Koehn et al., 2003; Och and Ney, 2004;
Galley et al., 2006; Liu et al., 2006; Chiang, 2007;
Chiang, 2010). However, similar to other natural
language processing(NLP) tasks, SMT systems of-
ten suffer from domain adaptation problem during
practical applications. The simple reason is that the
underlying statistical models always tend to closely
*Part of this work was done during the first author’s intern-
ship at Baidu.
approximate the empirical distributions of the train-
ing data, which typically consist of bilingual sen-
tences and monolingual target language sentences.
When the translated texts and the training data come
from the same domain, SMT systems can achieve
good performance, otherwise the translation quality
degrades dramatically. Therefore, it is of significant
importance to develop translation systems which can
be effectively transferred from one domain to anoth-
er, for example, from newswire to weblog.
According to adaptation emphases, domain adap-
tation in SMT can be classified into translation mod-
el adaptation and language model adaptation. Here
we focus on how to adapt a translation model, which
is trained from the large-scale out-of-domain bilin-
gual corpus, for domain-specific translation task,
leaving others for future work. In this aspect, pre-
vious methods can be divided into two categories:
one paid attention to collecting more sentence pairs
by information retrieval technology (Hildebrand et
al., 2005) or synthesized parallel sentences (Ueffing
et al., 2008; Wu et al., 2008; Bertoldi and Federico,
2009; Schwenk and Senellart, 2009), and the other
exploited the full potential of existing parallel cor-
pus in a mixture-modeling (Foster and Kuhn, 2007;
Civera and Juan, 2007; Lv et al., 2007) framework.
However, these approaches focused on the studies of
bilingual corpus synthesis and exploitation while ig-
noring the monolingual corpora, therefore limiting
the potential of further translation quality improve-
ment.
In this paper, we propose a novel adaptation
method to adapt the translation model for domain-
specific translation task by utilizing in-domain
</bodyText>
<page confidence="0.987668">
459
</page>
<note confidence="0.985771">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999739833333333">
monolingual corpora. Our approach is inspired by
the recent studies (Zhao and Xing, 2006; Zhao and
Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010;
Ruiz and Federico, 2011) which have shown that a
particular translation always appears in some spe-
cific topical contexts, and the topical context infor-
mation has a great effect on translation selection.
For example, “bank” often occurs in the sentences
related to the economy topic when translated into
“y´inh´ang”, and occurs in the sentences related to the
geography topic when translated to “h´e`an”. There-
fore, the co-occurrence frequency of the phrases in
some specific context can be used to constrain the
translation candidates of phrases. In a monolingual
corpus, if “bank” occurs more often in the sentences
related to the economy topic than the ones related
to the geography topic, it is more likely that “bank”
is translated to “y´inh´ang” than to “h´e`an”. With the
out-of-domain bilingual corpus, we first incorporate
the topic information into translation probability es-
timation, aiming to quantify the effect of the topical
context information on translation selection. Then,
we rescore all phrase pairs according to the phrase-
topic and the word-topic posterior distributions of
the additional in-domain monolingual corpora. As
compared to the previous works, our method takes
advantage of both the in-domain monolingual cor-
pora and the out-of-domain bilingual corpus to in-
corporate the topic information into our translation
model, thus breaking down the corpus barrier for
translation quality improvement. The experimental
results on the NIST data set demonstrate the effec-
tiveness of our method.
The reminder of this paper is organized as fol-
lows: Section 2 provides a brief description of trans-
lation probability estimation. Section 3 introduces
the adaptation method which incorporates the top-
ic information into the translation model; Section
4 describes and discusses the experimental results;
Section 5 briefly summarizes the recent related work
about translation model adaptation. Finally, we end
with a conclusion and the future work in Section 6.
</bodyText>
<sectionHeader confidence="0.982043" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.997747434782609">
The statistical translation model, which contains
phrase pairs with bi-directional phrase probabilities
and bi-directional lexical probabilities, has a great
effect on the performance of SMT system. Phrase
probability measures the co-occurrence frequency of
a phrase pair, and lexical probability is used to vali-
date the quality of the phrase pair by checking how
well its words are translated to each other.
According to the definition proposed by (Koehn
et al., 2003), given a source sentence f = fJ1 =
f1, ... , fj, ... , fJ, a target sentence e = eI1 =
e1, ... , ei, ... , eI, and its word alignment a which
is a subset of the Cartesian product of word position-
s: a C_ (j, i) : j = 1, ... , J; i = 1, ... , I, the phrase
pair (˜f, ˜e) is said to be consistent (Och and Ney,
2004) with the alignment if and only if: (1) there
must be at least one word inside one phrase aligned
to a word inside the other phrase and (2) no words
inside one phrase can be aligned to a word outside
the other phrase. After all consistent phrase pairs are
extracted from training corpus, the phrase probabil-
ities are estimated as relative frequencies (Och and
Ney, 2004):
</bodyText>
<equation confidence="0.9770325">
˜f) = count(
count(
</equation>
<bodyText confidence="0.994446">
Here count(˜f, ˜e) indicates how often the phrase pair
( ˜f, ˜e) occurs in the training corpus.
To obtain the corresponding lexical weight, we
first estimate a lexical translation probability distri-
bution w(eIf) by relative frequency from the train-
ing corpus:
</bodyText>
<equation confidence="0.98865125">
count(f, e)
w(elf) = E (2)
count(f, e&apos;)
el
</equation>
<bodyText confidence="0.990781">
Retaining the alignment a˜ between the phrase pair
(˜f, ˜e), the corresponding lexical weight is calculated
as
</bodyText>
<equation confidence="0.9995335">
1E w(eilfj) (3)
�{jI(j,i) E ˜all V(j,i)E˜a
</equation>
<bodyText confidence="0.999641857142857">
However, the above-mentioned method only
counts the co-occurrence frequency of bilingual
phrases, assuming that the translation probability is
independent of the context information. Thus, the
statistical model estimated from the training data is
not suitable for text translation in different domains,
resulting in a significant drop in translation quality.
</bodyText>
<equation confidence="0.99818075">
φ(˜e1
E
˜e1
˜f, ˜e)
(1)
˜f, ˜e&apos;)
pw(˜e�˜f, ˜a) = � �˜e�
i=1
</equation>
<page confidence="0.996319">
460
</page>
<sectionHeader confidence="0.940351" genericHeader="method">
3 Translation Model Adaptation via
Monolingual Topic Information
</sectionHeader>
<bodyText confidence="0.999901">
In this section, we first briefly review the principle
of Hidden Topic Markov Model(HTMM) which is
the basis of our method, then describe our approach
to translation model adaptation in detail.
</bodyText>
<subsectionHeader confidence="0.999768">
3.1 Hidden Topic Markov Model
</subsectionHeader>
<bodyText confidence="0.999965307692308">
During the last couple of years, topic models such
as Probabilistic Latent Semantic Analysis (Hof-
mann, 1999) and Latent Dirichlet Allocation mod-
el (Blei, 2003), have drawn more and more attention
and been applied successfully in NLP community.
Based on the “bag-of-words” assumption that the or-
der of words can be ignored, these methods model
the text corpus by using a co-occurrence matrix of
words and documents, and build generative model-
s to infer the latent aspects or topics. Using these
models, the words can be clustered into the derived
topics with a probability distribution, and the corre-
lation between words can be automatically captured
via topics.
However, the “bag-of-words” assumption is an
unrealistic oversimplification because it ignores the
order of words. To remedy this problem, Gruber et
al.(2007) propose HTMM, which models the topics
of words in the document as a Markov chain. Based
on the assumption that all words in the same sen-
tence have the same topic and the successive sen-
tences are more likely to have the same topic, HTM-
M incorporates the local dependency between words
by Hidden Markov Model for better topic estima-
tion.
HTMM can also be viewed as a soft clustering
tool for words in training corpus. That is, HT-
MM can estimate the probability distribution of a
topic over words, i.e. the topic-word distribution
P(word|topic) during training. Besides, HTMM
derives inherent topics in sentences rather than in
documents, so we can easily obtain the sentence-
topic distribution P (topic|sentence) in training
corpus. Adopting maximum likelihood estima-
tion(MLE), this posterior distribution makes it pos-
sible to effectively calculate the word-topic distri-
bution P(topic|word) and the phrase-topic distribu-
tion P (topic|phrase) both of which are very impor-
tant in our method.
</bodyText>
<subsectionHeader confidence="0.999381">
3.2 Adapted Phrase Probability Estimation
</subsectionHeader>
<bodyText confidence="0.991477">
We utilize the additional in-domain monolingual
corpora to adapt the out-of-domain translation mod-
el for domain-specific translation task. In detail, we
build an adapted translation model in the following
steps:
</bodyText>
<listItem confidence="0.99550625">
• Build a topic-specific translation model to
quantify the effect of the topic information on
the translation probability estimation.
• Estimate the topic posterior distributions of
phrases in the in-domain monolingual corpora.
• Score the phrase pairs according to the prede-
fined topic-specific translation model and the
topic posterior distribution of phrases.
</listItem>
<bodyText confidence="0.999977666666667">
Formally, we incorporate monolingual topic in-
formation into translation probability estimation,
and decompose the phrase probability φ(e |f)1 as
</bodyText>
<equation confidence="0.667788">
follows: � φ(�e, tf |f)
φ(�e |f) �
tf
E� φ(6 |f, tf) - P(tf |f) (4)
tf
</equation>
<bodyText confidence="0.927732086956522">
where φ(e |f, tf) indicates the probability of trans-
latingf into e� given the source-side topic
Pt ff,
( I f)
To compute φ(�e |1), we first apply HTMM to re-
denotes the phrase-topic distri bution of f.
spectively train two monolingual topic models with
the following corpora: one is the source part of
the out-of-domain bilingual corpus Cf out, the oth-
er is the in-domain monolingual corpus Cf in in the
source language. Then, we respectively estimate
�f, tf) and P(tf |f) from these two corpora. To
avoid confusion, we further refine φ(e |f, tf) and
tively. Here, tf out is the topic clustered from the
corpus Cf out, and tf in represents the topic derived
from the corpus Cf in.
However, the two above-mentioned probabilities
can not be directly multiplied in formula (4) be-
cause they are related to different topic spaces from
1Due to the limit of space, we omit the description of the cal-
culation method of the phrase probability φ( 1|e), which can be
adjusted in a similar way to φ(e |f) with the help of in-domain
monolingual corpus in the target language.
</bodyText>
<equation confidence="0.9863905">
φ(�e|
P(tf|
� �f), respec-
f) with φ(e |f, tf out) and P(tf in|
</equation>
<page confidence="0.989018">
461
</page>
<bodyText confidence="0.997088571428572">
different corpora. Besides, their topic dimension-
s are not assured to be the same. To solve this
problem, we introduce the topic mapping probabili-
ty P(tf out|tf in) to map the in-domain phrase-topic
distribution into the one in the out-domain topic s-
pace. To be specific, we obtain the out-of-domain
phrase-topic distribution P(tf out |f) as follows:
</bodyText>
<equation confidence="0.841383714285714">
P(tf out |�f) = X P(tf out|tf in) · P(tf in |f) (5)
tf in
Thus formula (4) can be further refined as the fol-
lowing formula:
φ(6 |�f) = X
tf out
·P(tf out|tf in) · P(tf in|
</equation>
<bodyText confidence="0.999709">
Next we will give detailed descriptions of the cal-
culation methods for the three probability distribu-
tions mentioned in formula (6).
</bodyText>
<subsubsectionHeader confidence="0.4797235">
3.2.1 Topic-Specific Phrase Translation
Probability φ(e |f, tf out)
</subsubsectionHeader>
<bodyText confidence="0.9399879">
We follow the common practice (Koehn et al.,
2003) to calculate the topic-specific phrase trans-
lation probability, and the only difference is that
our method takes the topical context information in-
to account when collecting the fractional counts of
phrase pairs. With the sentence-topic distribution
the conditional probability φ(e|
P (tf out|f) from the relevant topic model of Cf out,
f, tf out) can be eas-
ily obtained by MLE method:
</bodyText>
<equation confidence="0.987295">
φ(�e|�f, tf out)
P counthf,ei( f, 6) · P (tf out|f)
hf,ei∈Cout counthf,ei(�f,e0) · P(tf out|f7
_
</equation>
<bodyText confidence="0.999492333333333">
where Cout is the out-of-domain bilingual training
corpus, and counthf,ei(�f, e) denotes the number of
the phrase pair ( f, e) in sentence pair hf, ei.
</bodyText>
<subsubsectionHeader confidence="0.827421">
3.2.2 Topic Mapping Probability P(tf out|tf in)
</subsubsectionHeader>
<bodyText confidence="0.999468428571429">
Based on the two monolingual topic models re-
spectively trained from Cf in and Cf out, we com-
pute the topic mapping probability by using source
word f as the pivot variable. Noticing that there
are some words occurring in one corpus only, we
use the words belonging to both corpora during the
mapping procedure. Specifically, we decompose
</bodyText>
<equation confidence="0.893829333333333">
P(tf out|tf in) as follows:
P(tf out|tf in)
P(tf out|f) · P(f|tf in) (8)
</equation>
<bodyText confidence="0.9882614">
Here we first get P(f|tf in) directly from the top-
ic model related to Cf in. Then, considering the
sentence-topic distribution P (tf out|f) from the rel-
evant topic model of Cf out, we define the word-
topic distribution P(tf out|f) as:
</bodyText>
<equation confidence="0.999566">
P(tf out|f)
P countf(f) · P (tf out|f)
f∈Cf out countf(f) · P (tf out|f) (9)
</equation>
<bodyText confidence="0.9999185">
where countf (f) denotes the number of the word f
in sentence f.
</bodyText>
<subsubsectionHeader confidence="0.817572">
3.2.3 Phrase-Topic Distribution P(tf in |�f)
</subsubsectionHeader>
<bodyText confidence="0.99929675">
A simple way to compute the phrase-topic distri-
bution is to take the fractional counts from Cf in
and then adopt MLE to obtain relative probability.
However, it is infeasible in our model because some
phrases occur in Cf out while being absent in Cf in.
To solve this problem, we further compute this pos-
terior distribution by the interpolation of two model-
s:
</bodyText>
<equation confidence="0.99945">
P(tf in |f) = θ · Pmle(tf in |f)+
(1 − θ) · Pword(tf in |f) (10)
</equation>
<bodyText confidence="0.999747111111111">
where Pmle(tf in |f) indicates the phrase-topic dis-
tribution by MLE, Pword(tf in |f) denotes the
phrase-topic distribution which is decomposed into
the topic posterior distribution at the word level, and
θ is the interpolation weight that can be optimized
over the development data.
Given the number of the phrase f in sentence f
denoted as countf (�f), we compute the in-domain
phrase-topic distribution in the following way:
</bodyText>
<equation confidence="0.900584714285715">
f) · P(tf in|f)
(11)
f) · P(tf in|f)
X
tf in
φ(�e|
f, tf out)
P P
�e� hf,ei∈Cout
P
=f∈Cf out h Cf in
f) (6) P P
tf out f∈Cf out
Pmle(tf in |f)
P countf (
f∈Cf in
P
tf in
countf (
P
f∈Cf in
</equation>
<page confidence="0.993392">
462
</page>
<bodyText confidence="0.999291875">
Under the assumption that the topics of all word-
s in the same phrase are independent, we consid-
er two methods to calculate Pword(tf in |f). One is
a “Noisy-OR” combination method (Zens and Ney,
2004) which has shown good performance in calcu-
lating similarities between bags-of-words in differ-
ent languages. Using this method, Pword(tf in |f) is
defined as:
</bodyText>
<equation confidence="0.997328833333333">
Pword(tf in |f)
= 1 − Pword(
rl≈ 1 − P(�tf in|fj)
f;E f�
= 1 − rl (1 − P(tf in|fj)) (12)
f;E f�
</equation>
<bodyText confidence="0.994658625">
where Pword(�tf in |�f) represents the probability that
tf in is not the topic of the phrase f. Similarly,
P (tf in|fj) indicates the probability that tf in is not
the topic of the word fj.
The other method is an “Averaging” combination
one. With the assumption that tf in is the topic of f
if at least one of the words in f belongs to this topic,
we derive Pword(tf in |f) as follows:
</bodyText>
<equation confidence="0.985714">
Pword(tf in |�f) ≈ E P(tf in|fj)/ |�f |(13)
f;E f�
</equation>
<bodyText confidence="0.999739">
where |�f |denotes the number of words in phrase �f.
</bodyText>
<subsectionHeader confidence="0.988709">
3.3 Adapted Lexical Probability Estimation
</subsectionHeader>
<bodyText confidence="0.999992">
Now we briefly describe how to estimate the adapted
lexical weight for phrase pairs, which can be adjust-
ed in a similar way to the phrase probability.
Specifically, adopting our method, each word is
considered as one phrase consisting of only one
</bodyText>
<equation confidence="0.9779722">
word, so
E
w(e|f) =
tf out
·P(tf out|tf in) · P(tf in|f) (14)
</equation>
<bodyText confidence="0.99626">
Here we obtain w(e|f, tf out) with a simi-
lar approach to 0(e |f, tf out), and calculate
P(tf out|tf in) and P(tf in|f) by resorting to
formulas (8) and (9).
With the adjusted lexical translation probability,
we resort to formula (4) to update the lexical weight
for the phrase pair ( f, �e).
</bodyText>
<sectionHeader confidence="0.99868" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.9999564">
We evaluate our method on the Chinese-to-English
translation task for the weblog text. After a brief de-
scription of the experimental setup, we investigate
the effects of various factors on the translation sys-
tem performance.
</bodyText>
<subsectionHeader confidence="0.976704">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999932405405406">
In our experiments, the out-of-domain training cor-
pus comes from the FBIS corpus and the Hansard-
s part of LDC2004T07 corpus (54.6K documents
with 1M parallel sentences, 25.2M Chinese words
and 29M English words). We use the Chinese Sohu
weblog in 20091 and the English Blog Authorship
corpus2 (Schler et al., 2006) as the in-domain mono-
lingual corpora in the source language and target
language, respectively. To obtain more accurate top-
ic information by HTMM, we firstly filter the noisy
blog documents and the ones consisting of short sen-
tences. After filtering, there are totally 85K Chinese
blog documents with 2.1M sentences and 277K En-
glish blog documents with 4.3M sentences used in
our experiments. Then, we sample equal numbers of
documents from the in-domain monolingual corpo-
ra in the source language and the target language to
respectively train two in-domain topic models. The
web part of the 2006 NIST MT evaluation test da-
ta, consisting of 27 documents with 1048 sentences,
is used as the development set, and the weblog part
of the 2008 NIST MT test data, including 33 docu-
ments with 666 sentences, is our test set.
To obtain various topic distributions for the out-
of-domain training corpus and the in-domain mono-
lingual corpora in the source language and the tar-
get language respectively, we use HTMM tool devel-
oped by Gruber et al.(2007) to conduct topic model
training. During this process, we empirically set the
same parameter values for the HTMM training of d-
ifferent corpora: topics = 50, α = 1.5, 0 = 1.01,
iters = 100. See (Gruber et al., 2007) for the
meanings of these parameters. Besides, we set the
interpolation weight 0 in formula (10) to 0.5 by ob-
serving the results on development set in the addi-
tional experiments.
We choose MOSES, a famous open-source
</bodyText>
<footnote confidence="0.997865">
1http://blog.sohu.com/
2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html
</footnote>
<equation confidence="0.5760425">
f)
tf in|
E w(e|f, tf out)
tf in
</equation>
<page confidence="0.998557">
463
</page>
<bodyText confidence="0.9995865">
phrase-based machine translation system (Koehn
et al., 2007), as the experimental decoder.
GIZA++ (Och and Ney, 2003) and the heuristics
“grow-diag-final-and” are used to generate a word-
aligned corpus, from which we extract bilingual
phrases with maximum length 7. We use SRILM
Toolkits (Stolcke, 2002) to train two 4-gram lan-
guage models on the filtered English Blog Author-
ship corpus and the Xinhua portion of Gigaword
corpus, respectively. During decoding, we set the
ttable-limit as 20, the stack-size as 100, and per-
form minimum-error-rate training (Och and Ney,
2003) to tune the feature weights for the log-linear
model. The translation quality is evaluated by
case-insensitive BLEU-4 metric (Papineni et al.,
2002). Finally, we conduct paired bootstrap sam-
pling (Koehn, 2004) to test the significance in BLEU
score differences.
</bodyText>
<subsectionHeader confidence="0.903413">
4.2 Result and Analysis
4.2.1 Effect of Different Smoothing Methods
</subsectionHeader>
<bodyText confidence="0.9998909">
Our first experiments investigate the effect of dif-
ferent smoothing methods for the in-domain phrase-
topic distribution: “Noisy-OR” and “Averaging”.
We build adapted phrase tables with these two meth-
ods, and then respectively use them in place of the
out-of-domain phrase table to test the system perfor-
mance. For the purpose of studying the generality of
our approach, we carry out comparative experiments
on two sizes of in-domain monolingual corpora: 5K
and 40K.
</bodyText>
<table confidence="0.999958">
Adaptation (Dev) MT06 (Tst) MT08
Method Web Weblog
Baseline 30.98 20.22
Noisy-OR (5K) 31.16 20.45
Averaging (5K) 31.51 20.54
Noisy-OR (40K) 31.87 20.76
Averaging (40K) 31.89 21.11
</table>
<tableCaption confidence="0.9908215">
Table 1: Experimental results using different smoothing
methods.
</tableCaption>
<bodyText confidence="0.991587115384615">
Table 1 reports the BLEU scores of the translation
system under various conditions. Using the out-of-
domain phrase table, the baseline system achieves
a BLEU score of 20.22. In the experiments with
the small-scale in-domain monolingual corpora, the
BLEU scores acquired by two methods are 20.45
and 20.54, achieving absolute improvements of 0.23
and 0.32 on the test set, respectively. In the exper-
iments with the large-scale monolingual in-domain
corpora, similar results are obtained, with absolute
improvements of 0.54 and 0.89 over the baseline
system.
From the above experimental results, we know
that both “Noisy-OR” and “Averaging” combination
methods improve the performance over the base-
line, and “Averaging” method seems to be slight-
ly better. This finding fails to echo the promis-
ing results in the previous study (Zens and Ney,
2004). This is because the “Noisy-OR” method in-
volves the multiplication of the word-topic distribu-
tion (shown in formula (12)), which leads to much
sharper phrase-topic distribution than “Averaging”
method, and is more likely to introduce bias to the
translation probability estimation. Due to this rea-
son, all the following experiments only consider the
“Averaging”method.
</bodyText>
<subsectionHeader confidence="0.841006">
4.2.2 Effect of Combining Two Phrase Tables
</subsectionHeader>
<bodyText confidence="0.99988475">
In the above experiments, we replace the out-of-
domain phrase table with the adapted phrase table.
Here we combine these two phrase tables in a log-
linear framework to see if we could obtain further
improvement. To offer a clear description, we repre-
sent the out-of-domain phrase table and the adapted
phrase table with “OutBP” and “AdapBP”, respec-
tively.
</bodyText>
<table confidence="0.980621142857143">
Used Phrase (Dev) MT06 (Tst) MT08
Table Web Weblog
Baseline 30.98 20.22
AdapBp (5K) 31.51 20.54
+ OutBp 31.84 20.70
AdapBp (40K) 31.89 21.11
+ OutBp 32.05 21.20
</table>
<tableCaption confidence="0.971959">
Table 2: Experimental results using different phrase ta-
bles. OutBp: the out-of-domain phrase table. AdapBp:
the adapted phrase table.
</tableCaption>
<bodyText confidence="0.9817825">
Table 2 shows the results of experiments using d-
ifferent phrase tables. Applying our adaptation ap-
proach, both “AdapBP” and “OutBP + AdapBP”
consistently outperform the baseline, and the lat-
</bodyText>
<page confidence="0.999595">
464
</page>
<figureCaption confidence="0.999639">
Figure 1: Effect of in-domain monolingual corpus size on
translation quality.
</figureCaption>
<bodyText confidence="0.999992153846154">
ter produces further improvements over the former.
Specifically, the BLEU scores of the “OutBP +
AdapBP” method are 20.70 and 21.20, which ob-
tain 0.48 and 0.98 points higher than the baseline
method, and 0.16 and 0.09 points higher than the
‘AdapBP” method. The underlying reason is that the
probability distribution of each in-domain sentence
often converges on some topics in the “AdapBP”
method and some translation probabilities are over-
estimated, which leads to negative effects on the
translation quality. By using two tables together, our
approach reduces the bias introduced by “AdapBP”,
therefore further improving the translation quality.
</bodyText>
<subsectionHeader confidence="0.958459">
4.2.3 Effect of In-domain Monolingual Corpus
Size
</subsectionHeader>
<bodyText confidence="0.999941448275862">
Finally, we investigate the effect of in-domain
monolingual corpus size on translation quality. In
the experiment, we try different sizes of in-domain
documents to train different monolingual topic mod-
els: from 5K to 80K with an increment of 5K each
time. Note that here we only focus on the exper-
iments using the “OutBP + AdapBP” method, be-
cause this method performs better in the previous
experiments.
Figure 1 shows the BLEU scores of the transla-
tion system on the test set. It can be seen that the
more data, the better translation quality when the
corpus size is less than 30K. The overall BLEU
scores corresponding to the range of great N val-
ues are generally higher than the ones correspond-
ing to the range of small N values. For example, the
BLEU scores under the condition within the range
[25K, 80K] are all higher than the ones within the
range [5K, 20K]. When N is set to 55K, the BLEU
score of our system is 21.40, with 1.18 gains on the
baseline system. This difference is statistically sig-
nificant at P &lt; 0.01 using the significance test tool
developed by Zhang et al.(2004). For this experi-
mental result, we speculate that with the increment
of in-domain monolingual data, the corresponding
topic models provide more accurate topic informa-
tion to improve the translation system. However,
this effect weakens when the monolingual corpora
continue to increase.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.998797514285714">
Most previous researches about translation model
adaptation focused on parallel data collection. For
example, Hildebrand et al.(2005) employed infor-
mation retrieval technology to gather the bilingual
sentences, which are similar to the test set, from
available in-domain and out-of-domain training da-
ta to build an adaptive translation model. With
the same motivation, Munteanu and Marcu (2005)
extracted in-domain bilingual sentence pairs from
comparable corpora. Since large-scale monolin-
gual corpus is easier to obtain than parallel corpus,
there have been some studies on how to generate
parallel sentences with monolingual sentences. In
this respect, Ueffing et al. (2008) explored semi-
supervised learning to obtain synthetic parallel sen-
tences, and Wu et al. (2008) used an in-domain
translation dictionary and monolingual corpora to
adapt an out-of-domain translation model for the in-
domain text.
Differing from the above-mentioned works on
the acquirement of bilingual resource, several stud-
ies (Foster and Kuhn, 2007; Civera and Juan, 2007;
Lv et al., 2007) adopted mixture modeling frame-
work to exploit the full potential of the existing par-
allel corpus. Under this framework, the training cor-
pus is first divided into different parts, each of which
is used to train a sub translation model, then these
sub models are used together with different weights
during decoding. In addition, discriminative weight-
ing methods were proposed to assign appropriate
weights to the sentences from training corpus (Mat-
soukas et al., 2009) or the phrase pairs of phrase ta-
ble (Foster et al., 2010). Final experimental result-
s show that without using any additional resources,
these approaches all improve SMT performance sig-
</bodyText>
<page confidence="0.999156">
465
</page>
<bodyText confidence="0.99983585">
nificantly.
Our method deals with translation model adap-
tation by making use of the topical context, so let
us take a look at the recent research developmen-
t on the application of topic models in SMT. As-
suming each bilingual sentence constitutes a mix-
ture of hidden topics and each word pair follows a
topic-specific bilingual translation model, Zhao and
Xing (2006,2007) presented a bilingual topical ad-
mixture formalism to improve word alignment by
capturing topic sharing at different levels of linguis-
tic granularity. Tam et al.(2007) proposed a bilin-
gual LSA, which enforces one-to-one topic corre-
spondence and enables latent topic distributions to
be efficiently transferred across languages, to cross-
lingual language modeling and translation lexicon
adaptation. Recently, Gong and Zhou (2010) also
applied topic modeling into domain adaptation in
SMT. Their method employed one additional feature
function to capture the topic inherent in the source
phrase and help the decoder dynamically choose re-
lated target phrases according to the specific topic of
the source phrase.
Besides, our approach is also related to context-
dependent translation. Recent studies have shown
that SMT systems can benefit from the utiliza-
tion of context information. For example, trigger-
based lexicon model (Hasan et al., 2008; Mauser et
al., 2009) and context-dependent translation selec-
tion (Chan et al., 2007; Carpuat and Wu, 2007; He
et al., 2008; Liu et al., 2008). The former gener-
ated triplets to capture long-distance dependencies
that go beyond the local context of phrases, and the
latter built the classifiers which combine rich con-
text information to better select translation during
decoding. With the consideration of various local
context features, these approaches all yielded stable
improvements on different translation tasks.
As compared to the above-mentioned works, our
work has the following differences.
</bodyText>
<listItem confidence="0.989993142857143">
• We focus on how to adapt a translation mod-
el for domain-specific translation task with the
help of additional in-domain monolingual cor-
pora, which are far from full exploitation in the
parallel data collection and mixture modeling
framework.
• In addition to the utilization of in-domain
</listItem>
<bodyText confidence="0.999912944444444">
monolingual corpora, our method is differen-
t from the previous works (Zhao and Xing,
2006; Zhao and Xing, 2007; Tam et al., 2007;
Gong and Zhou, 2010) in the following aspect-
s: (1) we use a different topic model — HTMM
which has different assumption from PLSA and
LDA; (2) rather than modeling topic-dependent
translation lexicons in the training process, we
estimate topic-specific lexical probability by
taking account of topical context when extract-
ing word pairs, so our method can also be di-
rectly applied to topic-dependent phrase proba-
bility modeling. (3) Instead of rescoring phrase
pairs online, our approach calculate the transla-
tion probabilities offline, which brings no addi-
tional burden to translation systems and is suit-
able to translate the texts without the topic dis-
tribution information.
</bodyText>
<listItem confidence="0.7525275">
• Different from trigger-based lexicon model and
context-dependent translation selection both of
</listItem>
<bodyText confidence="0.981264692307692">
which put emphasis on solving the translation
ambiguity by the exploitation of the context in-
formation at the sentence level, we adopt the
topical context information in our method for
the following reasons: (1) the topic informa-
tion captures the context information beyond
the scope of sentence; (2) the topical context in-
formation is integrated into the posterior prob-
ability distribution, avoiding the sparseness of
word or POS features; (3) the topical context
information allows for more fine-grained dis-
tinction of different translations than the genre
information of corpus.
</bodyText>
<sectionHeader confidence="0.990338" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.9995565">
This paper presents a novel method for SMT sys-
tem adaptation by making use of the monolingual
corpora in new domains. Our approach first esti-
mates the translation probabilities from the out-of-
domain bilingual corpus given the topic information,
and then rescores the phrase pairs via topic mapping
and phrase-topic distribution probability estimation
from in-domain monolingual corpora. Experimental
results show that our method achieves better perfor-
mance than the baseline system, without increasing
the burden of the translation system.
In the future, we will verify our method on oth-
</bodyText>
<page confidence="0.998759">
466
</page>
<bodyText confidence="0.999959625">
er language pairs, for example, Chinese to Japanese.
Furthermore, since the in-domain phrase-topic dis-
tribution is currently estimated with simple smooth-
ing interpolations, we expect that the translation sys-
tem could benefit from other sophisticated smooth-
ing methods. Finally, the reasonable estimation of
topic number for better translation model adaptation
will also become our study emphasis.
</bodyText>
<sectionHeader confidence="0.952182" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.996532125">
The authors were supported by 863 State Key
Project (Grant No. 2011AA01A207), National
Natural Science Foundation of China (Grant Nos.
61005052 and 61103101), Key Technologies R&amp;D
Program of China (Grant No. 2012BAH14F03). We
thank the anonymous reviewers for their insightful
comments. We are also grateful to Ruiyu Fang and
Jinming Hu for their kind help in data processing.
</bodyText>
<sectionHeader confidence="0.998075" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999630329268292">
Michiel Bacchiani and Brian Roark. 2003. Unsuper-
vised Language Model Adaptation. In Proc. of ICAS-
SP 2003, pages 224-227.
Michiel Bacchiani and Brian Roark. 2005. Improving
Machine Translation Performance by Exploiting Non-
Parallel Corpora. Computational Linguistics, pages
477-504.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation with
Monolingual Resources. In Proc. of ACL Workshop
2009, pages 182-189.
David M. Blei. 2003. Latent Dirichlet Allocation. Jour-
nal of Machine Learning, pages 993-1022.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen and John Makhoul. 2007. Language Model
Adaptation in Machine Translation from Speech. In
Proc. of ICASSP 2007, pages 117-120.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. In Proc. of EMNLP 2007, pages 61-72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2006.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33-40.
Boxing Chen, George Foster and Roland Kuhn. 2010.
Bilingual Sense Similarity for Statistical Machine
Translation. In Proc. of ACL 2010, pages 834-843.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, pages 201-228.
David Chiang. 2010. Learning to Translate with Source
and Target Syntax. In Proc. ofACL 2010, pages 1443-
1452.
Jorge Civera and Alfons Juan. 2007. Domain Adaptation
in Statistical Machine Translation with Mixture Mod-
elling. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 177-180.
Matthias Eck, Stephan Vogel and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
Translation Based on Information Retrieval. In Proc.
of Fourth International Conference on Language Re-
sources and Evaluation, pages 327-330.
Matthias Eck, Stephan Vogel and Alex Waibel. 2005.
Low Cost Portability for Statistical Machine Transla-
tion Based on N-gram Coverage. In Proc. of MT Sum-
mit 2005, pages 227-234.
George Foster and Roland Kuhn. 2007. Mixture Model
Adaptation for SMT. In Proc. of the Second Workshop
on Statistical Machine Translation, pages 128-135.
George Foster, Cyril Goutte and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proc. of
EMNLP 2010, pages 451-459.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proc. of ACL
2006, pages 961-968.
Zhengxian Gong and Guodong Zhou. 2010. Improve
SMT with Source-side Topic-Document Distributions.
In Proc. of MT SUMMIT 2010, pages 24-28.
Amit Gruber, Michal Rosen-Zvi and Yair Weiss. 2007.
Hidden Topic Markov Models. In Journal of Machine
Learning Research, pages 163-170.
Saga Hasan, Juri Ganitkevitch, Hermann Ney and Jesus
Andres-Ferrer 2008. Triplet Lexicon Models for S-
tatistical Machine Translation. In Proc. of EMNLP
2008, pages 372-381.
Zhongjun He, Qun Liu and Shouxun Lin. 2008. Improv-
ing Statistical Machine Translation using Lexicalized
Rule Selection. In Proc. of COLING 2008, pages 321-
328.
Almut Silja Hildebrand. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation based
on Information Retrieval. In Proc. of EAMT 2005,
pages 133-142.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proc. of SIGIR 1999, pages 50-57.
Franz Joseph Och and Hermann Ney. 2003. A Systemat-
ic Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, pages 19-51.
Franz Joseph Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, pages 417-449.
</reference>
<page confidence="0.991427">
467
</page>
<reference confidence="0.999882976190477">
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388-395.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177-180.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. of ACL 2006, pages 609-616.
Yajuan Lv, Jin Huang and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of EMNLP 2007, pages 343-350.
Arne Mauser, Richard Zens and Evgeny Matusov, Saga
Hasan and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of International Workshop on
Spoken Language Translation, pages 103-110.
Arne Mauser, Saga Hasan and Hermann Ney 2009. Ex-
tending Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Proc. of
ACL 2009, pages 210-218.
Spyros Matsoukas, Antti-Veikko I. Rosti and Bing Zhang
2009. Discriminative Corpus Weight Estimation for
Machine Translation. In Proc. of EMNLP 2009, pages
708-717.
Nick Ruiz and Marcello Federico. 2011. Topic Adapta-
tion for Lecture Translation through Bilingual Latent
Semantic Models. In Proc. of ACL Workshop 2011,
pages 294-302.
Kishore Papineni, Salim Roukos, Todd Ward and WeiJing
Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation. In Proc. of ACL 2002,
pages 311-318.
Jonathan Schler, Moshe Koppel, Shlomo Argamon and
James Pennebaker. 2006. Effects of Age and Gender
on Blogging. In Proc. of 2006 AAAI Spring Sympo-
sium on Computational Approaches for Analyzing We-
blogs.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/french News Transla-
tion System by Lightly-supervised Training. In Proc.
of MT Summit XII.
Andreas Stolcke. 2002. Srilm - An Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002, pages 901-
904.
Yik-Cheung Tam, Ian R. Lane and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, pages 187-207.
Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.
2008. Semi-supervised Model Adaptation for Statisti-
cal Machine Translation. Machine Translation, pages
77-94.
Hua Wu, Haifeng Wang and Chengqing Zong. 2008. Do-
main Adaptation for Statistical Machine Translation
with Domain Dictionary and Monolingual Corpora. In
Proc. of COLING 2008, pages 993-1000.
Richard Zens and Hermann Ney. 2004. Improvments in
phrase-based statistical machine translation. In Proc.
of NAACL 2004, pages 257-264.
Ying Zhang, Almut Silja Hildebrand and Stephan Vogel.
2006. Distributed Language Modeling for N-best List
Re-ranking. In Proc. of EMNLP 2006, pages 216-223.
Bing Zhao, Matthias Eck and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING 2004, pages 411-417.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
Topic AdMixture Models for Word Alignment. In
Proc. of ACL/COLING 2006, pages 969-976.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual Topic Exploration, Word Alignment, and Trans-
lation. In Proc. of NIPS 2007, pages 1-8.
Qun Liu, Zhongjun He, Yang Liu and Shouxun Lin.
2008. Maximum Entropy based Rule Selection Model
for Syntax-based Statistical Machine Translation. In
Proc. of EMNLP 2008, pages 89-97.
</reference>
<page confidence="0.999125">
468
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.201400">
<title confidence="0.951079">Translation Model Adaptation for Statistical Machine Translation Topic</title>
<author confidence="0.967274">Hua Haifeng Yidong Xiaodong</author>
<affiliation confidence="0.9650735">University, Xiamen, of Computing Technology, Chinese Academy of Sciences, Beijing,</affiliation>
<address confidence="0.718242">Inc., Beijing, ydchen, mandel,</address>
<email confidence="0.7159255">hua,liuqun@ict.ac.cn</email>
<abstract confidence="0.999413857142857">To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Brian Roark</author>
</authors>
<title>Unsupervised Language Model Adaptation.</title>
<date>2003</date>
<booktitle>In Proc. of ICASSP</booktitle>
<pages>224--227</pages>
<marker>Bacchiani, Roark, 2003</marker>
<rawString>Michiel Bacchiani and Brian Roark. 2003. Unsupervised Language Model Adaptation. In Proc. of ICASSP 2003, pages 224-227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Brian Roark</author>
</authors>
<title>Improving Machine Translation Performance by Exploiting NonParallel Corpora. Computational Linguistics,</title>
<date>2005</date>
<pages>477--504</pages>
<marker>Bacchiani, Roark, 2005</marker>
<rawString>Michiel Bacchiani and Brian Roark. 2005. Improving Machine Translation Performance by Exploiting NonParallel Corpora. Computational Linguistics, pages 477-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain Adaptation for Statistical Machine Translation with Monolingual Resources.</title>
<date>2009</date>
<booktitle>In Proc. of ACL Workshop</booktitle>
<pages>182--189</pages>
<contexts>
<context position="3006" citStr="Bertoldi and Federico, 2009" startWordPosition="430" endWordPosition="433"> weblog. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain Adaptation for Statistical Machine Translation with Monolingual Resources. In Proc. of ACL Workshop 2009, pages 182-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning,</journal>
<pages>993--1022</pages>
<contexts>
<context position="8398" citStr="Blei, 2003" startWordPosition="1296" endWordPosition="1297">for text translation in different domains, resulting in a significant drop in translation quality. φ(˜e1 E ˜e1 ˜f, ˜e) (1) ˜f, ˜e&apos;) pw(˜e�˜f, ˜a) = � �˜e� i=1 460 3 Translation Model Adaptation via Monolingual Topic Information In this section, we first briefly review the principle of Hidden Topic Markov Model(HTMM) which is the basis of our method, then describe our approach to translation model adaptation in detail. 3.1 Hidden Topic Markov Model During the last couple of years, topic models such as Probabilistic Latent Semantic Analysis (Hofmann, 1999) and Latent Dirichlet Allocation model (Blei, 2003), have drawn more and more attention and been applied successfully in NLP community. Based on the “bag-of-words” assumption that the order of words can be ignored, these methods model the text corpus by using a co-occurrence matrix of words and documents, and build generative models to infer the latent aspects or topics. Using these models, the words can be clustered into the derived topics with a probability distribution, and the correlation between words can be automatically captured via topics. However, the “bag-of-words” assumption is an unrealistic oversimplification because it ignores th</context>
</contexts>
<marker>Blei, 2003</marker>
<rawString>David M. Blei. 2003. Latent Dirichlet Allocation. Journal of Machine Learning, pages 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Bulyko</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Long Nguyen</author>
<author>John Makhoul</author>
</authors>
<title>Language Model Adaptation in Machine Translation from Speech.</title>
<date>2007</date>
<booktitle>In Proc. of ICASSP</booktitle>
<pages>117--120</pages>
<marker>Bulyko, Matsoukas, Schwartz, Nguyen, Makhoul, 2007</marker>
<rawString>Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long Nguyen and John Makhoul. 2007. Language Model Adaptation in Machine Translation from Speech. In Proc. of ICASSP 2007, pages 117-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving Statistical Machine Translation Using Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>61--72</pages>
<contexts>
<context position="28108" citStr="Carpuat and Wu, 2007" startWordPosition="4548" endWordPosition="4551">) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of addit</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving Statistical Machine Translation Using Word Sense Disambiguation. In Proc. of EMNLP 2007, pages 61-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>33--40</pages>
<marker>Chan, Ng, Chiang, 2006</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2006. Word sense disambiguation improves statistical machine translation. In Proc. of ACL 2007, pages 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Bilingual Sense Similarity for Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>834--843</pages>
<marker>Chen, Foster, Kuhn, 2010</marker>
<rawString>Boxing Chen, George Foster and Roland Kuhn. 2010. Bilingual Sense Similarity for Statistical Machine Translation. In Proc. of ACL 2010, pages 834-843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-Based Translation. Computational Linguistics,</title>
<date>2007</date>
<pages>201--228</pages>
<contexts>
<context position="1557" citStr="Chiang, 2007" startWordPosition="216" endWordPosition="217">s the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely *Part of this work was done during the first author’s internship at Baidu. approximate the empirical distributions of the training data, which typically consist of bilingual sentences and monolingual target language sentences. When the translated texts and the training data come from the same domain, SMT systems can achieve good performance, oth</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, pages 201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to Translate with Source and Target Syntax.</title>
<date>2010</date>
<booktitle>In Proc. ofACL 2010,</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="1572" citStr="Chiang, 2010" startWordPosition="218" endWordPosition="219">ship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely *Part of this work was done during the first author’s internship at Baidu. approximate the empirical distributions of the training data, which typically consist of bilingual sentences and monolingual target language sentences. When the translated texts and the training data come from the same domain, SMT systems can achieve good performance, otherwise the tran</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to Translate with Source and Target Syntax. In Proc. ofACL 2010, pages 1443-1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Civera</author>
<author>Alfons Juan</author>
</authors>
<title>Domain Adaptation in Statistical Machine Translation with Mixture Modelling.</title>
<date>2007</date>
<booktitle>In Proc. of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="3176" citStr="Civera and Juan, 2007" startWordPosition="457" endWordPosition="460">dapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics monolingual corpo</context>
<context position="26015" citStr="Civera and Juan, 2007" startWordPosition="4218" endWordPosition="4221">ntence pairs from comparable corpora. Since large-scale monolingual corpus is easier to obtain than parallel corpus, there have been some studies on how to generate parallel sentences with monolingual sentences. In this respect, Ueffing et al. (2008) explored semisupervised learning to obtain synthetic parallel sentences, and Wu et al. (2008) used an in-domain translation dictionary and monolingual corpora to adapt an out-of-domain translation model for the indomain text. Differing from the above-mentioned works on the acquirement of bilingual resource, several studies (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) adopted mixture modeling framework to exploit the full potential of the existing parallel corpus. Under this framework, the training corpus is first divided into different parts, each of which is used to train a sub translation model, then these sub models are used together with different weights during decoding. In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al., 2009) or the phrase pairs of phrase table (Foster et al., 2010). Final experimental results show that without using any</context>
</contexts>
<marker>Civera, Juan, 2007</marker>
<rawString>Jorge Civera and Alfons Juan. 2007. Domain Adaptation in Statistical Machine Translation with Mixture Modelling. In Proc. of the Second Workshop on Statistical Machine Translation, pages 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Language Model Adaptation for Statistical Machine Translation Based on Information Retrieval.</title>
<date>2004</date>
<booktitle>In Proc. of Fourth International Conference on Language Resources and Evaluation,</booktitle>
<pages>327--330</pages>
<marker>Eck, Vogel, Waibel, 2004</marker>
<rawString>Matthias Eck, Stephan Vogel and Alex Waibel. 2004. Language Model Adaptation for Statistical Machine Translation Based on Information Retrieval. In Proc. of Fourth International Conference on Language Resources and Evaluation, pages 327-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Low Cost Portability for Statistical Machine Translation Based on N-gram Coverage.</title>
<date>2005</date>
<booktitle>In Proc. of MT</booktitle>
<pages>227--234</pages>
<location>Summit</location>
<marker>Eck, Vogel, Waibel, 2005</marker>
<rawString>Matthias Eck, Stephan Vogel and Alex Waibel. 2005. Low Cost Portability for Statistical Machine Translation Based on N-gram Coverage. In Proc. of MT Summit 2005, pages 227-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixture Model Adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proc. of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="3153" citStr="Foster and Kuhn, 2007" startWordPosition="453" endWordPosition="456">re we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Lingui</context>
<context position="25992" citStr="Foster and Kuhn, 2007" startWordPosition="4214" endWordPosition="4217"> in-domain bilingual sentence pairs from comparable corpora. Since large-scale monolingual corpus is easier to obtain than parallel corpus, there have been some studies on how to generate parallel sentences with monolingual sentences. In this respect, Ueffing et al. (2008) explored semisupervised learning to obtain synthetic parallel sentences, and Wu et al. (2008) used an in-domain translation dictionary and monolingual corpora to adapt an out-of-domain translation model for the indomain text. Differing from the above-mentioned works on the acquirement of bilingual resource, several studies (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) adopted mixture modeling framework to exploit the full potential of the existing parallel corpus. Under this framework, the training corpus is first divided into different parts, each of which is used to train a sub translation model, then these sub models are used together with different weights during decoding. In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al., 2009) or the phrase pairs of phrase table (Foster et al., 2010). Final experimental results show</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixture Model Adaptation for SMT. In Proc. of the Second Workshop on Statistical Machine Translation, pages 128-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>451--459</pages>
<contexts>
<context position="26559" citStr="Foster et al., 2010" startWordPosition="4309" endWordPosition="4312">gual resource, several studies (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) adopted mixture modeling framework to exploit the full potential of the existing parallel corpus. Under this framework, the training corpus is first divided into different parts, each of which is used to train a sub translation model, then these sub models are used together with different weights during decoding. In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al., 2009) or the phrase pairs of phrase table (Foster et al., 2010). Final experimental results show that without using any additional resources, these approaches all improve SMT performance sig465 nificantly. Our method deals with translation model adaptation by making use of the topical context, so let us take a look at the recent research development on the application of topic models in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sh</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation. In Proc. of EMNLP 2010, pages 451-459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of ContextRich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1525" citStr="Galley et al., 2006" startWordPosition="208" endWordPosition="211">ility estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely *Part of this work was done during the first author’s internship at Baidu. approximate the empirical distributions of the training data, which typically consist of bilingual sentences and monolingual target language sentences. When the translated texts and the training data come from the same domain, SMT systems c</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang and Ignacio Thayer. 2006. Scalable Inference and Training of ContextRich Syntactic Translation Models. In Proc. of ACL 2006, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Guodong Zhou</author>
</authors>
<title>Improve SMT with Source-side Topic-Document Distributions.</title>
<date>2010</date>
<booktitle>In Proc. of MT SUMMIT</booktitle>
<pages>24--28</pages>
<contexts>
<context position="3907" citStr="Gong and Zhou, 2010" startWordPosition="566" endWordPosition="569"> exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ang”, and occurs in the sentences related to the geography topic when translated to “h´e`an”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if “bank” occurs more ofte</context>
<context position="27489" citStr="Gong and Zhou (2010)" startWordPosition="4451" endWordPosition="4454"> topic models in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C</context>
<context position="29042" citStr="Gong and Zhou, 2010" startWordPosition="4695" endWordPosition="4698">ontext features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora, which are far from full exploitation in the parallel data collection and mixture modeling framework. • In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model — HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling. (3) Instead of rescoring phrase pairs online, our approach calculate the translation probabilities offline, which brings no additional burden to translation systems and is suitable to transla</context>
</contexts>
<marker>Gong, Zhou, 2010</marker>
<rawString>Zhengxian Gong and Guodong Zhou. 2010. Improve SMT with Source-side Topic-Document Distributions. In Proc. of MT SUMMIT 2010, pages 24-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden Topic Markov Models.</title>
<date>2007</date>
<journal>In Journal of Machine Learning Research,</journal>
<pages>163--170</pages>
<contexts>
<context position="18760" citStr="Gruber et al., 2007" startWordPosition="3083" endWordPosition="3086">ng of 27 documents with 1048 sentences, is used as the development set, and the weblog part of the 2008 NIST MT test data, including 33 documents with 666 sentences, is our test set. To obtain various topic distributions for the outof-domain training corpus and the in-domain monolingual corpora in the source language and the target language respectively, we use HTMM tool developed by Gruber et al.(2007) to conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, 0 = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight 0 in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1http://blog.sohu.com/ 2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html f) tf in| E w(e|f, tf out) tf in 463 phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRI</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michal Rosen-Zvi and Yair Weiss. 2007. Hidden Topic Markov Models. In Journal of Machine Learning Research, pages 163-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saga Hasan</author>
<author>Juri Ganitkevitch</author>
<author>Hermann Ney</author>
<author>Jesus Andres-Ferrer</author>
</authors>
<title>Triplet Lexicon Models for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>372--381</pages>
<contexts>
<context position="28001" citStr="Hasan et al., 2008" startWordPosition="4531" endWordPosition="4534">ages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences.</context>
</contexts>
<marker>Hasan, Ganitkevitch, Ney, Andres-Ferrer, 2008</marker>
<rawString>Saga Hasan, Juri Ganitkevitch, Hermann Ney and Jesus Andres-Ferrer 2008. Triplet Lexicon Models for Statistical Machine Translation. In Proc. of EMNLP 2008, pages 372-381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving Statistical Machine Translation using Lexicalized Rule Selection.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>321--328</pages>
<contexts>
<context position="28125" citStr="He et al., 2008" startWordPosition="4552" endWordPosition="4555">odeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain m</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu and Shouxun Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule Selection. In Proc. of COLING 2008, pages 321-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
</authors>
<title>Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval.</title>
<date>2005</date>
<booktitle>In Proc. of EAMT</booktitle>
<pages>133--142</pages>
<marker>Hildebrand, 2005</marker>
<rawString>Almut Silja Hildebrand. 2005. Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval. In Proc. of EAMT 2005, pages 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proc. of SIGIR</booktitle>
<pages>50--57</pages>
<contexts>
<context position="8347" citStr="Hofmann, 1999" startWordPosition="1287" endWordPosition="1289">odel estimated from the training data is not suitable for text translation in different domains, resulting in a significant drop in translation quality. φ(˜e1 E ˜e1 ˜f, ˜e) (1) ˜f, ˜e&apos;) pw(˜e�˜f, ˜a) = � �˜e� i=1 460 3 Translation Model Adaptation via Monolingual Topic Information In this section, we first briefly review the principle of Hidden Topic Markov Model(HTMM) which is the basis of our method, then describe our approach to translation model adaptation in detail. 3.1 Hidden Topic Markov Model During the last couple of years, topic models such as Probabilistic Latent Semantic Analysis (Hofmann, 1999) and Latent Dirichlet Allocation model (Blei, 2003), have drawn more and more attention and been applied successfully in NLP community. Based on the “bag-of-words” assumption that the order of words can be ignored, these methods model the text corpus by using a co-occurrence matrix of words and documents, and build generative models to infer the latent aspects or topics. Using these models, the words can be clustered into the derived topics with a probability distribution, and the correlation between words can be automatically captured via topics. However, the “bag-of-words” assumption is an u</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proc. of SIGIR 1999, pages 50-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics,</title>
<date>2003</date>
<pages>pages</pages>
<contexts>
<context position="19202" citStr="Och and Ney, 2003" startWordPosition="3150" endWordPosition="3153">ng this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, 0 = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight 0 in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1http://blog.sohu.com/ 2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html f) tf in| E w(e|f, tf out) tf in 463 phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, pages 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics,</title>
<date>2004</date>
<pages>417--449</pages>
<contexts>
<context position="1504" citStr="Och and Ney, 2004" startWordPosition="204" endWordPosition="207"> translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely *Part of this work was done during the first author’s internship at Baidu. approximate the empirical distributions of the training data, which typically consist of bilingual sentences and monolingual target language sentences. When the translated texts and the training data come from the same </context>
<context position="6678" citStr="Och and Ney, 2004" startWordPosition="1017" endWordPosition="1020">on the performance of SMT system. Phrase probability measures the co-occurrence frequency of a phrase pair, and lexical probability is used to validate the quality of the phrase pair by checking how well its words are translated to each other. According to the definition proposed by (Koehn et al., 2003), given a source sentence f = fJ1 = f1, ... , fj, ... , fJ, a target sentence e = eI1 = e1, ... , ei, ... , eI, and its word alignment a which is a subset of the Cartesian product of word positions: a C_ (j, i) : j = 1, ... , J; i = 1, ... , I, the phrase pair (˜f, ˜e) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word inside one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. After all consistent phrase pairs are extracted from training corpus, the phrase probabilities are estimated as relative frequencies (Och and Ney, 2004): ˜f) = count( count( Here count(˜f, ˜e) indicates how often the phrase pair ( ˜f, ˜e) occurs in the training corpus. To obtain the corresponding lexical weight, we first estimate a lexical translation probability distribution w(e</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, pages 417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLTNAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1485" citStr="Koehn et al., 2003" startWordPosition="200" endWordPosition="203">pic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely *Part of this work was done during the first author’s internship at Baidu. approximate the empirical distributions of the training data, which typically consist of bilingual sentences and monolingual target language sentences. When the translated texts and the training data </context>
<context position="6364" citStr="Koehn et al., 2003" startWordPosition="940" endWordPosition="943">mmarizes the recent related work about translation model adaptation. Finally, we end with a conclusion and the future work in Section 6. 2 Background The statistical translation model, which contains phrase pairs with bi-directional phrase probabilities and bi-directional lexical probabilities, has a great effect on the performance of SMT system. Phrase probability measures the co-occurrence frequency of a phrase pair, and lexical probability is used to validate the quality of the phrase pair by checking how well its words are translated to each other. According to the definition proposed by (Koehn et al., 2003), given a source sentence f = fJ1 = f1, ... , fj, ... , fJ, a target sentence e = eI1 = e1, ... , ei, ... , eI, and its word alignment a which is a subset of the Cartesian product of word positions: a C_ (j, i) : j = 1, ... , J; i = 1, ... , I, the phrase pair (˜f, ˜e) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word inside one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. After all consistent phrase pairs are extracted from training corpus</context>
<context position="12782" citStr="Koehn et al., 2003" startWordPosition="2019" endWordPosition="2022">ut|tf in) to map the in-domain phrase-topic distribution into the one in the out-domain topic space. To be specific, we obtain the out-of-domain phrase-topic distribution P(tf out |f) as follows: P(tf out |�f) = X P(tf out|tf in) · P(tf in |f) (5) tf in Thus formula (4) can be further refined as the following formula: φ(6 |�f) = X tf out ·P(tf out|tf in) · P(tf in| Next we will give detailed descriptions of the calculation methods for the three probability distributions mentioned in formula (6). 3.2.1 Topic-Specific Phrase Translation Probability φ(e |f, tf out) We follow the common practice (Koehn et al., 2003) to calculate the topic-specific phrase translation probability, and the only difference is that our method takes the topical context information into account when collecting the fractional counts of phrase pairs. With the sentence-topic distribution the conditional probability φ(e| P (tf out|f) from the relevant topic model of Cf out, f, tf out) can be easily obtained by MLE method: φ(�e|�f, tf out) P counthf,ei( f, 6) · P (tf out|f) hf,ei∈Cout counthf,ei(�f,e0) · P(tf out|f7 _ where Cout is the out-of-domain bilingual training corpus, and counthf,ei(�f, e) denotes the number of the phrase pa</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLTNAACL 2003, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="19868" citStr="Koehn, 2004" startWordPosition="3255" endWordPosition="3256">generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes of in-domain monolingual corpora: 5K and 40K. Adaptation (De</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proc. of EMNLP 2004, pages 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL 2007, Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="19145" citStr="Koehn et al., 2007" startWordPosition="3141" endWordPosition="3144"> Gruber et al.(2007) to conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, 0 = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight 0 in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1http://blog.sohu.com/ 2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html f) tf in| E w(e|f, tf out) tf in 463 phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evalu</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL 2007, Demonstration Session, pages 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-String Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1543" citStr="Liu et al., 2006" startWordPosition="212" endWordPosition="215"> method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely *Part of this work was done during the first author’s internship at Baidu. approximate the empirical distributions of the training data, which typically consist of bilingual sentences and monolingual target language sentences. When the translated texts and the training data come from the same domain, SMT systems can achieve good pe</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Treeto-String Alignment Template for Statistical Machine Translation. In Proc. of ACL 2006, pages 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lv</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving Statistical Machine Translation Performance by Training Data Selection and Optimization.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>343--350</pages>
<contexts>
<context position="3194" citStr="Lv et al., 2007" startWordPosition="461" endWordPosition="464">l, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics monolingual corpora. Our approach i</context>
<context position="26033" citStr="Lv et al., 2007" startWordPosition="4222" endWordPosition="4225">rable corpora. Since large-scale monolingual corpus is easier to obtain than parallel corpus, there have been some studies on how to generate parallel sentences with monolingual sentences. In this respect, Ueffing et al. (2008) explored semisupervised learning to obtain synthetic parallel sentences, and Wu et al. (2008) used an in-domain translation dictionary and monolingual corpora to adapt an out-of-domain translation model for the indomain text. Differing from the above-mentioned works on the acquirement of bilingual resource, several studies (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) adopted mixture modeling framework to exploit the full potential of the existing parallel corpus. Under this framework, the training corpus is first divided into different parts, each of which is used to train a sub translation model, then these sub models are used together with different weights during decoding. In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al., 2009) or the phrase pairs of phrase table (Foster et al., 2010). Final experimental results show that without using any additional resour</context>
</contexts>
<marker>Lv, Huang, Liu, 2007</marker>
<rawString>Yajuan Lv, Jin Huang and Qun Liu. 2007. Improving Statistical Machine Translation Performance by Training Data Selection and Optimization. In Proc. of EMNLP 2007, pages 343-350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Richard Zens</author>
<author>Evgeny Matusov</author>
<author>Saga Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Evaluation.</title>
<date>2006</date>
<booktitle>The RWTH Statistical Machine Translation System for the IWSLT</booktitle>
<pages>103--110</pages>
<marker>Mauser, Zens, Matusov, Hasan, Ney, 2006</marker>
<rawString>Arne Mauser, Richard Zens and Evgeny Matusov, Saga Hasan and Hermann Ney. 2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation. In Proc. of International Workshop on Spoken Language Translation, pages 103-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saga Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models.</title>
<date>2009</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>210--218</pages>
<contexts>
<context position="28023" citStr="Mauser et al., 2009" startWordPosition="4535" endWordPosition="4538">l language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to </context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saga Hasan and Hermann Ney 2009. Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models. In Proc. of ACL 2009, pages 210-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
</authors>
<title>Antti-Veikko I. Rosti and Bing Zhang 2009. Discriminative Corpus Weight Estimation for Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>708--717</pages>
<marker>Matsoukas, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti and Bing Zhang 2009. Discriminative Corpus Weight Estimation for Machine Translation. In Proc. of EMNLP 2009, pages 708-717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models.</title>
<date>2011</date>
<booktitle>In Proc. of ACL Workshop</booktitle>
<pages>294--302</pages>
<contexts>
<context position="3933" citStr="Ruiz and Federico, 2011" startWordPosition="570" endWordPosition="573">gnoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ang”, and occurs in the sentences related to the geography topic when translated to “h´e`an”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if “bank” occurs more often in the sentences related</context>
</contexts>
<marker>Ruiz, Federico, 2011</marker>
<rawString>Nick Ruiz and Marcello Federico. 2011. Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models. In Proc. of ACL Workshop 2011, pages 294-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19807" citStr="Papineni et al., 2002" startWordPosition="3244" endWordPosition="3247">ch and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proc. of ACL 2002, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Schler</author>
<author>Moshe Koppel</author>
<author>Shlomo Argamon</author>
<author>James Pennebaker</author>
</authors>
<title>Effects of Age and Gender on Blogging.</title>
<date>2006</date>
<booktitle>In Proc. of</booktitle>
<contexts>
<context position="17505" citStr="Schler et al., 2006" startWordPosition="2868" endWordPosition="2871">lexical weight for the phrase pair ( f, �e). 4 Experiment We evaluate our method on the Chinese-to-English translation task for the weblog text. After a brief description of the experimental setup, we investigate the effects of various factors on the translation system performance. 4.1 Experimental setup In our experiments, the out-of-domain training corpus comes from the FBIS corpus and the Hansards part of LDC2004T07 corpus (54.6K documents with 1M parallel sentences, 25.2M Chinese words and 29M English words). We use the Chinese Sohu weblog in 20091 and the English Blog Authorship corpus2 (Schler et al., 2006) as the in-domain monolingual corpora in the source language and target language, respectively. To obtain more accurate topic information by HTMM, we firstly filter the noisy blog documents and the ones consisting of short sentences. After filtering, there are totally 85K Chinese blog documents with 2.1M sentences and 277K English blog documents with 4.3M sentences used in our experiments. Then, we sample equal numbers of documents from the in-domain monolingual corpora in the source language and the target language to respectively train two in-domain topic models. The web part of the 2006 NIS</context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>Jonathan Schler, Moshe Koppel, Shlomo Argamon and James Pennebaker. 2006. Effects of Age and Gender on Blogging. In Proc. of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean Senellart</author>
</authors>
<title>Translation Model Adaptation for an Arabic/french News Translation System by Lightly-supervised Training.</title>
<date>2009</date>
<booktitle>In Proc. of MT Summit XII.</booktitle>
<contexts>
<context position="3036" citStr="Schwenk and Senellart, 2009" startWordPosition="434" endWordPosition="437">ion emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computati</context>
</contexts>
<marker>Schwenk, Senellart, 2009</marker>
<rawString>Holger Schwenk and Jean Senellart. 2009. Translation Model Adaptation for an Arabic/french News Translation System by Lightly-supervised Training. In Proc. of MT Summit XII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP</booktitle>
<pages>901--904</pages>
<contexts>
<context position="19387" citStr="Stolcke, 2002" startWordPosition="3180" endWordPosition="3181">ings of these parameters. Besides, we set the interpolation weight 0 in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1http://blog.sohu.com/ 2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html f) tf in| E w(e|f, tf out) tf in 463 phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Method</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - An Extensible Language Modeling Toolkit. In Proc. of ICSLP 2002, pages 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian R Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual LSA-based adaptation for statistical machine translation. Machine Translation,</title>
<date>2007</date>
<pages>187--207</pages>
<contexts>
<context position="3886" citStr="Tam et al., 2007" startWordPosition="562" endWordPosition="565">rpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ang”, and occurs in the sentences related to the geography topic when translated to “h´e`an”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if “b</context>
<context position="29020" citStr="Tam et al., 2007" startWordPosition="4691" endWordPosition="4694">of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora, which are far from full exploitation in the parallel data collection and mixture modeling framework. • In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model — HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling. (3) Instead of rescoring phrase pairs online, our approach calculate the translation probabilities offline, which brings no additional burden to translation systems and </context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian R. Lane and Tanja Schultz. 2007. Bilingual LSA-based adaptation for statistical machine translation. Machine Translation, pages 187-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Semi-supervised Model Adaptation for Statistical Machine Translation. Machine Translation,</title>
<date>2008</date>
<pages>77--94</pages>
<contexts>
<context position="2960" citStr="Ueffing et al., 2008" startWordPosition="422" endWordPosition="425"> another, for example, from newswire to weblog. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain</context>
<context position="25644" citStr="Ueffing et al. (2008)" startWordPosition="4162" endWordPosition="4165">ion focused on parallel data collection. For example, Hildebrand et al.(2005) employed information retrieval technology to gather the bilingual sentences, which are similar to the test set, from available in-domain and out-of-domain training data to build an adaptive translation model. With the same motivation, Munteanu and Marcu (2005) extracted in-domain bilingual sentence pairs from comparable corpora. Since large-scale monolingual corpus is easier to obtain than parallel corpus, there have been some studies on how to generate parallel sentences with monolingual sentences. In this respect, Ueffing et al. (2008) explored semisupervised learning to obtain synthetic parallel sentences, and Wu et al. (2008) used an in-domain translation dictionary and monolingual corpora to adapt an out-of-domain translation model for the indomain text. Differing from the above-mentioned works on the acquirement of bilingual resource, several studies (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) adopted mixture modeling framework to exploit the full potential of the existing parallel corpus. Under this framework, the training corpus is first divided into different parts, each of which is used to train </context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2008</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar. 2008. Semi-supervised Model Adaptation for Statistical Machine Translation. Machine Translation, pages 77-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Domain Adaptation for Statistical Machine Translation with Domain Dictionary and Monolingual Corpora.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>993--1000</pages>
<contexts>
<context position="2977" citStr="Wu et al., 2008" startWordPosition="426" endWordPosition="429"> from newswire to weblog. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings </context>
<context position="25738" citStr="Wu et al. (2008)" startWordPosition="4177" endWordPosition="4180">retrieval technology to gather the bilingual sentences, which are similar to the test set, from available in-domain and out-of-domain training data to build an adaptive translation model. With the same motivation, Munteanu and Marcu (2005) extracted in-domain bilingual sentence pairs from comparable corpora. Since large-scale monolingual corpus is easier to obtain than parallel corpus, there have been some studies on how to generate parallel sentences with monolingual sentences. In this respect, Ueffing et al. (2008) explored semisupervised learning to obtain synthetic parallel sentences, and Wu et al. (2008) used an in-domain translation dictionary and monolingual corpora to adapt an out-of-domain translation model for the indomain text. Differing from the above-mentioned works on the acquirement of bilingual resource, several studies (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) adopted mixture modeling framework to exploit the full potential of the existing parallel corpus. Under this framework, the training corpus is first divided into different parts, each of which is used to train a sub translation model, then these sub models are used together with different weights during</context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Hua Wu, Haifeng Wang and Chengqing Zong. 2008. Domain Adaptation for Statistical Machine Translation with Domain Dictionary and Monolingual Corpora. In Proc. of COLING 2008, pages 993-1000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvments in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of NAACL</booktitle>
<pages>257--264</pages>
<contexts>
<context position="15542" citStr="Zens and Ney, 2004" startWordPosition="2516" endWordPosition="2519">and θ is the interpolation weight that can be optimized over the development data. Given the number of the phrase f in sentence f denoted as countf (�f), we compute the in-domain phrase-topic distribution in the following way: f) · P(tf in|f) (11) f) · P(tf in|f) X tf in φ(�e| f, tf out) P P �e� hf,ei∈Cout P =f∈Cf out h Cf in f) (6) P P tf out f∈Cf out Pmle(tf in |f) P countf ( f∈Cf in P tf in countf ( P f∈Cf in 462 Under the assumption that the topics of all words in the same phrase are independent, we consider two methods to calculate Pword(tf in |f). One is a “Noisy-OR” combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. Using this method, Pword(tf in |f) is defined as: Pword(tf in |f) = 1 − Pword( rl≈ 1 − P(�tf in|fj) f;E f� = 1 − rl (1 − P(tf in|fj)) (12) f;E f� where Pword(�tf in |�f) represents the probability that tf in is not the topic of the phrase f. Similarly, P (tf in|fj) indicates the probability that tf in is not the topic of the word fj. The other method is an “Averaging” combination one. With the assumption that tf in is the topic of f if at least one of the words in f belongs to this topic</context>
<context position="21543" citStr="Zens and Ney, 2004" startWordPosition="3511" endWordPosition="3514">nolingual corpora, the BLEU scores acquired by two methods are 20.45 and 20.54, achieving absolute improvements of 0.23 and 0.32 on the test set, respectively. In the experiments with the large-scale monolingual in-domain corpora, similar results are obtained, with absolute improvements of 0.54 and 0.89 over the baseline system. From the above experimental results, we know that both “Noisy-OR” and “Averaging” combination methods improve the performance over the baseline, and “Averaging” method seems to be slightly better. This finding fails to echo the promising results in the previous study (Zens and Ney, 2004). This is because the “Noisy-OR” method involves the multiplication of the word-topic distribution (shown in formula (12)), which leads to much sharper phrase-topic distribution than “Averaging” method, and is more likely to introduce bias to the translation probability estimation. Due to this reason, all the following experiments only consider the “Averaging”method. 4.2.2 Effect of Combining Two Phrase Tables In the above experiments, we replace the out-ofdomain phrase table with the adapted phrase table. Here we combine these two phrase tables in a loglinear framework to see if we could obta</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvments in phrase-based statistical machine translation. In Proc. of NAACL 2004, pages 257-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Distributed Language Modeling for N-best List Re-ranking.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>216--223</pages>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Ying Zhang, Almut Silja Hildebrand and Stephan Vogel. 2006. Distributed Language Modeling for N-best List Re-ranking. In Proc. of EMNLP 2006, pages 216-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language Model Adaptation for Statistical Machine Translation with Structured Query Models.</title>
<date>2004</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>411--417</pages>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck and Stephan Vogel. 2004. Language Model Adaptation for Statistical Machine Translation with Structured Query Models. In Proc. of COLING 2004, pages 411-417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>BiTAM: Bilingual Topic AdMixture Models for Word Alignment.</title>
<date>2006</date>
<booktitle>In Proc. of ACL/COLING</booktitle>
<pages>969--976</pages>
<contexts>
<context position="3847" citStr="Zhao and Xing, 2006" startWordPosition="554" endWordPosition="557">hes focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ang”, and occurs in the sentences related to the geography topic when translated to “h´e`an”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of </context>
<context position="27055" citStr="Zhao and Xing (2006" startWordPosition="4390" endWordPosition="4393">ts to the sentences from training corpus (Matsoukas et al., 2009) or the phrase pairs of phrase table (Foster et al., 2010). Final experimental results show that without using any additional resources, these approaches all improve SMT performance sig465 nificantly. Our method deals with translation model adaptation by making use of the topical context, so let us take a look at the recent research development on the application of topic models in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase a</context>
<context position="28981" citStr="Zhao and Xing, 2006" startWordPosition="4683" endWordPosition="4686">n during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora, which are far from full exploitation in the parallel data collection and mixture modeling framework. • In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model — HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling. (3) Instead of rescoring phrase pairs online, our approach calculate the translation probabilities offline, which brings no additi</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual Topic AdMixture Models for Word Alignment. In Proc. of ACL/COLING 2006, pages 969-976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation.</title>
<date>2007</date>
<booktitle>In Proc. of NIPS</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3868" citStr="Zhao and Xing, 2007" startWordPosition="558" endWordPosition="561">udies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ang”, and occurs in the sentences related to the geography topic when translated to “h´e`an”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolin</context>
<context position="29002" citStr="Zhao and Xing, 2007" startWordPosition="4687" endWordPosition="4690">th the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora, which are far from full exploitation in the parallel data collection and mixture modeling framework. • In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model — HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling. (3) Instead of rescoring phrase pairs online, our approach calculate the translation probabilities offline, which brings no additional burden to transl</context>
</contexts>
<marker>Zhao, Xing, 2007</marker>
<rawString>Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation. In Proc. of NIPS 2007, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qun Liu</author>
<author>Zhongjun He</author>
<author>Yang Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>89--97</pages>
<contexts>
<context position="28144" citStr="Liu et al., 2008" startWordPosition="4556" endWordPosition="4559">in adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora,</context>
</contexts>
<marker>Liu, He, Liu, Lin, 2008</marker>
<rawString>Qun Liu, Zhongjun He, Yang Liu and Shouxun Lin. 2008. Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation. In Proc. of EMNLP 2008, pages 89-97.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>