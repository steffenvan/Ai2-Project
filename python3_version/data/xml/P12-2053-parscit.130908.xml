<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000883">
<title confidence="0.896152">
Information-theoretic Multi-view Domain Adaptation
</title>
<author confidence="0.992409">
Pei Yang1,3, Wei Gao2, Qi Tan1, Kam-Fai Wong3
</author>
<affiliation confidence="0.981378">
1South China University of Technology, Guangzhou, China
</affiliation>
<email confidence="0.943876">
{yangpei,tanqi}@scut.edu.cn
</email>
<affiliation confidence="0.775867666666667">
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
wgao@qf.org.qa
3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
</affiliation>
<email confidence="0.991557">
kfwong@se.cuhk.edu.hk
</email>
<sectionHeader confidence="0.998567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994552">
We use multiple views for cross-domain doc-
ument classification. The main idea is to
strengthen the views’ consistency for target
data with source training data by identify-
ing the correlations of domain-specific fea-
tures from different domains. We present
an Information-theoretic Multi-view Adapta-
tion Model (IMAM) based on a multi-way
clustering scheme, where word and link clus-
ters can draw together seemingly unrelated
domain-specific features from both sides and
iteratively boost the consistency between doc-
ument clusterings based on word and link
views. Experiments show that IMAM signifi-
cantly outperforms state-of-the-art baselines.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993575">
Domain adaptation has been shown useful to many
natural language processing applications including
document classification (Sarinnapakorn and Kubat,
2007), sentiment classification (Blitzer et al., 2007),
part-of-speech tagging (Jiang and Zhai, 2007) and
entity mention detection (Daum´e III and Marcu,
2006).
Documents can be represented by multiple inde-
pendent sets of features such as words and link struc-
tures of the documents. Multi-view learning aims
to improve classifiers by leveraging the redundancy
and consistency among these multiple views (Blum
and Mitchell, 1998; R¨uping and Scheffer, 2005; Ab-
ney, 2002). Existing methods were designed for
data from single domain, assuming that either view
alone is sufficient to predict the target class accu-
rately. However, this view-consistency assumption
is largely violated in the setting of domain adapta-
tion where training and test data are drawn from dif-
ferent distributions.
Little research was done for multi-view domain
adaptation. In this work, we present an Information-
theoretical Multi-view Adaptation Model (IMAM)
based on co-clustering framework (Dhillon et al.,
2003) that combines the two learning paradigms to
transfer class information across domains in multi-
ple transformed feature spaces. IMAM exploits a
multi-way-clustering-based classification scheme to
simultaneously cluster documents, words and links
into their respective clusters. In particular, the word
and link clusterings can automatically associate the
correlated features from different domains. Such
correlations bridge the domain gap and enhance the
consistency of views for clustering (i.e., classifying)
the target data. Results show that IMAM signifi-
cantly outperforms the state-of-the-art baselines.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999207076923077">
The work closely related to ours was done by Dai
et al. (2007), where they proposed co-clustering-
based classification (CoCC) for adaptation learning.
CoCC was extended from information-theoretic co-
clustering (Dhillon et al., 2003), where in-domain
constraints were added to word clusters to provide
the class structure and partial categorization knowl-
edge. However, CoCC is a single-view algorithm.
Although multi-view learning (Blum and
Mitchell, 1998; Dasgupta et al., 2001; Abney,
2002; Sridharan and Kakade, 2008) is common
within a single domain, it is not well studied under
cross-domain settings. Chen et al. (2011) proposed
</bodyText>
<page confidence="0.936867">
270
</page>
<note confidence="0.679093">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270–274,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998732416666667">
CODA for adaptation based on co-training (Blum
and Mitchell, 1998), which is however a pseudo
multi-view algorithm where original data has only
one view. Therefore, it is not suitable for the
true multi-view case as ours. Zhang et al. (2011)
proposed an instance-level multi-view transfer
algorithm that integrates classification loss and view
consistency terms based on large margin framework.
However, instance-based approach is generally poor
since new target features lack support from source
data (Blitzer et al., 2011). We focus on feature-level
multi-view adaptation.
</bodyText>
<sectionHeader confidence="0.999236" genericHeader="method">
3 Our Model
</sectionHeader>
<bodyText confidence="0.9997573">
Intuitively, source-specific and target-specific fea-
tures can be drawn together by mining their
co-occurrence with domain-independent (common)
features, which helps bridge the distribution gap.
Meanwhile, the view consistency on target data can
be strengthened if target-specific features are appro-
priately bundled with source-specific features. Our
model leverages the complementary cooperation be-
tween different views to yield better adaptation per-
formance.
</bodyText>
<subsectionHeader confidence="0.996133">
3.1 Representation
</subsectionHeader>
<bodyText confidence="0.999932888888889">
Let DS be the source training documents and DT
be the unlabeled target documents. Let C be the set
of class labels. Each source document ds E DS is
labeled with a unique class label c E C. Our goal
is to assign each target document dt E DT to an
appropriate class as accurately as possible.
Let W be the vocabulary of the entire document
collection D = DSUDT. Let L be the set of all links
(hyperlinks or citations) among documents. Each
d E D can be represented by two views, i.e., a bag-
of-words set {w} and a bag-of-links set {l}.
Our model explores multi-way clustering that si-
multaneously clusters documents, words and links.
Let ˆD, Wˆ and Lˆ be the respective clustering of doc-
uments, words and links. The clustering functions
are defined as CD(d) = dˆ for document, CW (w) =
wˆ for word and CL(l) = lˆ for link, where ˆd, wˆ and lˆ
represent the corresponding clusters.
</bodyText>
<subsectionHeader confidence="0.995384">
3.2 Objectives
</subsectionHeader>
<bodyText confidence="0.9986152">
We extend the information-theoretic co-clustering
framework (Dhillon et al., 2003) to incorporate the
loss from multiple views. LetT(X, Y ) be mutual in-
formation (MI) of variables X and Y , our objective
is to minimize the MI loss of two different views:
</bodyText>
<equation confidence="0.9999862">
0 = α · 0W + (1 − α) · 0L (1)
[ ]
W� ) + λ · T(C, W) − T(C, W� )
[ ]
�L) + λ · T(C, L) − T(C, �L)
</equation>
<bodyText confidence="0.9751102">
0W and 0L are the loss terms based on word view
and link view, respectively, traded off by α. λ bal-
ances the effect of word or link clusters from co-
clustering. When α = 1, the function relies on text
only that reduces to CoCC (Dai et al., 2007).
For any x E ˆx, we define conditional distribution
q(x|ˆy) = p(x|ˆx)p(ˆx|ˆy) under co-clustering ( ˆX, Yˆ)
based on Dhillon et al. (2003). Therefore, for any
w E ˆw, l E ˆl, d E dˆ and c E C, we can calculate
a set of conditional distributions: q(w|ˆd), q(d |ˆw),
q(l|ˆd), q(d|ˆl), q(c |ˆw), q(c|ˆl).
Eq. 1 is hard to optimize due to its combinatorial
nature. We transform it to the equivalent form based
on Kullback-Leibler (KL) divergence between two
conditional distributions p(x|y) and q(x|ˆy), where
</bodyText>
<equation confidence="0.889055076923077">
D(p(x|y)||q(x|ˆy)) = �x p(x|y)logp(x|y)
q(x|ˆy).
Lemma 1 (Objective functions) Equation 1 can
be turned into the form of alternate minimization:
(i) For document clustering, we minimize
�0 = p(d)ϕD(d, ˆd) + ϕC( Wˆ, ˆL),
d
where ϕC( Wˆ, ˆL) is a constant&apos; and
ϕD(d,ˆd) =α · D(p(w|d)||q(w|ˆd))
+ (1 − α) · D(p(l|d)||q(l |ˆd)).
(ii) For word and link clustering, we minimize
�0 = α p(w)ϕW(w, ˆw)+(1−α) � p(l)ϕL(l, ˆl),
w l
</equation>
<bodyText confidence="0.9240915">
where for any feature v (e.g., w or l) in feature set
V (e.g., W or L), we have
</bodyText>
<equation confidence="0.9912538">
ϕV (v, ˆv) =D(p(d|v)||q(d|ˆv))
+ λ · D(p(c|v)||q(c|ˆv)).
&apos;We can obtain that ϕC( W�, L) =
[ ]
λ α(T(C, W ) − T(C, W� )) + (1 − α)(T(C, L) − T(C, �L)) I
</equation>
<bodyText confidence="0.765132333333333">
which is constant since word/link clusters keep fixed during the
document clustering step.
where
</bodyText>
<equation confidence="0.994336333333333">
©W = T(DT, W) − T(
DT,
©L = T(DT, L) − T( DT,
</equation>
<page confidence="0.957204">
271
</page>
<bodyText confidence="0.998481">
Lemma 12 allows us to alternately reorder either
documents or both words and links by fixing the
other in such a way that the MI loss in Eq. 1 de-
creases monotonically.
</bodyText>
<sectionHeader confidence="0.943084" genericHeader="method">
4 Consistency of Multiple Views
</sectionHeader>
<bodyText confidence="0.999874142857143">
In this section, we present how the consistency of
document clustering on target data could be en-
hanced among multiple views, which is the key issue
of our multi-view adaptation method.
According to Lemma 1, minimizing ϕD(d, ˆd) for
each d can reduce the objective function value itera-
tively (t denotes round id):
</bodyText>
<equation confidence="0.970693285714286">
C(t+1)
D (d) = arg min
dˆ [ α &apos; D(p(w|d)||q(t)(w|ˆd))
]
+(1 − α) &apos; D(p(l|d)||q(t)(l |ˆd)) (2)
In each iteration, the optimal document cluster-
ing function C(t+1)
</equation>
<bodyText confidence="0.8821626">
D is to minimize the weighted sum
of KL-divergences used in word-view and link-view
document clustering functions as shown above. The
optimal word-view and link-view clustering func-
tions can be denoted as follows:
</bodyText>
<equation confidence="0.9707428">
C(t+1)
DW (d) = arg min
dˆ D(p(w|d)||q(t)(w |ˆd)) (3)
C(t+1)
DL (d) = arg min
dˆ D(p(l|d)||q(t)(l |ˆd)) (4)
Our central idea is that the document clusterings
C(t+1)
DW and C(t+1)
DL based on the two views are drawn
</equation>
<bodyText confidence="0.9668747">
closer in each iteration due to the word and link
clusterings that bring together seemingly unrelated
source-specific and target-specific features. Mean-
while, C(t+1)
D combines the two views and reallo-
cates the documents so that it remains consistent
with the view-based clusterings as much as possi-
ble. The more consistent the views, the better the
document clustering, and then the better the word
and link clustering, which creates a positive cycle.
</bodyText>
<subsectionHeader confidence="0.993957">
4.1 Disagreement Rate of Views
</subsectionHeader>
<bodyText confidence="0.999956333333333">
For any document, a consistency indicator function
with respect to the two view-based clusterings can
be defined as follows (t is omitted for simplicity):
</bodyText>
<footnote confidence="0.9130005">
2Due to space limit, the proof of all lemmas will be given in
a long version of the paper.
</footnote>
<equation confidence="0.7928903">
�Definition 1 (Indicator function) For any d E D,
1, if CDW (d) =CDL(d);
0, otherwise
Then we define the disagreement rate between two
view-based clustering functions:
Definition 2 (Disagreement rate)
E
d∈D δCDW ,CDL (d)
η(CDW ,CDL) = 1 − (5)
|D|
</equation>
<bodyText confidence="0.998462">
Abney (2002) suggests that the disagreement rate
of two independent hypotheses upper-bounds the er-
ror rate of either hypothesis. By minimizing the dis-
agreement rate on unlabeled data, the error rate of
each view can be minimized (so does the overall er-
ror). However, Eq. 5 is not continuous nor convex,
which is difficult to optimize directly. By using the
optimization based on Lemma 1, we can show em-
pirically that disagreement rate is monotonically de-
creased (see Section 5).
</bodyText>
<subsectionHeader confidence="0.987675">
4.2 View Combination
</subsectionHeader>
<bodyText confidence="0.98950225">
In practice, view-based document clusterings in
Eq. 3 and 4 are not computed explicitly. Instead,
Eq. 2 directly optimizes view combination and pro-
duces the document clustering. Therefore, it is nec-
essary to disclose how consistent it could be with the
view-based clusterings.
Suppose Q = {TD|TD(d) = ˆd, dˆ E ˆD} is
the set of all document clustering functions. For
any TD E Q, we obtain the disagreement rate
η(TD, CDW n CDL), where CDW n CDL denotes the
clustering resulting from the overlap of the view-
based clusterings.
</bodyText>
<construct confidence="0.768654285714286">
Lemma 2 CD always minimizes the disagreement
rate for any TD E Q such that
η(CD, CDW n CDL) = min η(TD, CDW n CDL)
FD∈Ω
Meanwhile, η(CD, CDW n CDL) = η(CDW ,CDL).
Lemma 2 suggests that IMAM always finds the
document clustering with the minimal disagreement
</construct>
<bodyText confidence="0.945547666666667">
rate to the overlap of view-based clusterings, and the
minimal value of disagreement rate equals to the dis-
agreement rate of the view-based clusterings.
</bodyText>
<equation confidence="0.908142">
δCDW ,CDL (d) =
</equation>
<page confidence="0.998508">
272
</page>
<tableCaption confidence="0.98309">
Table 1: View disagreement rate η and error rate ϵ that
decrease with iterations and their Pearson’s correlation γ.
</tableCaption>
<table confidence="0.999967363636364">
Round 1 2 3 4 5 y
DA-EC E 0.194 0.153 0.149 0.144 0.144 0.998
77 0.340 0.132 0.111 0.101 0.095
DA-NT E 0.147 0.083 0.071 0.065 0.064 0.996
77 0.295 0.100 0.076 0.069 0.064
DA-OS E 0.129 0.064 0.052 0.047 0.041 0.998
77 0.252 0.092 0.068 0.060 0.052
DA-ML E 0.166 0.102 0.071 0.065 0.064 0.984
77 0.306 0.107 0.076 0.062 0.054
EC-NT E 0.311 0.250 0.228 0.219 0.217 0.988
77 0.321 0.137 0.112 0.096 0.089
</table>
<tableCaption confidence="0.994459">
Table 2: Comparison of error rate with baselines.
</tableCaption>
<table confidence="0.999929">
Data TSVM Co-Train CoCC MVTL-LM IMAM
DA-EC 0.214 0.230 0.149 0.192 0.138
DA-NT 0.114 0.163 0.106 0.108 0.069
DA-OS 0.262 0.175 0.075 0.068 0.039
DA-ML 0.107 0.171 0.109 0.183 0.047
EC-NT 0.177 0.296 0.225 0.261 0.192
EC-OS 0.245 0.175 0.137 0.176 0.074
EC-ML 0.168 0.206 0.203 0.264 0.173
NT-OS 0.396 0.220 0.107 0.288 0.070
NT-ML 0.101 0.132 0.054 0.071 0.032
OS-ML 0.179 0.128 0.051 0.126 0.021
Average 0.196 0.190 0.122 0.174 0.085
</table>
<sectionHeader confidence="0.988837" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.751197">
Data and Setup
</subsectionHeader>
<bodyText confidence="0.99989447368421">
Cora (McCallum et al., 2000) is an online archive
of computer science articles. The documents in the
archive are categorized into a hierarchical structure.
We selected a subset of Cora, which contains 5 top
categories and 10 sub-categories. We used a similar
way as Dai et al. (2007) to construct our training and
test sets. For each set, we chose two top categories,
one as positive class and the other as the negative.
Different sub-categories were deemed as different
domains. The task is defined as top category classifi-
cation. For example, the dataset denoted as DA-EC
consists of source domain: DA 1(+), EC 1(-); and
target domain: DA 2(+), EC 2(-).
The classification error rate E is measured as the
proportion of misclassified target documents. In or-
der to avoid the infinity values, we applied Laplacian
smoothing when computing the KL-divergence. We
tuned α, A and the number of word/link clusters by
cross-validation on the training data.
</bodyText>
<sectionHeader confidence="0.851399" genericHeader="evaluation">
Results and Discussions
</sectionHeader>
<bodyText confidence="0.998465256410256">
Table 1 shows the monotonic decrease of view dis-
agreement rate 77 and error rate E with the iterations
and their Pearson’s correlation -y is nearly perfectly
positive. This indicates that IMAM gradually im-
proves adaptation by strengthening the view consis-
tency. This is achieved by the reinforcement of word
and link clusterings that draw together target- and
source-specific features that are originally unrelated
but co-occur with the common features.
We compared IMAM with (1) Transductive SVM
(TSVM) (Joachims, 1999) using both words and
links features; (2) Co-Training (Blum and Mitchell,
1998); (3) CoCC (Dai et al., 2007): Co-clustering-
based single-view transfer learner (with text view
only); and (4) MVTL-LM (Zhang et al., 2011):
Large-margin-based multi-view transfer learner.
Table 2 shows the results. Co-Training performed
a little better than TSVM by boosting the confidence
of classifiers built on the distinct views in a comple-
mentary way. But since Co-Training doesn’t con-
sider the distribution gap, it performed clearly worse
than CoCC even though CoCC has only one view.
IMAM significantly outperformed CoCC on all
the datasets. In average, the error rate of IMAM
is 30.3% lower than that of CoCC. This is because
IMAM effectively leverages distinct and comple-
mentary views. Compared to CoCC, using source
training data to improve the view consistency on tar-
get data is the key competency of IMAM.
MVTL-LM performed worse than CoCC. It sug-
gests that instance-based approach is not effective
when the data of different domains are drawn from
different feature spaces. Although MVTL-LM regu-
lates view consistency, it cannot identify the associ-
ations between target- and source-specific features
that is the key to the success of adaptation espe-
cially when domain gap is large and less common-
ality could be found. In contrast, CoCC and IMAM
uses multi-way clustering to find such correlations.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999967571428572">
We presented a novel feature-level multi-view do-
main adaptation approach. The thrust is to incor-
porate distinct views of document features into the
information-theoretic co-clustering framework and
strengthen the consistency of views on clustering
(i.e., classifying) target documents. The improve-
ments over the state-of-the-arts are significant.
</bodyText>
<page confidence="0.997656">
273
</page>
<sectionHeader confidence="0.99639" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999708661538461">
Steven Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 360-367.
John Blitzer, Mark Dredze and Fernado Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 440-447.
John Blitzer, Sham Kakade and Dean P. Foster. 2011.
Domain Adaptation with Coupled Subspaces. In Pro-
ceedings of the 14th International Conference on Arti-
ficial Intelligence and Statistics (AISTATS), pages 173-
181.
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the 11th Annual Conference on Computa-
tional Learning Theory, pages 92-100.
Minmin Chen, Killian Q. Weinberger and John Blitzer.
2011. Co-Training for Domain Adaptation. In Pro-
ceedings of NIPS, pages 1-9.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang and Yong
Yu. 2007. Co-clustering Based Classification for Out-
of-domain Documents. In Proceedings of the 13th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 210-219.
Sanjoy Dasgupta, Michael L. Littman and David
McAllester. 2001. PAC Generalization Bounds for
Co-Training. In Proceeding of NIPS, pages 375-382.
Hal Daum´e III and Daniel Marcu. 2006. Domain Adap-
tation for Statistical Classifiers. Journal of Artificial
Intelligence Research, 26(2006):101-126.
Inderjit S. Dhillon, Subramanyam Mallela and Dharmen-
dra S. Modha. 2003. Information-Theoretic Co-
clustering. In Proceedings of the ninth ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 210-219.
Thorsten Joachims. 1999. Transductive Inference for
Text Classification using Support Vector Machines. In
Proceedings of Sixteenth International Conference on
Machine Learning, pages 200-209.
Jing Jiang and Chengxiang Zhai. 2007. Instance Weight-
ing for Domain Adaptation in NLP. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 264-271.
Andrew K. McCallum, Kamal Nigam, Jason Rennie and
Kristie Seymore. 2000. Automating the Construction
of Internet Portals with Machine Learning. Informa-
tion Retrieval, 3(2):127-163.
Stephan R¨uping and Tobias Scheffer. 2005. Learning
with Multiple Views. In Proceedings of ICML Work-
shop on Learning with Multiple Views.
Kanoksri Sarinnapakorn and Miroslav Kubat. 2007.
Combining Sub-classifiers in Text Categorization: A
DST-Based Solution and a Case Study. IEEE Transac-
tions Knowledge and Data Engineering, 19(12):1638-
1651.
Karthik Sridharan and Sham M. Kakade. 2008. An In-
formation Theoretic Framework for Multi-view Learn-
ing. In Proceedings of the 21st Annual Conference on
Computational Learning Theory, pages 403-414.
Dan Zhang, Jingrui He, Yan Liu, Luo Si and Richard D.
Lawrence. 2011. Multi-view Transfer Learning with
a Large Margin Approach. In Proceedings of the 17th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 1208-1216.
</reference>
<page confidence="0.998266">
274
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872669">
<title confidence="0.999755">Information-theoretic Multi-view Domain Adaptation</title>
<author confidence="0.999828">Wei Qi Kam-Fai</author>
<affiliation confidence="0.9960065">China University of Technology, Guangzhou, Computing Research Institute, Qatar Foundation, Doha,</affiliation>
<address confidence="0.90971">Chinese University of Hong Kong, Shatin, N.T., Hong</address>
<email confidence="0.989643">kfwong@se.cuhk.edu.hk</email>
<abstract confidence="0.9984098125">We use multiple views for cross-domain document classification. The main idea is to strengthen the views’ consistency for target data with source training data by identifying the correlations of domain-specific features from different domains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM significantly outperforms state-of-the-art baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>360--367</pages>
<contexts>
<context position="1630" citStr="Abney, 2002" startWordPosition="220" endWordPosition="222"> adaptation has been shown useful to many natural language processing applications including document classification (Sarinnapakorn and Kubat, 2007), sentiment classification (Blitzer et al., 2007), part-of-speech tagging (Jiang and Zhai, 2007) and entity mention detection (Daum´e III and Marcu, 2006). Documents can be represented by multiple independent sets of features such as words and link structures of the documents. Multi-view learning aims to improve classifiers by leveraging the redundancy and consistency among these multiple views (Blum and Mitchell, 1998; R¨uping and Scheffer, 2005; Abney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class accurately. However, this view-consistency assumption is largely violated in the setting of domain adaptation where training and test data are drawn from different distributions. Little research was done for multi-view domain adaptation. In this work, we present an Informationtheoretical Multi-view Adaptation Model (IMAM) based on co-clustering framework (Dhillon et al., 2003) that combines the two learning paradigms to transfer class information across domain</context>
<context position="3256" citStr="Abney, 2002" startWordPosition="451" endWordPosition="452">ssifying) the target data. Results show that IMAM significantly outperforms the state-of-the-art baselines. 2 Related Work The work closely related to ours was done by Dai et al. (2007), where they proposed co-clusteringbased classification (CoCC) for adaptation learning. CoCC was extended from information-theoretic coclustering (Dhillon et al., 2003), where in-domain constraints were added to word clusters to provide the class structure and partial categorization knowledge. However, CoCC is a single-view algorithm. Although multi-view learning (Blum and Mitchell, 1998; Dasgupta et al., 2001; Abney, 2002; Sridharan and Kakade, 2008) is common within a single domain, it is not well studied under cross-domain settings. Chen et al. (2011) proposed 270 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270–274, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics CODA for adaptation based on co-training (Blum and Mitchell, 1998), which is however a pseudo multi-view algorithm where original data has only one view. Therefore, it is not suitable for the true multi-view case as ours. Zhang et al. (2011) proposed an inst</context>
<context position="9552" citStr="Abney (2002)" startWordPosition="1542" endWordPosition="1543">en the better the word and link clustering, which creates a positive cycle. 4.1 Disagreement Rate of Views For any document, a consistency indicator function with respect to the two view-based clusterings can be defined as follows (t is omitted for simplicity): 2Due to space limit, the proof of all lemmas will be given in a long version of the paper. �Definition 1 (Indicator function) For any d E D, 1, if CDW (d) =CDL(d); 0, otherwise Then we define the disagreement rate between two view-based clustering functions: Definition 2 (Disagreement rate) E d∈D δCDW ,CDL (d) η(CDW ,CDL) = 1 − (5) |D| Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the error rate of either hypothesis. By minimizing the disagreement rate on unlabeled data, the error rate of each view can be minimized (so does the overall error). However, Eq. 5 is not continuous nor convex, which is difficult to optimize directly. By using the optimization based on Lemma 1, we can show empirically that disagreement rate is monotonically decreased (see Section 5). 4.2 View Combination In practice, view-based document clusterings in Eq. 3 and 4 are not computed explicitly. Instead, Eq. 2 directly</context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>Steven Abney. 2002. Bootstrapping. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 360-367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernado Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="1215" citStr="Blitzer et al., 2007" startWordPosition="156" endWordPosition="159">ains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM significantly outperforms state-of-the-art baselines. 1 Introduction Domain adaptation has been shown useful to many natural language processing applications including document classification (Sarinnapakorn and Kubat, 2007), sentiment classification (Blitzer et al., 2007), part-of-speech tagging (Jiang and Zhai, 2007) and entity mention detection (Daum´e III and Marcu, 2006). Documents can be represented by multiple independent sets of features such as words and link structures of the documents. Multi-view learning aims to improve classifiers by leveraging the redundancy and consistency among these multiple views (Blum and Mitchell, 1998; R¨uping and Scheffer, 2005; Abney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class accurately. However, this view-consistency assump</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze and Fernado Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Sham Kakade</author>
<author>Dean P Foster</author>
</authors>
<title>Domain Adaptation with Coupled Subspaces.</title>
<date>2011</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),</booktitle>
<pages>173--181</pages>
<contexts>
<context position="4122" citStr="Blitzer et al., 2011" startWordPosition="577" endWordPosition="580">0–274, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics CODA for adaptation based on co-training (Blum and Mitchell, 1998), which is however a pseudo multi-view algorithm where original data has only one view. Therefore, it is not suitable for the true multi-view case as ours. Zhang et al. (2011) proposed an instance-level multi-view transfer algorithm that integrates classification loss and view consistency terms based on large margin framework. However, instance-based approach is generally poor since new target features lack support from source data (Blitzer et al., 2011). We focus on feature-level multi-view adaptation. 3 Our Model Intuitively, source-specific and target-specific features can be drawn together by mining their co-occurrence with domain-independent (common) features, which helps bridge the distribution gap. Meanwhile, the view consistency on target data can be strengthened if target-specific features are appropriately bundled with source-specific features. Our model leverages the complementary cooperation between different views to yield better adaptation performance. 3.1 Representation Let DS be the source training documents and DT be the unla</context>
</contexts>
<marker>Blitzer, Kakade, Foster, 2011</marker>
<rawString>John Blitzer, Sham Kakade and Dean P. Foster. 2011. Domain Adaptation with Coupled Subspaces. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 173-181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="1588" citStr="Blum and Mitchell, 1998" startWordPosition="212" endWordPosition="215">rms state-of-the-art baselines. 1 Introduction Domain adaptation has been shown useful to many natural language processing applications including document classification (Sarinnapakorn and Kubat, 2007), sentiment classification (Blitzer et al., 2007), part-of-speech tagging (Jiang and Zhai, 2007) and entity mention detection (Daum´e III and Marcu, 2006). Documents can be represented by multiple independent sets of features such as words and link structures of the documents. Multi-view learning aims to improve classifiers by leveraging the redundancy and consistency among these multiple views (Blum and Mitchell, 1998; R¨uping and Scheffer, 2005; Abney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class accurately. However, this view-consistency assumption is largely violated in the setting of domain adaptation where training and test data are drawn from different distributions. Little research was done for multi-view domain adaptation. In this work, we present an Informationtheoretical Multi-view Adaptation Model (IMAM) based on co-clustering framework (Dhillon et al., 2003) that combines the two learning paradigms t</context>
<context position="3220" citStr="Blum and Mitchell, 1998" startWordPosition="443" endWordPosition="446">e consistency of views for clustering (i.e., classifying) the target data. Results show that IMAM significantly outperforms the state-of-the-art baselines. 2 Related Work The work closely related to ours was done by Dai et al. (2007), where they proposed co-clusteringbased classification (CoCC) for adaptation learning. CoCC was extended from information-theoretic coclustering (Dhillon et al., 2003), where in-domain constraints were added to word clusters to provide the class structure and partial categorization knowledge. However, CoCC is a single-view algorithm. Although multi-view learning (Blum and Mitchell, 1998; Dasgupta et al., 2001; Abney, 2002; Sridharan and Kakade, 2008) is common within a single domain, it is not well studied under cross-domain settings. Chen et al. (2011) proposed 270 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270–274, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics CODA for adaptation based on co-training (Blum and Mitchell, 1998), which is however a pseudo multi-view algorithm where original data has only one view. Therefore, it is not suitable for the true multi-view case as ours. </context>
<context position="13613" citStr="Blum and Mitchell, 1998" startWordPosition="2223" endWordPosition="2226"> Results and Discussions Table 1 shows the monotonic decrease of view disagreement rate 77 and error rate E with the iterations and their Pearson’s correlation -y is nearly perfectly positive. This indicates that IMAM gradually improves adaptation by strengthening the view consistency. This is achieved by the reinforcement of word and link clusterings that draw together target- and source-specific features that are originally unrelated but co-occur with the common features. We compared IMAM with (1) Transductive SVM (TSVM) (Joachims, 1999) using both words and links features; (2) Co-Training (Blum and Mitchell, 1998); (3) CoCC (Dai et al., 2007): Co-clusteringbased single-view transfer learner (with text view only); and (4) MVTL-LM (Zhang et al., 2011): Large-margin-based multi-view transfer learner. Table 2 shows the results. Co-Training performed a little better than TSVM by boosting the confidence of classifiers built on the distinct views in a complementary way. But since Co-Training doesn’t consider the distribution gap, it performed clearly worse than CoCC even though CoCC has only one view. IMAM significantly outperformed CoCC on all the datasets. In average, the error rate of IMAM is 30.3% lower t</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Killian Q Weinberger</author>
<author>John Blitzer</author>
</authors>
<title>Co-Training for Domain Adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="3390" citStr="Chen et al. (2011)" startWordPosition="471" endWordPosition="474">work closely related to ours was done by Dai et al. (2007), where they proposed co-clusteringbased classification (CoCC) for adaptation learning. CoCC was extended from information-theoretic coclustering (Dhillon et al., 2003), where in-domain constraints were added to word clusters to provide the class structure and partial categorization knowledge. However, CoCC is a single-view algorithm. Although multi-view learning (Blum and Mitchell, 1998; Dasgupta et al., 2001; Abney, 2002; Sridharan and Kakade, 2008) is common within a single domain, it is not well studied under cross-domain settings. Chen et al. (2011) proposed 270 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270–274, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics CODA for adaptation based on co-training (Blum and Mitchell, 1998), which is however a pseudo multi-view algorithm where original data has only one view. Therefore, it is not suitable for the true multi-view case as ours. Zhang et al. (2011) proposed an instance-level multi-view transfer algorithm that integrates classification loss and view consistency terms based on large margin framewor</context>
</contexts>
<marker>Chen, Weinberger, Blitzer, 2011</marker>
<rawString>Minmin Chen, Killian Q. Weinberger and John Blitzer. 2011. Co-Training for Domain Adaptation. In Proceedings of NIPS, pages 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenyuan Dai</author>
</authors>
<title>Gui-Rong Xue, Qiang Yang and Yong Yu.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>210--219</pages>
<marker>Dai, 2007</marker>
<rawString>Wenyuan Dai, Gui-Rong Xue, Qiang Yang and Yong Yu. 2007. Co-clustering Based Classification for Outof-domain Documents. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 210-219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Michael L Littman</author>
<author>David McAllester</author>
</authors>
<title>PAC Generalization Bounds for Co-Training.</title>
<date>2001</date>
<booktitle>In Proceeding of NIPS,</booktitle>
<pages>375--382</pages>
<contexts>
<context position="3243" citStr="Dasgupta et al., 2001" startWordPosition="447" endWordPosition="450">r clustering (i.e., classifying) the target data. Results show that IMAM significantly outperforms the state-of-the-art baselines. 2 Related Work The work closely related to ours was done by Dai et al. (2007), where they proposed co-clusteringbased classification (CoCC) for adaptation learning. CoCC was extended from information-theoretic coclustering (Dhillon et al., 2003), where in-domain constraints were added to word clusters to provide the class structure and partial categorization knowledge. However, CoCC is a single-view algorithm. Although multi-view learning (Blum and Mitchell, 1998; Dasgupta et al., 2001; Abney, 2002; Sridharan and Kakade, 2008) is common within a single domain, it is not well studied under cross-domain settings. Chen et al. (2011) proposed 270 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270–274, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics CODA for adaptation based on co-training (Blum and Mitchell, 1998), which is however a pseudo multi-view algorithm where original data has only one view. Therefore, it is not suitable for the true multi-view case as ours. Zhang et al. (2011) pro</context>
</contexts>
<marker>Dasgupta, Littman, McAllester, 2001</marker>
<rawString>Sanjoy Dasgupta, Michael L. Littman and David McAllester. 2001. PAC Generalization Bounds for Co-Training. In Proceeding of NIPS, pages 375-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain Adaptation for Statistical Classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--2006</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain Adaptation for Statistical Classifiers. Journal of Artificial Intelligence Research, 26(2006):101-126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Subramanyam Mallela</author>
<author>Dharmendra S Modha</author>
</authors>
<title>Information-Theoretic Coclustering.</title>
<date>2003</date>
<booktitle>In Proceedings of the ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>210--219</pages>
<contexts>
<context position="2145" citStr="Dhillon et al., 2003" startWordPosition="295" endWordPosition="298"> consistency among these multiple views (Blum and Mitchell, 1998; R¨uping and Scheffer, 2005; Abney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class accurately. However, this view-consistency assumption is largely violated in the setting of domain adaptation where training and test data are drawn from different distributions. Little research was done for multi-view domain adaptation. In this work, we present an Informationtheoretical Multi-view Adaptation Model (IMAM) based on co-clustering framework (Dhillon et al., 2003) that combines the two learning paradigms to transfer class information across domains in multiple transformed feature spaces. IMAM exploits a multi-way-clustering-based classification scheme to simultaneously cluster documents, words and links into their respective clusters. In particular, the word and link clusterings can automatically associate the correlated features from different domains. Such correlations bridge the domain gap and enhance the consistency of views for clustering (i.e., classifying) the target data. Results show that IMAM significantly outperforms the state-of-the-art bas</context>
<context position="5638" citStr="Dhillon et al., 2003" startWordPosition="824" endWordPosition="827">DSUDT. Let L be the set of all links (hyperlinks or citations) among documents. Each d E D can be represented by two views, i.e., a bagof-words set {w} and a bag-of-links set {l}. Our model explores multi-way clustering that simultaneously clusters documents, words and links. Let ˆD, Wˆ and Lˆ be the respective clustering of documents, words and links. The clustering functions are defined as CD(d) = dˆ for document, CW (w) = wˆ for word and CL(l) = lˆ for link, where ˆd, wˆ and lˆ represent the corresponding clusters. 3.2 Objectives We extend the information-theoretic co-clustering framework (Dhillon et al., 2003) to incorporate the loss from multiple views. LetT(X, Y ) be mutual information (MI) of variables X and Y , our objective is to minimize the MI loss of two different views: 0 = α · 0W + (1 − α) · 0L (1) [ ] W� ) + λ · T(C, W) − T(C, W� ) [ ] �L) + λ · T(C, L) − T(C, �L) 0W and 0L are the loss terms based on word view and link view, respectively, traded off by α. λ balances the effect of word or link clusters from coclustering. When α = 1, the function relies on text only that reduces to CoCC (Dai et al., 2007). For any x E ˆx, we define conditional distribution q(x|ˆy) = p(x|ˆx)p(ˆx|ˆy) under </context>
</contexts>
<marker>Dhillon, Mallela, Modha, 2003</marker>
<rawString>Inderjit S. Dhillon, Subramanyam Mallela and Dharmendra S. Modha. 2003. Information-Theoretic Coclustering. In Proceedings of the ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 210-219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive Inference for Text Classification using Support Vector Machines.</title>
<date>1999</date>
<booktitle>In Proceedings of Sixteenth International Conference on Machine Learning,</booktitle>
<pages>200--209</pages>
<contexts>
<context position="13534" citStr="Joachims, 1999" startWordPosition="2213" endWordPosition="2214">number of word/link clusters by cross-validation on the training data. Results and Discussions Table 1 shows the monotonic decrease of view disagreement rate 77 and error rate E with the iterations and their Pearson’s correlation -y is nearly perfectly positive. This indicates that IMAM gradually improves adaptation by strengthening the view consistency. This is achieved by the reinforcement of word and link clusterings that draw together target- and source-specific features that are originally unrelated but co-occur with the common features. We compared IMAM with (1) Transductive SVM (TSVM) (Joachims, 1999) using both words and links features; (2) Co-Training (Blum and Mitchell, 1998); (3) CoCC (Dai et al., 2007): Co-clusteringbased single-view transfer learner (with text view only); and (4) MVTL-LM (Zhang et al., 2011): Large-margin-based multi-view transfer learner. Table 2 shows the results. Co-Training performed a little better than TSVM by boosting the confidence of classifiers built on the distinct views in a complementary way. But since Co-Training doesn’t consider the distribution gap, it performed clearly worse than CoCC even though CoCC has only one view. IMAM significantly outperforme</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Transductive Inference for Text Classification using Support Vector Machines. In Proceedings of Sixteenth International Conference on Machine Learning, pages 200-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Instance Weighting for Domain Adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>264--271</pages>
<contexts>
<context position="1262" citStr="Jiang and Zhai, 2007" startWordPosition="162" endWordPosition="165">-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM significantly outperforms state-of-the-art baselines. 1 Introduction Domain adaptation has been shown useful to many natural language processing applications including document classification (Sarinnapakorn and Kubat, 2007), sentiment classification (Blitzer et al., 2007), part-of-speech tagging (Jiang and Zhai, 2007) and entity mention detection (Daum´e III and Marcu, 2006). Documents can be represented by multiple independent sets of features such as words and link structures of the documents. Multi-view learning aims to improve classifiers by leveraging the redundancy and consistency among these multiple views (Blum and Mitchell, 1998; R¨uping and Scheffer, 2005; Abney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class accurately. However, this view-consistency assumption is largely violated in the setting of doma</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and Chengxiang Zhai. 2007. Instance Weighting for Domain Adaptation in NLP. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
<author>Kamal Nigam</author>
<author>Jason Rennie</author>
<author>Kristie Seymore</author>
</authors>
<date>2000</date>
<booktitle>Automating the Construction of Internet Portals with Machine Learning. Information Retrieval,</booktitle>
<pages>3--2</pages>
<contexts>
<context position="12068" citStr="McCallum et al., 2000" startWordPosition="1976" endWordPosition="1979">.311 0.250 0.228 0.219 0.217 0.988 77 0.321 0.137 0.112 0.096 0.089 Table 2: Comparison of error rate with baselines. Data TSVM Co-Train CoCC MVTL-LM IMAM DA-EC 0.214 0.230 0.149 0.192 0.138 DA-NT 0.114 0.163 0.106 0.108 0.069 DA-OS 0.262 0.175 0.075 0.068 0.039 DA-ML 0.107 0.171 0.109 0.183 0.047 EC-NT 0.177 0.296 0.225 0.261 0.192 EC-OS 0.245 0.175 0.137 0.176 0.074 EC-ML 0.168 0.206 0.203 0.264 0.173 NT-OS 0.396 0.220 0.107 0.288 0.070 NT-ML 0.101 0.132 0.054 0.071 0.032 OS-ML 0.179 0.128 0.051 0.126 0.021 Average 0.196 0.190 0.122 0.174 0.085 5 Experiments and Results Data and Setup Cora (McCallum et al., 2000) is an online archive of computer science articles. The documents in the archive are categorized into a hierarchical structure. We selected a subset of Cora, which contains 5 top categories and 10 sub-categories. We used a similar way as Dai et al. (2007) to construct our training and test sets. For each set, we chose two top categories, one as positive class and the other as the negative. Different sub-categories were deemed as different domains. The task is defined as top category classification. For example, the dataset denoted as DA-EC consists of source domain: DA 1(+), EC 1(-); and targe</context>
</contexts>
<marker>McCallum, Nigam, Rennie, Seymore, 2000</marker>
<rawString>Andrew K. McCallum, Kamal Nigam, Jason Rennie and Kristie Seymore. 2000. Automating the Construction of Internet Portals with Machine Learning. Information Retrieval, 3(2):127-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan R¨uping</author>
<author>Tobias Scheffer</author>
</authors>
<title>Learning with Multiple Views.</title>
<date>2005</date>
<booktitle>In Proceedings of ICML Workshop on Learning with Multiple Views.</booktitle>
<marker>R¨uping, Scheffer, 2005</marker>
<rawString>Stephan R¨uping and Tobias Scheffer. 2005. Learning with Multiple Views. In Proceedings of ICML Workshop on Learning with Multiple Views.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kanoksri Sarinnapakorn</author>
<author>Miroslav Kubat</author>
</authors>
<title>Combining Sub-classifiers in Text Categorization: A DST-Based Solution and a Case Study.</title>
<date>2007</date>
<journal>IEEE Transactions Knowledge and Data Engineering,</journal>
<pages>19--12</pages>
<contexts>
<context position="1166" citStr="Sarinnapakorn and Kubat, 2007" startWordPosition="150" endWordPosition="153">orrelations of domain-specific features from different domains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM significantly outperforms state-of-the-art baselines. 1 Introduction Domain adaptation has been shown useful to many natural language processing applications including document classification (Sarinnapakorn and Kubat, 2007), sentiment classification (Blitzer et al., 2007), part-of-speech tagging (Jiang and Zhai, 2007) and entity mention detection (Daum´e III and Marcu, 2006). Documents can be represented by multiple independent sets of features such as words and link structures of the documents. Multi-view learning aims to improve classifiers by leveraging the redundancy and consistency among these multiple views (Blum and Mitchell, 1998; R¨uping and Scheffer, 2005; Abney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class </context>
</contexts>
<marker>Sarinnapakorn, Kubat, 2007</marker>
<rawString>Kanoksri Sarinnapakorn and Miroslav Kubat. 2007. Combining Sub-classifiers in Text Categorization: A DST-Based Solution and a Case Study. IEEE Transactions Knowledge and Data Engineering, 19(12):1638-1651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Sridharan</author>
<author>Sham M Kakade</author>
</authors>
<title>An Information Theoretic Framework for Multi-view Learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 21st Annual Conference on Computational Learning Theory,</booktitle>
<pages>403--414</pages>
<contexts>
<context position="3285" citStr="Sridharan and Kakade, 2008" startWordPosition="453" endWordPosition="456"> target data. Results show that IMAM significantly outperforms the state-of-the-art baselines. 2 Related Work The work closely related to ours was done by Dai et al. (2007), where they proposed co-clusteringbased classification (CoCC) for adaptation learning. CoCC was extended from information-theoretic coclustering (Dhillon et al., 2003), where in-domain constraints were added to word clusters to provide the class structure and partial categorization knowledge. However, CoCC is a single-view algorithm. Although multi-view learning (Blum and Mitchell, 1998; Dasgupta et al., 2001; Abney, 2002; Sridharan and Kakade, 2008) is common within a single domain, it is not well studied under cross-domain settings. Chen et al. (2011) proposed 270 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270–274, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics CODA for adaptation based on co-training (Blum and Mitchell, 1998), which is however a pseudo multi-view algorithm where original data has only one view. Therefore, it is not suitable for the true multi-view case as ours. Zhang et al. (2011) proposed an instance-level multi-view transfe</context>
</contexts>
<marker>Sridharan, Kakade, 2008</marker>
<rawString>Karthik Sridharan and Sham M. Kakade. 2008. An Information Theoretic Framework for Multi-view Learning. In Proceedings of the 21st Annual Conference on Computational Learning Theory, pages 403-414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Zhang</author>
<author>Jingrui He</author>
<author>Yan Liu</author>
<author>Luo Si</author>
<author>Richard D Lawrence</author>
</authors>
<title>Multi-view Transfer Learning with a Large Margin Approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>1208--1216</pages>
<contexts>
<context position="3839" citStr="Zhang et al. (2011)" startWordPosition="539" endWordPosition="542">; Dasgupta et al., 2001; Abney, 2002; Sridharan and Kakade, 2008) is common within a single domain, it is not well studied under cross-domain settings. Chen et al. (2011) proposed 270 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270–274, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics CODA for adaptation based on co-training (Blum and Mitchell, 1998), which is however a pseudo multi-view algorithm where original data has only one view. Therefore, it is not suitable for the true multi-view case as ours. Zhang et al. (2011) proposed an instance-level multi-view transfer algorithm that integrates classification loss and view consistency terms based on large margin framework. However, instance-based approach is generally poor since new target features lack support from source data (Blitzer et al., 2011). We focus on feature-level multi-view adaptation. 3 Our Model Intuitively, source-specific and target-specific features can be drawn together by mining their co-occurrence with domain-independent (common) features, which helps bridge the distribution gap. Meanwhile, the view consistency on target data can be streng</context>
<context position="13751" citStr="Zhang et al., 2011" startWordPosition="2245" endWordPosition="2248">on’s correlation -y is nearly perfectly positive. This indicates that IMAM gradually improves adaptation by strengthening the view consistency. This is achieved by the reinforcement of word and link clusterings that draw together target- and source-specific features that are originally unrelated but co-occur with the common features. We compared IMAM with (1) Transductive SVM (TSVM) (Joachims, 1999) using both words and links features; (2) Co-Training (Blum and Mitchell, 1998); (3) CoCC (Dai et al., 2007): Co-clusteringbased single-view transfer learner (with text view only); and (4) MVTL-LM (Zhang et al., 2011): Large-margin-based multi-view transfer learner. Table 2 shows the results. Co-Training performed a little better than TSVM by boosting the confidence of classifiers built on the distinct views in a complementary way. But since Co-Training doesn’t consider the distribution gap, it performed clearly worse than CoCC even though CoCC has only one view. IMAM significantly outperformed CoCC on all the datasets. In average, the error rate of IMAM is 30.3% lower than that of CoCC. This is because IMAM effectively leverages distinct and complementary views. Compared to CoCC, using source training dat</context>
</contexts>
<marker>Zhang, He, Liu, Si, Lawrence, 2011</marker>
<rawString>Dan Zhang, Jingrui He, Yan Liu, Luo Si and Richard D. Lawrence. 2011. Multi-view Transfer Learning with a Large Margin Approach. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1208-1216.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>