<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015339">
<title confidence="0.997216">
CBSEAS, a Summarization System
Integration of Opinion Mining Techniques to Summarize Blogs
</title>
<author confidence="0.743062">
Aur´elien Bossard, Michel G´en´ereux and Thierry Poibeau
</author>
<note confidence="0.8629595">
Laboratoire d’Informatique de Paris-Nord
CNRS UMR 7030 and Universit´e Paris 13
</note>
<address confidence="0.729946">
93430 Villetaneuse — France
</address>
<email confidence="0.997572">
{firstname.lastname}@lipn.univ-paris13.fr
</email>
<sectionHeader confidence="0.99737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945923076923">
In this paper, we present a novel approach
for automatic summarization. Our system,
called CBSEAS, integrates a new method
to detect redundancy at its very core, and
produce more expressive summaries than
previous approaches. Moreover, we show
that our system is versatile enough to in-
tegrate opinion mining techniques, so that
it is capable of producing opinion oriented
summaries. The very competitive results
obtained during the last Text Evaluation
Conference (TAC 2008) show that our ap-
proach is efficient.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998400739130435">
During the past decade, automatic summarization,
supported by evaluation campaigns and a large re-
search community, has shown fast and deep im-
provements. Indeed, the research in this domain is
guided by strong industrial needs: fast processing
despite ever increasing amount of data.
In this paper, we present a novel approach for
automatic summarization. Our system, called CB-
SEAS, integrates a new method to detect redun-
dancy at its very core, and produce more expres-
sive summaries than previous approaches. The
system is flexible enough to produce opinion ori-
ented summaries by accommodating techniques to
mine documents that express different views or
commentaries. The very competitive results ob-
tained during the last Text Evaluation Conference
(TAC 2008) show that our approach is efficient.
This short paper is structured as follows: we
first give a quick overview of the state of the art.
We then describe our system, focusing on the most
important novel features implemented. Lastly, we
give the details of the results obtained on the TAC
2008 Opinion Pilot task.
</bodyText>
<sectionHeader confidence="0.999736" genericHeader="introduction">
2 Related works
</sectionHeader>
<bodyText confidence="0.999780076923077">
Interest in creating automatic summaries has be-
gun in the 1950s (Luhn, 1958). (Edmundson and
Wyllys, 1961) proposed features to assign a score
to each sentence of a corpus in order to rank these
sentences. The ones with the highest scores are
kept to produce the summary. The features they
used were sentence position (in a news article for
example, the first sentences are the most impor-
tant), proper names and keywords in the document
title, indicative phrases and sentence length.
Later on, summarizers aimed at eliminating re-
dundancy, especially for multi-documents summa-
rizing purpose. Identifying redundancy is a criti-
cal task, as information appearing several times in
different documents can be qualified as important.
Among recent approaches, the “centroid-based
summarization” method developed by (Radev et
al., 2004) consists in identifying the centroid
of a cluster of documents, in other words the
terms which best suit the documents to summa-
rize. Then, the sentences to be extracted are
the ones that contain the greatest number of cen-
troids. Radev implemented this method in an on-
line multi-document summarizer, MEAD.
Radev further improved MEAD using a differ-
ent method to extract sentences: “Graph-based
centrality” extractor (Erkan and Radev, 2004).
It consists in computing similarity between sen-
tences, and then selecting sentences which are
considered as “central” in a graph where nodes are
sentences and edges are similarities. Sentence se-
lection is then performed by picking the sentences
which have been visited most after a random walk
on the graph.
The last two systems are dealing with redun-
dancy as a post-processing step. (Zhu et al., 2007),
assuming that redundancy should be the concept
on what is based multi-document summarization,
offered a method to deal with redundancy at the
</bodyText>
<note confidence="0.3372385">
Proceedings of the EACL 2009 Demonstrations Session, pages 5–8,
Athens, Greece, 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.992258">
5
</page>
<bodyText confidence="0.999987230769231">
same time as sentence selection. For that purpose,
the authors used a “Markov absorbing chain ran-
dom walk” on a graph representing the different
sentences of the corpus to summarize.
MMR-MD, introduced by Carbonnel in (Car-
bonell and Goldstein, 1998), is a measure which
needs a passage clustering: all passages consid-
ered as synonyms are grouped into the same clus-
ters. MMR-MD takes into account the similarity
to a query, coverage of a passage (clusters that
it belongs to), content in the passage, similarity
to passages already selected for the summary, be-
longing to a cluster or to a document that has al-
ready contributed a passage to the summary.
The problem of this measure lies in the clus-
tering method: in the literature, clustering is gen-
erally fulfilled using a threshold. If a passage
has a similarity to a cluster centroid higher than
a threshold, then it is added to this cluster. This
makes it a supervised clustering method; an unsu-
pervised clustering method is best suited for au-
tomatic summarization, as the corpora we need
to summarize are different from one to another.
Moreover, sentence synonymy is also dependent
on the corpus granularity and on the user compres-
sion requirement.
</bodyText>
<sectionHeader confidence="0.986176333333333" genericHeader="method">
3 CBSEAS: A Clustering-Based
Sentence Extractor for Automatic
Summarization
</sectionHeader>
<bodyText confidence="0.999814714285714">
We assume that, in multi-document summariza-
tion, redundant pieces of information are the sin-
gle most important element to produce a good
summary. Therefore, the sentences which carry
those pieces of information have to be extracted.
Detecting these sentences conveying the same in-
formation is the first step of our approach. The de-
veloped algorithm first establishes the similarities
between all sentences of the documents to sum-
marize, then applies a clustering algorithm — fast
global k-means (L´opez-Escobar et al., 2006) — to
the similarity matrix in order to create clusters in
which sentences convey the same information.
First, our system ranks all the sentences accord-
ing to their similarity to the documents centroid.
We have chosen to build up the documents cen-
troid with the m most important terms, their im-
portance being reflected by the tf/idf of each term.
We then select the n2 best ranked sentences to cre-
ate a n sentences long summary. We do so because
the clustering algorithm we use to detect sentences
</bodyText>
<figureCaption confidence="0.998205">
Figure 1: Fast global k-means algorithm
</figureCaption>
<bodyText confidence="0.998916918918919">
conveying the same information, fast global k-
means, behaves better when it has to group n2
elements into n clusters. The similarity with the
centroid is a weighted sum of terms appearing in
both centroid and sentence, normalized by sen-
tence length.
Similarity between sentences is computed using
a variant of the “Jaccard” measure. If two terms
are not equal, we test their synonymy/hyperonymy
using the Wordnet taxonomy (Fellbaum, 1998). In
case they are synonyms or hyperonym/hyponym,
these terms are taken into account in the similar-
ity calculation, but weighted respectively half and
quarter in order to reflect that term equality is more
important than term semantic relation. We do this
in order to solve the problem pointed out in (Erkan
and Radev, 2004) (synonymy was not taken into
account for sentence similarity measures) and so
to enhance sentence similarity measure. It is cru-
cial to our system based on redundancy location as
redundancy assumption is dependent on sentence
similarities.
Once the similarities are computed, we cluster
the sentences using fast global k-means (descrip-
tion of the algorithm is in figure 1) using the simi-
larity matrix. It works well on a small data set with
a small number of dimensions, although it has not
yet scaled up as well as we would have expected.
This clustering step completed, we select one
sentence per cluster in order to produce a sum-
mary that contains most of the relevant informa-
tion/ideas in the original documents. We do so by
choosing the central sentence in each cluster. The
central sentence is the one which maximizes the
sum of similarities with the other sentences of its
cluster. It should be the one that characterizes best
the cluster in terms of information vehicled.
</bodyText>
<table confidence="0.843772428571429">
for all ejinE
C, +— ej
for i from 1 to k do
for j from 1 to i
center(Cj) +— em|emmaximizes sim(em, en)
eninCj
for all ej in E
</table>
<bodyText confidence="0.79752175">
ej __+ Cl|Clmaximizes sim(center(Cl, ej))
add a new cluster: Ci. It initially contains only its
center, the worst represented element in its cluster.
done
</bodyText>
<page confidence="0.997881">
6
</page>
<sectionHeader confidence="0.8902995" genericHeader="method">
4 TAC 2008: The Opinion
Summarization Task
</sectionHeader>
<bodyText confidence="0.999850466666667">
In order to evaluate our system, we participated
in the Text Analysis Conference (TAC) that pro-
posed in 2008 an opinion summarization task. The
goal is to produce fluent and well-organized sum-
maries of blogs. These summaries are oriented
by complex user queries, such as “Why do people
like ?” or “Why do people prefer... to...?”.
The results were analyzed manually, using the
PYRAMID method (Lin et al., 2006): the PYRA-
MID score of a summary depends on the number
of simple semantic units, units considered as im-
portant by the annotators. The TAC evaluation
for this task also included grammaticality, non-
redundancy, structure/coherence and overall flu-
ency scores.
</bodyText>
<sectionHeader confidence="0.956129" genericHeader="method">
5 CBSEAS Adaptation to the Opinion
Summarization Task
</sectionHeader>
<bodyText confidence="0.99999084">
Blog summarization is very different from a
newswire article or a scientific paper summa-
rization. Linguistic quality as well as reason-
ing structure are variable from one blogger to an-
other. We cannot use generalities on blog struc-
ture, neither on linguistic markers to improve
our summarization system. The other problem
with blogs is the noise due to the use of un-
usual language. We had to clean the blogs in a
pre-processing step: sentences with a ratio num-
ber offrequent words/total number of words below
a given threshold (0.35) were deemed too noisy
and discarded. Frequent words are the one hun-
dred most frequent words in the English language
which on average make up approximately half of
written texts (Fry et al., 2000).
Our system, CBSEAS, is a “standard” summa-
rization system. We had to adapt it in order to
deal with the specific task of summarizing opin-
ions. All sentences from the set of documents to
summarize were tagged following the opinion de-
tected in the blog post they originated from. We
used for that purpose a two-class (positive or neg-
ative) SVM classifier trained on movie reviews.
The idea behind the opinion classifier is to im-
prove summaries by selecting sentences having
the same opinionated polarity as the query, which
were tagged using a SVM trained on the manually
tagged queries from the training data provided ear-
lier in TAC.
As the Opinion Summarization Task was to pro-
duce a query-oriented summary, the sentence pre-
selection was changed, using the user query in-
stead of the documents centroid. We also changed
the sentence pre-selection ranking measure by
weighting terms according to their lexical cate-
gory; we have chosen to give more weight to
proper names than verbs adjectives, adverbs and
nouns. Indeed, opinions we had to summarize
were mostly on products or people.
While experimenting our system on TAC 2008
training data, we noticed that extracting sentences
which are closest to their cluster center was not
satisfactory. Some other sentences in the same
cluster were best fitted to a query-oriented sum-
mary. We added the sentence ranking used for the
sentence pre-selection to the final sentence extrac-
tor. Each sentence is given a score which is the
distance to the cluster center times the similarity
to the query.
</bodyText>
<sectionHeader confidence="0.7912435" genericHeader="method">
6 TAC 2008 Results on Opinion
Summarization Task
</sectionHeader>
<bodyText confidence="0.999758928571429">
Participants to the Opinion Summarization Task
were allowed to use extra-information given by
TAC organizers. These pieces of information are
called snippets. The snippets contain the relevant
information, and could be used as a stand-alone
dataset. Participants were classified into two dif-
ferent groups: one for those who did not use snip-
pets, and one for those who did. We did not use
snippets at all, as it is a more realistic challenge
to look directly at the blogs with no external help.
The results we present here are those of the partic-
ipants that were not using snippets. Indeed, sys-
tems using snippets obtained much higher scores
than the other systems. We cannot compare our
system to systems using snippets.
Our system obtained quite good results on
the “opinion task”: the scores can be found on
figure 2. As one can see, our responsiveness
scores are low compared to the others (responsive-
ness score corresponds to the following question:
“How much would you pay for that summary?”).
We suppose that despite the grammaticality, flu-
ency and pyramid scores of our summaries, judges
gave a bad responsiveness score to our summaries
because they are too long: we made the choice
to produce summaries with a compression rate of
10% when it was possible, the maximum length
authorized otherwise.
</bodyText>
<page confidence="0.999253">
7
</page>
<table confidence="0.998119857142857">
Evaluation CBSEAS Mean Best Worst Rank
Pyramid .169 .151 .251 .101 5/20
Grammatic. 5.95 5.14 7.54 3.54 3/20
Non-redun. 6.64 5.88 7.91 4.36 4/20
Structure 3.50 2.68 3.59 2.04 2/20
Fluency 4.45 3.43 5.32 2.64 2/20
Responsiv. 2.64 2.61 5.77 1.68 8/20
</table>
<figureCaption confidence="0.9997915">
Figure 2: Opinion task overall results
Figure 3: Opinion task results
</figureCaption>
<bodyText confidence="0.9983891">
However, we noticed that the quality of our
summaries was very erratic. We assume this is
due to the length of our summaries, as the longest
summaries are the ones which get the worst scores
in terms of pyramid f-score (fig 3). The length of
the summaries is a ratio of the original documents
length. The quality of the summaries would be
decreasing while the number of input sentences is
increasing.
Solutions to fix this problem could be:
</bodyText>
<listItem confidence="0.98928925">
• Define a better score for the correspondence
to a user query and remove sentences which
are under a threshold;
• Extract sentences from the clusters that con-
</listItem>
<bodyText confidence="0.9933155">
tain more than a predefined number of ele-
ments only.
This would result in improving the pertinence
of the extracted sentences. The users reading the
summaries would also be less disturbed by the
large amount of sentences a too long summary
provides. As the “opinion summarization” task
was evaluated manually and reflects well the qual-
ity of a summary for an operational use, the con-
clusions of this evaluation are good indicators of
the quality of the summaries produced by our sys-
tem.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999987615384615">
We presented here a new approach for multi-
document summarization. It uses an unsuper-
vised clustering method to group semantically re-
lated sentences together. It can be compared to
approaches using sentence neighbourhood (Erkan
and Radev, 2004), because the sentences which are
highly related to the highest number of sentences
are those which will be extracted first. How-
ever, our approach is different since sentence se-
lection is directly dependent on redundancy loca-
tion. Also, redundancy elimination, which is cru-
cial in multi-document summarization, takes place
in the same step as sentence selection.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999797">
Jaime Carbonell and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In SIGIR’98,
pages 335–336, New York, NY, USA. ACM.
Harold P. Edmundson and Ronald E. Wyllys. 1961.
Automatic abstracting and indexing—survey and
recommendations. Commun. ACM, 4(5):226–234.
G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Edward Bernard Fry, Jacqueline E. Kress, and
Dona Lee Fountoukidis. 2000. The Reading Teach-
ers Book of Lists. Jossey-Bass, 4th edition.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-
Yun Nie. 2006. An information-theoretic approach
to automatic evaluation of summaries. In Proceed-
ings of HLT-NAACL, pages 463–470, Morristown,
NJ, USA.
Sa´ul L´opez-Escobar, Jes´us Ariel Carrasco-Ochoa, and
Jos´e Francisco Martinez Trinidad. 2006. Fast
global -means with similarity functions algorithm.
In IDEAL, volume 4224 of Springer, Lecture Notes
in Computer Science, pages 512–521.
H.P. Luhn. 1958. The automatic creation of literature
abstracts. IBMJournal, 2(2):159–165.
Dragomir Radev et al. 2004. MEAD - a platform for
multidocument multilingual text summarization. In
Proceedings of LREC 2004, Lisbon, Portugal.
Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and
David Andrzejewski. 2007. Improving diversity
in ranking using absorbing random walks. In Pro-
ceedings of HLT-NAACL, pages 97–104, Rochester,
USA.
</reference>
<page confidence="0.998491">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444971">
<title confidence="0.9846865">CBSEAS, a Summarization System Integration of Opinion Mining Techniques to Summarize Blogs</title>
<author confidence="0.896902">Michel G´en´ereux Poibeau Bossard</author>
<affiliation confidence="0.629034">Laboratoire d’Informatique de Paris-Nord</affiliation>
<address confidence="0.791919">CNRS UMR 7030 and Universit´e Paris 13 93430 Villetaneuse — France</address>
<abstract confidence="0.967850428571429">In this paper, we present a novel approach for automatic summarization. Our system, called CBSEAS, integrates a new method to detect redundancy at its very core, and produce more expressive summaries than previous approaches. Moreover, we show that our system is versatile enough to integrate opinion mining techniques, so that it is capable of producing opinion oriented summaries. The very competitive results obtained during the last Text Evaluation Conference (TAC 2008) show that our approach is efficient.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In SIGIR’98,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4131" citStr="Carbonell and Goldstein, 1998" startWordPosition="633" endWordPosition="637">e last two systems are dealing with redundancy as a post-processing step. (Zhu et al., 2007), assuming that redundancy should be the concept on what is based multi-document summarization, offered a method to deal with redundancy at the Proceedings of the EACL 2009 Demonstrations Session, pages 5–8, Athens, Greece, 3 April 2009. c�2009 Association for Computational Linguistics 5 same time as sentence selection. For that purpose, the authors used a “Markov absorbing chain random walk” on a graph representing the different sentences of the corpus to summarize. MMR-MD, introduced by Carbonnel in (Carbonell and Goldstein, 1998), is a measure which needs a passage clustering: all passages considered as synonyms are grouped into the same clusters. MMR-MD takes into account the similarity to a query, coverage of a passage (clusters that it belongs to), content in the passage, similarity to passages already selected for the summary, belonging to a cluster or to a document that has already contributed a passage to the summary. The problem of this measure lies in the clustering method: in the literature, clustering is generally fulfilled using a threshold. If a passage has a similarity to a cluster centroid higher than a </context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR’98, pages 335–336, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold P Edmundson</author>
<author>Ronald E Wyllys</author>
</authors>
<title>Automatic abstracting and indexing—survey and recommendations.</title>
<date>1961</date>
<journal>Commun. ACM,</journal>
<volume>4</volume>
<issue>5</issue>
<contexts>
<context position="2029" citStr="Edmundson and Wyllys, 1961" startWordPosition="304" endWordPosition="307">ummaries by accommodating techniques to mine documents that express different views or commentaries. The very competitive results obtained during the last Text Evaluation Conference (TAC 2008) show that our approach is efficient. This short paper is structured as follows: we first give a quick overview of the state of the art. We then describe our system, focusing on the most important novel features implemented. Lastly, we give the details of the results obtained on the TAC 2008 Opinion Pilot task. 2 Related works Interest in creating automatic summaries has begun in the 1950s (Luhn, 1958). (Edmundson and Wyllys, 1961) proposed features to assign a score to each sentence of a corpus in order to rank these sentences. The ones with the highest scores are kept to produce the summary. The features they used were sentence position (in a news article for example, the first sentences are the most important), proper names and keywords in the document title, indicative phrases and sentence length. Later on, summarizers aimed at eliminating redundancy, especially for multi-documents summarizing purpose. Identifying redundancy is a critical task, as information appearing several times in different documents can be qua</context>
</contexts>
<marker>Edmundson, Wyllys, 1961</marker>
<rawString>Harold P. Edmundson and Ronald E. Wyllys. 1961. Automatic abstracting and indexing—survey and recommendations. Commun. ACM, 4(5):226–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR).</journal>
<contexts>
<context position="3189" citStr="Erkan and Radev, 2004" startWordPosition="485" endWordPosition="488">tion appearing several times in different documents can be qualified as important. Among recent approaches, the “centroid-based summarization” method developed by (Radev et al., 2004) consists in identifying the centroid of a cluster of documents, in other words the terms which best suit the documents to summarize. Then, the sentences to be extracted are the ones that contain the greatest number of centroids. Radev implemented this method in an online multi-document summarizer, MEAD. Radev further improved MEAD using a different method to extract sentences: “Graph-based centrality” extractor (Erkan and Radev, 2004). It consists in computing similarity between sentences, and then selecting sentences which are considered as “central” in a graph where nodes are sentences and edges are similarities. Sentence selection is then performed by picking the sentences which have been visited most after a random walk on the graph. The last two systems are dealing with redundancy as a post-processing step. (Zhu et al., 2007), assuming that redundancy should be the concept on what is based multi-document summarization, offered a method to deal with redundancy at the Proceedings of the EACL 2009 Demonstrations Session,</context>
<context position="6983" citStr="Erkan and Radev, 2004" startWordPosition="1104" endWordPosition="1107">e centroid is a weighted sum of terms appearing in both centroid and sentence, normalized by sentence length. Similarity between sentences is computed using a variant of the “Jaccard” measure. If two terms are not equal, we test their synonymy/hyperonymy using the Wordnet taxonomy (Fellbaum, 1998). In case they are synonyms or hyperonym/hyponym, these terms are taken into account in the similarity calculation, but weighted respectively half and quarter in order to reflect that term equality is more important than term semantic relation. We do this in order to solve the problem pointed out in (Erkan and Radev, 2004) (synonymy was not taken into account for sentence similarity measures) and so to enhance sentence similarity measure. It is crucial to our system based on redundancy location as redundancy assumption is dependent on sentence similarities. Once the similarities are computed, we cluster the sentences using fast global k-means (description of the algorithm is in figure 1) using the similarity matrix. It works well on a small data set with a small number of dimensions, although it has not yet scaled up as well as we would have expected. This clustering step completed, we select one sentence per c</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6659" citStr="Fellbaum, 1998" startWordPosition="1052" endWordPosition="1053">t the n2 best ranked sentences to create a n sentences long summary. We do so because the clustering algorithm we use to detect sentences Figure 1: Fast global k-means algorithm conveying the same information, fast global kmeans, behaves better when it has to group n2 elements into n clusters. The similarity with the centroid is a weighted sum of terms appearing in both centroid and sentence, normalized by sentence length. Similarity between sentences is computed using a variant of the “Jaccard” measure. If two terms are not equal, we test their synonymy/hyperonymy using the Wordnet taxonomy (Fellbaum, 1998). In case they are synonyms or hyperonym/hyponym, these terms are taken into account in the similarity calculation, but weighted respectively half and quarter in order to reflect that term equality is more important than term semantic relation. We do this in order to solve the problem pointed out in (Erkan and Radev, 2004) (synonymy was not taken into account for sentence similarity measures) and so to enhance sentence similarity measure. It is crucial to our system based on redundancy location as redundancy assumption is dependent on sentence similarities. Once the similarities are computed, </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Bernard Fry</author>
<author>Jacqueline E Kress</author>
<author>Dona Lee Fountoukidis</author>
</authors>
<title>The Reading Teachers Book of Lists.</title>
<date>2000</date>
<note>Jossey-Bass, 4th edition.</note>
<contexts>
<context position="9737" citStr="Fry et al., 2000" startWordPosition="1572" endWordPosition="1575">ality as well as reasoning structure are variable from one blogger to another. We cannot use generalities on blog structure, neither on linguistic markers to improve our summarization system. The other problem with blogs is the noise due to the use of unusual language. We had to clean the blogs in a pre-processing step: sentences with a ratio number offrequent words/total number of words below a given threshold (0.35) were deemed too noisy and discarded. Frequent words are the one hundred most frequent words in the English language which on average make up approximately half of written texts (Fry et al., 2000). Our system, CBSEAS, is a “standard” summarization system. We had to adapt it in order to deal with the specific task of summarizing opinions. All sentences from the set of documents to summarize were tagged following the opinion detected in the blog post they originated from. We used for that purpose a two-class (positive or negative) SVM classifier trained on movie reviews. The idea behind the opinion classifier is to improve summaries by selecting sentences having the same opinionated polarity as the query, which were tagged using a SVM trained on the manually tagged queries from the train</context>
</contexts>
<marker>Fry, Kress, Fountoukidis, 2000</marker>
<rawString>Edward Bernard Fry, Jacqueline E. Kress, and Dona Lee Fountoukidis. 2000. The Reading Teachers Book of Lists. Jossey-Bass, 4th edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Guihong Cao</author>
<author>Jianfeng Gao</author>
<author>JianYun Nie</author>
</authors>
<title>An information-theoretic approach to automatic evaluation of summaries.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>463--470</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8699" citStr="Lin et al., 2006" startWordPosition="1400" endWordPosition="1403">j in E ej __+ Cl|Clmaximizes sim(center(Cl, ej)) add a new cluster: Ci. It initially contains only its center, the worst represented element in its cluster. done 6 4 TAC 2008: The Opinion Summarization Task In order to evaluate our system, we participated in the Text Analysis Conference (TAC) that proposed in 2008 an opinion summarization task. The goal is to produce fluent and well-organized summaries of blogs. These summaries are oriented by complex user queries, such as “Why do people like ?” or “Why do people prefer... to...?”. The results were analyzed manually, using the PYRAMID method (Lin et al., 2006): the PYRAMID score of a summary depends on the number of simple semantic units, units considered as important by the annotators. The TAC evaluation for this task also included grammaticality, nonredundancy, structure/coherence and overall fluency scores. 5 CBSEAS Adaptation to the Opinion Summarization Task Blog summarization is very different from a newswire article or a scientific paper summarization. Linguistic quality as well as reasoning structure are variable from one blogger to another. We cannot use generalities on blog structure, neither on linguistic markers to improve our summariza</context>
</contexts>
<marker>Lin, Cao, Gao, Nie, 2006</marker>
<rawString>Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and JianYun Nie. 2006. An information-theoretic approach to automatic evaluation of summaries. In Proceedings of HLT-NAACL, pages 463–470, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sa´ul L´opez-Escobar</author>
<author>Jes´us Ariel Carrasco-Ochoa</author>
<author>Jos´e Francisco Martinez Trinidad</author>
</authors>
<title>Fast global -means with similarity functions algorithm.</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science,</journal>
<booktitle>In IDEAL,</booktitle>
<volume>4224</volume>
<pages>512--521</pages>
<publisher>Springer,</publisher>
<marker>L´opez-Escobar, Carrasco-Ochoa, Trinidad, 2006</marker>
<rawString>Sa´ul L´opez-Escobar, Jes´us Ariel Carrasco-Ochoa, and Jos´e Francisco Martinez Trinidad. 2006. Fast global -means with similarity functions algorithm. In IDEAL, volume 4224 of Springer, Lecture Notes in Computer Science, pages 512–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBMJournal,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="1999" citStr="Luhn, 1958" startWordPosition="302" endWordPosition="303">ion oriented summaries by accommodating techniques to mine documents that express different views or commentaries. The very competitive results obtained during the last Text Evaluation Conference (TAC 2008) show that our approach is efficient. This short paper is structured as follows: we first give a quick overview of the state of the art. We then describe our system, focusing on the most important novel features implemented. Lastly, we give the details of the results obtained on the TAC 2008 Opinion Pilot task. 2 Related works Interest in creating automatic summaries has begun in the 1950s (Luhn, 1958). (Edmundson and Wyllys, 1961) proposed features to assign a score to each sentence of a corpus in order to rank these sentences. The ones with the highest scores are kept to produce the summary. The features they used were sentence position (in a news article for example, the first sentences are the most important), proper names and keywords in the document title, indicative phrases and sentence length. Later on, summarizers aimed at eliminating redundancy, especially for multi-documents summarizing purpose. Identifying redundancy is a critical task, as information appearing several times in </context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H.P. Luhn. 1958. The automatic creation of literature abstracts. IBMJournal, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
</authors>
<title>MEAD - a platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="3189" citStr="Radev, 2004" startWordPosition="487" endWordPosition="488">ring several times in different documents can be qualified as important. Among recent approaches, the “centroid-based summarization” method developed by (Radev et al., 2004) consists in identifying the centroid of a cluster of documents, in other words the terms which best suit the documents to summarize. Then, the sentences to be extracted are the ones that contain the greatest number of centroids. Radev implemented this method in an online multi-document summarizer, MEAD. Radev further improved MEAD using a different method to extract sentences: “Graph-based centrality” extractor (Erkan and Radev, 2004). It consists in computing similarity between sentences, and then selecting sentences which are considered as “central” in a graph where nodes are sentences and edges are similarities. Sentence selection is then performed by picking the sentences which have been visited most after a random walk on the graph. The last two systems are dealing with redundancy as a post-processing step. (Zhu et al., 2007), assuming that redundancy should be the concept on what is based multi-document summarization, offered a method to deal with redundancy at the Proceedings of the EACL 2009 Demonstrations Session,</context>
<context position="6983" citStr="Radev, 2004" startWordPosition="1106" endWordPosition="1107"> is a weighted sum of terms appearing in both centroid and sentence, normalized by sentence length. Similarity between sentences is computed using a variant of the “Jaccard” measure. If two terms are not equal, we test their synonymy/hyperonymy using the Wordnet taxonomy (Fellbaum, 1998). In case they are synonyms or hyperonym/hyponym, these terms are taken into account in the similarity calculation, but weighted respectively half and quarter in order to reflect that term equality is more important than term semantic relation. We do this in order to solve the problem pointed out in (Erkan and Radev, 2004) (synonymy was not taken into account for sentence similarity measures) and so to enhance sentence similarity measure. It is crucial to our system based on redundancy location as redundancy assumption is dependent on sentence similarities. Once the similarities are computed, we cluster the sentences using fast global k-means (description of the algorithm is in figure 1) using the similarity matrix. It works well on a small data set with a small number of dimensions, although it has not yet scaled up as well as we would have expected. This clustering step completed, we select one sentence per c</context>
</contexts>
<marker>Radev, 2004</marker>
<rawString>Dragomir Radev et al. 2004. MEAD - a platform for multidocument multilingual text summarization. In Proceedings of LREC 2004, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Andrew Goldberg</author>
<author>Jurgen Van Gael</author>
<author>David Andrzejewski</author>
</authors>
<title>Improving diversity in ranking using absorbing random walks.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>97--104</pages>
<location>Rochester, USA.</location>
<marker>Zhu, Goldberg, Van Gael, Andrzejewski, 2007</marker>
<rawString>Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski. 2007. Improving diversity in ranking using absorbing random walks. In Proceedings of HLT-NAACL, pages 97–104, Rochester, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>