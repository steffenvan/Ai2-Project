<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010964">
<title confidence="0.969655">
Complete Syntactic Analysis Based on Multi-level Chunking
</title>
<author confidence="0.9357815">
ZhiPeng Jiang and Yu Zhao and Yi Guan and
Chao Li and Sheng Li
</author>
<affiliation confidence="0.867788666666667">
School of Computer Science and Technology,
Harbin Institute of Technology,
150001, Harbin, China
</affiliation>
<email confidence="0.780554333333333">
xyf-3456@163.com; woshizhaoy@gmail.com
guanyi@hit.edu.cn; beyondlee2008@yahoo.cn
lisheng@hit.edu.cn
</email>
<sectionHeader confidence="0.981356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986125">
This paper describes a complete syntactic
analysis system based on multi-level
chunking. On the basis of the correct se-
quences of Chinese words provided by
CLP2010, the system firstly has a Part-of-
speech (POS) tagging with Conditional
Random Fields (CRFs), and then does the
base chunking and complex chunking with
Maximum Entropy (ME), and finally gene-
rates a complete syntactic analysis tree.
The system took part in the Complete Sen-
tence Parsing Track of the Task 2 Chinese
Parsing in CLP2010, achieved the F-1
measure of 63.25% on the overall analysis,
ranked the sixth; POS accuracy rate of
89.62%, ranked the third.
</bodyText>
<sectionHeader confidence="0.992499" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965081632653">
Chunk is a group of adjacent words which belong
to the same s-projection set in a sentence, whose
syntactic structure is actually a tree (Abney, 1991),
but apart from the root node, all other nodes are
leaf nodes. Complete syntactic analysis requires a
series of analyzing processes, eventually to get a
full parsing tree. Parsing by chunks is proved to be
feasible (Abney, 1994).
The concept of chunking was first proposed by
Abney in 1991, who defined chunks in terms of
major heads, and parsed by chunks in 1994 (Ab-
ney, 1994). An additional chunk tag set {B, I, O}
was added to chunking (Ramshaw and Marcus,
1995), which limited dependencies between ele-
ments in a chunk, changed chunking into a ques-
tion of sequenced tags, to promote the develop-
ment of chunking. Chunking algorithm was ex-
tended to the bottom-up parser, which is trained
and tested on the Wall Street Journal (WSJ) part of
the Penn Treebank (Marcus, Santorini and Mar-
cinkiewicz 1993), and achieved a performance of
80.49% F-measure, the results show that it per-
formed better than a standard probabilistic con-
text-free grammar, and can improve performance
by adding the information of parent node (Sang,
2000).
On Chinese parsing, Maximum Entropy Model
was first used to have a POS tagging and chunking,
and then a full parsing tree was generated (Fung,
2004), training and testing in the Penn Chinese
Treebank, which achieved 79.56% F-measure. The
parsing process was divided into POS tagging,
base chunking and complex chunking, having a
POS tagging and chunking on a given sentence,
and then looping the process of complex chunking
up to identify the root node (Li and Zhou, 2009).
This parsing method is the basis of this paper. In
addition, we have the existing Chinese chunking
system in laboratory, which ranked first in Task 2:
Chinese Base Chunking of CIPS-ParsEval-2009,
so we try to apply chunking to complete syntactic
analysis in CLP2010, to achieve better results.
We will describe the POS tagging based on
CRFs in Section 2, including CRFs, feature tem-
plate selection and empirical results. Multi-level
chunking based on ME will be expounded in Sec-
tion 3, including ME, MEMM, base chunking and
complex chunking. Finally, we will summarize our
work in Section 4.
</bodyText>
<sectionHeader confidence="0.873424" genericHeader="method">
2 POS Tagging Based on CRFs
</sectionHeader>
<subsectionHeader confidence="0.989231">
2.1 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.999527666666667">
X is a random variable over data sequences to be
labeled, and Y is a random variable over corres-
ponding label sequences. All components Yi of Y
are assumed to range over a finite label alphabet.
For example, X might range over natural language
sentences and Y range over part-of-speech tags of
those sentences, a finite label alphabet is the set of
possible part-of-speech tags (Lafferty and McCal-
lum and Pereira, 2001). CRFs is represented by the
local feature vector f and the corresponding weight
vector, f is divided into the state feature s (y, x, i)
and transfer feature t (y, y&apos;, x, i), where y and y&apos; are
possible POS tags, x is the current input sentence, i
is the position of current term (Jiang and Guan and
Wang, 2006). Formalized as follows:
</bodyText>
<equation confidence="0.9995165">
s (y, x, i) = s (yi, x, i) (1)
� t(yi-1,yi,x,i) i&gt;1
(y,x,i) = �
� l
t � (2)
0 i=1
arg max   |
p  y x
</equation>
<bodyText confidence="0.9567385">
By the local feature of the formula (1) and (2),
the global features of x and y:
</bodyText>
<equation confidence="0.997737">
P(Y,x)=Lif(YIX,i) (3)
</equation>
<bodyText confidence="0.997641">
At this point of (X, Y), the conditional probabil-
ity distribution of CRFs:
</bodyText>
<equation confidence="0.997815">
Px (Y  |X) = exp(x F(Y,X))
Zx(X)
</equation>
<bodyText confidence="0.94309725">
where is a fac-
tor for normalizing. For the input sentence x, the
best sequence of POS tagging:
y
</bodyText>
<subsectionHeader confidence="0.999266">
2.2 Feature Template Selection
</subsectionHeader>
<bodyText confidence="0.999991615384616">
We use the template as a baseline which is taken
by Yang (2009) in CIPS-ParsEval-2009, directly
testing the performance, whose accuracy was
93.52%. On this basis, we adjust the feature tem-
plate through the experiment, and improve the
tagging accuracy of unknown words by introduc-
ing rules, in the same corpus for training and test-
ing, accuracy is to 93.89%. Adjusted feature tem-
plate is shown in Table 1, in which the term pre is
the first character in current word, suf is the last
character of current word, num is the number of
characters of current word, pos-1 is the tagging re-
sults of the previous word.
</bodyText>
<tableCaption confidence="0.993576">
Table 1: feature template
</tableCaption>
<bodyText confidence="0.986657666666667">
feature template
w2,w1,w0,w-1,w-2,w+1w0,w0w-1,pre0, pre0w0,suf0,
w0suf0,num,pos-1
</bodyText>
<subsectionHeader confidence="0.999876">
2.3 Empirical Results and Analysis
</subsectionHeader>
<bodyText confidence="0.9967288">
We divide the training data provided by CLP2010
into five folds, the first four of which are train cor-
pus, the last one is test corpus, on which we use
the CRF++ toolkit for training and testing. Tag-
ging results with different features are shown in
</bodyText>
<tableCaption confidence="0.9721625">
table 2.
Table 2: tagging results with different features
</tableCaption>
<table confidence="0.995404857142857">
Model Explain Accuracy
CRF baseline 93.52%
CRF1 add w-1, pos-1 93.58%
CRF2 add num 93.66%
CRF3 add num, w-1, pos-1 93.68%
CRF4 add num, rules 93.80%
CRF5 add num, w-1, pos-1, rules 93.89%
</table>
<bodyText confidence="0.999779666666667">
Tagging results show that the number of charac-
ter and POS information can be added to improve
the accuracy of tagging, but in CLP2010, the tag-
ging accuracy is only 89.62%, on the one hand it
may be caused by differences of corpus, on the
other hand it may be due to that we don’t use all
the features of CRFs but remove the features
which appear one time in order to reduce the train-
ing time.
</bodyText>
<sectionHeader confidence="0.993888" genericHeader="method">
3 Multi-level Chunking Based on ME
</sectionHeader>
<subsectionHeader confidence="0.9675925">
3.1 Maximum Entropy Models and Maxi-
mum Entropy Markov Models
</subsectionHeader>
<bodyText confidence="0.9995235">
Maximum entropy model is mainly used to esti-
mate the unknown probability distribution whose
entropy is the maximum under some existing con-
ditions. Suppose h is the observations of context, t
is tag, the conditional probability p (t  |h) can be
expressed as:
</bodyText>
<equation confidence="0.9226866">
exp(¦iOif i(t,h))
P(t|h) Z(h)
where fi is the feature of model,
Z(h)  Z exp(Z A i f i(t, h)) is a factor for nor-
t i
</equation>
<bodyText confidence="0.999504285714286">
malizing. is weigh of feature fi, training is the
process of seeking the value of .
Maximum entropy Markov model is the seria-
lized form of Maximum entropy model (McCal-
lum and Freitag and Pereira, 2000), for example,
transition probabilities and emission probabilities
are merged into a single conditional probability
</bodyText>
<equation confidence="0.818612">
(4)

y
</equation>
<bodyText confidence="0.9829792">
function in binary Maximum entropy
Markov model, is turned to to
be solved by adding features which can express
previously tagging information (Li and Sun and
Guan, 2009).
</bodyText>
<subsectionHeader confidence="0.999845">
3.2 Base Chunking
</subsectionHeader>
<bodyText confidence="0.929130954545454">
Following the method of multi-level chunking, we
first do the base chunking on the sentences which
are through the POS tagging, then loop the process
of complex chunking until they can’t be merged.
We use the existing Chinese base chunking system
to do base chunking in laboratory, which marks
boundaries and composition information of chunk
with MEMM, and achieved 93.196% F-measure in
Task 2: Chinese Base Chunking of CIPS-ParsEval
-2009. The input and output of base chunking are
as follows:
Input:中国/nS 传统/a 医学/n 是/v 中华/nR 民
族/n 在/p 长期/n 的/uJDE 医疗/n 、/wD 生活/n
实践/vN 中/f ,/wP 不断/d 积累/v ,/wP 反复/d
总结/v 而/c 逐渐/d 形成/v 的/uJDE 具有/v 独特
/a 理论/n 风格/n 的/uJDE 医学/n 体系/n 。/wE
Output:中国/nS [np 传统/a 医学/n ] 是/v [np 中
华/nR 民族/n ] 在/p 长期/n 的/uJDE [np 医疗
/n 、/wD 生活/n ] 实践/vN 中/f ,/wP [vp 不断
/d 积累/v ] ,/wP [vp 反复/d 总结/v ] 而/c [vp
逐渐/d 形成/v ] 的/uJDE 具有/v [np 独特/a 理论
/n ] 风格/n 的/uJDE [np 医学/n 体系/n ] 。/wE
</bodyText>
<subsectionHeader confidence="0.998255">
3.3 Complex Chunking
</subsectionHeader>
<bodyText confidence="0.997552052631579">
We take the sentences which are through POS tag-
ging and base chunking as input, using Li’s tag-
ging method and feature template. Categories of
complex chunk include xx_Start, xx_Middle,
xx_End and Other, where xx is a category of arbi-
trary chunk. The process of complex chunking is
shown as follows:
Step 1: input the sentences which are through POS
tagging and base chunking, for example:
中国/nS [np 传统/a 医学/n ] 的/uJDE [np 发生
/vN 发展/vN ] 及/c [np 学术/n 特点/n ]
Step 2: if there are some category tags in the sen-
tence, then turn a series of tags to brackets, for
instance, if continuous cells are marked as
xx_Start, xx_Middle, ..., xx_Middle, xx_End, then
the combination of continuous cells is a complex
chunk xx;
Step 3: determine the head words with the set of
rules, and compress the sentence:
</bodyText>
<equation confidence="0.8994415">
中国/nS [np 医学/n ] 的/uJDE [np 发展/vN ] 及
/c [np 特点/n ]
</equation>
<bodyText confidence="0.998972692307692">
Step 4: if the sentence can be merged, mark the
sentence with ME, then return step 2, else the
analysis process ends:
中国/nS@np_Start [np 医学/n ]@np_End 的
/uJDE@Other [np 发 展 /vN ]@np_Start 及
/c@np_Middle [np 特点/n ]@np_End
At last, the output is:
[np [np 中国/nS [np 传统/a 医学/n ] ] 的/uJDE
[np [np 发生/vN 发展/vN ] 及/c [np 学术/n 特
点/n ] ] ]
Following the above method, we first use the
Viterbi decoding, but in the decoding process we
encountered two problems:
</bodyText>
<listItem confidence="0.989560375">
1. Similar to the label xx_Start, whose back is only
xx_Middle or xx_End, so the probability of
xx_Start label turning to Other is 0, But, if only
using ME to predict, the probability may not be 0.
2. Viterbi decoding can’t solve that all the labels of
predicted results are Other, if all labels are Other,
they can’t be merged, this result doesn’t make
sense.
</listItem>
<bodyText confidence="0.982868854166667">
Solution:
For the first question, we add the initial transfer
matrix and the end transfer matrix in decoding
process, that is, the corresponding xx_Middle or
xx_End of xx_Start is seted to 1 in the transfer
matrix, the others are marked as 0, matrix multip-
lication is taken during the state transition. It can
effectively avoid errors caused by probability to
improve accuracy.
To rule out the second question, we use heuris-
tic search approach to decode, and exclude all
Other labels with the above matrix. In addition, we
defined another ME classifier to do some pruning
in the decoding process, the features of ME clas-
sifier are POS, the head word, the POS of head
word. The pseudo-code of Heuristic search is:
While searching priority queue is not empty
Take the node with the greatest priority in the
queue;
If the node’s depth = length of the chunking
results
Searching is over, reverse the search-
ing path to get searching results;
Else
Compute the probability of all candi-
date children nodes according to
the current probability;
Record searching path;
Press it into the priority queue;
In addition, we found that some punctuation at
the end of a sentence can’t be merged, probably
due to sparseness of data, according to that the
tone punctuation (period, exclamation mark, ques-
tion mark) at the end of the sentence can be added
to implement a complete sentence (zj) (Zhou,
2004), we carried out a separate deal with this sit-
uation, directly add punctuation at the end of the
sentence, to form a sentence.
In training data provided by CLP2010 in sub-
task: Complete Sentence Parsing, the head words
aren’t marked. We can’t use the statistical method
to determine the head words, but only by rules. We
take Li’s rule set as baseline, but the rule set was
used to supplement the statistical methods, so
some head words don’t appear in the rule set, re-
sulting in many head words are marked as NULL,
for this situation, we add some rules through expe-
riment, Table 3 lists some additional rules.
</bodyText>
<tableCaption confidence="0.998501">
Table 3: increasing part of rules
</tableCaption>
<table confidence="0.947917285714286">
parent head words
vp vp, vB, vSB, vM, vJY, vC, v
ap a, b, d
mp qN, qV, qC, q
dj vp, dj, ap, v, fj
dlc vp
mbar m, mp
</table>
<subsectionHeader confidence="0.99401">
3.4 Empirical Results and Analysis
</subsectionHeader>
<bodyText confidence="0.998134625">
We take the corpus which are through correct POS
tagging and base chunking for training and testing,
it is divided into five folds, the first four as train-
ing corpus, the last one as testing corpus, using the
existing ME toolkit to train and test model in la-
boratory. Table 4 shows the results on Viterbi de-
coding and Heuristic Search method, where head
words are determined by rules.
</bodyText>
<tableCaption confidence="0.989702">
Table 4: results with different decoding
</tableCaption>
<table confidence="0.9983955">
Decoding Accuracy Recall Fmeasure
Viterbi 84.87% 84.47% 84.67%
Heuristic 85.62% 85.19% 85.40%
Search
</table>
<tableCaption confidence="0.736291714285714">
The system participated in the Complete Sen-
tence Parsing of CLP2010, results are shown in
Table 5 below. Because we can’t determine the
head words by statistical method on the corpus
provided by CLP2010, resulting in the accuracy
decreasing, creating a great impact on results.
Table 5: the track results
</tableCaption>
<table confidence="0.998046">
Training Model use F-measure POS
mode Accuracy
Closed Single 63.25% 89.62%
</table>
<sectionHeader confidence="0.997743" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999988523809524">
In this paper, we use CRFs to have a POS tagging,
and increase the tagging accuracy by adjusting the
feature template; multi-level chunking is applied
to complete syntactic analysis, we do the base
chunking with MEMM to recognize boundaries
and components, and make the complex chunking
with ME to generate a full parsing tree; on decod-
ing, we add transfer matrix to improve perfor-
mance, and remove some features with a ME clas-
sifier to reduce training time.
As the training data are temporarily changed,
our system’s training on the Event Description
Sub-sentence Analysis of CLP2010 isn’t com-
pleted, and head words are marked in the training
corpus of this task, so our next step will be to
complete training and testing of this task, compare
the existing evaluation results, and use ME clas-
sifier to determine head words, analyze impact of
head words on system. On the POS tagging, we
will retain all features to train and compare tag-
ging results.
</bodyText>
<sectionHeader confidence="0.960074" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9997596">
We would like to thank XingJun Xu and BenYang
Li for their valuable advice to our work in Com-
plete Sentence Parsing of CLP2010. We also thank
JunHui Li, XiaoRui Yang and HaiLong Cao for
paving the way for our work.
</bodyText>
<sectionHeader confidence="0.994243" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999434621621622">
S. Abney (1991) Parsing by Chunks. Kluwer Academic
Publishers, Dordrecht, 257-278
Lance A. Ramshaw, Mitchell P. Marcus (1995) Text
Chunking Using Transformation-Based Learning. In
Proceeding of the Third ACL Workshop on Very
Large Corpora, USA, 87-88
Erik F. Tjong Kim Sang (2001) Transforming a Chunk-
er to a Parser. Computational Linguistics in the
Netherlands 2000, 6-8
YongSheng Yang, BenFeng Chen (2004) A Maximum-
Entropy Chinese Parser Augmented by Transforma-
tion-Based Learning. ACM Transactions on Asian
Language Information Processing, 4-8
John Lafferty, Andrew McCallum, and Fernando Perei-
ra (2001) Conditional Random Fields: Probabilistic
Models for Segmenting and Labeling Sequence Data.
Proceedings of the Eighteenth International Confe-
rence on Machine Learning, 282-289
Junhui Li, Guodong Zhou (2009) Soochow University
Report for the 1st China Workshop on Syntactic
Parsing. CIPS-ParsEval-2009, 5-8
Wei Jiang, Yi Guan, and Xiaolong Wang (2006) Condi-
tional Random Fields Based POS Tagging.Computer
Engineering and Applications, 14-15
Xiaorui Yang, Bingquan Liu, Chengjie Sun, and Lei
Lin (2009) InsunPOS: a CRF-based POS Tagging
System. CIPS-ParsEval-2009, 4-6
A. McCallum, D. Freitag, and F. Pereira (2000) Maxi-
mum Entropy Markov Models for Information Ex-
traction and Segmentation. Proceedings of ICML-
2000, Stanford University, USA, 591-598
Chao Li, Jian Sun, Yi Guan, Xingjun Xu, Lei Hou, and
Sheng Li (2009) Chinese Chunking With Maximum
Entropy Models. CIPS-ParsEval-2009, 2-4
Qiang Zhou (2004) Annotation Scheme for Chinese
Treebank. Journal of Chinese Information Processing,
4-5
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622392">
<title confidence="0.999639">Complete Syntactic Analysis Based on Multi-level Chunking</title>
<author confidence="0.9254865">Jiang Zhao Guan Li</author>
<affiliation confidence="0.999542">School of Computer Science and Harbin Institute of</affiliation>
<address confidence="0.998468">150001, Harbin,</address>
<email confidence="0.918408">xyf-3456@163.com;guanyi@hit.edu.cn;lisheng@hit.edu.cn</email>
<abstract confidence="0.995340470588235">This paper describes a complete syntactic analysis system based on multi-level chunking. On the basis of the correct sequences of Chinese words provided by CLP2010, the system firstly has a Part-ofspeech (POS) tagging with Conditional Random Fields (CRFs), and then does the base chunking and complex chunking with Maximum Entropy (ME), and finally generates a complete syntactic analysis tree. The system took part in the Complete Sentence Parsing Track of the Task 2 Chinese Parsing in CLP2010, achieved the F-1 measure of 63.25% on the overall analysis, ranked the sixth; POS accuracy rate of 89.62%, ranked the third.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by Chunks.</title>
<date>1991</date>
<pages>257--278</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="1114" citStr="Abney, 1991" startWordPosition="168" endWordPosition="169"> firstly has a Part-ofspeech (POS) tagging with Conditional Random Fields (CRFs), and then does the base chunking and complex chunking with Maximum Entropy (ME), and finally generates a complete syntactic analysis tree. The system took part in the Complete Sentence Parsing Track of the Task 2 Chinese Parsing in CLP2010, achieved the F-1 measure of 63.25% on the overall analysis, ranked the sixth; POS accuracy rate of 89.62%, ranked the third. 1 Introduction Chunk is a group of adjacent words which belong to the same s-projection set in a sentence, whose syntactic structure is actually a tree (Abney, 1991), but apart from the root node, all other nodes are leaf nodes. Complete syntactic analysis requires a series of analyzing processes, eventually to get a full parsing tree. Parsing by chunks is proved to be feasible (Abney, 1994). The concept of chunking was first proposed by Abney in 1991, who defined chunks in terms of major heads, and parsed by chunks in 1994 (Abney, 1994). An additional chunk tag set {B, I, O} was added to chunking (Ramshaw and Marcus, 1995), which limited dependencies between elements in a chunk, changed chunking into a question of sequenced tags, to promote the developme</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>S. Abney (1991) Parsing by Chunks. Kluwer Academic Publishers, Dordrecht, 257-278</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text Chunking Using Transformation-Based Learning.</title>
<date>1995</date>
<booktitle>In Proceeding of the Third ACL Workshop on Very Large Corpora, USA,</booktitle>
<pages>87--88</pages>
<contexts>
<context position="1580" citStr="Ramshaw and Marcus, 1995" startWordPosition="248" endWordPosition="251">oduction Chunk is a group of adjacent words which belong to the same s-projection set in a sentence, whose syntactic structure is actually a tree (Abney, 1991), but apart from the root node, all other nodes are leaf nodes. Complete syntactic analysis requires a series of analyzing processes, eventually to get a full parsing tree. Parsing by chunks is proved to be feasible (Abney, 1994). The concept of chunking was first proposed by Abney in 1991, who defined chunks in terms of major heads, and parsed by chunks in 1994 (Abney, 1994). An additional chunk tag set {B, I, O} was added to chunking (Ramshaw and Marcus, 1995), which limited dependencies between elements in a chunk, changed chunking into a question of sequenced tags, to promote the development of chunking. Chunking algorithm was extended to the bottom-up parser, which is trained and tested on the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus, Santorini and Marcinkiewicz 1993), and achieved a performance of 80.49% F-measure, the results show that it performed better than a standard probabilistic context-free grammar, and can improve performance by adding the information of parent node (Sang, 2000). On Chinese parsing, Maximum Entropy M</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw, Mitchell P. Marcus (1995) Text Chunking Using Transformation-Based Learning. In Proceeding of the Third ACL Workshop on Very Large Corpora, USA, 87-88</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang</title>
<date>2001</date>
<pages>6--8</pages>
<marker>Erik, 2001</marker>
<rawString>Erik F. Tjong Kim Sang (2001) Transforming a Chunker to a Parser. Computational Linguistics in the Netherlands 2000, 6-8</rawString>
</citation>
<citation valid="true">
<authors>
<author>YongSheng Yang</author>
</authors>
<title>BenFeng Chen</title>
<date>2004</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<pages>4--8</pages>
<marker>Yang, 2004</marker>
<rawString>YongSheng Yang, BenFeng Chen (2004) A MaximumEntropy Chinese Parser Augmented by Transformation-Based Learning. ACM Transactions on Asian Language Information Processing, 4-8</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira (2001) Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the Eighteenth International Conference on Machine Learning, 282-289</rawString>
</citation>
<citation valid="false">
<authors>
<author>Junhui Li</author>
</authors>
<title>Guodong Zhou (2009) Soochow University Report for the 1st China Workshop on Syntactic Parsing.</title>
<tech>CIPS-ParsEval-2009,</tech>
<pages>5--8</pages>
<marker>Li, </marker>
<rawString>Junhui Li, Guodong Zhou (2009) Soochow University Report for the 1st China Workshop on Syntactic Parsing. CIPS-ParsEval-2009, 5-8</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Jiang</author>
<author>Yi Guan</author>
<author>Xiaolong Wang</author>
</authors>
<date>2006</date>
<booktitle>Conditional Random Fields Based POS Tagging.Computer Engineering and Applications,</booktitle>
<pages>14--15</pages>
<marker>Jiang, Guan, Wang, 2006</marker>
<rawString>Wei Jiang, Yi Guan, and Xiaolong Wang (2006) Conditional Random Fields Based POS Tagging.Computer Engineering and Applications, 14-15</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaorui Yang</author>
<author>Bingquan Liu</author>
<author>Chengjie Sun</author>
<author>Lei Lin</author>
</authors>
<title>InsunPOS: a CRF-based POS Tagging System.</title>
<date>2009</date>
<tech>CIPS-ParsEval-2009,</tech>
<pages>4--6</pages>
<marker>Yang, Liu, Sun, Lin, 2009</marker>
<rawString>Xiaorui Yang, Bingquan Liu, Chengjie Sun, and Lei Lin (2009) InsunPOS: a CRF-based POS Tagging System. CIPS-ParsEval-2009, 4-6</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum Entropy Markov Models for Information Extraction and Segmentation.</title>
<date>2000</date>
<booktitle>Proceedings of ICML2000,</booktitle>
<pages>591--598</pages>
<location>Stanford University, USA,</location>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira (2000) Maximum Entropy Markov Models for Information Extraction and Segmentation. Proceedings of ICML2000, Stanford University, USA, 591-598</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Li</author>
<author>Jian Sun</author>
<author>Yi Guan</author>
<author>Xingjun Xu</author>
<author>Lei Hou</author>
<author>Sheng Li</author>
</authors>
<title>Chinese Chunking With Maximum Entropy Models.</title>
<date>2009</date>
<tech>CIPS-ParsEval-2009,</tech>
<pages>2--4</pages>
<marker>Li, Sun, Guan, Xu, Hou, Li, 2009</marker>
<rawString>Chao Li, Jian Sun, Yi Guan, Xingjun Xu, Lei Hou, and Sheng Li (2009) Chinese Chunking With Maximum Entropy Models. CIPS-ParsEval-2009, 2-4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Zhou</author>
</authors>
<title>Annotation Scheme for Chinese Treebank.</title>
<date>2004</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>4--5</pages>
<contexts>
<context position="11047" citStr="Zhou, 2004" startWordPosition="1920" endWordPosition="1921"> the greatest priority in the queue; If the node’s depth = length of the chunking results Searching is over, reverse the searching path to get searching results; Else Compute the probability of all candidate children nodes according to the current probability; Record searching path; Press it into the priority queue; In addition, we found that some punctuation at the end of a sentence can’t be merged, probably due to sparseness of data, according to that the tone punctuation (period, exclamation mark, question mark) at the end of the sentence can be added to implement a complete sentence (zj) (Zhou, 2004), we carried out a separate deal with this situation, directly add punctuation at the end of the sentence, to form a sentence. In training data provided by CLP2010 in subtask: Complete Sentence Parsing, the head words aren’t marked. We can’t use the statistical method to determine the head words, but only by rules. We take Li’s rule set as baseline, but the rule set was used to supplement the statistical methods, so some head words don’t appear in the rule set, resulting in many head words are marked as NULL, for this situation, we add some rules through experiment, Table 3 lists some addition</context>
</contexts>
<marker>Zhou, 2004</marker>
<rawString>Qiang Zhou (2004) Annotation Scheme for Chinese Treebank. Journal of Chinese Information Processing, 4-5</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>