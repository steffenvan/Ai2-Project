<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.864855">
Recall-Oriented Learning of Named Entities in Arabic Wikipedia
</title>
<author confidence="0.995896">
Behrang Mohit* Nathan Schneider† Rishav Bhowmick* Kemal Oflazer* Noah A. Smith†
</author>
<affiliation confidence="0.996481">
School of Computer Science, Carnegie Mellon University
</affiliation>
<address confidence="0.626208">
*P.O. Box 24866, Doha, Qatar †Pittsburgh, PA 15213, USA
</address>
<email confidence="0.996181">
{behrang@,nschneid@cs.,rishavb@qatar.,ko@cs.,nasmith@cs.}cmu.edu
</email>
<sectionHeader confidence="0.997345" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949263157895">
We consider the problem of NER in Arabic
Wikipedia, a semisupervised domain adap-
tation setting for which we have no labeled
training data in the target domain. To fa-
cilitate evaluation, we obtain annotations
for articles in four topical groups, allow-
ing annotators to identify domain-specific
entity types in addition to standard cate-
gories. Standard supervised learning on
newswire text leads to poor target-domain
recall. We train a sequence model and show
that a simple modification to the online
learner—a loss function encouraging it to
“arrogantly” favor recall over precision—
substantially improves recall and Fl. We
then adapt our model with self-training
on unlabeled target-domain data; enforc-
ing the same recall-oriented bias in the self-
training stage yields marginal gains.1
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940529411765">
This paper considers named entity recognition
(NER) in text that is different from most past re-
search on NER. Specifically, we consider Arabic
Wikipedia articles with diverse topics beyond the
commonly-used news domain. These data chal-
lenge past approaches in two ways:
First, Arabic is a morphologically rich lan-
guage (Habash, 2010). Named entities are ref-
erenced using complex syntactic constructions
(cf. English NEs, which are primarily sequences
of proper nouns). The Arabic script suppresses
most vowels, increasing lexical ambiguity, and
lacks capitalization, a key clue for English NER.
Second, much research has focused on the use
of news text for system building and evaluation.
Wikipedia articles are not news, belonging instead
to a wide range of domains that are not clearly
</bodyText>
<footnote confidence="0.971977333333333">
1The annotated dataset and a supplementary document
with additional details of this work can be found at:
http://www.ark.cs.cmu.edu/AQMAR
</footnote>
<bodyText confidence="0.997360333333334">
delineated. One hallmark of this divergence be-
tween Wikipedia and the news domain is a dif-
ference in the distributions of named entities. In-
deed, the classic named entity types (person, or-
ganization, location) may not be the most apt for
articles in other domains (e.g., scientific or social
topics). On the other hand, Wikipedia is a large
dataset, inviting semisupervised approaches.
In this paper, we describe advances on the prob-
lem of NER in Arabic Wikipedia. The techniques
are general and make use of well-understood
building blocks. Our contributions are:
</bodyText>
<listItem confidence="0.991898777777778">
• A small corpus of articles annotated in a new
scheme that provides more freedom for annota-
tors to adapt NE analysis to new domains;
• An “arrogant” learning approach designed to
boost recall in supervised training as well as
self-training; and
• An empirical evaluation of this technique as ap-
plied to a well-established discriminative NER
model and feature set.
</listItem>
<bodyText confidence="0.990080666666667">
Experiments show consistent gains on the chal-
lenging problem of identifying named entities in
Arabic Wikipedia text.
</bodyText>
<sectionHeader confidence="0.987284" genericHeader="introduction">
2 Arabic Wikipedia NE Annotation
</sectionHeader>
<bodyText confidence="0.9981701">
Most of the effort in NER has been fo-
cused around a small set of domains and
general-purpose entity classes relevant to those
domains—especially the categories PER(SON),
ORG(ANIZATION), and LOC(ATION) (POL),
which are highly prominent in news text. Ara-
bic is no exception: the publicly available NER
corpora—ACE (Walker et al., 2006), ANER (Be-
najiba et al., 2008), and OntoNotes (Hovy et al.,
2006)—all are in the news domain.2 However,
</bodyText>
<footnote confidence="0.8395388">
2OntoNotes contains news-related text. ACE includes
some text from blogs. In addition to the POL classes, both
corpora include additional NE classes such as facility, event,
product, vehicle, etc. These entities are infrequent and may
not be comprehensive enough to cover the larger set of pos-
</footnote>
<page confidence="0.8988">
162
</page>
<note confidence="0.795372714285714">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162–173,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
History Science Sports Technology
dev: Damascus Atom Ra´ul Gonz´ales Linux
Imam Hussein Shrine Nuclear power Real Madrid Solaris
test: Crusades Enrico Fermi 2004 Summer Olympics Computer
Islamic Golden Age Light Christiano Ronaldo Computer Software
</note>
<table confidence="0.733099571428571">
Islamic History Periodic Table Football Internet
Ibn Tolun Mosque Physics Portugal football team Richard Stallman
Ummaya Mosque Muhammad al-Razi FIFA World Cup X Window System
Claudio Filippone (PER) JñJ.ÊJ��¯ ñK�XñÊ¿; Linux (SOFTWARE) •º:J�Ë; Spanish
League (CHAMPIONSHIPS) ú��GAJ.ƒB@ ø
PðYË@; proton (PARTICLE) uñ&apos;ðQK.; nuclear
radiation (GENERIC-MISC) ø� ðñJ@ ¨Aª —VI; Real Zaragoza (ORG) a¢‚�¯Qå... ÈAK�P
</table>
<tableCaption confidence="0.952972">
Table 1: Translated titles
</tableCaption>
<bodyText confidence="0.974819396226415">
of Arabic Wikipedia arti-
cles in our development
and test sets, and some
NEs with standard and
article-specific classes.
Additionally, Prussia and
Amman were reserved
for training annotators,
and Gulf War for esti-
mating inter-annotator
agreement.
appropriate entity classes will vary widely by do-
main; occurrence rates for entity classes are quite
different in news text vs. Wikipedia, for instance
(Balasuriya et al., 2009). This is abundantly
clear in technical and scientific discourse, where
much of the terminology is domain-specific, but it
holds elsewhere. Non-POL entities in the history
domain, for instance, include important events
(wars, famines) and cultural movements (roman-
ticism). Ignoring such domain-critical entities
likely limits the usefulness of the NE analysis.
Recognizing this limitation, some work on
NER has sought to codify more robust invento-
ries of general-purpose entity types (Sekine et al.,
2002; Weischedel and Brunstein, 2005; Grouin
et al., 2011) or to enumerate domain-specific
types (Settles, 2004; Yao et al., 2003). Coarse,
general-purpose categories have also been used
for semantic tagging of nouns and verbs (Cia-
ramita and Johnson, 2003). Yet as the number
of classes or domains grows, rigorously docu-
menting and organizing the classes—even for a
single language—requires intensive effort. Ide-
ally, an NER system would refine the traditional
classes (Hovy et al., 2011) or identify new entity
classes when they arise in new domains, adapting
to new data. For this reason, we believe it is valu-
able to consider NER systems that identify (but
do not necessarily label) entity mentions, and also
to consider annotation schemes that allow annota-
tors more freedom in defining entity classes.
Our aim in creating an annotated dataset is to
provide a testbed for evaluation of new NER mod-
els. We will use these data as development and
sible NEs (Sekine et al., 2002). Nezda et al. (2006) anno-
tated and evaluated an Arabic NE corpus with an extended
set of 18 classes (including temporal and numeric entities);
this corpus has not been released publicly.
testing examples, but not as training data. In §4
we will discuss our semisupervised approach to
learning, which leverages ACE and ANER data
as an annotated training corpus.
</bodyText>
<subsectionHeader confidence="0.99924">
2.1 Annotation Strategy
</subsectionHeader>
<bodyText confidence="0.999954592592592">
We conducted a small annotation project on Ara-
bic Wikipedia articles. Two college-educated na-
tive Arabic speakers annotated about 3,000 sen-
tences from 31 articles. We identified four top-
ical areas of interest—history, technology, sci-
ence, and sports—and browsed these topics un-
til we had found 31 articles that we deemed sat-
isfactory on the basis of length (at least 1,000
words), cross-lingual linkages (associated articles
in English, German, and Chinese3), and subjec-
tive judgments of quality. The list of these arti-
cles along with sample NEs are presented in ta-
ble 1. These articles were then preprocessed to
extract main article text (eliminating tables, lists,
info-boxes, captions, etc.) for annotation.
Our approach follows ACE guidelines (LDC,
2005) in identifying NE boundaries and choos-
ing POL tags. In addition to this traditional form
of annotation, annotators were encouraged to ar-
ticulate one to three salient, article-specific en-
tity categories per article. For example, names
of particles (e.g., proton) are highly salient in the
Atom article. Annotators were asked to read the
entire article first, and then to decide which non-
traditional classes of entities would be important
in the context of article. In some cases, annotators
reported using heuristics (such as being proper
</bodyText>
<footnote confidence="0.871017666666667">
3These three languages have the most articles on
Wikipedia. Associated articles here are those that have been
manually hyperlinked from the Arabic page as cross-lingual
correspondences. They are not translations, but if the associ-
ations are accurate, these articles should be topically similar
to the Arabic page that links to them.
</footnote>
<page confidence="0.996739">
163
</page>
<table confidence="0.8020662">
Token position agreement rate 92.6% Cohen’s κ: 0.86
Token agreement rate 88.3% Cohen’s κ: 0.86
Token Fl between annotators 91.0%
Entity boundary match Fl 94.0%
Entity category match Fl 87.4%
</table>
<tableCaption confidence="0.995755">
Table 2: Inter-annotator agreement measurements.
</tableCaption>
<bodyText confidence="0.999436928571429">
nouns or having an English translation which is
conventionally capitalized) to help guide their de-
termination of non-canonical entities and entity
classes. Annotators produced written descriptions
of their classes, including example instances.
This scheme was chosen for its flexibility: in
contrast to a scenario with a fixed ontology, anno-
tators required minimal training beyond the POL
conventions, and did not have to worry about
delineating custom categories precisely enough
that they would extend straightforwardly to other
topics or domains. Of course, we expect inter-
annotator variability to be greater for these open-
ended classification criteria.
</bodyText>
<subsectionHeader confidence="0.999826">
2.2 Annotation Quality Evaluation
</subsectionHeader>
<bodyText confidence="0.999838">
During annotation, two articles (Prussia and Am-
man) were reserved for training annotators on
the task. Once they were accustomed to anno-
tation, both independently annotated a third ar-
ticle. We used this 4,750-word article (Gulf War,
</bodyText>
<equation confidence="0.892498">
éJ��KA�JË@ i. J�Ê�mÌ
�@ H. Qk) to measure inter-annotator
</equation>
<bodyText confidence="0.960660888888889">
agreement. Table 2 provides scores for token-
level agreement measures and entity-level FI be-
tween the two annotated versions of the article.4
These measures indicate strong agreement for
locating and categorizing NEs both at the token
and chunk levels. Closer examination of agree-
ment scores shows that PER and MIS classes have
the lowest rates of agreement. That the mis-
cellaneous class, used for infrequent or article-
specific NEs, receives poor agreement is unsur-
prising. The low agreement on the PER class
seems to be due to the use of titles and descriptive
terms in personal names. Despite explicit guide-
lines to exclude the titles, annotators disagreed on
the inclusion of descriptors that disambiguate the
�
NE (e.g., the father in H. B@ :€ñK. h. Qk.: George
Bush, the father).
</bodyText>
<footnote confidence="0.8798876">
4The position and boundary measures ignore the distinc-
tions between the POLM classes. To avoid artificial inflation
of the token and token position agreement rates, we exclude
the 81% of tokens tagged by both annotators as not belong-
ing to an entity.
</footnote>
<table confidence="0.688591181818182">
History: Gulf War, Prussia, Damascus, Crusades
WAR CONFLICT • • •
Science: Atom, Periodic table
THEORY • CHEMICAL • •
NAME ROMAN • PARTICLE • •
Sports: Football, Ra´ul Gonz´ales
SPORT ◦ CHAMPIONSHIP •
AWARD ◦ NAME ROMAN •
Technology: Computer, Richard Stallman
COMPUTER VARIETY ◦ SOFTWARE •
COMPONENT •
</table>
<tableCaption confidence="0.978714222222222">
Table 3: Custom NE categories suggested by one or
both annotators for 10 articles. Article titles are trans-
lated from Arabic. • indicates that both annotators vol-
unteered a category for an article; ◦ indicates that only
one annotator suggested the category. Annotators were
not given a predetermined set of possible categories;
rather, category matches between annotators were de-
termined by post hoc analysis. NAME ROMAN indi-
cates an NE rendered in Roman characters.
</tableCaption>
<subsectionHeader confidence="0.999232">
2.3 Validating Category Intuitions
</subsectionHeader>
<bodyText confidence="0.999981407407407">
To investigate the variability between annotators
with respect to custom category intuitions, we
asked our two annotators to independently read
10 of the articles in the data (scattered across our
four focus domains) and suggest up to 3 custom
categories for each. We assigned short names to
these suggestions, seen in table 3. In 13 cases,
both annotators suggested a category for an article
that was essentially the same (•); three such cat-
egories spanned multiple articles. In three cases
a category was suggested by only one annotator
(◦).5 Thus, we see that our annotators were gen-
erally, but not entirely, consistent with each other
in their creation of custom categories. Further, al-
most all of our article-specific categories corre-
spond to classes in the extended NE taxonomy of
(Sekine et al., 2002), which speaks to the reason-
ableness of both sets of categories—and by exten-
sion, our open-ended annotation process.
Our annotation of named entities outside of the
traditional POL classes creates a useful resource
for entity detection and recognition in new do-
mains. Even the ability to detect non-canonical
types of NEs should help applications such as QA
and MT (Toral et al., 2005; Babych and Hart-
ley, 2003). Possible avenues for future work
include annotating and projecting non-canonical
</bodyText>
<footnote confidence="0.9997">
5When it came to tagging NEs, one of the two annota-
tors was assigned to each article. Custom categories only
suggested by the other annotator were ignored.
</footnote>
<page confidence="0.998088">
164
</page>
<bodyText confidence="0.999938388888889">
NEs from English articles to their Arabic coun-
terparts (Hassan et al., 2007), automatically clus-
tering non-canonical types of entities into article-
specific or cross-article classes (cf. Frietag, 2004),
or using non-canonical classes to improve the
(author-specified) article categories in Wikipedia.
Hereafter, we merge all article-specific cate-
gories with the generic MIS category. The pro-
portion of entity mentions that are tagged as MIS,
while varying to a large extent by document, is
a major indication of the gulf between the news
data (&lt;10%) and the Wikipedia data (53% for the
development set, 37% for the test set).
Below, we aim to develop entity detection mod-
els that generalize beyond the traditional POL en-
tities. We do not address here the challenges of
automatically classifying entities or inferring non-
canonical groupings.
</bodyText>
<sectionHeader confidence="0.99835" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9999769375">
Table 4 summarizes the various corpora used in
this work.6 Our NE-annotated Wikipedia sub-
corpus, described above, consists of several Ara-
bic Wikipedia articles from four focus domains.7
We do not use these for supervised training data;
they serve only as development and test data. A
larger set of Arabic Wikipedia articles, selected
on the basis of quality heuristics, serves as unla-
beled data for semisupervised learning.
Our out-of-domain labeled NE data is drawn
from the ANER (Benajiba et al., 2007) and
ACE-2005 (Walker et al., 2006) newswire cor-
pora. Entity types in this data are POL cate-
gories (PER, ORG, LOC) and MIS. Portions of the
ACE corpus were held out as development and
test data; the remainder is used in training.
</bodyText>
<sectionHeader confidence="0.993776" genericHeader="method">
4 Models
</sectionHeader>
<bodyText confidence="0.99632325">
Our starting point for statistical NER is a feature-
based linear model over sequences, trained using
the structured perceptron (Collins, 2002).8
In addition to lexical and morphological9 fea-
</bodyText>
<footnote confidence="0.9866923">
6Additional details appear in the supplement.
7We downloaded a snapshot of Arabic Wikipedia
(http://ar.wikipedia.org) on 8/29/2009 and pre-
processed the articles to extract main body text and metadata
using the mwlib package for Python (PediaPress, 2010).
8A more leisurely discussion of the structured percep-
tron and its connection to empirical risk minimization can
be found in the supplementary document.
9We obtain morphological analyses from the MADA tool
(Habash and Rambow, 2005; Roth et al., 2008).
</footnote>
<table confidence="0.999570444444444">
Training words NEs
ACE+ANER 212,839 15,796
Wikipedia (unlabeled, 397 docs) 1,110,546 —
Development
ACE 7,776 638
Wikipedia (4 domains, 8 docs) 21,203 2,073
Test
ACE 7,789 621
Wikipedia (4 domains, 20 docs) 52,650 3,781
</table>
<tableCaption confidence="0.999655">
Table 4: Number of words (entity mentions) in data sets.
</tableCaption>
<bodyText confidence="0.998995736842105">
tures known to work well for Arabic NER (Be-
najiba et al., 2008; Abdul-Hamid and Darwish,
2010), we incorporate some additional features
enabled by Wikipedia. We do not employ a
gazetteer, as the construction of a broad-domain
gazetteer is a significant undertaking orthogo-
nal to the challenges of a new text domain like
Wikipedia.10 A descriptive list of our features is
available in the supplementary document.
We use a first-order structured perceptron; none
of our features consider more than a pair of con-
secutive BIO labels at a time. The model enforces
the constraint that NE sequences must begin with
B (so the bigram (O, I) is disallowed).
Training this model on ACE and ANER data
achieves performance comparable to the state of
the art (F1-measure11 above 69%), but fares much
worse on our Wikipedia test set (F1-measure
around 47%); details are given in §5.
</bodyText>
<subsectionHeader confidence="0.986413">
4.1 Recall-Oriented Perceptron
</subsectionHeader>
<bodyText confidence="0.999978222222222">
By augmenting the perceptron’s online update
with a cost function term, we can incorporate a
task-dependent notion of error into the objective,
as with structured SVMs (Taskar et al., 2004;
Tsochantaridis et al., 2005). Let c(y, y0) denote
a measure of error when y is the correct label se-
quence but y0 is predicted. For observed sequence
x and feature weights (model parameters) w, the
structured hinge loss is Ehz,,ye(x, y, w) _
</bodyText>
<equation confidence="0.836220333333333">
( )
w&gt;g(x, y0) + c(y, y0) − w&gt;g(x, y)
(1)
</equation>
<bodyText confidence="0.891588714285714">
The maximization problem inside the parentheses
is known as cost-augmented decoding. If c fac-
10A gazetteer ought to yield further improvements in line
with previous findings in NER (Ratinov and Roth, 2009).
11Though optimizing NER systems for Fl has been called
into question (Manning, 2006), no alternative metric has
achieved widespread acceptance in the community.
</bodyText>
<equation confidence="0.548864">
max
Y
</equation>
<page confidence="0.98668">
165
</page>
<bodyText confidence="0.9999335">
tors similarly to the feature function g(x, y), then
we can increase penalties for y that have more
local mistakes. This raises the learner’s aware-
ness about how it will be evaluated. Incorporat-
ing cost-augmented decoding into the perceptron
leads to this decoding step:
</bodyText>
<equation confidence="0.925887">
( )
w&gt;g(x, y0) + c(y, y0) , (2)
</equation>
<bodyText confidence="0.999920857142857">
which amounts to performing stochastic subgradi-
ent ascent on an objective function with the Eq. 1
loss (Ratliff et al., 2006).
In this framework, cost functions can be for-
mulated to distinguish between different types of
errors made during training. For a tag sequence
y = (y1, y2, ... , yM), Gimpel and Smith (2010b)
define word-local cost functions that differently
penalize precision errors (i.e., yi = O n yi =� O
for the ith word), recall errors (yi =� O n yi = O),
and entity class/position errors (other cases where
yi =� yi). As will be shown below, a key problem
in cross-domain NER is poor recall, so we will
penalize recall errors more severely:
</bodyText>
<equation confidence="0.488377333333333">
{ 0 if yi = y0i (3)
Q if yi =� O n y0 i = O
1 otherwise
</equation>
<bodyText confidence="0.999928642857143">
for a penalty parameter Q &gt; 1. We call our learner
the “recall-oriented” perceptron (ROP).
We note that Minkov et al. (2006) similarly ex-
plored the recall vs. precision tradeoff in NER.
Their technique was to directly tune the weight
of a single feature—the feature marking O (non-
entity tokens); a lower weight for this feature will
incur a greater penalty for predicting O. Below
we demonstrate that our method, which is less
coarse, is more successful in our setting.12
In our experiments we will show that injecting
“arrogance” into the learner via the recall-oriented
loss function substantially improves recall, espe-
cially for non-POL entities (§5.3).
</bodyText>
<subsectionHeader confidence="0.9982265">
4.2 Self-Training and Semisupervised
Learning
</subsectionHeader>
<bodyText confidence="0.966188777777778">
As we will show experimentally, the differences
between news text and Wikipedia text call for do-
main adaptation. In the case of Arabic Wikipedia,
12The distinction between the techniques is that our cost
function adjusts the whole model in order to perform better
at recall on the training data.
Input: labeled data ((x(n), y(n)))N n=1; unlabeled
data (x(j))J j=1; supervised learner L;
number of iterations T&apos;
</bodyText>
<equation confidence="0.941837571428571">
Output: w
w — L(((x(n), y(n)))Nn=1)
fort = 1 to T&apos; do
for j = 1 to J do
:0(j) — arg maxV wTg(.t(j), y)
w — L(((x(n), y(n)))Nn=1 U ((.t(j), y(j)))J j=1)
Algorithm 1: Self-training.
</equation>
<bodyText confidence="0.9995273125">
there is no available labeled training data. Yet
the available unlabeled data is vast, so we turn to
semisupervised learning.
Here we adapt self-training, a simple tech-
nique that leverages a supervised learner (like the
perceptron) to perform semisupervised learning
(Clark et al., 2003; Mihalcea, 2004; McClosky
et al., 2006). In our version, a model is trained
on the labeled data, then used to label the un-
labeled target data. We iterate between training
on the hypothetically-labeled target data plus the
original labeled set, and relabeling the target data;
see Algorithm 1. Before self-training, we remove
sentences hypothesized not to contain any named
entity mentions, which we found avoids further
encouragement of the model toward low recall.
</bodyText>
<sectionHeader confidence="0.999705" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9778435">
We investigate two questions in the context of
NER for Arabic Wikipedia:
</bodyText>
<listItem confidence="0.711632181818182">
• Loss function: Does integrating a cost func-
tion into our learning algorithm, as we have
done in the recall-oriented perceptron (§4.1),
improve recall and overall performance on
Wikipedia data?
• Semisupervised learning for domain adap-
tation: Can our models benefit from large
amounts of unlabeled Wikipedia data, in addi-
tion to the (out-of-domain) labeled data? We
experiment with a self-training phase following
the fully supervised learning phase.
</listItem>
<bodyText confidence="0.999944666666667">
We report experiments for the possible combi-
nations of the above ideas. These are summarized
in table 5. Note that the recall-oriented percep-
tron can be used for the supervised learning phase,
for the self-training phase, or both. This leaves us
with the following combinations:
</bodyText>
<listItem confidence="0.9999365">
• reg/none (baseline): regular supervised learner.
• ROP/none: recall-oriented supervised learner.
</listItem>
<equation confidence="0.9995022">
y� — arg max
Y/
M
c(y, y0) =
i=1
</equation>
<page confidence="0.991088">
166
</page>
<figureCaption confidence="0.831263">
Figure 1: Tuning the recall-oriented cost parame-
</figureCaption>
<table confidence="0.835478">
ter for different learning settings. We optimized
for development set F1, choosing penalty 0 = 200
for recall-oriented supervised learning (in the plot,
ROP/*—this is regardless of whether a stage of
self-training will follow); 0 = 100 for recall-
oriented self-training following recall-oriented su-
pervised learning (ROP/ROP); and 0 = 3200 for
recall-oriented self-training following regular super-
vised learning (reg/ROP).
</table>
<listItem confidence="0.989400666666667">
• reg/reg: standard self-training setup.
• ROP/reg: recall-oriented supervised learner, fol-
lowed by standard self-training.
• reg/ROP: regular supervised model as the initial la-
beler for recall-oriented self-training.
• ROP/ROP (the “double ROP” condition): recall-
oriented supervised model as the initial labeler for
recall-oriented self-training. Note that the two
ROPs can use different cost parameters.
</listItem>
<bodyText confidence="0.957541222222222">
For evaluating our models we consider the
named entity detection task, i.e., recognizing
which spans of words constitute entities. This
is measured by per-entity precision, recall, and
F1.13 To measure statistical significance of differ-
ences between models we use Gimpel and Smith’s
(2010) implementation of the paired bootstrap re-
sampler of (Koehn, 2004), taking 10,000 samples
for each comparison.
</bodyText>
<subsectionHeader confidence="0.863204">
5.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999366714285714">
Our baseline is the perceptron, trained on the
POL entity boundaries in the ACE+ANER cor-
pus (reg/none).14 Development data was used to
select the number of iterations (10). We per-
formed 3-fold cross-validation on the ACE data
and found wide variance in the in-domain entity
detection performance of this model:
</bodyText>
<table confidence="0.5900014">
P R F1
fold 1 70.43 63.08 66.55
fold 2 87.48 81.13 84.18
fold 3 65.09 51.13 57.27
average 74.33 65.11 69.33
</table>
<bodyText confidence="0.9984572">
(Fold 1 corresponds to the ACE test set described
in table 4.) We also trained the model to perform
POL detection and classification, achieving nearly
identical results in the 3-way cross-validation of
ACE data. From these data we conclude that our
</bodyText>
<footnote confidence="0.8128362">
13Only entity spans that exactly match the gold spans are
counted as correct. We calculated these scores with the
conlleval.pl script from the CoNLL 2003 shared task.
14In keeping with prior work, we ignore non-POL cate-
gories for the ACE evaluation.
</footnote>
<bodyText confidence="0.9921192">
baseline is on par with the state of the art for Ara-
bic NER on ACE news text (Abdul-Hamid and
Darwish, 2010).15
Here is the performance of the baseline entity
detection model on our 20-article test set:16
</bodyText>
<table confidence="0.9957835">
P R F1
technology 60.42 20.26 30.35
science 64.96 25.73 36.86
history 63.09 35.58 45.50
sports 71.66 59.94 65.28
overall 66.30 35.91 46.59
</table>
<bodyText confidence="0.99122">
Unsurprisingly, performance on Wikipedia data
varies widely across article domains and is much
lower than in-domain performance. Precision
scores fall between 60% and 72% for all domains,
but recall in most cases is far worse. Miscella-
neous class recall, in particular, suffers badly (un-
der 10%)—which partially accounts for the poor
recall in science and technology articles (they
have by far the highest proportion of MIS entities).
</bodyText>
<subsectionHeader confidence="0.999709">
5.2 Self-Training
</subsectionHeader>
<bodyText confidence="0.982016125">
Following Clark et al. (2003), we applied self-
training as described in Algorithm 1, with the
perceptron as the supervised learner. Our unla-
beled data consists of 397 Arabic Wikipedia ar-
ticles (1 million words) selected at random from
all articles exceeding a simple length threshold
(1,000 words); see table 4. We used only one iter-
ation (V = 1), as experiments on development
data showed no benefit from additional rounds.
Several rounds of self-training hurt performance,
15Abdul-Hamid and Darwish report as their best result a
macroaveraged Fl-score of 76. As they do not specify which
data they used for their held-out test set, we cannot perform
a direct comparison. However, our feature set is nearly a
superset of their best feature set, and their result lies well
within the range of results seen in our cross-validation folds.
</bodyText>
<footnote confidence="0.990517666666667">
16Our Wikipedia evaluations use models trained on
POLM entity boundaries in ACE. Per-domain and overall
scores are microaverages across articles.
</footnote>
<page confidence="0.947845">
167
</page>
<table confidence="0.99897275">
SELF-TRAINING
SUPERVISED none reg ROP
reg 66.3 35.9 46.59 66.7 35.6 46.41 59.2 40.3 47.97
ROP 60.9 44.7 51.59 59.8 46.2 52.11 58.0 47.4 52.16
</table>
<tableCaption confidence="0.95569625">
Table 5: Entity detection precision, recall, and Fl for each learning setting, microaveraged across the 24 articles
in our Wikipedia test set. Rows differ in the supervised learning condition on the ACE+ANER data (regular
vs. recall-oriented perceptron). Columns indicate whether this supervised learning phase was followed by self-
training on unlabeled Wikipedia data, and if so which version of the perceptron was used for self-training.
</tableCaption>
<table confidence="0.871085428571429">
baseline
entities words recall
PER 1081 1743 49.95
ORG 286 637 23.92
LOC 1019 1413 61.43
MIS 1395 2176 9.30
overall 3781 5969 35.91
</table>
<figureCaption confidence="0.985736111111111">
Figure 2: Recall improve-
ment over baseline in the test
set by gold NER category,
counts for those categories in
the data, and recall scores for
our baseline model. Markers
in the plot indicate different
experimental settings corre-
sponding to cells in table 5.
</figureCaption>
<bodyText confidence="0.975401857142857">
an effect attested in earlier research (Curran et al.,
2007) and sometimes known as “semantic drift.”
Results are shown in table 5. We find that stan-
dard self-training (the middle column) has very
little impact on performance.17 Why is this the
case? We venture that poor baseline recall and the
domain variability within Wikipedia are to blame.
</bodyText>
<subsectionHeader confidence="0.987775">
5.3 Recall-Oriented Learning
</subsectionHeader>
<bodyText confidence="0.976294368421053">
The recall-oriented bias can be introduced in ei-
ther or both of the stages of our semisupervised
learning framework: in the supervised learn-
ing phase, modifying the objective of our base-
line (§5.1); and within the self-training algorithm
(§5.2).18 As noted in §4.1, the aim of this ap-
proach is to discourage recall errors (false nega-
tives), which are the chief difficulty for the news
text–trained model in the new domain. We se-
lected the value of the false positive penalty for
cost-augmented decoding, Q, using the develop-
ment data (figure 1).
The results in table 5 demonstrate improve-
ments due to the recall-oriented bias in both
stages of learning.19 When used in the super-
17In neither case does regular self-training produce a sig-
nificantly different Fl score than no self-training.
18Standard Viterbi decoding was used to label the data
within the self-training algorithm; note that cost-augmented
decoding only makes sense in learning, not as a prediction
technique, since it deliberately introduces errors relative to a
correct output that must be provided.
19In terms of Fl, the worst of the 3 models with the ROP
supervised learner significantly outperforms the best model
with the regular supervised learner (p &lt; 0.005). The im-
vised phase (bottom left cell), the recall gains
are substantial—nearly 9% over the baseline. In-
tegrating this bias within self-training (last col-
umn of the table) produces a more modest im-
provement (less than 3%) relative to the base-
line. In both cases, the improvements to recall
more than compensate for the amount of degra-
dation to precision. This trend is robust: wher-
ever the recall-oriented perceptron is added, we
observe improvements in both recall and Fl. Per-
haps surprisingly, these gains are somewhat addi-
tive: using the ROP in both learning phases gives
a small (though not always significant) gain over
alternatives (standard supervised perceptron, no
self-training, or self-training with a standard per-
ceptron). In fact, when the standard supervised
learner is used, recall-oriented self-training suc-
ceeds despite the ineffectiveness of standard self-
training.
Performance breakdowns by (gold) class, fig-
ure 2, and domain, figure 3, further attest to the
robustness of the overall results. The most dra-
matic gains are in miscellaneous class recall—
each form of the recall bias produces an improve-
ment, and using this bias in both the supervised
and self-training phases is clearly most success-
ful for miscellaneous entities. Correspondingly,
the technology and science domains (in which this
class dominates—83% and 61% of mentions, ver-
provements due to self-training are marginal, however: ROP
self-training produces a significant gain only following reg-
ular supervised learning (p &lt; 0.05).
</bodyText>
<page confidence="0.995777">
168
</page>
<figureCaption confidence="0.692384909090909">
Figure 3: Supervised
learner precision vs.
recall as evaluated
on Wikipedia test
data in different
topical domains. The
regular perceptron
(baseline model) is
contrasted with ROP.
No self-training is
applied.
</figureCaption>
<bodyText confidence="0.9971714">
sus 6% and 12% for history and sports, respec-
tively) receive the biggest boost. Still, the gaps
between domains are not entirely removed.
Most improvements relate to the reduction of
false negatives, which fall into three groups:
</bodyText>
<listItem confidence="0.878973285714286">
(a) entities occurring infrequently or partially
in the labeled training data (e.g. uranium);
(b) domain-specific entities sharing lexical or con-
textual features with the POL entities (e.g. Linux,
titanium); and (c) words with Latin characters,
common in the science and technology domains.
(a) and (b) are mostly transliterations into Arabic.
</listItem>
<bodyText confidence="0.9999704">
An alternative—and simpler—approach to
controlling the precision-recall tradeoff is the
Minkov et al. (2006) strategy of tuning a single
feature weight subsequent to learning (see §4.1
above). We performed an oracle experiment to
determine how this compares to recall-oriented
learning in our setting. An oracle trained with
the method of Minkov et al. outperforms the three
models in table 5 that use the regular perceptron
for the supervised phase of learning, but under-
performs the supervised ROP conditions.20
Overall, we find that incorporating the recall-
oriented bias in learning is fruitful for adapting to
Wikipedia because the gains in recall outpace the
damage to precision.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.998084">
To our knowledge, this work is the first sugges-
tion that substantively modifying the supervised
learning criterion in a resource-rich domain can
reap benefits in subsequent semisupervised appli-
cation in a new domain. Past work has looked
20Tuning the O feature weight to optimize for Fl on our
test set, we found that oracle precision would be 66.2, recall
would be 39.0, and Fl would be 49.1. The Fl score of our
best model is nearly 3 points higher than the Minkov et al.–
style oracle, and over 4 points higher than the non-oracle
version where the development set is used for tuning.
at regularization (Chelba and Acero, 2006) and
feature design (Daum´e III, 2007); we alter the
loss function. Not surprisingly, the double-ROP
approach harms performance on the original do-
main (on ACE data, we achieve 55.41% F1, far
below the standard perceptron). Yet we observe
that models can be prepared for adaptation even
before a learner is exposed a new domain, sacri-
ficing performance in the original domain.
The recall-oriented bias is not merely encour-
aging the learner to identify entities already seen
in training. As recall increases, so does the num-
ber of new entity types recovered by the model:
of the 2,070 NE types in the test data that were
never seen in training, only 450 were ever found
by the baseline, versus 588 in the reg/ROP condi-
tion, 632 in the ROP/none condition, and 717 in
the double-ROP condition.
We note finally that our method is a simple
extension to the standard structured perceptron;
cost-augmented inference is often no more ex-
pensive than traditional inference, and the algo-
rithmic change is equivalent to adding one addi-
tional feature. Our recall-oriented cost function
is parameterized by a single value, Q; recall is
highly sensitive to the choice of this value (fig-
ure 1 shows how we tuned it on development
data), and thus we anticipate that, in general, such
tuning will be essential to leveraging the benefits
of arrogance.
</bodyText>
<sectionHeader confidence="0.999905" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.996263714285714">
Our approach draws on insights from work in
the areas of NER, domain adaptation, NLP with
Wikipedia, and semisupervised learning. As all
are broad areas of research, we highlight only the
most relevant contributions here.
Research in Arabic NER has been focused on
compiling and optimizing the gazetteers and fea-
</bodyText>
<page confidence="0.997462">
169
</page>
<bodyText confidence="0.999953586666667">
ture sets for standard sequential modeling algo-
rithms (Benajiba et al., 2008; Farber et al., 2008;
Shaalan and Raza, 2008; Abdul-Hamid and Dar-
wish, 2010). We make use of features identi-
fied in this prior work to construct a strong base-
line system. We are unaware of any Arabic NER
work that has addressed diverse text domains like
Wikipedia. Both the English and Arabic ver-
sions of Wikipedia have been used, however, as
resources in service of traditional NER (Kazama
and Torisawa, 2007; Benajiba et al., 2008). Attia
et al. (2010) heuristically induce a mapping be-
tween Arabic Wikipedia and Arabic WordNet to
construct Arabic NE gazetteers.
Balasuriya et al. (2009) highlight the substan-
tial divergence between entities appearing in En-
glish Wikipedia versus traditional corpora, and
the effects of this divergence on NER perfor-
mance. There is evidence that models trained
on Wikipedia data generalize and perform well
on corpora with narrower domains. Nothman
et al. (2009) and Balasuriya et al. (2009) show
that NER models trained on both automatically
and manually annotated Wikipedia corpora per-
form reasonably well on news corpora. The re-
verse scenario does not hold for models trained
on news text, a result we also observe in Arabic
NER. Other work has gone beyond the entity de-
tection problem: Florian et al. (2004) addition-
ally predict within-document entity coreference
for Arabic, Chinese, and English ACE text, while
Cucerzan (2007) aims to resolve every mention
detected in English Wikipedia pages to a canoni-
cal article devoted to the entity in question.
The domain and topic diversity of NEs has been
studied in the framework of domain adaptation
research. A group of these methods use self-
training and select the most informative features
and training instances to adapt a source domain
learner to the new target domain. Wu et al. (2009)
bootstrap the NER leaner with a subset of unla-
beled instances that bridge the source and target
domains. Jiang and Zhai (2006) and Daum´e III
(2007) make use of some labeled target-domain
data to tune or augment the features of the source
model towards the target domain. Here, in con-
trast, we use labeled target-domain data only for
tuning and evaluation. Another important dis-
tinction is that domain variation in this prior
work is restricted to topically-related corpora (e.g.
newswire vs. broadcast news), whereas in our
work, major topical differences distinguish the
training and test corpora—and consequently, their
salient NE classes. In these respects our NER
setting is closer to that of Florian et al. (2010),
who recognize English entities in noisy text, (Sur-
deanu et al., 2011), which concerns information
extraction in a topically distinct target domain,
and (Dalton et al., 2011), which addresses English
NER in noisy and topically divergent text.
Self-training (Clark et al., 2003; Mihalcea,
2004; McClosky et al., 2006) is widely used
in NLP and has inspired related techniques that
learn from automatically labeled data (Liang et
al., 2008; Petrov et al., 2010). Our self-training
procedure differs from some others in that we use
all of the automatically labeled examples, rather
than filtering them based on a confidence score.
Cost functions have been used in non-
structured classification settings to penalize cer-
tain types of errors more than others (Chan and
Stolfo, 1998; Domingos, 1999; Kiddon and Brun,
2011). The goal of optimizing our structured NER
model for recall is quite similar to the scenario ex-
plored by Minkov et al. (2006), as noted above.
</bodyText>
<sectionHeader confidence="0.991456" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999987846153846">
We explored the problem of learning an NER
model suited to domains for which no labeled
training data are available. A loss function to en-
courage recall over precision during supervised
discriminative learning substantially improves re-
call and overall entity detection performance, es-
pecially when combined with a semisupervised
learning regimen incorporating the same bias.
We have also developed a small corpus of Ara-
bic Wikipedia articles via a flexible entity an-
notation scheme spanning four topical domains
(publicly available at http://www.ark.cs.
cmu.edu/AQMAR).
</bodyText>
<sectionHeader confidence="0.986918" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9987619">
We thank Mariem Fekih Zguir and Reham Al Tamime
for assistance with annotation, Michael Heilman for
his tagger implementation, and Nizar Habash and col-
leagues for the MADA toolkit. We thank members of
the ARK group at CMU, Hal Daum´e, and anonymous
reviewers for their valuable suggestions. This publica-
tion was made possible by grant NPRP-08-485-1-083
from the Qatar National Research Fund (a member of
the Qatar Foundation). The statements made herein
are solely the responsibility of the authors.
</bodyText>
<page confidence="0.996521">
170
</page>
<sectionHeader confidence="0.970231" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995614375">
Ahmed Abdul-Hamid and Kareem Darwish. 2010.
Simplified feature set for Arabic named entity
recognition. In Proceedings of the 2010 Named En-
tities Workshop, pages 110–115, Uppsala, Sweden,
July. Association for Computational Linguistics.
Mohammed Attia, Antonio Toral, Lamia Tounsi, Mon-
ica Monachini, and Josef van Genabith. 2010.
An automatically built named entity lexicon for
Arabic. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-
lios Piperidis, Mike Rosner, and Daniel Tapias, ed-
itors, Proceedings of the Seventh Conference on
International Language Resources and Evaluation
(LREC’10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Bogdan Babych and Anthony Hartley. 2003. Im-
proving machine translation quality with automatic
named entity recognition. In Proceedings of the 7th
International EAMT Workshop on MT and Other
Language Technology Tools, EAMT ’03.
Dominic Balasuriya, Nicky Ringland, Joel Nothman,
Tara Murphy, and James R. Curran. 2009. Named
entity recognition in Wikipedia. In Proceedings
of the 2009 Workshop on The People’s Web Meets
NLP: Collaboratively Constructed Semantic Re-
sources, pages 10–18, Suntec, Singapore, August.
Association for Computational Linguistics.
Yassine Benajiba, Paolo Rosso, and Jos´e Miguel
Bened´ıRuiz. 2007. ANERsys: an Arabic named
entity recognition system based on maximum en-
tropy. In Alexander Gelbukh, editor, Proceedings
of CICLing, pages 143–153, Mexico City, Mexio.
Springer.
Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008.
Arabic named entity recognition using optimized
feature sets. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 284–293, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Philip K. Chan and Salvatore J. Stolfo. 1998. To-
ward scalable learning with non-uniform class and
cost distributions: a case study in credit card fraud
detection. In Proceedings of the Fourth Interna-
tional Conference on Knowledge Discovery and
Data Mining, pages 164–168, New York City, New
York, USA, August. AAAI Press.
Ciprian Chelba and Alex Acero. 2006. Adaptation of
maximum entropy capitalizer: Little data can help
a lot. Computer Speech and Language, 20(4):382–
399.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages
168–175.
Stephen Clark, James Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 49–55.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1–
8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708–716, Prague, Czech Republic,
June.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with Mutual
Exclusion Bootstrapping. In Proceedings of PA-
CLING, 2007.
Jeffrey Dalton, James Allan, and David A. Smith.
2011. Passage retrieval for incorporating global
evidence in sequence labeling. In Proceedings of
the 20th ACM International Conference on Infor-
mation and Knowledge Management (CIKM ’11),
pages 355–364, Glasgow, Scotland, UK, October.
ACM.
Hal Daum´e III. 2007. Frustratingly easy domain
adaptation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 256–263, Prague, Czech Republic,
June. Association for Computational Linguistics.
Pedro Domingos. 1999. MetaCost: a general method
for making classifiers cost-sensitive. Proceedings
of the Fifth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining,
pages 155–164.
Benjamin Farber, Dayne Freitag, Nizar Habash, and
Owen Rambow. 2008. Improving NER in Arabic
using a morphological tagger. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odjik, Stelios Piperidis, and Daniel Tapias,
editors, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC’08), pages
2509–2514, Marrakech, Morocco, May. European
Language Resources Association (ELRA).
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A
statistical model for multilingual entity detection
and tracking. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, Proceedings of the Hu-
man Language Technology Conference of the North
</reference>
<page confidence="0.991994">
171
</page>
<reference confidence="0.998758434782609">
American Chapter of the Association for Compu-
tational Linguistics: HLT-NAACL 2004, page 18,
Boston, Massachusetts, USA, May. Association for
Computational Linguistics.
Radu Florian, John Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection ro-
bustness to noisy input. In Proceedings of EMNLP
2010, pages 335–345, Cambridge, MA, October.
Association for Computational Linguistics.
Dayne Freitag. 2004. Trained named entity recog-
nition using distributional clusters. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 262–269, Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2010a. Softmax-
margin CRFs: Training log-linear models with loss
functions. In Proceedings of the Human Language
Technologies Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 733–736, Los Angeles, California,
USA, June.
Kevin Gimpel and Noah A. Smith. 2010b.
Softmax-margin training for structured log-
linear models. Technical Report CMU-LTI-
10-008, Carnegie Mellon University. http:
//www.lti.cs.cmu.edu/research/
reports/2010/cmulti10008.pdf.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Karn Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: from guidelines to evaluation,
an overview. In Proceedings of the 5th Linguis-
tic Annotation Workshop, pages 92–100, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morpholog-
ical disambiguation in one fell swoop. In Proceed-
ings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
573–580, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan and Claypool Pub-
lishers.
Ahmed Hassan, Haytham Fahmy, and Hany Hassan.
2007. Improving named entity translation by ex-
ploiting comparable and parallel corpora. In Pro-
ceedings of the Conference on Recent Advances
in Natural Language Processing (RANLP ’07),
Borovets, Bulgaria.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the Human Language Technology Conference of
the NAACL (HLT-NAACL), pages 57–60, New York
City, USA, June. Association for Computational
Linguistics.
Dirk Hovy, Chunliang Zhang, Eduard Hovy, and
Anselmo Peas. 2011. Unsupervised discovery of
domain-specific knowledge from text. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1466–1475, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Jing Jiang and ChengXiang Zhai. 2006. Exploit-
ing domain structure for named entity recognition.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL (HLT-NAACL), pages
74–81, New York City, USA, June. Association for
Computational Linguistics.
Jun’ichi Kazama and Kentaro Torisawa. 2007.
Exploiting Wikipedia as external knowledge for
named entity recognition. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 698–707, Prague, Czech Republic,
June. Association for Computational Linguistics.
Chloe Kiddon and Yuriy Brun. 2011. That’s what
she said: double entendre identification. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 89–94, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction)
Arabic annotation guidelines for entities, version
5.3.3. Linguistic Data Consortium, Philadelphia.
Percy Liang, Hal Daum´e III, and Dan Klein. 2008.
Structure compilation: trading structure for fea-
tures. In Proceedings of the 25th International Con-
ference on Machine Learning (ICML), pages 592–
599, Helsinki, Finland.
Chris Manning. 2006. Doing named entity recogni-
tion? Don’t optimize for Fl. http://nlpers.
blogspot.com/2006/08/doing-named-
entity-recognition-dont.html.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152–159, New York City, USA, June. Association
for Computational Linguistics.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In HLT-NAACL
2004 Workshop: Eighth Conference on Computa-
tional Natural Language Learning (CoNLL-2004),
Boston, Massachusetts, USA.
</reference>
<page confidence="0.97631">
172
</page>
<reference confidence="0.999813854368932">
Einat Minkov, Richard Wang, Anthony Tomasic, and
William Cohen. 2006. NER systems that suit user’s
preferences: adjusting the recall-precision trade-off
for entity extraction. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 93–96,
New York City, USA, June. Association for Com-
putational Linguistics.
Luke Nezda, Andrew Hickl, John Lehmann, and Sar-
mad Fayyaz. 2006. What in the world is a Shahab?
Wide coverage named entity recognition for Arabic.
In Proccedings of LREC, pages 41–46.
Joel Nothman, Tara Murphy, and James R. Curran.
2009. Analysing Wikipedia and gold-standard cor-
pora for NER training. In Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2009),
pages 612–620, Athens, Greece, March. Associa-
tion for Computational Linguistics.
PediaPress. 2010. mwlib. http://code.
pediapress.com/wiki/wiki/mwlib.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705–713, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147–155, Boulder, Colorado,
June. Association for Computational Linguistics.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. Subgradient methods for maxi-
mum margin structured learning. In ICML Work-
shop on Learning in Structured Output Spaces,
Pittsburgh, Pennsylvania, USA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona
Diab, and Cynthia Rudin. 2008. Arabic morpho-
logical tagging, diacritization, and lemmatization
using lexeme models and feature ranking. In Pro-
ceedings of ACL-08: HLT, pages 117–120, Colum-
bus, Ohio, June. Association for Computational
Linguistics.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Pro-
ceedings of LREC.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Nigel Collier, Patrick Ruch, and Adeline
Nazarenko, editors, COLING 2004 International
Joint workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA/BioNLP)
2004, pages 107–110, Geneva, Switzerland, Au-
gust. COLING.
Khaled Shaalan and Hafsa Raza. 2008. Arabic
named entity recognition from diverse text types. In
Advances in Natural Language Processing, pages
440–451. Springer.
Mihai Surdeanu, David McClosky, Mason R. Smith,
Andrey Gusev, and Christopher D. Manning. 2011.
Customizing an information extraction system to
a new domain. In Proceedings of the ACL 2011
Workshop on Relational Models of Semantics, Port-
land, Oregon, USA, June. Association for Compu-
tational Linguistics.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004. Max-margin Markov networks. In Sebastian
Thrun, Lawrence Saul, and Bernhard Sch¨olkopf,
editors, Advances in Neural Information Processing
Systems 16. MIT Press.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mu˜noz. 2005. Improving question an-
swering using named entity recognition. Natu-
ral Language Processing and Information Systems,
3513/2005:181–191.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453–1484, September.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multi-
lingual training corpus. LDC2006T06, Linguistic
Data Consortium, Philadelphia.
Ralph Weischedel and Ada Brunstein. 2005.
BBN pronoun coreference and entity type cor-
pus. LDC2005T33, Linguistic Data Consortium,
Philadelphia.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named
entity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1523–1532, Singapore, August.
Association for Computational Linguistics.
Tianfang Yao, Wei Ding, and Gregor Erbach. 2003.
CHINERS: a Chinese named entity recognition sys-
tem for the sports domain. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing, pages 55–62, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.999106">
173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.224994">
<title confidence="0.953469">Recall-Oriented Learning of Named Entities in Arabic Wikipedia</title>
<author confidence="0.38879">A</author>
<affiliation confidence="0.995114">School of Computer Science, Carnegie Mellon</affiliation>
<address confidence="0.9977">Box 24866, Doha, Qatar PA 15213,</address>
<abstract confidence="0.999904368421052">We consider the problem of NER in Arabic Wikipedia, a semisupervised domain adaptation setting for which we have no labeled training data in the target domain. To facilitate evaluation, we obtain annotations for articles in four topical groups, allowing annotators to identify domain-specific entity types in addition to standard categories. Standard supervised learning on newswire text leads to poor target-domain recall. We train a sequence model and show that a simple modification to the online learner—a loss function encouraging it to “arrogantly” favor recall over precision— improves recall and We then adapt our model with self-training on unlabeled target-domain data; enforcing the same recall-oriented bias in the self-</abstract>
<intro confidence="0.58085">stage yields marginal</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abdul-Hamid</author>
<author>Kareem Darwish</author>
</authors>
<title>Simplified feature set for Arabic named entity recognition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Named Entities Workshop,</booktitle>
<pages>110--115</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="15865" citStr="Abdul-Hamid and Darwish, 2010" startWordPosition="2462" endWordPosition="2465">, 2010). 8A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). Training words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal to the challenges of a new text domain like Wikipedia.10 A descriptive list of our features is available in the supplementary document. We use a first-order structured perceptron; none of our features consider more than a pair of consecutive BIO labels at a time. The model enforces the constraint that NE sequences must begin with B (so the bigram (O, I) is disallowed). Training this model on ACE and ANER data achie</context>
<context position="23853" citStr="Abdul-Hamid and Darwish, 2010" startWordPosition="3762" endWordPosition="3765">rage 74.33 65.11 69.33 (Fold 1 corresponds to the ACE test set described in table 4.) We also trained the model to perform POL detection and classification, achieving nearly identical results in the 3-way cross-validation of ACE data. From these data we conclude that our 13Only entity spans that exactly match the gold spans are counted as correct. We calculated these scores with the conlleval.pl script from the CoNLL 2003 shared task. 14In keeping with prior work, we ignore non-POL categories for the ACE evaluation. baseline is on par with the state of the art for Arabic NER on ACE news text (Abdul-Hamid and Darwish, 2010).15 Here is the performance of the baseline entity detection model on our 20-article test set:16 P R F1 technology 60.42 20.26 30.35 science 64.96 25.73 36.86 history 63.09 35.58 45.50 sports 71.66 59.94 65.28 overall 66.30 35.91 46.59 Unsurprisingly, performance on Wikipedia data varies widely across article domains and is much lower than in-domain performance. Precision scores fall between 60% and 72% for all domains, but recall in most cases is far worse. Miscellaneous class recall, in particular, suffers badly (under 10%)—which partially accounts for the poor recall in science and technolo</context>
<context position="33553" citStr="Abdul-Hamid and Darwish, 2010" startWordPosition="5317" endWordPosition="5321">we tuned it on development data), and thus we anticipate that, in general, such tuning will be essential to leveraging the benefits of arrogance. 7 Related Work Our approach draws on insights from work in the areas of NER, domain adaptation, NLP with Wikipedia, and semisupervised learning. As all are broad areas of research, we highlight only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia v</context>
</contexts>
<marker>Abdul-Hamid, Darwish, 2010</marker>
<rawString>Ahmed Abdul-Hamid and Kareem Darwish. 2010. Simplified feature set for Arabic named entity recognition. In Proceedings of the 2010 Named Entities Workshop, pages 110–115, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mohammed Attia</author>
<author>Antonio Toral</author>
<author>Lamia Tounsi</author>
<author>Monica Monachini</author>
<author>Josef van Genabith</author>
</authors>
<title>An automatically built named entity lexicon for Arabic.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<marker>Attia, Toral, Tounsi, Monachini, van Genabith, 2010</marker>
<rawString>Mohammed Attia, Antonio Toral, Lamia Tounsi, Monica Monachini, and Josef van Genabith. 2010. An automatically built named entity lexicon for Arabic. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
</authors>
<title>Improving machine translation quality with automatic named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th International EAMT Workshop on MT and Other Language Technology Tools, EAMT ’03.</booktitle>
<contexts>
<context position="12967" citStr="Babych and Hartley, 2003" startWordPosition="2001" endWordPosition="2005">but not entirely, consistent with each other in their creation of custom categories. Further, almost all of our article-specific categories correspond to classes in the extended NE taxonomy of (Sekine et al., 2002), which speaks to the reasonableness of both sets of categories—and by extension, our open-ended annotation process. Our annotation of named entities outside of the traditional POL classes creates a useful resource for entity detection and recognition in new domains. Even the ability to detect non-canonical types of NEs should help applications such as QA and MT (Toral et al., 2005; Babych and Hartley, 2003). Possible avenues for future work include annotating and projecting non-canonical 5When it came to tagging NEs, one of the two annotators was assigned to each article. Custom categories only suggested by the other annotator were ignored. 164 NEs from English articles to their Arabic counterparts (Hassan et al., 2007), automatically clustering non-canonical types of entities into articlespecific or cross-article classes (cf. Frietag, 2004), or using non-canonical classes to improve the (author-specified) article categories in Wikipedia. Hereafter, we merge all article-specific categories with </context>
</contexts>
<marker>Babych, Hartley, 2003</marker>
<rawString>Bogdan Babych and Anthony Hartley. 2003. Improving machine translation quality with automatic named entity recognition. In Proceedings of the 7th International EAMT Workshop on MT and Other Language Technology Tools, EAMT ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Balasuriya</author>
<author>Nicky Ringland</author>
<author>Joel Nothman</author>
<author>Tara Murphy</author>
<author>James R Curran</author>
</authors>
<title>Named entity recognition in Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5188" citStr="Balasuriya et al., 2009" startWordPosition="771" endWordPosition="774"> •º:J�Ë; Spanish League (CHAMPIONSHIPS) ú��GAJ.ƒB@ ø PðYË@; proton (PARTICLE) uñ&apos;ðQK.; nuclear radiation (GENERIC-MISC) ø� ðñJ@ ¨Aª —VI; Real Zaragoza (ORG) a¢‚�¯Qå... ÈAK�P Table 1: Translated titles of Arabic Wikipedia articles in our development and test sets, and some NEs with standard and article-specific classes. Additionally, Prussia and Amman were reserved for training annotators, and Gulf War for estimating inter-annotator agreement. appropriate entity classes will vary widely by domain; occurrence rates for entity classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Se</context>
<context position="34066" citStr="Balasuriya et al. (2009)" startWordPosition="5404" endWordPosition="5407"> algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone b</context>
</contexts>
<marker>Balasuriya, Ringland, Nothman, Murphy, Curran, 2009</marker>
<rawString>Dominic Balasuriya, Nicky Ringland, Joel Nothman, Tara Murphy, and James R. Curran. 2009. Named entity recognition in Wikipedia. In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 10–18, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yassine Benajiba</author>
<author>Paolo Rosso</author>
<author>Jos´e Miguel Bened´ıRuiz</author>
</authors>
<title>ANERsys: an Arabic named entity recognition system based on maximum entropy.</title>
<date>2007</date>
<booktitle>Proceedings of CICLing,</booktitle>
<pages>143--153</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer.</publisher>
<location>Mexico City, Mexio.</location>
<marker>Benajiba, Rosso, Bened´ıRuiz, 2007</marker>
<rawString>Yassine Benajiba, Paolo Rosso, and Jos´e Miguel Bened´ıRuiz. 2007. ANERsys: an Arabic named entity recognition system based on maximum entropy. In Alexander Gelbukh, editor, Proceedings of CICLing, pages 143–153, Mexico City, Mexio. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yassine Benajiba</author>
<author>Mona Diab</author>
<author>Paolo Rosso</author>
</authors>
<title>Arabic named entity recognition using optimized feature sets.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>284--293</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="3499" citStr="Benajiba et al., 2008" startWordPosition="526" endWordPosition="530">ical evaluation of this technique as applied to a well-established discriminative NER model and feature set. Experiments show consistent gains on the challenging problem of identifying named entities in Arabic Wikipedia text. 2 Arabic Wikipedia NE Annotation Most of the effort in NER has been focused around a small set of domains and general-purpose entity classes relevant to those domains—especially the categories PER(SON), ORG(ANIZATION), and LOC(ATION) (POL), which are highly prominent in news text. Arabic is no exception: the publicly available NER corpora—ACE (Walker et al., 2006), ANER (Benajiba et al., 2008), and OntoNotes (Hovy et al., 2006)—all are in the news domain.2 However, 2OntoNotes contains news-related text. ACE includes some text from blogs. In addition to the POL classes, both corpora include additional NE classes such as facility, event, product, vehicle, etc. These entities are infrequent and may not be comprehensive enough to cover the larger set of pos162 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162–173, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics History Science Spo</context>
<context position="15833" citStr="Benajiba et al., 2008" startWordPosition="2457" endWordPosition="2461"> for Python (PediaPress, 2010). 8A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). Training words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal to the challenges of a new text domain like Wikipedia.10 A descriptive list of our features is available in the supplementary document. We use a first-order structured perceptron; none of our features consider more than a pair of consecutive BIO labels at a time. The model enforces the constraint that NE sequences must begin with B (so the bigram (O, I) is disallowed). Training this </context>
<context position="33476" citStr="Benajiba et al., 2008" startWordPosition="5305" endWordPosition="5308">is highly sensitive to the choice of this value (figure 1 shows how we tuned it on development data), and thus we anticipate that, in general, such tuning will be essential to leveraging the benefits of arrogance. 7 Related Work Our approach draws on insights from work in the areas of NER, domain adaptation, NLP with Wikipedia, and semisupervised learning. As all are broad areas of research, we highlight only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight</context>
</contexts>
<marker>Benajiba, Diab, Rosso, 2008</marker>
<rawString>Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008. Arabic named entity recognition using optimized feature sets. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 284–293, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip K Chan</author>
<author>Salvatore J Stolfo</author>
</authors>
<title>Toward scalable learning with non-uniform class and cost distributions: a case study in credit card fraud detection.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>164--168</pages>
<publisher>AAAI Press.</publisher>
<location>New York City, New York, USA,</location>
<contexts>
<context position="36748" citStr="Chan and Stolfo, 1998" startWordPosition="5835" endWordPosition="5838">(Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled training data are available. A loss function to encourage recall over precision during supervised discriminative learning substantially improves recall and overall entity detection performance, especially when combined with a semisupervised learning regimen incorporating the same bias. We have also developed a sma</context>
</contexts>
<marker>Chan, Stolfo, 1998</marker>
<rawString>Philip K. Chan and Salvatore J. Stolfo. 1998. Toward scalable learning with non-uniform class and cost distributions: a case study in credit card fraud detection. In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, pages 164–168, New York City, New York, USA, August. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot. Computer Speech and Language,</title>
<date>2006</date>
<volume>20</volume>
<issue>4</issue>
<pages>399</pages>
<contexts>
<context position="31737" citStr="Chelba and Acero, 2006" startWordPosition="5015" endWordPosition="5018">knowledge, this work is the first suggestion that substantively modifying the supervised learning criterion in a resource-rich domain can reap benefits in subsequent semisupervised application in a new domain. Past work has looked 20Tuning the O feature weight to optimize for Fl on our test set, we found that oracle precision would be 66.2, recall would be 39.0, and Fl would be 49.1. The Fl score of our best model is nearly 3 points higher than the Minkov et al.– style oracle, and over 4 points higher than the non-oracle version where the development set is used for tuning. at regularization (Chelba and Acero, 2006) and feature design (Daum´e III, 2007); we alter the loss function. Not surprisingly, the double-ROP approach harms performance on the original domain (on ACE data, we achieve 55.41% F1, far below the standard perceptron). Yet we observe that models can be prepared for adaptation even before a learner is exposed a new domain, sacrificing performance in the original domain. The recall-oriented bias is not merely encouraging the learner to identify entities already seen in training. As recall increases, so does the number of new entity types recovered by the model: of the 2,070 NE types in the t</context>
</contexts>
<marker>Chelba, Acero, 2006</marker>
<rawString>Ciprian Chelba and Alex Acero. 2006. Adaptation of maximum entropy capitalizer: Little data can help a lot. Computer Speech and Language, 20(4):382– 399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Supersense tagging of unknown nouns in WordNet.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>168--175</pages>
<contexts>
<context position="5944" citStr="Ciaramita and Johnson, 2003" startWordPosition="881" endWordPosition="885"> elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to pro</context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2003. Supersense tagging of unknown nouns in WordNet. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 168–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
<author>Miles Osborne</author>
</authors>
<title>Bootstrapping POS-taggers using unlabelled data.</title>
<date>2003</date>
<booktitle>Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003,</booktitle>
<pages>49--55</pages>
<editor>In Walter Daelemans and Miles Osborne, editors,</editor>
<contexts>
<context position="20117" citStr="Clark et al., 2003" startWordPosition="3181" endWordPosition="3184">tter at recall on the training data. Input: labeled data ((x(n), y(n)))N n=1; unlabeled data (x(j))J j=1; supervised learner L; number of iterations T&apos; Output: w w — L(((x(n), y(n)))Nn=1) fort = 1 to T&apos; do for j = 1 to J do :0(j) — arg maxV wTg(.t(j), y) w — L(((x(n), y(n)))Nn=1 U ((.t(j), y(j)))J j=1) Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 Experiments We investigate two questions in the context of NER for Arabic Wikipedia: • Loss function: Does integrating a cost funct</context>
<context position="24571" citStr="Clark et al. (2003)" startWordPosition="3874" endWordPosition="3877">R F1 technology 60.42 20.26 30.35 science 64.96 25.73 36.86 history 63.09 35.58 45.50 sports 71.66 59.94 65.28 overall 66.30 35.91 46.59 Unsurprisingly, performance on Wikipedia data varies widely across article domains and is much lower than in-domain performance. Precision scores fall between 60% and 72% for all domains, but recall in most cases is far worse. Miscellaneous class recall, in particular, suffers badly (under 10%)—which partially accounts for the poor recall in science and technology articles (they have by far the highest proportion of MIS entities). 5.2 Self-Training Following Clark et al. (2003), we applied selftraining as described in Algorithm 1, with the perceptron as the supervised learner. Our unlabeled data consists of 397 Arabic Wikipedia articles (1 million words) selected at random from all articles exceeding a simple length threshold (1,000 words); see table 4. We used only one iteration (V = 1), as experiments on development data showed no benefit from additional rounds. Several rounds of self-training hurt performance, 15Abdul-Hamid and Darwish report as their best result a macroaveraged Fl-score of 76. As they do not specify which data they used for their held-out test s</context>
<context position="36250" citStr="Clark et al., 2003" startWordPosition="5755" endWordPosition="5758">tinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall i</context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>Stephen Clark, James Curran, and Miles Osborne. 2003. Bootstrapping POS-taggers using unlabelled data. In Walter Daelemans and Miles Osborne, editors, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14941" citStr="Collins, 2002" startWordPosition="2321" endWordPosition="2322"> test data. A larger set of Arabic Wikipedia articles, selected on the basis of quality heuristics, serves as unlabeled data for semisupervised learning. Our out-of-domain labeled NE data is drawn from the ANER (Benajiba et al., 2007) and ACE-2005 (Walker et al., 2006) newswire corpora. Entity types in this data are POL categories (PER, ORG, LOC) and MIS. Portions of the ACE corpus were held out as development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 fea6Additional details appear in the supplement. 7We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). Training words NEs ACE+ANER 212,839 15,796 Wik</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1– 8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>708--716</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="34844" citStr="Cucerzan (2007)" startWordPosition="5529" endWordPosition="5530">ance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain da</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 708–716, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Tara Murphy</author>
<author>Bernhard Scholz</author>
</authors>
<title>Minimising semantic drift with Mutual Exclusion Bootstrapping.</title>
<date>2007</date>
<booktitle>In Proceedings of PACLING,</booktitle>
<contexts>
<context position="26561" citStr="Curran et al., 2007" startWordPosition="4196" endWordPosition="4199"> whether this supervised learning phase was followed by selftraining on unlabeled Wikipedia data, and if so which version of the perceptron was used for self-training. baseline entities words recall PER 1081 1743 49.95 ORG 286 637 23.92 LOC 1019 1413 61.43 MIS 1395 2176 9.30 overall 3781 5969 35.91 Figure 2: Recall improvement over baseline in the test set by gold NER category, counts for those categories in the data, and recall scores for our baseline model. Markers in the plot indicate different experimental settings corresponding to cells in table 5. an effect attested in earlier research (Curran et al., 2007) and sometimes known as “semantic drift.” Results are shown in table 5. We find that standard self-training (the middle column) has very little impact on performance.17 Why is this the case? We venture that poor baseline recall and the domain variability within Wikipedia are to blame. 5.3 Recall-Oriented Learning The recall-oriented bias can be introduced in either or both of the stages of our semisupervised learning framework: in the supervised learning phase, modifying the objective of our baseline (§5.1); and within the self-training algorithm (§5.2).18 As noted in §4.1, the aim of this app</context>
</contexts>
<marker>Curran, Murphy, Scholz, 2007</marker>
<rawString>James R. Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with Mutual Exclusion Bootstrapping. In Proceedings of PACLING, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dalton</author>
<author>James Allan</author>
<author>David A Smith</author>
</authors>
<title>Passage retrieval for incorporating global evidence in sequence labeling.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM ’11),</booktitle>
<pages>355--364</pages>
<publisher>ACM.</publisher>
<location>Glasgow, Scotland, UK,</location>
<contexts>
<context position="36148" citStr="Dalton et al., 2011" startWordPosition="5740" endWordPosition="5743">re, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998</context>
</contexts>
<marker>Dalton, Allan, Smith, 2011</marker>
<rawString>Jeffrey Dalton, James Allan, and David A. Smith. 2011. Passage retrieval for incorporating global evidence in sequence labeling. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM ’11), pages 355–364, Glasgow, Scotland, UK, October. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
</authors>
<title>MetaCost: a general method for making classifiers cost-sensitive.</title>
<date>1999</date>
<booktitle>Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>155--164</pages>
<contexts>
<context position="36764" citStr="Domingos, 1999" startWordPosition="5839" endWordPosition="5840">which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled training data are available. A loss function to encourage recall over precision during supervised discriminative learning substantially improves recall and overall entity detection performance, especially when combined with a semisupervised learning regimen incorporating the same bias. We have also developed a small corpus of Ara</context>
</contexts>
<marker>Domingos, 1999</marker>
<rawString>Pedro Domingos. 1999. MetaCost: a general method for making classifiers cost-sensitive. Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 155–164.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Benjamin Farber</author>
<author>Dayne Freitag</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Improving NER in Arabic using a morphological tagger.</title>
<date>2008</date>
<booktitle>Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<pages>2509--2514</pages>
<editor>In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors,</editor>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="33497" citStr="Farber et al., 2008" startWordPosition="5309" endWordPosition="5312">the choice of this value (figure 1 shows how we tuned it on development data), and thus we anticipate that, in general, such tuning will be essential to leveraging the benefits of arrogance. 7 Related Work Our approach draws on insights from work in the areas of NER, domain adaptation, NLP with Wikipedia, and semisupervised learning. As all are broad areas of research, we highlight only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial dive</context>
</contexts>
<marker>Farber, Freitag, Habash, Rambow, 2008</marker>
<rawString>Benjamin Farber, Dayne Freitag, Nizar Habash, and Owen Rambow. 2008. Improving NER in Arabic using a morphological tagger. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), pages 2509–2514, Marrakech, Morocco, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
</authors>
<title>Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos.</title>
<date>2004</date>
<booktitle>Proceedings of the Human Language Technology Conference of the North</booktitle>
<editor>In Susan Dumais, Daniel Marcu, and Salim Roukos, editors,</editor>
<marker>Florian, 2004</marker>
<rawString>Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Susan Dumais, Daniel Marcu, and Salim Roukos, editors, Proceedings of the Human Language Technology Conference of the North</rawString>
</citation>
<citation valid="true">
<title>American Chapter of the Association for Computational Linguistics: HLT-NAACL</title>
<date>2004</date>
<pages>18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="34723" citStr="(2004)" startWordPosition="5514" endWordPosition="5514">ies appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that br</context>
</contexts>
<marker>2004</marker>
<rawString>American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, page 18, Boston, Massachusetts, USA, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>John Pitrelli</author>
<author>Salim Roukos</author>
<author>Imed Zitouni</author>
</authors>
<title>Improving mention detection robustness to noisy input.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<pages>335--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="35973" citStr="Florian et al. (2010)" startWordPosition="5713" endWordPosition="5716">mains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them </context>
</contexts>
<marker>Florian, Pitrelli, Roukos, Zitouni, 2010</marker>
<rawString>Radu Florian, John Pitrelli, Salim Roukos, and Imed Zitouni. 2010. Improving mention detection robustness to noisy input. In Proceedings of EMNLP 2010, pages 335–345, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Trained named entity recognition using distributional clusters.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>262--269</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<marker>Freitag, 2004</marker>
<rawString>Dayne Freitag. 2004. Trained named entity recognition using distributional clusters. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 262–269, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Softmaxmargin CRFs: Training log-linear models with loss functions.</title>
<date>2010</date>
<booktitle>In Proceedings of the Human Language Technologies Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>733--736</pages>
<location>Los Angeles, California, USA,</location>
<contexts>
<context position="18139" citStr="Gimpel and Smith (2010" startWordPosition="2843" endWordPosition="2846">165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorporating cost-augmented decoding into the perceptron leads to this decoding step: ( ) w&gt;g(x, y0) + c(y, y0) , (2) which amounts to performing stochastic subgradient ascent on an objective function with the Eq. 1 loss (Ratliff et al., 2006). In this framework, cost functions can be formulated to distinguish between different types of errors made during training. For a tag sequence y = (y1, y2, ... , yM), Gimpel and Smith (2010b) define word-local cost functions that differently penalize precision errors (i.e., yi = O n yi =� O for the ith word), recall errors (yi =� O n yi = O), and entity class/position errors (other cases where yi =� yi). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely: { 0 if yi = y0i (3) Q if yi =� O n y0 i = O 1 otherwise for a penalty parameter Q &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tradeoff in NER. Their technique </context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2010a. Softmaxmargin CRFs: Training log-linear models with loss functions. In Proceedings of the Human Language Technologies Conference of the North American Chapter of the Association for Computational Linguistics, pages 733–736, Los Angeles, California, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Softmax-margin training for structured loglinear models.</title>
<date>2010</date>
<tech>Technical Report CMU-LTI10-008,</tech>
<institution>Carnegie Mellon University.</institution>
<note>http: //www.lti.cs.cmu.edu/research/ reports/2010/cmulti10008.pdf.</note>
<contexts>
<context position="18139" citStr="Gimpel and Smith (2010" startWordPosition="2843" endWordPosition="2846">165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorporating cost-augmented decoding into the perceptron leads to this decoding step: ( ) w&gt;g(x, y0) + c(y, y0) , (2) which amounts to performing stochastic subgradient ascent on an objective function with the Eq. 1 loss (Ratliff et al., 2006). In this framework, cost functions can be formulated to distinguish between different types of errors made during training. For a tag sequence y = (y1, y2, ... , yM), Gimpel and Smith (2010b) define word-local cost functions that differently penalize precision errors (i.e., yi = O n yi =� O for the ith word), recall errors (yi =� O n yi = O), and entity class/position errors (other cases where yi =� yi). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely: { 0 if yi = y0i (3) Q if yi =� O n y0 i = O 1 otherwise for a penalty parameter Q &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tradeoff in NER. Their technique </context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2010b. Softmax-margin training for structured loglinear models. Technical Report CMU-LTI10-008, Carnegie Mellon University. http: //www.lti.cs.cmu.edu/research/ reports/2010/cmulti10008.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Grouin</author>
<author>Sophie Rosset</author>
<author>Pierre Zweigenbaum</author>
<author>Karn Fort</author>
<author>Olivier Galibert</author>
<author>Ludovic Quintard</author>
</authors>
<title>Proposal for an extension of traditional named entities: from guidelines to evaluation, an overview.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th Linguistic Annotation Workshop,</booktitle>
<pages>92--100</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5746" citStr="Grouin et al., 2011" startWordPosition="852" endWordPosition="855">ews text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (bu</context>
</contexts>
<marker>Grouin, Rosset, Zweigenbaum, Fort, Galibert, Quintard, 2011</marker>
<rawString>Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum, Karn Fort, Olivier Galibert, and Ludovic Quintard. 2011. Proposal for an extension of traditional named entities: from guidelines to evaluation, an overview. In Proceedings of the 5th Linguistic Annotation Workshop, pages 92–100, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>573--580</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="15473" citStr="Habash and Rambow, 2005" startWordPosition="2397" endWordPosition="2400">ebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 fea6Additional details appear in the supplement. 7We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). Training words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal to the challenges of a new</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 573–580, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing.</title>
<date>2010</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="1464" citStr="Habash, 2010" startWordPosition="209" endWordPosition="210"> encouraging it to “arrogantly” favor recall over precision— substantially improves recall and Fl. We then adapt our model with self-training on unlabeled target-domain data; enforcing the same recall-oriented bias in the selftraining stage yields marginal gains.1 1 Introduction This paper considers named entity recognition (NER) in text that is different from most past research on NER. Specifically, we consider Arabic Wikipedia articles with diverse topics beyond the commonly-used news domain. These data challenge past approaches in two ways: First, Arabic is a morphologically rich language (Habash, 2010). Named entities are referenced using complex syntactic constructions (cf. English NEs, which are primarily sequences of proper nouns). The Arabic script suppresses most vowels, increasing lexical ambiguity, and lacks capitalization, a key clue for English NER. Second, much research has focused on the use of news text for system building and evaluation. Wikipedia articles are not news, belonging instead to a wide range of domains that are not clearly 1The annotated dataset and a supplementary document with additional details of this work can be found at: http://www.ark.cs.cmu.edu/AQMAR delinea</context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Habash. 2010. Introduction to Arabic Natural Language Processing. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Haytham Fahmy</author>
<author>Hany Hassan</author>
</authors>
<title>Improving named entity translation by exploiting comparable and parallel corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Recent Advances in Natural Language Processing (RANLP ’07),</booktitle>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="13286" citStr="Hassan et al., 2007" startWordPosition="2053" endWordPosition="2056">ocess. Our annotation of named entities outside of the traditional POL classes creates a useful resource for entity detection and recognition in new domains. Even the ability to detect non-canonical types of NEs should help applications such as QA and MT (Toral et al., 2005; Babych and Hartley, 2003). Possible avenues for future work include annotating and projecting non-canonical 5When it came to tagging NEs, one of the two annotators was assigned to each article. Custom categories only suggested by the other annotator were ignored. 164 NEs from English articles to their Arabic counterparts (Hassan et al., 2007), automatically clustering non-canonical types of entities into articlespecific or cross-article classes (cf. Frietag, 2004), or using non-canonical classes to improve the (author-specified) article categories in Wikipedia. Hereafter, we merge all article-specific categories with the generic MIS category. The proportion of entity mentions that are tagged as MIS, while varying to a large extent by document, is a major indication of the gulf between the news data (&lt;10%) and the Wikipedia data (53% for the development set, 37% for the test set). Below, we aim to develop entity detection models th</context>
</contexts>
<marker>Hassan, Fahmy, Hassan, 2007</marker>
<rawString>Ahmed Hassan, Haytham Fahmy, and Hany Hassan. 2007. Improving named entity translation by exploiting comparable and parallel corpora. In Proceedings of the Conference on Recent Advances in Natural Language Processing (RANLP ’07), Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL (HLT-NAACL),</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="3534" citStr="Hovy et al., 2006" startWordPosition="533" endWordPosition="536">plied to a well-established discriminative NER model and feature set. Experiments show consistent gains on the challenging problem of identifying named entities in Arabic Wikipedia text. 2 Arabic Wikipedia NE Annotation Most of the effort in NER has been focused around a small set of domains and general-purpose entity classes relevant to those domains—especially the categories PER(SON), ORG(ANIZATION), and LOC(ATION) (POL), which are highly prominent in news text. Arabic is no exception: the publicly available NER corpora—ACE (Walker et al., 2006), ANER (Benajiba et al., 2008), and OntoNotes (Hovy et al., 2006)—all are in the news domain.2 However, 2OntoNotes contains news-related text. ACE includes some text from blogs. In addition to the POL classes, both corpora include additional NE classes such as facility, event, product, vehicle, etc. These entities are infrequent and may not be comprehensive enough to cover the larger set of pos162 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162–173, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics History Science Sports Technology dev: Damascus Atom R</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL (HLT-NAACL), pages 57–60, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Chunliang Zhang</author>
<author>Eduard Hovy</author>
<author>Anselmo Peas</author>
</authors>
<title>Unsupervised discovery of domain-specific knowledge from text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1466--1475</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6176" citStr="Hovy et al., 2011" startWordPosition="918" endWordPosition="921">ng this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to provide a testbed for evaluation of new NER models. We will use these data as development and sible NEs (Sekine et al., 2002). Nezda et al. (2006) annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (includin</context>
</contexts>
<marker>Hovy, Zhang, Hovy, Peas, 2011</marker>
<rawString>Dirk Hovy, Chunliang Zhang, Eduard Hovy, and Anselmo Peas. 2011. Unsupervised discovery of domain-specific knowledge from text. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1466–1475, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Exploiting domain structure for named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL (HLT-NAACL),</booktitle>
<pages>74--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="35380" citStr="Jiang and Zhai (2006)" startWordPosition="5619" endWordPosition="5622">t entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who r</context>
</contexts>
<marker>Jiang, Zhai, 2006</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2006. Exploiting domain structure for named entity recognition. In Proceedings of the Human Language Technology Conference of the NAACL (HLT-NAACL), pages 74–81, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>698--707</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="33886" citStr="Kazama and Torisawa, 2007" startWordPosition="5376" endWordPosition="5379">t only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedi</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 698–707, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chloe Kiddon</author>
<author>Yuriy Brun</author>
</authors>
<title>That’s what she said: double entendre identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>89--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="36788" citStr="Kiddon and Brun, 2011" startWordPosition="5841" endWordPosition="5844">English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled training data are available. A loss function to encourage recall over precision during supervised discriminative learning substantially improves recall and overall entity detection performance, especially when combined with a semisupervised learning regimen incorporating the same bias. We have also developed a small corpus of Arabic Wikipedia articles v</context>
</contexts>
<marker>Kiddon, Brun, 2011</marker>
<rawString>Chloe Kiddon and Yuriy Brun. 2011. That’s what she said: double entendre identification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 89–94, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="22769" citStr="Koehn, 2004" startWordPosition="3580" endWordPosition="3581">ised model as the initial labeler for recall-oriented self-training. • ROP/ROP (the “double ROP” condition): recalloriented supervised model as the initial labeler for recall-oriented self-training. Note that the two ROPs can use different cost parameters. For evaluating our models we consider the named entity detection task, i.e., recognizing which spans of words constitute entities. This is measured by per-entity precision, recall, and F1.13 To measure statistical significance of differences between models we use Gimpel and Smith’s (2010) implementation of the paired bootstrap resampler of (Koehn, 2004), taking 10,000 samples for each comparison. 5.1 Baseline Our baseline is the perceptron, trained on the POL entity boundaries in the ACE+ANER corpus (reg/none).14 Development data was used to select the number of iterations (10). We performed 3-fold cross-validation on the ACE data and found wide variance in the in-domain entity detection performance of this model: P R F1 fold 1 70.43 63.08 66.55 fold 2 87.48 81.13 84.18 fold 3 65.09 51.13 57.27 average 74.33 65.11 69.33 (Fold 1 corresponds to the ACE test set described in table 4.) We also trained the model to perform POL detection and class</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>ACE (Automatic Content Extraction) Arabic annotation guidelines for entities, version 5.3.3. Linguistic Data Consortium,</title>
<date>2005</date>
<location>Philadelphia.</location>
<contexts>
<context position="7810" citStr="LDC, 2005" startWordPosition="1186" endWordPosition="1187">om 31 articles. We identified four topical areas of interest—history, technology, science, and sports—and browsed these topics until we had found 31 articles that we deemed satisfactory on the basis of length (at least 1,000 words), cross-lingual linkages (associated articles in English, German, and Chinese3), and subjective judgments of quality. The list of these articles along with sample NEs are presented in table 1. These articles were then preprocessed to extract main article text (eliminating tables, lists, info-boxes, captions, etc.) for annotation. Our approach follows ACE guidelines (LDC, 2005) in identifying NE boundaries and choosing POL tags. In addition to this traditional form of annotation, annotators were encouraged to articulate one to three salient, article-specific entity categories per article. For example, names of particles (e.g., proton) are highly salient in the Atom article. Annotators were asked to read the entire article first, and then to decide which nontraditional classes of entities would be important in the context of article. In some cases, annotators reported using heuristics (such as being proper 3These three languages have the most articles on Wikipedia. A</context>
</contexts>
<marker>LDC, 2005</marker>
<rawString>LDC. 2005. ACE (Automatic Content Extraction) Arabic annotation guidelines for entities, version 5.3.3. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daum´e</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: trading structure for features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning (ICML),</booktitle>
<pages>592--599</pages>
<location>Helsinki, Finland.</location>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daum´e III, and Dan Klein. 2008. Structure compilation: trading structure for features. In Proceedings of the 25th International Conference on Machine Learning (ICML), pages 592– 599, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Manning</author>
</authors>
<title>Doing named entity recognition? Don’t optimize for Fl.</title>
<date>2006</date>
<note>http://nlpers. blogspot.com/2006/08/doing-namedentity-recognition-dont.html.</note>
<contexts>
<context position="17434" citStr="Manning, 2006" startWordPosition="2724" endWordPosition="2725">h structured SVMs (Taskar et al., 2004; Tsochantaridis et al., 2005). Let c(y, y0) denote a measure of error when y is the correct label sequence but y0 is predicted. For observed sequence x and feature weights (model parameters) w, the structured hinge loss is Ehz,,ye(x, y, w) _ ( ) w&gt;g(x, y0) + c(y, y0) − w&gt;g(x, y) (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11Though optimizing NER systems for Fl has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. max Y 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorporating cost-augmented decoding into the perceptron leads to this decoding step: ( ) w&gt;g(x, y0) + c(y, y0) , (2) which amounts to performing stochastic subgradient ascent on an objective function with the Eq. 1 loss (Ratliff et al., 2006). In this framework, cost functions can be formulated to distinguish between differen</context>
</contexts>
<marker>Manning, 2006</marker>
<rawString>Chris Manning. 2006. Doing named entity recognition? Don’t optimize for Fl. http://nlpers. blogspot.com/2006/08/doing-namedentity-recognition-dont.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="20157" citStr="McClosky et al., 2006" startWordPosition="3187" endWordPosition="3190"> Input: labeled data ((x(n), y(n)))N n=1; unlabeled data (x(j))J j=1; supervised learner L; number of iterations T&apos; Output: w w — L(((x(n), y(n)))Nn=1) fort = 1 to T&apos; do for j = 1 to J do :0(j) — arg maxV wTg(.t(j), y) w — L(((x(n), y(n)))Nn=1 U ((.t(j), y(j)))J j=1) Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 Experiments We investigate two questions in the context of NER for Arabic Wikipedia: • Loss function: Does integrating a cost function into our learning algorithm, as we h</context>
<context position="36290" citStr="McClosky et al., 2006" startWordPosition="5761" endWordPosition="5764"> this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Co-training and self-training for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004),</booktitle>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="20133" citStr="Mihalcea, 2004" startWordPosition="3185" endWordPosition="3186">e training data. Input: labeled data ((x(n), y(n)))N n=1; unlabeled data (x(j))J j=1; supervised learner L; number of iterations T&apos; Output: w w — L(((x(n), y(n)))Nn=1) fort = 1 to T&apos; do for j = 1 to J do :0(j) — arg maxV wTg(.t(j), y) w — L(((x(n), y(n)))Nn=1 U ((.t(j), y(j)))J j=1) Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 Experiments We investigate two questions in the context of NER for Arabic Wikipedia: • Loss function: Does integrating a cost function into our lea</context>
<context position="36266" citStr="Mihalcea, 2004" startWordPosition="5759" endWordPosition="5760">ain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar </context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Co-training and self-training for word sense disambiguation. In HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004), Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Richard Wang</author>
<author>Anthony Tomasic</author>
<author>William Cohen</author>
</authors>
<title>NER systems that suit user’s preferences: adjusting the recall-precision trade-off for entity extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>93--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="18661" citStr="Minkov et al. (2006)" startWordPosition="2946" endWordPosition="2949"> of errors made during training. For a tag sequence y = (y1, y2, ... , yM), Gimpel and Smith (2010b) define word-local cost functions that differently penalize precision errors (i.e., yi = O n yi =� O for the ith word), recall errors (yi =� O n yi = O), and entity class/position errors (other cases where yi =� yi). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely: { 0 if yi = y0i (3) Q if yi =� O n y0 i = O 1 otherwise for a penalty parameter Q &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tradeoff in NER. Their technique was to directly tune the weight of a single feature—the feature marking O (nonentity tokens); a lower weight for this feature will incur a greater penalty for predicting O. Below we demonstrate that our method, which is less coarse, is more successful in our setting.12 In our experiments we will show that injecting “arrogance” into the learner via the recall-oriented loss function substantially improves recall, especially for non-POL entities (§5.3). 4.2 Self-Training and Semisupervised Learning As we will show exper</context>
<context position="30517" citStr="Minkov et al. (2006)" startWordPosition="4813" endWordPosition="4816">he biggest boost. Still, the gaps between domains are not entirely removed. Most improvements relate to the reduction of false negatives, which fall into three groups: (a) entities occurring infrequently or partially in the labeled training data (e.g. uranium); (b) domain-specific entities sharing lexical or contextual features with the POL entities (e.g. Linux, titanium); and (c) words with Latin characters, common in the science and technology domains. (a) and (b) are mostly transliterations into Arabic. An alternative—and simpler—approach to controlling the precision-recall tradeoff is the Minkov et al. (2006) strategy of tuning a single feature weight subsequent to learning (see §4.1 above). We performed an oracle experiment to determine how this compares to recall-oriented learning in our setting. An oracle trained with the method of Minkov et al. outperforms the three models in table 5 that use the regular perceptron for the supervised phase of learning, but underperforms the supervised ROP conditions.20 Overall, we find that incorporating the recalloriented bias in learning is fruitful for adapting to Wikipedia because the gains in recall outpace the damage to precision. 6 Discussion To our kno</context>
<context position="36914" citStr="Minkov et al. (2006)" startWordPosition="5864" endWordPosition="5867">widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled training data are available. A loss function to encourage recall over precision during supervised discriminative learning substantially improves recall and overall entity detection performance, especially when combined with a semisupervised learning regimen incorporating the same bias. We have also developed a small corpus of Arabic Wikipedia articles via a flexible entity annotation scheme spanning four topical domains (publicly available at http://www.ark.cs. cmu.edu/AQMAR).</context>
</contexts>
<marker>Minkov, Wang, Tomasic, Cohen, 2006</marker>
<rawString>Einat Minkov, Richard Wang, Anthony Tomasic, and William Cohen. 2006. NER systems that suit user’s preferences: adjusting the recall-precision trade-off for entity extraction. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 93–96, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Nezda</author>
<author>Andrew Hickl</author>
<author>John Lehmann</author>
<author>Sarmad Fayyaz</author>
</authors>
<title>What in the world is a Shahab? Wide coverage named entity recognition for Arabic.</title>
<date>2006</date>
<booktitle>In Proccedings of LREC,</booktitle>
<pages>41--46</pages>
<contexts>
<context position="6687" citStr="Nezda et al. (2006)" startWordPosition="1008" endWordPosition="1011">e—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to provide a testbed for evaluation of new NER models. We will use these data as development and sible NEs (Sekine et al., 2002). Nezda et al. (2006) annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (including temporal and numeric entities); this corpus has not been released publicly. testing examples, but not as training data. In §4 we will discuss our semisupervised approach to learning, which leverages ACE and ANER data as an annotated training corpus. 2.1 Annotation Strategy We conducted a small annotation project on Arabic Wikipedia articles. Two college-educated native Arabic speakers annotated about 3,000 sentences from 31 articles. We identified four topical areas of interest—history, technology, scien</context>
</contexts>
<marker>Nezda, Hickl, Lehmann, Fayyaz, 2006</marker>
<rawString>Luke Nezda, Andrew Hickl, John Lehmann, and Sarmad Fayyaz. 2006. What in the world is a Shahab? Wide coverage named entity recognition for Arabic. In Proccedings of LREC, pages 41–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Nothman</author>
<author>Tara Murphy</author>
<author>James R Curran</author>
</authors>
<title>Analysing Wikipedia and gold-standard corpora for NER training.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>612--620</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="34374" citStr="Nothman et al. (2009)" startWordPosition="5451" endWordPosition="5454">glish and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The d</context>
</contexts>
<marker>Nothman, Murphy, Curran, 2009</marker>
<rawString>Joel Nothman, Tara Murphy, and James R. Curran. 2009. Analysing Wikipedia and gold-standard corpora for NER training. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2009), pages 612–620, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PediaPress</author>
</authors>
<date>2010</date>
<note>mwlib. http://code. pediapress.com/wiki/wiki/mwlib.</note>
<contexts>
<context position="15242" citStr="PediaPress, 2010" startWordPosition="2364" endWordPosition="2365">es in this data are POL categories (PER, ORG, LOC) and MIS. Portions of the ACE corpus were held out as development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 fea6Additional details appear in the supplement. 7We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). Training words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-H</context>
</contexts>
<marker>PediaPress, 2010</marker>
<rawString>PediaPress. 2010. mwlib. http://code. pediapress.com/wiki/wiki/mwlib.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Pi-Chuan Chang</author>
<author>Michael Ringgaard</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>705--713</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="36433" citStr="Petrov et al., 2010" startWordPosition="5785" endWordPosition="5788">stinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled </context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705–713, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>147--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="17348" citStr="Ratinov and Roth, 2009" startWordPosition="2709" endWordPosition="2712">t function term, we can incorporate a task-dependent notion of error into the objective, as with structured SVMs (Taskar et al., 2004; Tsochantaridis et al., 2005). Let c(y, y0) denote a measure of error when y is the correct label sequence but y0 is predicted. For observed sequence x and feature weights (model parameters) w, the structured hinge loss is Ehz,,ye(x, y, w) _ ( ) w&gt;g(x, y0) + c(y, y0) − w&gt;g(x, y) (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11Though optimizing NER systems for Fl has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. max Y 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorporating cost-augmented decoding into the perceptron leads to this decoding step: ( ) w&gt;g(x, y0) + c(y, y0) , (2) which amounts to performing stochastic subgradient ascent on an objective function with the Eq. 1 loss (Ratliff et al., 2006</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 147–155, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan D Ratliff</author>
<author>J Andrew Bagnell</author>
<author>Martin A Zinkevich</author>
</authors>
<title>Subgradient methods for maximum margin structured learning.</title>
<date>2006</date>
<booktitle>In ICML Workshop on Learning in Structured Output Spaces,</booktitle>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="17949" citStr="Ratliff et al., 2006" startWordPosition="2809" endWordPosition="2812">inov and Roth, 2009). 11Though optimizing NER systems for Fl has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. max Y 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorporating cost-augmented decoding into the perceptron leads to this decoding step: ( ) w&gt;g(x, y0) + c(y, y0) , (2) which amounts to performing stochastic subgradient ascent on an objective function with the Eq. 1 loss (Ratliff et al., 2006). In this framework, cost functions can be formulated to distinguish between different types of errors made during training. For a tag sequence y = (y1, y2, ... , yM), Gimpel and Smith (2010b) define word-local cost functions that differently penalize precision errors (i.e., yi = O n yi =� O for the ith word), recall errors (yi =� O n yi = O), and entity class/position errors (other cases where yi =� yi). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely: { 0 if yi = y0i (3) Q if yi =� O n y0 i = O 1 otherwise for a penalt</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2006</marker>
<rawString>Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. 2006. Subgradient methods for maximum margin structured learning. In ICML Workshop on Learning in Structured Output Spaces, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Nizar Habash</author>
<author>Mona Diab</author>
<author>Cynthia Rudin</author>
</authors>
<title>Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>117--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="15493" citStr="Roth et al., 2008" startWordPosition="2401" endWordPosition="2404">sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 fea6Additional details appear in the supplement. 7We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). Training words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal to the challenges of a new text domain like Wi</context>
</contexts>
<marker>Roth, Rambow, Habash, Diab, Rudin, 2008</marker>
<rawString>Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin. 2008. Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking. In Proceedings of ACL-08: HLT, pages 117–120, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
<author>Chikashi Nobata</author>
</authors>
<title>Extended named entity hierarchy.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="5692" citStr="Sekine et al., 2002" startWordPosition="844" endWordPosition="847">nce rates for entity classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it</context>
<context position="12556" citStr="Sekine et al., 2002" startWordPosition="1934" endWordPosition="1937">ross our four focus domains) and suggest up to 3 custom categories for each. We assigned short names to these suggestions, seen in table 3. In 13 cases, both annotators suggested a category for an article that was essentially the same (•); three such categories spanned multiple articles. In three cases a category was suggested by only one annotator (◦).5 Thus, we see that our annotators were generally, but not entirely, consistent with each other in their creation of custom categories. Further, almost all of our article-specific categories correspond to classes in the extended NE taxonomy of (Sekine et al., 2002), which speaks to the reasonableness of both sets of categories—and by extension, our open-ended annotation process. Our annotation of named entities outside of the traditional POL classes creates a useful resource for entity detection and recognition in new domains. Even the ability to detect non-canonical types of NEs should help applications such as QA and MT (Toral et al., 2005; Babych and Hartley, 2003). Possible avenues for future work include annotating and projecting non-canonical 5When it came to tagging NEs, one of the two annotators was assigned to each article. Custom categories on</context>
</contexts>
<marker>Sekine, Sudo, Nobata, 2002</marker>
<rawString>Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata. 2002. Extended named entity hierarchy. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Biomedical named entity recognition using conditional random fields and rich feature sets.</title>
<date>2004</date>
<booktitle>COLING 2004 International Joint workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP) 2004,</booktitle>
<pages>107--110</pages>
<editor>In Nigel Collier, Patrick Ruch, and Adeline Nazarenko, editors,</editor>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5799" citStr="Settles, 2004" startWordPosition="861" endWordPosition="862">9). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also</context>
</contexts>
<marker>Settles, 2004</marker>
<rawString>Burr Settles. 2004. Biomedical named entity recognition using conditional random fields and rich feature sets. In Nigel Collier, Patrick Ruch, and Adeline Nazarenko, editors, COLING 2004 International Joint workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP) 2004, pages 107–110, Geneva, Switzerland, August. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled Shaalan</author>
<author>Hafsa Raza</author>
</authors>
<title>Arabic named entity recognition from diverse text types.</title>
<date>2008</date>
<booktitle>In Advances in Natural Language Processing,</booktitle>
<pages>440--451</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="33521" citStr="Shaalan and Raza, 2008" startWordPosition="5313" endWordPosition="5316">lue (figure 1 shows how we tuned it on development data), and thus we anticipate that, in general, such tuning will be essential to leveraging the benefits of arrogance. 7 Related Work Our approach draws on insights from work in the areas of NER, domain adaptation, NLP with Wikipedia, and semisupervised learning. As all are broad areas of research, we highlight only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities </context>
</contexts>
<marker>Shaalan, Raza, 2008</marker>
<rawString>Khaled Shaalan and Hafsa Raza. 2008. Arabic named entity recognition from diverse text types. In Advances in Natural Language Processing, pages 440–451. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>David McClosky</author>
<author>Mason R Smith</author>
<author>Andrey Gusev</author>
<author>Christopher D Manning</author>
</authors>
<title>Customizing an information extraction system to a new domain.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="36044" citStr="Surdeanu et al., 2011" startWordPosition="5724" endWordPosition="5728">beled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstruct</context>
</contexts>
<marker>Surdeanu, McClosky, Smith, Gusev, Manning, 2011</marker>
<rawString>Mihai Surdeanu, David McClosky, Mason R. Smith, Andrey Gusev, and Christopher D. Manning. 2011. Customizing an information extraction system to a new domain. In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems 16.</booktitle>
<editor>In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨olkopf, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16858" citStr="Taskar et al., 2004" startWordPosition="2623" endWordPosition="2626">features consider more than a pair of consecutive BIO labels at a time. The model enforces the constraint that NE sequences must begin with B (so the bigram (O, I) is disallowed). Training this model on ACE and ANER data achieves performance comparable to the state of the art (F1-measure11 above 69%), but fares much worse on our Wikipedia test set (F1-measure around 47%); details are given in §5. 4.1 Recall-Oriented Perceptron By augmenting the perceptron’s online update with a cost function term, we can incorporate a task-dependent notion of error into the objective, as with structured SVMs (Taskar et al., 2004; Tsochantaridis et al., 2005). Let c(y, y0) denote a measure of error when y is the correct label sequence but y0 is predicted. For observed sequence x and feature weights (model parameters) w, the structured hinge loss is Ehz,,ye(x, y, w) _ ( ) w&gt;g(x, y0) + c(y, y0) − w&gt;g(x, y) (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11Though optimizing NER systems for Fl has been called into question (Manning, 2006), no alternative metric </context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2004</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004. Max-margin Markov networks. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
<author>Elisa Noguera</author>
<author>Fernando Llopis</author>
<author>Rafael Mu˜noz</author>
</authors>
<title>Improving question answering using named entity recognition.</title>
<date>2005</date>
<booktitle>Natural Language Processing and Information Systems,</booktitle>
<pages>3513--2005</pages>
<marker>Toral, Noguera, Llopis, Mu˜noz, 2005</marker>
<rawString>Antonio Toral, Elisa Noguera, Fernando Llopis, and Rafael Mu˜noz. 2005. Improving question answering using named entity recognition. Natural Language Processing and Information Systems, 3513/2005:181–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1453</pages>
<contexts>
<context position="16888" citStr="Tsochantaridis et al., 2005" startWordPosition="2627" endWordPosition="2630">e than a pair of consecutive BIO labels at a time. The model enforces the constraint that NE sequences must begin with B (so the bigram (O, I) is disallowed). Training this model on ACE and ANER data achieves performance comparable to the state of the art (F1-measure11 above 69%), but fares much worse on our Wikipedia test set (F1-measure around 47%); details are given in §5. 4.1 Recall-Oriented Perceptron By augmenting the perceptron’s online update with a cost function term, we can incorporate a task-dependent notion of error into the objective, as with structured SVMs (Taskar et al., 2004; Tsochantaridis et al., 2005). Let c(y, y0) denote a measure of error when y is the correct label sequence but y0 is predicted. For observed sequence x and feature weights (model parameters) w, the structured hinge loss is Ehz,,ye(x, y, w) _ ( ) w&gt;g(x, y0) + c(y, y0) − w&gt;g(x, y) (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11Though optimizing NER systems for Fl has been called into question (Manning, 2006), no alternative metric has achieved widespread accept</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Walker</author>
<author>Stephanie Strassel</author>
<author>Julie Medero</author>
<author>Kazuaki Maeda</author>
</authors>
<title>multilingual training corpus. LDC2006T06, Linguistic Data Consortium,</title>
<date>2006</date>
<publisher>ACE</publisher>
<location>Philadelphia.</location>
<contexts>
<context position="3469" citStr="Walker et al., 2006" startWordPosition="521" endWordPosition="524">elf-training; and • An empirical evaluation of this technique as applied to a well-established discriminative NER model and feature set. Experiments show consistent gains on the challenging problem of identifying named entities in Arabic Wikipedia text. 2 Arabic Wikipedia NE Annotation Most of the effort in NER has been focused around a small set of domains and general-purpose entity classes relevant to those domains—especially the categories PER(SON), ORG(ANIZATION), and LOC(ATION) (POL), which are highly prominent in news text. Arabic is no exception: the publicly available NER corpora—ACE (Walker et al., 2006), ANER (Benajiba et al., 2008), and OntoNotes (Hovy et al., 2006)—all are in the news domain.2 However, 2OntoNotes contains news-related text. ACE includes some text from blogs. In addition to the POL classes, both corpora include additional NE classes such as facility, event, product, vehicle, etc. These entities are infrequent and may not be comprehensive enough to cover the larger set of pos162 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162–173, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational L</context>
<context position="14596" citStr="Walker et al., 2006" startWordPosition="2260" endWordPosition="2263">f automatically classifying entities or inferring noncanonical groupings. 3 Data Table 4 summarizes the various corpora used in this work.6 Our NE-annotated Wikipedia subcorpus, described above, consists of several Arabic Wikipedia articles from four focus domains.7 We do not use these for supervised training data; they serve only as development and test data. A larger set of Arabic Wikipedia articles, selected on the basis of quality heuristics, serves as unlabeled data for semisupervised learning. Our out-of-domain labeled NE data is drawn from the ANER (Benajiba et al., 2007) and ACE-2005 (Walker et al., 2006) newswire corpora. Entity types in this data are POL categories (PER, ORG, LOC) and MIS. Portions of the ACE corpus were held out as development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 fea6Additional details appear in the supplement. 7We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using t</context>
</contexts>
<marker>Walker, Strassel, Medero, Maeda, 2006</marker>
<rawString>Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. ACE 2005 multilingual training corpus. LDC2006T06, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus. LDC2005T33, Linguistic Data Consortium,</title>
<date>2005</date>
<location>Philadelphia.</location>
<contexts>
<context position="5724" citStr="Weischedel and Brunstein, 2005" startWordPosition="848" endWordPosition="851">classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER sys</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN pronoun coreference and entity type corpus. LDC2005T33, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Wu</author>
<author>Wee Sun Lee</author>
<author>Nan Ye</author>
<author>Hai Leong Chieu</author>
</authors>
<title>Domain adaptive bootstrapping for named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1523--1532</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35253" citStr="Wu et al. (2009)" startWordPosition="5597" endWordPosition="5600">ic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—</context>
</contexts>
<marker>Wu, Lee, Ye, Chieu, 2009</marker>
<rawString>Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu. 2009. Domain adaptive bootstrapping for named entity recognition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1523–1532, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tianfang Yao</author>
<author>Wei Ding</author>
<author>Gregor Erbach</author>
</authors>
<title>CHINERS: a Chinese named entity recognition system for the sports domain.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>55--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="5818" citStr="Yao et al., 2003" startWordPosition="863" endWordPosition="866">ndantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annota</context>
</contexts>
<marker>Yao, Ding, Erbach, 2003</marker>
<rawString>Tianfang Yao, Wei Ding, and Gregor Erbach. 2003. CHINERS: a Chinese named entity recognition system for the sports domain. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 55–62, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>