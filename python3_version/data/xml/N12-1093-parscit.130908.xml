<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017086">
<title confidence="0.992675">
Unsupervised Concept-to-text Generation with Hypergraphs
</title>
<author confidence="0.970463">
Ioannis Konstas and Mirella Lapata
</author>
<affiliation confidence="0.9974365">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.993829">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.997221">
i.konstas@sms.ed.ac.uk, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999643578947368">
Concept-to-text generation refers to the task of
automatically producing textual output from
non-linguistic input. We present a joint model
that captures content selection (“what to say”)
and surface realization (“how to say”) in
an unsupervised domain-independent fashion.
Rather than breaking up the generation pro-
cess into a sequence of local decisions, we de-
fine a probabilistic context-free grammar that
globally describes the inherent structure of the
input (a corpus of database records and text
describing some of them). We represent our
grammar compactly as a weighted hypergraph
and recast generation as the task of finding the
best derivation tree for a given input. Experi-
mental evaluation on several domains achieves
competitive results with state-of-the-art sys-
tems that use domain specific constraints, ex-
plicit feature engineering or labeled data.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973191489362">
Concept-to-text generation broadly refers to the task
of automatically producing textual output from non-
linguistic input (Reiter and Dale, 2000). Depend-
ing on the application and the domain at hand, the
input may assume various representations includ-
ing databases of records, expert system knowledge
bases, simulations of physical systems and so on.
Figure 1 shows input examples and their correspond-
ing text for three domains, air travel, sportscasting
and weather forecast generation.
A typical concept-to-text generation system im-
plements a pipeline architecture consisting of three
core stages, namely text planning (determining the
content and structure of the target text), sentence
planning (determining the structure and lexical con-
tent of individual sentences), and surface realiza-
tion (rendering the specification chosen by the sen-
tence planner into a surface string). Traditionally,
these components are hand-engineered in order to
generate high quality text, however at the expense
of portability and scalability. It is thus no surprise
that recent years have witnessed a growing interest
in automatic methods for creating trainable genera-
tion components. Examples include learning which
database records should be present in a text (Duboue
and McKeown, 2002; Barzilay and Lapata, 2005)
and how these should be verbalized (Liang et al.,
2009). Besides concentrating on isolated compo-
nents, a few approaches have emerged that tackle
concept-to-text generation end-to-end. Due to the
complexity of the task, most models simplify the
generation process, e.g., by creating output that con-
sists of a few sentences, thus obviating the need for
document planning, or by treating sentence planning
and surface realization as one component. A com-
mon modeling strategy is to break up the genera-
tion process into a sequence of local decisions, each
learned separately (Reiter et al., 2005; Belz, 2008;
Chen and Mooney, 2008; Angeli et al., 2010; Kim
and Mooney, 2010).
In this paper we describe an end-to-end gen-
eration model that performs content selection and
surface realization jointly. Given a corpus of
database records and textual descriptions (for some
of them), we define a probabilistic context-free
grammar (PCFG) that captures the structure of the
database and how it can be rendered into natural
</bodyText>
<page confidence="0.690289">
752
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<figure confidence="0.992608">
(a) (b)
</figure>
<figureCaption confidence="0.975808">
Figure 1: Input-output examples for (a) query generation in the air travel domain, (b) weather forecast generation, and
(c) sportscasting.
</figureCaption>
<figure confidence="0.999595888888889">
Time
06:00-21:00
Min Mean Max
15 20 30
South wind around 20 mph.
Cloudy, with a low around 10.
(c)
Flight
From To
phoenix new york
Search
Day
Day Dep/Ar
sunday departure
Type What
query flight
List flights from phoenix to new york on sunday
Pass
From To
pink3 pink7
Bad Pass
From To
pink7 purple3
Turn Over
From To
pink7 purple3
pink3 passes the ball to pink7
Temperature
Min Mean Max
Time
9 15 21
06:00-21:00
Cloud Sky Cover
Time
Percent (%)
06:00-09:00
09:00-12:00
25-50
50-75
Wind Speed
Wind Direction
Time
Mode
06:00-21:00
S
</figure>
<bodyText confidence="0.999623863636364">
language. This grammar represents a set of trees
which we encode compactly using a weighted hy-
pergraph (or packed forest), a data structure that de-
fines a probability (or weight) for each tree. Gen-
eration then boils down to finding the best deriva-
tion tree in the hypergraph which can be done effi-
ciently using the Viterbi algorithm. In order to en-
sure that our generation output is fluent, we intersect
our grammar with a language model and perform
decoding using a dynamic programming algorithm
(Huang and Chiang, 2007).
Our model is conceptually simpler than previous
approaches and encodes information about the do-
main and its structure globally, by considering the
input space simultaneously during generation. Our
only assumption is that the input must be a set of
records essentially corresponding to database-like
tables whose columns describe fields of a certain
type. Experimental evaluation on three domains ob-
tains results competitive to the state of the art with-
out using any domain specific constraints, explicit
feature engineering or labeled data.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9741237">
Our work is situated within the broader class of
data-driven approaches to content selection and sur-
face realization. Barzilay and Lapata (2005) focus
on the former problem which they view as an in-
stance of collective classification (Barzilay and La-
pata, 2005). Given a corpus of database records
and texts describing some of them, they learn a con-
tent selection model that simultaneously optimizes
local label assignments and their pairwise relations.
Building on this work, Liang et al. (2009) present a
hierarchical hidden semi-Markov generative model
that first determines which facts to discuss and then
generates words from the predicates and arguments
of the chosen facts.
A few approaches have emerged more recently
that combine content selection and surface realiza-
tion. Kim and Mooney (2010) adopt a two-stage ap-
proach: using a generative model similar to Liang et
al. (2009), they first decide what to say and then ver-
balize the selected input with WASP−1, an existing
generation system (Wong and Mooney, 2007). In
contrast, Angeli et al. (2010) propose a unified con-
tent selection and surface realization model which
also operates over the alignment output produced
by Liang et al. (2009). Their model decomposes
into a sequence of discriminative local decisions.
They first determine which records in the database
to talk about, then which fields of those records
to mention, and finally which words to use to de-
scribe the chosen fields. Each of these decisions
is implemented as a log-linear model with features
learned from training data. Their surface realiza-
tion component is based on templates that are au-
tomatically extracted and smoothed with domain-
specific constraints in order to guarantee fluent out-
put. Other related work (Wong and Mooney, 2007;
Lu and Ng, 2011). has focused on generating natural
language sentences from logical form (i.e., lambda-
expressions) using mostly synchronous context-free
grammars (SCFGs).
</bodyText>
<page confidence="0.998132">
753
</page>
<bodyText confidence="0.999432523809524">
Similar to Angeli et al. (2010), we also present
an end-to-end system that performs content selec-
tion and surface realization. However, rather than
breaking up the generation task into a sequence of
local decisions, we optimize what to say and how
to say simultaneously. We do not learn mappings
from a logical form, but rather focus on input which
is less constrained, possibly more noisy and with a
looser structure. Our key insight is to convert the
set of database records serving as input to our gen-
erator into a PCFG that is neither hand crafted nor
domain specific but simply describes the structure
of the database. The approach is conceptually sim-
ple, does not rely on discriminative training or any
feature engineering. We represent the grammar and
its derivations compactly as a weighted hypergraph
which we intersect with a language model in order
to generate fluent output. This allows us to easily
port surface generation to different domains without
having to extract new templates or enforce domain
specific constraints.
</bodyText>
<sectionHeader confidence="0.984805" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.99999704">
We assume our generator takes as input a set of
database records d and produces text w that verbal-
izes some of these records. Each record r E d has a
type r.t and a set of fields f associated with it. Fields
have different values f.v and types f.t (i.e., in-
teger or categorical). For example, in Figure 1b,
wind speed is a record type with four fields: time,
min, mean, and max. The values of these fields are
06:00-21:00, 15, 20, and 30, respectively; the type
of time is categorical, whereas all other fields are
integers.
During training, our algorithm is given a cor-
pus consisting of several scenarios, i.e., database
records paired with texts like those shown in Fig-
ure 1. In the weather forecast domain, a scenario cor-
responds to weather-related measurements of tem-
perature, wind, speed, and so on collected for a spe-
cific day and time (e.g., day or night). In sportscast-
ing, scenarios describe individual events in the soc-
cer game (e.g., passing or kicking the ball). In the air
travel domain, scenarios comprise of flight-related
details (e.g., origin, destination, day, time). Our goal
then is to reduce the tasks of content selection and
surface realization into a common probabilistic pars-
ing problem. We do this by abstracting the struc-
ture of the database (and accompanying texts) into
a PCFG whose probabilities are learned from train-
ing data.1 Specifically, we convert the database into
rewrite rules and represent them as a weighted di-
rected hypergraph (Gallo et al., 1993). Instead of
learning the probabilities on the PCFG, we directly
compute the weights on the hyperarcs using a dy-
namic program similar to the inside-outside algo-
rithm (Li and Eisner, 2009). During testing, we are
given a set of database records without the corre-
sponding text. Using the trained grammar we com-
pile a hypergraph specific to this test input and de-
code it approximately via cube pruning (Chiang,
2007).
The choice of the hypergraph framework is moti-
vated by at least three reasons. Firstly, hypergraphs
can be used to represent the search space of most
parsers (Klein and Manning, 2001). Secondly, they
are more efficient and faster than the common CYK
parser-based representation for PCFGs by a factor
of more than ten (Huang and Chiang, 2007). And
thirdly, the hypergraph representation allows us to
integrate an n-gram language model and perform de-
coding efficiently using k-best Viterbi search, opti-
mizing what to say and how to say at the same time.
</bodyText>
<subsectionHeader confidence="0.995355">
3.1 Grammar Definition
</subsectionHeader>
<bodyText confidence="0.9997757">
Our model captures the inherent structure of the
database with a number of CFG rewrite rules, in
a similar way to how Liang et al. (2009) define
Markov chains in the different levels of their hierar-
chical model. These rules are purely syntactic (de-
scribing the intuitive relationship between records,
records and fields, fields and corresponding words),
and could apply to any database with similar struc-
ture irrespectively of the semantics of the domain.
Our grammar is defined in Table 1 (rules (1)–(9)).
Rule weights are governed by an underlying multi-
nomial distribution and are shown in square brack-
ets. Non-terminal symbols are in capitals and de-
1An alternative would be to learn a SCFG between the
database input and the accompanying text. However, this would
involve considerable overhead in terms of alignment (as the
database and the text do not together constitute a clean parallel
corpus, but rather a noisy comparable corpus), as well as gram-
mar training and decoding using state-of-the art SMT methods,
which we manage to avoid with our simpler approach.
</bodyText>
<page confidence="0.985755">
754
</page>
<listItem confidence="0.989022777777778">
1. S —* R(start) [Pr = 1]
2. R(ri.t) —* FS(rj,start) R(rj.t) [P(rj.t |ri.t) - X]
3. R(ri.t) —* FS(rj,start) [P(rj.t |ri.t) - X]
4. FS(r,r.fi) —* F(r,r.fj) FS(r,r.fj) [P(fj  |fi)]
5. FS(r,r. fi) —* F(r,r.fj) [P(fj  |fi)]
6. F(r,r.f) —* W(r,r.f) F(r,r.f) [P(w|w-1,r,r.f)]
7. F(r,r.f) —* W(r,r.f) [P(w|w-1,r,r.f)]
8. W(r,r.f) —* a [P(a|r,r.f, f.t, f.v)]
9. W(r,r.f) —* g(f.v)
</listItem>
<tableCaption confidence="0.758669666666667">
[P(g(f.v).mode|r,r.f, f.t = int)]
Table 1: Grammar rules and their weights shown in
square brackets.
</tableCaption>
<bodyText confidence="0.998376686567164">
note intermediate states; the terminal symbol a
corresponds to all words seen in the training set,
and g(f.v) is a function for generating integer num-
bers given the value of a field f. All non-terminals,
save the start symbol S, have one or more features
(shown in parentheses) that act as constraints, sim-
ilar to number and gender agreement constraints in
augmented syntactic rules.
Rule (1) denotes the expansion from the start
symbol S to record R, which has the special ‘start’
record type (hence the notation R(start)). Rule (2)
defines a chain between two consecutive records,
i.e., going from a source record ri to a target rj.
Here, FS(rj,rj.f) represents the set of fields of the
target rj, following the source record R(ri).
For example, the rule R(skyCover1.t) —*
FS(temperature1,start)R(temperature1.t) can
be interpreted as follows. Given that we have
talked about skyCover1, we will next talk about
temperature1 and thus emit its corresponding fields.
R(temperature1.t) is a non-terminal place-holder
for the continuation of the chain of records, and
start in FS is a special boundary field between
consecutive records. The weight of this rule is the
bigram probability of two records conditioned on
their record type, multiplied with a normalization
factor X. We have also defined a null record type
i.e., a record that has no fields and acts as a
smoother for words that may not correspond to a
particular record. Rule (3) is simply an escape rule,
so that the parsing process (on the record level) can
finish.
Rule (4) is the equivalent of rule (2) at the
field level, i.e., it describes the chaining of
two consecutive fields fi and fj. Non-terminal
F(r,r.f) refers to field f of record r. For
example, the rule FS(windSpeed1,min) —*
F(windSpeed1,max)FS(windSpeed1,max), spec-
ifies that we should talk about the field max of
record windSpeed1, after talking about the field
min. Analogously to the record level, we have also
included a special null field type for the emission
of words that do not correspond to a specific record
field. Rule (6) defines the expansion of field F to
a sequence of (binarized) words W, with a weight
equal to the bigram probability of the current word
given the previous word, the current record, and
field. This is an attempt at capturing contextual
dependencies between words over and above to
integrating a language model during decoding (see
Section 3.3).
Rules (8) and (9) define the emission of words and
integer numbers from W, given a field type and its
value. Rule (8) emits a single word from the vocabu-
lary of the training set. Its weight defines a multino-
mial distribution over all seen words, for every value
of field f, given that the field type is categorical or
the special null field. Rule (9) is identical but for
fields whose type is integer. Function g(f.v) gener-
ates an integer number given the field value, using
either of the following six ways (Liang et al., 2009):
identical to the field value, rounding up or rounding
down to a multiple of 5, rounding off to the clos-
est multiple of 5 and finally adding or subtracting
some unexplained noise.2 The weight is a multino-
mial over the six generation function modes, given
the record field f.
</bodyText>
<subsectionHeader confidence="0.998475">
3.2 Hypergraph Construction
</subsectionHeader>
<bodyText confidence="0.999947857142857">
So far we have defined a probabilistic grammar
that captures the structure of a database d with
records and fields as intermediate non-terminals, and
words w (from the associated text) as terminals. Us-
ing this grammar and the CYK parsing algorithm,
we could obtain the top scoring derivation of records
and fields for a given input (i.e., a sequence of
</bodyText>
<footnote confidence="0.993071">
2The noise is modeled as a geometric distribution.
</footnote>
<page confidence="0.992664">
755
</page>
<figureCaption confidence="0.987202">
Figure 2: Partial hypergraph representation for the sentence “Sunny with a low around 30 .” For the sake of readability,
we show a partial span on the first two words without weights on the hyperarcs.
</figureCaption>
<figure confidence="0.997586482758621">
F0,1(skyCover1,%)
W0,1(skyCover1,%)
sunny
FS1,1(skyCover1,%)
R0,2(start)
W0,1(skyCover1,time)
F0,1(skyCover1,time)
···
FS0,1(skyCover1,start)
S0,7
FS1,1(skyCover1,time)
R0,1(start)
FS1,2(temp1,start)
F1,2(temp1,min)
W0,1(temp1,min)
g0,1(min,v=10)
R1,1(skyCover1.t)
FS2,2(temp1,min)
R2,2(temp1.t)
F1,2(temp1,max)
W0,1(temp1,max)
R1,1(temp1.t)
FS2,2(temp1,max)
R2,2(skyCover1.t)
FS0,1(temp1,start)
g0,1(max,v=20)
with
···
FS1,2(skyCover1,start)
</figure>
<bodyText confidence="0.99972575">
words) as well as the optimal segmentation of the
text, provided we have a trained set of weights. The
inside-outside algorithm is commonly used for esti-
mating the weights of a PCFG. However, we first
transform the CYK parser and our grammar into
a hypergraph and then compute the weights using
inside-outside. Huang and Chiang (2005) define a
weighted directed hypergraph as follows:
</bodyText>
<construct confidence="0.918999909090909">
Definition 1 An ordered hypergraph H is a tuple
hN,E,t,Ri, where N is a finite set of nodes, E
is a finite set of hyperarcs and R is the set of
weights. Each hyperarc e ∈ E is a triple e =
hT(e),h(e), f(e)i, where h(e) ∈ N is its head node,
T(e) ∈ N∗ is a set of tail nodes and f (e) is a mono-
tonic weight function R|T(e) |to R and t ∈ N is a tar-
get node.
Definition 2 We impose the arity of a hyperarc to be
|e |= |T(e) |= 2, in other words, each head node is
connected with at most two tail nodes.
</construct>
<bodyText confidence="0.981855592592593">
Given a context-free grammar G = hN,T,P,Si
(where N is the set of variables, T the set of ter-
minals, P the set of production rules, and S ∈ N the
start symbol) and an input string w, we can map the
standard weighted CYK algorithm to a hypergraph
as follows. Each node [A,i, j] in the hypergraph
corresponds to non-terminal A spanning words wi
to wj of the input. Each rewrite rule A → BC in P,
with three free indices i &lt; j &lt; k, is mapped to
the hyperarc h((B,i, j),(C, j,k)),(A,i,k), fi, where
f = f ((B,i, j)) f ((C, j,k)) · Pr(A → BC).3 The hy-
3Similarly, rewrite rules of type A → B are mapped to the
hyperarc h(B,i, j),(A,i, j), fi, with f = f((B,i, j))·Pr(A → B).
pergraph can be thus viewed as a compiled lattice
of the corresponding chart graph. Figure 2 shows
an example hypergraph for a grammar defined on
database input similar to Figure (1b).
In order to learn the weights on the hyperarcs we
perform the following procedure iteratively in an
EM fashion (Li and Eisner, 2009). For each train-
ing scenario we build its hypergraph representation.
Next, we perform inference by calculating the in-
side and outside scores of the hypergraph, so as to
compute the posterior distribution over its hyperarcs
(E-step). Finally, we collectively update the posteri-
ors on the parameters-weights, i.e., rule probabilities
and emission multinomial distributions (M-step).
</bodyText>
<subsectionHeader confidence="0.996162">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.99999">
In the framework outlined above, parsing an input
string w (given some learned weights) boils down
to traversing the hypergraph in a particular order.
(Note that the hypergraph should be acyclic, which
is always guaranteed by the grammar in Table 1). In
generation, our aim is to verbalize an input scenario
from a database d (see Figure 1). We thus find the
best text by maximizing:
</bodyText>
<equation confidence="0.856281">
argmax P(w|d) = argmaxP(w) · P(d|w) (1)
w w
</equation>
<bodyText confidence="0.999949333333333">
where P(d|w) is the decoding likelihood for a se-
quence of words w, P(w) is a measure of the qual-
ity of each output (given by a language model),
and P(w|d) the posterior of the best output for
database d. Note that calculating P(d|w) requires
deciding on the output length |w|. Rather than set-
</bodyText>
<page confidence="0.996805">
756
</page>
<bodyText confidence="0.999939666666667">
ting w to a fixed length, we rely on a linear regres-
sion predictor that uses the counts of each record
type per scenario as features and is able to produce
variable length texts.
In order to perform decoding with an n-gram lan-
guage model, we adopt Huang and Chiang’s (2007)
dynamic-programming algorithm for SCFG-based
systems. Each node in the hypergraph is split into
a set of compound items, namely +LM items. Each
+LM item is of the form (na?b), where a and b are
boundary words of the generation string, and ? is a
place-holder symbol for an elided part of that string,
indicating a sub-generation part ranging from a to b.
An example +LM deduction of a single hyperarc of
the hypergraph in Figure 2 using bigrams is:
</bodyText>
<equation confidence="0.998048">
(2)
FS1,2(temp1,start)low : (w1,g1),
R2,2(temp1.t)around?degrees : (w2,g2)
R1,1(skyCover1.t)low?degrees : (w,g1g2)
w = w1 +w2 +ew + Plm(around I low) (3)
</equation>
<bodyText confidence="0.999992272727273">
where w1,w2 are node weights, g1,g2 are the corre-
sponding sub-generations, ew is the weight of the hy-
perarc and w the weight of the resulting +LM item.
Plm and (na?b) are defined as in Chiang (2007) in a
generic fashion, allowing extension to an arbitrary
size of n-gram grammars.
Naive traversal of the hypergraph bottom-up
would explore all possible +LM deductions along
each hyperarc, and would increase decoding com-
plexity to an infeasible O(2nn2), assuming a trigram
model and a constant number of emissions at the ter-
minal nodes. To ensure tractability, we adopt cube
pruning, a popular approach in syntax-inspired ma-
chine translation (Chiang, 2007). The idea is to use a
beam-search over the intersection grammar coupled
with the cube-pruning heuristic. The beam limits the
number of derivations for each node, whereas cube-
pruning further limits the number of +LM items con-
sidered for inclusion in the beam. Since f (e) in Def-
inition 1 is monotonic, we can select the k-best items
without computing all possible +LM items.
Our decoder follows Huang and Chiang (2007)
but importantly differs in the treatment of leaf nodes
in the hypergraph (see rules (8) and (9)). In the
SCFG context, the Viterbi algorithm consumes ter-
minals from the source string in a bottom-up fashion
and creates sub-translations according to the CFG
rule that holds each time. In the concept-to-text
generation context, however, we do not observe the
words; instead, for each leaf node we emit the k-best
words from the underlying multinomial distribution
(see weights on rules (8) and (9)) and continue build-
ing our sub-generations bottom-up.
</bodyText>
<sectionHeader confidence="0.99838" genericHeader="method">
4 Experimental Design
</sectionHeader>
<bodyText confidence="0.999935289473684">
Data We used our system to generate soccer com-
mentaries, weather forecasts, and spontaneous utter-
ances relevant to the air travel domain (examples
are given in Figure 1). For the first domain we
used the dataset of Chen and Mooney (2008), which
consists of 1,539 scenarios from the 2001–2004
Robocup game finals. Each scenario contains on av-
erage |d |= 2.4 records, each paired with a short sen-
tence (5.7 words). This domain has a small vocabu-
lary (214 words) and simple syntax (e.g., a transitive
verb with its subject and object). Records in this
dataset (henceforth ROBOCUP) were aligned man-
ually to their corresponding sentences (Chen and
Mooney, 2008). Given the relatively small size of
this dataset, we performed cross-validation follow-
ing previous work (Chen and Mooney, 2008; An-
geli et al., 2010). We trained our system on three
ROBOCUP games and tested on the fourth, averaging
over the four train/test splits.
For weather forecast generation, we used the
dataset of Liang et al. (2009), which consists of
29,528 weather scenarios for 3,753 major US cities
(collected over four days). The vocabulary in this
domain (henceforth WEATHERGOV) is comparable
to ROBOCUP (345 words), however, the texts are
longer (Iwl = 29.3) and more varied. On average,
each forecast has 4 sentences and the content selec-
tion problem is more challenging; only 5.8 out of
the 36 records per scenario are mentioned in the text
which roughly corresponds to 1.4 records per sen-
tence. We used 25,000 scenarios from WEATHER-
GOV for training, 1,000 scenarios for development
and 3,528 scenarios for testing. This is the same par-
tition used in Angeli et al. (2010).
For the air travel domain we used the ATIS dataset
(Dahl et al., 1994), consisting of 5,426 scenar-
ios. These are transcriptions of spontaneous utter-
ances of users interacting with a hypothetical on-
</bodyText>
<page confidence="0.992576">
757
</page>
<table confidence="0.998431214285714">
WEATHERGOV ATIS ROBOCUP
Near 57. Near 57. Near 57. Near 57. Near What what what what flights Pink9 to to Pink7 kicks
pWq 57. Near 57. Near 57. Near 57. Near 57. from Denver Phoenix
1- Near 57. Near 57. South wind.
As high as 23 mph. Chance of precipitation Show me the flights from Pink9 passes back to Pink7
is 20. Breezy, with a chance of showers. Denver to Phoenix
Mostly cloudy, with a high near 57. South
k- wind between 3 and 9 mph.
GEL A chance of rain or drizzle, with a high near Show me the flights leave Pink9 kicks to Pink7
N 57. South wind between 3 and 9 mph. from Nashville to Phoenix
A slight chance of showers. Mostly cloudy, List flights from Denver to Pink9 passes back to Pink7
with a high near 58. South wind between 3 Phoenix
and 9 mph, with gusts as high as 23 mph.
H Chance of precipitation is 20%.
</table>
<tableCaption confidence="0.9929605">
Table 2: System output on WEATHERGOV, ATIS, and ROBOCUP (1-BEST, k-BEST, ANGELI) and corresponding
human-authored text (HUMAN).
</tableCaption>
<bodyText confidence="0.986298681818182">
line flight booking system. We used the dataset
introduced in Zettlemoyer and Collins (2007)4 and
automatically converted their lambda-calculus ex-
pressions to attribute-value pairs following the con-
ventions adopted by Liang et al. (2009). For ex-
ample, the scenario in Figure 1(a) was initially
represented as: A.x.flight(x) n from(x,phoenix) n
to(x,new york)nday(x,sunday).5 In contrast to the
two previous datasets, ATIS has a much richer vo-
cabulary (927 words); each scenario corresponds
to a single sentence (average length is 11.2 words)
with 2.65 out of 19 record types mentioned on av-
erage. Following Zettlemoyer and Collins (2007),
we trained on 4,962 scenarios and tested on ATIS
NOV93 which contains 448 examples.
Model Parameters Our model has two parame-
ters, namely the number of k grammar derivations
considered by the decoder and the order of the
language model. We tuned k experimentally on
held-out data taken from WEATHERGOV, ROBOCUP,
and ATIS, respectively. The optimal value was k=15
for WEATHERGOV, k=25 for ROBOCUP, and k = 40
</bodyText>
<footnote confidence="0.974789666666667">
4The original corpus contains user utterances of single dia-
logue turns which would result in trivial scenarios. Zettlemoyer
and Collins (2007) concatenate all user utterances referring to
the same dialogue act, (e.g., book a flight), thus yielding more
complex scenarios with longer sentences.
5The resulting dataset and a technical report describ-
ing the mapping procedure in detail are available from
http://homepages.inf.ed.ac.uk/s0793019/index.php?
page=resources
</footnote>
<bodyText confidence="0.999875103448276">
for ATIS. For the ROBOCUP domain, we used a bi-
gram language model which was considered suffi-
cient given that the average text length is small. For
WEATHERGOV and ATIS, we used a trigram language
model.
System Comparison We evaluated two configu-
rations of our system. A baseline that uses the top
scoring derivation in each subgeneration (1-BEST)
and another version which makes better use of our
decoding algorithm and considers the best k deriva-
tions (i.e., 15 for WEATHERGOV, 40 for ATIS, and
25 for ROBOCUP). We compared our output to An-
geli et al. (2010) whose approach is closest to ours
and state-of-the-art on the WEATHERGOV domain.
For ROBOCUP, we also compare against the best-
published results (Kim and Mooney, 2010).
Evaluation We evaluated system output automat-
ically, using the BLEU modified precision score
(Papineni et al., 2002) with the human-written text
as reference. In addition, we evaluated the gener-
ated text by eliciting human judgments. Participants
were presented with a scenario and its correspond-
ing verbalization and were asked to rate the latter
along two dimensions: fluency (is the text grammat-
ical and overall understandable?) and semantic cor-
rectness (does the meaning conveyed by the text cor-
respond to the database input?). The subjects used a
five point rating scale where a high number indicates
better performance. We randomly selected 12 doc-
</bodyText>
<page confidence="0.994021">
758
</page>
<table confidence="0.9992775">
ROBOCUP WEATHERGOV ATIS
System BLEU BLEU BLEU
1-BEST 10.79 8.64 11.85
k-BEST 30.90 33.70 29.30
ANGELI 28.70 38.40 26.77
KIM-MOONEY 47.27 — —
</table>
<tableCaption confidence="0.9920415">
Table 3: BLEU scores on ROBOCUP (fixed content se-
lection), WEATHERGOV, and ATIS.
</tableCaption>
<bodyText confidence="0.999881">
uments from the test set (for each domain) and gen-
erated output with our models (1-BEST and k-BEST)
and Angeli et al.’s (2010) model (see Figure 2 for
examples of system output). We also included the
original text (HUMAN) as gold standard. We thus
obtained ratings for 48 (12 x 4) scenario-text pairs
for each domain. The study was conducted over the
Internet using WebExp (Keller et al., 2009) and was
completed by 114 volunteers, all self reported native
English speakers.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.994163153846154">
We conducted two experiments on the ROBOCUP do-
main. We first assessed the performance of our gen-
erator (k-BEST) on joint content selection and sur-
face realization and obtained a BLEU score of 24.88.
In comparison, the baseline’s (1-BEST) BLEU score
was 8.01. In a second experiment we forced the
generator to use the gold-standard records from the
database. This was necessary in order to compare
with previous work (Angeli et al., 2010; Kim and
Mooney, 2010).6 Our results are summarized in Ta-
ble 3. Overall, our generator performs better than
the baseline and Angeli et al. (2010). We observe
a substantial increase in performance compared to
the joint content selection and surface realization
setting. This is expected as the generator is faced
with an easier task and there is less scope for error.
Our model does not outperform Kim and Mooney
(2010), however, this is not entirely surprising as
their model requires considerable more supervision
(e.g., during parameter initialization) and includes a
post-hoc re-ordering component.
6Angeli et al. (2010) and Kim and Mooney (2010) fix con-
tent selection both at the record and field level. We let our gen-
erator select the appropriate fields, since these are at most two
per record type and this level of complexity can be easily tack-
led during decoding.
</bodyText>
<table confidence="0.999293166666667">
System ROBOCUP WEATHERGOV ATIS
F SC F SC F SC
1-BEST 2.47*† 2.33*† 1.82*† 2.05*† 2.40*† 2.46*†
k-BEST 4.31* 3.96* 3.92* 3.30* 4.01 3.87
ANGELI 4.03*† 3.70*† 4.26* 3.60* 3.56*† 3.33*†
HUMAN 4.47† 4.37† 4.61† 4.03† 4.10 4.01
</table>
<tableCaption confidence="0.99403825">
Table 4: Mean ratings for fluency (F) and semantic cor-
rectness (SC) on system output elicited by humans on
ROBOCUP, WEATHERGOV, and ATIS (*: sig. diff. from
HUMAN; †: sig. diff. from k-BEST.)
</tableCaption>
<bodyText confidence="0.999959833333333">
With regard to WEATHERGOV, our generator im-
proves over the baseline but lags behind Angeli et
al. (2010). Since our system emits words based on
a language model rather than a template, it displays
more freedom in word order and lexical choice, and
is thus penalized by BLEU when creating output that
is overly distinct from the reference. On ATIS, our
model outperforms both the baseline and Angeli et
al. This is the most challenging domain with re-
gard to surface realization with a vocabulary larger
than ROBOCUP and WEATHERGOV by factors of 2.7
and 4.3, respectively.
The results of our human evaluation study are
shown in Table 3. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system
type (1-BEST, k-BEST, ANGELI, and HUMAN) on the
fluency and semantic correctness ratings. Means
differences were compared using a post-hoc Tukey
test. On ROBOCUP, our system (k-BEST) is signif-
icantly better than the baseline (1-BEST) and AN-
GELI both in terms of fluency and semantic correct-
ness (a &lt; 0.05). On WEATHERGOV, our generator
performs comparably to ANGELI on fluency and se-
mantic correctness (the differences in the means are
not statistically significant); 1-BEST is significantly
worse than 15-BEST and ANGELI (a &lt; 0.05). On
ATIS, k-BEST is significantly more fluent and seman-
tically correct than 1-BEST and ANGELI (a &lt; 0.01).
There was no statistically significant difference be-
tween the output of our system and the original ATIS
sentences.
In sum, we observe that taking the k-best deriva-
tions into account boosts performance (the 1-BEST
system is consistently worse). Our model is on par
with ANGELI on WEATHERGOV but performs better
on ROBOCUP and ATIS when evaluated both auto-
</bodyText>
<page confidence="0.995205">
759
</page>
<bodyText confidence="0.9999772">
matically and by humans. In general, a large part of
our output resembles the human text, which demon-
strates that our simple language model yields coher-
ent sentences (without any template engineering), at
least for the domains under consideration.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999983611111111">
We have presented an end-to-end generation system
that performs both content selection and surface re-
alization. Central to our approach is the encoding
of generation as a parsing problem. We reformulate
the input (a set of database records and text describ-
ing some of them) as a PCFG and show how to find
the best derivation using the hypergraph framework.
Despite its simplicity, our model is able to obtain
performance comparable to the state of the art. We
argue that our approach is computationally efficient
and viable in practical applications. Porting the sys-
tem to a different domain is straightforward, assum-
ing a database and corresponding (unaligned) text.
As long as the database is compatible with the struc-
ture of the grammar in Table 1, we need only retrain
to obtain the weights on the hyperarcs and a domain
specific language model.
Our model takes into account the k-best deriva-
tions at decoding time, however inspection of these
shows that it often fails to select the best one. In
the future, we plan to remedy this by using forest
reranking, a technique that approximately reranks
a packed forest of exponentially many derivations
(Huang, 2008). We would also like to scale our
model to more challenging domains (e.g., product
descriptions) and to enrich our generator with some
notion of discourse planning. An interesting ques-
tion is how to extend the PCFG-based approach ad-
vocated here so as to capture discourse-level docu-
ment structure.
Acknowledgments We are grateful to Percy Liang
and Gabor Angeli for providing us with their code
and data. We would also like to thank Luke Zettle-
moyer and Tom Kwiatkowski for sharing their ATIS
dataset with us and Frank Keller for his feedback on
an earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.996447" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872137254902">
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502–512, Cambridge, MA.
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive content selection for concept-to-text generation.
In Proceedings of Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 331–338, Vancouver, British Columbia.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 14(4):431–455.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128–135, Helsinki, Finland.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis task:
the atis-3 corpus. In Proceedings of the Workshop on
Human Language Technology, pages 43–48, Plains-
boro, NJ.
Pablo A. Duboue and Kathleen R. McKeown. 2002.
Content planner construction via evolutionary algo-
rithms and a corpus-based fitness function. In Pro-
ceedings of International Natural Language Genera-
tion, pages 89–96, Ramapo Mountains, NY.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and appli-
cations. Discrete Applied Mathematics, 42:177–201.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International Work-
shop on Parsing Technology, pages 53–64, Vancouver,
British Columbia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144–151,
Prague, Czech Republic.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586–594, Columbus, Ohio.
Frank Keller, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of Web ex-
periments: A case study using the WebExp software
package. Behavior Research Methods, 41(1):1–12.
</reference>
<page confidence="0.953703">
760
</page>
<reference confidence="0.999779115384615">
Joohyun Kim and Raymond Mooney. 2010. Generative
alignment and semantic parsing for learning from am-
biguous supervision. In Proceedings of the 23rd Con-
ference on Computational Linguistics, pages 543–551,
Beijing, China.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the 7th Interna-
tional Workshop on Parsing Technologies, pages 123–
134, Beijing, China.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 40–51, Suntec, Sin-
gapore.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In roceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 91–99, Suntec, Singapore.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-
to-string model for language generation from typed
lambda calculus expressions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 1611–1622, Edinburgh,
UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian
Davy. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137–
169.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172–179, Rochester, NY.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 678–687, Prague, Czech Republic.
</reference>
<page confidence="0.997424">
761
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.688262">
<title confidence="0.989004">Unsupervised Concept-to-text Generation with Hypergraphs</title>
<author confidence="0.720014">Konstas</author>
<affiliation confidence="0.995177">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.984099">10 Crichton Street, Edinburgh EH8</address>
<email confidence="0.998111">i.konstas@sms.ed.ac.uk,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.9985788">Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection (“what to say”) and surface realization (“how to say”) in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we define a probabilistic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We represent our grammar compactly as a weighted hypergraph and recast generation as the task of finding the best derivation tree for a given input. Experimental evaluation on several domains achieves competitive results with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>502--512</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="3102" citStr="Angeli et al., 2010" startWordPosition="451" endWordPosition="454">balized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics (a) (b)</context>
<context position="6506" citStr="Angeli et al. (2010)" startWordPosition="996" endWordPosition="999">rwise relations. Building on this work, Liang et al. (2009) present a hierarchical hidden semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. A few approaches have emerged more recently that combine content selection and surface realization. Kim and Mooney (2010) adopt a two-stage approach: using a generative model similar to Liang et al. (2009), they first decide what to say and then verbalize the selected input with WASP−1, an existing generation system (Wong and Mooney, 2007). In contrast, Angeli et al. (2010) propose a unified content selection and surface realization model which also operates over the alignment output produced by Liang et al. (2009). Their model decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component is based on templates that are automatically extracted and smoothe</context>
<context position="23087" citStr="Angeli et al., 2010" startWordPosition="3760" endWordPosition="3764">domain we used the dataset of Chen and Mooney (2008), which consists of 1,539 scenarios from the 2001–2004 Robocup game finals. Each scenario contains on average |d |= 2.4 records, each paired with a short sentence (5.7 words). This domain has a small vocabulary (214 words) and simple syntax (e.g., a transitive verb with its subject and object). Records in this dataset (henceforth ROBOCUP) were aligned manually to their corresponding sentences (Chen and Mooney, 2008). Given the relatively small size of this dataset, we performed cross-validation following previous work (Chen and Mooney, 2008; Angeli et al., 2010). We trained our system on three ROBOCUP games and tested on the fourth, averaging over the four train/test splits. For weather forecast generation, we used the dataset of Liang et al. (2009), which consists of 29,528 weather scenarios for 3,753 major US cities (collected over four days). The vocabulary in this domain (henceforth WEATHERGOV) is comparable to ROBOCUP (345 words), however, the texts are longer (Iwl = 29.3) and more varied. On average, each forecast has 4 sentences and the content selection problem is more challenging; only 5.8 out of the 36 records per scenario are mentioned in </context>
<context position="27152" citStr="Angeli et al. (2010)" startWordPosition="4436" endWordPosition="4440">p://homepages.inf.ed.ac.uk/s0793019/index.php? page=resources for ATIS. For the ROBOCUP domain, we used a bigram language model which was considered sufficient given that the average text length is small. For WEATHERGOV and ATIS, we used a trigram language model. System Comparison We evaluated two configurations of our system. A baseline that uses the top scoring derivation in each subgeneration (1-BEST) and another version which makes better use of our decoding algorithm and considers the best k derivations (i.e., 15 for WEATHERGOV, 40 for ATIS, and 25 for ROBOCUP). We compared our output to Angeli et al. (2010) whose approach is closest to ours and state-of-the-art on the WEATHERGOV domain. For ROBOCUP, we also compare against the bestpublished results (Kim and Mooney, 2010). Evaluation We evaluated system output automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along two dimensions: fluency (is the text grammatical and overall understandable?)</context>
<context position="29121" citStr="Angeli et al., 2010" startWordPosition="4758" endWordPosition="4761">or each domain. The study was conducted over the Internet using WebExp (Keller et al., 2009) and was completed by 114 volunteers, all self reported native English speakers. 5 Results We conducted two experiments on the ROBOCUP domain. We first assessed the performance of our generator (k-BEST) on joint content selection and surface realization and obtained a BLEU score of 24.88. In comparison, the baseline’s (1-BEST) BLEU score was 8.01. In a second experiment we forced the generator to use the gold-standard records from the database. This was necessary in order to compare with previous work (Angeli et al., 2010; Kim and Mooney, 2010).6 Our results are summarized in Table 3. Overall, our generator performs better than the baseline and Angeli et al. (2010). We observe a substantial increase in performance compared to the joint content selection and surface realization setting. This is expected as the generator is faced with an easier task and there is less scope for error. Our model does not outperform Kim and Mooney (2010), however, this is not entirely surprising as their model requires considerable more supervision (e.g., during parameter initialization) and includes a post-hoc re-ordering componen</context>
<context position="30513" citStr="Angeli et al. (2010)" startWordPosition="4990" endWordPosition="4993"> are at most two per record type and this level of complexity can be easily tackled during decoding. System ROBOCUP WEATHERGOV ATIS F SC F SC F SC 1-BEST 2.47*† 2.33*† 1.82*† 2.05*† 2.40*† 2.46*† k-BEST 4.31* 3.96* 3.92* 3.30* 4.01 3.87 ANGELI 4.03*† 3.70*† 4.26* 3.60* 3.56*† 3.33*† HUMAN 4.47† 4.37† 4.61† 4.03† 4.10 4.01 Table 4: Mean ratings for fluency (F) and semantic correctness (SC) on system output elicited by humans on ROBOCUP, WEATHERGOV, and ATIS (*: sig. diff. from HUMAN; †: sig. diff. from k-BEST.) With regard to WEATHERGOV, our generator improves over the baseline but lags behind Angeli et al. (2010). Since our system emits words based on a language model rather than a template, it displays more freedom in word order and lexical choice, and is thus penalized by BLEU when creating output that is overly distinct from the reference. On ATIS, our model outperforms both the baseline and Angeli et al. This is the most challenging domain with regard to surface realization with a vocabulary larger than ROBOCUP and WEATHERGOV by factors of 2.7 and 4.3, respectively. The results of our human evaluation study are shown in Table 3. We carried out an Analysis of Variance (ANOVA) to examine the effect </context>
</contexts>
<marker>Angeli, Liang, Klein, 2010</marker>
<rawString>Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Collective content selection for concept-to-text generation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>331--338</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="2455" citStr="Barzilay and Lapata, 2005" startWordPosition="347" endWordPosition="350"> text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2</context>
<context position="5594" citStr="Barzilay and Lapata (2005)" startWordPosition="849" endWordPosition="852">ormation about the domain and its structure globally, by considering the input space simultaneously during generation. Our only assumption is that the input must be a set of records essentially corresponding to database-like tables whose columns describe fields of a certain type. Experimental evaluation on three domains obtains results competitive to the state of the art without using any domain specific constraints, explicit feature engineering or labeled data. 2 Related Work Our work is situated within the broader class of data-driven approaches to content selection and surface realization. Barzilay and Lapata (2005) focus on the former problem which they view as an instance of collective classification (Barzilay and Lapata, 2005). Given a corpus of database records and texts describing some of them, they learn a content selection model that simultaneously optimizes local label assignments and their pairwise relations. Building on this work, Liang et al. (2009) present a hierarchical hidden semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. A few approaches have emerged more recently that combine content</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing, pages 331–338, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="3058" citStr="Belz, 2008" startWordPosition="445" endWordPosition="446">, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, Montr´eal, Canada, June 3-8, 2012. c�2012 Assoc</context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of International Conference on Machine Learning,</booktitle>
<pages>128--135</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="3081" citStr="Chen and Mooney, 2008" startWordPosition="447" endWordPosition="450">how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computationa</context>
<context position="22519" citStr="Chen and Mooney (2008)" startWordPosition="3668" endWordPosition="3671"> bottom-up fashion and creates sub-translations according to the CFG rule that holds each time. In the concept-to-text generation context, however, we do not observe the words; instead, for each leaf node we emit the k-best words from the underlying multinomial distribution (see weights on rules (8) and (9)) and continue building our sub-generations bottom-up. 4 Experimental Design Data We used our system to generate soccer commentaries, weather forecasts, and spontaneous utterances relevant to the air travel domain (examples are given in Figure 1). For the first domain we used the dataset of Chen and Mooney (2008), which consists of 1,539 scenarios from the 2001–2004 Robocup game finals. Each scenario contains on average |d |= 2.4 records, each paired with a short sentence (5.7 words). This domain has a small vocabulary (214 words) and simple syntax (e.g., a transitive verb with its subject and object). Records in this dataset (henceforth ROBOCUP) were aligned manually to their corresponding sentences (Chen and Mooney, 2008). Given the relatively small size of this dataset, we performed cross-validation following previous work (Chen and Mooney, 2008; Angeli et al., 2010). We trained our system on three</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In Proceedings of International Conference on Machine Learning, pages 128–135, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4892" citStr="Chiang, 2007" startWordPosition="745" endWordPosition="746">t (%) 06:00-09:00 09:00-12:00 25-50 50-75 Wind Speed Wind Direction Time Mode 06:00-21:00 S language. This grammar represents a set of trees which we encode compactly using a weighted hypergraph (or packed forest), a data structure that defines a probability (or weight) for each tree. Generation then boils down to finding the best derivation tree in the hypergraph which can be done efficiently using the Viterbi algorithm. In order to ensure that our generation output is fluent, we intersect our grammar with a language model and perform decoding using a dynamic programming algorithm (Huang and Chiang, 2007). Our model is conceptually simpler than previous approaches and encodes information about the domain and its structure globally, by considering the input space simultaneously during generation. Our only assumption is that the input must be a set of records essentially corresponding to database-like tables whose columns describe fields of a certain type. Experimental evaluation on three domains obtains results competitive to the state of the art without using any domain specific constraints, explicit feature engineering or labeled data. 2 Related Work Our work is situated within the broader cl</context>
<context position="10356" citStr="Chiang, 2007" startWordPosition="1635" endWordPosition="1636">xts) into a PCFG whose probabilities are learned from training data.1 Specifically, we convert the database into rewrite rules and represent them as a weighted directed hypergraph (Gallo et al., 1993). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyperarcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph representation allows us to integrate an n-gram language model and perform decoding efficiently using k-best Viterbi search, optimizing what to say and how to say at the same time. 3.1 Grammar Definition Our model captures the </context>
<context position="20834" citStr="Chiang (2007)" startWordPosition="3397" endWordPosition="3398">b are boundary words of the generation string, and ? is a place-holder symbol for an elided part of that string, indicating a sub-generation part ranging from a to b. An example +LM deduction of a single hyperarc of the hypergraph in Figure 2 using bigrams is: (2) FS1,2(temp1,start)low : (w1,g1), R2,2(temp1.t)around?degrees : (w2,g2) R1,1(skyCover1.t)low?degrees : (w,g1g2) w = w1 +w2 +ew + Plm(around I low) (3) where w1,w2 are node weights, g1,g2 are the corresponding sub-generations, ew is the weight of the hyperarc and w the weight of the resulting +LM item. Plm and (na?b) are defined as in Chiang (2007) in a generic fashion, allowing extension to an arbitrary size of n-gram grammars. Naive traversal of the hypergraph bottom-up would explore all possible +LM deductions along each hyperarc, and would increase decoding complexity to an infeasible O(2nn2), assuming a trigram model and a constant number of emissions at the terminal nodes. To ensure tractability, we adopt cube pruning, a popular approach in syntax-inspired machine translation (Chiang, 2007). The idea is to use a beam-search over the intersection grammar coupled with the cube-pruning heuristic. The beam limits the number of derivat</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah A Dahl</author>
<author>Madeleine Bates</author>
<author>Michael Brown</author>
<author>William Fisher</author>
<author>Kate Hunicke-Smith</author>
<author>David Pallett</author>
<author>Christine Pao</author>
<author>Alexander Rudnicky</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Expanding the scope of the atis task: the atis-3 corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology,</booktitle>
<pages>43--48</pages>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="23998" citStr="Dahl et al., 1994" startWordPosition="3915" endWordPosition="3918"> vocabulary in this domain (henceforth WEATHERGOV) is comparable to ROBOCUP (345 words), however, the texts are longer (Iwl = 29.3) and more varied. On average, each forecast has 4 sentences and the content selection problem is more challenging; only 5.8 out of the 36 records per scenario are mentioned in the text which roughly corresponds to 1.4 records per sentence. We used 25,000 scenarios from WEATHERGOV for training, 1,000 scenarios for development and 3,528 scenarios for testing. This is the same partition used in Angeli et al. (2010). For the air travel domain we used the ATIS dataset (Dahl et al., 1994), consisting of 5,426 scenarios. These are transcriptions of spontaneous utterances of users interacting with a hypothetical on757 WEATHERGOV ATIS ROBOCUP Near 57. Near 57. Near 57. Near 57. Near What what what what flights Pink9 to to Pink7 kicks pWq 57. Near 57. Near 57. Near 57. Near 57. from Denver Phoenix 1- Near 57. Near 57. South wind. As high as 23 mph. Chance of precipitation Show me the flights from Pink9 passes back to Pink7 is 20. Breezy, with a chance of showers. Denver to Phoenix Mostly cloudy, with a high near 57. South k- wind between 3 and 9 mph. GEL A chance of rain or drizzl</context>
</contexts>
<marker>Dahl, Bates, Brown, Fisher, Hunicke-Smith, Pallett, Pao, Rudnicky, Shriberg, 1994</marker>
<rawString>Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the atis task: the atis-3 corpus. In Proceedings of the Workshop on Human Language Technology, pages 43–48, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Content planner construction via evolutionary algorithms and a corpus-based fitness function.</title>
<date>2002</date>
<booktitle>In Proceedings of International Natural Language Generation,</booktitle>
<pages>89--96</pages>
<location>Ramapo Mountains, NY.</location>
<contexts>
<context position="2427" citStr="Duboue and McKeown, 2002" startWordPosition="343" endWordPosition="346">nd structure of the target text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (</context>
</contexts>
<marker>Duboue, McKeown, 2002</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2002. Content planner construction via evolutionary algorithms and a corpus-based fitness function. In Proceedings of International Natural Language Generation, pages 89–96, Ramapo Mountains, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Gallo</author>
<author>Giustino Longo</author>
<author>Stefano Pallottino</author>
<author>Sang Nguyen</author>
</authors>
<title>Directed hypergraphs and applications. Discrete Applied Mathematics,</title>
<date>1993</date>
<pages>42--177</pages>
<contexts>
<context position="9943" citStr="Gallo et al., 1993" startWordPosition="1563" endWordPosition="1566">rtscasting, scenarios describe individual events in the soccer game (e.g., passing or kicking the ball). In the air travel domain, scenarios comprise of flight-related details (e.g., origin, destination, day, time). Our goal then is to reduce the tasks of content selection and surface realization into a common probabilistic parsing problem. We do this by abstracting the structure of the database (and accompanying texts) into a PCFG whose probabilities are learned from training data.1 Specifically, we convert the database into rewrite rules and represent them as a weighted directed hypergraph (Gallo et al., 1993). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyperarcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). </context>
</contexts>
<marker>Gallo, Longo, Pallottino, Nguyen, 1993</marker>
<rawString>Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. 1993. Directed hypergraphs and applications. Discrete Applied Mathematics, 42:177–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technology,</booktitle>
<pages>53--64</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="17104" citStr="Huang and Chiang (2005)" startWordPosition="2714" endWordPosition="2717">kyCover1,time) R0,1(start) FS1,2(temp1,start) F1,2(temp1,min) W0,1(temp1,min) g0,1(min,v=10) R1,1(skyCover1.t) FS2,2(temp1,min) R2,2(temp1.t) F1,2(temp1,max) W0,1(temp1,max) R1,1(temp1.t) FS2,2(temp1,max) R2,2(skyCover1.t) FS0,1(temp1,start) g0,1(max,v=20) with ··· FS1,2(skyCover1,start) words) as well as the optimal segmentation of the text, provided we have a trained set of weights. The inside-outside algorithm is commonly used for estimating the weights of a PCFG. However, we first transform the CYK parser and our grammar into a hypergraph and then compute the weights using inside-outside. Huang and Chiang (2005) define a weighted directed hypergraph as follows: Definition 1 An ordered hypergraph H is a tuple hN,E,t,Ri, where N is a finite set of nodes, E is a finite set of hyperarcs and R is the set of weights. Each hyperarc e ∈ E is a triple e = hT(e),h(e), f(e)i, where h(e) ∈ N is its head node, T(e) ∈ N∗ is a set of tail nodes and f (e) is a monotonic weight function R|T(e) |to R and t ∈ N is a target node. Definition 2 We impose the arity of a hyperarc to be |e |= |T(e) |= 2, in other words, each head node is connected with at most two tail nodes. Given a context-free grammar G = hN,T,P,Si (where</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the 9th International Workshop on Parsing Technology, pages 53–64, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4892" citStr="Huang and Chiang, 2007" startWordPosition="743" endWordPosition="746">ime Percent (%) 06:00-09:00 09:00-12:00 25-50 50-75 Wind Speed Wind Direction Time Mode 06:00-21:00 S language. This grammar represents a set of trees which we encode compactly using a weighted hypergraph (or packed forest), a data structure that defines a probability (or weight) for each tree. Generation then boils down to finding the best derivation tree in the hypergraph which can be done efficiently using the Viterbi algorithm. In order to ensure that our generation output is fluent, we intersect our grammar with a language model and perform decoding using a dynamic programming algorithm (Huang and Chiang, 2007). Our model is conceptually simpler than previous approaches and encodes information about the domain and its structure globally, by considering the input space simultaneously during generation. Our only assumption is that the input must be a set of records essentially corresponding to database-like tables whose columns describe fields of a certain type. Experimental evaluation on three domains obtains results competitive to the state of the art without using any domain specific constraints, explicit feature engineering or labeled data. 2 Related Work Our work is situated within the broader cl</context>
<context position="10699" citStr="Huang and Chiang, 2007" startWordPosition="1690" endWordPosition="1693"> to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph representation allows us to integrate an n-gram language model and perform decoding efficiently using k-best Viterbi search, optimizing what to say and how to say at the same time. 3.1 Grammar Definition Our model captures the inherent structure of the database with a number of CFG rewrite rules, in a similar way to how Liang et al. (2009) define Markov chains in the different levels of their hierarchical model. These rules are purely syntactic (describing the intuitive relationship between records, records and fields, fields and corresponding words), and could ap</context>
<context position="21709" citStr="Huang and Chiang (2007)" startWordPosition="3536" endWordPosition="3539">), assuming a trigram model and a constant number of emissions at the terminal nodes. To ensure tractability, we adopt cube pruning, a popular approach in syntax-inspired machine translation (Chiang, 2007). The idea is to use a beam-search over the intersection grammar coupled with the cube-pruning heuristic. The beam limits the number of derivations for each node, whereas cubepruning further limits the number of +LM items considered for inclusion in the beam. Since f (e) in Definition 1 is monotonic, we can select the k-best items without computing all possible +LM items. Our decoder follows Huang and Chiang (2007) but importantly differs in the treatment of leaf nodes in the hypergraph (see rules (8) and (9)). In the SCFG context, the Viterbi algorithm consumes terminals from the source string in a bottom-up fashion and creates sub-translations according to the CFG rule that holds each time. In the concept-to-text generation context, however, we do not observe the words; instead, for each leaf node we emit the k-best words from the underlying multinomial distribution (see weights on rules (8) and (9)) and continue building our sub-generations bottom-up. 4 Experimental Design Data We used our system to </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>586--594</pages>
<location>Columbus, Ohio.</location>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-08: HLT, pages 586–594, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Subahshini Gunasekharan</author>
<author>Neil Mayo</author>
<author>Martin Corley</author>
</authors>
<title>Timing accuracy of Web experiments: A case study using the WebExp software package.</title>
<date>2009</date>
<journal>Behavior Research Methods,</journal>
<volume>41</volume>
<issue>1</issue>
<contexts>
<context position="28594" citStr="Keller et al., 2009" startWordPosition="4671" endWordPosition="4674">758 ROBOCUP WEATHERGOV ATIS System BLEU BLEU BLEU 1-BEST 10.79 8.64 11.85 k-BEST 30.90 33.70 29.30 ANGELI 28.70 38.40 26.77 KIM-MOONEY 47.27 — — Table 3: BLEU scores on ROBOCUP (fixed content selection), WEATHERGOV, and ATIS. uments from the test set (for each domain) and generated output with our models (1-BEST and k-BEST) and Angeli et al.’s (2010) model (see Figure 2 for examples of system output). We also included the original text (HUMAN) as gold standard. We thus obtained ratings for 48 (12 x 4) scenario-text pairs for each domain. The study was conducted over the Internet using WebExp (Keller et al., 2009) and was completed by 114 volunteers, all self reported native English speakers. 5 Results We conducted two experiments on the ROBOCUP domain. We first assessed the performance of our generator (k-BEST) on joint content selection and surface realization and obtained a BLEU score of 24.88. In comparison, the baseline’s (1-BEST) BLEU score was 8.01. In a second experiment we forced the generator to use the gold-standard records from the database. This was necessary in order to compare with previous work (Angeli et al., 2010; Kim and Mooney, 2010).6 Our results are summarized in Table 3. Overall,</context>
</contexts>
<marker>Keller, Gunasekharan, Mayo, Corley, 2009</marker>
<rawString>Frank Keller, Subahshini Gunasekharan, Neil Mayo, and Martin Corley. 2009. Timing accuracy of Web experiments: A case study using the WebExp software package. Behavior Research Methods, 41(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond Mooney</author>
</authors>
<title>Generative alignment and semantic parsing for learning from ambiguous supervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd Conference on Computational Linguistics,</booktitle>
<pages>543--551</pages>
<location>Beijing, China.</location>
<contexts>
<context position="3125" citStr="Kim and Mooney, 2010" startWordPosition="455" endWordPosition="458">, 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics (a) (b) Figure 1: Input-output</context>
<context position="6251" citStr="Kim and Mooney (2010)" startWordPosition="952" endWordPosition="955">hey view as an instance of collective classification (Barzilay and Lapata, 2005). Given a corpus of database records and texts describing some of them, they learn a content selection model that simultaneously optimizes local label assignments and their pairwise relations. Building on this work, Liang et al. (2009) present a hierarchical hidden semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. A few approaches have emerged more recently that combine content selection and surface realization. Kim and Mooney (2010) adopt a two-stage approach: using a generative model similar to Liang et al. (2009), they first decide what to say and then verbalize the selected input with WASP−1, an existing generation system (Wong and Mooney, 2007). In contrast, Angeli et al. (2010) propose a unified content selection and surface realization model which also operates over the alignment output produced by Liang et al. (2009). Their model decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally </context>
<context position="27319" citStr="Kim and Mooney, 2010" startWordPosition="4463" endWordPosition="4466">that the average text length is small. For WEATHERGOV and ATIS, we used a trigram language model. System Comparison We evaluated two configurations of our system. A baseline that uses the top scoring derivation in each subgeneration (1-BEST) and another version which makes better use of our decoding algorithm and considers the best k derivations (i.e., 15 for WEATHERGOV, 40 for ATIS, and 25 for ROBOCUP). We compared our output to Angeli et al. (2010) whose approach is closest to ours and state-of-the-art on the WEATHERGOV domain. For ROBOCUP, we also compare against the bestpublished results (Kim and Mooney, 2010). Evaluation We evaluated system output automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along two dimensions: fluency (is the text grammatical and overall understandable?) and semantic correctness (does the meaning conveyed by the text correspond to the database input?). The subjects used a five point rating scale where a high number in</context>
<context position="29144" citStr="Kim and Mooney, 2010" startWordPosition="4762" endWordPosition="4765">tudy was conducted over the Internet using WebExp (Keller et al., 2009) and was completed by 114 volunteers, all self reported native English speakers. 5 Results We conducted two experiments on the ROBOCUP domain. We first assessed the performance of our generator (k-BEST) on joint content selection and surface realization and obtained a BLEU score of 24.88. In comparison, the baseline’s (1-BEST) BLEU score was 8.01. In a second experiment we forced the generator to use the gold-standard records from the database. This was necessary in order to compare with previous work (Angeli et al., 2010; Kim and Mooney, 2010).6 Our results are summarized in Table 3. Overall, our generator performs better than the baseline and Angeli et al. (2010). We observe a substantial increase in performance compared to the joint content selection and surface realization setting. This is expected as the generator is faced with an easier task and there is less scope for error. Our model does not outperform Kim and Mooney (2010), however, this is not entirely surprising as their model requires considerable more supervision (e.g., during parameter initialization) and includes a post-hoc re-ordering component. 6Angeli et al. (2010</context>
</contexts>
<marker>Kim, Mooney, 2010</marker>
<rawString>Joohyun Kim and Raymond Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In Proceedings of the 23rd Conference on Computational Linguistics, pages 543–551, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th International Workshop on Parsing Technologies,</booktitle>
<pages>123--134</pages>
<location>Beijing, China.</location>
<contexts>
<context position="10541" citStr="Klein and Manning, 2001" startWordPosition="1664" endWordPosition="1667">graph (Gallo et al., 1993). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyperarcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph representation allows us to integrate an n-gram language model and perform decoding efficiently using k-best Viterbi search, optimizing what to say and how to say at the same time. 3.1 Grammar Definition Our model captures the inherent structure of the database with a number of CFG rewrite rules, in a similar way to how Liang et al. (2009) define Markov chains in the different levels of their hierarchical mod</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proceedings of the 7th International Workshop on Parsing Technologies, pages 123– 134, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>40--51</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="10130" citStr="Li and Eisner, 2009" startWordPosition="1594" endWordPosition="1597">gin, destination, day, time). Our goal then is to reduce the tasks of content selection and surface realization into a common probabilistic parsing problem. We do this by abstracting the structure of the database (and accompanying texts) into a PCFG whose probabilities are learned from training data.1 Specifically, we convert the database into rewrite rules and represent them as a weighted directed hypergraph (Gallo et al., 1993). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyperarcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph r</context>
<context position="18641" citStr="Li and Eisner, 2009" startWordPosition="3022" endWordPosition="3025">h rewrite rule A → BC in P, with three free indices i &lt; j &lt; k, is mapped to the hyperarc h((B,i, j),(C, j,k)),(A,i,k), fi, where f = f ((B,i, j)) f ((C, j,k)) · Pr(A → BC).3 The hy3Similarly, rewrite rules of type A → B are mapped to the hyperarc h(B,i, j),(A,i, j), fi, with f = f((B,i, j))·Pr(A → B). pergraph can be thus viewed as a compiled lattice of the corresponding chart graph. Figure 2 shows an example hypergraph for a grammar defined on database input similar to Figure (1b). In order to learn the weights on the hyperarcs we perform the following procedure iteratively in an EM fashion (Li and Eisner, 2009). For each training scenario we build its hypergraph representation. Next, we perform inference by calculating the inside and outside scores of the hypergraph, so as to compute the posterior distribution over its hyperarcs (E-step). Finally, we collectively update the posteriors on the parameters-weights, i.e., rule probabilities and emission multinomial distributions (M-step). 3.3 Decoding In the framework outlined above, parsing an input string w (given some learned weights) boils down to traversing the hypergraph in a particular order. (Note that the hypergraph should be acyclic, which is a</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40–51, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In roceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>91--99</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2511" citStr="Liang et al., 2009" startWordPosition="357" endWordPosition="360">l content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and</context>
<context position="5945" citStr="Liang et al. (2009)" startWordPosition="906" endWordPosition="909">tate of the art without using any domain specific constraints, explicit feature engineering or labeled data. 2 Related Work Our work is situated within the broader class of data-driven approaches to content selection and surface realization. Barzilay and Lapata (2005) focus on the former problem which they view as an instance of collective classification (Barzilay and Lapata, 2005). Given a corpus of database records and texts describing some of them, they learn a content selection model that simultaneously optimizes local label assignments and their pairwise relations. Building on this work, Liang et al. (2009) present a hierarchical hidden semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. A few approaches have emerged more recently that combine content selection and surface realization. Kim and Mooney (2010) adopt a two-stage approach: using a generative model similar to Liang et al. (2009), they first decide what to say and then verbalize the selected input with WASP−1, an existing generation system (Wong and Mooney, 2007). In contrast, Angeli et al. (2010) propose a unified content selection an</context>
<context position="11070" citStr="Liang et al. (2009)" startWordPosition="1754" endWordPosition="1757">graphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph representation allows us to integrate an n-gram language model and perform decoding efficiently using k-best Viterbi search, optimizing what to say and how to say at the same time. 3.1 Grammar Definition Our model captures the inherent structure of the database with a number of CFG rewrite rules, in a similar way to how Liang et al. (2009) define Markov chains in the different levels of their hierarchical model. These rules are purely syntactic (describing the intuitive relationship between records, records and fields, fields and corresponding words), and could apply to any database with similar structure irrespectively of the semantics of the domain. Our grammar is defined in Table 1 (rules (1)–(9)). Rule weights are governed by an underlying multinomial distribution and are shown in square brackets. Non-terminal symbols are in capitals and de1An alternative would be to learn a SCFG between the database input and the accompany</context>
<context position="15414" citStr="Liang et al., 2009" startWordPosition="2473" endWordPosition="2476">en words over and above to integrating a language model during decoding (see Section 3.3). Rules (8) and (9) define the emission of words and integer numbers from W, given a field type and its value. Rule (8) emits a single word from the vocabulary of the training set. Its weight defines a multinomial distribution over all seen words, for every value of field f, given that the field type is categorical or the special null field. Rule (9) is identical but for fields whose type is integer. Function g(f.v) generates an integer number given the field value, using either of the following six ways (Liang et al., 2009): identical to the field value, rounding up or rounding down to a multiple of 5, rounding off to the closest multiple of 5 and finally adding or subtracting some unexplained noise.2 The weight is a multinomial over the six generation function modes, given the record field f. 3.2 Hypergraph Construction So far we have defined a probabilistic grammar that captures the structure of a database d with records and fields as intermediate non-terminals, and words w (from the associated text) as terminals. Using this grammar and the CYK parsing algorithm, we could obtain the top scoring derivation of r</context>
<context position="23278" citStr="Liang et al. (2009)" startWordPosition="3793" endWordPosition="3796"> with a short sentence (5.7 words). This domain has a small vocabulary (214 words) and simple syntax (e.g., a transitive verb with its subject and object). Records in this dataset (henceforth ROBOCUP) were aligned manually to their corresponding sentences (Chen and Mooney, 2008). Given the relatively small size of this dataset, we performed cross-validation following previous work (Chen and Mooney, 2008; Angeli et al., 2010). We trained our system on three ROBOCUP games and tested on the fourth, averaging over the four train/test splits. For weather forecast generation, we used the dataset of Liang et al. (2009), which consists of 29,528 weather scenarios for 3,753 major US cities (collected over four days). The vocabulary in this domain (henceforth WEATHERGOV) is comparable to ROBOCUP (345 words), however, the texts are longer (Iwl = 29.3) and more varied. On average, each forecast has 4 sentences and the content selection problem is more challenging; only 5.8 out of the 36 records per scenario are mentioned in the text which roughly corresponds to 1.4 records per sentence. We used 25,000 scenarios from WEATHERGOV for training, 1,000 scenarios for development and 3,528 scenarios for testing. This is</context>
<context position="25317" citStr="Liang et al. (2009)" startWordPosition="4148" endWordPosition="4151">mph. from Nashville to Phoenix A slight chance of showers. Mostly cloudy, List flights from Denver to Pink9 passes back to Pink7 with a high near 58. South wind between 3 Phoenix and 9 mph, with gusts as high as 23 mph. H Chance of precipitation is 20%. Table 2: System output on WEATHERGOV, ATIS, and ROBOCUP (1-BEST, k-BEST, ANGELI) and corresponding human-authored text (HUMAN). line flight booking system. We used the dataset introduced in Zettlemoyer and Collins (2007)4 and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al. (2009). For example, the scenario in Figure 1(a) was initially represented as: A.x.flight(x) n from(x,phoenix) n to(x,new york)nday(x,sunday).5 In contrast to the two previous datasets, ATIS has a much richer vocabulary (927 words); each scenario corresponds to a single sentence (average length is 11.2 words) with 2.65 out of 19 record types mentioned on average. Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. Model Parameters Our model has two parameters, namely the number of k grammar derivations considered by the decode</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In roceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 91–99, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A probabilistic forestto-string model for language generation from typed lambda calculus expressions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1611--1622</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="7236" citStr="Lu and Ng, 2011" startWordPosition="1114" endWordPosition="1117">roduced by Liang et al. (2009). Their model decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component is based on templates that are automatically extracted and smoothed with domainspecific constraints in order to guarantee fluent output. Other related work (Wong and Mooney, 2007; Lu and Ng, 2011). has focused on generating natural language sentences from logical form (i.e., lambdaexpressions) using mostly synchronous context-free grammars (SCFGs). 753 Similar to Angeli et al. (2010), we also present an end-to-end system that performs content selection and surface realization. However, rather than breaking up the generation task into a sequence of local decisions, we optimize what to say and how to say simultaneously. We do not learn mappings from a logical form, but rather focus on input which is less constrained, possibly more noisy and with a looser structure. Our key insight is to </context>
</contexts>
<marker>Lu, Ng, 2011</marker>
<rawString>Wei Lu and Hwee Tou Ng. 2011. A probabilistic forestto-string model for language generation from typed lambda calculus expressions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1611–1622, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="27437" citStr="Papineni et al., 2002" startWordPosition="4480" endWordPosition="4483">e evaluated two configurations of our system. A baseline that uses the top scoring derivation in each subgeneration (1-BEST) and another version which makes better use of our decoding algorithm and considers the best k derivations (i.e., 15 for WEATHERGOV, 40 for ATIS, and 25 for ROBOCUP). We compared our output to Angeli et al. (2010) whose approach is closest to ours and state-of-the-art on the WEATHERGOV domain. For ROBOCUP, we also compare against the bestpublished results (Kim and Mooney, 2010). Evaluation We evaluated system output automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along two dimensions: fluency (is the text grammatical and overall understandable?) and semantic correctness (does the meaning conveyed by the text correspond to the database input?). The subjects used a five point rating scale where a high number indicates better performance. We randomly selected 12 doc758 ROBOCUP WEATHERGOV ATIS System BLEU BLEU BLEU 1-BEST 10.79 </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building natural language generation systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="1301" citStr="Reiter and Dale, 2000" startWordPosition="177" endWordPosition="180">ar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We represent our grammar compactly as a weighted hypergraph and recast generation as the task of finding the best derivation tree for a given input. Experimental evaluation on several domains achieves competitive results with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data. 1 Introduction Concept-to-text generation broadly refers to the task of automatically producing textual output from nonlinguistic input (Reiter and Dale, 2000). Depending on the application and the domain at hand, the input may assume various representations including databases of records, expert system knowledge bases, simulations of physical systems and so on. Figure 1 shows input examples and their corresponding text for three domains, air travel, sportscasting and weather forecast generation. A typical concept-to-text generation system implements a pipeline architecture consisting of three core stages, namely text planning (determining the content and structure of the target text), sentence planning (determining the structure and lexical content</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge University Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
<author>Jim Hunter</author>
<author>Ian Davy</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>167</volume>
<pages>169</pages>
<contexts>
<context position="3046" citStr="Reiter et al., 2005" startWordPosition="441" endWordPosition="444">; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, Montr´eal, Canada, June 3-8, 2012. </context>
</contexts>
<marker>Reiter, Sripada, Hunter, Davy, 2005</marker>
<rawString>Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian Davy. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence, 167:137– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology and the Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>172--179</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="6471" citStr="Wong and Mooney, 2007" startWordPosition="990" endWordPosition="993">local label assignments and their pairwise relations. Building on this work, Liang et al. (2009) present a hierarchical hidden semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. A few approaches have emerged more recently that combine content selection and surface realization. Kim and Mooney (2010) adopt a two-stage approach: using a generative model similar to Liang et al. (2009), they first decide what to say and then verbalize the selected input with WASP−1, an existing generation system (Wong and Mooney, 2007). In contrast, Angeli et al. (2010) propose a unified content selection and surface realization model which also operates over the alignment output produced by Liang et al. (2009). Their model decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component is based on templates that are </context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In Proceedings of the Human Language Technology and the Conference of the North American Chapter of the Association for Computational Linguistics, pages 172–179, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>678--687</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="25172" citStr="Zettlemoyer and Collins (2007)" startWordPosition="4128" endWordPosition="4131">wind between 3 and 9 mph. GEL A chance of rain or drizzle, with a high near Show me the flights leave Pink9 kicks to Pink7 N 57. South wind between 3 and 9 mph. from Nashville to Phoenix A slight chance of showers. Mostly cloudy, List flights from Denver to Pink9 passes back to Pink7 with a high near 58. South wind between 3 Phoenix and 9 mph, with gusts as high as 23 mph. H Chance of precipitation is 20%. Table 2: System output on WEATHERGOV, ATIS, and ROBOCUP (1-BEST, k-BEST, ANGELI) and corresponding human-authored text (HUMAN). line flight booking system. We used the dataset introduced in Zettlemoyer and Collins (2007)4 and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al. (2009). For example, the scenario in Figure 1(a) was initially represented as: A.x.flight(x) n from(x,phoenix) n to(x,new york)nday(x,sunday).5 In contrast to the two previous datasets, ATIS has a much richer vocabulary (927 words); each scenario corresponds to a single sentence (average length is 11.2 words) with 2.65 out of 19 record types mentioned on average. Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV9</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 678–687, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>