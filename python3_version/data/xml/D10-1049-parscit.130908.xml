<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988851">
A Simple Domain-Independent Probabilistic Approach to Generation
</title>
<author confidence="0.691265">
Gabor Angeli
</author>
<address confidence="0.659747">
UC Berkeley
Berkeley, CA 94720
</address>
<email confidence="0.997255">
gangeli@berkeley.edu
</email>
<author confidence="0.842017">
Percy Liang
</author>
<address confidence="0.6604185">
UC Berkeley
Berkeley, CA 94720
</address>
<email confidence="0.996133">
pliang@cs.berkeley.edu
</email>
<author confidence="0.791302">
Dan Klein
</author>
<address confidence="0.669542">
UC Berkeley
Berkeley, CA 94720
</address>
<email confidence="0.997763">
klein@cs.berkeley.edu
</email>
<sectionHeader confidence="0.99561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921384615385">
We present a simple, robust generation system
which performs content selection and surface
realization in a unified, domain-independent
framework. In our approach, we break up
the end-to-end generation process into a se-
quence of local decisions, arranged hierar-
chically and each trained discriminatively.
We deployed our system in three different
domains—Robocup sportscasting, technical
weather forecasts, and common weather fore-
casts, obtaining results comparable to state-of-
the-art domain-specific systems both in terms
of BLEU scores and human evaluation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976588235294">
In this paper, we focus on the problem of generat-
ing descriptive text given a world state represented
by a set of database records. While existing gen-
eration systems can be engineered to obtain good
performance on particular domains (e.g., Dale et
al. (2003), Green (2006), Turner et al. (2009), Re-
iter et al. (2005), inter alia), it is often difficult
to adapt them across different domains. Further-
more, content selection (what to say: see Barzilay
and Lee (2004), Foster and White (2004), inter alia)
and surface realization (how to say it: see Ratna-
parkhi (2002), Wong and Mooney (2007), Chen and
Mooney (2008), Lu et al. (2009), etc.) are typically
handled separately. Our goal is to build a simple,
flexible system which is domain-independent and
performs content selection and surface realization in
a unified framework.
We operate in a setting in which we are only given
examples consisting of (i) a set of database records
(input) and (ii) example human-generated text de-
scribing some of those records (output). We use the
model of Liang et al. (2009) to automatically induce
the correspondences between words in the text and
the actual database records mentioned.
We break up the full generation process into a se-
quence of local decisions, training a log-linear clas-
sifier for each type of decision. We use a simple
but expressive set of domain-independent features,
where each decision is allowed to depend on the en-
tire history of previous decisions, as in the model
of Ratnaparkhi (2002). These long-range contextual
dependencies turn out to be critical for accurate gen-
eration.
More specifically, our model is defined in terms
of three types of decisions. The first type
chooses records from the database (macro content
selection)—for example, wind speed, in the case
of generating weather forecasts. The second type
chooses a subset of fields from a record (micro con-
tent selection)—e.g., the minimum and maximum
temperature. The third type chooses a suitable tem-
plate to render the content (surface realization)—
e.g., winds between [min] and [max] mph; templates
are automatically extracted from training data.
We tested our approach in three domains:
ROBOCUP, for sportscasting (Chen and Mooney,
2008); SUMTIME, for technical weather forecast
generation (Reiter et al., 2005); and WEATHERGOV,
for common weather forecast generation (Liang et
al., 2009). We performed both automatic (BLEU)
and human evaluation. On WEATHERGOV, we
</bodyText>
<page confidence="0.954156">
502
</page>
<note confidence="0.62396875">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
w: purple3 made a bad pass
that was picked off by pink9
</note>
<figure confidence="0.999263095238095">
(a) ROBOCUP
s: temperature(time=5pm-6am,min=48,mean=53,max=61)
windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10)
windDir(time=5pm-6am,mode=SSW)
gust(time=5pm-6am,min=0,mean=0,max=0)
skyCover(time=5pm-9pm,mode=0-25)
skyCover(time=2am-6am,mode=75-100)
precipPotential(time=5pm-6am,min=2,mean=14,max=20)
rainChance(time=5pm-6am,mode=someChance)
w: a 20 percent chance of showers after midnight . increasing clouds ,
with a low around 48 southwest wind between 5 and 10 mph
(b) WEATHERGOv
s: pass(arg1=purple6, arg2=purple3)
kick(arg1=purple3)
badPass(arg1=purple3,arg2=pink9)
turnover(arg1=purple3,arg2=pink9)
s: wind10m(time=6am,dir=SW,min=16,max=20,gust min=0,gust max=-)
wind10m(time=9pm,dir=SSW,min=28,max=32,gust min=40,gust max=-)
wind10m(time=12am,dir=-,min=24,max=28,gust min=36,gust max=-)
w: sw 16 - 20 backing ssw 28 - 32 gusts 40 by mid evening easing 24 - 28 gusts 36 late evening
(c) SUMTIME
</figure>
<figureCaption confidence="0.997691666666667">
Figure 1: Example scenarios (a scenario is a world state s paired with a text w) for each of the three domains. Each row in the
world state denotes a record. Our generation task is to map a world state s (input) to a text w (output). Note that this mapping
involves both content selection and surface realization.
</figureCaption>
<bodyText confidence="0.999677625">
achieved a BLEU score of 51.5 on the combined task
of content selection and generation, which is more
than a two-fold improvement over a model similar
to that of Liang et al. (2009). On ROBOCUP and
SUMTIME, we achieved results comparable to the
state-of-the-art. most importantly, we obtained these
results with a general-purpose approach that we be-
lieve is simpler than current state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.907071" genericHeader="introduction">
2 Setup and Domains
</sectionHeader>
<bodyText confidence="0.9999815">
Our goal is to generate a text given a world state.
The world state, denoted s, is represented by a set
of database records. Define T to be a set of record
types, where each record type t E T is associated
with a set of fields FIELDS(t). Each record r E s
has a record type r.t E T and afield value r.v[f] for
each field f E FIELDS(t). The text, denoted w, is
represented by a sequence of tokenized words. We
use the term scenario to denote a world state s paired
with a text w.
In this paper, we conducted experiments on three
domains, which are detailed in the following subsec-
tions. Example scenarios for each domain are de-
tailed in Figure 1.
</bodyText>
<subsectionHeader confidence="0.917767">
2.1 ROBOCUP: Sportscasting
</subsectionHeader>
<bodyText confidence="0.999460391304348">
A world state in the ROBOCUP domain is a set of
event records (meaning representations in the termi-
nology of Chen and Mooney (2008)) generated by
a robot soccer simulator. For example, the record
pass(arg1=pink1,arg2=pink5) denotes a passing
event; records of this type (pass) have two fields:
arg1 (the agent) and arg2 (the recipient). As the
game progresses, human commentators talk about
some of the events in the game, e.g., purple3 made
a bad pass that was picked off by pink9.
We used the dataset created by Chen and Mooney
(2008), which contains 1919 scenarios from the
2001–2004 Robocup finals. Each scenario con-
sists of a single sentence representing a fragment
of a commentary on the game, paired with a set
of candidate records, which were recorded within
five seconds of the commentary. The records in the
ROBOCUP dataset data were aligned by Chen and
Mooney (2008). Each scenario contains on average
s = 2.4 records and 5.7 words. See Figure 1(a) for
an example of a scenario. Content selection in this
domain is choosing the single record to talk about,
and surface realization is talking about it.
</bodyText>
<page confidence="0.996964">
503
</page>
<subsectionHeader confidence="0.952823">
2.2 SUMTIME: Technical Weather Forecasts
</subsectionHeader>
<bodyText confidence="0.999941411764706">
Reiter et al. (2005) developed a generation system
and created the SUMTIME-METEO corpus, which
consists of marine wind weather forecasts used by
offshore oil rigs, generated by the output of weather
simulators. More specifically, these forecasts de-
scribe various aspects of the wind at different times
during the forecast period.
We used the version of the SUMTIME-METEO
corpus created by Belz (2008). The dataset consists
of 469 scenarios, each containing on average s =
2.6 records and 16.2 words. See Figure 1(c) for an
example of a scenario. This task requires no content
selection, only surface realization: The records are
given in some fixed order and the task is to generate
from each of these records in turn; of course, due
to contextual dependencies, these records cannot be
generated independently.
</bodyText>
<subsectionHeader confidence="0.9426215">
2.3 WEATHERGOV: Common Weather
Forecasts
</subsectionHeader>
<bodyText confidence="0.999956346153846">
In the WEATHERGOV domain, the world state con-
tains detailed information about a local weather
forecast (e.g., temperature, rain chance, etc.). The
text is a short forecast report based on this informa-
tion.
We used the dataset created by Liang et al. (2009).
The world state is summarized by records which ag-
gregate measurements over selected time intervals.
The dataset consists of 29,528 scenarios, each con-
taining on average s = 36 records and 28.7 words.
See Figure 1(b) for an example of a scenario.
While SUMTIME and WEATHERGOV are both
weather domains, there are significant differences
between the two. SUMTIME forecasts are in-
tended to be read by trained meteorologists, and thus
the text is quite abbreviated. On the other hand,
WEATHERGOV texts are intended to be read by the
general public and thus is more English-like. Fur-
thermore, SUMTIME does not require content selec-
tion, whereas content selection is a major focus of
WEATHERGOV. Indeed, on average, only 5 of 36
records are actually mentioned in a WEATHERGOV
scenario. Also, WEATHERGOV is more complex:
The text is more varied, there are multiple record
types, and there are about ten times as many records
in each world state.
</bodyText>
<subsectionHeader confidence="0.836323">
Generation Process
</subsectionHeader>
<bodyText confidence="0.832837">
for i = 1, 2,... :
choose a record ri E s
if ri = STOP: return
choose afield set Fi C FIELDS(ri.t)
choose a template Ti E TEMPLATES(ri.t, Fi)
</bodyText>
<figureCaption confidence="0.676056">
Figure 2: Pseudocode for the generation process. The generated
text w is a deterministic function of the decisions.
</figureCaption>
<sectionHeader confidence="0.962751" genericHeader="method">
3 The Generation Process
</sectionHeader>
<bodyText confidence="0.999967714285715">
To model the process of generating a text w from a
world state s, we decompose the generation process
into a sequence of local decisions. There are two as-
pects of this decomposition that we need to specify:
(i) how the decisions are structured; and (ii) what
pieces of information govern the decisions.
The decisions are structured hierarchically into
three types of decisions: (i) record decisions, which
determine which records in the world state to talk
about (macro content selection); (ii) field set deci-
sions, which determine which fields of those records
to mention (micro content selection); and (iii) tem-
plate decisions, which determine the actual words
to use to describe the chosen fields (surface realiza-
tion). Figure 2 shows the pseudocode for the gen-
eration process, while Figure 3 depicts an example
of the generation process on a WEATHERGOV sce-
nario.
Each of these decisions is governed by a set of
feature templates (see Figure 4), which are repre-
sented as functions of the current decision and past
decisions. The feature weights are learned from
training data (see Section 4.3).
We chose a set of generic domain-independent
feature templates, described in the sections below.
These features can, in general, depend on the current
decision and all previous decisions. For example, re-
ferring to Figure 4, R2 features on the record choice
depend on all the previous record decisions, and R5
features depend on the most recent template deci-
sion. This is in contrast with most systems for con-
tent selection (Barzilay and Lee, 2004) and surface
realization (Belz, 2008), where decisions must de-
compose locally according to either a graph or tree.
The ability to use global features in this manner is
</bodyText>
<page confidence="0.985202">
504
</page>
<figure confidence="0.338880421052632">
World
state
Decisions
skyCover1: skyCover(time=5pm-6am,mode=50-75)
temperature1: temperature(time=5pm-6am,min=44,mean=49,max=60)
...
Record r1 = skyCover1 r2 = temperature1 r3 = STOP
Field set F1 = {mode} F2 = {time, min}
Template T1 = (mostly cloudy ,) T2 = (with a low around [min] .)
Text mostly cloudy , with a low around 45 .
Specific active (nonzero) features for highlighted decisions
(R1) Qr2.t = temperature and (r1.t, r0.t) = (skyCover, START)]
Qr2.t = temperature and (r1.t) = (skyCover)]
(R2) Qr2.t = temperature and {r1.t} = {skyCover}]
(R3) Qr2.t = temperature and rj.t =� temperature Hj G 2]
(R4) Qr2.t = temperature and r2.v[time] = 5pm-6am]
Qr2.t = temperature and r2.v[min] = low]
Qr2.t = temperature and r2.v[mean] = low]
Qr2.t = temperature and r2.v[max] = medium]
</figure>
<equation confidence="0.991970923076923">
(F1) QF2 = {time, min}]
(F2) QF2 = {time, min} and r2.v[time] = 5pm-6am]
(F2) QF2 = {time, min} and r2.v[min] = low]
(W1) QBASE(T2) = (with a low around [min])]
QCOARSE(T2) = (with a [time] around [min])]
(W2) QBASE(T2) = (with a low around [min]) and r2.v[time] = 5pm-6am]
QCOARSE(T2) = (with a [time] around [min]) and r2.v[time] = 5pm-6am]
QBASE(T2) = (with a low around [min]) and r2.v[min] = low]
QCOARSE(T2) = (with a [time] around [min]) and r2.v[min] = low]
(W3) logp_(with  |cloudy,)
r2 = temperature1
F2 = {time, min}
T2 = (with a low around [min])
</equation>
<figureCaption confidence="0.934584">
Figure 3: The generation process on an example WEATHERGOV scenario. The figure is divided into two parts: The upper part of
the figure shows the generation of text from the world state via a sequence of seven decisions (in boxes). Three of these decisions
are highlighted and the features that govern these decisions are shown in the lower part of the figure. Note that different decisions
in the generation process would result in different features being active (nonzero).
</figureCaption>
<figure confidence="0.959198363636364">
Feature Templates
Record R1† list of last k record types Jri.t = * and (ri_1.t,...,ri_k.t) = *K for k E {1,21
R2 set of previous record types Jri.t = * and {rj.t : j G il = *K
R3 record type already generated Jrj.t = ri.t for some j G iK
R4 field values Jri.t = * and ri.v[f] = *K for f E FsELDS(ri.t)
R5† stop under language model (LM) Jri.t = STOPK x log pLm(STOP I previous two words generated)
Field set F1† field set JFi = *K
F2 field values JFi = * and ri.v[f] = *K for f E Fi
Template W1† base/coarse generation template Jh(Ti) = *K for h E {BASE, COARSEI
W2 field values Jh(Ti) = * and ri.v[f] = *K for f E Fi, h E {BASE, COARSEI
W3† first word of template under LM log pLm(first word in Ti I previous two words)
</figure>
<figureCaption confidence="0.7222004">
Figure 4: Feature templates that govern the record, field set, and template decisions. Each line specifies the name, informal
description, and formal description of a set of features, obtained by ranging * over possible values (for example, for Qri.t = *J, *
ranges over all record types T). Notation: Qej returns 1 if the expression e is true and 0 if it is false. These feature templates are
domain-independent; that is, they are used to create features automatically across domains. Feature templates marked with † are
included in our baseline system (Section 5.2).
</figureCaption>
<bodyText confidence="0.981387">
one of the principal advantages of our approach.
</bodyText>
<subsectionHeader confidence="0.997829">
3.1 Record Decisions
</subsectionHeader>
<bodyText confidence="0.999652285714286">
Record decisions are responsible for macro content
selection. Each record decision chooses a record ri
from the world state s according to features of the
following types:
R1 captures the discourse coherence aspect of
content selection; for example, we learn that
windSpeed tends to follow windDir (but not al-
</bodyText>
<page confidence="0.989418">
505
</page>
<bodyText confidence="0.999429769230769">
ways). R2 captures an unordered notion of
coherence—simply which sets of record types are
preferable; for example, we learn that rainChance
is not generated if sleetChance already was men-
tioned. R3 is a coarser version of R2, capturing
how likely it is to propose a record of a type that has
already been generated. R4 captures the important
aspect of content selection that the records chosen
depend on their field values;1 for example, we learn
that snowChance is not chosen unless there is snow.
R5 allows the language model to indicate whether a
STOP record is appropriate; this helps prevent sen-
tences from ending abruptly.
</bodyText>
<subsectionHeader confidence="0.998962">
3.2 Field Set Decisions
</subsectionHeader>
<bodyText confidence="0.9999299375">
Field set decisions are responsible for micro con-
tent selection, i.e., which fields of a record are men-
tioned. Each field set decision chooses a subset of
fields Fi from the set of fields FIELDS(ri.t) of the
record ri that was just generated. These decisions
are made based on two types of features:
F1 captures which sets of fields are talked
about together; for example, we learn that {mean}
and {min, max} are preferred field sets for the
windSpeed record. By defining features on the en-
tire field set, we can capture any correlation structure
over the fields; in contrast, Liang et al. (2009) gen-
erates a sequence of fields in which a field can only
depend on the previous one.
F2 allows the field set to be chosen based on the
values of the fields, analogously to R4.
</bodyText>
<subsectionHeader confidence="0.998088">
3.3 Template Decisions
</subsectionHeader>
<bodyText confidence="0.997143470588235">
Template decisions perform surface realization. A
template is a sequence of elements, where each ele-
ment is either a word (e.g., around) or a field (e.g.,
[min]). Given the record ri and field set Fi that we
are generating from, the goal is to choose a template
Ti (Section 4.3.2 describes how we define the set
of possible templates). The features that govern the
choice of Ti are as follows:
W1 captures a priori preferences for generation
templates given field sets. There are two ways
to control this preference, BASE and COARSE.
1We map a numeric field value onto one of five categories
(very-low, low, medium, high, or very-high) based
on its value with respect to the mean and standard deviation of
values of that field in the training data.
BASE(Ti) denotes the template Ti itself, thus allow-
ing us to remember exactly which templates were
useful. To guard against overfitting, we also use
COARSE(Ti), which maps Ti to a coarsened version
of Ti, in which more words are replaced with their
associated fields (see Figure 5 for an example).
W2 captures a dependence on the values of fields
in the field set, and is analogous to R4 and F2. Fi-
nally, W3 contributes a language model probability,
to ensure smooth transitions between templates.
After Ti has been chosen, each field in the tem-
plate is replaced with a word given the correspond-
ing field value in the world state. In particular, a
word is chosen from the parameters learned in the
model of Liang et al. (2009). In the example in Fig-
ure 3, the [min] field in T2 has value 44, which is
rendered to the word 45 (rounding and other noisy
deviations are common in the WEATHERGOV do-
main).
</bodyText>
<sectionHeader confidence="0.739917" genericHeader="method">
4 Learning a Probabilistic Model
</sectionHeader>
<bodyText confidence="0.999953">
Having described all the features, we now present a
conditional probabilistic model over texts w given
world states s (Section 4.1). Section 4.2 describes
how to use the model for generation, and Section 4.3
describes how to learn the model.
</bodyText>
<subsectionHeader confidence="0.993598">
4.1 Model
</subsectionHeader>
<bodyText confidence="0.9931704">
Recall from Section 3 that the generation process
generates r1, F1, T1, r2, F2, T2, ... , STOP. To unify
notation, denote this sequence of decisions as d =
(d1,... , d|d|).
Our probability model is defined as follows:
</bodyText>
<equation confidence="0.978687666666667">
|d|
p(d  |s;θ) = H p(dj  |d&lt;j; θ), (1)
j=1
</equation>
<bodyText confidence="0.999975666666666">
where d&lt;j = (d1,... , dj_1) is the history of de-
cisions and θ are the model parameters (feature
weights). Note that the text w (the output) is a de-
terministic function of the decisions d. We use the
features described in Section 3 to define a log-linear
model for each decision:
</bodyText>
<equation confidence="0.821467666666667">
exp{φj(dj, d&lt;j, s)�θ}
p(dj  |d&lt;j,s; θ) = � exp{φj(d� j, d&lt;j, s)�θ}, (2)
d�jEDj
</equation>
<bodyText confidence="0.9991325">
where θ are all the parameters (feature weights), φj
is the feature vector for the j-th decision, and Dj is
</bodyText>
<page confidence="0.989255">
506
</page>
<bodyText confidence="0.999771285714286">
the domain of the j-th decision (either records, field
sets, or templates).
This chaining of log-linear models was used in
Ratnaparkhi (1998) for tagging and parsing, and in
Ratnaparkhi (2002) for surface realization. The abil-
ity to condition on arbitrary histories is a defining
property of these models.
</bodyText>
<subsectionHeader confidence="0.996956">
4.2 Using the Model for Generation
</subsectionHeader>
<bodyText confidence="0.999862285714286">
Suppose we have learned a model with parameters 0
(how to obtain 0 is discussed in Section 4.3). Given
a world state s, we would like to use our model to
generate an output text w via a decision sequence d.
In our experiments, we choose d by sequentially
choosing the best decision in a greedy fashion (until
the STOP record is generated):
</bodyText>
<equation confidence="0.635522">
dj = argmax p(d�j  |d&lt;j, s; 0). (3)
d,�
</equation>
<bodyText confidence="0.999901625">
Alternatively, instead of choosing the best decision
at each point, we can sample from the distribution:
dj — p(dj  |d&lt;j, s; 0), which provides more diverse
generated texts at the expense of a slight degradation
in quality.
Both greedy search and sampling are very effi-
cient. Another option is to try to find the Viterbi
decision sequence, i.e., the one with the maximum
joint probability: d = argmaxd, p(d&apos;  |s; 0). How-
ever, this computation is intractable due to features
depending arbitrarily on past decisions, making dy-
namic programming infeasible. We tried using beam
search to approximate this optimization, but we ac-
tually found that beam search performed worse than
greedy. Belz (2008) also found that greedy was more
effective than Viterbi for their model.
</bodyText>
<subsectionHeader confidence="0.998978">
4.3 Learning
</subsectionHeader>
<bodyText confidence="0.998516666666667">
Now we turn our attention to learning the parame-
ters 0 of our model. We are given a set of N sce-
narios I(s(i), w(i))}Ni=1 as training data. Note that
our model is defined over the decision sequence d
which contains information not present in w. In Sec-
tions 4.3.1 and 4.3.2, we show how we fill in this
missing information to obtain d(i) for each training
scenario i.
Assuming this missing information is filled, we
end up with a standard supervised learning problem,
which can be solved by maximize the (conditional)
likelihood of the training data:
</bodyText>
<equation confidence="0.98403475">
�
log p(d(i)
j  |d(i)
&lt;j; 0) � A||0||2, (4)
</equation>
<bodyText confidence="0.998802333333333">
where A &gt; 0 is a regularization parameter. The ob-
jective function in (4) is optimized using the stan-
dard L-BFGS algorithm (Liu and Nocedal, 1989).
</bodyText>
<subsectionHeader confidence="0.870946">
4.3.1 Latent Alignments
</subsectionHeader>
<bodyText confidence="0.999158">
As mentioned previously, our training data in-
cludes only the world state s and generated text w,
not the full sequence of decisions d needed for train-
ing. Intuitively, we know what was generated but not
why it was generated.
We use the model of Liang et al. (2009) to im-
pute the decisions d. They introduce a generative
model p(a, w|s), where the latent alignment a spec-
ifies (1) the sequence of records that were chosen,
(2) the sequence of fields that were chosen, and (3)
which words in the text were spanned by the chosen
records and fields. The model is learned in an unsu-
pervised manner using EM to produce a observing
only w and s.
An example of an alignment is given in the left
part of Figure 5. This information specifies the
record decisions and a set of fields for each record.
Because the induced alignments can be noisy, we
need to process them to obtain cleaner template de-
cisions. This is the subject of the next section.
</bodyText>
<subsectionHeader confidence="0.740269">
4.3.2 Template Extraction
</subsectionHeader>
<bodyText confidence="0.999855466666667">
Given an aligned training scenario (Figure 5), we
would like to extract two types of templates.
For each record, an aligned training scenario
specifies a sequence of fields and the text that
is spanned by each field. We create a template
by abstracting fields—that is, replacing the words
spanned by a field by the field itself. We call the
resulting template COARSE. The problem with us-
ing this template directly is that fields can be noisy
due to errors from the unsupervised model.
Therefore, we also create a BASE template which
only abstracts a subset of the fields. In particular,
we define a trigger pattern which specifies a simple
condition under which a field should be abstracted.
For WEATHERGOV, we only abstract fields that
</bodyText>
<figure confidence="0.98858095">
max � N fid(&apos;)fi
θERd � i=1 �
j=1
507
Records:
Fields:
Text:
skyCover,
temperature,
time=17-30
low around
min=44
45
mode=50-75
mostly cloudy,
with a
skyCover temperature
mean=49 COARSE ([mode]) (with a [time] [min] [mean])
. � BASE (most cloudy ,) (with a low around [min] .)
Aligned training scenario Templates extracted
</figure>
<figureCaption confidence="0.9980008">
Figure 5: An example of template extraction from an imperfectly aligned training scenario. Note that these alignments are noisy
(e.g., [mean] aligns to a period). Therefore, for each record (skyCover and temperature in this case), we extract two templates:
(1) a COARSE template, which takes the text spanned by the record and abstracts away all fields in the scenario ([mode], [time],
[min], and [mean] in the example); and (2) a BASE template, which only abstracts away fields whose spanned text matches a simple
pattern (e.g., numbers in WEATHERGOV, corresponding to [min] in the example).
</figureCaption>
<bodyText confidence="0.974873428571428">
span numbers; for SUMTIME, fields that span num-
bers and wind directions; and for ROBOCUP, fields
that span words starting with purple or pink.
For each record ri, we define Ti so that BASE(Ti)
and COARSE(Ti) are the corresponding two ex-
tracted templates. We restrict Fi to the set of ab-
stracted fields in the COARSE template
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999923">
We now present an empirical evaluation of our sys-
tem on our three domains—ROBOCUP, SUMTIME,
and WEATHERGOV.
</bodyText>
<subsectionHeader confidence="0.968866">
5.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999846315789474">
Automatic Evaluation To evaluate surface real-
ization (or, combined content selection and surface
realization), we measured the BLEU score (Papineni
et al., 2002) (the precision of 4-grams with a brevity
penalty) of the system-generated output with respect
to the human-generated output.
To evaluate macro content selection, we measured
the Fl score (the harmonic mean of precision and
recall) of the set of records chosen with respect to
the human-annotated set of records.
Human Evaluation We conducted a human eval-
uation using Amazon Mechanical Turk. For each
domain, we chose 100 scenarios randomly from the
test set. We ran each system under consideration on
each of these scenarios, and presented each resulting
output to 10 evaluators.2 Evaluators were given in-
structions to rank an output on the basis of English
fluency and semantic correctness on the following
scale:
</bodyText>
<footnote confidence="0.969653333333333">
2To minimize bias, we evaluated all the systems at once,
randomly shuffling the outputs of the systems. The evaluators
were not necessarily the same 10 evaluators.
</footnote>
<figure confidence="0.902597666666667">
English Fluency Semantic Correctness
Flawless Perfect
Near Perfect
Minor Errors
Major Errors
Completely Wrong
</figure>
<bodyText confidence="0.9545556">
Evaluators were also given additional domain-
specific information: (1) the background of the
domain (e.g., that SUMTIME reports are techni-
cal weather reports); (2) general properties of the
desired output (e.g., that SUMTIME texts should
mention every record whereas WEATHERGOV texts
need not); and (3) peculiarities of the text (e.g., the
suffix ly in SUMTIME should exist as a separate to-
ken from its stem, or that pink goalie and pink1 have
the same meaning in ROBOCUP).
</bodyText>
<subsectionHeader confidence="0.995186">
5.2 Systems
</subsectionHeader>
<bodyText confidence="0.9982565">
We evaluated the following systems on our three do-
mains:
</bodyText>
<listItem confidence="0.982143214285714">
• HUMAN is the human-generated output.
• OURSYSTEM uses all the features in Figure 4
and is trained according to Section 4.3.
• BASELINE is OURSYSTEM using a subset of
the features (those marked with † in Fig-
ure 4). In contrast to OURSYSTEM, the in-
cluded features only depend on a local con-
text of decisions in a manner similar to
the generative model of Liang et al. (2009)
and the pCRU-greedy system of Belz (2008).
BASELINE also excludes features that depend
on values of the world state.
• The existing state-of-the-art domain-specific
system for each domain.
</listItem>
<subsectionHeader confidence="0.976798">
5.3 ROBOCUP Results
</subsectionHeader>
<bodyText confidence="0.999088">
Following the evaluation methodology of Chen and
Mooney (2008), we trained our system on three
</bodyText>
<figure confidence="0.9943251">
Score
5
4
3
2
1
Good
Non-native
Disfluent
Gibberish
</figure>
<page confidence="0.986654">
508
</page>
<table confidence="0.999396666666667">
System F1 BLEU* English Semantic
Fluency Correctness
BASELINE 78.7 24.8 4.28 ± 0.78 4.15 ± 1.14
OURSYSTEM 79.9 28.8 4.34 ± 0.69 4.17 ± 1.21
WASPER-GEN 72.0 28.7 4.43 ± 0.76 4.27 ± 1.15
HUMAN — — 4.43 ± 0.69 4.30 ± 1.07
</table>
<tableCaption confidence="0.94410975">
Table 1: ROBOCUP results. WASPER-GEN is described in
Chen and Mooney (2008). The BLEU is reported on systems
that use fixed human-annotated records (in other words, we
evaluate surface realization given perfect content selection).
</tableCaption>
<figureCaption confidence="0.858404">
Figure 6: Outputs of systems on an example ROBOCUP sce-
nario. There are some minor differences between the outputs.
Recall that OURSYSTEM differs from BASELINE mostly in
</figureCaption>
<bodyText confidence="0.981951846153846">
the addition of feature W2, which captures dependencies be-
tween field values (e.g., purple10) and the template chosen
(e.g., [arg1] passes to [arg2]). This allows us to capture value-
dependent preferences for different realizations (e.g., passes to
over kicks to). Also, HUMAN uses passes back to, but this word
choice requires knowledge of passing records in previous sce-
narios, which none of the systems have access to. It would nat-
ural, however, to add features that would capture these longer-
range dependencies in our framework.
Robocup games and tested on the fourth, averaging
over the four train/test splits. We report the average
test accuracy weighted by the number of scenarios
in a game. First, we evaluated macro content selec-
tion. Table 1 shows that OURSYSTEM significantly
outperforms BASELINE and WASPER-GEN on F1.
To compare with Chen and Mooney (2008) on
surface realization, we fixed each system’s record
decisions to the ones given by the annotated data
and enforced that all the fields of that record are
chosen. Table 1 shows that OURSYSTEM sig-
nificantly outperforms BASELINE and is compara-
ble to WASPER-GEN on BLEU. On human eval-
uation, OURSYSTEM outperforms BASELINE, but
WASPER-GEN outperforms OURSYSTEM. See
Figure 6 for example outputs from the various sys-
tems.
</bodyText>
<table confidence="0.9992555">
BLEU English Semantic
Fluency Correctness
BASELINE 32.9 4.23 ± 0.71 4.26 ± 0.85
OURSYSTEM 55.1 4.25 ± 0.69 4.27 ± 0.82
OURSYSTEM-CUSTOM 62.3 4.12 ± 0.78 4.33 ± 0.91
pCRU-greedy 63.6 4.18 ± 0.71 4.49 ± 0.73
SUMTIME-Hybrid 52.7 — —
HUMAN — 4.09 ± 0.83 4.37 ± 0.87
</table>
<tableCaption confidence="0.933263">
Table 2: SUMTIME results. The SUMTIME-Hybrid system
is described in (Reiter et al., 2005); pCRU-greedy, in (Belz,
2008).
</tableCaption>
<subsectionHeader confidence="0.956851">
5.4 SUMTIME Results
</subsectionHeader>
<bodyText confidence="0.9998454">
The SUMTIME task only requires micro content se-
lection and surface realization because the sequence
of records to be generated is fixed; only these as-
pects are evaluated. Following the methodology of
Belz (2008), we used five-fold cross validation.
We found that using the unsupervised model of
Liang et al. (2009) to automatically produce aligned
training scenarios (Section 4.3.1) was less effec-
tive than it was in the other two domains due to
two factors: (i) there are fewer training examples
in SUMTIME and unsupervised learning typically
works better with a large amount of data; and (ii)
the alignment model does not exploit the temporal
structure in the SUMTIME world state. Therefore,
we used a small set of simple regular expressions to
produce aligned training scenarios.
Table 2 shows that OURSYSTEM signif-
icantly outperforms BASELINE as well as
SUMTIME-Hybrid, a hand-crafted system, on
BLEU. Note that OURSYSTEM is domain-
independent and has not been specifically tuned
to SUMTIME. However, OURSYSTEM is outper-
formed by the state-of-the-art statistical system
pCRU-greedy.
Custom Features One of the advantages of our
feature-based approach is that it is straightforward to
incorporate domain-specific features to capture spe-
cific properties of a domain. To this end, we define
the following set of feature templates in place of our
generic feature templates from Figure 4:
</bodyText>
<listItem confidence="0.9750216">
• F1&apos;: Value of time
• F2&apos;: Existence of gusts/wind direction/wind
speeds
• W1&apos;: Change in wind direction (clockwise,
counterclockwise, or none)
</listItem>
<figure confidence="0.995378555555555">
pass,
passes back to
arg2=purple9
purple9
arg1=purple10
purple10
Records:
HUMAN Fields:
Text:
pass,
OURSYSTEM
arg1=purple10
purple10
passes to
arg2=purple9
purple9
Records:
Fields:
Text:
WASPER-GEN cordsText: purple10 passes to purple9
Records:
Fields:
Text:
pass,
arg2=purple9
purple9
kicks to
arg1=purple10
purple10
BASELINE
509
gust-min=30
gusts 30
gradually decreasing
time=12am
by late evening
min=18
18
max=22
22
min=10
10
max=14
14
-
-
windDir1
windDir2
HUMAN
Records:
Fields:
Text:
dir=nne
nne
Records:
Fields:
Text:
windDir2
windDir1
BASELINE
dir=nne
nne
min=18
18
min=10
10
max=22
22
max=14
14
increasing
-
-
Records:
Fields:
Text:
windDir1
windDir2
OURSYSTEM-CUSTOM
dir=nne
nne
min=18
18
min=10
10
max=14
14
gust-min=30
gusts 30
max=22
22
-
-
gust-min=30
gusts 30
gradually decreasing
time=12am
by late evening
pCRU-greedy cordsText: nne 18 - 22 gusts 30 easing 10 - 14 by late evening
</figure>
<figureCaption confidence="0.914877857142857">
Figure 7: Outputs of systems on an example SUMTIME scenario. Two notable differences between OURSYSTEM-CUSTOM and
BASELINE arise due to OURSYSTEM-CUSTOM’s value-dependent features. For example, OURSYSTEM-CUSTOM can choose
whether to include the time field (windDir2) or not (windDir1), depending on the value of the time (F1&apos;), thereby improving content
selection. OURSYSTEM-CUSTOM also improves surface realization, choosing gradually decreasing over BASELINE’s increasing.
Interestingly, this improvement comes from the joint effort of two features: W2&apos; prefers decreasing over increasing in this case,
and W5&apos; adds the modifier gradually. An important strength of log-linear models is the ability to combine soft preferences from
many features.
</figureCaption>
<listItem confidence="0.88997">
• W2&apos;: Change in wind speed F1 BLEU* English Semantic
Fluency Correctness
• W3&apos;: Change in wind direction and speed BASELINE 22.1 22.2 4.07 ± 0.59 3.41 ± 1.16
OURSYSTEM 65.4 51.5 4.12 ± 0.74 4.22 ± 0.89
• W4&apos;: Existence of gust min and/or max
HUMAN — — 4.14 ± 0.71 3.85 ± 0.99
• W5&apos;: Time elapsed since last record Table 3: WEATHERGOV results. The BLEU score is on joint
• W6&apos;: Whether wind is a cardinal direction (N, content selection and surface realization and is modified to not
</listItem>
<bodyText confidence="0.971510692307692">
The E, S, W) penalize numeric deviations of at most 5.
resulting system, which we call
OURSYSTEM-CUSTOM, obtains a BLEU score
which is comparable to pCRU-greedy.
An important aspect of our system that it is flexi-
ble and quick to deploy. According to Belz (2008),
SUMTIME-Hybrid took twelve person-months to
build, while pCRU-greedy took one month. Having
developed OURSYSTEM in a domain-independent
way, we only needed to do simple reformatting upon
receiving the SUMTIME data. Furthermore, it took
only a few days to develop the custom features
above to create OURSYSTEM-CUSTOM, which has
BLEU performance comparable to the state-of-the-
art pCRU-greedy system.
We also conducted human evaluations on the four
systems shown in Table 2. Note that this evalua-
tion is rather difficult for Mechanical Turkers since
SUMTIME texts are rather technical compared to
those in other domains. Interestingly, all systems
outperform HUMAN on English fluency; this result
corroborates the findings of Belz (2008). On se-
mantic correctness, all systems perform comparably
to HUMAN, except pCRU-greedy, which performs
slightly better. See Figure 7 for a comparison of the
outputs generated by the various systems.
</bodyText>
<subsectionHeader confidence="0.800994">
5.5 WEATHERGOV Results
</subsectionHeader>
<bodyText confidence="0.999959636363636">
We evaluate the WEATHERGOV corpus on the joint
task of content selection and surface realization.
We split our corpus into 25,000 scenarios for train-
ing, 1,000 for development, and 3,528 for testing.
In WEATHERGOV, numeric field values are often
rounded or noisily perturbed, so it is difficult to gen-
erate precisely matching numbers. Therefore, we
used a modified BLEU score where numbers dif-
fering by at most five are treated as equal. Fur-
thermore, WEATHERGOV is evaluated on the joint
content selection and surface realization task, un-
like ROBOCUP, where content selection and surface
realization were treated separately, and SUMTIME,
where content selection was not applicable.
Table 3 shows the results. We see that
OURSYSTEM substantially outperforms BASELINE,
especially on BLEU score and semantic correctness.
This difference shows that taking non-local context
into account is very important in this domain. This
result is not surprising, since WEATHERGOV is the
most complicated of the three domains, and this
complexity is exactly where non-locality is neces-
</bodyText>
<page confidence="0.964138">
510
</page>
<figure confidence="0.999624492063492">
HUMAN
BASELINE
Records:
Fields:
Text:
Records:
Fields:
Text:
cover=50-75
mostly cloudy ,
a chance of showers ,
skyCover,
rainChance2
with a
none
,
time=5pm-6am
low
with gusts as high as
temperature,
around
gust,
min=59
57
max=21
20
.
mph .
windDir,
mode=sse
south
chance of precipitation is
wind between
precipPotential,
min=7
5
max=10
10
windSpeed,
and
max=15
10
% .
mph .
skyCover,
mostly cloudy,
Records:
Fields:
Text:
temperature,
windDir,
with a low around
min=59
59
. south wind between
windSpeed,
and
max=15
15
min=7
7
mph.
OURSYSTEM
</figure>
<figureCaption confidence="0.9721626">
Figure 8: Outputs of systems on an example WEATHERGOV scenario. Most of the gains of OURSYSTEM over BASELINE come
from improved content selection. For example, BASELINE chooses rainChance because it happens to be the most common first
record type in the training data. However, since OURSYSTEM has features that depend on the value of rainChance (noChance
in this case), it has learned to disprefer talking about rain when there is no rain. Also, OURSYSTEM has additional features on the
entire history of chosen records, which enables it to choose a better sequence of records.
</figureCaption>
<bodyText confidence="0.9952126">
sary. Interestingly, OURSYSTEM even outperforms
HUMAN on semantic correctness, perhaps due to
generating more straightforward renderings of the
world state. Figure 8 describes example outputs for
each system.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999919">
There has been a fair amount of work both on con-
tent selection and surface realization. In content se-
lection, Barzilay and Lee (2004) use an approach
based on local classification with edge-wise scores
between local decisions. Our model, on the other
hand, can capture higher-order constraints to enforce
global coherence.
Liang et al. (2009) introduces a generative model
of the text given the world state, and in some ways is
similar in spirit to our model. Although that model
is capable of generation in principle, it was de-
signed for unsupervised induction of hidden align-
ments (which is exactly what we use it for). Even
if combined with a language model, generated text
was much worse than our baseline.
The prominent approach for surface realization
is rendering the text from a grammar. Wong and
Mooney (2007) and Chen and Mooney (2008) use
synchronous grammars that map a logical form, rep-
resented as a tree, into a parse of the text. Soricut
and Marcu (2006) uses tree structures called WIDL-
expressions (the acronym corresponds to four opera-
tions akin to the rewrite rules of a grammar) to repre-
sent the realization process, and, like our approach,
operates in a log-linear framework. Belz (2008) and
Belz and Kow (2009) also perform surface realiza-
tion from a PCFG-like grammar. Lu et al. (2009)
uses a conditional random field model over trees.
Other authors have performed surface realization us-
ing various grammar formalisms, for instance CCG
(White et al., 2007), HPSG (Nakanishi et al., 2005),
and LFG (Cahill and van Genabith, 2006).
In each of the above cases, the decomposable
structure of the tree/grammar enables tractability.
However, we saw that it was important to include
features that captured long-range dependencies. Our
model is also similar in spirit to Ratnaparkhi (2002)
in the use of non-local features, but we operate at
three levels of hierarchy to include both content se-
lection and surface realization.
One issue that arises with long-range dependen-
cies is the lack of efficient algorithms for finding the
optimal text. Koller and Striegnitz (2002) perform
surface realization of a flat semantics, which is NP-
hard, so they recast the problem as non-projective
dependency parsing. Ratnaparkhi (2002) uses beam
search to find an approximate solution. We found
that a greedy approach obtained better results than
beam search; Belz (2008) found greedy approaches
to be effective as well.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999974888888889">
We have developed a simple yet powerful generation
system that combines both content selection and sur-
face realization in a domain independent way. De-
spite our approach being domain-independent, we
were able to obtain performance comparable to the
state-of-the-art across three domains. Additionally,
the feature-based design of our approach makes it
easy to incorporate domain-specific knowledge to
increase performance even further.
</bodyText>
<page confidence="0.996066">
511
</page>
<sectionHeader confidence="0.995884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999659511627907">
R. Barzilay and L. Lee. 2004. Catching the drift: Prob-
abilistic content models, with applications to genera-
tion and summarization. In Human Language Tech-
nology and North American Association for Computa-
tional Linguistics (HLT/NAACL).
A. Belz and E. Kow. 2009. System building cost vs.
output quality in data-to-text generation. In European
Workshop on Natural Language Generation, pages
16–24.
A. Belz. 2008. Automatic generation of weather forecast
texts using comprehensive probabilistic generation-
space models. Natural Language Engineering,
14(4):1–26.
Aoife Cahill and Josef van Genabith. 2006. Robust pcfg-
based generation using automatically acquired LFG
approximations. In Association for Computational
Linguistics (ACL), pages 1033–1040, Morristown, NJ,
USA. Association for Computational Linguistics.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128–135.
R. Dale, S. Geldof, and J. Prost. 2003. Coral: using natu-
ral language generation for navigational assistance. In
Australasian computer science conference, pages 35–
44.
M. E. Foster and M. White. 2004. Techniques for text
planning with XSLT. In Workshop on NLP and XML:
RDF/RDFS and OWL in Language Technology, pages
1–8.
N. Green. 2006. Generation of biomedical arguments for
lay readers. In International Natural Language Gen-
eration Conference, pages 114–121.
A. Koller and K. Striegnitz. 2002. Generation as de-
pendency parsing. In Association for Computational
Linguistics (ACL), pages 17–24.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503–528.
W. Lu, H. T. Ng, and W. S. Lee. 2009. Natural lan-
guage generation with tree conditional random fields.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 400–409.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Parsing ’05: Pro-
ceedings of the Ninth International Workshop on Pars-
ing Technology, pages 93–102, Morristown, NJ, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Association for Computational Linguis-
tics (ACL).
A. Ratnaparkhi. 1998. Maximum entropy models for nat-
ural language ambiguity resolution. Ph.D. thesis, Uni-
versity of Pennsylvania.
A. Ratnaparkhi. 2002. Trainable approaches to surface
natural language generation and their application to
conversational dialog systems. Computer, Speech &amp;
Language, 16:435–455.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005.
Choosing words in computer-generated weather fore-
casts. Artificial Intelligence, 167:137–169.
R. Soricut and D. Marcu. 2006. Stochastic language
generation using WIDL-expressions and its applica-
tion in machine translation and summarization. In As-
sociation for Computational Linguistics (ACL), pages
1105–1112.
R. Turner, Y. Sripada, and E. Reiter. 2009. Gener-
ating approximate geographic descriptions. In Eu-
ropean Workshop on Natural Language Generation,
pages 42–49.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realization
with CCG. In In Proceedings of the Workshop on Us-
ing Corpora for NLG: Language Generation and Ma-
chine Translation (UCNLG+MT).
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960–967.
</reference>
<page confidence="0.997044">
512
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.737371">
<title confidence="0.99985">A Simple Domain-Independent Probabilistic Approach to Generation</title>
<author confidence="0.96045">Gabor</author>
<affiliation confidence="0.990843">UC</affiliation>
<address confidence="0.941548">Berkeley, CA</address>
<email confidence="0.99949">gangeli@berkeley.edu</email>
<author confidence="0.97756">Percy</author>
<affiliation confidence="0.99795">UC</affiliation>
<address confidence="0.935856">Berkeley, CA</address>
<email confidence="0.99896">pliang@cs.berkeley.edu</email>
<author confidence="0.965591">Dan</author>
<affiliation confidence="0.999224">UC</affiliation>
<address confidence="0.944655">Berkeley, CA</address>
<email confidence="0.999243">klein@cs.berkeley.edu</email>
<abstract confidence="0.998207928571429">We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework. In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="1308" citStr="Barzilay and Lee (2004)" startWordPosition="186" endWordPosition="189"> weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to a</context>
<context position="10918" citStr="Barzilay and Lee, 2004" startWordPosition="1707" endWordPosition="1710"> templates (see Figure 4), which are represented as functions of the current decision and past decisions. The feature weights are learned from training data (see Section 4.3). We chose a set of generic domain-independent feature templates, described in the sections below. These features can, in general, depend on the current decision and all previous decisions. For example, referring to Figure 4, R2 features on the record choice depend on all the previous record decisions, and R5 features depend on the most recent template decision. This is in contrast with most systems for content selection (Barzilay and Lee, 2004) and surface realization (Belz, 2008), where decisions must decompose locally according to either a graph or tree. The ability to use global features in this manner is 504 World state Decisions skyCover1: skyCover(time=5pm-6am,mode=50-75) temperature1: temperature(time=5pm-6am,min=44,mean=49,max=60) ... Record r1 = skyCover1 r2 = temperature1 r3 = STOP Field set F1 = {mode} F2 = {time, min} Template T1 = (mostly cloudy ,) T2 = (with a low around [min] .) Text mostly cloudy , with a low around 45 . Specific active (nonzero) features for highlighted decisions (R1) Qr2.t = temperature and (r1.t, </context>
<context position="36207" citStr="Barzilay and Lee (2004)" startWordPosition="5908" endWordPosition="5911">epend on the value of rainChance (noChance in this case), it has learned to disprefer talking about rain when there is no rain. Also, OURSYSTEM has additional features on the entire history of chosen records, which enables it to choose a better sequence of records. sary. Interestingly, OURSYSTEM even outperforms HUMAN on semantic correctness, perhaps due to generating more straightforward renderings of the world state. Figure 8 describes example outputs for each system. 6 Related Work There has been a fair amount of work both on content selection and surface realization. In content selection, Barzilay and Lee (2004) use an approach based on local classification with edge-wise scores between local decisions. Our model, on the other hand, can capture higher-order constraints to enforce global coherence. Liang et al. (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>R. Barzilay and L. Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Kow</author>
</authors>
<title>System building cost vs. output quality in data-to-text generation.</title>
<date>2009</date>
<booktitle>In European Workshop on Natural Language Generation,</booktitle>
<pages>16--24</pages>
<contexts>
<context position="37305" citStr="Belz and Kow (2009)" startWordPosition="6092" endWordPosition="6095"> it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local fea</context>
</contexts>
<marker>Belz, Kow, 2009</marker>
<rawString>A. Belz and E. Kow. 2009. System building cost vs. output quality in data-to-text generation. In European Workshop on Natural Language Generation, pages 16–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generationspace models.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="7423" citStr="Belz (2008)" startWordPosition="1128" endWordPosition="1129"> Figure 1(a) for an example of a scenario. Content selection in this domain is choosing the single record to talk about, and surface realization is talking about it. 503 2.2 SUMTIME: Technical Weather Forecasts Reiter et al. (2005) developed a generation system and created the SUMTIME-METEO corpus, which consists of marine wind weather forecasts used by offshore oil rigs, generated by the output of weather simulators. More specifically, these forecasts describe various aspects of the wind at different times during the forecast period. We used the version of the SUMTIME-METEO corpus created by Belz (2008). The dataset consists of 469 scenarios, each containing on average s = 2.6 records and 16.2 words. See Figure 1(c) for an example of a scenario. This task requires no content selection, only surface realization: The records are given in some fixed order and the task is to generate from each of these records in turn; of course, due to contextual dependencies, these records cannot be generated independently. 2.3 WEATHERGOV: Common Weather Forecasts In the WEATHERGOV domain, the world state contains detailed information about a local weather forecast (e.g., temperature, rain chance, etc.). The t</context>
<context position="10955" citStr="Belz, 2008" startWordPosition="1714" endWordPosition="1715">as functions of the current decision and past decisions. The feature weights are learned from training data (see Section 4.3). We chose a set of generic domain-independent feature templates, described in the sections below. These features can, in general, depend on the current decision and all previous decisions. For example, referring to Figure 4, R2 features on the record choice depend on all the previous record decisions, and R5 features depend on the most recent template decision. This is in contrast with most systems for content selection (Barzilay and Lee, 2004) and surface realization (Belz, 2008), where decisions must decompose locally according to either a graph or tree. The ability to use global features in this manner is 504 World state Decisions skyCover1: skyCover(time=5pm-6am,mode=50-75) temperature1: temperature(time=5pm-6am,min=44,mean=49,max=60) ... Record r1 = skyCover1 r2 = temperature1 r3 = STOP Field set F1 = {mode} F2 = {time, min} Template T1 = (mostly cloudy ,) T2 = (with a low around [min] .) Text mostly cloudy , with a low around 45 . Specific active (nonzero) features for highlighted decisions (R1) Qr2.t = temperature and (r1.t, r0.t) = (skyCover, START)] Qr2.t = te</context>
<context position="20104" citStr="Belz (2008)" startWordPosition="3309" endWordPosition="3310"> from the distribution: dj — p(dj |d&lt;j, s; 0), which provides more diverse generated texts at the expense of a slight degradation in quality. Both greedy search and sampling are very efficient. Another option is to try to find the Viterbi decision sequence, i.e., the one with the maximum joint probability: d = argmaxd, p(d&apos; |s; 0). However, this computation is intractable due to features depending arbitrarily on past decisions, making dynamic programming infeasible. We tried using beam search to approximate this optimization, but we actually found that beam search performed worse than greedy. Belz (2008) also found that greedy was more effective than Viterbi for their model. 4.3 Learning Now we turn our attention to learning the parameters 0 of our model. We are given a set of N scenarios I(s(i), w(i))}Ni=1 as training data. Note that our model is defined over the decision sequence d which contains information not present in w. In Sections 4.3.1 and 4.3.2, we show how we fill in this missing information to obtain d(i) for each training scenario i. Assuming this missing information is filled, we end up with a standard supervised learning problem, which can be solved by maximize the (conditiona</context>
<context position="26149" citStr="Belz (2008)" startWordPosition="4327" endWordPosition="4328">y in SUMTIME should exist as a separate token from its stem, or that pink goalie and pink1 have the same meaning in ROBOCUP). 5.2 Systems We evaluated the following systems on our three domains: • HUMAN is the human-generated output. • OURSYSTEM uses all the features in Figure 4 and is trained according to Section 4.3. • BASELINE is OURSYSTEM using a subset of the features (those marked with † in Figure 4). In contrast to OURSYSTEM, the included features only depend on a local context of decisions in a manner similar to the generative model of Liang et al. (2009) and the pCRU-greedy system of Belz (2008). BASELINE also excludes features that depend on values of the world state. • The existing state-of-the-art domain-specific system for each domain. 5.3 ROBOCUP Results Following the evaluation methodology of Chen and Mooney (2008), we trained our system on three Score 5 4 3 2 1 Good Non-native Disfluent Gibberish 508 System F1 BLEU* English Semantic Fluency Correctness BASELINE 78.7 24.8 4.28 ± 0.78 4.15 ± 1.14 OURSYSTEM 79.9 28.8 4.34 ± 0.69 4.17 ± 1.21 WASPER-GEN 72.0 28.7 4.43 ± 0.76 4.27 ± 1.15 HUMAN — — 4.43 ± 0.69 4.30 ± 1.07 Table 1: ROBOCUP results. WASPER-GEN is described in Chen and </context>
<context position="28755" citStr="Belz, 2008" startWordPosition="4754" endWordPosition="4755">TEM significantly outperforms BASELINE and is comparable to WASPER-GEN on BLEU. On human evaluation, OURSYSTEM outperforms BASELINE, but WASPER-GEN outperforms OURSYSTEM. See Figure 6 for example outputs from the various systems. BLEU English Semantic Fluency Correctness BASELINE 32.9 4.23 ± 0.71 4.26 ± 0.85 OURSYSTEM 55.1 4.25 ± 0.69 4.27 ± 0.82 OURSYSTEM-CUSTOM 62.3 4.12 ± 0.78 4.33 ± 0.91 pCRU-greedy 63.6 4.18 ± 0.71 4.49 ± 0.73 SUMTIME-Hybrid 52.7 — — HUMAN — 4.09 ± 0.83 4.37 ± 0.87 Table 2: SUMTIME results. The SUMTIME-Hybrid system is described in (Reiter et al., 2005); pCRU-greedy, in (Belz, 2008). 5.4 SUMTIME Results The SUMTIME task only requires micro content selection and surface realization because the sequence of records to be generated is fixed; only these aspects are evaluated. Following the methodology of Belz (2008), we used five-fold cross validation. We found that using the unsupervised model of Liang et al. (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically works better with a large amount o</context>
<context position="32699" citStr="Belz (2008)" startWordPosition="5366" endWordPosition="5367"> 4.07 ± 0.59 3.41 ± 1.16 OURSYSTEM 65.4 51.5 4.12 ± 0.74 4.22 ± 0.89 • W4&apos;: Existence of gust min and/or max HUMAN — — 4.14 ± 0.71 3.85 ± 0.99 • W5&apos;: Time elapsed since last record Table 3: WEATHERGOV results. The BLEU score is on joint • W6&apos;: Whether wind is a cardinal direction (N, content selection and surface realization and is modified to not The E, S, W) penalize numeric deviations of at most 5. resulting system, which we call OURSYSTEM-CUSTOM, obtains a BLEU score which is comparable to pCRU-greedy. An important aspect of our system that it is flexible and quick to deploy. According to Belz (2008), SUMTIME-Hybrid took twelve person-months to build, while pCRU-greedy took one month. Having developed OURSYSTEM in a domain-independent way, we only needed to do simple reformatting upon receiving the SUMTIME data. Furthermore, it took only a few days to develop the custom features above to create OURSYSTEM-CUSTOM, which has BLEU performance comparable to the state-of-theart pCRU-greedy system. We also conducted human evaluations on the four systems shown in Table 2. Note that this evaluation is rather difficult for Mechanical Turkers since SUMTIME texts are rather technical compared to thos</context>
<context position="37281" citStr="Belz (2008)" startWordPosition="6089" endWordPosition="6090">ctly what we use it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in </context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>A. Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generationspace models. Natural Language Engineering, 14(4):1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Robust pcfgbased generation using automatically acquired LFG approximations.</title>
<date>2006</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1033--1040</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>Aoife Cahill and Josef van Genabith. 2006. Robust pcfgbased generation using automatically acquired LFG approximations. In Association for Computational Linguistics (ACL), pages 1033–1040, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>R J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>128--135</pages>
<contexts>
<context position="1457" citStr="Chen and Mooney (2008)" startWordPosition="211" endWordPosition="214">duction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We break up the full generation process </context>
<context position="3062" citStr="Chen and Mooney, 2008" startWordPosition="466" endWordPosition="469">specifically, our model is defined in terms of three types of decisions. The first type chooses records from the database (macro content selection)—for example, wind speed, in the case of generating weather forecasts. The second type chooses a subset of fields from a record (micro content selection)—e.g., the minimum and maximum temperature. The third type chooses a suitable template to render the content (surface realization)— e.g., winds between [min] and [max] mph; templates are automatically extracted from training data. We tested our approach in three domains: ROBOCUP, for sportscasting (Chen and Mooney, 2008); SUMTIME, for technical weather forecast generation (Reiter et al., 2005); and WEATHERGOV, for common weather forecast generation (Liang et al., 2009). We performed both automatic (BLEU) and human evaluation. On WEATHERGOV, we 502 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics w: purple3 made a bad pass that was picked off by pink9 (a) ROBOCUP s: temperature(time=5pm-6am,min=48,mean=53,max=61) windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10)</context>
<context position="5996" citStr="Chen and Mooney (2008)" startWordPosition="894" endWordPosition="897">ype t E T is associated with a set of fields FIELDS(t). Each record r E s has a record type r.t E T and afield value r.v[f] for each field f E FIELDS(t). The text, denoted w, is represented by a sequence of tokenized words. We use the term scenario to denote a world state s paired with a text w. In this paper, we conducted experiments on three domains, which are detailed in the following subsections. Example scenarios for each domain are detailed in Figure 1. 2.1 ROBOCUP: Sportscasting A world state in the ROBOCUP domain is a set of event records (meaning representations in the terminology of Chen and Mooney (2008)) generated by a robot soccer simulator. For example, the record pass(arg1=pink1,arg2=pink5) denotes a passing event; records of this type (pass) have two fields: arg1 (the agent) and arg2 (the recipient). As the game progresses, human commentators talk about some of the events in the game, e.g., purple3 made a bad pass that was picked off by pink9. We used the dataset created by Chen and Mooney (2008), which contains 1919 scenarios from the 2001–2004 Robocup finals. Each scenario consists of a single sentence representing a fragment of a commentary on the game, paired with a set of candidate </context>
<context position="26379" citStr="Chen and Mooney (2008)" startWordPosition="4358" endWordPosition="4361">erated output. • OURSYSTEM uses all the features in Figure 4 and is trained according to Section 4.3. • BASELINE is OURSYSTEM using a subset of the features (those marked with † in Figure 4). In contrast to OURSYSTEM, the included features only depend on a local context of decisions in a manner similar to the generative model of Liang et al. (2009) and the pCRU-greedy system of Belz (2008). BASELINE also excludes features that depend on values of the world state. • The existing state-of-the-art domain-specific system for each domain. 5.3 ROBOCUP Results Following the evaluation methodology of Chen and Mooney (2008), we trained our system on three Score 5 4 3 2 1 Good Non-native Disfluent Gibberish 508 System F1 BLEU* English Semantic Fluency Correctness BASELINE 78.7 24.8 4.28 ± 0.78 4.15 ± 1.14 OURSYSTEM 79.9 28.8 4.34 ± 0.69 4.17 ± 1.21 WASPER-GEN 72.0 28.7 4.43 ± 0.76 4.27 ± 1.15 HUMAN — — 4.43 ± 0.69 4.30 ± 1.07 Table 1: ROBOCUP results. WASPER-GEN is described in Chen and Mooney (2008). The BLEU is reported on systems that use fixed human-annotated records (in other words, we evaluate surface realization given perfect content selection). Figure 6: Outputs of systems on an example ROBOCUP scenario. </context>
<context position="27954" citStr="Chen and Mooney (2008)" startWordPosition="4618" endWordPosition="4621">to over kicks to). Also, HUMAN uses passes back to, but this word choice requires knowledge of passing records in previous scenarios, which none of the systems have access to. It would natural, however, to add features that would capture these longerrange dependencies in our framework. Robocup games and tested on the fourth, averaging over the four train/test splits. We report the average test accuracy weighted by the number of scenarios in a game. First, we evaluated macro content selection. Table 1 shows that OURSYSTEM significantly outperforms BASELINE and WASPER-GEN on F1. To compare with Chen and Mooney (2008) on surface realization, we fixed each system’s record decisions to the ones given by the annotated data and enforced that all the fields of that record are chosen. Table 1 shows that OURSYSTEM significantly outperforms BASELINE and is comparable to WASPER-GEN on BLEU. On human evaluation, OURSYSTEM outperforms BASELINE, but WASPER-GEN outperforms OURSYSTEM. See Figure 6 for example outputs from the various systems. BLEU English Semantic Fluency Correctness BASELINE 32.9 4.23 ± 0.71 4.26 ± 0.85 OURSYSTEM 55.1 4.25 ± 0.69 4.27 ± 0.82 OURSYSTEM-CUSTOM 62.3 4.12 ± 0.78 4.33 ± 0.91 pCRU-greedy 63.</context>
<context position="36919" citStr="Chen and Mooney (2008)" startWordPosition="6026" endWordPosition="6029">ns. Our model, on the other hand, can capture higher-order constraints to enforce global coherence. Liang et al. (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for </context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>D. L. Chen and R. J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In International Conference on Machine Learning (ICML), pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>S Geldof</author>
<author>J Prost</author>
</authors>
<title>Coral: using natural language generation for navigational assistance.</title>
<date>2003</date>
<booktitle>In Australasian computer science conference,</booktitle>
<pages>35--44</pages>
<contexts>
<context position="1101" citStr="Dale et al. (2003)" startWordPosition="151" endWordPosition="154">a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in whi</context>
</contexts>
<marker>Dale, Geldof, Prost, 2003</marker>
<rawString>R. Dale, S. Geldof, and J. Prost. 2003. Coral: using natural language generation for navigational assistance. In Australasian computer science conference, pages 35– 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>M White</author>
</authors>
<title>Techniques for text planning with XSLT.</title>
<date>2004</date>
<booktitle>In Workshop on NLP and XML: RDF/RDFS and OWL in Language Technology,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1333" citStr="Foster and White (2004)" startWordPosition="190" endWordPosition="193">ning results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the c</context>
</contexts>
<marker>Foster, White, 2004</marker>
<rawString>M. E. Foster and M. White. 2004. Techniques for text planning with XSLT. In Workshop on NLP and XML: RDF/RDFS and OWL in Language Technology, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Green</author>
</authors>
<title>Generation of biomedical arguments for lay readers.</title>
<date>2006</date>
<booktitle>In International Natural Language Generation Conference,</booktitle>
<pages>114--121</pages>
<contexts>
<context position="1115" citStr="Green (2006)" startWordPosition="155" endWordPosition="156">decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only</context>
</contexts>
<marker>Green, 2006</marker>
<rawString>N. Green. 2006. Generation of biomedical arguments for lay readers. In International Natural Language Generation Conference, pages 114–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>K Striegnitz</author>
</authors>
<title>Generation as dependency parsing.</title>
<date>2002</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>17--24</pages>
<contexts>
<context position="38160" citStr="Koller and Striegnitz (2002)" startWordPosition="6227" endWordPosition="6230">(White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnitz (2002) perform surface realization of a flat semantics, which is NPhard, so they recast the problem as non-projective dependency parsing. Ratnaparkhi (2002) uses beam search to find an approximate solution. We found that a greedy approach obtained better results than beam search; Belz (2008) found greedy approaches to be effective as well. 7 Conclusion We have developed a simple yet powerful generation system that combines both content selection and surface realization in a domain independent way. Despite our approach being domain-independent, we were able to obtain performance comparable to the sta</context>
</contexts>
<marker>Koller, Striegnitz, 2002</marker>
<rawString>A. Koller and K. Striegnitz. 2002. Generation as dependency parsing. In Association for Computational Linguistics (ACL), pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</booktitle>
<contexts>
<context position="1903" citStr="Liang et al. (2009)" startWordPosition="285" endWordPosition="288"> Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We break up the full generation process into a sequence of local decisions, training a log-linear classifier for each type of decision. We use a simple but expressive set of domain-independent features, where each decision is allowed to depend on the entire history of previous decisions, as in the model of Ratnaparkhi (2002). These long-range contextual dependencies turn out to be critical for accurate generation. More specifically, our model is defined in terms of three types of d</context>
<context position="3213" citStr="Liang et al., 2009" startWordPosition="487" endWordPosition="490">ample, wind speed, in the case of generating weather forecasts. The second type chooses a subset of fields from a record (micro content selection)—e.g., the minimum and maximum temperature. The third type chooses a suitable template to render the content (surface realization)— e.g., winds between [min] and [max] mph; templates are automatically extracted from training data. We tested our approach in three domains: ROBOCUP, for sportscasting (Chen and Mooney, 2008); SUMTIME, for technical weather forecast generation (Reiter et al., 2005); and WEATHERGOV, for common weather forecast generation (Liang et al., 2009). We performed both automatic (BLEU) and human evaluation. On WEATHERGOV, we 502 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics w: purple3 made a bad pass that was picked off by pink9 (a) ROBOCUP s: temperature(time=5pm-6am,min=48,mean=53,max=61) windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10) windDir(time=5pm-6am,mode=SSW) gust(time=5pm-6am,min=0,mean=0,max=0) skyCover(time=5pm-9pm,mode=0-25) skyCover(time=2am-6am,mode=75-100) precipPotenti</context>
<context position="4948" citStr="Liang et al. (2009)" startWordPosition="706" endWordPosition="709">n=36,gust max=-) w: sw 16 - 20 backing ssw 28 - 32 gusts 40 by mid evening easing 24 - 28 gusts 36 late evening (c) SUMTIME Figure 1: Example scenarios (a scenario is a world state s paired with a text w) for each of the three domains. Each row in the world state denotes a record. Our generation task is to map a world state s (input) to a text w (output). Note that this mapping involves both content selection and surface realization. achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al. (2009). On ROBOCUP and SUMTIME, we achieved results comparable to the state-of-the-art. most importantly, we obtained these results with a general-purpose approach that we believe is simpler than current state-of-the-art systems. 2 Setup and Domains Our goal is to generate a text given a world state. The world state, denoted s, is represented by a set of database records. Define T to be a set of record types, where each record type t E T is associated with a set of fields FIELDS(t). Each record r E s has a record type r.t E T and afield value r.v[f] for each field f E FIELDS(t). The text, denoted w,</context>
<context position="8131" citStr="Liang et al. (2009)" startWordPosition="1242" endWordPosition="1245">6.2 words. See Figure 1(c) for an example of a scenario. This task requires no content selection, only surface realization: The records are given in some fixed order and the task is to generate from each of these records in turn; of course, due to contextual dependencies, these records cannot be generated independently. 2.3 WEATHERGOV: Common Weather Forecasts In the WEATHERGOV domain, the world state contains detailed information about a local weather forecast (e.g., temperature, rain chance, etc.). The text is a short forecast report based on this information. We used the dataset created by Liang et al. (2009). The world state is summarized by records which aggregate measurements over selected time intervals. The dataset consists of 29,528 scenarios, each containing on average s = 36 records and 28.7 words. See Figure 1(b) for an example of a scenario. While SUMTIME and WEATHERGOV are both weather domains, there are significant differences between the two. SUMTIME forecasts are intended to be read by trained meteorologists, and thus the text is quite abbreviated. On the other hand, WEATHERGOV texts are intended to be read by the general public and thus is more English-like. Furthermore, SUMTIME doe</context>
<context position="15830" citStr="Liang et al. (2009)" startWordPosition="2555" endWordPosition="2558">.2 Field Set Decisions Field set decisions are responsible for micro content selection, i.e., which fields of a record are mentioned. Each field set decision chooses a subset of fields Fi from the set of fields FIELDS(ri.t) of the record ri that was just generated. These decisions are made based on two types of features: F1 captures which sets of fields are talked about together; for example, we learn that {mean} and {min, max} are preferred field sets for the windSpeed record. By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al. (2009) generates a sequence of fields in which a field can only depend on the previous one. F2 allows the field set to be chosen based on the values of the fields, analogously to R4. 3.3 Template Decisions Template decisions perform surface realization. A template is a sequence of elements, where each element is either a word (e.g., around) or a field (e.g., [min]). Given the record ri and field set Fi that we are generating from, the goal is to choose a template Ti (Section 4.3.2 describes how we define the set of possible templates). The features that govern the choice of Ti are as follows: W1 cap</context>
<context position="17505" citStr="Liang et al. (2009)" startWordPosition="2851" endWordPosition="2854">To guard against overfitting, we also use COARSE(Ti), which maps Ti to a coarsened version of Ti, in which more words are replaced with their associated fields (see Figure 5 for an example). W2 captures a dependence on the values of fields in the field set, and is analogous to R4 and F2. Finally, W3 contributes a language model probability, to ensure smooth transitions between templates. After Ti has been chosen, each field in the template is replaced with a word given the corresponding field value in the world state. In particular, a word is chosen from the parameters learned in the model of Liang et al. (2009). In the example in Figure 3, the [min] field in T2 has value 44, which is rendered to the word 45 (rounding and other noisy deviations are common in the WEATHERGOV domain). 4 Learning a Probabilistic Model Having described all the features, we now present a conditional probabilistic model over texts w given world states s (Section 4.1). Section 4.2 describes how to use the model for generation, and Section 4.3 describes how to learn the model. 4.1 Model Recall from Section 3 that the generation process generates r1, F1, T1, r2, F2, T2, ... , STOP. To unify notation, denote this sequence of de</context>
<context position="21218" citStr="Liang et al. (2009)" startWordPosition="3506" endWordPosition="3509">d, we end up with a standard supervised learning problem, which can be solved by maximize the (conditional) likelihood of the training data: � log p(d(i) j |d(i) &lt;j; 0) � A||0||2, (4) where A &gt; 0 is a regularization parameter. The objective function in (4) is optimized using the standard L-BFGS algorithm (Liu and Nocedal, 1989). 4.3.1 Latent Alignments As mentioned previously, our training data includes only the world state s and generated text w, not the full sequence of decisions d needed for training. Intuitively, we know what was generated but not why it was generated. We use the model of Liang et al. (2009) to impute the decisions d. They introduce a generative model p(a, w|s), where the latent alignment a specifies (1) the sequence of records that were chosen, (2) the sequence of fields that were chosen, and (3) which words in the text were spanned by the chosen records and fields. The model is learned in an unsupervised manner using EM to produce a observing only w and s. An example of an alignment is given in the left part of Figure 5. This information specifies the record decisions and a set of fields for each record. Because the induced alignments can be noisy, we need to process them to ob</context>
<context position="26107" citStr="Liang et al. (2009)" startWordPosition="4318" endWordPosition="4321"> (3) peculiarities of the text (e.g., the suffix ly in SUMTIME should exist as a separate token from its stem, or that pink goalie and pink1 have the same meaning in ROBOCUP). 5.2 Systems We evaluated the following systems on our three domains: • HUMAN is the human-generated output. • OURSYSTEM uses all the features in Figure 4 and is trained according to Section 4.3. • BASELINE is OURSYSTEM using a subset of the features (those marked with † in Figure 4). In contrast to OURSYSTEM, the included features only depend on a local context of decisions in a manner similar to the generative model of Liang et al. (2009) and the pCRU-greedy system of Belz (2008). BASELINE also excludes features that depend on values of the world state. • The existing state-of-the-art domain-specific system for each domain. 5.3 ROBOCUP Results Following the evaluation methodology of Chen and Mooney (2008), we trained our system on three Score 5 4 3 2 1 Good Non-native Disfluent Gibberish 508 System F1 BLEU* English Semantic Fluency Correctness BASELINE 78.7 24.8 4.28 ± 0.78 4.15 ± 1.14 OURSYSTEM 79.9 28.8 4.34 ± 0.69 4.17 ± 1.21 WASPER-GEN 72.0 28.7 4.43 ± 0.76 4.27 ± 1.15 HUMAN — — 4.43 ± 0.69 4.30 ± 1.07 Table 1: ROBOCUP res</context>
<context position="29091" citStr="Liang et al. (2009)" startWordPosition="4806" endWordPosition="4809">5 ± 0.69 4.27 ± 0.82 OURSYSTEM-CUSTOM 62.3 4.12 ± 0.78 4.33 ± 0.91 pCRU-greedy 63.6 4.18 ± 0.71 4.49 ± 0.73 SUMTIME-Hybrid 52.7 — — HUMAN — 4.09 ± 0.83 4.37 ± 0.87 Table 2: SUMTIME results. The SUMTIME-Hybrid system is described in (Reiter et al., 2005); pCRU-greedy, in (Belz, 2008). 5.4 SUMTIME Results The SUMTIME task only requires micro content selection and surface realization because the sequence of records to be generated is fixed; only these aspects are evaluated. Following the methodology of Belz (2008), we used five-fold cross validation. We found that using the unsupervised model of Liang et al. (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically works better with a large amount of data; and (ii) the alignment model does not exploit the temporal structure in the SUMTIME world state. Therefore, we used a small set of simple regular expressions to produce aligned training scenarios. Table 2 shows that OURSYSTEM significantly outperforms BASELINE as well as SUMTIME-Hybrid, a hand-crafted system, on BLEU. Note tha</context>
<context position="36416" citStr="Liang et al. (2009)" startWordPosition="5939" endWordPosition="5942">h enables it to choose a better sequence of records. sary. Interestingly, OURSYSTEM even outperforms HUMAN on semantic correctness, perhaps due to generating more straightforward renderings of the world state. Figure 8 describes example outputs for each system. 6 Related Work There has been a fair amount of work both on content selection and surface realization. In content selection, Barzilay and Lee (2004) use an approach based on local classification with edge-wise scores between local decisions. Our model, on the other hand, can capture higher-order constraints to enforce global coherence. Liang et al. (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the tex</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lu</author>
<author>H T Ng</author>
<author>W S Lee</author>
</authors>
<title>Natural language generation with tree conditional random fields.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>400--409</pages>
<contexts>
<context position="1475" citStr="Lu et al. (2009)" startWordPosition="215" endWordPosition="218">e focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We break up the full generation process into a sequence of</context>
<context position="37381" citStr="Lu et al. (2009)" startWordPosition="6105" endWordPosition="6108"> than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content s</context>
</contexts>
<marker>Lu, Ng, Lee, 2009</marker>
<rawString>W. Lu, H. T. Ng, and W. S. Lee. 2009. Natural language generation with tree conditional random fields. In Empirical Methods in Natural Language Processing (EMNLP), pages 400–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Parsing ’05: Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>93--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="37583" citStr="Nakanishi et al., 2005" startWordPosition="6136" endWordPosition="6139">cal form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnitz (2002) perform surface realiz</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Parsing ’05: Proceedings of the Ninth International Workshop on Parsing Technology, pages 93–102, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="24201" citStr="Papineni et al., 2002" startWordPosition="4005" endWordPosition="4008">ers; for SUMTIME, fields that span numbers and wind directions; and for ROBOCUP, fields that span words starting with purple or pink. For each record ri, we define Ti so that BASE(Ti) and COARSE(Ti) are the corresponding two extracted templates. We restrict Fi to the set of abstracted fields in the COARSE template 5 Experiments We now present an empirical evaluation of our system on our three domains—ROBOCUP, SUMTIME, and WEATHERGOV. 5.1 Evaluation Metrics Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al., 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. To evaluate macro content selection, we measured the Fl score (the harmonic mean of precision and recall) of the set of records chosen with respect to the human-annotated set of records. Human Evaluation We conducted a human evaluation using Amazon Mechanical Turk. For each domain, we chose 100 scenarios randomly from the test set. We ran each system under consideration on each of these scenarios, and presented each resulting output to 10 evaluators.2 Evaluators were gi</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum entropy models for natural language ambiguity resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18833" citStr="Ratnaparkhi (1998)" startWordPosition="3092" endWordPosition="3093">(1) j=1 where d&lt;j = (d1,... , dj_1) is the history of decisions and θ are the model parameters (feature weights). Note that the text w (the output) is a deterministic function of the decisions d. We use the features described in Section 3 to define a log-linear model for each decision: exp{φj(dj, d&lt;j, s)�θ} p(dj |d&lt;j,s; θ) = � exp{φj(d� j, d&lt;j, s)�θ}, (2) d�jEDj where θ are all the parameters (feature weights), φj is the feature vector for the j-th decision, and Dj is 506 the domain of the j-th decision (either records, field sets, or templates). This chaining of log-linear models was used in Ratnaparkhi (1998) for tagging and parsing, and in Ratnaparkhi (2002) for surface realization. The ability to condition on arbitrary histories is a defining property of these models. 4.2 Using the Model for Generation Suppose we have learned a model with parameters 0 (how to obtain 0 is discussed in Section 4.3). Given a world state s, we would like to use our model to generate an output text w via a decision sequence d. In our experiments, we choose d by sequentially choosing the best decision in a greedy fashion (until the STOP record is generated): dj = argmax p(d�j |d&lt;j, s; 0). (3) d,� Alternatively, instea</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>A. Ratnaparkhi. 1998. Maximum entropy models for natural language ambiguity resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Trainable approaches to surface natural language generation and their application to conversational dialog systems.</title>
<date>2002</date>
<journal>Computer, Speech &amp; Language,</journal>
<pages>16--435</pages>
<contexts>
<context position="1409" citStr="Ratnaparkhi (2002)" startWordPosition="204" endWordPosition="206">of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records men</context>
<context position="18884" citStr="Ratnaparkhi (2002)" startWordPosition="3100" endWordPosition="3101">of decisions and θ are the model parameters (feature weights). Note that the text w (the output) is a deterministic function of the decisions d. We use the features described in Section 3 to define a log-linear model for each decision: exp{φj(dj, d&lt;j, s)�θ} p(dj |d&lt;j,s; θ) = � exp{φj(d� j, d&lt;j, s)�θ}, (2) d�jEDj where θ are all the parameters (feature weights), φj is the feature vector for the j-th decision, and Dj is 506 the domain of the j-th decision (either records, field sets, or templates). This chaining of log-linear models was used in Ratnaparkhi (1998) for tagging and parsing, and in Ratnaparkhi (2002) for surface realization. The ability to condition on arbitrary histories is a defining property of these models. 4.2 Using the Model for Generation Suppose we have learned a model with parameters 0 (how to obtain 0 is discussed in Section 4.3). Given a world state s, we would like to use our model to generate an output text w via a decision sequence d. In our experiments, we choose d by sequentially choosing the best decision in a greedy fashion (until the STOP record is generated): dj = argmax p(d�j |d&lt;j, s; 0). (3) d,� Alternatively, instead of choosing the best decision at each point, we c</context>
<context position="37877" citStr="Ratnaparkhi (2002)" startWordPosition="6183" endWordPosition="6184">framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnitz (2002) perform surface realization of a flat semantics, which is NPhard, so they recast the problem as non-projective dependency parsing. Ratnaparkhi (2002) uses beam search to find an approximate solution. We found that a greedy approach obtained better results than beam search; Belz (2008) found greedy approaches to be </context>
</contexts>
<marker>Ratnaparkhi, 2002</marker>
<rawString>A. Ratnaparkhi. 2002. Trainable approaches to surface natural language generation and their application to conversational dialog systems. Computer, Speech &amp; Language, 16:435–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
<author>J Hunter</author>
<author>J Yu</author>
<author>I Davy</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>167--137</pages>
<contexts>
<context position="1159" citStr="Reiter et al. (2005)" startWordPosition="161" endWordPosition="165">nd each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consisting of (i) a set of d</context>
<context position="3136" citStr="Reiter et al., 2005" startWordPosition="476" endWordPosition="479"> first type chooses records from the database (macro content selection)—for example, wind speed, in the case of generating weather forecasts. The second type chooses a subset of fields from a record (micro content selection)—e.g., the minimum and maximum temperature. The third type chooses a suitable template to render the content (surface realization)— e.g., winds between [min] and [max] mph; templates are automatically extracted from training data. We tested our approach in three domains: ROBOCUP, for sportscasting (Chen and Mooney, 2008); SUMTIME, for technical weather forecast generation (Reiter et al., 2005); and WEATHERGOV, for common weather forecast generation (Liang et al., 2009). We performed both automatic (BLEU) and human evaluation. On WEATHERGOV, we 502 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics w: purple3 made a bad pass that was picked off by pink9 (a) ROBOCUP s: temperature(time=5pm-6am,min=48,mean=53,max=61) windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10) windDir(time=5pm-6am,mode=SSW) gust(time=5pm-6am,min=0,mean=0,max=0) skyC</context>
<context position="7043" citStr="Reiter et al. (2005)" startWordPosition="1068" endWordPosition="1071">scenarios from the 2001–2004 Robocup finals. Each scenario consists of a single sentence representing a fragment of a commentary on the game, paired with a set of candidate records, which were recorded within five seconds of the commentary. The records in the ROBOCUP dataset data were aligned by Chen and Mooney (2008). Each scenario contains on average s = 2.4 records and 5.7 words. See Figure 1(a) for an example of a scenario. Content selection in this domain is choosing the single record to talk about, and surface realization is talking about it. 503 2.2 SUMTIME: Technical Weather Forecasts Reiter et al. (2005) developed a generation system and created the SUMTIME-METEO corpus, which consists of marine wind weather forecasts used by offshore oil rigs, generated by the output of weather simulators. More specifically, these forecasts describe various aspects of the wind at different times during the forecast period. We used the version of the SUMTIME-METEO corpus created by Belz (2008). The dataset consists of 469 scenarios, each containing on average s = 2.6 records and 16.2 words. See Figure 1(c) for an example of a scenario. This task requires no content selection, only surface realization: The rec</context>
<context position="28725" citStr="Reiter et al., 2005" startWordPosition="4748" endWordPosition="4751">d are chosen. Table 1 shows that OURSYSTEM significantly outperforms BASELINE and is comparable to WASPER-GEN on BLEU. On human evaluation, OURSYSTEM outperforms BASELINE, but WASPER-GEN outperforms OURSYSTEM. See Figure 6 for example outputs from the various systems. BLEU English Semantic Fluency Correctness BASELINE 32.9 4.23 ± 0.71 4.26 ± 0.85 OURSYSTEM 55.1 4.25 ± 0.69 4.27 ± 0.82 OURSYSTEM-CUSTOM 62.3 4.12 ± 0.78 4.33 ± 0.91 pCRU-greedy 63.6 4.18 ± 0.71 4.49 ± 0.73 SUMTIME-Hybrid 52.7 — — HUMAN — 4.09 ± 0.83 4.37 ± 0.87 Table 2: SUMTIME results. The SUMTIME-Hybrid system is described in (Reiter et al., 2005); pCRU-greedy, in (Belz, 2008). 5.4 SUMTIME Results The SUMTIME task only requires micro content selection and surface realization because the sequence of records to be generated is fixed; only these aspects are evaluated. Following the methodology of Belz (2008), we used five-fold cross validation. We found that using the unsupervised model of Liang et al. (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically work</context>
</contexts>
<marker>Reiter, Sripada, Hunter, Yu, Davy, 2005</marker>
<rawString>E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence, 167:137–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Stochastic language generation using WIDL-expressions and its application in machine translation and summarization.</title>
<date>2006</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1105--1112</pages>
<contexts>
<context position="37043" citStr="Soricut and Marcu (2006)" startWordPosition="6049" endWordPosition="6052">troduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the ab</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>R. Soricut and D. Marcu. 2006. Stochastic language generation using WIDL-expressions and its application in machine translation and summarization. In Association for Computational Linguistics (ACL), pages 1105–1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Turner</author>
<author>Y Sripada</author>
<author>E Reiter</author>
</authors>
<title>Generating approximate geographic descriptions.</title>
<date>2009</date>
<booktitle>In European Workshop on Natural Language Generation,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="1137" citStr="Turner et al. (2009)" startWordPosition="157" endWordPosition="160">anged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consis</context>
</contexts>
<marker>Turner, Sripada, Reiter, 2009</marker>
<rawString>R. Turner, Y. Sripada, and E. Reiter. 2009. Generating approximate geographic descriptions. In European Workshop on Natural Language Generation, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
<author>Scott Martin</author>
</authors>
<title>Towards broad coverage surface realization with CCG. In</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</booktitle>
<contexts>
<context position="37552" citStr="White et al., 2007" startWordPosition="6131" endWordPosition="6134">us grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnit</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>Michael White, Rajakrishnan Rajkumar, and Scott Martin. 2007. Towards broad coverage surface realization with CCG. In In Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>960--967</pages>
<contexts>
<context position="1433" citStr="Wong and Mooney (2007)" startWordPosition="207" endWordPosition="210">uman evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We break up the </context>
<context position="36892" citStr="Wong and Mooney (2007)" startWordPosition="6021" endWordPosition="6024">cores between local decisions. Our model, on the other hand, can capture higher-order constraints to enforce global coherence. Liang et al. (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) uses a conditional random field model over trees. Other authors have performed surface realization using vario</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>