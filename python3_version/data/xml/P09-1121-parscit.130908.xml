<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.995656">
Exploiting Bilingual Information to Improve Web Search
</title>
<author confidence="0.996977">
Wei Gao1, John Blitzer2, Ming Zhou3, and Kam-Fai Wong1
</author>
<affiliation confidence="0.994952">
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong, China
</affiliation>
<email confidence="0.947495">
{wgao,kfwong}@se.cuhk.edu.hk
</email>
<affiliation confidence="0.891337">
2Computer Science Division, University of California at Berkeley, CA 94720-1776, USA
</affiliation>
<email confidence="0.961735">
blitzer@cs.berkeley.edu
</email>
<affiliation confidence="0.872055">
3Microsoft Research Asia, Beijing 100190, China
</affiliation>
<email confidence="0.997215">
mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.994761" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876130434783">
Web search quality can vary widely across
languages, even for the same information
need. We propose to exploit this variation
in quality by learning a ranking function
on bilingual queries: queries that appear in
query logs for two languages but represent
equivalent search interests. For a given
bilingual query, along with correspond-
ing monolingual query log and monolin-
gual ranking, we generate a ranking on
pairs of documents, one from each lan-
guage. Then we learn a linear ranking
function which exploits bilingual features
on pairs of documents, as well as standard
monolingual features. Finally, we show
how to reconstruct monolingual ranking
from a learned bilingual ranking. Us-
ing publicly available Chinese and English
query logs, we demonstrate for both lan-
guages that our ranking technique exploit-
ing bilingual data leads to significant im-
provements over a state-of-the-art mono-
lingual ranking algorithm.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975945454546">
Web search quality can vary widely across lan-
guages, even for a single query and search en-
gine. For example, we might expect that rank-
ing search results for the query 4;E—_&apos;�WT VTliWT
(Thomas Hobbes) to be more difficult in Chinese
than it is in English, even while holding the ba-
sic ranking function constant. At the same time,
ranking search results for the query Han Feizi (0
IFT) is likely to be harder in English than in Chi-
nese. A large portion of web queries have such
properties that they are originated in a language
different from the one they are searched.
This variance in problem difficulty across lan-
guages is not unique to web search; it appears in
a wide range of natural language processing prob-
lems. Much recent work on bilingual data has fo-
cused on exploiting these variations in difficulty
to improve a variety of monolingual tasks, includ-
ing parsing (Hwa et al., 2005; Smith and Smith,
2004; Burkett and Klein, 2008; Snyder and Barzi-
lay, 2008), named entity recognition (Chang et al.,
2009), and topic clustering (Wu and Oard, 2008).
In this work, we exploit a similar intuition to im-
prove monolingual web search.
Our problem setting differs from cross-lingual
web search, where the goal is to return machine-
translated results from one language in response to
a query from another (Lavrenko et al., 2002). We
operate under the assumption that for many mono-
lingual English queries (e.g., Han Feizi), there ex-
ist good documents in English. If we have Chinese
information as well, we can exploit it to help find
these documents. As we will see, machine trans-
lation can provide important predictive informa-
tion in our setting, but we do not wish to display
machine-translated output to the user.
We approach our problem by learning a rank-
ing function for bilingual queries – queries that
are easily translated (e.g., with machine transla-
tion) and appear in the query logs of two languages
(e.g., English and Chinese). Given query logs
in both languages, we identify bilingual queries
with sufficient clickthrough statistics in both sides.
Large-scale aggregated clickthrough data were
proved useful and effective in learning ranking
functions (Dou et al., 2008). Using these statis-
tics, we can construct a ranking over pairs of docu-
ments, one from each language. We use this rank-
ing to learn a linear scoring function on pairs of
documents given a bilingual query.
We find that our bilingual rankings have good
monolingual ranking properties. In particular,
given an optimal pairwise bilingual ranking, we
show that simple heuristics can effectively approx-
imate the optimal monolingual ranking. Using
</bodyText>
<page confidence="0.958363">
1075
</page>
<note confidence="0.999665">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1075–1083,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.9858345">
Figure 1: Proportion of bilingual queries in the
query logs of different languages.
</figureCaption>
<bodyText confidence="0.999896833333333">
these heuristics and our learned pairwise scoring
function, we can derive a ranking for new, unseen
bilingual queries. We develop and test our bilin-
gual ranker on English and Chinese with two large,
publicly available query logs from the AOL search
engine1 (English query log) (Pass et al., 2006)
and the Sougou search engine2 (Chinese query
log) (Liu et al., 2007). For both languages, we
achieve significant improvements over monolin-
gual Ranking SVM (RSVM) baselines (Herbrich
et al., 2000; Joachims, 2002), which exploit a va-
riety of monolingual features.
</bodyText>
<sectionHeader confidence="0.97371" genericHeader="method">
2 Bilingual Query Statistics
</sectionHeader>
<bodyText confidence="0.994112611111111">
We designate a query as bilingual if the concept
has been searched by users of both two languages.
As a result, not only does it occur in the query log
of its own language, but its translation also appears
in the log of the second language. So a bilingual
query yields reasonable queries in both languages.
Of course, most queries are not bilingual. For ex-
ample, our English log contains map of Alabama,
but not our Chinese log. In this case, we wouldn’t
expect the Chinese results for the query’s transla-
tion, P51rAL—_L�AM, to be helpful in ranking the
English results.
In total, we extracted 4.8 million English
queries from AOL log, of which 1.3% of their
translations appear in Sogou log. Similarly, of our
3.1 million Chinese queries from Sogou log, 2.3%
of their translations appear in AOL log. By to-
tal number of queries issued (i.e., counting dupli-
</bodyText>
<footnote confidence="0.9999655">
1http://search.aol.com
2http://www.sogou.com
</footnote>
<bodyText confidence="0.999156">
cates), the proportion of bilingual queries is much
higher. As Figure 1 shows as the number of times
a query is issued increases, so does the chance of
it being bilingual. In particular, nearly 45% of the
highest-frequency English queries and 35% of the
highest-frequency Chinese queries are bilingual.
</bodyText>
<sectionHeader confidence="0.91572" genericHeader="method">
3 Learning to Rank Using Bilingual
Information
</sectionHeader>
<bodyText confidence="0.999977538461538">
Given a set of bilingual queries, we now de-
scribe how to learn a ranking function for mono-
lingual data that exploits information from both
languages. Our procedure has three steps: Given
two monolingual rankings, we construct a bilin-
gual ranking on pairs of documents, one from each
language. Then we learn a linear scoring function
for pairs of documents that exploits monolingual
information (in both languages) and bilingual in-
formation. Finally, given this ranking function on
pairs and a new bilingual query, we reconstruct a
monolingual ranking for the language of interest.
This section addresses these steps in turn.
</bodyText>
<subsectionHeader confidence="0.999778">
3.1 Creating Bilingual Training Data
</subsectionHeader>
<bodyText confidence="0.999935296296296">
Without loss of generality, suppose we rank En-
glish documents with constraints from Chinese
documents. Given an English log Le and a Chi-
nese log Lc, our ranking algorithm takes as input
a bilingual query pair q = (qe7 qc) where qe E Le
and qc E Lc, a set of returned English documents
{eijN i�1 from qe, and a set of constraint Chinese
documents {cjjnj�1 from qc. In order to create
bilingual ranking data, we first generate monolin-
gual ranking data from clickthrough statistics. For
each language-query-document triple, we calcu-
late the aggregated click count across all users and
rank documents according to this statistic. We de-
note the count of a page as C(ei) or C(cj).
The use of clickthrough statistics as feedback
for learning ranking functions is not without con-
troversy, but recent empirical results on large
data sets suggest that the aggregated user clicks
provides an informative indicator of relevance
preference for a query. Joachims et al. (2007)
showed that relative feedback signals generated
from clicks correspond well with human judg-
ments. Dou et al. (2008) revealed that a straight-
forward use of aggregated clicks can achieve a bet-
ter ranking than using explicitly labeled data be-
cause clickthrough data contain fine-grained dif-
ferences between documents useful for learning an
</bodyText>
<figure confidence="0.9562566">
50
English
Chinese
40
35
30
25
20
15
10
5
01 10 100 1,000 10,000 50,000
Frequency (# of times that queries are issued)
Proportion of bilingual queries (%)
45
</figure>
<page confidence="0.997067">
1076
</page>
<tableCaption confidence="0.940977">
Table 1: Clickthrough data of a bilingual query
pair extracted from query logs.
</tableCaption>
<equation confidence="0.951004">
Bilingual query pair (Mazda, &apos;M_k)
doc URL click #
e1 www.mazda.com 229
e2 www.mazdausa.com 185
e3 www.mazda.co.uk 5
e4 www.starmazda.com 2
e5 www.mazdamotosports.com 2
c1 www.faw-mazda.com 50
c2 price.pcauto.com.cn/brand. 43
jsp?bid=17
c3 auto.sina.com.cn/salon/ 20
FORD/MAZDA.shtml
c4 car.autohome.com.cn/brand/ 18
119/
c5 jsp.auto.sohu.com/view/ 9
brand-bid-263.html
</equation>
<bodyText confidence="0.9998261875">
accurate and reliable ranking. Therefore, we lever-
age aggregated clicks for comparing the relevance
order of documents. Note that there is nothing
specific to our technique that requires clickthrough
statistics. Indeed, our methods could easily be em-
ployed with human annotated data. Table 1 gives
an example of a bilingual query pair and the ag-
gregated click count of each result page.
Given two monolingual documents, a prefer-
ence order can be inferred if one document is
clicked more often than another. To allow for
cross-lingual information, we extend the order of
individual documents into that of bilingual docu-
ment pairs: given two bilingual document pairs,
we will write (e�1), c�1)) &gt;_ (e�2), c�2)) to indi-
cate that the pair of (e�1), c�1)) is ranked higher
</bodyText>
<equation confidence="0.956473333333333">
� �
e(2)
than the pair of i , c(2) .
j
Definition 1 (e�1), c�1)) &gt;_ (e�2), c�2)) if and
only if one of the following relations hold:
1. C(e(1)
i ) &gt; C(e(2)
i ) and C(c(1)
j ) &gt; C(c(2)
j )
2. C(e(1)
i ) &gt; C(e(2)
i ) and C(c(1)
j ) &gt; C(c(2)
</equation>
<bodyText confidence="0.926058333333333">
j )
Note, however, that from a purely monolingual
perspective, this definition introduces orderings on
documents that should not initially have existed.
For English ranking, for example, we may have
(eZ1), cj1))&gt;_(e.2), cj2)) even when C(e�1)) =
C(e(2)
i ). This leads us to the following asymmet-
ric definition of &gt;_ that we use in practice:
</bodyText>
<equation confidence="0.9226298">
Definition 2 (e�1), cj(1)) &gt;_ (e�2), cj(2)) if and
only if C(e(1)
i ) &gt; C(e(2)
i ) and C(c(1)
j ) &gt; C(c(2)
</equation>
<bodyText confidence="0.988708545454545">
j )
With this definition, we can unambiguously
compare the relevance of bilingual document pairs
based on the order of monolingual documents.
The advantages are two-fold: (1) we can treat mul-
tiple cross-lingual document similarities the same
way as the commonly used query-document fea-
tures in a uniform manner of learning; (2) with the
similarities, the relevance estimation on bilingual
document pairs can be enhanced, and this in return
can improve the ranking of documents.
</bodyText>
<subsectionHeader confidence="0.998069">
3.2 Ranking Model
</subsectionHeader>
<bodyText confidence="0.998507967741936">
Given a pair of bilingual queries (qe, qc), we
can extract the set of corresponding bilin-
gual document pairs and their click counts
{(ei, cj), (C(ei), C(cj))}, where i = 1, ... , N
and j = 1, ... , n. Based on that, we produce a
set of bilingual ranking instances S = {Φij, zij},
where each Φij = {xi; yj; sij} is the feature
vector of (ei, cj) consisting of three components:
xi = f(qe, ei) is the vector of monolingual rele-
vancy features of ei, yi = f(qc, cj) is the vector
of monolingual relevancy features of cj, and sij =
sim(ei, cj) is the vector of cross-lingual similari-
ties between ei and cj, and zij = (C(ei), C(cj))
is the corresponding click counts.
The task is to select the optimal function that
minimizes a given loss with respect to the order
of ranked bilingual document pairs and the gold.
We resort to Ranking SVM (RSVM) (Herbrich et
al., 2000; Joachims, 2002) learning for classifica-
tion on pairs of instances. Compared the base-
line RSVM (monolingual), our algorithm learns
to classify on pairs of bilingual document pairs
rather than on pairs of individual documents.
Let f being a linear function:
f~w(ei,cj) = ~wx xi + ~wy yj + ~ws sij (1)
where w~ = {~wx; ~wy; ~ws} denotes the weight vec-
tor, in which the elements correspond to the rele-
vancy features and similarities. For any two bilin-
gual document pairs, their preference relation is
measured by the difference of the functional val-
ues of Equation 1:
</bodyText>
<equation confidence="0.991684">
� � � �
e(1)
i , c(1) e(2)
&gt;_ i , c(2) �
j j
� �
� �
e(1)
f~w i , c(1) e(2)
− f~w i , c(2) &gt; 0 �
� ~wx x(1)j− xi2)) + ~wy (yj1) − yj2)) +
~ws s(1)− s�?)) &gt; 0
</equation>
<page confidence="0.897055">
1077
</page>
<bodyText confidence="0.9378624">
We then create a new training corpus based on the
preference ordering of any two such pairs: S0 =
{�0 ij,z0 ij}, where the new feature vector becomes
&lt; Xi(1) − X92); yj1) − y�2); s(�) − s�u )} ,
and the class label
</bodyText>
<equation confidence="0.98179675">
� � � �
+1, if i , c(1) e(2)
� i , c(2) ;
j j
� � � �
−1, if i , c(2) e(1)
� i , c(1)
j j
</equation>
<bodyText confidence="0.6143705">
is a binary preference value depending on the or-
der of bilingual document pairs. The problem is to
</bodyText>
<equation confidence="0.8891198">
2k~wk2 + λEE
1jξij
i
subject to bilingual constraints: z0ij · (~w · V.ij) ≥
1 − ξij and ξij ≥ 0.
</equation>
<bodyText confidence="0.999975833333334">
There are potentially F = nN bilingual docu-
ment pairs for each query, and the number of com-
parable pairs may be much larger due to the com-
binatorial nature (but less than F(F − 1)/2). To
speed up training, we resort to stochastic gradient
descent (SGD) optimizer (Shalev-Shwartz et al.,
2007) to approximate the true gradient of the loss
function evaluated on a single instance (i.e., per
constraint). The parameters are then adjusted by
an amount proportional to this approximate gradi-
ent. For large data set, SGD-RSVM can be much
faster than batch-mode gradient descent.
</bodyText>
<subsectionHeader confidence="0.968299">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.9997684">
The solution w~ forms a vector orthogonal to the
hyper-plane of RSVM. To predict the order of
bilingual document pairs, the ranking score can
be simply calculated by Equation 1. However, a
prominent problem is how to derive the full order
of monolingual documents for output from the or-
der of bilingual document pairs. To our knowl-
edge, there is no precise conversion algorithm in
polynomial time. We thus adopt two heuristics for
approximating the true document score:
</bodyText>
<listItem confidence="0.991853142857143">
• H-1 (max score): Choose the maximum
score of the pair as the score of document,
i.e., score(ei) = maxj(f(ei, cj)).
• H-2 (mean score): Average over all the
scores of pairs associated with the ranked
document as the score of this document, i.e.,
score(ei) = 1/n Ej f(ei, cj).
</listItem>
<bodyText confidence="0.999862272727273">
Intuitively, for the rank score of a single docu-
ment, H-2 combines the “voting” scores from its n
constraint documents weighted equally, while H-1
simply chooses the maximum one. A formal ap-
proach to the problem is to leverage rank aggre-
gation formalism (Dwork et a., 2001; Liu et al.,
2007), which will be left for our future work. The
two simple heuristics are employed here because
of their simplicity and efficiency. The time com-
plexity of the approximation is linear to the num-
ber of ranked documents given n is constant.
</bodyText>
<sectionHeader confidence="0.995098" genericHeader="method">
4 Features and Similarities
</sectionHeader>
<bodyText confidence="0.9999399">
Standard features for learning to rank include vari-
ous query-document features, e.g., BM25 (Robert-
son, 1997), as well as query-independent features,
e.g., PageRank (Brin and Page, 1998). Our feature
space consists of both these standard monolingual
features and cross-lingual similarities among doc-
uments. The cross-lingual similarities are valu-
ated using different translation mechanisms, e.g.,
dictionary-based translation or machine transla-
tion, or even without any translation at all.
</bodyText>
<subsectionHeader confidence="0.998462">
4.1 Monolingual Relevancy Features
</subsectionHeader>
<bodyText confidence="0.999987">
In learning to rank, the relevancy between query
and documents and the measures based on link
analysis are commonly used as features. The dis-
cussion on their details is beyond the scope of this
paper. Readers may refer to (Liu et al., 2007)
for the definitions of many such features. We im-
plement six of these features that are considered
the most typical shown as Table 2. These include
sets of measures such as BM25, language-model-
based IR score, and PageRank. Because most con-
ventional IR and web search relevancy measures
fall into this category, we call them altogether IR
features in what follows. Note that for a given
bilingual document pair (e, c), the monolingual IR
features consist of relevance score vectors f(qe, e)
in English and f(qc, c) in Chinese.
</bodyText>
<subsectionHeader confidence="0.996995">
4.2 Cross-lingual Document Similarities
</subsectionHeader>
<bodyText confidence="0.999526666666667">
To measure the document similarity across dif-
ferent languages, we define the similarity vector
sim(e, c) as a series of functions mapping a bilin-
gual document pair to positive real numbers. In-
tuitively, a good similarity function is one which
maps cross-lingual relevant documents into close
scores and maintains a large distance between ir-
relevant and relevant documents. Four categories
of similarity measures are employed.
</bodyText>
<footnote confidence="0.6381815">
Dictionary-based Similarity (DIC): For
dictionary-based document translation, we use
</footnote>
<equation confidence="0.351576666666667">
z0ij = I
solve SVM objective: min
w~
</equation>
<page confidence="0.930047">
1078
</page>
<tableCaption confidence="0.876535">
Table 2: List of monolingual relevancy measures
used as IR features in our model.
</tableCaption>
<table confidence="0.99951">
IR Feature Description
BM25 Okapi BM25 score (Robertson, 1997)
BM25 PRF Okapi BM25 score with pseudo-
relevance feedback (Robertson and
Jones, 1976)
LM DIR Language-model-based IR score with
Dirichlet smoothing (Zhai and Lafferty,
2001)
LM JM Language-model-based IR score with
Jelinek-Mercer smoothing (Zhai and
Lafferty, 2001)
LM ABS Language-model-based IR score with
absolute discounting (Zhai and Lafferty,
2001)
PageRank PageRank score (Brin and Page, 1998)
</table>
<bodyText confidence="0.990748083333333">
the similarity measure proposed by Mathieu et
al. (2004). Given a bilingual dictionary, we let
T (e, c) denote the set of word pairs (we, wc) such
that we is a word in English document e, and wc
is a word in Chinese document c, and we is the
English translation of wc. We define tf(we, e)
and tf(wc, c) to be the term frequency of we in
e and that of wc in c, respectively. Let df(we)
and df(wc) be the English document frequency
for we and Chinese document frequency for
wc. If ne (nc) is the total number of English
(Chinese), then the bilingual idf is defined as
</bodyText>
<equation confidence="0.603139">
idf(we, wc) = log ne+nc
</equation>
<bodyText confidence="0.86667">
df(we)+df(wc). Then the
cross-lingual document similarity is calculated by
</bodyText>
<equation confidence="0.9243375">
Etf(we,e)tf(wc,c)idf(we,wc)2
sim(e, c) = (we,wc)ET(e,c) ,/Z
</equation>
<bodyText confidence="0.997226076923077">
where Z is a normalization coefficient (see Math-
ieu et al. (2004) for detail). This similarity func-
tion can be understood as the cross-lingual coun-
terpart of the monolingual cosine similarity func-
tion (Salton, 1998).
Similarity Based on Machine Translation
(MT): For machine translation, the cross-lingual
measure actually becomes a monolingual similar-
ity between one document and another’s transla-
tion. We therefore adopt cosine function for it di-
rectly (Salton, 1998).
Translation Ratio (RATIO): Translation ratio
is defined as two sets of ratios of translatable terms
using a bilingual dictionary: RATIO FOR – what
percent of words in e can be translated to words in
c; RATIO BACK – what percent of words in c can
be translated back to words in e.
URL LCS Ratio (URL): The ratio of longest
common subsequence (Cormen et al., 2001) be-
tween the URLs of two pages being compared.
This measure is useful to capture pages in different
languages but with similar URLs such as www.
airbus.com, www.airbus.com.cn, etc.
Note that each set of similarities above except
URL includes 3 values based on different fields of
web page: title, body, and title+body.
</bodyText>
<sectionHeader confidence="0.996104" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999898">
This section presents evaluation metric, data sets
and experiments for our proposed ranker.
</bodyText>
<subsectionHeader confidence="0.98221">
5.1 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999981321428571">
Commonly adopted metrics for ranking, such as
mean average precision (Buckley and Voorhees,
2000) and Normalized Discounted Cumulative
Gain (J¨arvelin and Kek¨al¨ainen, 2000), is designed
for data sets with human relevance judgment,
which is not available to us. Therefore, we
use the Kendall’s tau coefficient (Kendall, 1938;
Joachims, 2002) to measure the degree of correla-
tion between two rankings. For simplicity, let’s as-
sume strict orderings of any given ranking. There-
fore we ignore all the pairs with ties (instances
with the identical click count). Kendall’s tau is
defined as T(ra, rb) = (P − Q)/(P + Q), where
P is the number of concordant pairs and Q is the
number of disconcordant pairs in the given order-
ings ra and rb. The value is a real number within
[−1,+1], where −1 indicates a complete inver-
sion, and +1 stands for perfect agreement, and a
value of zero indicates no correlation.
Existing ranking techniques heavily depend on
human relevance judgment that is very costly to
obtain. Similar to Dou et al (2008), our method
utilizes the automatically aggregated click count in
query logs as the gold for deriving the true order
of relevancy, but we use the clickthrough of dif-
ferent languages. We average Kendall’s tau values
between the algorithm output and the gold based
on click frequency for all test queries.
</bodyText>
<subsectionHeader confidence="0.99806">
5.2 Data Sets
</subsectionHeader>
<bodyText confidence="0.999917888888889">
Query logs can be the basis for constructing high
quality ranking corpus. Due to the proprietary
issue of log, no public ranking corpus based on
real-world search engine log is currently avail-
able. Moreover, to build a predictable bilingual
ranking corpus, the logs of different languages are
needed and have to meet certain conditions: (1)
they should be sufficiently large so that a good
number of bilingual query pairs could be identi-
</bodyText>
<page confidence="0.999463">
1079
</page>
<tableCaption confidence="0.999719">
Table 3: Statistics on AOL and Sogou query logs.
</tableCaption>
<table confidence="0.998765285714286">
AOL(EN) Sogou(CH)
# sessions 657,426 5,131,000
# unique queries 10,154,743 3,117,902
# clicked queries 4,811,650 3,117,590
# clicked URLs 1,632,788 8,627,174
time span 2006/03-05 2006/08
size 2.12GB 1.56GB
</table>
<bodyText confidence="0.97910865625">
fied; (2) for the identified query pairs, there should
be sufficient statistics of associated clickthrough
data; (3) The click frequency should be well dis-
tributed at both sides so that the preference order
between bilingual document pairs can be derived
for SVM learning.
For these reasons, we use two independent and
publicly accessible query logs to construct our
bilingual ranking corpus: English AOL log3 and
Chinese Sogou log4. Table 3 shows some statis-
tics of these two large query logs.
We automatically identify 10,544 bilingual
query pairs from the two logs using the Java
API for Google Translate5, in which each query
has certain number of clicked URLs. To bet-
ter control the bilingual equivalency of queries,
we make sure the bilingual queries in each of
these pairs are bi-directional translations. Then
we download all their clicked pages, which re-
sults in 70,180 English6 and 111,197 Chinese doc-
uments. These documents form two independent
collections, which are indexed separately for re-
trieval and feature calculation.
For good quality, it is necessary to have suffi-
cient clickthrough data for each query. So we fur-
ther identify 1,084 out of 10,544 bilingual query
pairs, in which each query has at least 10 clicked
and downloadable documents. This smaller col-
lection is used for learning our model, which con-
tains 21,711 English and 28,578 Chinese docu-
ments7. In order to compute cross-lingual doc-
ument similarities based on machine translation
</bodyText>
<footnote confidence="0.991186416666667">
3http://gregsadetsky.com/aol-data/
4http://www.sogou.com/labs/dl/q.html
5http://code.google.com/p/
google-api-translate-java/
6AOL log only records the domain portion of the clicked
URLs, which misleads document downloading. We use the
“search within site or domain” function of a major search en-
gine to approximate the real clicked URLs by keeping the
first returned result for each query.
7Because Sogou log has a lot more clicked URLs, for bal-
ancing with the number of English pages, we kept at most 50
pages per Chinese query.
</footnote>
<tableCaption confidence="0.8356704">
Table 4: Kendall’s tau values of English ranking.
The significant improvements over baseline (99%
confidence) are bolded with the p-values given in
parenthesis. * indicates significant improvement
over IR (no similarity). n = 5.
</tableCaption>
<table confidence="0.999616928571428">
Models Pair H-1(max) H-2 (mean)
RSVM (baseline) n/a 0.2424 0.2424
IR (no similarity) 0.2783 0.2445 0.2445
IR+DIC 0.2909 0.2453 0.2496
IR+MT 0.2858 0.2488* 0.2494*
(p=0.0003) (p=0.0004)
IR+DIC+MT 0.2901 0.2481 0.2514*
(p=0.0009)
IR+DIC+RATIO 0.2946 0.2466 0.2519*
(p=0.0004)
IR+DIC+MT 0.2473* 0.2539*
+RATIO 0.2940 (p=0.0009) (p=1.5e-5)
IR+DIC+MT 0.2533* 0.2577*
+RATIO+URL 0.2979 (p=2.2e-5) (p=4.4e-7)
</table>
<bodyText confidence="0.97987">
(see Section 4.2), we automatically translate all
these 50,298 documents using Google Translate,
i.e., English to Chinese and vice versa. Then the
bilingual document pairs are constructed, and all
the monolingual features and cross-lingual simi-
larities are computed (see Section 4.1&amp;4.2).
</bodyText>
<subsectionHeader confidence="0.999008">
5.3 English Ranking Performance
</subsectionHeader>
<bodyText confidence="0.999975346153846">
Here we examine the ranking performance of our
English ranker under different similarity settings.
We use traditional RSVM (Herbrich et al., 2000;
Joachims, 2002) without any bilingual considera-
tion as the baseline, which uses only English IR
features. We conduct this experiment using all the
1,084 bilingual query pairs with 4-fold cross vali-
dation (each fold with 271 query pairs). The num-
ber of constraint documents n is empirically set as
5. The results are shown in Table 4.
Clearly, bilingual constraints are helpful to
improve English ranking. Our pairwise set-
tings unanimously outperforms the RSVM base-
line. The paired two-tailed t-test (Smucker et
al., 2007) shows that most improvements resulted
from heuristic H-2 (mean score) are statistically
significant at 99% confidence level (p&lt;0.01). Rel-
atively fewer significant improvements can be
made by heuristic H-1 (max score). This is be-
cause the maximum score on pair is just a rough
approximation to the optimal document score. But
this simple scheme works surprisingly well and
still consistently outperforms the baseline.
Note that our bilingual model with only IR fea-
tures, i.e., IR (no similarity), also outperforms the
baseline. The reason is that in this setting there are
</bodyText>
<page confidence="0.977721">
1080
</page>
<figure confidence="0.971978666666667">
Kendall’s tau
1 2 3 4 5 6 7 8 9 10
# of constraint documents in a different language
</figure>
<figureCaption confidence="0.985273">
Figure 2: English ranking results vary with the
</figureCaption>
<bodyText confidence="0.930173358974359">
number of constraint Chinese documents.
IR features of n Chinese documents introduced in
addition to the IR features of English documents
in the baseline.
The DIC similarity does not work as effectively
as MT. This may be due to the limitation of bilin-
gual dictionary alone for translating documents,
where the issues like out-of-vocabulary words and
translation ambiguity are common but can be bet-
ter dealt with by MT. When DIC is combined with
RATIO, which considers both forward and back-
ward translation of words, it can capture the corre-
lation between bilingually very similar pages, thus
performs better.
We find that the URL similarity, although sim-
ple, is very useful and improves 1.5–2.4% of
Kendall’s tau value than not using it. This is be-
cause the URLs of the top Chinese (constraint)
documents are often similar to many of returned
English URLs which are generally more regu-
lar. For example, in query pair (Toyota Camry,
-1-:F9&apos;fi�T,-), 9/13 English pages are anchored by
the URLs containing keywords “toyota” and/or
“camry”, and 3/5 constraint documents’ URLs
also contain them. In contrast, the URLs of re-
turned Chinese pages are less regular in general.
This also explains why this measure does not im-
prove much for Chinese ranking (see Section 5.4).
We also vary the parameter n to study how
the performance changes with different number of
constraint Chinese documents. Figure 2 shows the
results using heuristic H-2. More constraint doc-
uments are generally helpful, but when only one
constraint document is used, it may be detrimen-
Table 5: Kendall’s tau values of Chinese ranking.
The significant improvements over baseline (99%
confidence) are bolded with the p-values given in
parenthesis. * indicates significant improvement
over IR (no similarity). n = 5.
</bodyText>
<table confidence="0.9996362">
Models Pair H-1(max) H-2 (mean)
RSVM (baseline) n/a 0.2935 0.2935
IR (no similarity) 0.3201 0.2938 0.2938
IR+DIC 0.3220 0.2970 0.2973*
(p=0.0060) (p=0.0020)
IR+MT 0.3299 0.2992* 0.3008*
(p=0.0034) (p=0.0003)
IR+DIC+MT 0.3295 0.2991* 0.3004*
(p=0.0014) (p=0.0008)
IR+DIC+RATIO 0.3240 0.2972* 0.2968*
(p=0.0010) (p=0.0014)
IR+DIC+MT 0.2973* 0.3007*
+RATIO 0.3303 (p=0.0004) (p=0.0002)
IR+DIC+MT 0.2981* 0.3024*
+RATIO+URL 0.3288 (p=0.0005) (p=1.5e-6)
</table>
<bodyText confidence="0.9998868">
tal to the ranking for some features. One explana-
tion is that the document clicked most often is not
necessarily relevant, and it is very likely that no
English page is similar to the first Chinese page.
Joachims et al. (2007) found that users’ click be-
havior is biased by the rank of search engine at
the first and/or second positions (especially the
first). More constraint pages are helpful because
the pages after the first are less biased and the click
counts can reflect the relevancy more accurately.
</bodyText>
<subsectionHeader confidence="0.998609">
5.4 Chinese Ranking Performance
</subsectionHeader>
<bodyText confidence="0.99998695">
We also benchmark Chinese ranking with English
constraint documents under the similar configura-
tions as Section 5.3. The results are given by Ta-
ble 5 and Figure 3.
As shown in Table 5, improvements on Chinese
ranking are even more encouraging. Kendall’s tau
values under all the settings are significantly better
than not only the baseline but also IR (no similar-
ity). This may suggest that English information is
generally more helpful to Chinese ranking than the
other way round. The reason is straightforward:
there are a high proportion of Chinese queries hav-
ing English or foreign-language origins in our data
set. For these queries, relevant information at Chi-
nese side may be relatively poorer, so the English
ranking can be more reliable. As far as we can, we
manually identified 215 such queries from all the
1,084 bilingual queries (amount to 23.2%).
To shed more light on this finding, we exam-
ine top-20 queries improved most by our method
</bodyText>
<figure confidence="0.98351875">
0.26
0.255
0.25
0.245
RSVM (baseline)
IR+DIC
IR+MT
IR+DIC+MT
IR+DIC+RAIO+MT
IR+DIC+RAIO+MT+URL
0.24
0.235
0.23
1081
1 2 3 4 5 6 7 8 9 10
# of constraint documents in a different language
</figure>
<figureCaption confidence="0.997125">
Figure 3: Chinese ranking results vary with the
</figureCaption>
<bodyText confidence="0.961707">
number of constraint English documents.
(with all features and similarities) over the base-
line. As shown in Table 6, most of the top im-
proved Chinese queries are about concepts origi-
nated from English or other languages, or some-
thing non-local (bolded). Interestingly, iRr{
9i (political catoons) are among these Chinese
queries improved most by English ranking, which
is believed as rare (or sensitive) content on Chi-
nese web. In contrast, top English queries are short
of this type of queries. But we can still see Bruce
Lee (&apos;M&apos;,J2), a Chinese Kung-Fu actor, and pe-
ony (î[), the national flower of China. Their
information tends to be more popular on Chinese
web, and thus helpful to English ranking. For the
exceptions like Sunrider (AhW9y) and Aniston
(�WW(î), despite their English origins, we find
they have surprisingly sparse click counts in En-
glish log while Chinese users look much more in-
terested and provide a lot of clickthrough that is
helpful.
</bodyText>
<sectionHeader confidence="0.998086" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999862636363636">
We aim to improve web search ranking for an
important set of queries, called bilingual queries,
by exploiting bilingual information derived from
clickthrough logs of different languages. The
thrust of our technique is using search ranking
of one language and cross-lingual information to
help ranking of another language. Our pairwise
ranking scheme based on bilingual document pairs
can easily integrate all kinds of similarities into
the existing framework and significantly improves
both English and Chinese ranking performance.
</bodyText>
<tableCaption confidence="0.962633">
Table 6: Top 20 most improved bilingual queries.
Bold means a positive example for our hypothesis.
* marks an exception.
</tableCaption>
<table confidence="0.9755956875">
Most improved CH queries Most improved EN queries
tfUN (salmonella) free online tv (1/2 �V&amp;quot;ž
41)
��� (scotland) weapons (AW
rNOW4 (caffeine) lily (WA)
ò�Õ (epitaph) cable (žN&amp;quot;)
� � � �(british his- *sunrider (fd1W9y)
tory)
iRr{9i (political car- *aniston (�WW(î)
toons)
1/29Es%k_PL, (immune sys- clothes (VBR)
tem)
V49ËE (wine bottles) *three little pigs (HaB
�)
z¿fIJ (hungary) hair care (#P&amp;)
T7K (witchcraft) neon (�W})
�2;M (popcorn) bruce lee (�B�)
M&amp;I (impetigo) radish (Y )
_R _t 17 * it (bathroom chile (WfIJ)
design)
n-A-T (pigeon) peony (î[)
IM} (polar bear) toothache (¿ )
4�A•M (map of africa) free online translation (1/2
�V&amp;quot; *)
RL Tli RL õ (labrador water (7JC)
retriever)
M2RL�rcyA (pamela oil (,75A)
anderson)
3RJ%BR (yoga clothing) shopping network (rA �
n)
IR ” (federal ex- *prince harry (����)
press)
</table>
<bodyText confidence="0.999747384615385">
Our model can be generally applied to other
search ranking problems, such as ranking us-
ing monolingual similarities or ranking for cross-
lingual/multilingual web search. Another interest-
ing direction is to study the recovery of the optimal
document ordering from pairwise ordering using
well-founded formalism such as rank aggregation
approaches (Dwork et a., 2001; Liu et al., 2007).
Furthermore, we may involve more sophisti-
cated monolingual features that do not transfer
cross-lingually but are asymmetric for either side,
such as clustering, document classification fea-
tures built from domain taxonomies like DMOZ.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997206">
This work is partially supported by the Innova-
tion Technology Fund, Hong Kong (project No.:
ITS/182/08). We would like to thank Cheng Niu
for the insightful advice and anonymous reviewers
for the useful comments.
</bodyText>
<figure confidence="0.993989588235294">
0.302
0.3
0.304
0.298
Kendall’s tau
0.296
0.294
0.288
RSVM (baseline)
IR+DIC
IR+MT
IR+DIC+MT
IR+DIC+RATIO+MT
IR+DIC+RATIO+MT+URL
0.286
0.292
0.29
</figure>
<page confidence="0.992605">
1082
</page>
<note confidence="0.976699166666667">
Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and
Hang Li. 2007. LECTOR: Benchmark Dataset for
Research on Learning to Rank for Information Re-
trieval. In Proceedings of SIGIR 2007 Workshop on
Learning to Rank for Information Retrieval, pp. 3-
10, Amsterdam, The Netherland.
</note>
<sectionHeader confidence="0.825448" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992828119565218">
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-Scale Hypertextual Web Search Engine.
In Proceedings of WWW.
Chris Buckley and Ellen M. Voorhees. 2000. Evaluat-
ing Evaluation Measure Stability. In Proceedings of
ACM SIGIR, pp. 33-40.
David Burkett and Dan Klein. 2008. Two Languages
are Better than One (for Syntactic Parsing). In Pro-
ceedings of EMNLP, pp. 877-886.
Ming-Wei Chang, Dan Goldwasser Dan Roth and
Yuancheng Tu. 2009. Unsupervised Constraint
Driven Learning for Transliteration Discovery. In
Proceedings of NAACL-HLT.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest and Clifford Stein. 2001. Introduction to Al-
gorithms (2nd Edition), MIT Press, pp. 350-355.
Zhicheng Dou, Ruihua Song, Xiaojie Yuan and Ji-Rong
Wen. 2008. Are Click-through Data Adequate for
Learning Web Search Rankings? In Proceedings of
ACM CIKM, pp. 73-82.
Cynthia Dwork, Ravi Kumar, Moni Naor and D.
Sivakumar. 2001. Rank Aggregation Methods for
the Web. In Proceedings of WWW, pp. 613-622.
Ralf Herbrich, Thore Graepel and Klaus Obermayer.
2000. Large Margin Rank Boundaries for Ordinal
Regression. Advances in Large Margin Classifiers,
The MIT Press, pp. 115-132.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak 2005. Bootstrap-
ping Parsers via Syntactic Projection across Parallel
Texts. Natural Language Engineering, 11(3):311-
325.
Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2000. IR Eval-
uation Methods for Retrieving Highly Relevant Doc-
uments. In Proceedings of ACM SIGIR, pp. 41-48.
Thorsten Joachims. 2002. Optimizing Search Engines
Using Clickthrough Data. In Proceedings of ACM
SIGKDD, pp. 133-142.
Thorsten Joachims, Laura Granka, Bing Pan, Helene
Hembrooke, Filip Radlinski and Geri Gay 2007.
Evaluating the Accuracy of Implicit Feedback from
Clicks and Query Reformulations in Web Search.
ACM Transaction on Information Systems, 25(2):7.
M. Kendall. 1938. A New Measure of Rank Correla-
tion. Biometrika, 30:81-89.
Victor Lavrenko, Martin Choquette and Bruce W.
Croft. 2002. Cross-Lingual Relevance Models. In
Proceedings of ACM SIGIR, pp. 175-182.
Yiqun Liu, Yupeng Fu, Min Zhang, Shaoping Ma and
Liyun Ru. 2007. Automatic Search Engine Perfor-
mance Evaluation with Click-through Data Analy-
sis. In Proceedings of WWW, pp. 1133-1134.
Yu-Ting Liu, Tie-Yan Liu, Tao Qin, Zhi-Ming Ma, and
Hang Li. 2007. Supervised Rank Aggregation. In
Proceedings of WWW, pp. 481-489.
Benoit Mathieu, Romanic Besancon and Christian
Fluhr. 2004. Multilingual Document Clusters Dis-
covery. In proceedings of Recherche d’Information
Assist´ee par Ordinateur (RIAO), pp. 1-10.
Greg Pass, Abdur Chowdhury and Cayley Torgeson.
2006. A Picture of Search. In Proceedings of the
1st International Conference on Scalable Informa-
tion Systems (INFOSCALE), Hong Kong.
S. E. Robertson. 1997. Overview of the OKAPI
Projects. Journal of Documentation, 53(1):3-7.
S. E. Robertson and K. Sparc Jones. 1976. Relevance
Weighting of Search Terms. Journal of the Ameri-
can Society of Information Science, 27(3):129-146.
Gerard Salton. 1998. Automatic Text Processing.
Addison-Wesley Publishing Company.
Shai Shalev-Shwartz, Yoram Singer and Nathan Sre-
bro. 2007. Pegasos: Primal Estimated sub-
GrAdient SOlver for SVM. In Proceedings of
ICML, pp. 807-814.
David A. Smith and Noah A. Smith. 2004. Bilingual
Parsing with Factored Estimation: Using English to
Parse Korean. In Proceedings of EMNLP.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A Comparison of Statistical Significance
Tests for Information Retrieval Evaluation. In Pro-
ceedings of ACM CIKM, pp. 623-632.
Benjamin Snyder and Regina Barzilay. 2008. Unsu-
pervised Multilingual Learning for Morphological
Segmentation. In Proceedings ofACL, pp. 737-745.
Yejun Wu and Douglas W. Oard. 2008. Bilingual
Topic Aspect Classification with a Few Training Ex-
amples. In Proceedings of ACM SIGIR, pp. 203-
210.
Chengxiang Zhai and John Lafferty. 2001. A Study of
Smoothing Methods for Language Models Applied
to Ad Hoc Information Retrieval. In Proceedings of
ACM SIGIR, pp. 334-342.
</reference>
<page confidence="0.985746">
1083
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.139509">
<title confidence="0.993929">Exploiting Bilingual Information to Improve Web Search</title>
<author confidence="0.998563">John Ming</author>
<affiliation confidence="0.591962">Chinese University of Hong Kong, Shatin, N.T., Hong Kong, China</affiliation>
<email confidence="0.953536">wgao@se.cuhk.edu.hk</email>
<email confidence="0.953536">kfwong@se.cuhk.edu.hk</email>
<address confidence="0.621505">Science Division, University of California at Berkeley, CA 94720-1776, USA</address>
<email confidence="0.983577">blitzer@cs.berkeley.edu</email>
<note confidence="0.43946">Research Asia, Beijing 100190, China</note>
<email confidence="0.999309">mingzhou@microsoft.com</email>
<abstract confidence="0.998292375">Web search quality can vary widely across languages, even for the same information need. We propose to exploit this variation in quality by learning a ranking function on bilingual queries: queries that appear in query logs for two languages but represent equivalent search interests. For a given bilingual query, along with corresponding monolingual query log and monolingual ranking, we generate a ranking on pairs of documents, one from each language. Then we learn a linear ranking function which exploits bilingual features on pairs of documents, as well as standard monolingual features. Finally, we show how to reconstruct monolingual ranking from a learned bilingual ranking. Using publicly available Chinese and English query logs, we demonstrate for both languages that our ranking technique exploiting bilingual data leads to significant improvements over a state-of-the-art monolingual ranking algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The Anatomy of a Large-Scale Hypertextual Web Search Engine.</title>
<date>1998</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="14725" citStr="Brin and Page, 1998" startWordPosition="2493" endWordPosition="2496">eighted equally, while H-1 simply chooses the maximum one. A formal approach to the problem is to leverage rank aggregation formalism (Dwork et a., 2001; Liu et al., 2007), which will be left for our future work. The two simple heuristics are employed here because of their simplicity and efficiency. The time complexity of the approximation is linear to the number of ranked documents given n is constant. 4 Features and Similarities Standard features for learning to rank include various query-document features, e.g., BM25 (Robertson, 1997), as well as query-independent features, e.g., PageRank (Brin and Page, 1998). Our feature space consists of both these standard monolingual features and cross-lingual similarities among documents. The cross-lingual similarities are valuated using different translation mechanisms, e.g., dictionary-based translation or machine translation, or even without any translation at all. 4.1 Monolingual Relevancy Features In learning to rank, the relevancy between query and documents and the measures based on link analysis are commonly used as features. The discussion on their details is beyond the scope of this paper. Readers may refer to (Liu et al., 2007) for the definitions </context>
<context position="16966" citStr="Brin and Page, 1998" startWordPosition="2835" endWordPosition="2838">based document translation, we use z0ij = I solve SVM objective: min w~ 1078 Table 2: List of monolingual relevancy measures used as IR features in our model. IR Feature Description BM25 Okapi BM25 score (Robertson, 1997) BM25 PRF Okapi BM25 score with pseudorelevance feedback (Robertson and Jones, 1976) LM DIR Language-model-based IR score with Dirichlet smoothing (Zhai and Lafferty, 2001) LM JM Language-model-based IR score with Jelinek-Mercer smoothing (Zhai and Lafferty, 2001) LM ABS Language-model-based IR score with absolute discounting (Zhai and Lafferty, 2001) PageRank PageRank score (Brin and Page, 1998) the similarity measure proposed by Mathieu et al. (2004). Given a bilingual dictionary, we let T (e, c) denote the set of word pairs (we, wc) such that we is a word in English document e, and wc is a word in Chinese document c, and we is the English translation of wc. We define tf(we, e) and tf(wc, c) to be the term frequency of we in e and that of wc in c, respectively. Let df(we) and df(wc) be the English document frequency for we and Chinese document frequency for wc. If ne (nc) is the total number of English (Chinese), then the bilingual idf is defined as idf(we, wc) = log ne+nc df(we)+df</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Ellen M Voorhees</author>
</authors>
<title>Evaluating Evaluation Measure Stability.</title>
<date>2000</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="19081" citStr="Buckley and Voorhees, 2000" startWordPosition="3188" endWordPosition="3191">io (URL): The ratio of longest common subsequence (Cormen et al., 2001) between the URLs of two pages being compared. This measure is useful to capture pages in different languages but with similar URLs such as www. airbus.com, www.airbus.com.cn, etc. Note that each set of similarities above except URL includes 3 values based on different fields of web page: title, body, and title+body. 5 Experiments and Results This section presents evaluation metric, data sets and experiments for our proposed ranker. 5.1 Evaluation Metric Commonly adopted metrics for ranking, such as mean average precision (Buckley and Voorhees, 2000) and Normalized Discounted Cumulative Gain (J¨arvelin and Kek¨al¨ainen, 2000), is designed for data sets with human relevance judgment, which is not available to us. Therefore, we use the Kendall’s tau coefficient (Kendall, 1938; Joachims, 2002) to measure the degree of correlation between two rankings. For simplicity, let’s assume strict orderings of any given ranking. Therefore we ignore all the pairs with ties (instances with the identical click count). Kendall’s tau is defined as T(ra, rb) = (P − Q)/(P + Q), where P is the number of concordant pairs and Q is the number of disconcordant pai</context>
</contexts>
<marker>Buckley, Voorhees, 2000</marker>
<rawString>Chris Buckley and Ellen M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In Proceedings of ACM SIGIR, pp. 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two Languages are Better than One (for Syntactic Parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>877--886</pages>
<contexts>
<context position="2265" citStr="Burkett and Klein, 2008" startWordPosition="354" endWordPosition="357"> the same time, ranking search results for the query Han Feizi (0 IFT) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these </context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two Languages are Better than One (for Syntactic Parsing). In Proceedings of EMNLP, pp. 877-886.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Dan Goldwasser Dan Roth</author>
<author>Yuancheng Tu</author>
</authors>
<title>Unsupervised Constraint Driven Learning for Transliteration Discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2340" citStr="Chang et al., 2009" startWordPosition="366" endWordPosition="369">y to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these documents. As we will see, machine translation can provide important predic</context>
</contexts>
<marker>Chang, Roth, Tu, 2009</marker>
<rawString>Ming-Wei Chang, Dan Goldwasser Dan Roth and Yuancheng Tu. 2009. Unsupervised Constraint Driven Learning for Transliteration Discovery. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<date>2001</date>
<booktitle>Introduction to Algorithms (2nd Edition),</booktitle>
<pages>350--355</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="18525" citStr="Cormen et al., 2001" startWordPosition="3102" endWordPosition="3105">8). Similarity Based on Machine Translation (MT): For machine translation, the cross-lingual measure actually becomes a monolingual similarity between one document and another’s translation. We therefore adopt cosine function for it directly (Salton, 1998). Translation Ratio (RATIO): Translation ratio is defined as two sets of ratios of translatable terms using a bilingual dictionary: RATIO FOR – what percent of words in e can be translated to words in c; RATIO BACK – what percent of words in c can be translated back to words in e. URL LCS Ratio (URL): The ratio of longest common subsequence (Cormen et al., 2001) between the URLs of two pages being compared. This measure is useful to capture pages in different languages but with similar URLs such as www. airbus.com, www.airbus.com.cn, etc. Note that each set of similarities above except URL includes 3 values based on different fields of web page: title, body, and title+body. 5 Experiments and Results This section presents evaluation metric, data sets and experiments for our proposed ranker. 5.1 Evaluation Metric Commonly adopted metrics for ranking, such as mean average precision (Buckley and Voorhees, 2000) and Normalized Discounted Cumulative Gain (</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. 2001. Introduction to Algorithms (2nd Edition), MIT Press, pp. 350-355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Dou</author>
<author>Ruihua Song</author>
<author>Xiaojie Yuan</author>
<author>Ji-Rong Wen</author>
</authors>
<title>Are Click-through Data Adequate for Learning Web Search Rankings?</title>
<date>2008</date>
<booktitle>In Proceedings of ACM CIKM,</booktitle>
<pages>73--82</pages>
<contexts>
<context position="3507" citStr="Dou et al., 2008" startWordPosition="555" endWordPosition="558">, machine translation can provide important predictive information in our setting, but we do not wish to display machine-translated output to the user. We approach our problem by learning a ranking function for bilingual queries – queries that are easily translated (e.g., with machine translation) and appear in the query logs of two languages (e.g., English and Chinese). Given query logs in both languages, we identify bilingual queries with sufficient clickthrough statistics in both sides. Large-scale aggregated clickthrough data were proved useful and effective in learning ranking functions (Dou et al., 2008). Using these statistics, we can construct a ranking over pairs of documents, one from each language. We use this ranking to learn a linear scoring function on pairs of documents given a bilingual query. We find that our bilingual rankings have good monolingual ranking properties. In particular, given an optimal pairwise bilingual ranking, we show that simple heuristics can effectively approximate the optimal monolingual ranking. Using 1075 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1075–1083, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNL</context>
<context position="7772" citStr="Dou et al. (2008)" startWordPosition="1252" endWordPosition="1255">hrough statistics. For each language-query-document triple, we calculate the aggregated click count across all users and rank documents according to this statistic. We denote the count of a page as C(ei) or C(cj). The use of clickthrough statistics as feedback for learning ranking functions is not without controversy, but recent empirical results on large data sets suggest that the aggregated user clicks provides an informative indicator of relevance preference for a query. Joachims et al. (2007) showed that relative feedback signals generated from clicks correspond well with human judgments. Dou et al. (2008) revealed that a straightforward use of aggregated clicks can achieve a better ranking than using explicitly labeled data because clickthrough data contain fine-grained differences between documents useful for learning an 50 English Chinese 40 35 30 25 20 15 10 5 01 10 100 1,000 10,000 50,000 Frequency (# of times that queries are issued) Proportion of bilingual queries (%) 45 1076 Table 1: Clickthrough data of a bilingual query pair extracted from query logs. Bilingual query pair (Mazda, &apos;M_k) doc URL click # e1 www.mazda.com 229 e2 www.mazdausa.com 185 e3 www.mazda.co.uk 5 e4 www.starmazda.c</context>
<context position="20014" citStr="Dou et al (2008)" startWordPosition="3346" endWordPosition="3349">ty, let’s assume strict orderings of any given ranking. Therefore we ignore all the pairs with ties (instances with the identical click count). Kendall’s tau is defined as T(ra, rb) = (P − Q)/(P + Q), where P is the number of concordant pairs and Q is the number of disconcordant pairs in the given orderings ra and rb. The value is a real number within [−1,+1], where −1 indicates a complete inversion, and +1 stands for perfect agreement, and a value of zero indicates no correlation. Existing ranking techniques heavily depend on human relevance judgment that is very costly to obtain. Similar to Dou et al (2008), our method utilizes the automatically aggregated click count in query logs as the gold for deriving the true order of relevancy, but we use the clickthrough of different languages. We average Kendall’s tau values between the algorithm output and the gold based on click frequency for all test queries. 5.2 Data Sets Query logs can be the basis for constructing high quality ranking corpus. Due to the proprietary issue of log, no public ranking corpus based on real-world search engine log is currently available. Moreover, to build a predictable bilingual ranking corpus, the logs of different lan</context>
</contexts>
<marker>Dou, Song, Yuan, Wen, 2008</marker>
<rawString>Zhicheng Dou, Ruihua Song, Xiaojie Yuan and Ji-Rong Wen. 2008. Are Click-through Data Adequate for Learning Web Search Rankings? In Proceedings of ACM CIKM, pp. 73-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Dwork</author>
<author>Ravi Kumar</author>
<author>Moni Naor</author>
<author>D Sivakumar</author>
</authors>
<title>Rank Aggregation Methods for the Web. In</title>
<date>2001</date>
<booktitle>Proceedings of WWW,</booktitle>
<pages>613--622</pages>
<marker>Dwork, Kumar, Naor, Sivakumar, 2001</marker>
<rawString>Cynthia Dwork, Ravi Kumar, Moni Naor and D. Sivakumar. 2001. Rank Aggregation Methods for the Web. In Proceedings of WWW, pp. 613-622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
</authors>
<title>Thore Graepel and Klaus Obermayer.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers, The</booktitle>
<pages>115--132</pages>
<publisher>MIT Press,</publisher>
<marker>Herbrich, 2000</marker>
<rawString>Ralf Herbrich, Thore Graepel and Klaus Obermayer. 2000. Large Margin Rank Boundaries for Ordinal Regression. Advances in Large Margin Classifiers, The MIT Press, pp. 115-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
</authors>
<title>Clara Cabezas, and Okan Kolak</title>
<date>2005</date>
<pages>11--3</pages>
<contexts>
<context position="2217" citStr="Hwa et al., 2005" startWordPosition="346" endWordPosition="349">g the basic ranking function constant. At the same time, ranking search results for the query Han Feizi (0 IFT) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese informatio</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak 2005. Bootstrapping Parsers via Syntactic Projection across Parallel Texts. Natural Language Engineering, 11(3):311-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo J¨arvelin</author>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>IR Evaluation Methods for Retrieving Highly Relevant Documents.</title>
<date>2000</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>41--48</pages>
<marker>J¨arvelin, Kek¨al¨ainen, 2000</marker>
<rawString>Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2000. IR Evaluation Methods for Retrieving Highly Relevant Documents. In Proceedings of ACM SIGIR, pp. 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing Search Engines Using Clickthrough Data.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="4701" citStr="Joachims, 2002" startWordPosition="749" endWordPosition="750">. c�2009 ACL and AFNLP Figure 1: Proportion of bilingual queries in the query logs of different languages. these heuristics and our learned pairwise scoring function, we can derive a ranking for new, unseen bilingual queries. We develop and test our bilingual ranker on English and Chinese with two large, publicly available query logs from the AOL search engine1 (English query log) (Pass et al., 2006) and the Sougou search engine2 (Chinese query log) (Liu et al., 2007). For both languages, we achieve significant improvements over monolingual Ranking SVM (RSVM) baselines (Herbrich et al., 2000; Joachims, 2002), which exploit a variety of monolingual features. 2 Bilingual Query Statistics We designate a query as bilingual if the concept has been searched by users of both two languages. As a result, not only does it occur in the query log of its own language, but its translation also appears in the log of the second language. So a bilingual query yields reasonable queries in both languages. Of course, most queries are not bilingual. For example, our English log contains map of Alabama, but not our Chinese log. In this case, we wouldn’t expect the Chinese results for the query’s translation, P51rAL—_L</context>
<context position="11438" citStr="Joachims, 2002" startWordPosition="1884" endWordPosition="1885">S = {Φij, zij}, where each Φij = {xi; yj; sij} is the feature vector of (ei, cj) consisting of three components: xi = f(qe, ei) is the vector of monolingual relevancy features of ei, yi = f(qc, cj) is the vector of monolingual relevancy features of cj, and sij = sim(ei, cj) is the vector of cross-lingual similarities between ei and cj, and zij = (C(ei), C(cj)) is the corresponding click counts. The task is to select the optimal function that minimizes a given loss with respect to the order of ranked bilingual document pairs and the gold. We resort to Ranking SVM (RSVM) (Herbrich et al., 2000; Joachims, 2002) learning for classification on pairs of instances. Compared the baseline RSVM (monolingual), our algorithm learns to classify on pairs of bilingual document pairs rather than on pairs of individual documents. Let f being a linear function: f~w(ei,cj) = ~wx xi + ~wy yj + ~ws sij (1) where w~ = {~wx; ~wy; ~ws} denotes the weight vector, in which the elements correspond to the relevancy features and similarities. For any two bilingual document pairs, their preference relation is measured by the difference of the functional values of Equation 1: � � � � e(1) i , c(1) e(2) &gt;_ i , c(2) � j j � � � </context>
<context position="19326" citStr="Joachims, 2002" startWordPosition="3225" endWordPosition="3226">that each set of similarities above except URL includes 3 values based on different fields of web page: title, body, and title+body. 5 Experiments and Results This section presents evaluation metric, data sets and experiments for our proposed ranker. 5.1 Evaluation Metric Commonly adopted metrics for ranking, such as mean average precision (Buckley and Voorhees, 2000) and Normalized Discounted Cumulative Gain (J¨arvelin and Kek¨al¨ainen, 2000), is designed for data sets with human relevance judgment, which is not available to us. Therefore, we use the Kendall’s tau coefficient (Kendall, 1938; Joachims, 2002) to measure the degree of correlation between two rankings. For simplicity, let’s assume strict orderings of any given ranking. Therefore we ignore all the pairs with ties (instances with the identical click count). Kendall’s tau is defined as T(ra, rb) = (P − Q)/(P + Q), where P is the number of concordant pairs and Q is the number of disconcordant pairs in the given orderings ra and rb. The value is a real number within [−1,+1], where −1 indicates a complete inversion, and +1 stands for perfect agreement, and a value of zero indicates no correlation. Existing ranking techniques heavily depen</context>
<context position="24138" citStr="Joachims, 2002" startWordPosition="3976" endWordPosition="3977">04) IR+DIC+MT 0.2473* 0.2539* +RATIO 0.2940 (p=0.0009) (p=1.5e-5) IR+DIC+MT 0.2533* 0.2577* +RATIO+URL 0.2979 (p=2.2e-5) (p=4.4e-7) (see Section 4.2), we automatically translate all these 50,298 documents using Google Translate, i.e., English to Chinese and vice versa. Then the bilingual document pairs are constructed, and all the monolingual features and cross-lingual similarities are computed (see Section 4.1&amp;4.2). 5.3 English Ranking Performance Here we examine the ranking performance of our English ranker under different similarity settings. We use traditional RSVM (Herbrich et al., 2000; Joachims, 2002) without any bilingual consideration as the baseline, which uses only English IR features. We conduct this experiment using all the 1,084 bilingual query pairs with 4-fold cross validation (each fold with 271 query pairs). The number of constraint documents n is empirically set as 5. The results are shown in Table 4. Clearly, bilingual constraints are helpful to improve English ranking. Our pairwise settings unanimously outperforms the RSVM baseline. The paired two-tailed t-test (Smucker et al., 2007) shows that most improvements resulted from heuristic H-2 (mean score) are statistically signi</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In Proceedings of ACM SIGKDD, pp. 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Laura Granka</author>
<author>Bing Pan</author>
<author>Helene Hembrooke</author>
<author>Filip Radlinski</author>
<author>Geri Gay</author>
</authors>
<title>Evaluating the Accuracy of Implicit Feedback from Clicks and Query Reformulations in Web Search.</title>
<date>2007</date>
<journal>ACM Transaction on Information Systems,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="7656" citStr="Joachims et al. (2007)" startWordPosition="1234" endWordPosition="1237">ments {cjjnj�1 from qc. In order to create bilingual ranking data, we first generate monolingual ranking data from clickthrough statistics. For each language-query-document triple, we calculate the aggregated click count across all users and rank documents according to this statistic. We denote the count of a page as C(ei) or C(cj). The use of clickthrough statistics as feedback for learning ranking functions is not without controversy, but recent empirical results on large data sets suggest that the aggregated user clicks provides an informative indicator of relevance preference for a query. Joachims et al. (2007) showed that relative feedback signals generated from clicks correspond well with human judgments. Dou et al. (2008) revealed that a straightforward use of aggregated clicks can achieve a better ranking than using explicitly labeled data because clickthrough data contain fine-grained differences between documents useful for learning an 50 English Chinese 40 35 30 25 20 15 10 5 01 10 100 1,000 10,000 50,000 Frequency (# of times that queries are issued) Proportion of bilingual queries (%) 45 1076 Table 1: Clickthrough data of a bilingual query pair extracted from query logs. Bilingual query pai</context>
<context position="27808" citStr="Joachims et al. (2007)" startWordPosition="4559" endWordPosition="4562">.2935 0.2935 IR (no similarity) 0.3201 0.2938 0.2938 IR+DIC 0.3220 0.2970 0.2973* (p=0.0060) (p=0.0020) IR+MT 0.3299 0.2992* 0.3008* (p=0.0034) (p=0.0003) IR+DIC+MT 0.3295 0.2991* 0.3004* (p=0.0014) (p=0.0008) IR+DIC+RATIO 0.3240 0.2972* 0.2968* (p=0.0010) (p=0.0014) IR+DIC+MT 0.2973* 0.3007* +RATIO 0.3303 (p=0.0004) (p=0.0002) IR+DIC+MT 0.2981* 0.3024* +RATIO+URL 0.3288 (p=0.0005) (p=1.5e-6) tal to the ranking for some features. One explanation is that the document clicked most often is not necessarily relevant, and it is very likely that no English page is similar to the first Chinese page. Joachims et al. (2007) found that users’ click behavior is biased by the rank of search engine at the first and/or second positions (especially the first). More constraint pages are helpful because the pages after the first are less biased and the click counts can reflect the relevancy more accurately. 5.4 Chinese Ranking Performance We also benchmark Chinese ranking with English constraint documents under the similar configurations as Section 5.3. The results are given by Table 5 and Figure 3. As shown in Table 5, improvements on Chinese ranking are even more encouraging. Kendall’s tau values under all the setting</context>
</contexts>
<marker>Joachims, Granka, Pan, Hembrooke, Radlinski, Gay, 2007</marker>
<rawString>Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, Filip Radlinski and Geri Gay 2007. Evaluating the Accuracy of Implicit Feedback from Clicks and Query Reformulations in Web Search. ACM Transaction on Information Systems, 25(2):7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kendall</author>
</authors>
<title>A New Measure of Rank Correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--81</pages>
<contexts>
<context position="19309" citStr="Kendall, 1938" startWordPosition="3223" endWordPosition="3224">.cn, etc. Note that each set of similarities above except URL includes 3 values based on different fields of web page: title, body, and title+body. 5 Experiments and Results This section presents evaluation metric, data sets and experiments for our proposed ranker. 5.1 Evaluation Metric Commonly adopted metrics for ranking, such as mean average precision (Buckley and Voorhees, 2000) and Normalized Discounted Cumulative Gain (J¨arvelin and Kek¨al¨ainen, 2000), is designed for data sets with human relevance judgment, which is not available to us. Therefore, we use the Kendall’s tau coefficient (Kendall, 1938; Joachims, 2002) to measure the degree of correlation between two rankings. For simplicity, let’s assume strict orderings of any given ranking. Therefore we ignore all the pairs with ties (instances with the identical click count). Kendall’s tau is defined as T(ra, rb) = (P − Q)/(P + Q), where P is the number of concordant pairs and Q is the number of disconcordant pairs in the given orderings ra and rb. The value is a real number within [−1,+1], where −1 indicates a complete inversion, and +1 stands for perfect agreement, and a value of zero indicates no correlation. Existing ranking techniq</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>M. Kendall. 1938. A New Measure of Rank Correlation. Biometrika, 30:81-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Martin Choquette</author>
<author>Bruce W Croft</author>
</authors>
<title>Cross-Lingual Relevance Models.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="2654" citStr="Lavrenko et al., 2002" startWordPosition="418" endWordPosition="421">sing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these documents. As we will see, machine translation can provide important predictive information in our setting, but we do not wish to display machine-translated output to the user. We approach our problem by learning a ranking function for bilingual queries – queries that are easily translated (e.g., with machine translation) and appear in the query logs of two languages (e.g., English and </context>
</contexts>
<marker>Lavrenko, Choquette, Croft, 2002</marker>
<rawString>Victor Lavrenko, Martin Choquette and Bruce W. Croft. 2002. Cross-Lingual Relevance Models. In Proceedings of ACM SIGIR, pp. 175-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiqun Liu</author>
<author>Yupeng Fu</author>
<author>Min Zhang</author>
<author>Shaoping Ma</author>
<author>Liyun Ru</author>
</authors>
<title>Automatic Search Engine Performance Evaluation with Click-through Data Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>1133--1134</pages>
<contexts>
<context position="4558" citStr="Liu et al., 2007" startWordPosition="727" endWordPosition="730">Using 1075 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1075–1083, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP Figure 1: Proportion of bilingual queries in the query logs of different languages. these heuristics and our learned pairwise scoring function, we can derive a ranking for new, unseen bilingual queries. We develop and test our bilingual ranker on English and Chinese with two large, publicly available query logs from the AOL search engine1 (English query log) (Pass et al., 2006) and the Sougou search engine2 (Chinese query log) (Liu et al., 2007). For both languages, we achieve significant improvements over monolingual Ranking SVM (RSVM) baselines (Herbrich et al., 2000; Joachims, 2002), which exploit a variety of monolingual features. 2 Bilingual Query Statistics We designate a query as bilingual if the concept has been searched by users of both two languages. As a result, not only does it occur in the query log of its own language, but its translation also appears in the log of the second language. So a bilingual query yields reasonable queries in both languages. Of course, most queries are not bilingual. For example, our English lo</context>
<context position="14276" citStr="Liu et al., 2007" startWordPosition="2421" endWordPosition="2424">stics for approximating the true document score: • H-1 (max score): Choose the maximum score of the pair as the score of document, i.e., score(ei) = maxj(f(ei, cj)). • H-2 (mean score): Average over all the scores of pairs associated with the ranked document as the score of this document, i.e., score(ei) = 1/n Ej f(ei, cj). Intuitively, for the rank score of a single document, H-2 combines the “voting” scores from its n constraint documents weighted equally, while H-1 simply chooses the maximum one. A formal approach to the problem is to leverage rank aggregation formalism (Dwork et a., 2001; Liu et al., 2007), which will be left for our future work. The two simple heuristics are employed here because of their simplicity and efficiency. The time complexity of the approximation is linear to the number of ranked documents given n is constant. 4 Features and Similarities Standard features for learning to rank include various query-document features, e.g., BM25 (Robertson, 1997), as well as query-independent features, e.g., PageRank (Brin and Page, 1998). Our feature space consists of both these standard monolingual features and cross-lingual similarities among documents. The cross-lingual similarities</context>
<context position="32154" citStr="Liu et al., 2007" startWordPosition="5264" endWordPosition="5267">A•M (map of africa) free online translation (1/2 �V&amp;quot; *) RL Tli RL õ (labrador water (7JC) retriever) M2RL�rcyA (pamela oil (,75A) anderson) 3RJ%BR (yoga clothing) shopping network (rA � n) IR ” (federal ex- *prince harry (����) press) Our model can be generally applied to other search ranking problems, such as ranking using monolingual similarities or ranking for crosslingual/multilingual web search. Another interesting direction is to study the recovery of the optimal document ordering from pairwise ordering using well-founded formalism such as rank aggregation approaches (Dwork et a., 2001; Liu et al., 2007). Furthermore, we may involve more sophisticated monolingual features that do not transfer cross-lingually but are asymmetric for either side, such as clustering, document classification features built from domain taxonomies like DMOZ. Acknowledgments This work is partially supported by the Innovation Technology Fund, Hong Kong (project No.: ITS/182/08). We would like to thank Cheng Niu for the insightful advice and anonymous reviewers for the useful comments. 0.302 0.3 0.304 0.298 Kendall’s tau 0.296 0.294 0.288 RSVM (baseline) IR+DIC IR+MT IR+DIC+MT IR+DIC+RATIO+MT IR+DIC+RATIO+MT+URL 0.286 </context>
</contexts>
<marker>Liu, Fu, Zhang, Ma, Ru, 2007</marker>
<rawString>Yiqun Liu, Yupeng Fu, Min Zhang, Shaoping Ma and Liyun Ru. 2007. Automatic Search Engine Performance Evaluation with Click-through Data Analysis. In Proceedings of WWW, pp. 1133-1134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Ting Liu</author>
<author>Tie-Yan Liu</author>
<author>Tao Qin</author>
<author>Zhi-Ming Ma</author>
<author>Hang Li</author>
</authors>
<title>Supervised Rank Aggregation.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>481--489</pages>
<contexts>
<context position="4558" citStr="Liu et al., 2007" startWordPosition="727" endWordPosition="730">Using 1075 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1075–1083, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP Figure 1: Proportion of bilingual queries in the query logs of different languages. these heuristics and our learned pairwise scoring function, we can derive a ranking for new, unseen bilingual queries. We develop and test our bilingual ranker on English and Chinese with two large, publicly available query logs from the AOL search engine1 (English query log) (Pass et al., 2006) and the Sougou search engine2 (Chinese query log) (Liu et al., 2007). For both languages, we achieve significant improvements over monolingual Ranking SVM (RSVM) baselines (Herbrich et al., 2000; Joachims, 2002), which exploit a variety of monolingual features. 2 Bilingual Query Statistics We designate a query as bilingual if the concept has been searched by users of both two languages. As a result, not only does it occur in the query log of its own language, but its translation also appears in the log of the second language. So a bilingual query yields reasonable queries in both languages. Of course, most queries are not bilingual. For example, our English lo</context>
<context position="14276" citStr="Liu et al., 2007" startWordPosition="2421" endWordPosition="2424">stics for approximating the true document score: • H-1 (max score): Choose the maximum score of the pair as the score of document, i.e., score(ei) = maxj(f(ei, cj)). • H-2 (mean score): Average over all the scores of pairs associated with the ranked document as the score of this document, i.e., score(ei) = 1/n Ej f(ei, cj). Intuitively, for the rank score of a single document, H-2 combines the “voting” scores from its n constraint documents weighted equally, while H-1 simply chooses the maximum one. A formal approach to the problem is to leverage rank aggregation formalism (Dwork et a., 2001; Liu et al., 2007), which will be left for our future work. The two simple heuristics are employed here because of their simplicity and efficiency. The time complexity of the approximation is linear to the number of ranked documents given n is constant. 4 Features and Similarities Standard features for learning to rank include various query-document features, e.g., BM25 (Robertson, 1997), as well as query-independent features, e.g., PageRank (Brin and Page, 1998). Our feature space consists of both these standard monolingual features and cross-lingual similarities among documents. The cross-lingual similarities</context>
<context position="32154" citStr="Liu et al., 2007" startWordPosition="5264" endWordPosition="5267">A•M (map of africa) free online translation (1/2 �V&amp;quot; *) RL Tli RL õ (labrador water (7JC) retriever) M2RL�rcyA (pamela oil (,75A) anderson) 3RJ%BR (yoga clothing) shopping network (rA � n) IR ” (federal ex- *prince harry (����) press) Our model can be generally applied to other search ranking problems, such as ranking using monolingual similarities or ranking for crosslingual/multilingual web search. Another interesting direction is to study the recovery of the optimal document ordering from pairwise ordering using well-founded formalism such as rank aggregation approaches (Dwork et a., 2001; Liu et al., 2007). Furthermore, we may involve more sophisticated monolingual features that do not transfer cross-lingually but are asymmetric for either side, such as clustering, document classification features built from domain taxonomies like DMOZ. Acknowledgments This work is partially supported by the Innovation Technology Fund, Hong Kong (project No.: ITS/182/08). We would like to thank Cheng Niu for the insightful advice and anonymous reviewers for the useful comments. 0.302 0.3 0.304 0.298 Kendall’s tau 0.296 0.294 0.288 RSVM (baseline) IR+DIC IR+MT IR+DIC+MT IR+DIC+RATIO+MT IR+DIC+RATIO+MT+URL 0.286 </context>
</contexts>
<marker>Liu, Liu, Qin, Ma, Li, 2007</marker>
<rawString>Yu-Ting Liu, Tie-Yan Liu, Tao Qin, Zhi-Ming Ma, and Hang Li. 2007. Supervised Rank Aggregation. In Proceedings of WWW, pp. 481-489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Mathieu</author>
<author>Romanic Besancon</author>
<author>Christian Fluhr</author>
</authors>
<title>Multilingual Document Clusters Discovery.</title>
<date>2004</date>
<booktitle>In proceedings of Recherche d’Information Assist´ee par Ordinateur (RIAO),</booktitle>
<pages>1--10</pages>
<contexts>
<context position="17023" citStr="Mathieu et al. (2004)" startWordPosition="2844" endWordPosition="2847">jective: min w~ 1078 Table 2: List of monolingual relevancy measures used as IR features in our model. IR Feature Description BM25 Okapi BM25 score (Robertson, 1997) BM25 PRF Okapi BM25 score with pseudorelevance feedback (Robertson and Jones, 1976) LM DIR Language-model-based IR score with Dirichlet smoothing (Zhai and Lafferty, 2001) LM JM Language-model-based IR score with Jelinek-Mercer smoothing (Zhai and Lafferty, 2001) LM ABS Language-model-based IR score with absolute discounting (Zhai and Lafferty, 2001) PageRank PageRank score (Brin and Page, 1998) the similarity measure proposed by Mathieu et al. (2004). Given a bilingual dictionary, we let T (e, c) denote the set of word pairs (we, wc) such that we is a word in English document e, and wc is a word in Chinese document c, and we is the English translation of wc. We define tf(we, e) and tf(wc, c) to be the term frequency of we in e and that of wc in c, respectively. Let df(we) and df(wc) be the English document frequency for we and Chinese document frequency for wc. If ne (nc) is the total number of English (Chinese), then the bilingual idf is defined as idf(we, wc) = log ne+nc df(we)+df(wc). Then the cross-lingual document similarity is calcu</context>
</contexts>
<marker>Mathieu, Besancon, Fluhr, 2004</marker>
<rawString>Benoit Mathieu, Romanic Besancon and Christian Fluhr. 2004. Multilingual Document Clusters Discovery. In proceedings of Recherche d’Information Assist´ee par Ordinateur (RIAO), pp. 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Pass</author>
<author>Abdur Chowdhury</author>
<author>Cayley Torgeson</author>
</authors>
<title>A Picture of Search.</title>
<date>2006</date>
<booktitle>In Proceedings of the 1st International Conference on Scalable Information Systems (INFOSCALE), Hong Kong.</booktitle>
<contexts>
<context position="4489" citStr="Pass et al., 2006" startWordPosition="715" endWordPosition="718">uristics can effectively approximate the optimal monolingual ranking. Using 1075 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1075–1083, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP Figure 1: Proportion of bilingual queries in the query logs of different languages. these heuristics and our learned pairwise scoring function, we can derive a ranking for new, unseen bilingual queries. We develop and test our bilingual ranker on English and Chinese with two large, publicly available query logs from the AOL search engine1 (English query log) (Pass et al., 2006) and the Sougou search engine2 (Chinese query log) (Liu et al., 2007). For both languages, we achieve significant improvements over monolingual Ranking SVM (RSVM) baselines (Herbrich et al., 2000; Joachims, 2002), which exploit a variety of monolingual features. 2 Bilingual Query Statistics We designate a query as bilingual if the concept has been searched by users of both two languages. As a result, not only does it occur in the query log of its own language, but its translation also appears in the log of the second language. So a bilingual query yields reasonable queries in both languages. O</context>
</contexts>
<marker>Pass, Chowdhury, Torgeson, 2006</marker>
<rawString>Greg Pass, Abdur Chowdhury and Cayley Torgeson. 2006. A Picture of Search. In Proceedings of the 1st International Conference on Scalable Information Systems (INFOSCALE), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
</authors>
<date>1997</date>
<journal>Overview of the OKAPI Projects. Journal of Documentation,</journal>
<pages>53--1</pages>
<contexts>
<context position="14648" citStr="Robertson, 1997" startWordPosition="2483" endWordPosition="2485">ument, H-2 combines the “voting” scores from its n constraint documents weighted equally, while H-1 simply chooses the maximum one. A formal approach to the problem is to leverage rank aggregation formalism (Dwork et a., 2001; Liu et al., 2007), which will be left for our future work. The two simple heuristics are employed here because of their simplicity and efficiency. The time complexity of the approximation is linear to the number of ranked documents given n is constant. 4 Features and Similarities Standard features for learning to rank include various query-document features, e.g., BM25 (Robertson, 1997), as well as query-independent features, e.g., PageRank (Brin and Page, 1998). Our feature space consists of both these standard monolingual features and cross-lingual similarities among documents. The cross-lingual similarities are valuated using different translation mechanisms, e.g., dictionary-based translation or machine translation, or even without any translation at all. 4.1 Monolingual Relevancy Features In learning to rank, the relevancy between query and documents and the measures based on link analysis are commonly used as features. The discussion on their details is beyond the scop</context>
<context position="16567" citStr="Robertson, 1997" startWordPosition="2781" endWordPosition="2782">larity vector sim(e, c) as a series of functions mapping a bilingual document pair to positive real numbers. Intuitively, a good similarity function is one which maps cross-lingual relevant documents into close scores and maintains a large distance between irrelevant and relevant documents. Four categories of similarity measures are employed. Dictionary-based Similarity (DIC): For dictionary-based document translation, we use z0ij = I solve SVM objective: min w~ 1078 Table 2: List of monolingual relevancy measures used as IR features in our model. IR Feature Description BM25 Okapi BM25 score (Robertson, 1997) BM25 PRF Okapi BM25 score with pseudorelevance feedback (Robertson and Jones, 1976) LM DIR Language-model-based IR score with Dirichlet smoothing (Zhai and Lafferty, 2001) LM JM Language-model-based IR score with Jelinek-Mercer smoothing (Zhai and Lafferty, 2001) LM ABS Language-model-based IR score with absolute discounting (Zhai and Lafferty, 2001) PageRank PageRank score (Brin and Page, 1998) the similarity measure proposed by Mathieu et al. (2004). Given a bilingual dictionary, we let T (e, c) denote the set of word pairs (we, wc) such that we is a word in English document e, and wc is a </context>
</contexts>
<marker>Robertson, 1997</marker>
<rawString>S. E. Robertson. 1997. Overview of the OKAPI Projects. Journal of Documentation, 53(1):3-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>K Sparc Jones</author>
</authors>
<title>Relevance Weighting of Search Terms.</title>
<date>1976</date>
<journal>Journal of the American Society of Information Science,</journal>
<pages>27--3</pages>
<contexts>
<context position="16651" citStr="Robertson and Jones, 1976" startWordPosition="2792" endWordPosition="2795">ent pair to positive real numbers. Intuitively, a good similarity function is one which maps cross-lingual relevant documents into close scores and maintains a large distance between irrelevant and relevant documents. Four categories of similarity measures are employed. Dictionary-based Similarity (DIC): For dictionary-based document translation, we use z0ij = I solve SVM objective: min w~ 1078 Table 2: List of monolingual relevancy measures used as IR features in our model. IR Feature Description BM25 Okapi BM25 score (Robertson, 1997) BM25 PRF Okapi BM25 score with pseudorelevance feedback (Robertson and Jones, 1976) LM DIR Language-model-based IR score with Dirichlet smoothing (Zhai and Lafferty, 2001) LM JM Language-model-based IR score with Jelinek-Mercer smoothing (Zhai and Lafferty, 2001) LM ABS Language-model-based IR score with absolute discounting (Zhai and Lafferty, 2001) PageRank PageRank score (Brin and Page, 1998) the similarity measure proposed by Mathieu et al. (2004). Given a bilingual dictionary, we let T (e, c) denote the set of word pairs (we, wc) such that we is a word in English document e, and wc is a word in Chinese document c, and we is the English translation of wc. We define tf(we</context>
</contexts>
<marker>Robertson, Jones, 1976</marker>
<rawString>S. E. Robertson and K. Sparc Jones. 1976. Relevance Weighting of Search Terms. Journal of the American Society of Information Science, 27(3):129-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing.</title>
<date>1998</date>
<publisher>Addison-Wesley Publishing Company.</publisher>
<contexts>
<context position="17907" citStr="Salton, 1998" startWordPosition="3002" endWordPosition="3003"> of we in e and that of wc in c, respectively. Let df(we) and df(wc) be the English document frequency for we and Chinese document frequency for wc. If ne (nc) is the total number of English (Chinese), then the bilingual idf is defined as idf(we, wc) = log ne+nc df(we)+df(wc). Then the cross-lingual document similarity is calculated by Etf(we,e)tf(wc,c)idf(we,wc)2 sim(e, c) = (we,wc)ET(e,c) ,/Z where Z is a normalization coefficient (see Mathieu et al. (2004) for detail). This similarity function can be understood as the cross-lingual counterpart of the monolingual cosine similarity function (Salton, 1998). Similarity Based on Machine Translation (MT): For machine translation, the cross-lingual measure actually becomes a monolingual similarity between one document and another’s translation. We therefore adopt cosine function for it directly (Salton, 1998). Translation Ratio (RATIO): Translation ratio is defined as two sets of ratios of translatable terms using a bilingual dictionary: RATIO FOR – what percent of words in e can be translated to words in c; RATIO BACK – what percent of words in c can be translated back to words in e. URL LCS Ratio (URL): The ratio of longest common subsequence (Co</context>
</contexts>
<marker>Salton, 1998</marker>
<rawString>Gerard Salton. 1998. Automatic Text Processing. Addison-Wesley Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal Estimated subGrAdient SOlver for SVM.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="12943" citStr="Shalev-Shwartz et al., 2007" startWordPosition="2196" endWordPosition="2199"> yj1) − y�2); s(�) − s�u )} , and the class label � � � � +1, if i , c(1) e(2) � i , c(2) ; j j � � � � −1, if i , c(2) e(1) � i , c(1) j j is a binary preference value depending on the order of bilingual document pairs. The problem is to 2k~wk2 + λEE 1jξij i subject to bilingual constraints: z0ij · (~w · V.ij) ≥ 1 − ξij and ξij ≥ 0. There are potentially F = nN bilingual document pairs for each query, and the number of comparable pairs may be much larger due to the combinatorial nature (but less than F(F − 1)/2). To speed up training, we resort to stochastic gradient descent (SGD) optimizer (Shalev-Shwartz et al., 2007) to approximate the true gradient of the loss function evaluated on a single instance (i.e., per constraint). The parameters are then adjusted by an amount proportional to this approximate gradient. For large data set, SGD-RSVM can be much faster than batch-mode gradient descent. 3.3 Inference The solution w~ forms a vector orthogonal to the hyper-plane of RSVM. To predict the order of bilingual document pairs, the ranking score can be simply calculated by Equation 1. However, a prominent problem is how to derive the full order of monolingual documents for output from the order of bilingual do</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer and Nathan Srebro. 2007. Pegasos: Primal Estimated subGrAdient SOlver for SVM. In Proceedings of ICML, pp. 807-814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual Parsing with Factored Estimation: Using English to Parse Korean.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2240" citStr="Smith and Smith, 2004" startWordPosition="350" endWordPosition="353">g function constant. At the same time, ranking search results for the query Han Feizi (0 IFT) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can explo</context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual Parsing with Factored Estimation: Using English to Parse Korean. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Smucker</author>
<author>James Allan</author>
<author>Ben Carterette</author>
</authors>
<title>A Comparison of Statistical Significance Tests for Information Retrieval Evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACM CIKM,</booktitle>
<pages>623--632</pages>
<contexts>
<context position="24644" citStr="Smucker et al., 2007" startWordPosition="4056" endWordPosition="4059">English ranker under different similarity settings. We use traditional RSVM (Herbrich et al., 2000; Joachims, 2002) without any bilingual consideration as the baseline, which uses only English IR features. We conduct this experiment using all the 1,084 bilingual query pairs with 4-fold cross validation (each fold with 271 query pairs). The number of constraint documents n is empirically set as 5. The results are shown in Table 4. Clearly, bilingual constraints are helpful to improve English ranking. Our pairwise settings unanimously outperforms the RSVM baseline. The paired two-tailed t-test (Smucker et al., 2007) shows that most improvements resulted from heuristic H-2 (mean score) are statistically significant at 99% confidence level (p&lt;0.01). Relatively fewer significant improvements can be made by heuristic H-1 (max score). This is because the maximum score on pair is just a rough approximation to the optimal document score. But this simple scheme works surprisingly well and still consistently outperforms the baseline. Note that our bilingual model with only IR features, i.e., IR (no similarity), also outperforms the baseline. The reason is that in this setting there are 1080 Kendall’s tau 1 2 3 4 </context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of Statistical Significance Tests for Information Retrieval Evaluation. In Proceedings of ACM CIKM, pp. 623-632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised Multilingual Learning for Morphological Segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>737--745</pages>
<contexts>
<context position="2293" citStr="Snyder and Barzilay, 2008" startWordPosition="358" endWordPosition="362">earch results for the query Han Feizi (0 IFT) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these documents. As we will see, m</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised Multilingual Learning for Morphological Segmentation. In Proceedings ofACL, pp. 737-745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejun Wu</author>
<author>Douglas W Oard</author>
</authors>
<title>Bilingual Topic Aspect Classification with a Few Training Examples.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>203--210</pages>
<contexts>
<context position="2382" citStr="Wu and Oard, 2008" startWordPosition="373" endWordPosition="376">A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these documents. As we will see, machine translation can provide important predictive information in our setting, but we do</context>
</contexts>
<marker>Wu, Oard, 2008</marker>
<rawString>Yejun Wu and Douglas W. Oard. 2008. Bilingual Topic Aspect Classification with a Few Training Examples. In Proceedings of ACM SIGIR, pp. 203-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="16739" citStr="Zhai and Lafferty, 2001" startWordPosition="2804" endWordPosition="2807">ps cross-lingual relevant documents into close scores and maintains a large distance between irrelevant and relevant documents. Four categories of similarity measures are employed. Dictionary-based Similarity (DIC): For dictionary-based document translation, we use z0ij = I solve SVM objective: min w~ 1078 Table 2: List of monolingual relevancy measures used as IR features in our model. IR Feature Description BM25 Okapi BM25 score (Robertson, 1997) BM25 PRF Okapi BM25 score with pseudorelevance feedback (Robertson and Jones, 1976) LM DIR Language-model-based IR score with Dirichlet smoothing (Zhai and Lafferty, 2001) LM JM Language-model-based IR score with Jelinek-Mercer smoothing (Zhai and Lafferty, 2001) LM ABS Language-model-based IR score with absolute discounting (Zhai and Lafferty, 2001) PageRank PageRank score (Brin and Page, 1998) the similarity measure proposed by Mathieu et al. (2004). Given a bilingual dictionary, we let T (e, c) denote the set of word pairs (we, wc) such that we is a word in English document e, and wc is a word in Chinese document c, and we is the English translation of wc. We define tf(we, e) and tf(wc, c) to be the term frequency of we in e and that of wc in c, respectively</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. In Proceedings of ACM SIGIR, pp. 334-342.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>