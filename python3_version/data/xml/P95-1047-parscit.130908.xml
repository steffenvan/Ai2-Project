<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000195">
<title confidence="0.955827">
Acquiring a Lexicon from Unsegmented Speech
</title>
<author confidence="0.783695">
Carl de Marcken
</author>
<affiliation confidence="0.70561">
MIT Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.8148435">
545 Technology Square, NE43-804
Cambridge, MA, 02139, USA
</address>
<email confidence="0.999008">
cgdemarc@ai.mit.edu
</email>
<sectionHeader confidence="0.99739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997954">
We present work-in-progress on the ma-
chine acquisition of a lexicon from sen-
tences that are each an unsegmented phone
sequence paired with a primitive represen-
tation of meaning. A simple exploratory
algorithm is described, along with the di-
rection of current work and a discussion
of the relevance of the problem for child
language acquisition and computer speech
recognition.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9499398">
We are interested in how a lexicon of discrete words
can be acquired from continuous speech, a prob-
lem fundamental both to child language acquisition
and to the automated induction of computer speech
recognition systems; see (Olivier, 1968; Wolff, 1982;
Cartwright and Brent, 1994) for previous computa-
tional work in this area. For the time being, we ap-
proximate the problem as induction from phone se-
quences rather than acoustic pressure, and assume
that learning takes place in an environment where
simple semantic representations of the speech intent
are available to the acquisition mechanism.
For example, we approximate the greater problem
as that of learning from inputs like
Phon. Input: /33rmbItsineYb3wt/
</bodyText>
<subsectionHeader confidence="0.265075">
Sem. Input: f BOAT A IN RABBIT THE BE 1
(The rabbit&apos;s in a boat.)
</subsectionHeader>
<bodyText confidence="0.999228461538461">
where the semantic input is an unordered set of iden-
tifiers corresponding to word paradigms. Obviously
the artificial pseudo-semantic representations make
the problem much easier: we experiment with them
as a first step, somewhere between learning language
&amp;quot;from a radio&amp;quot; and providing an unambiguous tex-
tual transcription, as might be used for training a
speech recognition system.
Our goal is to create a program that, after train-
ing on many such pairs, can segment a new phonetic
utterance into a sequence of morpheme identifiers.
Such output could be used as input to many gram-
mar acquisition programs.
</bodyText>
<sectionHeader confidence="0.966311" genericHeader="method">
2 A Simple Prototype
</sectionHeader>
<bodyText confidence="0.996067571428571">
We have implemented a simple algorithm as an ex-
ploratory effort. It maintains a single dictionary, a
set of words. Each word consists of a phone sequence
and a set of sememes (semantic symbols). Initially,
the dictionary is empty. When presented with an
utterance, the algorithm goes through the following
sequence of actions:
</bodyText>
<listItem confidence="0.978745083333333">
• It attempts to cover (&amp;quot;parse&amp;quot;) the utterance
phones and semantic symbols with a sequence
of words from the dictionary, each word offset a
certain distance into the phone sequence, with
words potentially overlapping.
• It then creates new words that account for un-
covered portions of the utterance, and adjusts
words from the parse to better fit the utterance.
• Finally, it reparses the utterance with the old
dictionary and the new words, and adds the new
words to the dictionary if the resulting parse
covers the utterance well.
</listItem>
<bodyText confidence="0.999139888888889">
Occasionally, the program removes rarely-used
words from the dictionary, and removes words which
can themselves be parsed. The general operation of
the program should be made clearer by the follow-
ing two examples. In the first, the program starts
with an empty dictionary, early in the acquisition
process, and receives the simple utterance /nina/
NINA } (a child&apos;s name). Naturally, it is unable to
parse the input.
</bodyText>
<subsectionHeader confidence="0.858098">
Phones Sememes
</subsectionHeader>
<construct confidence="0.77794725">
Utterance: /nina/ { NINA }
Words:
Unparsed: /nina/ { NINA }
Mismatched:
</construct>
<bodyText confidence="0.788579428571429">
From the unparsed portion of the sentence, the
program creates a new word, /nina/ { NINA }. It
then rep arses
Utterance:
Words:
Unparsed:
Mismatched:
</bodyText>
<equation confidence="0.546546333333333">
Phones Sememes
/nina/ { NINA }
/nina/ { NINA }
</equation>
<page confidence="0.988032">
311
</page>
<bodyText confidence="0.984518947368421">
Having successfully parsed the input, it adds the
new word to the dictionary. Later in the acquisition
process, it encounters the sentence you kicked off
the sock, when the dictionary contains (among other
words) /yu/ { YOU }, /6a/ { THE }, and /rsak/ {
SOCK }.
utterance. Figure 1 contains the first 6 utterances
from the database.
We describe the results of a single run of the al-
gorithm, trained on one exposure to each of the
34438 utterances, containing a total of 2158 differ-
ent stems. The final dictionary contains 1182 words,
where some entries are different forms of a com-
mon stem. 82 of the words in the dictionary have
never been used in a good parse. We eliminate these
words, leaving 1100. Figure 2 presents some entries
in the final dictionary, and figure 3 presents all 21
(2%) of the dictionary entries that might be reason-
ably considered mistakes.
</bodyText>
<figure confidence="0.692023764705882">
Phones
Utterance: yu 1 tafaasa
/yu/
/5a/
/rsak/
iktaf
r
Sememes
KICK YOU OFF
SOCK THE }
{ YOU }
{ THE }
{ SOCK )
KICK OFF
Words:
Unparse :
Mismatched:
</figure>
<bodyText confidence="0.999431473684211">
The program creates the new word /kiktaf/ {
KICK OFF } to account for the unparsed portion of
the input, and /sak/ { SOCK } to fix the mismatched
phone. It reparses,
On this basis, it adds /Icilitaf/ { KICK OFF } and
/sak/ { SOCK } to the dictionary. /rsak/ { SOCK
}, not used in this analysis, is eventually discarded
from the dictionary for lack of use. /kiktaf/ { KICK
OFF } is later found to be parsable into two sub-
words, and also discarded.
One can view this procedure as a variant of the
expectation-maximization (Dempster et al., 1977)
procedure, with the parse of each utterance as the
hidden variables. There is currently no preference
for which words are used in a parse, save to mini-
mize mismatches and unparsed portions of the input,
but obviously a word grammar could be learned in
conjunction with this acquisition process, and used
as a disambiguation step.
</bodyText>
<sectionHeader confidence="0.990314" genericHeader="method">
3 Tests and Results
</sectionHeader>
<bodyText confidence="0.999839916666667">
To test the algorithm, we used 34438 utterances
from the Childes database of mothers&apos; speech to chil-
dren (MacWhinney and Snow, 1985; Suppes, 1973).
These text utterances were run through a publicly
available text-to-phone engine. A semantic dictio-
nary was created by hand, in which each root word
from the utterances was mapped to a correspond-
ing sememe. Various forms of a root (&amp;quot;see&amp;quot;, &amp;quot;saw&amp;quot;,
&amp;quot;seeing&amp;quot;) all map to the same sememe, e.g., SEE .
Semantic representations for a given utterance are
merely unordered sets of sememes generated by tak-
ing the union of the sememe for each word in the
</bodyText>
<figure confidence="0.996021636363636">
Phones Sememes Phones Sememes
/yu/ { YOU } /bik/ { BEAK }
/aa/ / { THE } /we/ { WAY )
lwatl { WHAT } /hi/ { HEY }
/tu/ { TO } /brik/ { BREAK }
/du/ { DO } /fiug3/ { FINGER }
/e/ { A } /kis/ { KISS }
/it/ { IT } /tap/ { TOP }
/al/ { I } /lipid/ { CALL }
/in/ { IN } /egz/ { EGG }
/wi/ { WE } /OW { THING }
</figure>
<figureCaption confidence="0.9205425">
Figure 2: Dictionary entries. The left 10 are the
10 words used most frequently in good parses. The
right 10 were selected randomly from the 1100 en-
tries.
</figureCaption>
<equation confidence="0.999075583333333">
/i0/ { BE } /nupis/ { SNOOPY }
/ig/ { YOU } /wo/ { WILL }
PO { DO } /zu/ { AT ZOO }
/fiz/ { SHE BE } /don/ { DO }
/shappin/ { HAPPEN } /self/ { YOU )
/t/{ NOT) /a/{ BE)
/skatt/ { BOB SCOTT } /snind/ { MUD }
/nidaliz/ { NEEDLE BE } /ere/ { BE }
is/tine/ { SOMETHING } /dont/ { DO NOT }
/wataraiz/ { WHAT BE THESE }
/wathappind/ { WHAT HAPPEN}
/dranA53wiz/ { DROWN OTHERWISE }
</equation>
<figureCaption confidence="0.852091">
Figure 3: All of the significant dictionary errors.
</figureCaption>
<bodyText confidence="0.996360846153846">
Some of them, like /fiz/ are conglomerations that
should have been divided. Others, like /t/, /wo/,
and /don/ demonstrate how the system compen-
sates for the morphological irregularity of English
contractions. The /in/ problem is discussed in the
text; misanalysis of the role of /ID/ also manifests
itself on something.
The most obvious error visible in figure 3 is the
suffix -ing (hg/), which should be have an empty se-
meme set. Indeed, such a word is properly hypothe-
sized but a special mechanism prevents semantically
empty words from being added to the dictionary.
Without this mechanism, the system would chance
</bodyText>
<figure confidence="0.947627705882353">
Phones
tterance: yu i tafaasa
Words: /yu/
/kikt3f/
/5a/
/sak/
unused /rsa.k/
nparse
Mismatched:
Sememes
KICK YOU OFF
SOCK THE )
{ YOU }
{ KICK OFF }
{ THE )
{ SOCK }
{ SOCK }
</figure>
<page confidence="0.968004">
312
</page>
<bodyText confidence="0.935410714285714">
Sentence Phones Sememes
this is a book. /8isizebuk/ { THIS BE A BOOK }
what do you see in the book? /watduyusimaabuk/ { WHAT DO YOU SEE IN THE BOOK 1
how many rabbits? /haumenirabbits/ { HOW MANY RABBIT }
how many? /haumeni/ { HOW MANY }
one rabbit. /wAnrabbit/ { ONE RABBIT }
what is the rabbit doing? /watiz8arabbitdunj/ { WHAT BE THE RABBIT DO }
</bodyText>
<figureCaption confidence="0.999575">
Figure 1: The first 6 utterances from the Childes database used to test the algorithm.
</figureCaption>
<bodyText confidence="0.99985715">
upon a new word like ring, /riu/, use the /Ili/ {} to
account for most of the sound, and build a new word
/r/ { RING } to cover the rest; witness something in
figure 3. Most other semantically-empty affixes (plu-
ral /s/ for instance) are also properly hypothesized
and disallowed, but the dictionary learns multiple
entries to account for them (/eg/ &amp;quot;egg&amp;quot; and /egz/
&amp;quot;eggs&amp;quot;). The system learns synonyms (&amp;quot;is&amp;quot;, &amp;quot;was&amp;quot;,
&amp;quot;am&amp;quot;, ...) and homonyms (&amp;quot;read&amp;quot;, &amp;quot;red&amp;quot;; &amp;quot;know&amp;quot;,
&amp;quot;no&amp;quot;) without difficulty.
Removing the restriction on empty semantics, and
also setting the semantics of the function words a,
an, the, that and of to 0, the most common empty
words learned are given in figure 4. The ring prob-
lem surfaces: among other words learned are now
/k/ { CAR } and /br/ { BRING }. To fix such prob-
lems, it is obvious more constraint on morpheme
order must be incorporated into the parsing pro-
cess, perhaps in the form of a statistical grammar
acquired simultaneously with the dictionary.
</bodyText>
<table confidence="0.406404">
Word Source Word Source
/ID/ 0 -ing /wo/ 0 ?
/8a/ {} the /e/ {} a
/o/ {} ? /an/ 0 an
/r/ {} you/your /ay/ 0 of
/s/ {} plural -$ /z/ {} plural -s
hi/ 0 isfs
</table>
<figureCaption confidence="0.968684">
Figure 4: The most common semantically empty
words in the final dictionary.
</figureCaption>
<sectionHeader confidence="0.995859" genericHeader="method">
4 Current Directions
</sectionHeader>
<bodyText confidence="0.9999835">
The algorithm described above is extremely simple,
as was the input fed to it. In particular,
</bodyText>
<listItem confidence="0.559488857142857">
• The input was phonetically oversimplified, each
word pronounced the same way each time it oc-
curred, regardless of environment. There was
no phonological noise and no cross-word effects.
• The semantic representations were not only
noise free and unambiguous, but corresponded
directly to the words in the utterance.
</listItem>
<bodyText confidence="0.9999678125">
To better investigate more realistic formulations
of the acquisition problem, we are extending our
coverage to actual phonetic transcriptions of speech,
by allowing for various phonological processes and
noise, and by building in probabilistic models of
morphology and syntax. We are further reducing
the information present in the semantic input by
removing all function word symbols and merging
various content symbols to encompass several word
paradigms. We hope to transition to phonemic in-
put produced by a phoneme-based speech recognizer
in the near future.
Finally, we are instituting an objective test mea-
sure: rather than examining the dictionary directly,
we will compare segmentation and morpheme-
labeling to textual transcripts of the input speech.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="conclusions">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9572125">
This research is supported by NSF grant 9211041-
ASC and ARPA under the HPCC program.
</bodyText>
<sectionHeader confidence="0.999044" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999306714285714">
Timothy Andrew Cartwright and Michael R. Brent.
1994. Segmenting speech without a lexicon: Evi-
dence for a bootstrapping model of lexical acqui-
sition. In Proc. of the 16th Annual Meeting of the
Cognitive Science Society, Hillsdale, New Jersey.
A. P. Dempster, N. M. Liard, and D. B. Rubin. 1977.
Maximum liklihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, B(39):1-38.
B. MacWhinney and C. Snow. 1985. The child lan-
guage data exchange system. Journal of Child
Language, 12:271-296.
Donald Cort Olivier. 1968. Stochastic Grammars
and Language Acquisition Mechanisms. Ph.D.
thesis, Harvard University, Cambridge, Mas-
sachusetts.
Patrick Suppes. 1973. The semantics of children&apos;s
language. American Psychologist.
J. Gerald Wolff. 1982. Language acquisition, data
compression and generalization. Language and
Communication, 2(0:57-89.
</reference>
<page confidence="0.999512">
313
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931332">
<title confidence="0.999921">Acquiring a Lexicon from Unsegmented Speech</title>
<author confidence="0.996825">Carl de_Marcken</author>
<affiliation confidence="0.999985">MIT Artificial Intelligence Laboratory</affiliation>
<address confidence="0.9998655">545 Technology Square, NE43-804 Cambridge, MA, 02139, USA</address>
<email confidence="0.999728">cgdemarc@ai.mit.edu</email>
<abstract confidence="0.994068272727273">We present work-in-progress on the machine acquisition of a lexicon from sentences that are each an unsegmented phone sequence paired with a primitive representation of meaning. A simple exploratory algorithm is described, along with the direction of current work and a discussion of the relevance of the problem for child language acquisition and computer speech recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Andrew Cartwright</author>
<author>Michael R Brent</author>
</authors>
<title>Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition.</title>
<date>1994</date>
<booktitle>In Proc. of the 16th Annual Meeting of the Cognitive Science Society,</booktitle>
<location>Hillsdale, New Jersey.</location>
<contexts>
<context position="859" citStr="Cartwright and Brent, 1994" startWordPosition="125" endWordPosition="128">uisition of a lexicon from sentences that are each an unsegmented phone sequence paired with a primitive representation of meaning. A simple exploratory algorithm is described, along with the direction of current work and a discussion of the relevance of the problem for child language acquisition and computer speech recognition. 1 Introduction We are interested in how a lexicon of discrete words can be acquired from continuous speech, a problem fundamental both to child language acquisition and to the automated induction of computer speech recognition systems; see (Olivier, 1968; Wolff, 1982; Cartwright and Brent, 1994) for previous computational work in this area. For the time being, we approximate the problem as induction from phone sequences rather than acoustic pressure, and assume that learning takes place in an environment where simple semantic representations of the speech intent are available to the acquisition mechanism. For example, we approximate the greater problem as that of learning from inputs like Phon. Input: /33rmbItsineYb3wt/ Sem. Input: f BOAT A IN RABBIT THE BE 1 (The rabbit&apos;s in a boat.) where the semantic input is an unordered set of identifiers corresponding to word paradigms. Obvious</context>
</contexts>
<marker>Cartwright, Brent, 1994</marker>
<rawString>Timothy Andrew Cartwright and Michael R. Brent. 1994. Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. In Proc. of the 16th Annual Meeting of the Cognitive Science Society, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Liard</author>
<author>D B Rubin</author>
</authors>
<title>Maximum liklihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="5111" citStr="Dempster et al., 1977" startWordPosition="855" endWordPosition="858"> OFF SOCK THE } { YOU } { THE } { SOCK ) KICK OFF Words: Unparse : Mismatched: The program creates the new word /kiktaf/ { KICK OFF } to account for the unparsed portion of the input, and /sak/ { SOCK } to fix the mismatched phone. It reparses, On this basis, it adds /Icilitaf/ { KICK OFF } and /sak/ { SOCK } to the dictionary. /rsak/ { SOCK }, not used in this analysis, is eventually discarded from the dictionary for lack of use. /kiktaf/ { KICK OFF } is later found to be parsable into two subwords, and also discarded. One can view this procedure as a variant of the expectation-maximization (Dempster et al., 1977) procedure, with the parse of each utterance as the hidden variables. There is currently no preference for which words are used in a parse, save to minimize mismatches and unparsed portions of the input, but obviously a word grammar could be learned in conjunction with this acquisition process, and used as a disambiguation step. 3 Tests and Results To test the algorithm, we used 34438 utterances from the Childes database of mothers&apos; speech to children (MacWhinney and Snow, 1985; Suppes, 1973). These text utterances were run through a publicly available text-to-phone engine. A semantic dictiona</context>
</contexts>
<marker>Dempster, Liard, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Liard, and D. B. Rubin. 1977. Maximum liklihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B(39):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
<author>C Snow</author>
</authors>
<title>The child language data exchange system.</title>
<date>1985</date>
<journal>Journal of Child Language,</journal>
<pages>12--271</pages>
<contexts>
<context position="5593" citStr="MacWhinney and Snow, 1985" startWordPosition="936" endWordPosition="939">arsable into two subwords, and also discarded. One can view this procedure as a variant of the expectation-maximization (Dempster et al., 1977) procedure, with the parse of each utterance as the hidden variables. There is currently no preference for which words are used in a parse, save to minimize mismatches and unparsed portions of the input, but obviously a word grammar could be learned in conjunction with this acquisition process, and used as a disambiguation step. 3 Tests and Results To test the algorithm, we used 34438 utterances from the Childes database of mothers&apos; speech to children (MacWhinney and Snow, 1985; Suppes, 1973). These text utterances were run through a publicly available text-to-phone engine. A semantic dictionary was created by hand, in which each root word from the utterances was mapped to a corresponding sememe. Various forms of a root (&amp;quot;see&amp;quot;, &amp;quot;saw&amp;quot;, &amp;quot;seeing&amp;quot;) all map to the same sememe, e.g., SEE . Semantic representations for a given utterance are merely unordered sets of sememes generated by taking the union of the sememe for each word in the Phones Sememes Phones Sememes /yu/ { YOU } /bik/ { BEAK } /aa/ / { THE } /we/ { WAY ) lwatl { WHAT } /hi/ { HEY } /tu/ { TO } /brik/ { BRE</context>
</contexts>
<marker>MacWhinney, Snow, 1985</marker>
<rawString>B. MacWhinney and C. Snow. 1985. The child language data exchange system. Journal of Child Language, 12:271-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Cort Olivier</author>
</authors>
<title>Stochastic Grammars and Language Acquisition Mechanisms.</title>
<date>1968</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University,</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="817" citStr="Olivier, 1968" startWordPosition="121" endWordPosition="122">-progress on the machine acquisition of a lexicon from sentences that are each an unsegmented phone sequence paired with a primitive representation of meaning. A simple exploratory algorithm is described, along with the direction of current work and a discussion of the relevance of the problem for child language acquisition and computer speech recognition. 1 Introduction We are interested in how a lexicon of discrete words can be acquired from continuous speech, a problem fundamental both to child language acquisition and to the automated induction of computer speech recognition systems; see (Olivier, 1968; Wolff, 1982; Cartwright and Brent, 1994) for previous computational work in this area. For the time being, we approximate the problem as induction from phone sequences rather than acoustic pressure, and assume that learning takes place in an environment where simple semantic representations of the speech intent are available to the acquisition mechanism. For example, we approximate the greater problem as that of learning from inputs like Phon. Input: /33rmbItsineYb3wt/ Sem. Input: f BOAT A IN RABBIT THE BE 1 (The rabbit&apos;s in a boat.) where the semantic input is an unordered set of identifier</context>
</contexts>
<marker>Olivier, 1968</marker>
<rawString>Donald Cort Olivier. 1968. Stochastic Grammars and Language Acquisition Mechanisms. Ph.D. thesis, Harvard University, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Suppes</author>
</authors>
<title>The semantics of children&apos;s language.</title>
<date>1973</date>
<publisher>American Psychologist.</publisher>
<contexts>
<context position="5608" citStr="Suppes, 1973" startWordPosition="940" endWordPosition="941">and also discarded. One can view this procedure as a variant of the expectation-maximization (Dempster et al., 1977) procedure, with the parse of each utterance as the hidden variables. There is currently no preference for which words are used in a parse, save to minimize mismatches and unparsed portions of the input, but obviously a word grammar could be learned in conjunction with this acquisition process, and used as a disambiguation step. 3 Tests and Results To test the algorithm, we used 34438 utterances from the Childes database of mothers&apos; speech to children (MacWhinney and Snow, 1985; Suppes, 1973). These text utterances were run through a publicly available text-to-phone engine. A semantic dictionary was created by hand, in which each root word from the utterances was mapped to a corresponding sememe. Various forms of a root (&amp;quot;see&amp;quot;, &amp;quot;saw&amp;quot;, &amp;quot;seeing&amp;quot;) all map to the same sememe, e.g., SEE . Semantic representations for a given utterance are merely unordered sets of sememes generated by taking the union of the sememe for each word in the Phones Sememes Phones Sememes /yu/ { YOU } /bik/ { BEAK } /aa/ / { THE } /we/ { WAY ) lwatl { WHAT } /hi/ { HEY } /tu/ { TO } /brik/ { BREAK } /du/ { DO </context>
</contexts>
<marker>Suppes, 1973</marker>
<rawString>Patrick Suppes. 1973. The semantics of children&apos;s language. American Psychologist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerald Wolff</author>
</authors>
<title>Language acquisition, data compression and generalization.</title>
<date>1982</date>
<journal>Language and Communication,</journal>
<pages>2--0</pages>
<contexts>
<context position="830" citStr="Wolff, 1982" startWordPosition="123" endWordPosition="124">e machine acquisition of a lexicon from sentences that are each an unsegmented phone sequence paired with a primitive representation of meaning. A simple exploratory algorithm is described, along with the direction of current work and a discussion of the relevance of the problem for child language acquisition and computer speech recognition. 1 Introduction We are interested in how a lexicon of discrete words can be acquired from continuous speech, a problem fundamental both to child language acquisition and to the automated induction of computer speech recognition systems; see (Olivier, 1968; Wolff, 1982; Cartwright and Brent, 1994) for previous computational work in this area. For the time being, we approximate the problem as induction from phone sequences rather than acoustic pressure, and assume that learning takes place in an environment where simple semantic representations of the speech intent are available to the acquisition mechanism. For example, we approximate the greater problem as that of learning from inputs like Phon. Input: /33rmbItsineYb3wt/ Sem. Input: f BOAT A IN RABBIT THE BE 1 (The rabbit&apos;s in a boat.) where the semantic input is an unordered set of identifiers correspondi</context>
</contexts>
<marker>Wolff, 1982</marker>
<rawString>J. Gerald Wolff. 1982. Language acquisition, data compression and generalization. Language and Communication, 2(0:57-89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>