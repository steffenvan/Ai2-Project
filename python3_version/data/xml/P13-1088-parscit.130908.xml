<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.972126">
The Role of Syntax in Vector Space Models of Compositional Semantics
</title>
<author confidence="0.988748">
Karl Moritz Hermann and Phil Blunsom
</author>
<affiliation confidence="0.9977555">
Department of Computer Science
University of Oxford
</affiliation>
<address confidence="0.996954">
Oxford, OX1 3QD, UK
</address>
<email confidence="0.999432">
{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989515">
Modelling the compositional process by
which the meaning of an utterance arises
from the meaning of its parts is a funda-
mental task of Natural Language Process-
ing. In this paper we draw upon recent
advances in the learning of vector space
representations of sentential semantics and
the transparent interface between syntax
and semantics provided by Combinatory
Categorial Grammar to introduce Com-
binatory Categorial Autoencoders. This
model leverages the CCG combinatory op-
erators to guide a non-linear transforma-
tion of meaning within a sentence. We use
this model to learn high dimensional em-
beddings for sentences and evaluate them
in a range of tasks, demonstrating that
the incorporation of syntax allows a con-
cise model to learn representations that are
both effective and general.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943086206897">
Since Frege stated his ‘Principle of Semantic
Compositionality’ in 1892 researchers have pon-
dered both how the meaning of a complex expres-
sion is determined by the meanings of its parts,
and how those parts are combined. (Frege, 1892;
Pelletier, 1994). Over a hundred years on the
choice of representational unit for this process
of compositional semantics, and how these units
combine, remain open questions.
Frege’s principle may be debatable from a lin-
guistic and philosophical standpoint, but it has
provided a basis for a range of formal approaches
to semantics which attempt to capture meaning in
logical models. The Montague grammar (Mon-
tague, 1970) is a prime example for this, build-
ing a model of composition based on lambda-
calculus and formal logic. More recent work
in this field includes the Combinatory Categorial
Grammar (CCG), which also places increased em-
phasis on syntactic coverage (Szabolcsi, 1989).
Recently those searching for the right represen-
tation for compositional semantics have drawn in-
spiration from the success of distributional mod-
els of lexical semantics. This approach represents
single words as distributional vectors, implying
that a word’s meaning is a function of the envi-
ronment it appears in, be that its syntactic role or
co-occurrences with other words (Pereira et al.,
1993; Sch¨utze, 1998). While distributional se-
mantics is easily applied to single words, spar-
sity implies that attempts to directly extract distri-
butional representations for larger expressions are
doomed to fail. Only in the past few years has
it been attempted to extend these representations
to semantic composition. Most approaches here
use the idea of vector-matrix composition to learn
larger representations from single-word encodings
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011; Socher et al., 2012b). While
these models have proved very promising for com-
positional semantics, they make minimal use of
linguistic information beyond the word level.
In this paper we bridge the gap between recent
advances in machine learning and more traditional
approaches within computational linguistics. We
achieve this goal by employing the CCG formal-
ism to consider compositional structures at any
point in a parse tree. CCG is attractive both for its
transparent interface between syntax and seman-
tics, and a small but powerful set of combinatory
operators with which we can parametrise our non-
linear transformations of compositional meaning.
We present a novel class of recursive mod-
els, the Combinatory Categorial Autoencoders
(CCAE), which marry a semantic process pro-
vided by a recursive autoencoder with the syn-
tactic representations of the CCG formalism.
Through this model we seek to answer two ques-
</bodyText>
<page confidence="0.980816">
894
</page>
<note confidence="0.9144035">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 894–904,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999376333333334">
tions: Can recursive vector space models be recon-
ciled with a more formal notion of compositional-
ity; and is there a role for syntax in guiding seman-
tics in these types of models? CCAEs make use of
CCG combinators and types by conditioning each
composition function on its equivalent step in a
CCG proof. In terms of learning complexity and
space requirements, our models strike a balance
between simpler greedy approaches (Socher et
al., 2011b) and the larger recursive vector-matrix
models (Socher et al., 2012b).
We show that this combination of state of the art
machine learning and an advanced linguistic for-
malism translates into concise models with com-
petitive performance on a variety of tasks. In both
sentiment and compound similarity experiments
we show that our CCAE models match or better
comparable recursive autoencoder models.1
</bodyText>
<sectionHeader confidence="0.997101" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999822">
There exist a number of formal approaches to lan-
guage that provide mechanisms for composition-
ality. Generative Grammars (Jackendoff, 1972)
treat semantics, and thus compositionality, essen-
tially as an extension of syntax, with the generative
(syntactic) process yielding a structure that can be
interpreted semantically. By contrast Montague
grammar achieves greater separation between the
semantic and the syntactic by using lambda calcu-
lus to express meaning. However, this greater sep-
aration between surface form and meaning comes
at a price in the form of reduced computability.
While this is beyond the scope of this paper, see
e.g. Kracht (2008) for a detailed analysis of com-
positionality in these formalisms.
</bodyText>
<subsectionHeader confidence="0.975467">
2.1 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.998299">
In this paper we focus on CCG, a linguistically
expressive yet computationally efficient grammar
formalism. It uses a constituency-based structure
with complex syntactic types (categories) from
which sentences can be deduced using a small
number of combinators. CCG relies on combi-
natory logic (as opposed to lambda calculus) to
build its expressions. For a detailed introduction
and analysis vis-`a-vis other grammar formalisms
see e.g. Steedman and Baldridge (2011).
CCG has been described as having a transpar-
ent surface between the syntactic and the seman-
</bodyText>
<footnote confidence="0.9689825">
1A C++ implementation of our models is available at
http://www.karlmoritz.com/
</footnote>
<figure confidence="0.9262856">
Tina likes tigers
N (S[dcl]\NP)/NP N
NP NP
S[dcl]\NP
S[dcl]
</figure>
<figureCaption confidence="0.9827655">
Figure 1: CCG derivation for Tina likes tigers with
forward (&gt;) and backward application (&lt;).
</figureCaption>
<bodyText confidence="0.999726821428572">
tic. It is this property which makes it attractive
for our purposes of providing a conditioning struc-
ture for semantic operators. A second benefit of
the formalism is that it is designed with computa-
tional efficiency in mind. While one could debate
the relative merits of various linguistic formalisms
the existence of mature tools and resources, such
as the CCGBank (Hockenmaier and Steedman,
2007), the Groningen Meaning Bank (Basile et al.,
2012) and the C&amp;C Tools (Curran et al., 2007) is
another big advantage for CCG.
CCG’s transparent surface stems from its cate-
gorial property: Each point in a derivation corre-
sponds directly to an interpretable category. These
categories (or types) associated with each term in a
CCG govern how this term can be combined with
other terms in a larger structure, implicitly making
them semantically expressive.
For instance in Figure 1, the word likes has type
(S[dcl]\NP)/NP, which means that it first looks
for a type NP to its right hand side. Subsequently
the expression likes tigers (as type S[dcl]\NP) re-
quires a second NP on its left. The final type of
the phrase S[dcl] indicates a sentence and hence a
complete CCG proof. Thus at each point in a CCG
parse we can deduce the possible next steps in the
derivation by considering the available types and
combinatory rules.
</bodyText>
<subsectionHeader confidence="0.998037">
2.2 Vector Space Models of Semantics
</subsectionHeader>
<bodyText confidence="0.997223833333333">
Vector-based approaches for semantic tasks have
become increasingly popular in recent years.
Distributional representations encode an ex-
pression by its environment, assuming the context-
dependent nature of meaning according to which
one “shall know a word by the company it keeps”
(Firth, 1957). Effectively this is usually achieved
by considering the co-occurrence with other words
in large corpora or the syntactic roles a word per-
forms.
Distributional representations are frequently
used to encode single words as vectors. Such rep-
</bodyText>
<page confidence="0.998085">
895
</page>
<bodyText confidence="0.999878918918919">
resentations have then successfully been applied
to a number of tasks including word sense disam-
biguation (Sch¨utze, 1998) and selectional prefer-
ence (Pereira et al., 1993; Lin, 1999).
While it is theoretically possible to apply the
same mechanism to larger expressions, sparsity
prevents learning meaningful distributional repre-
sentations for expressions much larger than single
words.2
Vector space models of compositional seman-
tics aim to fill this gap by providing a methodol-
ogy for deriving the representation of an expres-
sion from those of its parts. While distributional
representations frequently serve to encode single
words in such approaches this is not a strict re-
quirement.
There are a number of ideas on how to de-
fine composition in such vector spaces. A gen-
eral framework for semantic vector composition
was proposed in Mitchell and Lapata (2008), with
Mitchell and Lapata (2010) and more recently Bla-
coe and Lapata (2012) providing good overviews
of this topic. Notable approaches to this issue in-
clude Baroni and Zamparelli (2010), who com-
pose nouns and adjectives by representing them as
vectors and matrices, respectively, with the com-
positional representation achieved by multiplica-
tion. Grefenstette and Sadrzadeh (2011) use a sim-
ilar approach with matrices for relational words
and vectors for arguments. These two approaches
are combined in Grefenstette et al. (2013), produc-
ing a tensor-based semantic framework with ten-
sor contraction as composition operation.
Another set of models that have very success-
fully been applied in this area are recursive autoen-
coders (Socher et al., 2011a; Socher et al., 2011b),
which are discussed in the next section.
</bodyText>
<subsectionHeader confidence="0.998731">
2.3 Recursive Autoencoders
</subsectionHeader>
<bodyText confidence="0.9998915">
Autoencoders are a useful tool to compress in-
formation. One can think of an autoencoder
as a funnel through which information has to
pass (see Figure 2). By forcing the autoencoder
to reconstruct an input given only the reduced
amount of information available inside the funnel
it serves as a compression tool, representing high-
dimensional objects in a lower-dimensional space.
Typically a given autoencoder, that is the func-
tions for encoding and reconstructing data, are
</bodyText>
<footnote confidence="0.996545333333333">
2The experimental setup in (Baroni and Zamparelli, 2010)
is one of the few examples where distributional representa-
tions are used for word pairs.
</footnote>
<figureCaption confidence="0.994372">
Figure 2: A simple three-layer autoencoder. The
</figureCaption>
<bodyText confidence="0.999515133333333">
input represented by the vector at the bottom is
being encoded in a smaller vector (middle), from
which it is then reconstructed (top) into the same
dimensionality as the original input vector.
used on multiple inputs. By optimizing the two
functions to minimize the difference between all
inputs and their respective reconstructions, this au-
toencoder will effectively discover some hidden
structures within the data that can be exploited to
represent it more efficiently.
As a simple example, assume input vectors
xi E Rn, i E (0..N), weight matrices Wenc E
R(m×n),Wrec E R(n×m) and biases benc E Rm,
brec E Rn. The encoding matrix and bias are used
to create an encoding ei from xi:
</bodyText>
<equation confidence="0.7736624">
ei = fenc(xi) = Wencxi + benc (1)
Subsequently e E Rm is used to reconstruct x as
x0 using the reconstruction matrix and bias:
x0 i = frec(ei) = Wrecei + brec (2)
θ = (Wenc, Wrec, benc, brec) can then be learned
</equation>
<bodyText confidence="0.993243">
by minimizing the error function describing the
difference between x0 and x:
</bodyText>
<equation confidence="0.9946975">
� �
�x0 i − xi �2 (3)
</equation>
<bodyText confidence="0.990691636363636">
Now, if m &lt; n, this will intuitively lead to ei
encoding a latent structure contained in xi and
shared across all xj, j E (0..N), with θ encoding
and decoding to and from that hidden structure.
It is possible to apply multiple autoencoders on
top of each other, creating a deep autoencoder
(Bengio et al., 2007; Hinton and Salakhutdinov,
2006). For such a multi-layered model to learn
anything beyond what a single layer could learn, a
non-linear transformation g needs to be applied at
each layer. Usually, a variant of the sigmoid (σ)
</bodyText>
<equation confidence="0.891777">
1 N
E= 2 i
</equation>
<page confidence="0.981769">
896
</page>
<figureCaption confidence="0.993739">
Figure 3: RAE with three inputs. Vectors with
filled (blue) circles represent input and hidden
units; blanks (white) denote reconstruction layers.
</figureCaption>
<bodyText confidence="0.372132">
or hyperbolic tangent (tanh) function is used for
g (LeCun et al., 1998).
</bodyText>
<equation confidence="0.9948625">
fenc(xi) = g (W encxi + benc) (4)
frec(ei) = g (Wrecei + brec)
</equation>
<bodyText confidence="0.998303333333333">
Furthermore, autoencoders can easily be used as
a composition function by concatenating two input
vectors, such that:
</bodyText>
<equation confidence="0.986721">
e = f(x1, x2) = g (W(x1Ix2) + b) (5)
(x&apos;2) = g (W&apos;e + b&apos;)
1Ix&apos;
</equation>
<bodyText confidence="0.999644545454546">
Extending this idea, recursive autoencoders (RAE)
allow the modelling of data of variable size. By
setting the n = 2m, it is possible to recursively
combine a structure into an autoencoder tree. See
Figure 3 for an example, where x1, x2, x3 are re-
cursively encoded into y2.
The recursive application of autoencoders was
first introduced in Pollack (1990), whose recursive
auto-associative memories learn vector represen-
tations over pre-specified recursive data structures.
More recently this idea was extended and applied
to dynamic structures (Socher et al., 2011b).
These types of models have become increas-
ingly prominent since developments within the
field of Deep Learning have made the training
of such hierarchical structures more effective and
tractable (LeCun et al., 1998; Hinton et al., 2006).
Intuitively the top layer of an RAE will encode
aspects of the information stored in all of the input
vectors. Previously, RAE have successfully been
applied to a number of tasks including sentiment
analysis, paraphrase detection, relation extraction
</bodyText>
<table confidence="0.8658214">
Model CCG Elements
CCAE-A parse
CCAE-B parse + rules
CCAE-C parse + rules + types
CCAE-D parse + rules + child types
</table>
<tableCaption confidence="0.598125">
Table 1: Aspects of the CCG formalism used by
the different models explored in this paper.
and 3D object identification (Blacoe and Lapata,
2012; Socher et al., 2011b; Socher et al., 2012a).
</tableCaption>
<sectionHeader confidence="0.991152" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999981931034483">
The models in this paper combine the power of
recursive, vector-based models with the linguistic
intuition of the CCG formalism. Their purpose is
to learn semantically meaningful vector represen-
tations for sentences and phrases of variable size,
while the purpose of this paper is to investigate
the use of syntax and linguistic formalisms in such
vector-based compositional models.
We assume a CCG parse to be given. Let C de-
note the set of combinatory rules, and T the set
of categories used, respectively. We use the parse
tree to structure an RAE, so that each combina-
tory step is represented by an autoencoder func-
tion. We refer to these models Categorial Com-
binatory Autoencoders (CCAE). In total this pa-
per describes four models making increasing use
of the CCG formalism (see table 1).
As an internal baseline we use model CCAE-
A, which is an RAE structured along a CCG parse
tree. CCAE-A uses a single weight matrix each for
the encoding and reconstruction step (see Table 2.
This model is similar to Socher et al. (2011b), ex-
cept that we use a fixed structure in place of the
greedy tree building approach. As CCAE-A uses
only minimal syntactic guidance, this should al-
low us to better ascertain to what degree the use of
syntax helps our semantic models.
Our second model (CCAE-B) uses the compo-
sition function in equation (6), with c E C.
</bodyText>
<equation confidence="0.9528005">
fenc(x, y, c) = g (Wcenc(xIy) + bcenc) (6)
frec(e, c) = g (Wcrece + bcrec)
</equation>
<bodyText confidence="0.999689333333333">
This means that for every combinatory rule we de-
fine an equivalent autoencoder composition func-
tion by parametrizing both the weight matrix and
bias on the combinatory rule (e.g. Figure 4).
In this model, as in the following ones, we as-
sume a reconstruction step symmetric with the
</bodyText>
<page confidence="0.970616">
897
</page>
<table confidence="0.425262">
Model Encoding Function
CCAE-A f(x, y)= g (W (x�y) + b)
CCAE-B f(x, y, c)= g (Wc(xIly) + bc)
~� ~
CCAE-C f(x, y, c, t)= g p∈{c,t} (Wp(x�y) + bp)
CCAE-D f(x, y, c, tx, ty)= g (Wc (Wtxx + Wtyy~ + bc~
</table>
<tableCaption confidence="0.747991">
Table 2: Encoding functions of the four CCAE models discussed in this paper.
</tableCaption>
<equation confidence="0.996279666666667">
α : X/Y β : Y g(W &gt;
&gt; enc(α�β) + b&gt;enc)
αβ : X
</equation>
<figureCaption confidence="0.997963666666667">
Figure 4: Forward application as CCG combinator
and autoencoder rule respectively.
Figure 5: CCAE-B applied to Tina likes tigers.
</figureCaption>
<bodyText confidence="0.998108966666667">
Next to each vector are the CCG category (top)
and the word or function representing it (bottom).
lex describes the unary type-changing operation.
&gt; and &lt; are forward and backward application.
composition step. For the remainder of this paper
we will focus on the composition step and drop the
use of enc and rec in variable names where it isn’t
explicitly required. Figure 5 shows model CCAE-
B applied to our previous example sentence.
While CCAE-B uses only the combinatory
rules, we want to make fuller use of the linguis-
tic information available in CCG. For this pur-
pose, we build another model CCAE-C, which
parametrizes on both the combinatory rule c E C
and the CCG category t E T at every step (see
Figure 2). This model provides an additional de-
gree of insight, as the categories T are semanti-
cally and syntactically more expressive than the
CCG combinatory rules by themselves. Summing
over weights parametrised on c and t respectively,
adds an additional degree of freedom and also al-
lows for some model smoothing.
An alternative approach is encoded in model
CCAE-D. Here we consider the categories not of
the element represented, but of the elements it is
generated from together with the combinatory rule
applied to them. The intuition is that in the first
step we transform two expressions based on their
syntax. Subsequently we combine these two con-
ditioned on their joint combinatory rule.
</bodyText>
<sectionHeader confidence="0.996944" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.999975454545455">
In this section we briefly discuss unsupervised
learning for our models. Subsequently we de-
scribe how these models can be extended to allow
for semi-supervised training and evaluation.
Let θ = (W, B, L) be our model parameters
and λ a vector with regularization parameters for
all model parameters. W represents the set of all
weight matrices, B the set of all biases and L the
set of all word vectors. Let N be the set of training
data consisting of tree-nodes n with inputs xn, yn
and reconstruction rn. The error given n is:
</bodyText>
<equation confidence="0.99105075">
� �
1 � �
E(n|θ) = �rn − (xn�yn) �
2
</equation>
<bodyText confidence="0.999930625">
The gradient of the regularised objective func-
tion then becomes:
We learn the gradient using backpropagation
through structure (Goller and K¨uchler, 1996), and
minimize the objective function using L-BFGS.
For more details about the partial derivatives
used for backpropagation, see the documentation
accompanying our model implementation.3
</bodyText>
<equation confidence="0.973520583333333">
3http://www.karlmoritz.com/
∂J
∂θ =
N
1
N
n
∂E(n|θ) θ
∂θ + λ(8)
2
i
(7)
</equation>
<page confidence="0.984645">
898
</page>
<subsectionHeader confidence="0.982122">
4.1 Supervised Learning
</subsectionHeader>
<bodyText confidence="0.9999354">
The unsupervised method described so far learns
a vector representation for each sentence. Such a
representation can be useful for some tasks such as
paraphrase detection, but is not sufficient for other
tasks such as sentiment classification, which we
are considering in this paper.
In order to extract sentiment from our models,
we extend them by adding a supervised classifier
on top, using the learned representations v as input
for a binary classification model:
</bodyText>
<equation confidence="0.835754">
pred(l=1|v, θ) = sigmoid(Wlabel v + blabel) (9)
</equation>
<bodyText confidence="0.924734">
Given our corpus of CCG parses with label pairs
(N, l), the new objective function becomes:
</bodyText>
<equation confidence="0.9997675">
J = N
1 X E(N,l, θ) + 2 ||θ||2 (10)
λ
(N,l)
Assuming each node n E N contains children
xn, yn, encoding en and reconstruction rn, so that
n = {x, y, e, r} this breaks down into:
E(N,l, θ) = (11)
X αErec (n, θ) + (1−α)Elbl(en, l, θ)
n∈N
� �
1 �Erec (n, θ) = 2 [xn I yn] − rn 2 (12)
Elbl(e,l, θ) = 2 �l − e�2
1 (13)
</equation>
<bodyText confidence="0.999791333333333">
This method of introducing a supervised aspect
to the autoencoder largely follows the model de-
scribed in Socher et al. (2011b).
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999985583333333">
We describe a number of standard evaluations
to determine the comparative performance of our
model. The first task of sentiment analysis allows
us to compare our CCG-conditioned RAE with
similar, existing models. In a second experiment,
we apply our model to a compound similarity eval-
uation, which allows us to evaluate our models
against a larger class of vector-based models (Bla-
coe and Lapata, 2012). We conclude with some
qualitative analysis to get a better idea of whether
the combination of CCG and RAE can learn se-
mantically expressive embeddings.
In our experiments we use the hyperbolic tan-
gent as nonlinearity g. Unless stated otherwise we
use word-vectors of size 50, initialized using the
embeddings provided by Turian et al. (2010) based
on the model of Collobert and Weston (2008).4
We use the C&amp;C parser (Clark and Curran,
2007) to generate CCG parse trees for the data
used in our experiments. For models CCAE-C and
CCAE-D we use the 25 most frequent CCG cate-
gories (as extracted from the British National Cor-
pus) with an additional general weight matrix in
order to catch all remaining types.
</bodyText>
<subsectionHeader confidence="0.998785">
5.1 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.981694457142857">
We evaluate our model on the MPQA opinion
corpus (Wiebe et al., 2005), which annotates ex-
pressions for sentiment.5 The corpus consists of
10,624 instances with approximately 70 percent
describing a negative sentiment. We apply the
same pre-processing as (Nakagawa et al., 2010)
and (Socher et al., 2011b) by using an additional
sentiment lexicon (Wilson et al., 2005) during the
model training for this experiment.
As a second corpus we make use of the sentence
polarity (SP) dataset v1.0 (Pang and Lee, 2005).6
This dataset consists of 10662 sentences extracted
from movie reviews which are manually labelled
with positive or negative sentiment and equally
distributed across sentiment.
Experiment 1: Semi-Supervised Training In
the first experiment, we use the semi-supervised
training strategy described previously and initial-
ize our models with the embeddings provided by
Turian et al. (2010). The results of this evalua-
tion are in Table 3. While we achieve the best per-
formance on the MPQA corpus, the results on the
SP corpus are less convincing. Perhaps surpris-
ingly, the simplest model CCAE-A outperforms
the other models on this dataset.
When considering the two datasets, sparsity
seems a likely explanation for this difference in
results: In the MPQA experiment most instances
are very short with an average length of 3 words,
while the average sentence length in the SP corpus
is 21 words. The MPQA task is further simplified
through the use or an additional sentiment lexicon.
Considering dictionary size, the SP corpus has a
dictionary of 22k words, more than three times the
size of the MPQA dictionary.
</bodyText>
<footnote confidence="0.996941">
4http://www.metaoptimize.com/projects/
wordreprs/
5http://mpqa.cs.pitt.edu/
6http://www.cs.cornell.edu/people/
pabo/movie-review-data/
</footnote>
<page confidence="0.99666">
899
</page>
<note confidence="0.674660428571429">
Method MPQA SP
Voting with two lexica 81.7 63.1
MV-RNN (Socher et al., 2012b) - 79.0
RAE (rand) (Socher et al., 2011b) 85.7 76.8
TCRF (Nakagawa et al., 2010) 86.1 77.3
RAE (init) (Socher et al., 2011b) 86.4 77.7
NB (Wang and Manning, 2012) 86.7 79.4
</note>
<table confidence="0.9750425">
CCAE-A 86.3 77.8
CCAE-B 87.1 77.1
CCAE-C 87.1 77.3
CCAE-D 87.2 76.7
</table>
<tableCaption confidence="0.997882">
Table 3: Accuracy of sentiment classification on
</tableCaption>
<bodyText confidence="0.993044054054054">
the sentiment polarity (SP) and MPQA datasets.
For NB we only display the best result among a
larger group of models analysed in that paper.
This issue of sparsity is exacerbated in the more
complex CCAE models, where the training points
are spread across different CCG types and rules.
While the initialization of the word vectors with
previously learned embeddings (as was previously
shown by Socher et al. (2011b)) helps the mod-
els, all other model variables such as composition
weights and biases are still initialised randomly
and thus highly dependent on the amount of train-
ing data available.
Experiment 2: Pretraining Due to our analy-
sis of the results of the initial experiment, we ran a
second series of experiments on the SP corpus. We
follow (Scheible and Sch¨utze, 2013) for this sec-
ond series of experiments, which are carried out on
a random 90/10 training-testing split, with some
data reserved for development.
Instead of initialising the model with external
word embeddings, we first train it on a large
amount of data with the aim of overcoming the
sparsity issues encountered in the previous exper-
iment. Learning is thus divided into two steps:
The first, unsupervised training phase, uses the
British National Corpus together with the SP cor-
pus. In this phase only the reconstruction signal
is used to learn word embeddings and transforma-
tion matrices. Subsequently, in the second phase,
only the SP corpus is used, this time with both the
reconstruction and the label error.
By learning word embeddings and composition
matrices on more data, the model is likely to gen-
eralise better. Particularly for the more complex
models, where the composition functions are con-
ditioned on various CCG parameters, this should
</bodyText>
<table confidence="0.861764833333333">
Training
Model Regular Pretraining
CCAE-A 77.8 79.5
CCAE-B 76.9 79.8
CCAE-C 77.1 81.0
CCAE-D 76.9 79.7
</table>
<tableCaption confidence="0.983121">
Table 4: Effect of pretraining on model perfor-
</tableCaption>
<bodyText confidence="0.987510846153846">
mance on the SP dataset. Results are reported on a
random subsection of the SP corpus; thus numbers
for the regular training method differ slightly from
those in Table 3.
help to overcome issues of sparsity.
If we consider the results of the pre-trained ex-
periments in Table 4, this seems to be the case.
In fact, the trend of the previous results has been
reversed, with the more complex models now per-
forming best, whereas in the previous experiments
the simpler models performed better. Using the
Turian embeddings instead of random initialisa-
tion did not improve results in this setup.
</bodyText>
<subsectionHeader confidence="0.999541">
5.2 Compound Similarity
</subsectionHeader>
<bodyText confidence="0.99979256">
In a second experiment we use the dataset from
Mitchell and Lapata (2010) which contains sim-
ilarity judgements for adjective-noun, noun-noun
and verb-object pairs.7 All compound pairs have
been ranked for semantic similarity by a number of
human annotators. The task is thus to rank these
pairs of word pairs by their semantic similarity.
For instance, the two compounds vast amount
and large quantity are given a high similarity score
by the human judges, while northern region and
early age are assigned no similarity at all.
We train our models as fully unsupervised au-
toencoders on the British National Corpus for this
task. We assume fixed parse trees for all of the
compounds (Figure 6), and use these to compute
compound level vectors for all word pairs. We
subsequently use the cosine distance between each
compound pair as our similarity measure. We
use Spearman’s rank correlation coefficient (ρ) for
evaluation; hence there is no need to rescale our
scores (-1.0 – 1.0) to the original scale (1.0 – 7.0).
Blacoe and Lapata (2012) have an extensive
comparison of the performance of various vector-
based models on this data set to which we compare
our model in Table 5. The CCAE models outper-
</bodyText>
<footnote confidence="0.986104">
7http://homepages.inf.ed.ac.uk/mlap/
resources/index.html
</footnote>
<page confidence="0.987882">
900
</page>
<figure confidence="0.997229230769231">
N
N
S\NP
Verb Object
VB NN
(S\NP)/NP N
NP
Noun Noun
NN NN
N/N N
Adjective Noun
JJ NN
N/N N
</figure>
<figureCaption confidence="0.997844">
Figure 6: Assumed CCG parse structure for the compound similarity evaluation.
</figureCaption>
<table confidence="0.9993865">
Method Adj-N N-N V-Obj
Human 0.52 0.49 0.55
(Blacoe and Lapata, 2012)
O/+ 0.21 - 0.48 0.22 - 0.50 0.18 - 0.35
RAE 0.19 - 0.31 0.24 - 0.30 0.09 - 0.28
CCAE-B 0.38 0.44 0.34
CCAE-C 0.38 0.41 0.23
CCAE-D 0.41 0.44 0.29
</table>
<tableCaption confidence="0.996395">
Table 5: Correlation coefficients of model predic-
</tableCaption>
<bodyText confidence="0.925713333333333">
tions for the compound similarity task. Numbers
show Spearman’s rank correlation coefficient (ρ).
Higher numbers indicate better correlation.
form the RAE models provided by Blacoe and La-
pata (2012), and score towards the upper end of the
range of other models considered in that paper.
</bodyText>
<subsectionHeader confidence="0.999479">
5.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.988193833333333">
To get better insight into our models we also per-
form a small qualitative analysis. Using one of the
models trained on the MPQA corpus, we gener-
ate word-level representations of all phrases in this
corpus and subsequently identify the most related
expressions by using the cosine distance measure.
We perform this experiment on all expressions of
length 5, considering all expressions with a word
length between 3 and 7 as potential matches.
As can be seen in Table 6, this works with vary-
ing success. Linking expressions such as convey-
ing the message ofpeace and safeguard(ing) peace
and security suggests that the model does learn
some form of semantics.
On the other hand, the connection between ex-
pressed their satisfaction and support and ex-
pressed their admiration and surprise suggests
that the pure word level content still has an impact
on the model analysis. Likewise, the expressions
is a story of success and is a staunch supporter
have some lexical but little semantic overlap. Fur-
ther reducing this link between the lexical and the
semantic representation is an issue that should be
addressed in future work in this area.
</bodyText>
<sectionHeader confidence="0.998575" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999905853658536">
Overall, our models compare favourably with the
state of the art. On the MPQA corpus model
CCAE-D achieves the best published results we
are aware of, whereas on the SP corpus we achieve
competitive results. With an additional, unsuper-
vised training step we achieved results beyond the
current state of the art on this task, too.
Semantics The qualitative analysis and the ex-
periment on compounds demonstrate that the
CCAE models are capable of learning semantics.
An advantage of our approach—and of autoen-
coders generally—is their ability to learn in an
unsupervised setting. The pre-training step for
the sentiment task was essentially the same train-
ing step as used in the compound similarity task.
While other models such as the MV-RNN (Socher
et al., 2012b) achieve good results on a particu-
lar task, they do not allow unsupervised training.
This prevents the possiblity of pretraining, which
we showed to have a big impact on results, and fur-
ther prevents the training of general models: The
CCAE models can be used for multiple tasks with-
out the need to re-train the main model.
Complexity Previously in this paper we argued
that our models combined the strengths of other
approaches. By using a grammar formalism we
increase the expressive power of the model while
the complexity remains low. For the complex-
ity analysis see Table 7. We strike a balance be-
tween the greedy approaches (e.g. Socher et al.
(2011b)), where learning is quadratic in the length
of each sentence and existing syntax-driven ap-
proaches such as the MV-RNN of Socher et al.
(2012b), where the size of the model, that is the
number of variables that needs to be learned, is
quadratic in the size of the word-embeddings.
Sparsity Parametrizing on CCG types and rules
increases the size of the model compared to a
greedy RAE (Socher et al., 2011b). The effect
of this was highlighted by the sentiment analysis
task, with the more complex models performing
</bodyText>
<page confidence="0.983503">
901
</page>
<figure confidence="0.818046">
Expression
Most Similar
</figure>
<bodyText confidence="0.96389915">
convey the message of peace
keep alight the flame of
has a reason to repent
a significant and successful strike
it is reassuring to believe
expressed their satisfaction and support
is a story of success
are lining up to condemn
more sanctions should be imposed
could fray the bilateral goodwill
safeguard peace and security
keep up the hope
has no right
a much better position
it is a positive development
expressed their admiration and surprise
is a staunch supporter
are going to voice their concerns
charges being leveled
could cause serious damage
</bodyText>
<tableCaption confidence="0.990105">
Table 6: Phrases from the MPQA corpus and their semantically closest match according to CCAE-D.
</tableCaption>
<table confidence="0.8239058">
Complexity
Model Size Learning
MV-RNN 0(nw2) O(l)
RAE 0(nw) O(l2)
CCAE-* 0(nw) O(l)
</table>
<tableCaption confidence="0.980875">
Table 7: Comparison of models. n is dictionary
</tableCaption>
<bodyText confidence="0.998321037037037">
size, w embedding width, l is sentence length. We
can assume l » n » w. Additional factors such
as CCG rules and types are treated as small con-
stants for the purposes of this analysis.
worse in comparison with the simpler ones. We
were able to overcome this issue by using addi-
tional training data. Beyond this, it would also be
interesting to investigate the relationships between
different types and to derive functions to incorpo-
rate this into the learning procedure. For instance
model learning could be adjusted to enforce some
mirroring effects between the weight matrices of
forward and backward application, or to support
similarities between those of forward application
and composition.
CCG-Vector Interface Exactly how the infor-
mation contained in a CCG derivation is best ap-
plied to a vector space model of compositionality
is another issue for future research. Our investi-
gation of this matter by exploring different model
setups has proved somewhat inconclusive. While
CCAE-D incorporated the deepest conditioning on
the CCG structure, it did not decisively outperform
the simpler CCAE-B which just conditioned on
the combinatory operators. Issues of sparsity, as
shown in our experiments on pretraining, have a
significant influence, which requires further study.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999775185185185">
In this paper we have brought a more formal no-
tion of semantic compositionality to vector space
models based on recursive autoencoders. This was
achieved through the use of the CCG formalism
to provide a conditioning structure for the matrix
vector products that define the RAE.
We have explored a number of models, each of
which conditions the compositional operations on
different aspects of the CCG derivation. Our ex-
perimental findings indicate a clear advantage for
a deeper integration of syntax over models that use
only the bracketing structure of the parse tree.
The most effective way to condition the compo-
sitional operators on the syntax remains unclear.
Once the issue of sparsity had been addressed, the
complex models outperformed the simpler ones.
Among the complex models, however, we could
not establish significant or consistent differences
to convincingly argue for a particular approach.
While the connections between formal linguis-
tics and vector space approaches to NLP may not
be immediately obvious, we believe that there is a
case for the continued investigation of ways to best
combine these two schools of thought. This paper
represents one step towards the reconciliation of
traditional formal approaches to compositional se-
mantics with modern machine learning.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999988333333333">
We thank the anonymous reviewers for their feed-
back and Richard Socher for providing additional
insight into his models. Karl Moritz would further
like to thank Sebastian Riedel for hosting him at
UCL while this paper was written. This work has
been supported by the EPSRC.
</bodyText>
<page confidence="0.993845">
902
</page>
<sectionHeader confidence="0.996288" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998410533980582">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of LREC, pages
3196–3200, Istanbul, Turkey.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems 19, pages 153–160.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP-CoNLL,
pages 546–556.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. CL, 33(4):493–552, December.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&amp;c
and boxer. In Proceedings of ACL Demo and Poster
Sessions, pages 33–36.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
55. 1952-59:1–32.
Gottfried Frege. 1892. ¨Uber Sinn und Bedeutung. In
Mark Textor, editor, Funktion - Begriff - Bedeutung,
volume 4 of Sammlung Philosophie. Vandenhoeck
&amp; Ruprecht, G¨ottingen.
Christoph Goller and Andreas K¨uchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Proceedings
of the ICNN-96, pages 347–352. IEEE.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394–1404.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504–507.
Geoffrey E. Hinton, Simon Osindero, Max Welling,
and Yee Whye Teh. 2006. Unsupervised discovery
of nonlinear structure using contrastive backpropa-
gation. Cognitive Science, 30(4):725–731.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
CL, 33(3):355–396, September.
Ray Jackendoff. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge, MA.
Marcus Kracht. 2008. Compositionality in Montague
Grammar. In Edouard Machery und Markus Wern-
ing Wolfram Hinzen, editor, Handbook of Composi-
tionality, pages 47 – 63. Oxford University Press.
Yann LeCun, Leon Bottou, Genevieve Orr, and Klaus-
Robert Muller. 1998. Efficient backprop. In G. Orr
and Muller K., editors, Neural Networks: Tricks of
the trade. Springer.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL,
pages 317–324.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings
ofACL, pages 236–244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
R. Montague. 1970. Universal grammar. Theoria,
36(3):373–398.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In NAACL-
HLT, pages 786–794.
Bo Pang and Lillian Lee. 2005. Seeing stars: exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings ofACL,
pages 115–124.
Francis Jeffry Pelletier. 1994. The principle of seman-
tic compositionality. Topoi, 13:11–24.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings ofACL, ACL ’93, pages 183–190.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46:77–105.
Christian Scheible and Hinrich Sch¨utze. 2013. Cutting
recursive autoencoder trees. In Proceedings of the
International Conference on Learning Representa-
tions.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. CL, 24(1):97–123, March.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011a.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in
Neural Information Processing Systems 24.
</reference>
<page confidence="0.991396">
903
</page>
<reference confidence="0.999275815789474">
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151–161.
Richard Socher, Brody Huval, Bharath Bhat, Christo-
pher D. Manning, and Andrew Y. Ng. 2012a.
Convolutional-Recursive Deep Learning for 3D Ob-
ject Classification. In Advances in Neural Informa-
tion Processing Systems 25.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012b. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of EMNLP-CoNLL, pages 1201–
1211.
Mark Steedman and Jason Baldridge, 2011. Combina-
tory Categorial Grammar, pages 181–224. Wiley-
Blackwell.
Anna Szabolcsi. 1989. Bound Variables in Syntax:
Are There Any? In Renate Bartsch, Johan van Ben-
them, and Peter van Emde Boas, editors, Semantics
and Contextual Expression, pages 295–318. Foris,
Dordrecht.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384–394.
Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: simple, good sentiment and topic
classification. In Proceedings of ACL, pages 90–94.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of EMNLP-
HLT, HLT ’05, pages 347–354.
</reference>
<page confidence="0.998556">
904
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.704696">
<title confidence="0.999045">The Role of Syntax in Vector Space Models of Compositional Semantics</title>
<author confidence="0.999974">Moritz Hermann</author>
<affiliation confidence="0.9982635">Department of Computer University of</affiliation>
<address confidence="0.719631">Oxford, OX1 3QD,</address>
<abstract confidence="0.998481523809524">Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="2820" citStr="Baroni and Zamparelli, 2010" startWordPosition="428" endWordPosition="431">mplying that a word’s meaning is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition. Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012b). While these models have proved very promising for compositional semantics, they make minimal use of linguistic information beyond the word level. In this paper we bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics. We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree. CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operato</context>
<context position="9305" citStr="Baroni and Zamparelli (2010)" startWordPosition="1443" endWordPosition="1446">ional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another set of models that have very successfully been applied in this area are recursive autoencoders (Socher et al., 2011a; Socher et al., 2011b), which are di</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valerio Basile</author>
<author>Johan Bos</author>
<author>Kilian Evang</author>
<author>Noortje Venhuizen</author>
</authors>
<title>Developing a large semantically annotated corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>3196--3200</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="6813" citStr="Basile et al., 2012" startWordPosition="1043" endWordPosition="1046">.karlmoritz.com/ Tina likes tigers N (S[dcl]\NP)/NP N NP NP S[dcl]\NP S[dcl] Figure 1: CCG derivation for Tina likes tigers with forward (&gt;) and backward application (&lt;). tic. It is this property which makes it attractive for our purposes of providing a conditioning structure for semantic operators. A second benefit of the formalism is that it is designed with computational efficiency in mind. While one could debate the relative merits of various linguistic formalisms the existence of mature tools and resources, such as the CCGBank (Hockenmaier and Steedman, 2007), the Groningen Meaning Bank (Basile et al., 2012) and the C&amp;C Tools (Curran et al., 2007) is another big advantage for CCG. CCG’s transparent surface stems from its categorial property: Each point in a derivation corresponds directly to an interpretable category. These categories (or types) associated with each term in a CCG govern how this term can be combined with other terms in a larger structure, implicitly making them semantically expressive. For instance in Figure 1, the word likes has type (S[dcl]\NP)/NP, which means that it first looks for a type NP to its right hand side. Subsequently the expression likes tigers (as type S[dcl]\NP) </context>
</contexts>
<marker>Basile, Bos, Evang, Venhuizen, 2012</marker>
<rawString>Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012. Developing a large semantically annotated corpus. In Proceedings of LREC, pages 3196–3200, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Pascal Lamblin</author>
<author>Dan Popovici</author>
<author>Hugo Larochelle</author>
</authors>
<title>Greedy layer-wise training of deep networks.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19,</booktitle>
<pages>153--160</pages>
<contexts>
<context position="11934" citStr="Bengio et al., 2007" startWordPosition="1884" endWordPosition="1887"> = fenc(xi) = Wencxi + benc (1) Subsequently e E Rm is used to reconstruct x as x0 using the reconstruction matrix and bias: x0 i = frec(ei) = Wrecei + brec (2) θ = (Wenc, Wrec, benc, brec) can then be learned by minimizing the error function describing the difference between x0 and x: � � �x0 i − xi �2 (3) Now, if m &lt; n, this will intuitively lead to ei encoding a latent structure contained in xi and shared across all xj, j E (0..N), with θ encoding and decoding to and from that hidden structure. It is possible to apply multiple autoencoders on top of each other, creating a deep autoencoder (Bengio et al., 2007; Hinton and Salakhutdinov, 2006). For such a multi-layered model to learn anything beyond what a single layer could learn, a non-linear transformation g needs to be applied at each layer. Usually, a variant of the sigmoid (σ) 1 N E= 2 i 896 Figure 3: RAE with three inputs. Vectors with filled (blue) circles represent input and hidden units; blanks (white) denote reconstruction layers. or hyperbolic tangent (tanh) function is used for g (LeCun et al., 1998). fenc(xi) = g (W encxi + benc) (4) frec(ei) = g (Wrecei + brec) Furthermore, autoencoders can easily be used as a composition function by </context>
</contexts>
<marker>Bengio, Lamblin, Popovici, Larochelle, 2007</marker>
<rawString>Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. 2007. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems 19, pages 153–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>546--556</pages>
<contexts>
<context position="9195" citStr="Blacoe and Lapata (2012)" startWordPosition="1425" endWordPosition="1429">ributional representations for expressions much larger than single words.2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another set of models that have very successfully b</context>
<context position="13958" citStr="Blacoe and Lapata, 2012" startWordPosition="2217" endWordPosition="2220">aining of such hierarchical structures more effective and tractable (LeCun et al., 1998; Hinton et al., 2006). Intuitively the top layer of an RAE will encode aspects of the information stored in all of the input vectors. Previously, RAE have successfully been applied to a number of tasks including sentiment analysis, paraphrase detection, relation extraction Model CCG Elements CCAE-A parse CCAE-B parse + rules CCAE-C parse + rules + types CCAE-D parse + rules + child types Table 1: Aspects of the CCG formalism used by the different models explored in this paper. and 3D object identification (Blacoe and Lapata, 2012; Socher et al., 2011b; Socher et al., 2012a). 3 Model The models in this paper combine the power of recursive, vector-based models with the linguistic intuition of the CCG formalism. Their purpose is to learn semantically meaningful vector representations for sentences and phrases of variable size, while the purpose of this paper is to investigate the use of syntax and linguistic formalisms in such vector-based compositional models. We assume a CCG parse to be given. Let C denote the set of combinatory rules, and T the set of categories used, respectively. We use the parse tree to structure a</context>
<context position="20065" citStr="Blacoe and Lapata, 2012" startWordPosition="3294" endWordPosition="3298"> � � 1 �Erec (n, θ) = 2 [xn I yn] − rn 2 (12) Elbl(e,l, θ) = 2 �l − e�2 1 (13) This method of introducing a supervised aspect to the autoencoder largely follows the model described in Socher et al. (2011b). 5 Experiments We describe a number of standard evaluations to determine the comparative performance of our model. The first task of sentiment analysis allows us to compare our CCG-conditioned RAE with similar, existing models. In a second experiment, we apply our model to a compound similarity evaluation, which allows us to evaluate our models against a larger class of vector-based models (Blacoe and Lapata, 2012). We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings. In our experiments we use the hyperbolic tangent as nonlinearity g. Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al. (2010) based on the model of Collobert and Weston (2008).4 We use the C&amp;C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from </context>
<context position="26460" citStr="Blacoe and Lapata (2012)" startWordPosition="4336" endWordPosition="4339">gh similarity score by the human judges, while northern region and early age are assigned no similarity at all. We train our models as fully unsupervised autoencoders on the British National Corpus for this task. We assume fixed parse trees for all of the compounds (Figure 6), and use these to compute compound level vectors for all word pairs. We subsequently use the cosine distance between each compound pair as our similarity measure. We use Spearman’s rank correlation coefficient (ρ) for evaluation; hence there is no need to rescale our scores (-1.0 – 1.0) to the original scale (1.0 – 7.0). Blacoe and Lapata (2012) have an extensive comparison of the performance of various vectorbased models on this data set to which we compare our model in Table 5. The CCAE models outper7http://homepages.inf.ed.ac.uk/mlap/ resources/index.html 900 N N S\NP Verb Object VB NN (S\NP)/NP N NP Noun Noun NN NN N/N N Adjective Noun JJ NN N/N N Figure 6: Assumed CCG parse structure for the compound similarity evaluation. Method Adj-N N-N V-Obj Human 0.52 0.49 0.55 (Blacoe and Lapata, 2012) O/+ 0.21 - 0.48 0.22 - 0.50 0.18 - 0.35 RAE 0.19 - 0.31 0.24 - 0.30 0.09 - 0.28 CCAE-B 0.38 0.44 0.34 CCAE-C 0.38 0.41 0.23 CCAE-D 0.41 0.4</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of EMNLP-CoNLL, pages 546–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with ccg and log-linear models.</title>
<date>2007</date>
<journal>CL,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="20507" citStr="Clark and Curran, 2007" startWordPosition="3369" endWordPosition="3372">nd experiment, we apply our model to a compound similarity evaluation, which allows us to evaluate our models against a larger class of vector-based models (Blacoe and Lapata, 2012). We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings. In our experiments we use the hyperbolic tangent as nonlinearity g. Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al. (2010) based on the model of Collobert and Weston (2008).4 We use the C&amp;C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by us</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with ccg and log-linear models. CL, 33(4):493–552, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="20458" citStr="Collobert and Weston (2008)" startWordPosition="3360" endWordPosition="3363">ditioned RAE with similar, existing models. In a second experiment, we apply our model to a compound similarity evaluation, which allows us to evaluate our models against a larger class of vector-based models (Blacoe and Lapata, 2012). We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings. In our experiments we use the hyperbolic tangent as nonlinearity g. Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al. (2010) based on the model of Collobert and Weston (2008).4 We use the C&amp;C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakaga</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically motivated large-scale nlp with c&amp;c and boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL Demo and Poster Sessions,</booktitle>
<pages>33--36</pages>
<contexts>
<context position="6853" citStr="Curran et al., 2007" startWordPosition="1051" endWordPosition="1054">dcl]\NP)/NP N NP NP S[dcl]\NP S[dcl] Figure 1: CCG derivation for Tina likes tigers with forward (&gt;) and backward application (&lt;). tic. It is this property which makes it attractive for our purposes of providing a conditioning structure for semantic operators. A second benefit of the formalism is that it is designed with computational efficiency in mind. While one could debate the relative merits of various linguistic formalisms the existence of mature tools and resources, such as the CCGBank (Hockenmaier and Steedman, 2007), the Groningen Meaning Bank (Basile et al., 2012) and the C&amp;C Tools (Curran et al., 2007) is another big advantage for CCG. CCG’s transparent surface stems from its categorial property: Each point in a derivation corresponds directly to an interpretable category. These categories (or types) associated with each term in a CCG govern how this term can be combined with other terms in a larger structure, implicitly making them semantically expressive. For instance in Figure 1, the word likes has type (S[dcl]\NP)/NP, which means that it first looks for a type NP to its right hand side. Subsequently the expression likes tigers (as type S[dcl]\NP) requires a second NP on its left. The fi</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale nlp with c&amp;c and boxer. In Proceedings of ACL Demo and Poster Sessions, pages 33–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory</title>
<date>1957</date>
<pages>1930--55</pages>
<contexts>
<context position="8014" citStr="Firth, 1957" startWordPosition="1243" endWordPosition="1244">S[dcl]\NP) requires a second NP on its left. The final type of the phrase S[dcl] indicates a sentence and hence a complete CCG proof. Thus at each point in a CCG parse we can deduce the possible next steps in the derivation by considering the available types and combinatory rules. 2.2 Vector Space Models of Semantics Vector-based approaches for semantic tasks have become increasingly popular in recent years. Distributional representations encode an expression by its environment, assuming the contextdependent nature of meaning according to which one “shall know a word by the company it keeps” (Firth, 1957). Effectively this is usually achieved by considering the co-occurrence with other words in large corpora or the syntactic roles a word performs. Distributional representations are frequently used to encode single words as vectors. Such rep895 resentations have then successfully been applied to a number of tasks including word sense disambiguation (Sch¨utze, 1998) and selectional preference (Pereira et al., 1993; Lin, 1999). While it is theoretically possible to apply the same mechanism to larger expressions, sparsity prevents learning meaningful distributional representations for expressions </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. 1957. A synopsis of linguistic theory 1930-55. 1952-59:1–32.</rawString>
</citation>
<citation valid="false">
<title>Uber Sinn und Bedeutung.</title>
<volume>4</volume>
<editor>In Mark Textor, editor, Funktion - Begriff - Bedeutung,</editor>
<marker></marker>
<rawString>Gottfried Frege. 1892. ¨Uber Sinn und Bedeutung. In Mark Textor, editor, Funktion - Begriff - Bedeutung, volume 4 of Sammlung Philosophie. Vandenhoeck &amp; Ruprecht, G¨ottingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas K¨uchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Proceedings of the ICNN-96,</booktitle>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>Christoph Goller and Andreas K¨uchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Proceedings of the ICNN-96, pages 347–352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1394--1404</pages>
<contexts>
<context position="2854" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="432" endWordPosition="435"> is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition. Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012b). While these models have proved very promising for compositional semantics, they make minimal use of linguistic information beyond the word level. In this paper we bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics. We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree. CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operators with which we can parametrise o</context>
<context position="9499" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="1470" endWordPosition="1473">ve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another set of models that have very successfully been applied in this area are recursive autoencoders (Socher et al., 2011a; Socher et al., 2011b), which are discussed in the next section. 2.3 Recursive Autoencoders Autoencoders are a useful tool to compress information. One can think of an autoencoder as a funnel through which information has to pass </context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of EMNLP, pages 1394–1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<contexts>
<context position="9648" citStr="Grefenstette et al. (2013)" startWordPosition="1494" endWordPosition="1497">s. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another set of models that have very successfully been applied in this area are recursive autoencoders (Socher et al., 2011a; Socher et al., 2011b), which are discussed in the next section. 2.3 Recursive Autoencoders Autoencoders are a useful tool to compress information. One can think of an autoencoder as a funnel through which information has to pass (see Figure 2). By forcing the autoencoder to reconstruct an input given only the reduced amount of information available inside the funnel it serves</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>R R Salakhutdinov</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<volume>313</volume>
<issue>5786</issue>
<contexts>
<context position="11967" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="1888" endWordPosition="1891">+ benc (1) Subsequently e E Rm is used to reconstruct x as x0 using the reconstruction matrix and bias: x0 i = frec(ei) = Wrecei + brec (2) θ = (Wenc, Wrec, benc, brec) can then be learned by minimizing the error function describing the difference between x0 and x: � � �x0 i − xi �2 (3) Now, if m &lt; n, this will intuitively lead to ei encoding a latent structure contained in xi and shared across all xj, j E (0..N), with θ encoding and decoding to and from that hidden structure. It is possible to apply multiple autoencoders on top of each other, creating a deep autoencoder (Bengio et al., 2007; Hinton and Salakhutdinov, 2006). For such a multi-layered model to learn anything beyond what a single layer could learn, a non-linear transformation g needs to be applied at each layer. Usually, a variant of the sigmoid (σ) 1 N E= 2 i 896 Figure 3: RAE with three inputs. Vectors with filled (blue) circles represent input and hidden units; blanks (white) denote reconstruction layers. or hyperbolic tangent (tanh) function is used for g (LeCun et al., 1998). fenc(xi) = g (W encxi + benc) (4) frec(ei) = g (Wrecei + brec) Furthermore, autoencoders can easily be used as a composition function by concatenating two input vectors, </context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Simon Osindero</author>
<author>Max Welling</author>
<author>Yee Whye Teh</author>
</authors>
<title>Unsupervised discovery of nonlinear structure using contrastive backpropagation.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="13444" citStr="Hinton et al., 2006" startWordPosition="2133" endWordPosition="2136">coder tree. See Figure 3 for an example, where x1, x2, x3 are recursively encoded into y2. The recursive application of autoencoders was first introduced in Pollack (1990), whose recursive auto-associative memories learn vector representations over pre-specified recursive data structures. More recently this idea was extended and applied to dynamic structures (Socher et al., 2011b). These types of models have become increasingly prominent since developments within the field of Deep Learning have made the training of such hierarchical structures more effective and tractable (LeCun et al., 1998; Hinton et al., 2006). Intuitively the top layer of an RAE will encode aspects of the information stored in all of the input vectors. Previously, RAE have successfully been applied to a number of tasks including sentiment analysis, paraphrase detection, relation extraction Model CCG Elements CCAE-A parse CCAE-B parse + rules CCAE-C parse + rules + types CCAE-D parse + rules + child types Table 1: Aspects of the CCG formalism used by the different models explored in this paper. and 3D object identification (Blacoe and Lapata, 2012; Socher et al., 2011b; Socher et al., 2012a). 3 Model The models in this paper combin</context>
</contexts>
<marker>Hinton, Osindero, Welling, Teh, 2006</marker>
<rawString>Geoffrey E. Hinton, Simon Osindero, Max Welling, and Yee Whye Teh. 2006. Unsupervised discovery of nonlinear structure using contrastive backpropagation. Cognitive Science, 30(4):725–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>CL,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="6763" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1035" endWordPosition="1038">A C++ implementation of our models is available at http://www.karlmoritz.com/ Tina likes tigers N (S[dcl]\NP)/NP N NP NP S[dcl]\NP S[dcl] Figure 1: CCG derivation for Tina likes tigers with forward (&gt;) and backward application (&lt;). tic. It is this property which makes it attractive for our purposes of providing a conditioning structure for semantic operators. A second benefit of the formalism is that it is designed with computational efficiency in mind. While one could debate the relative merits of various linguistic formalisms the existence of mature tools and resources, such as the CCGBank (Hockenmaier and Steedman, 2007), the Groningen Meaning Bank (Basile et al., 2012) and the C&amp;C Tools (Curran et al., 2007) is another big advantage for CCG. CCG’s transparent surface stems from its categorial property: Each point in a derivation corresponds directly to an interpretable category. These categories (or types) associated with each term in a CCG govern how this term can be combined with other terms in a larger structure, implicitly making them semantically expressive. For instance in Figure 1, the word likes has type (S[dcl]\NP)/NP, which means that it first looks for a type NP to its right hand side. Subsequentl</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. CL, 33(3):355–396, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Interpretation in Generative Grammar.</title>
<date>1972</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4958" citStr="Jackendoff, 1972" startWordPosition="761" endWordPosition="762"> a balance between simpler greedy approaches (Socher et al., 2011b) and the larger recursive vector-matrix models (Socher et al., 2012b). We show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks. In both sentiment and compound similarity experiments we show that our CCAE models match or better comparable recursive autoencoder models.1 2 Background There exist a number of formal approaches to language that provide mechanisms for compositionality. Generative Grammars (Jackendoff, 1972) treat semantics, and thus compositionality, essentially as an extension of syntax, with the generative (syntactic) process yielding a structure that can be interpreted semantically. By contrast Montague grammar achieves greater separation between the semantic and the syntactic by using lambda calculus to express meaning. However, this greater separation between surface form and meaning comes at a price in the form of reduced computability. While this is beyond the scope of this paper, see e.g. Kracht (2008) for a detailed analysis of compositionality in these formalisms. 2.1 Combinatory Categ</context>
</contexts>
<marker>Jackendoff, 1972</marker>
<rawString>Ray Jackendoff. 1972. Semantic Interpretation in Generative Grammar. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Kracht</author>
</authors>
<title>Compositionality in Montague Grammar.</title>
<date>2008</date>
<booktitle>In Edouard Machery und Markus Werning Wolfram Hinzen, editor, Handbook of Compositionality,</booktitle>
<pages>47--63</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="5471" citStr="Kracht (2008)" startWordPosition="840" endWordPosition="841">hes to language that provide mechanisms for compositionality. Generative Grammars (Jackendoff, 1972) treat semantics, and thus compositionality, essentially as an extension of syntax, with the generative (syntactic) process yielding a structure that can be interpreted semantically. By contrast Montague grammar achieves greater separation between the semantic and the syntactic by using lambda calculus to express meaning. However, this greater separation between surface form and meaning comes at a price in the form of reduced computability. While this is beyond the scope of this paper, see e.g. Kracht (2008) for a detailed analysis of compositionality in these formalisms. 2.1 Combinatory Categorial Grammar In this paper we focus on CCG, a linguistically expressive yet computationally efficient grammar formalism. It uses a constituency-based structure with complex syntactic types (categories) from which sentences can be deduced using a small number of combinators. CCG relies on combinatory logic (as opposed to lambda calculus) to build its expressions. For a detailed introduction and analysis vis-`a-vis other grammar formalisms see e.g. Steedman and Baldridge (2011). CCG has been described as havi</context>
</contexts>
<marker>Kracht, 2008</marker>
<rawString>Marcus Kracht. 2008. Compositionality in Montague Grammar. In Edouard Machery und Markus Werning Wolfram Hinzen, editor, Handbook of Compositionality, pages 47 – 63. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>Leon Bottou</author>
<author>Genevieve Orr</author>
<author>KlausRobert Muller</author>
</authors>
<title>Efficient backprop.</title>
<date>1998</date>
<booktitle>Neural Networks: Tricks of the trade.</booktitle>
<editor>In G. Orr and Muller K., editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="12395" citStr="LeCun et al., 1998" startWordPosition="1961" endWordPosition="1964">coding to and from that hidden structure. It is possible to apply multiple autoencoders on top of each other, creating a deep autoencoder (Bengio et al., 2007; Hinton and Salakhutdinov, 2006). For such a multi-layered model to learn anything beyond what a single layer could learn, a non-linear transformation g needs to be applied at each layer. Usually, a variant of the sigmoid (σ) 1 N E= 2 i 896 Figure 3: RAE with three inputs. Vectors with filled (blue) circles represent input and hidden units; blanks (white) denote reconstruction layers. or hyperbolic tangent (tanh) function is used for g (LeCun et al., 1998). fenc(xi) = g (W encxi + benc) (4) frec(ei) = g (Wrecei + brec) Furthermore, autoencoders can easily be used as a composition function by concatenating two input vectors, such that: e = f(x1, x2) = g (W(x1Ix2) + b) (5) (x&apos;2) = g (W&apos;e + b&apos;) 1Ix&apos; Extending this idea, recursive autoencoders (RAE) allow the modelling of data of variable size. By setting the n = 2m, it is possible to recursively combine a structure into an autoencoder tree. See Figure 3 for an example, where x1, x2, x3 are recursively encoded into y2. The recursive application of autoencoders was first introduced in Pollack (1990)</context>
</contexts>
<marker>LeCun, Bottou, Orr, Muller, 1998</marker>
<rawString>Yann LeCun, Leon Bottou, Genevieve Orr, and KlausRobert Muller. 1998. Efficient backprop. In G. Orr and Muller K., editors, Neural Networks: Tricks of the trade. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>317--324</pages>
<contexts>
<context position="8441" citStr="Lin, 1999" startWordPosition="1308" endWordPosition="1309">presentations encode an expression by its environment, assuming the contextdependent nature of meaning according to which one “shall know a word by the company it keeps” (Firth, 1957). Effectively this is usually achieved by considering the co-occurrence with other words in large corpora or the syntactic roles a word performs. Distributional representations are frequently used to encode single words as vectors. Such rep895 resentations have then successfully been applied to a number of tasks including word sense disambiguation (Sch¨utze, 1998) and selectional preference (Pereira et al., 1993; Lin, 1999). While it is theoretically possible to apply the same mechanism to larger expressions, sparsity prevents learning meaningful distributional representations for expressions much larger than single words.2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framew</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of ACL, pages 317–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In In Proceedings ofACL,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="9119" citStr="Mitchell and Lapata (2008)" startWordPosition="1413" endWordPosition="1416">me mechanism to larger expressions, sparsity prevents learning meaningful distributional representations for expressions much larger than single words.2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction a</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In In Proceedings ofACL, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="9152" citStr="Mitchell and Lapata (2010)" startWordPosition="1418" endWordPosition="1421">s, sparsity prevents learning meaningful distributional representations for expressions much larger than single words.2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another </context>
<context position="25493" citStr="Mitchell and Lapata (2010)" startWordPosition="4176" endWordPosition="4179">section of the SP corpus; thus numbers for the regular training method differ slightly from those in Table 3. help to overcome issues of sparsity. If we consider the results of the pre-trained experiments in Table 4, this seems to be the case. In fact, the trend of the previous results has been reversed, with the more complex models now performing best, whereas in the previous experiments the simpler models performed better. Using the Turian embeddings instead of random initialisation did not improve results in this setup. 5.2 Compound Similarity In a second experiment we use the dataset from Mitchell and Lapata (2010) which contains similarity judgements for adjective-noun, noun-noun and verb-object pairs.7 All compound pairs have been ranked for semantic similarity by a number of human annotators. The task is thus to rank these pairs of word pairs by their semantic similarity. For instance, the two compounds vast amount and large quantity are given a high similarity score by the human judges, while northern region and early age are assigned no similarity at all. We train our models as fully unsupervised autoencoders on the British National Corpus for this task. We assume fixed parse trees for all of the c</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>Universal grammar.</title>
<date>1970</date>
<journal>Theoria,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="1694" citStr="Montague, 1970" startWordPosition="258" endWordPosition="260">ntic Compositionality’ in 1892 researchers have pondered both how the meaning of a complex expression is determined by the meanings of its parts, and how those parts are combined. (Frege, 1892; Pelletier, 1994). Over a hundred years on the choice of representational unit for this process of compositional semantics, and how these units combine, remain open questions. Frege’s principle may be debatable from a linguistic and philosophical standpoint, but it has provided a basis for a range of formal approaches to semantics which attempt to capture meaning in logical models. The Montague grammar (Montague, 1970) is a prime example for this, building a model of composition based on lambdacalculus and formal logic. More recent work in this field includes the Combinatory Categorial Grammar (CCG), which also places increased emphasis on syntactic coverage (Szabolcsi, 1989). Recently those searching for the right representation for compositional semantics have drawn inspiration from the success of distributional models of lexical semantics. This approach represents single words as distributional vectors, implying that a word’s meaning is a function of the environment it appears in, be that its syntactic r</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>R. Montague. 1970. Universal grammar. Theoria, 36(3):373–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using crfs with hidden variables.</title>
<date>2010</date>
<booktitle>In NAACLHLT,</booktitle>
<pages>786--794</pages>
<contexts>
<context position="21074" citStr="Nakagawa et al., 2010" startWordPosition="3462" endWordPosition="3465">(2008).4 We use the C&amp;C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. Experiment 1: Semi-Supervised Training In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by T</context>
<context position="22712" citStr="Nakagawa et al., 2010" startWordPosition="3714" endWordPosition="3717">ery short with an average length of 3 words, while the average sentence length in the SP corpus is 21 words. The MPQA task is further simplified through the use or an additional sentiment lexicon. Considering dictionary size, the SP corpus has a dictionary of 22k words, more than three times the size of the MPQA dictionary. 4http://www.metaoptimize.com/projects/ wordreprs/ 5http://mpqa.cs.pitt.edu/ 6http://www.cs.cornell.edu/people/ pabo/movie-review-data/ 899 Method MPQA SP Voting with two lexica 81.7 63.1 MV-RNN (Socher et al., 2012b) - 79.0 RAE (rand) (Socher et al., 2011b) 85.7 76.8 TCRF (Nakagawa et al., 2010) 86.1 77.3 RAE (init) (Socher et al., 2011b) 86.4 77.7 NB (Wang and Manning, 2012) 86.7 79.4 CCAE-A 86.3 77.8 CCAE-B 87.1 77.1 CCAE-C 87.1 77.3 CCAE-D 87.2 76.7 Table 3: Accuracy of sentiment classification on the sentiment polarity (SP) and MPQA datasets. For NB we only display the best result among a larger group of models analysed in that paper. This issue of sparsity is exacerbated in the more complex CCAE models, where the training points are spread across different CCG types and rules. While the initialization of the word vectors with previously learned embeddings (as was previously show</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using crfs with hidden variables. In NAACLHLT, pages 786–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="21306" citStr="Pang and Lee, 2005" startWordPosition="3502" endWordPosition="3505">orpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. Experiment 1: Semi-Supervised Training In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al. (2010). The results of this evaluation are in Table 3. While we achieve the best performance on the MPQA corpus, the results on the SP corpus are less convincing. Perhaps surprisingly, the simplest model CCAE-A outperfo</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings ofACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Jeffry Pelletier</author>
</authors>
<title>The principle of semantic compositionality.</title>
<date>1994</date>
<tech>Topoi,</tech>
<pages>13--11</pages>
<contexts>
<context position="1289" citStr="Pelletier, 1994" startWordPosition="195" endWordPosition="196">el leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general. 1 Introduction Since Frege stated his ‘Principle of Semantic Compositionality’ in 1892 researchers have pondered both how the meaning of a complex expression is determined by the meanings of its parts, and how those parts are combined. (Frege, 1892; Pelletier, 1994). Over a hundred years on the choice of representational unit for this process of compositional semantics, and how these units combine, remain open questions. Frege’s principle may be debatable from a linguistic and philosophical standpoint, but it has provided a basis for a range of formal approaches to semantics which attempt to capture meaning in logical models. The Montague grammar (Montague, 1970) is a prime example for this, building a model of composition based on lambdacalculus and formal logic. More recent work in this field includes the Combinatory Categorial Grammar (CCG), which als</context>
</contexts>
<marker>Pelletier, 1994</marker>
<rawString>Francis Jeffry Pelletier. 1994. The principle of semantic compositionality. Topoi, 13:11–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings ofACL, ACL ’93,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="2354" citStr="Pereira et al., 1993" startWordPosition="361" endWordPosition="364"> model of composition based on lambdacalculus and formal logic. More recent work in this field includes the Combinatory Categorial Grammar (CCG), which also places increased emphasis on syntactic coverage (Szabolcsi, 1989). Recently those searching for the right representation for compositional semantics have drawn inspiration from the success of distributional models of lexical semantics. This approach represents single words as distributional vectors, implying that a word’s meaning is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition. Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012b). While these models have proved very promising for compositional semantics, </context>
<context position="8429" citStr="Pereira et al., 1993" startWordPosition="1304" endWordPosition="1307">ars. Distributional representations encode an expression by its environment, assuming the contextdependent nature of meaning according to which one “shall know a word by the company it keeps” (Firth, 1957). Effectively this is usually achieved by considering the co-occurrence with other words in large corpora or the syntactic roles a word performs. Distributional representations are frequently used to encode single words as vectors. Such rep895 resentations have then successfully been applied to a number of tasks including word sense disambiguation (Sch¨utze, 1998) and selectional preference (Pereira et al., 1993; Lin, 1999). While it is theoretically possible to apply the same mechanism to larger expressions, sparsity prevents learning meaningful distributional representations for expressions much larger than single words.2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A ge</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In Proceedings ofACL, ACL ’93, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--77</pages>
<contexts>
<context position="12995" citStr="Pollack (1990)" startWordPosition="2070" endWordPosition="2071"> et al., 1998). fenc(xi) = g (W encxi + benc) (4) frec(ei) = g (Wrecei + brec) Furthermore, autoencoders can easily be used as a composition function by concatenating two input vectors, such that: e = f(x1, x2) = g (W(x1Ix2) + b) (5) (x&apos;2) = g (W&apos;e + b&apos;) 1Ix&apos; Extending this idea, recursive autoencoders (RAE) allow the modelling of data of variable size. By setting the n = 2m, it is possible to recursively combine a structure into an autoencoder tree. See Figure 3 for an example, where x1, x2, x3 are recursively encoded into y2. The recursive application of autoencoders was first introduced in Pollack (1990), whose recursive auto-associative memories learn vector representations over pre-specified recursive data structures. More recently this idea was extended and applied to dynamic structures (Socher et al., 2011b). These types of models have become increasingly prominent since developments within the field of Deep Learning have made the training of such hierarchical structures more effective and tractable (LeCun et al., 1998; Hinton et al., 2006). Intuitively the top layer of an RAE will encode aspects of the information stored in all of the input vectors. Previously, RAE have successfully been</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Jordan B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46:77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Scheible</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Cutting recursive autoencoder trees.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Learning Representations.</booktitle>
<marker>Scheible, Sch¨utze, 2013</marker>
<rawString>Christian Scheible and Hinrich Sch¨utze. 2013. Cutting recursive autoencoder trees. In Proceedings of the International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>CL,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. CL, 24(1):97–123, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24.</booktitle>
<contexts>
<context position="4406" citStr="Socher et al., 2011" startWordPosition="676" endWordPosition="679"> of the 51st Annual Meeting of the Association for Computational Linguistics, pages 894–904, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions: Can recursive vector space models be reconciled with a more formal notion of compositionality; and is there a role for syntax in guiding semantics in these types of models? CCAEs make use of CCG combinators and types by conditioning each composition function on its equivalent step in a CCG proof. In terms of learning complexity and space requirements, our models strike a balance between simpler greedy approaches (Socher et al., 2011b) and the larger recursive vector-matrix models (Socher et al., 2012b). We show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks. In both sentiment and compound similarity experiments we show that our CCAE models match or better comparable recursive autoencoder models.1 2 Background There exist a number of formal approaches to language that provide mechanisms for compositionality. Generative Grammars (Jackendoff, 1972) treat semantics, and thus compositionality, ess</context>
<context position="9867" citStr="Socher et al., 2011" startWordPosition="1529" endWordPosition="1532">oaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another set of models that have very successfully been applied in this area are recursive autoencoders (Socher et al., 2011a; Socher et al., 2011b), which are discussed in the next section. 2.3 Recursive Autoencoders Autoencoders are a useful tool to compress information. One can think of an autoencoder as a funnel through which information has to pass (see Figure 2). By forcing the autoencoder to reconstruct an input given only the reduced amount of information available inside the funnel it serves as a compression tool, representing highdimensional objects in a lower-dimensional space. Typically a given autoencoder, that is the functions for encoding and reconstructing data, are 2The experimental setup in (Baron</context>
<context position="13205" citStr="Socher et al., 2011" startWordPosition="2096" endWordPosition="2099">, x2) = g (W(x1Ix2) + b) (5) (x&apos;2) = g (W&apos;e + b&apos;) 1Ix&apos; Extending this idea, recursive autoencoders (RAE) allow the modelling of data of variable size. By setting the n = 2m, it is possible to recursively combine a structure into an autoencoder tree. See Figure 3 for an example, where x1, x2, x3 are recursively encoded into y2. The recursive application of autoencoders was first introduced in Pollack (1990), whose recursive auto-associative memories learn vector representations over pre-specified recursive data structures. More recently this idea was extended and applied to dynamic structures (Socher et al., 2011b). These types of models have become increasingly prominent since developments within the field of Deep Learning have made the training of such hierarchical structures more effective and tractable (LeCun et al., 1998; Hinton et al., 2006). Intuitively the top layer of an RAE will encode aspects of the information stored in all of the input vectors. Previously, RAE have successfully been applied to a number of tasks including sentiment analysis, paraphrase detection, relation extraction Model CCG Elements CCAE-A parse CCAE-B parse + rules CCAE-C parse + rules + types CCAE-D parse + rules + chi</context>
<context position="15041" citStr="Socher et al. (2011" startWordPosition="2404" endWordPosition="2407">e given. Let C denote the set of combinatory rules, and T the set of categories used, respectively. We use the parse tree to structure an RAE, so that each combinatory step is represented by an autoencoder function. We refer to these models Categorial Combinatory Autoencoders (CCAE). In total this paper describes four models making increasing use of the CCG formalism (see table 1). As an internal baseline we use model CCAEA, which is an RAE structured along a CCG parse tree. CCAE-A uses a single weight matrix each for the encoding and reconstruction step (see Table 2. This model is similar to Socher et al. (2011b), except that we use a fixed structure in place of the greedy tree building approach. As CCAE-A uses only minimal syntactic guidance, this should allow us to better ascertain to what degree the use of syntax helps our semantic models. Our second model (CCAE-B) uses the composition function in equation (6), with c E C. fenc(x, y, c) = g (Wcenc(xIy) + bcenc) (6) frec(e, c) = g (Wcrece + bcrec) This means that for every combinatory rule we define an equivalent autoencoder composition function by parametrizing both the weight matrix and bias on the combinatory rule (e.g. Figure 4). In this model</context>
<context position="19644" citStr="Socher et al. (2011" startWordPosition="3228" endWordPosition="3231">input for a binary classification model: pred(l=1|v, θ) = sigmoid(Wlabel v + blabel) (9) Given our corpus of CCG parses with label pairs (N, l), the new objective function becomes: J = N 1 X E(N,l, θ) + 2 ||θ||2 (10) λ (N,l) Assuming each node n E N contains children xn, yn, encoding en and reconstruction rn, so that n = {x, y, e, r} this breaks down into: E(N,l, θ) = (11) X αErec (n, θ) + (1−α)Elbl(en, l, θ) n∈N � � 1 �Erec (n, θ) = 2 [xn I yn] − rn 2 (12) Elbl(e,l, θ) = 2 �l − e�2 1 (13) This method of introducing a supervised aspect to the autoencoder largely follows the model described in Socher et al. (2011b). 5 Experiments We describe a number of standard evaluations to determine the comparative performance of our model. The first task of sentiment analysis allows us to compare our CCG-conditioned RAE with similar, existing models. In a second experiment, we apply our model to a compound similarity evaluation, which allows us to evaluate our models against a larger class of vector-based models (Blacoe and Lapata, 2012). We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings. In our experiments we use</context>
<context position="21099" citStr="Socher et al., 2011" startWordPosition="3467" endWordPosition="3470">er (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. Experiment 1: Semi-Supervised Training In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al. (2010). The </context>
<context position="22671" citStr="Socher et al., 2011" startWordPosition="3707" endWordPosition="3710">e MPQA experiment most instances are very short with an average length of 3 words, while the average sentence length in the SP corpus is 21 words. The MPQA task is further simplified through the use or an additional sentiment lexicon. Considering dictionary size, the SP corpus has a dictionary of 22k words, more than three times the size of the MPQA dictionary. 4http://www.metaoptimize.com/projects/ wordreprs/ 5http://mpqa.cs.pitt.edu/ 6http://www.cs.cornell.edu/people/ pabo/movie-review-data/ 899 Method MPQA SP Voting with two lexica 81.7 63.1 MV-RNN (Socher et al., 2012b) - 79.0 RAE (rand) (Socher et al., 2011b) 85.7 76.8 TCRF (Nakagawa et al., 2010) 86.1 77.3 RAE (init) (Socher et al., 2011b) 86.4 77.7 NB (Wang and Manning, 2012) 86.7 79.4 CCAE-A 86.3 77.8 CCAE-B 87.1 77.1 CCAE-C 87.1 77.3 CCAE-D 87.2 76.7 Table 3: Accuracy of sentiment classification on the sentiment polarity (SP) and MPQA datasets. For NB we only display the best result among a larger group of models analysed in that paper. This issue of sparsity is exacerbated in the more complex CCAE models, where the training points are spread across different CCG types and rules. While the initialization of the word vectors with previously l</context>
<context position="29995" citStr="Socher et al. (2011" startWordPosition="4930" endWordPosition="4933">r task, they do not allow unsupervised training. This prevents the possiblity of pretraining, which we showed to have a big impact on results, and further prevents the training of general models: The CCAE models can be used for multiple tasks without the need to re-train the main model. Complexity Previously in this paper we argued that our models combined the strengths of other approaches. By using a grammar formalism we increase the expressive power of the model while the complexity remains low. For the complexity analysis see Table 7. We strike a balance between the greedy approaches (e.g. Socher et al. (2011b)), where learning is quadratic in the length of each sentence and existing syntax-driven approaches such as the MV-RNN of Socher et al. (2012b), where the size of the model, that is the number of variables that needs to be learned, is quadratic in the size of the word-embeddings. Sparsity Parametrizing on CCG types and rules increases the size of the model compared to a greedy RAE (Socher et al., 2011b). The effect of this was highlighted by the sentiment analysis task, with the more complex models performing 901 Expression Most Similar convey the message of peace keep alight the flame of ha</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="4406" citStr="Socher et al., 2011" startWordPosition="676" endWordPosition="679"> of the 51st Annual Meeting of the Association for Computational Linguistics, pages 894–904, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions: Can recursive vector space models be reconciled with a more formal notion of compositionality; and is there a role for syntax in guiding semantics in these types of models? CCAEs make use of CCG combinators and types by conditioning each composition function on its equivalent step in a CCG proof. In terms of learning complexity and space requirements, our models strike a balance between simpler greedy approaches (Socher et al., 2011b) and the larger recursive vector-matrix models (Socher et al., 2012b). We show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks. In both sentiment and compound similarity experiments we show that our CCAE models match or better comparable recursive autoencoder models.1 2 Background There exist a number of formal approaches to language that provide mechanisms for compositionality. Generative Grammars (Jackendoff, 1972) treat semantics, and thus compositionality, ess</context>
<context position="9867" citStr="Socher et al., 2011" startWordPosition="1529" endWordPosition="1532">oaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another set of models that have very successfully been applied in this area are recursive autoencoders (Socher et al., 2011a; Socher et al., 2011b), which are discussed in the next section. 2.3 Recursive Autoencoders Autoencoders are a useful tool to compress information. One can think of an autoencoder as a funnel through which information has to pass (see Figure 2). By forcing the autoencoder to reconstruct an input given only the reduced amount of information available inside the funnel it serves as a compression tool, representing highdimensional objects in a lower-dimensional space. Typically a given autoencoder, that is the functions for encoding and reconstructing data, are 2The experimental setup in (Baron</context>
<context position="13205" citStr="Socher et al., 2011" startWordPosition="2096" endWordPosition="2099">, x2) = g (W(x1Ix2) + b) (5) (x&apos;2) = g (W&apos;e + b&apos;) 1Ix&apos; Extending this idea, recursive autoencoders (RAE) allow the modelling of data of variable size. By setting the n = 2m, it is possible to recursively combine a structure into an autoencoder tree. See Figure 3 for an example, where x1, x2, x3 are recursively encoded into y2. The recursive application of autoencoders was first introduced in Pollack (1990), whose recursive auto-associative memories learn vector representations over pre-specified recursive data structures. More recently this idea was extended and applied to dynamic structures (Socher et al., 2011b). These types of models have become increasingly prominent since developments within the field of Deep Learning have made the training of such hierarchical structures more effective and tractable (LeCun et al., 1998; Hinton et al., 2006). Intuitively the top layer of an RAE will encode aspects of the information stored in all of the input vectors. Previously, RAE have successfully been applied to a number of tasks including sentiment analysis, paraphrase detection, relation extraction Model CCG Elements CCAE-A parse CCAE-B parse + rules CCAE-C parse + rules + types CCAE-D parse + rules + chi</context>
<context position="15041" citStr="Socher et al. (2011" startWordPosition="2404" endWordPosition="2407">e given. Let C denote the set of combinatory rules, and T the set of categories used, respectively. We use the parse tree to structure an RAE, so that each combinatory step is represented by an autoencoder function. We refer to these models Categorial Combinatory Autoencoders (CCAE). In total this paper describes four models making increasing use of the CCG formalism (see table 1). As an internal baseline we use model CCAEA, which is an RAE structured along a CCG parse tree. CCAE-A uses a single weight matrix each for the encoding and reconstruction step (see Table 2. This model is similar to Socher et al. (2011b), except that we use a fixed structure in place of the greedy tree building approach. As CCAE-A uses only minimal syntactic guidance, this should allow us to better ascertain to what degree the use of syntax helps our semantic models. Our second model (CCAE-B) uses the composition function in equation (6), with c E C. fenc(x, y, c) = g (Wcenc(xIy) + bcenc) (6) frec(e, c) = g (Wcrece + bcrec) This means that for every combinatory rule we define an equivalent autoencoder composition function by parametrizing both the weight matrix and bias on the combinatory rule (e.g. Figure 4). In this model</context>
<context position="19644" citStr="Socher et al. (2011" startWordPosition="3228" endWordPosition="3231">input for a binary classification model: pred(l=1|v, θ) = sigmoid(Wlabel v + blabel) (9) Given our corpus of CCG parses with label pairs (N, l), the new objective function becomes: J = N 1 X E(N,l, θ) + 2 ||θ||2 (10) λ (N,l) Assuming each node n E N contains children xn, yn, encoding en and reconstruction rn, so that n = {x, y, e, r} this breaks down into: E(N,l, θ) = (11) X αErec (n, θ) + (1−α)Elbl(en, l, θ) n∈N � � 1 �Erec (n, θ) = 2 [xn I yn] − rn 2 (12) Elbl(e,l, θ) = 2 �l − e�2 1 (13) This method of introducing a supervised aspect to the autoencoder largely follows the model described in Socher et al. (2011b). 5 Experiments We describe a number of standard evaluations to determine the comparative performance of our model. The first task of sentiment analysis allows us to compare our CCG-conditioned RAE with similar, existing models. In a second experiment, we apply our model to a compound similarity evaluation, which allows us to evaluate our models against a larger class of vector-based models (Blacoe and Lapata, 2012). We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings. In our experiments we use</context>
<context position="21099" citStr="Socher et al., 2011" startWordPosition="3467" endWordPosition="3470">er (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. Experiment 1: Semi-Supervised Training In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al. (2010). The </context>
<context position="22671" citStr="Socher et al., 2011" startWordPosition="3707" endWordPosition="3710">e MPQA experiment most instances are very short with an average length of 3 words, while the average sentence length in the SP corpus is 21 words. The MPQA task is further simplified through the use or an additional sentiment lexicon. Considering dictionary size, the SP corpus has a dictionary of 22k words, more than three times the size of the MPQA dictionary. 4http://www.metaoptimize.com/projects/ wordreprs/ 5http://mpqa.cs.pitt.edu/ 6http://www.cs.cornell.edu/people/ pabo/movie-review-data/ 899 Method MPQA SP Voting with two lexica 81.7 63.1 MV-RNN (Socher et al., 2012b) - 79.0 RAE (rand) (Socher et al., 2011b) 85.7 76.8 TCRF (Nakagawa et al., 2010) 86.1 77.3 RAE (init) (Socher et al., 2011b) 86.4 77.7 NB (Wang and Manning, 2012) 86.7 79.4 CCAE-A 86.3 77.8 CCAE-B 87.1 77.1 CCAE-C 87.1 77.3 CCAE-D 87.2 76.7 Table 3: Accuracy of sentiment classification on the sentiment polarity (SP) and MPQA datasets. For NB we only display the best result among a larger group of models analysed in that paper. This issue of sparsity is exacerbated in the more complex CCAE models, where the training points are spread across different CCG types and rules. While the initialization of the word vectors with previously l</context>
<context position="29995" citStr="Socher et al. (2011" startWordPosition="4930" endWordPosition="4933">r task, they do not allow unsupervised training. This prevents the possiblity of pretraining, which we showed to have a big impact on results, and further prevents the training of general models: The CCAE models can be used for multiple tasks without the need to re-train the main model. Complexity Previously in this paper we argued that our models combined the strengths of other approaches. By using a grammar formalism we increase the expressive power of the model while the complexity remains low. For the complexity analysis see Table 7. We strike a balance between the greedy approaches (e.g. Socher et al. (2011b)), where learning is quadratic in the length of each sentence and existing syntax-driven approaches such as the MV-RNN of Socher et al. (2012b), where the size of the model, that is the number of variables that needs to be learned, is quadratic in the size of the word-embeddings. Sparsity Parametrizing on CCG types and rules increases the size of the model compared to a greedy RAE (Socher et al., 2011b). The effect of this was highlighted by the sentiment analysis task, with the more complex models performing 901 Expression Most Similar convey the message of peace keep alight the flame of ha</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of EMNLP, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Bharath Bhat</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Convolutional-Recursive Deep Learning for 3D Object Classification.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25.</booktitle>
<contexts>
<context position="2875" citStr="Socher et al., 2012" startWordPosition="436" endWordPosition="439">it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition. Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012b). While these models have proved very promising for compositional semantics, they make minimal use of linguistic information beyond the word level. In this paper we bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics. We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree. CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operators with which we can parametrise our nonlinear transfor</context>
<context position="4475" citStr="Socher et al., 2012" startWordPosition="686" endWordPosition="689">uistics, pages 894–904, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions: Can recursive vector space models be reconciled with a more formal notion of compositionality; and is there a role for syntax in guiding semantics in these types of models? CCAEs make use of CCG combinators and types by conditioning each composition function on its equivalent step in a CCG proof. In terms of learning complexity and space requirements, our models strike a balance between simpler greedy approaches (Socher et al., 2011b) and the larger recursive vector-matrix models (Socher et al., 2012b). We show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks. In both sentiment and compound similarity experiments we show that our CCAE models match or better comparable recursive autoencoder models.1 2 Background There exist a number of formal approaches to language that provide mechanisms for compositionality. Generative Grammars (Jackendoff, 1972) treat semantics, and thus compositionality, essentially as an extension of syntax, with the generative (syntactic) p</context>
<context position="14001" citStr="Socher et al., 2012" startWordPosition="2225" endWordPosition="2228">ective and tractable (LeCun et al., 1998; Hinton et al., 2006). Intuitively the top layer of an RAE will encode aspects of the information stored in all of the input vectors. Previously, RAE have successfully been applied to a number of tasks including sentiment analysis, paraphrase detection, relation extraction Model CCG Elements CCAE-A parse CCAE-B parse + rules CCAE-C parse + rules + types CCAE-D parse + rules + child types Table 1: Aspects of the CCG formalism used by the different models explored in this paper. and 3D object identification (Blacoe and Lapata, 2012; Socher et al., 2011b; Socher et al., 2012a). 3 Model The models in this paper combine the power of recursive, vector-based models with the linguistic intuition of the CCG formalism. Their purpose is to learn semantically meaningful vector representations for sentences and phrases of variable size, while the purpose of this paper is to investigate the use of syntax and linguistic formalisms in such vector-based compositional models. We assume a CCG parse to be given. Let C denote the set of combinatory rules, and T the set of categories used, respectively. We use the parse tree to structure an RAE, so that each combinatory step is rep</context>
<context position="22630" citStr="Socher et al., 2012" startWordPosition="3699" endWordPosition="3702">ion for this difference in results: In the MPQA experiment most instances are very short with an average length of 3 words, while the average sentence length in the SP corpus is 21 words. The MPQA task is further simplified through the use or an additional sentiment lexicon. Considering dictionary size, the SP corpus has a dictionary of 22k words, more than three times the size of the MPQA dictionary. 4http://www.metaoptimize.com/projects/ wordreprs/ 5http://mpqa.cs.pitt.edu/ 6http://www.cs.cornell.edu/people/ pabo/movie-review-data/ 899 Method MPQA SP Voting with two lexica 81.7 63.1 MV-RNN (Socher et al., 2012b) - 79.0 RAE (rand) (Socher et al., 2011b) 85.7 76.8 TCRF (Nakagawa et al., 2010) 86.1 77.3 RAE (init) (Socher et al., 2011b) 86.4 77.7 NB (Wang and Manning, 2012) 86.7 79.4 CCAE-A 86.3 77.8 CCAE-B 87.1 77.1 CCAE-C 87.1 77.3 CCAE-D 87.2 76.7 Table 3: Accuracy of sentiment classification on the sentiment polarity (SP) and MPQA datasets. For NB we only display the best result among a larger group of models analysed in that paper. This issue of sparsity is exacerbated in the more complex CCAE models, where the training points are spread across different CCG types and rules. While the initializat</context>
<context position="29338" citStr="Socher et al., 2012" startWordPosition="4817" endWordPosition="4820">ware of, whereas on the SP corpus we achieve competitive results. With an additional, unsupervised training step we achieved results beyond the current state of the art on this task, too. Semantics The qualitative analysis and the experiment on compounds demonstrate that the CCAE models are capable of learning semantics. An advantage of our approach—and of autoencoders generally—is their ability to learn in an unsupervised setting. The pre-training step for the sentiment task was essentially the same training step as used in the compound similarity task. While other models such as the MV-RNN (Socher et al., 2012b) achieve good results on a particular task, they do not allow unsupervised training. This prevents the possiblity of pretraining, which we showed to have a big impact on results, and further prevents the training of general models: The CCAE models can be used for multiple tasks without the need to re-train the main model. Complexity Previously in this paper we argued that our models combined the strengths of other approaches. By using a grammar formalism we increase the expressive power of the model while the complexity remains low. For the complexity analysis see Table 7. We strike a balanc</context>
</contexts>
<marker>Socher, Huval, Bhat, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Bharath Bhat, Christopher D. Manning, and Andrew Y. Ng. 2012a. Convolutional-Recursive Deep Learning for 3D Object Classification. In Advances in Neural Information Processing Systems 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="2875" citStr="Socher et al., 2012" startWordPosition="436" endWordPosition="439">it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition. Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012b). While these models have proved very promising for compositional semantics, they make minimal use of linguistic information beyond the word level. In this paper we bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics. We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree. CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operators with which we can parametrise our nonlinear transfor</context>
<context position="4475" citStr="Socher et al., 2012" startWordPosition="686" endWordPosition="689">uistics, pages 894–904, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tions: Can recursive vector space models be reconciled with a more formal notion of compositionality; and is there a role for syntax in guiding semantics in these types of models? CCAEs make use of CCG combinators and types by conditioning each composition function on its equivalent step in a CCG proof. In terms of learning complexity and space requirements, our models strike a balance between simpler greedy approaches (Socher et al., 2011b) and the larger recursive vector-matrix models (Socher et al., 2012b). We show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks. In both sentiment and compound similarity experiments we show that our CCAE models match or better comparable recursive autoencoder models.1 2 Background There exist a number of formal approaches to language that provide mechanisms for compositionality. Generative Grammars (Jackendoff, 1972) treat semantics, and thus compositionality, essentially as an extension of syntax, with the generative (syntactic) p</context>
<context position="14001" citStr="Socher et al., 2012" startWordPosition="2225" endWordPosition="2228">ective and tractable (LeCun et al., 1998; Hinton et al., 2006). Intuitively the top layer of an RAE will encode aspects of the information stored in all of the input vectors. Previously, RAE have successfully been applied to a number of tasks including sentiment analysis, paraphrase detection, relation extraction Model CCG Elements CCAE-A parse CCAE-B parse + rules CCAE-C parse + rules + types CCAE-D parse + rules + child types Table 1: Aspects of the CCG formalism used by the different models explored in this paper. and 3D object identification (Blacoe and Lapata, 2012; Socher et al., 2011b; Socher et al., 2012a). 3 Model The models in this paper combine the power of recursive, vector-based models with the linguistic intuition of the CCG formalism. Their purpose is to learn semantically meaningful vector representations for sentences and phrases of variable size, while the purpose of this paper is to investigate the use of syntax and linguistic formalisms in such vector-based compositional models. We assume a CCG parse to be given. Let C denote the set of combinatory rules, and T the set of categories used, respectively. We use the parse tree to structure an RAE, so that each combinatory step is rep</context>
<context position="22630" citStr="Socher et al., 2012" startWordPosition="3699" endWordPosition="3702">ion for this difference in results: In the MPQA experiment most instances are very short with an average length of 3 words, while the average sentence length in the SP corpus is 21 words. The MPQA task is further simplified through the use or an additional sentiment lexicon. Considering dictionary size, the SP corpus has a dictionary of 22k words, more than three times the size of the MPQA dictionary. 4http://www.metaoptimize.com/projects/ wordreprs/ 5http://mpqa.cs.pitt.edu/ 6http://www.cs.cornell.edu/people/ pabo/movie-review-data/ 899 Method MPQA SP Voting with two lexica 81.7 63.1 MV-RNN (Socher et al., 2012b) - 79.0 RAE (rand) (Socher et al., 2011b) 85.7 76.8 TCRF (Nakagawa et al., 2010) 86.1 77.3 RAE (init) (Socher et al., 2011b) 86.4 77.7 NB (Wang and Manning, 2012) 86.7 79.4 CCAE-A 86.3 77.8 CCAE-B 87.1 77.1 CCAE-C 87.1 77.3 CCAE-D 87.2 76.7 Table 3: Accuracy of sentiment classification on the sentiment polarity (SP) and MPQA datasets. For NB we only display the best result among a larger group of models analysed in that paper. This issue of sparsity is exacerbated in the more complex CCAE models, where the training points are spread across different CCG types and rules. While the initializat</context>
<context position="29338" citStr="Socher et al., 2012" startWordPosition="4817" endWordPosition="4820">ware of, whereas on the SP corpus we achieve competitive results. With an additional, unsupervised training step we achieved results beyond the current state of the art on this task, too. Semantics The qualitative analysis and the experiment on compounds demonstrate that the CCAE models are capable of learning semantics. An advantage of our approach—and of autoencoders generally—is their ability to learn in an unsupervised setting. The pre-training step for the sentiment task was essentially the same training step as used in the compound similarity task. While other models such as the MV-RNN (Socher et al., 2012b) achieve good results on a particular task, they do not allow unsupervised training. This prevents the possiblity of pretraining, which we showed to have a big impact on results, and further prevents the training of general models: The CCAE models can be used for multiple tasks without the need to re-train the main model. Complexity Previously in this paper we argued that our models combined the strengths of other approaches. By using a grammar formalism we increase the expressive power of the model while the complexity remains low. For the complexity analysis see Table 7. We strike a balanc</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012b. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP-CoNLL, pages 1201– 1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Jason Baldridge</author>
</authors>
<title>Combinatory Categorial Grammar,</title>
<date>2011</date>
<pages>181--224</pages>
<publisher>WileyBlackwell.</publisher>
<contexts>
<context position="6039" citStr="Steedman and Baldridge (2011)" startWordPosition="920" endWordPosition="923">his is beyond the scope of this paper, see e.g. Kracht (2008) for a detailed analysis of compositionality in these formalisms. 2.1 Combinatory Categorial Grammar In this paper we focus on CCG, a linguistically expressive yet computationally efficient grammar formalism. It uses a constituency-based structure with complex syntactic types (categories) from which sentences can be deduced using a small number of combinators. CCG relies on combinatory logic (as opposed to lambda calculus) to build its expressions. For a detailed introduction and analysis vis-`a-vis other grammar formalisms see e.g. Steedman and Baldridge (2011). CCG has been described as having a transparent surface between the syntactic and the seman1A C++ implementation of our models is available at http://www.karlmoritz.com/ Tina likes tigers N (S[dcl]\NP)/NP N NP NP S[dcl]\NP S[dcl] Figure 1: CCG derivation for Tina likes tigers with forward (&gt;) and backward application (&lt;). tic. It is this property which makes it attractive for our purposes of providing a conditioning structure for semantic operators. A second benefit of the formalism is that it is designed with computational efficiency in mind. While one could debate the relative merits of var</context>
</contexts>
<marker>Steedman, Baldridge, 2011</marker>
<rawString>Mark Steedman and Jason Baldridge, 2011. Combinatory Categorial Grammar, pages 181–224. WileyBlackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Szabolcsi</author>
</authors>
<title>Bound Variables in Syntax: Are There Any?</title>
<date>1989</date>
<booktitle>Semantics and Contextual Expression,</booktitle>
<pages>295--318</pages>
<editor>In Renate Bartsch, Johan van Benthem, and Peter van Emde Boas, editors,</editor>
<location>Foris, Dordrecht.</location>
<contexts>
<context position="1956" citStr="Szabolcsi, 1989" startWordPosition="301" endWordPosition="302">ional unit for this process of compositional semantics, and how these units combine, remain open questions. Frege’s principle may be debatable from a linguistic and philosophical standpoint, but it has provided a basis for a range of formal approaches to semantics which attempt to capture meaning in logical models. The Montague grammar (Montague, 1970) is a prime example for this, building a model of composition based on lambdacalculus and formal logic. More recent work in this field includes the Combinatory Categorial Grammar (CCG), which also places increased emphasis on syntactic coverage (Szabolcsi, 1989). Recently those searching for the right representation for compositional semantics have drawn inspiration from the success of distributional models of lexical semantics. This approach represents single words as distributional vectors, implying that a word’s meaning is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed t</context>
</contexts>
<marker>Szabolcsi, 1989</marker>
<rawString>Anna Szabolcsi. 1989. Bound Variables in Syntax: Are There Any? In Renate Bartsch, Johan van Benthem, and Peter van Emde Boas, editors, Semantics and Contextual Expression, pages 295–318. Foris, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="20408" citStr="Turian et al. (2010)" startWordPosition="3351" endWordPosition="3354">t analysis allows us to compare our CCG-conditioned RAE with similar, existing models. In a second experiment, we apply our model to a compound similarity evaluation, which allows us to evaluate our models against a larger class of vector-based models (Blacoe and Lapata, 2012). We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings. In our experiments we use the hyperbolic tangent as nonlinearity g. Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al. (2010) based on the model of Collobert and Weston (2008).4 We use the C&amp;C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sent</context>
<context position="21693" citStr="Turian et al. (2010)" startWordPosition="3556" endWordPosition="3559">) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. Experiment 1: Semi-Supervised Training In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al. (2010). The results of this evaluation are in Table 3. While we achieve the best performance on the MPQA corpus, the results on the SP corpus are less convincing. Perhaps surprisingly, the simplest model CCAE-A outperforms the other models on this dataset. When considering the two datasets, sparsity seems a likely explanation for this difference in results: In the MPQA experiment most instances are very short with an average length of 3 words, while the average sentence length in the SP corpus is 21 words. The MPQA task is further simplified through the use or an additional sentiment lexicon. Consid</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>90--94</pages>
<contexts>
<context position="22794" citStr="Wang and Manning, 2012" startWordPosition="3729" endWordPosition="3732"> the SP corpus is 21 words. The MPQA task is further simplified through the use or an additional sentiment lexicon. Considering dictionary size, the SP corpus has a dictionary of 22k words, more than three times the size of the MPQA dictionary. 4http://www.metaoptimize.com/projects/ wordreprs/ 5http://mpqa.cs.pitt.edu/ 6http://www.cs.cornell.edu/people/ pabo/movie-review-data/ 899 Method MPQA SP Voting with two lexica 81.7 63.1 MV-RNN (Socher et al., 2012b) - 79.0 RAE (rand) (Socher et al., 2011b) 85.7 76.8 TCRF (Nakagawa et al., 2010) 86.1 77.3 RAE (init) (Socher et al., 2011b) 86.4 77.7 NB (Wang and Manning, 2012) 86.7 79.4 CCAE-A 86.3 77.8 CCAE-B 87.1 77.1 CCAE-C 87.1 77.3 CCAE-D 87.2 76.7 Table 3: Accuracy of sentiment classification on the sentiment polarity (SP) and MPQA datasets. For NB we only display the best result among a larger group of models analysed in that paper. This issue of sparsity is exacerbated in the more complex CCAE models, where the training points are spread across different CCG types and rules. While the initialization of the word vectors with previously learned embeddings (as was previously shown by Socher et al. (2011b)) helps the models, all other model variables such as co</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D. Manning. 2012. Baselines and bigrams: simple, good sentiment and topic classification. In Proceedings of ACL, pages 90–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="20866" citStr="Wiebe et al., 2005" startWordPosition="3432" endWordPosition="3435">e hyperbolic tangent as nonlinearity g. Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al. (2010) based on the model of Collobert and Weston (2008).4 We use the C&amp;C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLPHLT, HLT ’05,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="21164" citStr="Wilson et al., 2005" startWordPosition="3477" endWordPosition="3480">data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. 5.1 Sentiment Analysis We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. Experiment 1: Semi-Supervised Training In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al. (2010). The results of this evaluation are in Table 3. While we achieve the b</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of EMNLPHLT, HLT ’05, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>