<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000445">
<title confidence="0.989562">
More than Words:
Syntactic Packaging and Implicit Sentiment
</title>
<author confidence="0.89453">
Stephan Greene∗
</author>
<affiliation confidence="0.656207">
ATG, Inc.
</affiliation>
<address confidence="0.9096215">
1111 19th St, NW Suite 600
Washington, DC 20036
</address>
<email confidence="0.997677">
sgreene@atg.com
</email>
<author confidence="0.837238">
Philip Resnik
</author>
<affiliation confidence="0.803862">
Linguistics / UMIACS CLIP Laboratory
University of Maryland
</affiliation>
<address confidence="0.945264">
College Park, MD 20742
</address>
<email confidence="0.999478">
resnik@umiacs.umd.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999603928571429">
Work on sentiment analysis often focuses on
the words and phrases that people use in
overtly opinionated text. In this paper, we in-
troduce a new approach to the problem that
focuses not on lexical indicators, but on the
syntactic “packaging” of ideas, which is well
suited to investigating the identification of im-
plicit sentiment, or perspective. We establish a
strong predictive connection between linguis-
tically well motivated features and implicit
sentiment, and then show how computational
approximations of these features can be used
to improve on existing state-of-the-art senti-
ment classification results.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982888897959184">
As Pang and Lee (2008) observe, the last several
years have seen a “land rush” in research on senti-
ment analysis and opinion mining, with a frequent
emphasis on the identification of opinions in evalua-
tive text such as movie or product reviews. How-
ever, sentiment also may be carried implicitly by
statements that are not only non-evaluative, but not
even visibly subjective. Consider, for example, the
following two descriptions of the same (invented)
event:
1(a) On November 25, a soldier veered his jeep into
a crowded market and killed three civilians.
(b) On November 25, a soldier’s jeep veered into a
crowded market, causing three civilian deaths.
∗This work was done while the first author was a student in
the Department of Linguistics, University of Maryland.
Both descriptions appear on the surface to be objec-
tive statements, and they use nearly the same words.
Lexically, the sentences’ first clauses differ only in
the difference between ’s and his to express the rela-
tionship between the soldier and the jeep, and in the
second clauses both kill and death are terms with
negative connotations, at least according to the Gen-
eral Inquirer lexicon (Stone, 1966). Yet the descrip-
tions clearly differ in the feelings they evoke: if the
soldier were being tried for his role in what hap-
pened on November 25, surely the prosecutor would
be more likely to say (1a) to the jury, and the defense
attorney (1b), rather than the reverse.1
Why, then, should a description like (1a) be per-
ceived as less sympathetic to the soldier than (1b)?
If the difference is not in the words, it must be in
the way they are put together; that is, the structure
of the sentence. In Section 2, we offer a specific hy-
pothesis about the connection between structure and
implicit sentiment: we suggest that the relationship
is mediated by a set of “grammatically relevant” se-
mantic properties well known to be important cross-
linguistically in characterizing the interface between
syntax and lexical semantics. In Section 3, we val-
idate this hypothesis by means of a human ratings
study, showing that these properties are highly pre-
dictive of human sentiment ratings. In Section 4, we
introduce observable proxies for underlying seman-
tics (OPUS), a practical way to approximate the rele-
vant semantic properties automatically as features in
a supervised learning setting. In Section 5, we show
that these features improve on the existing state of
the art in automatic sentiment classification. Sec-
</bodyText>
<footnote confidence="0.976575">
1We refer readers not sharing this intuition to Section 3.
</footnote>
<page confidence="0.946392">
503
</page>
<note confidence="0.9047935">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 503–511,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.677547">
tions 6 and 7 discuss related work and summarize.
</bodyText>
<sectionHeader confidence="0.882068" genericHeader="method">
2 Linguistic Motivation
</sectionHeader>
<bodyText confidence="0.999965073170732">
Verbal descriptions of an event often carry along
with them an underlying attitude toward what is be-
ing described. By framing the same event in differ-
ent ways, speakers or authors “select some aspects
of a perceived reality and make them more salient
in a communicating text, in such a way as to pro-
mote a particular problem definition, causal inter-
pretation, moral evaluation, and/or treatment recom-
mendation” (Entman, 1993, p. 52). Clearly lexi-
cal choices can accomplish this kind of selection,
e.g. choosing to describe a person as a terrorist
rather than a freedom fighter, or referencing killer
whales rather than orcas.2 Syntactic choices can
also have framing effects. For example, Ronald Rea-
gan’s famous use of the passive construction, “Mis-
takes were made” (in the context of the Iran-Contra
scandal), is a classic example of framing or spin:
used without a by-phrase, the passive avoids iden-
tifying a causal agent and therefore sidesteps the is-
sue of responsibility (Broder, 2007). A toddler who
says “My toy broke” instead of “I broke my toy” is
employing the same linguistic strategy.
Linguists have long studied syntactic variation
in descriptions of the same event, often under the
general heading of syntactic diathesis alternations
(Levin, 1993; Levin and Hovav, 2005). This line
of research has established a set of semantic prop-
erties that are widely viewed as “grammatically rel-
evant” in the sense that they enable generalizations
about syntactic “packaging” of meaning within (and
across) the world’s languages. For example, the
verb break in English participates in the causative-
inchoative alternation (causative event X broke Y
can also be expressed without overt causation as Y
broke), but the verb climb does not (X also causes
the event in X climbed Y, but that event cannot be
expressed as Y climbed). These facts about partic-
ipation in the alternation turn out to be connected
with the fact that a breaking event entails a change of
state in Y but a climbing event does not. Grammati-
cally relevant semantic properties of events and their
</bodyText>
<footnote confidence="0.493221666666667">
2Supporters of an endangered species listing in Puget Sound
generally referred to the animals as orcas, while opponents gen-
erally said killer whales (Harden, 2006).
</footnote>
<bodyText confidence="0.999521325581395">
participants — causation, change of state, and others
— are central not only in theoretical work on lex-
ical semantics, but in computational approaches to
the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr,
1993; Wu and Palmer, 1994; Dang et al., 1998)).
The approach we propose draws on two influ-
ential discussions about grammatically relevant se-
mantic properties in theoretical work on lexical se-
mantics. First, Dowty (1991) characterizes gram-
matically relevant properties of a verb’s arguments
(e.g. subject and object) via inferences that follow
from the meaning of the verb. For example, expres-
sions like X murders Y or X interrogates Y entail
that subject X caused the event.3 Second, Hopper
and Thompson (1980) characterize “semantic transi-
tivity” using similar properties, connecting semantic
features to morphosyntactic behavior across a wide
variety of languages.
Bringing together Dowty with Hopper and
Thompson, we find 13 semantic properties or-
ganized into three groups, corresponding to the
three components of a canonical transitive clause,
expressed as X verb Y in English.4 Proper-
ties associated with X involve volitional involve-
ment in the event or state, causation of the event,
sentience/awareness and/or perception, causing a
change of state in Y, kinesis or movement, and ex-
istence independent of the event. Properties asso-
ciated with the event or state conveyed by the verb
include aspectual features of telicity (a defined end-
point) and punctuality (the latter of which may be
inversely related to a property known as incremen-
tal theme). Properties associated with Y include
affectedness, change of state, (lack of) kinesis or
movement, and (lack of) existence independent of
the event.
Now, observe that this set of semantic proper-
ties involves many of the questions that would nat-
urally help to shape one’s opinion about the event
described by veer in (1). Was anyone or anything
affected by what took place, and to what degree?
Did the event just happen or was it caused? Did the
event reach a defined endpoint? Did participation in
</bodyText>
<footnote confidence="0.9982234">
3Kako (2006) has verified that people make these inferences
based on X’s syntactic position even when a semantically empty
nonsense verb is used.
4We are deliberately sidestepping the choice of terminology
for X and Y, e.g. proto-Patient, theme, etc.
</footnote>
<page confidence="0.997792">
504
</page>
<bodyText confidence="0.9999485">
the event involve conscious thought or intent? Our
hypothesis is that the syntactic aspects of “framing”,
as characterized by Entman, involve manipulation of
these semantic properties, even when overt opinions
are not being expressed. That is, we propose a con-
nection between syntactic choices and implicit senti-
ment mediated by the very same semantic properties
that linguists have already identified as central when
connecting surface expression to underlying mean-
ing more generally.
</bodyText>
<sectionHeader confidence="0.993362" genericHeader="method">
3 Empirical Validation
</sectionHeader>
<bodyText confidence="0.999897166666667">
We validated the hypothesized connection between
implicit sentiment and grammatically relevant se-
mantic properties using psycholinguistic methods,
by varying the syntactic form of event descriptions,
and showing that the semantic properties of descrip-
tions do indeed predict perceived sentiment.5
</bodyText>
<subsectionHeader confidence="0.999706">
3.1 Semantic property ratings
</subsectionHeader>
<bodyText confidence="0.99905796">
Materials. Stimuli were constructed using 11
verbs of killing, which are widely viewed as proto-
typical for the semantic properties of interest here
(Lemmens, 1998): X killed Y normally involves
conscious, intentional causation by X of a kinetic
event that causes a (rather decisive and clearly ter-
minated!) change of state in Y . The verbs comprise
two classes: the “transitive” class, involving ex-
ternally caused change-of-state verbs (kill, slaugh-
ter, assassinate, shoot, poison), and the “ergative”
class (strangle, smother, choke, drown, suffocate,
starve), within which verbs are internally caused
(McKoon and MacFarland, 2000) or otherwise em-
phasize properties of the object. Variation of syntac-
tic description involved two forms: a transitive syn-
tactic frame with a human agent as subject (“transi-
tive form”, 2a), and a nominalization of the verb as
subject and the verb kill as the predicate (“nominal-
ized form”, 2b).
2(a) The gunmen shot the opposition leader
(b) The shooting killed the opposition leader
Participants and procedure. A set of 18 vol-
unteer participants, all native speakers of English,
were presented with event descriptions and asked to
answer questions probing both Dowty’s proto-role
</bodyText>
<footnote confidence="0.43318">
5Full details and materials in Greene (2007).
</footnote>
<bodyText confidence="0.999883857142857">
properties as well as Hopper and Thompson’s se-
mantic transitivity components, responding via rat-
ings on a 1-to-7 scale. For example, the questions
probing volition were: “In this event, how likely
is it that (subject) chose to be involved?”, where
(subject) was the gunmen and the shooting, for 2(a-
b), respectively.6
</bodyText>
<subsectionHeader confidence="0.99991">
3.2 Sentiment ratings
</subsectionHeader>
<bodyText confidence="0.999945357142857">
Materials. We used the materials above to con-
struct short, newspaper-like paragraphs, each one
accompanied by a “headline” version of the same
syntactic descriptions used above. For example,
given this paragraph:
A man has been charged for the suffocation of a
woman early Tuesday morning. City police say
the man suffocated the 24-year-old woman using
a plastic garbage bag. The woman, who police say
had a previous relationship with her attacker, was
on her way to work when the incident happened.
Based on information provided by neighbors, po-
lice were able to identify the suspect, who was ar-
rested at gunpoint later the same day.
</bodyText>
<listItem confidence="0.9204245">
the three alternative headlines would be:
3(a) Man suffocates 24-year old woman
(b) Suffocation kills 24-year-old woman
(c) 24-year-old woman is suffocated
</listItem>
<bodyText confidence="0.993733">
Some paragraphs were based on actual news sto-
ries.7 In all paragraphs, there is an obvious nomi-
nal referent for both the perpetrator and the victim,
it is clear that the victim dies, and the perpetrator
in the scenario is responsible for the resulting death
directly rather than indirectly (e.g. through negli-
6Standard experimental design methods were followed with
respect to counterbalancing, block design, and distractor stim-
uli; for example, no participant saw more than one of 2(a) or
2(b), and all participants saw equal numbers of transitive and
nominalized descriptions. The phrase In this event was repeated
in each question and emphasized visually in order to encourage
participants to focus on the particular event described in the sen-
tence, rather than on the entities or events denoted in general.
7In those cases no proper names were used, to avoid any
inadvertent emotional reactions or legal issues, although the de-
scriptions retained emotional impact because we wanted readers
to have some emotional basis with which to judge the headlines.
</bodyText>
<page confidence="0.989553">
505
</page>
<bodyText confidence="0.999922">
gence).8 The stem of the nominalization always ap-
peared in the event description in either verbal or
nominal form.
Participants and procedure. A set of 31 volun-
teers, all native speakers of English, were presented
with the paragraph-length descriptions and accom-
panying headlines. As a measure of sentiment, par-
ticipants were asked to rate headlines on a 1-to-7
scale with respect to how sympathetic they perceive
the headline to be toward the perpetrator. For exam-
ple, given the paragraph and one of the associated
headlines in (3), a participant would be asked to rate
“How sympathetic or unsympathetic is this headline
to the man?”9
</bodyText>
<subsectionHeader confidence="0.999442">
3.3 Analysis and discussion
</subsectionHeader>
<bodyText confidence="0.999160037037037">
Unsurprisingly, but reassuringly, an analysis of the
sentiment ratings yields a significant effect of syn-
tactic form on sympathy toward the perpetrator
(F(2,369) = 33.902,p &lt; .001), using a mixed
model ANOVA run with the headline form as fixed
effect. The transitive form of the headline yielded
significantly lower sympathy ratings than the nom-
inalized or passive forms in pairwise comparisons
(both p &lt; .001). We have thus confirmed empir-
ically that Reagan’s “Mistakes were made” was a
wise choice of phrasing on his part.
More important, we are now in a position to ex-
amine the relationship between syntactic forms and
perceived sentiment in more detail. We performed
regression analyses treating the 13 semantic prop-
erty ratings plus the identity of the verb as indepen-
dent variables to predict sympathy rating as a de-
pendent variable, using the 24 stimulus sentences
that bridged both collections of ratings.10 Consid-
8An alert reader may observe that headlines with nominal-
ized subjects using the verb kill require some other nominaliza-
tion, so they don’t say “Killing kills victim”. For these cases
in the data, an appropriate nominalization drawn from the event
description was used (e.g., explosion).
9Again, standard experimental design methods were used
with respect to block design, distractor stimuli, etc. The phrase
this headline was emphasized to stress that it is the headline
being rated, not the story. A second question rating sympathy
toward the victim was also asked in each case, as an additional
distractor.
10These involved only the transitive and nominalized forms,
because many of the questions were inapplicable to the passive
form. Since the two ratings studies involved different subject
ering semantic properties individually, we find that
volition has the strongest correlation with sympathy
(a negative correlation, with r = −.776), followed
by sentience (r = −.764) and kinesis/movement
(r = −.751). Although performing a multiple re-
gression with all variables for this size dataset is im-
possible, owing to overfitting (as a rule of thumb,
5 to 10 observed items are necessary per each in-
dependent variable), a multiple regression involving
verb, volition, and telicity as independent variables
yields R = .88, R2 = .78 (p &lt; .001). The value for
adjusted R2, which explicitly takes into account the
small number of observations, is 74.1.
In summary, then, this ratings study confirms the
influence of syntactic choices on perceptions of im-
plicit sentiment. Furthermore, it provides support
for the idea that this influence is mediated by “gram-
matically relevant” semantic properties, demonstrat-
ing that these accounted for approximately 75% of
the variance in implicit sentiment expressed by al-
ternative headlines describing the same event.
</bodyText>
<sectionHeader confidence="0.996291" genericHeader="method">
4 Observable Approximation
</sectionHeader>
<bodyText confidence="0.999555416666667">
Thus far, we have established a predictive connec-
tion between syntactic choices and underlying or im-
plicit sentiment, mediated by grammatically relevant
semantic properties. In an ideal world, we could har-
ness the predictive power of those properties by us-
ing volition, causation, telicity, etc. as features for
regression or classification in sentiment prediction
tasks. Unfortunately, the properties are not directly
observable, and neither automatic annotators nor la-
beled training data currently exist.
We therefore pursue a different strategy, which we
refer to as observable proxies for underlying seman-
tics (OPUS). It can be viewed as a middle ground
between relying on construction-level syntactic dis-
tinctions (such as the 3-way transitive, nominalized
subject, passive distinction in Section 3) and an-
notation of fine-grained semantic properties. The
key idea is to use observable grammatical relations,
drawn from the usages of terms determined to be
relevant to a domain, as proxies for the underlying
semantic properties that gave rise to their syntactic
realization using those relations. Automatically cre-
pools, regression models were run over the mean values of each
observation in the experimental data.
</bodyText>
<page confidence="0.9915">
506
</page>
<bodyText confidence="0.979145676470588">
ated features based on those observable proxies are
then used in classification as described in Section 5.
In order to identify the set T of terms relevant
to a particular document collection, we adopt the
relative frequency ratio (Damerau, 1993), R(t) =
Rtdomain/Rtreference, where Rt, = ft Nc is the ratio of
c
term t’s frequency in corpus c to the size N, of that
corpus. R(t) is a simple but effective comparison
of a term’s prevalence in a particular collection as
compared to a general reference corpus. We used
the British National Corpus as the reference because
it is both very large and representative of text from a
wide variety of domains and genres. The threshold
of R(t) permitting membership in T is an experi-
mental parameter.
OPUS features are defined in terms of syntactic
dependency relations involving terms in T. Given a
set D of syntactic dependency relations, features are
of the form t : d or d : t, with d ∈ D, t ∈ T. That
is, they are term-dependency pairs extracted from
term-dependency-term dependency tuples, preserv-
ing whether the term is the head or the dependent
in the dependency relation. In addition, we add two
construction-specific features: TRANS:v, which rep-
resents verb v in a canonical, syntactically transitive
usage, and NOOBJ:v, present when verb v is used
without a direct object.11
Example 4 shows source text (bolded clause in
4a), an illustrative subset of parser dependencies
(4b), and corresponding OPUS features (4c):
4(a) Life Without Parole does not eliminate the risk
that the prisoner will murder a guard, a visi-
tor, or another inmate.
</bodyText>
<listItem confidence="0.94487925">
(b) nsubj(murder, prisoner); aux(murder, will);
dobj(murder, guard)
(c) TRANS:murder, murder:nsubj, nsubj:prisoner,
murder:aux, aux:will, murder:dobj, dobj:guard
</listItem>
<bodyText confidence="0.979902538461538">
Intuitively the presence of TRANS:murder suggests
the entire complex of semantic properties discussed
in Section 2, bringing together the impliciation of
volition, causation, etc. on the part of prisoner
(as does nsubj:prisoner), affectedness and change of
state on the part of guard (as does dobj:guard), and
so forth.
11We parsed English text using the Stanford parser.
The NOOBJ features can capture a habitual read-
ing, or in some cases a detransitivizing effect as-
sociated with omission of the direct object (Olsen
and Resnik, 1997). The bold text in (5) yields
NOOBJ:kill as a feature.
5(a) At the same time, we should never ignore the
risks of allowing the inmate to kill again.
In this case, omitting the direct object decreases the
extent to which the killing event is interpreted as
telic, and it eliminates the possibility of attributing
change-of-state to a specific affected object (much
like “Mistakes were made” avoids attributing cause
to a specified subject), placing the phrasing at a
less “semantically transitive” point on the transi-
tivity continuum (Hopper and Thompson, 1980).
Some informants find a perceptible increase in neg-
ative sentiment toward inmate when the sentence is
phrased as in 5(b):
</bodyText>
<listItem confidence="0.743639666666667">
5(b) At the same time, we should never ignore the
risks of allowing the inmate to kill someone
again.
</listItem>
<sectionHeader confidence="0.970064" genericHeader="method">
5 Computational Application
</sectionHeader>
<bodyText confidence="0.9990912">
Having discussed linguistic motivation, empirical
validation, and practical approximation of seman-
tically relevant features, we now present two stud-
ies demonstrating their value in sentiment classifica-
tion. For the first study, we have constructed a new
data set particularly well suited for testing our ap-
proach, based on writing about the death penalty. In
our second study, we make a direct comparison with
prior state-of-the-art classification using the Bitter
Lemons corpus of Lin et al. (2006).
</bodyText>
<subsectionHeader confidence="0.997785">
5.1 Predicting Opinions of the Death Penalty
</subsectionHeader>
<bodyText confidence="0.9999914">
Corpus. We constructed a new corpus for exper-
imentation on implicit sentiment by downloading
the contents of pro- and anti-death-penalty Web
sites and manually checking, for a large subset,
that the viewpoints expressed in documents were as
expected. The collection, which we will refer to
as the DP corpus, comprises documents from five
pro-death-penalty sites and three anti-death-penalty
sites, and the corpus was engineered to have an even
balance, 596 documents per side.12
</bodyText>
<footnote confidence="0.722972">
12Details in Greene (2007).
</footnote>
<page confidence="0.996078">
507
</page>
<bodyText confidence="0.998041413043479">
Frequent bigram baseline. We adopted a super-
vised classification approach based on word n-gram
features, using SVM classification in the WEKA
machine learning package. In initial exploration us-
ing both unigrams and bigrams, and using both word
forms and stems, we found that performance did not
differ significantly, and chose stemmed bigrams for
our baseline comparisons. In order to control for the
difference in the number of features available to the
classifier in our comparisons, we use the N most fre-
quent stemmed bigrams as the baseline feature set
where N is matched to number of OPUS features
used in the comparison condition.
OPUS-kill verbs: OPUS features for manually
selected verbs. We created OPUS features for 14
verbs — those used in Section 3, plus murder, exe-
cute, and stab and their nominalizations (including
both event and -er nominals, e.g. both killing and
killer) — generating N = 1016 distinct features.
OPUS-domain: OPUS features for domain-
relevant verbs. We created OPUS features for the
117 verbs for which the relative frequency ratio
was greater than 1. This list includes many of the
kill verbs we used in Section 3, and introduces,
among others, many transitive verbs describing acts
of physical force (e.g. rape, rob, steal, beat, strike,
force, fight) as well as domain-relevant verbs such
as testify, convict, and sentence. Included verbs near
the borderline included, for example, hold, watch,
allow, and try. Extracting OPUS features for these
verbs yielded N = 7552 features.
Evaluation. Cross-validation at the document
level does not test what we are interested in, since
a classifier might well learn to bucket documents ac-
cording to Web site, not according to pro- or anti-
death-penalty sentiment. To avoid this difficulty, we
performed site-wise cross-validation. We restricted
our attention to the two sites from each perspec-
tive with the most documents, which we refer to as
pro], pro2, anti], and anti2, yielding 4-fold cross-
validation. Each fold ftrain,test is defined as con-
taining all documents from one pro and one anti site
for training, using all documents from the remain-
ing pro and anti sites for testing. So, for exam-
ple, fold f11,22 uses all documents from pro] and
anti] in training, and all documents from pro2 and
</bodyText>
<table confidence="0.999722">
Condition N features SVM accuracy
Baseline 1016 68.37
OPUS-kill verbs 1016 82.09
Baseline 7552 71.96
OPUS-domain 7552 88.10
</table>
<tableCaption confidence="0.8949995">
Table 1: Results for 4-fold site-wise cross-validation us-
ing the DP corpus
</tableCaption>
<table confidence="0.99970675">
Condition N features SVM accuracy
Baseline 1518 55.95
OPUS-frequent verbs 1518 55.95
OPUS-kill verbs 1062 66.67
</table>
<tableCaption confidence="0.996041">
Table 2: DP corpus comparison for OPUS features based
on frequent vs. domain-relevant verbs
</tableCaption>
<bodyText confidence="0.995125666666667">
anti2 for testing.13 As Table 1 shows, OPUS fea-
tures provide substantial and statistically significant
gains (p &lt; .001).
As a reality check to verify that it is domain-
relevant verb usages and the encoding of events they
embody that truly drives improved classification, we
extracted OPUS features for the 14 most frequent
verbs found in the DP Corpus that were not in our
manually created list of kill verbs, along with their
nominalizations. Table 2 shows the results of a clas-
sification experiment using a single train-test split,
training on 1062 documents from pro], pro2, anti],
anti2 and testing on 84 test documents from the sig-
nificantly smaller remaining sites. Using OPUS
features for the most frequent non-kill verbs fails
to beat the baseline, establishing that it is not sim-
ply term frequency, the presence of particular gram-
matical relations, or a larger feature set that the kill-
verb OPUS model was able to exploit, but rather the
properties of event encodings involving the kill verbs
themselves.
</bodyText>
<subsectionHeader confidence="0.974096">
5.2 Predicting Points of View in the
Israeli-Palestinian Conflict
</subsectionHeader>
<bodyText confidence="0.9999652">
In order to make a direct comparison here with prior
state-of-the-art work on sentiment analysis, we re-
port on sentiment classification using OPUS features
in experiments using a publicly available corpus in-
volving opposing perspectives, the Bitter Lemons
</bodyText>
<footnote confidence="0.941435333333333">
13Site (# of documents): pro1= clarkprosecutor.org (437),
pro2= prodeathpenalty.com (117), anti1= deathpenaltyinfo.org
(319), anti2= nodeathpenalty.org (212)
</footnote>
<page confidence="0.986324">
508
</page>
<bodyText confidence="0.996313288888889">
(hence BL) corpus introduced by Lin et al. (2006).
Corpus. The Bitter Lemons corpus comprises es-
says posted at www.bitterlemons.org,which,
in the words of the site, “present Israeli and Pales-
tinian viewpoints on prominent issues of concern”.
As a corpus, it has a number of interesting proper-
ties. First, its topic area is one of significant interest
and considerable controversy, yet the general tenor
of the web site is one that eschews an overly shrill
or extreme style of writing. Second, the site is orga-
nized in terms of issue-focused weekly editions that
include essays with contrasting viewpoints from the
site’s two editors, plus two essays, also contrasting,
from guest editors. This creates a natural balance be-
tween the two sides and across the subtopics being
discussed. The BL corpus as prepared by Lin et al.
contains 297 documents from each of the Israeli and
Palestinian viewpoints, averaging 700-800 words in
length.
Lin et al. classifiers. Lin et al. report results on
distinguishing Israeli vs. Palestinian perspectives
using an SVM classifier, a naive Bayes classifier
NB-M using maximum a posteriori estimation, and a
naive Bayes classifier NB-B using full Bayesian in-
ference. (Document perspectives are labeled clearly
on the site.) We continue to use the WEKA SVM
classifier, but compare our results to both their SVM
and NB-B, since the latter achieved their best results.
OPUS features. As in Section 5.1, we experi-
mented with OPUS features driven by automati-
cally extracted lists of domain-relevant verbs. For
these experiments, we included domain-relevant
nouns, and we varied a threshold p for the rela-
tive frequency ratio, including only terms for which
log(R(t)) &gt; p. In addition, we introduced a gen-
eral filter on OPUS features, eliminating syntactic
dependency types that do not usefully reflect seman-
tically relevant properties: det, predet, preconj, prt,
aux, auxpas, cc, punct, complm, mark, rel, ref, expl.
Evaluation. Lin et al. describe two test scenar-
ios. In the first, referred to as Test Scenario 1, they
trained on documents written by the site’s guests,
and tested on documents from the site’s editors. Test
Scenario 2 represents the reverse, training on docu-
ments from the site editors and testing on documents
</bodyText>
<figureCaption confidence="0.998988">
Figure 1: Results on the Bitter Lemons corpus
</figureCaption>
<bodyText confidence="0.999766136363637">
from guest authors. As in our site-wise cross vali-
dation for the DP corpus, this strategy ensures that
what is being tested is classification according to the
viewpoint, not author or topic.
Figure 1 (top) summarizes a large set of experi-
ments for Test Scenario 1, in which we varied the
values of p for verbs and nouns. Each experiment,
using aparticular (p(verbs), p(nouns)), corresponds
to a vertical strip on the x-axis. The points on that
strip include the p values for verbs and nouns, mea-
sured by the scale on the y-axis at the left of the
figure; the accuracy of Lin et al.’s SVM (88.22% ac-
curacy, constant across all our variations); the accu-
racy of Lin et al.’s NB-B classifier (93.46% accu-
racy, constant across all our variations), and the ac-
curacy of our SVM classifier using OPUS features,
which varies depending on the p values. Across 423
experiments, our average accuracy is 95.41%, with
the best accuracy achieved being 97.64%. Our clas-
sifier underperformed NB-B slightly, with accura-
cies from 92.93% to 93.27%, in just 8 of the 423
experiments.
</bodyText>
<figureCaption confidence="0.725143">
Figure 1 (bottom) provides a similar summary for
</figureCaption>
<figure confidence="0.938511404255319">
Term Threshold (ρ)
Term Threshold (ρ)
12
10
12
10
8
6
4
2
0
8
6
4
2
0
Individual Experiment (ρ values and accuracy)
Individual Experiment (ρ values and accuracy)
Classification Accuracy, BL Corpus
Test Scenario 2 (GeneralFilter)
Classification Accuracy, BL Corpus
Test Scenario 1 (GeneralFilter)
85
80
75
70
65
98
96
94
92
90
88
86
84
Percent Correct
Percent Correct
ρ (Verb)
ρ (Noun)
OPUS
Lin 2006 NB-B
Lin 2006 SVM
ρ (Verb)
ρ (Noun)
OPUS
Lin 2006 NB-B
Lin 2006 SVM
</figure>
<page confidence="0.993655">
509
</page>
<bodyText confidence="0.9999609375">
experiments in Test Scenario 2. The first thing to no-
tice is that accuracy for all methods is lower than for
Test Scenario 1. This is not terribly surprising: it is
likely that training a classifier on the more uniform
authorship of the editor documents builds a model
that generalizes less well to the more diverse au-
thorship of the guest documents (though accuracy
is still quite high). In addition, the editor-authored
documents comprise a smaller training set, consist-
ing of 7,899 sentences, while the guest documents
have a total of 11,033 sentences, a 28% difference.
In scenario 2, we obtain average accuracy across ex-
periments of 83.12%, with a maximum of 85.86%,
in this case outperforming the 81.48% obtained by
Lin’s SVM fairly consistently, and in some cases ap-
proaching or matching NB-B at 85.85%.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99995978">
Pang and Lee’s (2008) excellent monograph pro-
vides a thorough, well organized, and relatively re-
cent description of computational work on senti-
ment, opinion, and subjectivity analysis.
The problem of classifying underlying sentiment
in statements that are not overtly subjective is less
studied within the NLP literature, but it has received
some attention in other fields. These include, for ex-
ample, research on content analysis in journalism,
media studies, and political economy (Gentzkow
and Shapiro, 2006a; Gentzkow and Shapiro, 2006b;
Groseclose and Milyo, 2005; Fader et al., 2007); au-
tomatic identification of customer attitudes for busi-
ness e-mail routing (Durbin et al., 2003). And, of
course, the study of perceptions in politics and me-
dia bears a strong family resemblance to real-world
marketing problems involving reputation manage-
ment and business intelligence (Glance et al., 2005).
Within computational linguistics, what we call
implicit sentiment was introduced as a topic of study
by Lin et al. (2006) under the rubric of identifying
perspective, though similar work had begun earlier
in the realm of political science (e.g. (Laver et al.,
2003)). Other recent work focusing on the notion of
perspective or ideology has been reported by Martin
and Vanberg (2008) and Mullen and Malouf (2008).
Among prior authors, Gamon’s (2004) research is
perhaps closest to the work described here, in that
he uses some features based on a sentence’s logical
form, generated using a proprietary system. How-
ever, his features are templatic in nature in that they
do not couple specific lexical entries with their logi-
cal form. Hearst (1992) and Mulder et al. (2004) de-
scribe systems that make use of argument structure
features coupled with lexical information, though
neither provides implementation details or experi-
mental results.
In terms of computational experimentation, work
by Thomas et al. (2006), predicting yes and no
votes in corpus of United States Congressional floor
debate speeches, is quite relevant. They combined
SVM classification with a min-cut model on graphs
in order to exploit both direct textual evidence and
constraints suggested by the structure of Congres-
sional debates, e.g. the fact that the same individ-
ual rarely gives one speech in favor of a bill and an-
other opposing it. We have extend their method to
use OPUS features in the SVM and obtained signifi-
cant improvements over their classification accuracy
(Greene, 2007; Greene and Resnik, in preparation).
</bodyText>
<sectionHeader confidence="0.999214" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99961505">
In this paper we have introduced an approach to
implicit sentiment motivated by theoretical work in
lexical semantics, presenting evidence for the role of
semantic properties in human sentiment judgments.
This research is, to our knowledge, the first to draw
an explicit and empirically supported connection be-
tween theoretically motivated work in lexical se-
mantics and readers’ perception of sentiment. In ad-
dition, we have reported positive sentiment classifi-
cation results within a standard supervised learning
setting, employing a practical first approximation to
those semantic properties, including positive results
in a direct comparison with the previous state of the
art.
Because we computed OPUS features for opin-
ionated as well as non-evaluative language in our
corpora, obtaining overall positive results, we be-
lieve these features may also improve conventional
opinion labeling for subjective text. This will be in-
vestigated in future work.
</bodyText>
<sectionHeader confidence="0.998624" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994642">
The authors gratefully acknowledge useful discus-
sions with Don Hindle and Chip Denman.
</bodyText>
<page confidence="0.991973">
510
</page>
<sectionHeader confidence="0.995879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872519230769">
John Broder. 2007. Familiar fallback for officials: ’mis-
takes were made’. New York Times. March 14.
F. J. Damerau. 1993. Generating and evaluating domain-
oriented multi-word terms from texts. Information
Processing and Management, 29:433–447.
Hoa Trang Dang, Karin Kipper, Martha Palmer, and
Joseph Rosenzweig. 1998. Investigating Regu-
lar Sense Extensions Based on Intersective Levin
Classes. In ACL/COLING 98, pages 293–299, Mon-
treal, Canada, August 10–14.
Bonnie J. Dorr. 1993. Machine Translation: A Viewfrom
the Lexicon. The MIT Press, Cambridge, MA.
David Dowty. 1991. Thematic Proto-Roles and Argu-
ment Selection. Language, 67:547–619.
S. D. Durbin, J. N. Richter, and D. Warner. 2003. A sys-
tem for affective rating of texts. In Proc. 3rd Workshop
on Operational Text Classification, KDD-2003.
Robert M. Entman. 1993. Framing: Toward clarification
of a fractured paradigm. Journal of Communication,
43(4):51–58.
Anthony Fader, Dragomir R. Radev, Michael H. Crespin,
Burt L. Monroe, Kevin M. Quinn, and Michael Co-
laresi. 2007. MavenRank: Identifying influential
members of the US Senate using lexical centrality. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. In Proc. COLING.
M. Gentzkow and J. Shapiro. 2006a. Media bias and
reputation. Journal of Political Economy, 114:280–
316.
M. Gentzkow and J. Shapiro. 2006b. What drives
media slant? Evidence from U.S. newspapers.
http://ssrn.com/abstract=947640.
Natalie Glance, Matthew Hurst, Kamal Nigam, Matthew
Siegler, Robert Stockton, and Takashi Tomokiyo.
2005. Deriving marketing intelligence from online
discussion. In Proc. KDD’05, pages 419–428, New
York, NY, USA. ACM.
Stephan Greene. 2007. Spin: Lexical Semantics, Tran-
sitivity, and the Identification of Implicit Sentiment.
Ph.D. thesis, University of Maryland.
T. Groseclose and J. Milyo. 2005. A measure of media
bias. The Quarterly Journal ofEconomics, 120:1191–
1237.
B. Harden. 2006. On Puget Sound, It’s Orca vs. Inc. The
Washington Post. July 26, page A3.
Marti Hearst. 1992. Direction-based text interpretation
as an information access refinement. In Paul Jacobs,
editor, Text-Based Intelligent Systems, pages 257–274.
Lawrence Erlbaum Associates.
Paul Hopper and Sandra Thompson. 1980. Transitivity
in Grammar and Discourse. Language, 56:251–295.
E. Kako. 2006. Thematic role properties of subjects and
objects. Cognition, 101(1):1–42, August.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311–331.
M. Lemmens. 1998. Lexical perspectives on transitivity
and ergativity. John Benjamins.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Research Surveys in Linguistics.
Cambridge University Press, New York.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you on?
identifying perspectives at the document and sentence
levels. In Proceedings of the Conference on Natural
Language Learning (CoNLL).
Lanny W. Martin and Georg Vanberg. 2008. A ro-
bust transformation procedure for interpreting political
text. Political Analysis, 16(1):93–100.
G. McKoon and T. MacFarland. 2000. Externally and
internally caused change of state verbs. Language,
pages 833–858.
M. Mulder, A. Nijholt, M. den Uyl, and P. Terpstra.
2004. A lexical grammatical implementation of affect.
In Proc. TSD-04, Lecture notes in computer science
3206, pages 171–178). Springer-Verlag.
Tony Mullen and Robert Malouf. 2008. Taking sides:
User classification for informal online political dis-
course. Internet Research, 18:177–190.
Mari Broman Olsen and Philip Resnik. 1997. Implicit
Object Constructions and the (In)transitivity Contin-
uum. In 33rd Proceedings of the Chicago Linguistic
Society, pages 327–336.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
James Pustejovsky. 1991. The Generative Lexicon.
Computational Linguistics, 17(4):409–441.
Philip J. Stone. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. The MIT Press.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition
from Congressional floor-debate transcripts. In Proc.
EMNLP, pages 327–335.
Zhibao Wu and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proc. ACL, pages 133–138,
Las Cruces, New Mexico.
</reference>
<page confidence="0.997713">
511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489765">
<title confidence="0.815616">More than Words: Syntactic Packaging and Implicit Sentiment</title>
<affiliation confidence="0.947965">ATG,</affiliation>
<address confidence="0.9432775">1111 19th St, NW Suite Washington, DC</address>
<email confidence="0.999006">sgreene@atg.com</email>
<author confidence="0.970712">Philip</author>
<affiliation confidence="0.9935825">Linguistics / UMIACS CLIP University of</affiliation>
<address confidence="0.985597">College Park, MD</address>
<email confidence="0.999467">resnik@umiacs.umd.edu</email>
<abstract confidence="0.9989534">Work on sentiment analysis often focuses on the words and phrases that people use in overtly opinionated text. In this paper, we introduce a new approach to the problem that focuses not on lexical indicators, but on the syntactic “packaging” of ideas, which is well to investigating the identification of imor perspective. We establish a strong predictive connection between linguistically well motivated features and implicit sentiment, and then show how computational approximations of these features can be used to improve on existing state-of-the-art sentiment classification results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Broder</author>
</authors>
<title>Familiar fallback for officials: ’mistakes were made’.</title>
<date>2007</date>
<location>New York Times.</location>
<contexts>
<context position="4705" citStr="Broder, 2007" startWordPosition="756" endWordPosition="757"> and/or treatment recommendation” (Entman, 1993, p. 52). Clearly lexical choices can accomplish this kind of selection, e.g. choosing to describe a person as a terrorist rather than a freedom fighter, or referencing killer whales rather than orcas.2 Syntactic choices can also have framing effects. For example, Ronald Reagan’s famous use of the passive construction, “Mistakes were made” (in the context of the Iran-Contra scandal), is a classic example of framing or spin: used without a by-phrase, the passive avoids identifying a causal agent and therefore sidesteps the issue of responsibility (Broder, 2007). A toddler who says “My toy broke” instead of “I broke my toy” is employing the same linguistic strategy. Linguists have long studied syntactic variation in descriptions of the same event, often under the general heading of syntactic diathesis alternations (Levin, 1993; Levin and Hovav, 2005). This line of research has established a set of semantic properties that are widely viewed as “grammatically relevant” in the sense that they enable generalizations about syntactic “packaging” of meaning within (and across) the world’s languages. For example, the verb break in English participates in the</context>
</contexts>
<marker>Broder, 2007</marker>
<rawString>John Broder. 2007. Familiar fallback for officials: ’mistakes were made’. New York Times. March 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Damerau</author>
</authors>
<title>Generating and evaluating domainoriented multi-word terms from texts.</title>
<date>1993</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>29--433</pages>
<contexts>
<context position="17482" citStr="Damerau, 1993" startWordPosition="2763" endWordPosition="2764">The key idea is to use observable grammatical relations, drawn from the usages of terms determined to be relevant to a domain, as proxies for the underlying semantic properties that gave rise to their syntactic realization using those relations. Automatically crepools, regression models were run over the mean values of each observation in the experimental data. 506 ated features based on those observable proxies are then used in classification as described in Section 5. In order to identify the set T of terms relevant to a particular document collection, we adopt the relative frequency ratio (Damerau, 1993), R(t) = Rtdomain/Rtreference, where Rt, = ft Nc is the ratio of c term t’s frequency in corpus c to the size N, of that corpus. R(t) is a simple but effective comparison of a term’s prevalence in a particular collection as compared to a general reference corpus. We used the British National Corpus as the reference because it is both very large and representative of text from a wide variety of domains and genres. The threshold of R(t) permitting membership in T is an experimental parameter. OPUS features are defined in terms of syntactic dependency relations involving terms in T. Given a set D</context>
</contexts>
<marker>Damerau, 1993</marker>
<rawString>F. J. Damerau. 1993. Generating and evaluating domainoriented multi-word terms from texts. Information Processing and Management, 29:433–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Karin Kipper</author>
<author>Martha Palmer</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Investigating Regular Sense Extensions Based on Intersective Levin Classes.</title>
<date>1998</date>
<booktitle>In ACL/COLING 98,</booktitle>
<pages>293--299</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="6197" citStr="Dang et al., 1998" startWordPosition="999" endWordPosition="1002">he alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammatically relevant semantic properties of events and their 2Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semantic features to morphosyntactic behavior across a wide </context>
</contexts>
<marker>Dang, Kipper, Palmer, Rosenzweig, 1998</marker>
<rawString>Hoa Trang Dang, Karin Kipper, Martha Palmer, and Joseph Rosenzweig. 1998. Investigating Regular Sense Extensions Based on Intersective Levin Classes. In ACL/COLING 98, pages 293–299, Montreal, Canada, August 10–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Machine Translation: A Viewfrom the Lexicon.</title>
<date>1993</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6156" citStr="Dorr, 1993" startWordPosition="993" endWordPosition="994">se facts about participation in the alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammatically relevant semantic properties of events and their 2Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semantic features t</context>
</contexts>
<marker>Dorr, 1993</marker>
<rawString>Bonnie J. Dorr. 1993. Machine Translation: A Viewfrom the Lexicon. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<date>1991</date>
<booktitle>Thematic Proto-Roles and Argument Selection. Language,</booktitle>
<pages>67--547</pages>
<contexts>
<context position="6371" citStr="Dowty (1991)" startWordPosition="1027" endWordPosition="1028"> of events and their 2Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semantic features to morphosyntactic behavior across a wide variety of languages. Bringing together Dowty with Hopper and Thompson, we find 13 semantic properties organized into three groups, corresponding to the three components of a</context>
</contexts>
<marker>Dowty, 1991</marker>
<rawString>David Dowty. 1991. Thematic Proto-Roles and Argument Selection. Language, 67:547–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Durbin</author>
<author>J N Richter</author>
<author>D Warner</author>
</authors>
<title>A system for affective rating of texts.</title>
<date>2003</date>
<booktitle>In Proc. 3rd Workshop on Operational Text Classification, KDD-2003.</booktitle>
<contexts>
<context position="30965" citStr="Durbin et al., 2003" startWordPosition="4938" endWordPosition="4941"> organized, and relatively recent description of computational work on sentiment, opinion, and subjectivity analysis. The problem of classifying underlying sentiment in statements that are not overtly subjective is less studied within the NLP literature, but it has received some attention in other fields. These include, for example, research on content analysis in journalism, media studies, and political economy (Gentzkow and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; Groseclose and Milyo, 2005; Fader et al., 2007); automatic identification of customer attitudes for business e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work focusing on the notion of perspective or ideology has been reported by Martin and Vanberg (2008) and </context>
</contexts>
<marker>Durbin, Richter, Warner, 2003</marker>
<rawString>S. D. Durbin, J. N. Richter, and D. Warner. 2003. A system for affective rating of texts. In Proc. 3rd Workshop on Operational Text Classification, KDD-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Entman</author>
</authors>
<title>Framing: Toward clarification of a fractured paradigm.</title>
<date>1993</date>
<journal>Journal of Communication,</journal>
<volume>43</volume>
<issue>4</issue>
<contexts>
<context position="4139" citStr="Entman, 1993" startWordPosition="664" endWordPosition="665">Chapter of the ACL, pages 503–511, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tions 6 and 7 discuss related work and summarize. 2 Linguistic Motivation Verbal descriptions of an event often carry along with them an underlying attitude toward what is being described. By framing the same event in different ways, speakers or authors “select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Clearly lexical choices can accomplish this kind of selection, e.g. choosing to describe a person as a terrorist rather than a freedom fighter, or referencing killer whales rather than orcas.2 Syntactic choices can also have framing effects. For example, Ronald Reagan’s famous use of the passive construction, “Mistakes were made” (in the context of the Iran-Contra scandal), is a classic example of framing or spin: used without a by-phrase, the passive avoids identifying a causal agent and therefore sidesteps the issue of responsibility (Broder, 2007). A toddler who says “My toy broke</context>
</contexts>
<marker>Entman, 1993</marker>
<rawString>Robert M. Entman. 1993. Framing: Toward clarification of a fractured paradigm. Journal of Communication, 43(4):51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Dragomir R Radev</author>
<author>Michael H Crespin</author>
<author>Burt L Monroe</author>
<author>Kevin M Quinn</author>
<author>Michael Colaresi</author>
</authors>
<title>MavenRank: Identifying influential members of the US Senate using lexical centrality.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="30867" citStr="Fader et al., 2007" startWordPosition="4923" endWordPosition="4926">B-B at 85.85%. 6 Related Work Pang and Lee’s (2008) excellent monograph provides a thorough, well organized, and relatively recent description of computational work on sentiment, opinion, and subjectivity analysis. The problem of classifying underlying sentiment in statements that are not overtly subjective is less studied within the NLP literature, but it has received some attention in other fields. These include, for example, research on content analysis in journalism, media studies, and political economy (Gentzkow and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; Groseclose and Milyo, 2005; Fader et al., 2007); automatic identification of customer attitudes for business e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work foc</context>
</contexts>
<marker>Fader, Radev, Crespin, Monroe, Quinn, Colaresi, 2007</marker>
<rawString>Anthony Fader, Dragomir R. Radev, Michael H. Crespin, Burt L. Monroe, Kevin M. Quinn, and Michael Colaresi. 2007. MavenRank: Identifying influential members of the US Senate using lexical centrality. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis.</title>
<date>2004</date>
<booktitle>In Proc. COLING.</booktitle>
<marker>Gamon, 2004</marker>
<rawString>Michael Gamon. 2004. Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gentzkow</author>
<author>J Shapiro</author>
</authors>
<title>Media bias and reputation.</title>
<date>2006</date>
<journal>Journal of Political Economy,</journal>
<volume>114</volume>
<pages>316</pages>
<contexts>
<context position="30788" citStr="Gentzkow and Shapiro, 2006" startWordPosition="4911" endWordPosition="4914">obtained by Lin’s SVM fairly consistently, and in some cases approaching or matching NB-B at 85.85%. 6 Related Work Pang and Lee’s (2008) excellent monograph provides a thorough, well organized, and relatively recent description of computational work on sentiment, opinion, and subjectivity analysis. The problem of classifying underlying sentiment in statements that are not overtly subjective is less studied within the NLP literature, but it has received some attention in other fields. These include, for example, research on content analysis in journalism, media studies, and political economy (Gentzkow and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; Groseclose and Milyo, 2005; Fader et al., 2007); automatic identification of customer attitudes for business e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in th</context>
</contexts>
<marker>Gentzkow, Shapiro, 2006</marker>
<rawString>M. Gentzkow and J. Shapiro. 2006a. Media bias and reputation. Journal of Political Economy, 114:280– 316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gentzkow</author>
<author>J Shapiro</author>
</authors>
<title>What drives media slant? Evidence from U.S.</title>
<date>2006</date>
<note>newspapers. http://ssrn.com/abstract=947640.</note>
<contexts>
<context position="30788" citStr="Gentzkow and Shapiro, 2006" startWordPosition="4911" endWordPosition="4914">obtained by Lin’s SVM fairly consistently, and in some cases approaching or matching NB-B at 85.85%. 6 Related Work Pang and Lee’s (2008) excellent monograph provides a thorough, well organized, and relatively recent description of computational work on sentiment, opinion, and subjectivity analysis. The problem of classifying underlying sentiment in statements that are not overtly subjective is less studied within the NLP literature, but it has received some attention in other fields. These include, for example, research on content analysis in journalism, media studies, and political economy (Gentzkow and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; Groseclose and Milyo, 2005; Fader et al., 2007); automatic identification of customer attitudes for business e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in th</context>
</contexts>
<marker>Gentzkow, Shapiro, 2006</marker>
<rawString>M. Gentzkow and J. Shapiro. 2006b. What drives media slant? Evidence from U.S. newspapers. http://ssrn.com/abstract=947640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalie Glance</author>
<author>Matthew Hurst</author>
<author>Kamal Nigam</author>
<author>Matthew Siegler</author>
<author>Robert Stockton</author>
<author>Takashi Tomokiyo</author>
</authors>
<title>Deriving marketing intelligence from online discussion.</title>
<date>2005</date>
<booktitle>In Proc. KDD’05,</booktitle>
<pages>419--428</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="31176" citStr="Glance et al., 2005" startWordPosition="4970" endWordPosition="4973"> is less studied within the NLP literature, but it has received some attention in other fields. These include, for example, research on content analysis in journalism, media studies, and political economy (Gentzkow and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; Groseclose and Milyo, 2005; Fader et al., 2007); automatic identification of customer attitudes for business e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work focusing on the notion of perspective or ideology has been reported by Martin and Vanberg (2008) and Mullen and Malouf (2008). Among prior authors, Gamon’s (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence’s logical form, generated using a proprieta</context>
</contexts>
<marker>Glance, Hurst, Nigam, Siegler, Stockton, Tomokiyo, 2005</marker>
<rawString>Natalie Glance, Matthew Hurst, Kamal Nigam, Matthew Siegler, Robert Stockton, and Takashi Tomokiyo. 2005. Deriving marketing intelligence from online discussion. In Proc. KDD’05, pages 419–428, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
</authors>
<title>Spin: Lexical Semantics, Transitivity, and the Identification of Implicit Sentiment.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland.</institution>
<contexts>
<context position="10352" citStr="Greene (2007)" startWordPosition="1641" endWordPosition="1642">se emphasize properties of the object. Variation of syntactic description involved two forms: a transitive syntactic frame with a human agent as subject (“transitive form”, 2a), and a nominalization of the verb as subject and the verb kill as the predicate (“nominalized form”, 2b). 2(a) The gunmen shot the opposition leader (b) The shooting killed the opposition leader Participants and procedure. A set of 18 volunteer participants, all native speakers of English, were presented with event descriptions and asked to answer questions probing both Dowty’s proto-role 5Full details and materials in Greene (2007). properties as well as Hopper and Thompson’s semantic transitivity components, responding via ratings on a 1-to-7 scale. For example, the questions probing volition were: “In this event, how likely is it that (subject) chose to be involved?”, where (subject) was the gunmen and the shooting, for 2(ab), respectively.6 3.2 Sentiment ratings Materials. We used the materials above to construct short, newspaper-like paragraphs, each one accompanied by a “headline” version of the same syntactic descriptions used above. For example, given this paragraph: A man has been charged for the suffocation of </context>
<context position="21388" citStr="Greene (2007)" startWordPosition="3383" endWordPosition="3384">ification using the Bitter Lemons corpus of Lin et al. (2006). 5.1 Predicting Opinions of the Death Penalty Corpus. We constructed a new corpus for experimentation on implicit sentiment by downloading the contents of pro- and anti-death-penalty Web sites and manually checking, for a large subset, that the viewpoints expressed in documents were as expected. The collection, which we will refer to as the DP corpus, comprises documents from five pro-death-penalty sites and three anti-death-penalty sites, and the corpus was engineered to have an even balance, 596 documents per side.12 12Details in Greene (2007). 507 Frequent bigram baseline. We adopted a supervised classification approach based on word n-gram features, using SVM classification in the WEKA machine learning package. In initial exploration using both unigrams and bigrams, and using both word forms and stems, we found that performance did not differ significantly, and chose stemmed bigrams for our baseline comparisons. In order to control for the difference in the number of features available to the classifier in our comparisons, we use the N most frequent stemmed bigrams as the baseline feature set where N is matched to number of OPUS </context>
<context position="32736" citStr="Greene, 2007" startWordPosition="5223" endWordPosition="5224">s of computational experimentation, work by Thomas et al. (2006), predicting yes and no votes in corpus of United States Congressional floor debate speeches, is quite relevant. They combined SVM classification with a min-cut model on graphs in order to exploit both direct textual evidence and constraints suggested by the structure of Congressional debates, e.g. the fact that the same individual rarely gives one speech in favor of a bill and another opposing it. We have extend their method to use OPUS features in the SVM and obtained significant improvements over their classification accuracy (Greene, 2007; Greene and Resnik, in preparation). 7 Conclusions In this paper we have introduced an approach to implicit sentiment motivated by theoretical work in lexical semantics, presenting evidence for the role of semantic properties in human sentiment judgments. This research is, to our knowledge, the first to draw an explicit and empirically supported connection between theoretically motivated work in lexical semantics and readers’ perception of sentiment. In addition, we have reported positive sentiment classification results within a standard supervised learning setting, employing a practical fir</context>
</contexts>
<marker>Greene, 2007</marker>
<rawString>Stephan Greene. 2007. Spin: Lexical Semantics, Transitivity, and the Identification of Implicit Sentiment. Ph.D. thesis, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Groseclose</author>
<author>J Milyo</author>
</authors>
<title>A measure of media bias.</title>
<date>2005</date>
<journal>The Quarterly Journal ofEconomics,</journal>
<volume>120</volume>
<pages>1237</pages>
<contexts>
<context position="30846" citStr="Groseclose and Milyo, 2005" startWordPosition="4919" endWordPosition="4922">es approaching or matching NB-B at 85.85%. 6 Related Work Pang and Lee’s (2008) excellent monograph provides a thorough, well organized, and relatively recent description of computational work on sentiment, opinion, and subjectivity analysis. The problem of classifying underlying sentiment in statements that are not overtly subjective is less studied within the NLP literature, but it has received some attention in other fields. These include, for example, research on content analysis in journalism, media studies, and political economy (Gentzkow and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; Groseclose and Milyo, 2005; Fader et al., 2007); automatic identification of customer attitudes for business e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). </context>
</contexts>
<marker>Groseclose, Milyo, 2005</marker>
<rawString>T. Groseclose and J. Milyo. 2005. A measure of media bias. The Quarterly Journal ofEconomics, 120:1191– 1237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Harden</author>
</authors>
<title>On Puget Sound, It’s Orca vs.</title>
<date>2006</date>
<volume>26</volume>
<pages>3</pages>
<publisher>Inc. The</publisher>
<location>Washington Post.</location>
<contexts>
<context position="5943" citStr="Harden, 2006" startWordPosition="959" endWordPosition="960">rnation (causative event X broke Y can also be expressed without overt causation as Y broke), but the verb climb does not (X also causes the event in X climbed Y, but that event cannot be expressed as Y climbed). These facts about participation in the alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammatically relevant semantic properties of events and their 2Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expre</context>
</contexts>
<marker>Harden, 2006</marker>
<rawString>B. Harden. 2006. On Puget Sound, It’s Orca vs. Inc. The Washington Post. July 26, page A3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Direction-based text interpretation as an information access refinement.</title>
<date>1992</date>
<booktitle>Text-Based Intelligent Systems,</booktitle>
<pages>257--274</pages>
<editor>In Paul Jacobs, editor,</editor>
<contexts>
<context position="31923" citStr="Hearst (1992)" startWordPosition="5094" endWordPosition="5095">rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work focusing on the notion of perspective or ideology has been reported by Martin and Vanberg (2008) and Mullen and Malouf (2008). Among prior authors, Gamon’s (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence’s logical form, generated using a proprietary system. However, his features are templatic in nature in that they do not couple specific lexical entries with their logical form. Hearst (1992) and Mulder et al. (2004) describe systems that make use of argument structure features coupled with lexical information, though neither provides implementation details or experimental results. In terms of computational experimentation, work by Thomas et al. (2006), predicting yes and no votes in corpus of United States Congressional floor debate speeches, is quite relevant. They combined SVM classification with a min-cut model on graphs in order to exploit both direct textual evidence and constraints suggested by the structure of Congressional debates, e.g. the fact that the same individual r</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Direction-based text interpretation as an information access refinement. In Paul Jacobs, editor, Text-Based Intelligent Systems, pages 257–274. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Hopper</author>
<author>Sandra Thompson</author>
</authors>
<date>1980</date>
<booktitle>Transitivity in Grammar and Discourse. Language,</booktitle>
<pages>56--251</pages>
<contexts>
<context position="6662" citStr="Hopper and Thompson (1980)" startWordPosition="1072" endWordPosition="1075"> work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semantic features to morphosyntactic behavior across a wide variety of languages. Bringing together Dowty with Hopper and Thompson, we find 13 semantic properties organized into three groups, corresponding to the three components of a canonical transitive clause, expressed as X verb Y in English.4 Properties associated with X involve volitional involvement in the event or state, causation of the event, sentience/awareness and/or perception, causing a change of state in Y, kinesis or movement, and existence independent o</context>
<context position="20084" citStr="Hopper and Thompson, 1980" startWordPosition="3179" endWordPosition="3182">ciated with omission of the direct object (Olsen and Resnik, 1997). The bold text in (5) yields NOOBJ:kill as a feature. 5(a) At the same time, we should never ignore the risks of allowing the inmate to kill again. In this case, omitting the direct object decreases the extent to which the killing event is interpreted as telic, and it eliminates the possibility of attributing change-of-state to a specific affected object (much like “Mistakes were made” avoids attributing cause to a specified subject), placing the phrasing at a less “semantically transitive” point on the transitivity continuum (Hopper and Thompson, 1980). Some informants find a perceptible increase in negative sentiment toward inmate when the sentence is phrased as in 5(b): 5(b) At the same time, we should never ignore the risks of allowing the inmate to kill someone again. 5 Computational Application Having discussed linguistic motivation, empirical validation, and practical approximation of semantically relevant features, we now present two studies demonstrating their value in sentiment classification. For the first study, we have constructed a new data set particularly well suited for testing our approach, based on writing about the death </context>
</contexts>
<marker>Hopper, Thompson, 1980</marker>
<rawString>Paul Hopper and Sandra Thompson. 1980. Transitivity in Grammar and Discourse. Language, 56:251–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kako</author>
</authors>
<title>Thematic role properties of subjects and objects.</title>
<date>2006</date>
<journal>Cognition,</journal>
<volume>101</volume>
<issue>1</issue>
<contexts>
<context position="8017" citStr="Kako (2006)" startWordPosition="1291" endWordPosition="1292">punctuality (the latter of which may be inversely related to a property known as incremental theme). Properties associated with Y include affectedness, change of state, (lack of) kinesis or movement, and (lack of) existence independent of the event. Now, observe that this set of semantic properties involves many of the questions that would naturally help to shape one’s opinion about the event described by veer in (1). Was anyone or anything affected by what took place, and to what degree? Did the event just happen or was it caused? Did the event reach a defined endpoint? Did participation in 3Kako (2006) has verified that people make these inferences based on X’s syntactic position even when a semantically empty nonsense verb is used. 4We are deliberately sidestepping the choice of terminology for X and Y, e.g. proto-Patient, theme, etc. 504 the event involve conscious thought or intent? Our hypothesis is that the syntactic aspects of “framing”, as characterized by Entman, involve manipulation of these semantic properties, even when overt opinions are not being expressed. That is, we propose a connection between syntactic choices and implicit sentiment mediated by the very same semantic prope</context>
</contexts>
<marker>Kako, 2006</marker>
<rawString>E. Kako. 2006. Thematic role properties of subjects and objects. Cognition, 101(1):1–42, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Laver</author>
<author>Kenneth Benoit</author>
<author>John Garry</author>
</authors>
<title>Extracting policy positions from political texts using words as data.</title>
<date>2003</date>
<journal>American Political Science Review,</journal>
<volume>97</volume>
<issue>2</issue>
<contexts>
<context position="31443" citStr="Laver et al., 2003" startWordPosition="5013" endWordPosition="5016">eclose and Milyo, 2005; Fader et al., 2007); automatic identification of customer attitudes for business e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work focusing on the notion of perspective or ideology has been reported by Martin and Vanberg (2008) and Mullen and Malouf (2008). Among prior authors, Gamon’s (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence’s logical form, generated using a proprietary system. However, his features are templatic in nature in that they do not couple specific lexical entries with their logical form. Hearst (1992) and Mulder et al. (2004) describe systems that make use of argument structure features coupled with lexical information</context>
</contexts>
<marker>Laver, Benoit, Garry, 2003</marker>
<rawString>Michael Laver, Kenneth Benoit, and John Garry. 2003. Extracting policy positions from political texts using words as data. American Political Science Review, 97(2):311–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lemmens</author>
</authors>
<title>Lexical perspectives on transitivity and ergativity.</title>
<date>1998</date>
<journal>John Benjamins.</journal>
<contexts>
<context position="9259" citStr="Lemmens, 1998" startWordPosition="1472" endWordPosition="1473">ready identified as central when connecting surface expression to underlying meaning more generally. 3 Empirical Validation We validated the hypothesized connection between implicit sentiment and grammatically relevant semantic properties using psycholinguistic methods, by varying the syntactic form of event descriptions, and showing that the semantic properties of descriptions do indeed predict perceived sentiment.5 3.1 Semantic property ratings Materials. Stimuli were constructed using 11 verbs of killing, which are widely viewed as prototypical for the semantic properties of interest here (Lemmens, 1998): X killed Y normally involves conscious, intentional causation by X of a kinetic event that causes a (rather decisive and clearly terminated!) change of state in Y . The verbs comprise two classes: the “transitive” class, involving externally caused change-of-state verbs (kill, slaughter, assassinate, shoot, poison), and the “ergative” class (strangle, smother, choke, drown, suffocate, starve), within which verbs are internally caused (McKoon and MacFarland, 2000) or otherwise emphasize properties of the object. Variation of syntactic description involved two forms: a transitive syntactic fra</context>
</contexts>
<marker>Lemmens, 1998</marker>
<rawString>M. Lemmens. 1998. Lexical perspectives on transitivity and ergativity. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>Malka Rappaport Hovav</author>
</authors>
<title>Argument Realization. Research Surveys in Linguistics.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="4999" citStr="Levin and Hovav, 2005" startWordPosition="800" endWordPosition="803">ming effects. For example, Ronald Reagan’s famous use of the passive construction, “Mistakes were made” (in the context of the Iran-Contra scandal), is a classic example of framing or spin: used without a by-phrase, the passive avoids identifying a causal agent and therefore sidesteps the issue of responsibility (Broder, 2007). A toddler who says “My toy broke” instead of “I broke my toy” is employing the same linguistic strategy. Linguists have long studied syntactic variation in descriptions of the same event, often under the general heading of syntactic diathesis alternations (Levin, 1993; Levin and Hovav, 2005). This line of research has established a set of semantic properties that are widely viewed as “grammatically relevant” in the sense that they enable generalizations about syntactic “packaging” of meaning within (and across) the world’s languages. For example, the verb break in English participates in the causativeinchoative alternation (causative event X broke Y can also be expressed without overt causation as Y broke), but the verb climb does not (X also causes the event in X climbed Y, but that event cannot be expressed as Y climbed). These facts about participation in the alternation turn </context>
</contexts>
<marker>Levin, Hovav, 2005</marker>
<rawString>Beth Levin and Malka Rappaport Hovav. 2005. Argument Realization. Research Surveys in Linguistics. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="4975" citStr="Levin, 1993" startWordPosition="798" endWordPosition="799">also have framing effects. For example, Ronald Reagan’s famous use of the passive construction, “Mistakes were made” (in the context of the Iran-Contra scandal), is a classic example of framing or spin: used without a by-phrase, the passive avoids identifying a causal agent and therefore sidesteps the issue of responsibility (Broder, 2007). A toddler who says “My toy broke” instead of “I broke my toy” is employing the same linguistic strategy. Linguists have long studied syntactic variation in descriptions of the same event, often under the general heading of syntactic diathesis alternations (Levin, 1993; Levin and Hovav, 2005). This line of research has established a set of semantic properties that are widely viewed as “grammatically relevant” in the sense that they enable generalizations about syntactic “packaging” of meaning within (and across) the world’s languages. For example, the verb break in English participates in the causativeinchoative alternation (causative event X broke Y can also be expressed without overt causation as Y broke), but the verb climb does not (X also causes the event in X climbed Y, but that event cannot be expressed as Y climbed). These facts about participation </context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on? identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="20836" citStr="Lin et al. (2006)" startWordPosition="3297" endWordPosition="3300">same time, we should never ignore the risks of allowing the inmate to kill someone again. 5 Computational Application Having discussed linguistic motivation, empirical validation, and practical approximation of semantically relevant features, we now present two studies demonstrating their value in sentiment classification. For the first study, we have constructed a new data set particularly well suited for testing our approach, based on writing about the death penalty. In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al. (2006). 5.1 Predicting Opinions of the Death Penalty Corpus. We constructed a new corpus for experimentation on implicit sentiment by downloading the contents of pro- and anti-death-penalty Web sites and manually checking, for a large subset, that the viewpoints expressed in documents were as expected. The collection, which we will refer to as the DP corpus, comprises documents from five pro-death-penalty sites and three anti-death-penalty sites, and the corpus was engineered to have an even balance, 596 documents per side.12 12Details in Greene (2007). 507 Frequent bigram baseline. We adopted a sup</context>
<context position="25616" citStr="Lin et al. (2006)" startWordPosition="4054" endWordPosition="4057">, but rather the properties of event encodings involving the kill verbs themselves. 5.2 Predicting Points of View in the Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we report on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons 13Site (# of documents): pro1= clarkprosecutor.org (437), pro2= prodeathpenalty.com (117), anti1= deathpenaltyinfo.org (319), anti2= nodeathpenalty.org (212) 508 (hence BL) corpus introduced by Lin et al. (2006). Corpus. The Bitter Lemons corpus comprises essays posted at www.bitterlemons.org,which, in the words of the site, “present Israeli and Palestinian viewpoints on prominent issues of concern”. As a corpus, it has a number of interesting properties. First, its topic area is one of significant interest and considerable controversy, yet the general tenor of the web site is one that eschews an overly shrill or extreme style of writing. Second, the site is organized in terms of issue-focused weekly editions that include essays with contrasting viewpoints from the site’s two editors, plus two essays</context>
<context position="31299" citStr="Lin et al. (2006)" startWordPosition="4990" endWordPosition="4993">search on content analysis in journalism, media studies, and political economy (Gentzkow and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; Groseclose and Milyo, 2005; Fader et al., 2007); automatic identification of customer attitudes for business e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work focusing on the notion of perspective or ideology has been reported by Martin and Vanberg (2008) and Mullen and Malouf (2008). Among prior authors, Gamon’s (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence’s logical form, generated using a proprietary system. However, his features are templatic in nature in that they do not couple specific lexical entries with their log</context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on? identifying perspectives at the document and sentence levels. In Proceedings of the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanny W Martin</author>
<author>Georg Vanberg</author>
</authors>
<title>A robust transformation procedure for interpreting political text. Political Analysis,</title>
<date>2008</date>
<contexts>
<context position="31560" citStr="Martin and Vanberg (2008)" startWordPosition="5032" endWordPosition="5035"> routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work focusing on the notion of perspective or ideology has been reported by Martin and Vanberg (2008) and Mullen and Malouf (2008). Among prior authors, Gamon’s (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence’s logical form, generated using a proprietary system. However, his features are templatic in nature in that they do not couple specific lexical entries with their logical form. Hearst (1992) and Mulder et al. (2004) describe systems that make use of argument structure features coupled with lexical information, though neither provides implementation details or experimental results. In terms of computational experimentation, </context>
</contexts>
<marker>Martin, Vanberg, 2008</marker>
<rawString>Lanny W. Martin and Georg Vanberg. 2008. A robust transformation procedure for interpreting political text. Political Analysis, 16(1):93–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G McKoon</author>
<author>T MacFarland</author>
</authors>
<title>Externally and internally caused change of state verbs.</title>
<date>2000</date>
<journal>Language,</journal>
<pages>833--858</pages>
<contexts>
<context position="9728" citStr="McKoon and MacFarland, 2000" startWordPosition="1539" endWordPosition="1542">rials. Stimuli were constructed using 11 verbs of killing, which are widely viewed as prototypical for the semantic properties of interest here (Lemmens, 1998): X killed Y normally involves conscious, intentional causation by X of a kinetic event that causes a (rather decisive and clearly terminated!) change of state in Y . The verbs comprise two classes: the “transitive” class, involving externally caused change-of-state verbs (kill, slaughter, assassinate, shoot, poison), and the “ergative” class (strangle, smother, choke, drown, suffocate, starve), within which verbs are internally caused (McKoon and MacFarland, 2000) or otherwise emphasize properties of the object. Variation of syntactic description involved two forms: a transitive syntactic frame with a human agent as subject (“transitive form”, 2a), and a nominalization of the verb as subject and the verb kill as the predicate (“nominalized form”, 2b). 2(a) The gunmen shot the opposition leader (b) The shooting killed the opposition leader Participants and procedure. A set of 18 volunteer participants, all native speakers of English, were presented with event descriptions and asked to answer questions probing both Dowty’s proto-role 5Full details and ma</context>
</contexts>
<marker>McKoon, MacFarland, 2000</marker>
<rawString>G. McKoon and T. MacFarland. 2000. Externally and internally caused change of state verbs. Language, pages 833–858.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mulder</author>
<author>A Nijholt</author>
<author>M den Uyl</author>
<author>P Terpstra</author>
</authors>
<title>A lexical grammatical implementation of affect.</title>
<date>2004</date>
<booktitle>In Proc. TSD-04, Lecture notes in computer science 3206,</booktitle>
<pages>171--178</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Mulder, Nijholt, den Uyl, Terpstra, 2004</marker>
<rawString>M. Mulder, A. Nijholt, M. den Uyl, and P. Terpstra. 2004. A lexical grammatical implementation of affect. In Proc. TSD-04, Lecture notes in computer science 3206, pages 171–178). Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Robert Malouf</author>
</authors>
<title>Taking sides: User classification for informal online political discourse.</title>
<date>2008</date>
<journal>Internet Research,</journal>
<pages>18--177</pages>
<contexts>
<context position="31589" citStr="Mullen and Malouf (2008)" startWordPosition="5037" endWordPosition="5040">. And, of course, the study of perceptions in politics and media bears a strong family resemblance to real-world marketing problems involving reputation management and business intelligence (Glance et al., 2005). Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work focusing on the notion of perspective or ideology has been reported by Martin and Vanberg (2008) and Mullen and Malouf (2008). Among prior authors, Gamon’s (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence’s logical form, generated using a proprietary system. However, his features are templatic in nature in that they do not couple specific lexical entries with their logical form. Hearst (1992) and Mulder et al. (2004) describe systems that make use of argument structure features coupled with lexical information, though neither provides implementation details or experimental results. In terms of computational experimentation, work by Thomas et al. (2006),</context>
</contexts>
<marker>Mullen, Malouf, 2008</marker>
<rawString>Tony Mullen and Robert Malouf. 2008. Taking sides: User classification for informal online political discourse. Internet Research, 18:177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Broman Olsen</author>
<author>Philip Resnik</author>
</authors>
<title>Implicit Object Constructions and the (In)transitivity Continuum.</title>
<date>1997</date>
<booktitle>In 33rd Proceedings of the Chicago Linguistic Society,</booktitle>
<pages>327--336</pages>
<contexts>
<context position="19524" citStr="Olsen and Resnik, 1997" startWordPosition="3089" endWordPosition="3092">NS:murder, murder:nsubj, nsubj:prisoner, murder:aux, aux:will, murder:dobj, dobj:guard Intuitively the presence of TRANS:murder suggests the entire complex of semantic properties discussed in Section 2, bringing together the impliciation of volition, causation, etc. on the part of prisoner (as does nsubj:prisoner), affectedness and change of state on the part of guard (as does dobj:guard), and so forth. 11We parsed English text using the Stanford parser. The NOOBJ features can capture a habitual reading, or in some cases a detransitivizing effect associated with omission of the direct object (Olsen and Resnik, 1997). The bold text in (5) yields NOOBJ:kill as a feature. 5(a) At the same time, we should never ignore the risks of allowing the inmate to kill again. In this case, omitting the direct object decreases the extent to which the killing event is interpreted as telic, and it eliminates the possibility of attributing change-of-state to a specific affected object (much like “Mistakes were made” avoids attributing cause to a specified subject), placing the phrasing at a less “semantically transitive” point on the transitivity continuum (Hopper and Thompson, 1980). Some informants find a perceptible inc</context>
</contexts>
<marker>Olsen, Resnik, 1997</marker>
<rawString>Mari Broman Olsen and Philip Resnik. 1997. Implicit Object Constructions and the (In)transitivity Continuum. In 33rd Proceedings of the Chicago Linguistic Society, pages 327–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="929" citStr="Pang and Lee (2008)" startWordPosition="134" endWordPosition="137"> focuses on the words and phrases that people use in overtly opinionated text. In this paper, we introduce a new approach to the problem that focuses not on lexical indicators, but on the syntactic “packaging” of ideas, which is well suited to investigating the identification of implicit sentiment, or perspective. We establish a strong predictive connection between linguistically well motivated features and implicit sentiment, and then show how computational approximations of these features can be used to improve on existing state-of-the-art sentiment classification results. 1 Introduction As Pang and Lee (2008) observe, the last several years have seen a “land rush” in research on sentiment analysis and opinion mining, with a frequent emphasis on the identification of opinions in evaluative text such as movie or product reviews. However, sentiment also may be carried implicitly by statements that are not only non-evaluative, but not even visibly subjective. Consider, for example, the following two descriptions of the same (invented) event: 1(a) On November 25, a soldier veered his jeep into a crowded market and killed three civilians. (b) On November 25, a soldier’s jeep veered into a crowded market</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The Generative Lexicon.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="6144" citStr="Pustejovsky, 1991" startWordPosition="991" endWordPosition="992"> as Y climbed). These facts about participation in the alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammatically relevant semantic properties of events and their 2Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semanti</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>James Pustejovsky. 1991. The Generative Lexicon. Computational Linguistics, 17(4):409–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2080" citStr="Stone, 1966" startWordPosition="326" endWordPosition="327">On November 25, a soldier’s jeep veered into a crowded market, causing three civilian deaths. ∗This work was done while the first author was a student in the Department of Linguistics, University of Maryland. Both descriptions appear on the surface to be objective statements, and they use nearly the same words. Lexically, the sentences’ first clauses differ only in the difference between ’s and his to express the relationship between the soldier and the jeep, and in the second clauses both kill and death are terms with negative connotations, at least according to the General Inquirer lexicon (Stone, 1966). Yet the descriptions clearly differ in the feelings they evoke: if the soldier were being tried for his role in what happened on November 25, surely the prosecutor would be more likely to say (1a) to the jury, and the defense attorney (1b), rather than the reverse.1 Why, then, should a description like (1a) be perceived as less sympathetic to the soldier than (1b)? If the difference is not in the words, it must be in the way they are put together; that is, the structure of the sentence. In Section 2, we offer a specific hypothesis about the connection between structure and implicit sentiment</context>
</contexts>
<marker>Stone, 1966</marker>
<rawString>Philip J. Stone. 1966. The General Inquirer: A Computer Approach to Content Analysis. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="32188" citStr="Thomas et al. (2006)" startWordPosition="5131" endWordPosition="5134">len and Malouf (2008). Among prior authors, Gamon’s (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence’s logical form, generated using a proprietary system. However, his features are templatic in nature in that they do not couple specific lexical entries with their logical form. Hearst (1992) and Mulder et al. (2004) describe systems that make use of argument structure features coupled with lexical information, though neither provides implementation details or experimental results. In terms of computational experimentation, work by Thomas et al. (2006), predicting yes and no votes in corpus of United States Congressional floor debate speeches, is quite relevant. They combined SVM classification with a min-cut model on graphs in order to exploit both direct textual evidence and constraints suggested by the structure of Congressional debates, e.g. the fact that the same individual rarely gives one speech in favor of a bill and another opposing it. We have extend their method to use OPUS features in the SVM and obtained significant improvements over their classification accuracy (Greene, 2007; Greene and Resnik, in preparation). 7 Conclusions </context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proc. EMNLP, pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb Semantics and Lexical Selection.</title>
<date>1994</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="6177" citStr="Wu and Palmer, 1994" startWordPosition="995" endWordPosition="998">ut participation in the alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammatically relevant semantic properties of events and their 2Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semantic features to morphosyntactic beh</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibao Wu and Martha Palmer. 1994. Verb Semantics and Lexical Selection. In Proc. ACL, pages 133–138, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>