<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.987568">
Tracking unbounded Topic Streams
</title>
<author confidence="0.998966">
Dominik Wurzer
</author>
<affiliation confidence="0.744156">
School of Informatics
University of Edinburgh
d.s.wurzer
</affiliation>
<email confidence="0.916873">
@sms.ed.ac.uk
</email>
<author confidence="0.994446">
Victor Lavrenko
</author>
<affiliation confidence="0.746781333333333">
School of Informatics
University of Edinburgh
vlavrenk
</affiliation>
<email confidence="0.856988">
@inf.ed.ac.uk
</email>
<note confidence="0.9233155">
Miles Osborne
Bloomberg
London
mosborne29
</note>
<email confidence="0.86848">
@bloomberg.net
</email>
<sectionHeader confidence="0.995729" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999734352941177">
Tracking topics on social media streams is
non-trivial as the number of topics men-
tioned grows without bound. This com-
plexity is compounded when we want to
track such topics against other fast mov-
ing streams. We go beyond traditional
small scale topic tracking and consider a
stream of topics against another document
stream. We introduce two tracking ap-
proaches which are fully applicable to true
streaming environments. When tracking
4.4 million topics against 52 million doc-
uments in constant time and space, we
demonstrate that counter to expectations,
simple single-pass clustering can outper-
form locality sensitive hashing for nearest
neighbour search on streams.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999833189655173">
The emergence of massive social media streams
has sparked a growing need for systems able to
process them. While previous research (Hassan
et al., 2009; Becker et al., 2009; Petrovic et
al., 2010; Cataldi et al., (2010); Weng et al.,
(2011); Petrovic 2013) has focused on detecting
new topics in unbounded textual streams, less
attention was paid to following (tracking) the
steadily growing set of topics. Standard topic
tracking (Allan, 2002) deals with helping human
analysts follow and monitor ongoing events on
massive data streams. By pairing topics with
relevant documents, topic tracking splits a noisy
stream of documents into sub-streams grouped
by their target topics. This is a crucial task for
financial and security analysts who are interested
in pulling together relevant information from
unstructured and noisy data streams. Other fields
like summarization or topic modeling benefit
from topic tracking as a mean to generate their
data sources.
In todays data streams however, new topics
emerge on a continual basis and we are interested
in following all instead of just a small fraction
of newly detected topics. Since its introduction
(Allan, 2002), standard topic tracking typically
operates on a small scale and against a static
set of predefined target topics. We go beyond
such approaches and deal for the first time with
massive, unbounded topic streams. Examples
of unbounded topic streams include all events
reported by news agencies each day across the
world; popular examples of unbounded document
streams include social media services such as
Twitter. Tracking streams of topics allows re-
search tasks like topic-modeling or summarization
to be applied to millions of topics, a scale that
is several orders of magnitude larger than those
of current publications. We present two massive
scale topic tracking systems capable of tracking
unbounded topic streams. One is based on locality
sensitive hashing (LSH) and the other on clus-
tering. Since we operate on two unbounded data
sources we are subject to the streaming model of
computation (Muthukrishnan, 2005), which re-
quires instant and single-pass decision making in
constant time and space. Contrary to expectations,
we find that nearest neighbour search on a stream
based on clustering performs faster than LSH for
the same level of accuracy. This is surprising as
LSH is widely believed to be the fastest way of
nearest neighbour search. Our experiments reveal
how simple single-pass clustering outperforms
LSH in terms of effectiveness and efficiency. Our
results are general and apply to any setting where
we have massive or infinite numbers of topics,
matched against unboundedly large document
streams.
</bodyText>
<page confidence="0.923922">
1765
</page>
<note confidence="0.985276">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1765–1773,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<title confidence="0.309234">
Contributions
</title>
<listItem confidence="0.986782384615385">
• For the first time we show how it is possi-
ble to track an unbounded stream of topics
in constant time and space, while maintain-
ing a level of effectiveness that is statistically
indistinguishable from an exact tracking sys-
tem
• We show how single-pass clustering can out-
perform locality sensitive hashing in terms
of effectiveness and efficiency for identifying
nearest neighbours in a stream
• We demonstrate that standard measures of
similarity are sub-optimal when matching
short documents against long documents
</listItem>
<sectionHeader confidence="0.999664" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999981326530612">
Topic or event tracking was first introduced in the
Topic Detection and Tracking (TDT) program (Al-
lan, 2002). In TDT, topic tracking involves mon-
itoring a stream of news documents with the in-
tent to identify those documents relevant to a small
predefined set of target topics. During the course
of TDT, research focused extensively on the effec-
tiveness of tracking systems, neglecting scale and
efficiency. The three official data sets only range
from 25k to 74k documents with a few hundred
topics (Allan, 2002).
More recently, the rise of publicly available
real-time social media streams triggered new
research on topic detection and tracking, in-
tended to apply the technology to those high
volume document streams. The novel data
streams differ from the TDT data sets in their
volume and level of noise. To provide real-
time applications, traditional methods need to
be overhauled to keep computation feasible.
It became common practice to limit data sets to
cope with the computational effort. Popular strate-
gies involve reducing the number of tracked top-
ics (Lin et al., 2011; Nichols et al., 2012;) as
well as sampling the document stream (Ghosh et
al., 2013). These approaches have proven to be
efficient in cutting down workload but they also
limit an application’s performance. Furthermore,
Sayyadi et al. (2009) discovered and tracked top-
ics in social streams based on keyword graphs.
They applied the sliding window principle to keep
the computation feasible, although their data set
only contained 18k documents. Yang et al. 2012
tracked topics in tweet streams using language
models. To cope with the computational effort
they assume a small set of topics of only a few
dozen, which are defined in advance. Tang et al.
(2011) tracked a single topic on a few thousand
blogs based on semantic graph topic models. Pon
et al. (2007) recommend news by tracking multi-
ple topics for a user but their data sets only span
several thousand documents and a few topics.
Further related work includes the real-time filter-
ing task, introduced as part of TREC’s Microblog
Track in 2012 (Soboroff et al., 2012). Hong et al.
(2013) explore topic tracking in tweet streams in
relation to the TREC real-time filtering task by re-
lying on a sliding window principle, while focus-
ing on the cold start problem.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="method">
3 Topic Tracking
</sectionHeader>
<subsectionHeader confidence="0.999769">
3.1 Traditional Approach
</subsectionHeader>
<bodyText confidence="0.999695466666667">
Numerous approaches to topic tracking have
emerged, spanning from probabilistic retrieval to
statistical classification frameworks. While there
is no single general approach, we define the tradi-
tional approach to tracking from a high-level per-
spective covering the basic principle of all previ-
ous approaches. We do not make any assump-
tions about the kind of topics, documents or dis-
tance functions used. As defined by TDT (Allan,
2002), we assume, we operate on an unbounded
document stream with the goal of tracking a fixed
set of target topics. Although topics are allowed to
drift conceptually and evolve over time, new top-
ics would always trigger the start of a new tracking
system.
</bodyText>
<figure confidence="0.622798636363636">
Algorithm 1 Traditional Tracking
INPUT:
TOPIC-SET {t e T}
DOCUMENT-STREAM {d e D}
OUTPUT:
relevant topic-document pairs {t, d}
while documents d in stream D do
for all topics t in set T do
similarity = computeSimilarity(d,t)
if similarity &gt; threshold then
emit relevant {t, d}
</figure>
<bodyText confidence="0.999612">
As seen in Algorithm 1, documents arrive one at
a time, requiring instant decision making through
single pass processing. Each document is com-
pared to all topics representations to identify the
closest topic. The tracking decision is based on the
similarity to the closest topic and usually defined
by a thresholding strategy. Because incoming doc-
uments can be relevant to more than one topic, we
</bodyText>
<page confidence="0.984767">
1766
</page>
<bodyText confidence="0.99975225">
need to match it against all of them. Due to its sim-
plicity, the traditional tracking approach is highly
efficient when applied to a fairly low number of
topics.
</bodyText>
<subsectionHeader confidence="0.999788">
3.2 Shortcomings of the traditional approach
</subsectionHeader>
<bodyText confidence="0.999986588235294">
The traditional approach - though low in compu-
tational effort - becomes challenging when scal-
ing up the number of target topics. The compu-
tational effort arises from the number of compar-
isons made (the number of documents times top-
ics). That explains, why researches following the
traditional approach have either lowered the num-
ber of documents or topics. Heuristics and index-
ing methods increase the performance but offer no
solution scalable to true streaming environments
because they only allow for one-side scaling (ei-
ther a large number of documents or topics). In-
creasing either of the two components by a single
document, increases the computational effort by
the magnitude of the other one. For the extreme
case of pushing to an infinite number of topics,
tracking in constant space is a necessity.
</bodyText>
<sectionHeader confidence="0.902162" genericHeader="method">
4 Tracking at scale
</sectionHeader>
<bodyText confidence="0.999959631578947">
Before directly turning to a full streaming set up
in constant space, we approach tracking a topic
stream on a document stream in unbounded space.
The key to scale up documents and topics, lies
in reducing the number of necessary comparisons.
Throughout the remainder of this paper we repre-
sent documents and topics arriving from a steady
high volume stream by term-weighted vectors in
the vector space.
In order to cut down the search space, we encap-
sulate every topic vector by a hypothetical region
marking its area of proximity. Those regions are
intend to capture documents that are more likely
to be relevant. Ideally, these regions form a hy-
persphere centred around every topic vector with
a radius equal to the maximum distance to rele-
vant documents. The tracking procedure is then
reduced to determining whether an incoming doc-
ument is also enclosed by any of the hyperspheres.
</bodyText>
<subsectionHeader confidence="0.996174">
4.1 Approximated Tracking
</subsectionHeader>
<bodyText confidence="0.999756290322581">
Our first attempt to reach sub-linear execution
time uses random segmentation of the vector space
using hashing techniques. We frame the track-
ing process as a nearest neighbour search prob-
lem, as defined by Gionis et al. (1999). Docu-
ments arriving from a stream are seen as queries
and the closest topics are the nearest neighbours to
be identified. We explore locality sensitive hash-
ing (LSH), as described by Indyk et al. (1998),
to approach high dimensional nearest neighbour
search for topic tracking in sub-linear time. LSH,
which has been used to speed up NLP applica-
tions (Ravichandran et al., 2005), provides hash
functions that guarantee that similar documents
are more likely to be hashed to the same binary
hash key than distant ones. Hash functions capture
similarities between vectors in high dimensions
and represent them on a low dimensional binary
level. We apply the scheme by Charikar (2002),
which describes the probabilistic bounds for the
cosine similarity between two vectors. Each bit in
a hash key represents a documents position with
respect to a randomly placed hyperplane. Those
planes segment the vector space, forming high di-
mensional polygon shaped buckets. Documents
and topics are placed into a bucket by determin-
ing on which side of each the hyperplanes they are
positioned. We interpret these buckets as regions
of proximity as the collision probability is directly
proportional to the cosine similarity between two
vectors.
</bodyText>
<figure confidence="0.844467833333333">
Algorithm 2 LSH-based Tracking
INPUT:
TOPIC-STREAM {T}
DOCUMENT-STREAM {D}
OUTPUT:
relevant topic-document pairs {t, d}
</figure>
<construct confidence="0.489089222222222">
while document d in T, D do
if d e T then
hashKeys = hashLSH(d)
store hashKeys in hashTables
else if d e D then
candidateSet = lookupHashtables(hashLSH(d))
for all topics t in candidateSet do
if similarity(d,t) &gt; threshold then
emit relevant {t, d}
</construct>
<bodyText confidence="0.999678181818182">
Algorithm 2 outlines the pseudo code to LSH-
based tracking. Whenever a topic arrives, it is
hashed, placing it into a bucket. To increase col-
lision probability with similar documents, we re-
peat the hashing process with different hash func-
tions, storing a topic and hash-key tuple in a hash
table. On each arrival of a new document the same
hash functions are applied and the key is matched
against the hash tables, yielding a set of candidate
topics. The probabilistic bounds of the hashing
scheme guarantee that topics in the candidate set
</bodyText>
<page confidence="0.962333">
1767
</page>
<bodyText confidence="0.999703571428571">
are on average more likely to be similar to the doc-
ument than others.
We then match each topic in the candidate set
against the document to lower the false positive
rate of LSH (Gionis, et al., 1999). The number
of exact comparisons necessary is reduced to the
number of topics in the candidate set.
</bodyText>
<subsectionHeader confidence="0.994605">
4.2 Cluster based Tracking
</subsectionHeader>
<bodyText confidence="0.9997005">
LSH based tracking segments the vector-space
randomly without consideration of the data’s dis-
tribution. In contrast, we now propose a data de-
pendent approach through document clustering.
The main motivation for data dependent space
segmentation is increased effectiveness resulting
from taking the topic distribution within the vec-
tor space into account when forming the regions of
proximity. We construct these regions by group-
ing similar topics to form clusters represented by a
centroid. When tracking a document, it is matched
against the centroids instead of all topics, yield-
ing a set of candidate topics. This allows reducing
the number of comparisons necessary to only the
number of centroids plus the number of topics cap-
tured by the closest cluster.
</bodyText>
<figure confidence="0.8589092">
Algorithm 3 Cluster based Tracking
INPUT:
INITIAL-CLUSTER-SET {c e C}
TOPIC-STREAM {T}
DOCUMENT-STREAM {D}
</figure>
<bodyText confidence="0.597682333333333">
threshold for spawning a new cluster {thrspawn}
threshold for adapting an existing cluster {thradapt}
OUTPUT:
relevant topic-document pairs {t, d}
while document d in T, D do
ifdeTthen
</bodyText>
<equation confidence="0.9906073">
cmin = argmin,{distances(d, c e C)}
if distance(d,cmin) &gt; thrspawn then
spawnNewCluster(d → C)
else if distance(d,cmin) &lt; thradapt then
contribute,assign(cmin,d)
else
assign(cmin,d)
else if d e D then
cmin = argmin,{distances(d,c e C)}
candidateSet = {t a cmin}
</equation>
<bodyText confidence="0.96215703030303">
for all topics t in candidateSet do
if similarity(d,t) &gt; threshold then
emit relevant {t, d}
While the literature provides a vast diversity of
clustering methods for textual documents, our
requirements regarding tracking streams of top-
ics naturally reduce the selection to lightweight
single-pass algorithms. Yang et al. (2012) pro-
vided evidence that in extreme settings simple ap-
proaches work well in terms of balancing effec-
tiveness, efficiency and scalability. We identified
ArteCM by Carullo et al. (2008), originally in-
tended to cluster documents for the web, as suit-
able. Algorithm 3 outlines our approach for clus-
ter based tracking. Given an initial set of 4 ran-
dom centroids, we compare each arriving topic to
all centroids. We associate the new topic with the
cluster whenever it is close enough. Particularly
close documents contribute to a cluster, allowing
it to drift towards topic dense regions. If the docu-
ment is distant to all existing clusters, we spawn a
new cluster based on the document.
Documents arriving from the document stream are
exactly matched against all centroids to determine
the k-closest clusters. Topics associated with those
clusters are subsequently exhaustively compared
with the document, yielding topic-document pairs
considered to be relevant. Probing more than one
cluster increases the probability of finding similar
topics. This does not correlate with soft-clustering
methods as multiple probing happens at querying
time while topics are assigned under a hard clus-
tering paradigm.
</bodyText>
<subsectionHeader confidence="0.99802">
4.3 Algorithm Comparison
</subsectionHeader>
<bodyText confidence="0.999456">
Both the LSH- and the cluster-based tracking al-
gorithm provide two parameters that are conceptu-
ally directly comparable to each other. The num-
ber of bits per hash key and the threshold for
spawning new clusters directly determine the size
of the candidate set by either varying the bucket
size or the cluster radius. The size of the candi-
date set trades a gain in efficiency against a loss
in effectiveness. Fewer topics in the candidate set
heavily reduce the search space for the tracking
process but increase the chance of missing a rele-
vant topic. Bigger sets are more likely to cover rel-
evant topics but require more computational effort
during the exact comparison step. The proposed
algorithms allow continuously adjusting the can-
didate set size between two extremes of having all
topics in a single set and having a separate set for
each topic.
The second parameter both algorithms have in
common, is the number of probes to increase the
probability of identifying similar topics. While
LSH-based tracking offers the number of hash ta-
bles, cluster-based tracking provides the number
of clusters probed. We again encounter a trade-off
between gains in efficiency at the cost of effective-
</bodyText>
<page confidence="0.964141">
1768
</page>
<bodyText confidence="0.999847666666667">
ness. Each additionally probed cluster or looked
up table increases the chance of finding relevant
topics as well as the computational effort.
</bodyText>
<sectionHeader confidence="0.881619" genericHeader="method">
5 Tracking Streams in Constant Space
</sectionHeader>
<bodyText confidence="0.999084823529412">
Operation in constant space is crucial when track-
ing topic streams. We ensure this by placing an
upper limit on the number of concurrently tracked
topics. Whenever the limit is reached, an active
topic is deleted and subsequently not considered
any longer. The strategy for selecting deletion
candidates is heavily application dependant. To
handle topic streams, LSH-based tracking replaces
the entries of an active topic in its hash-tables by
the values of the new topic, whenever the maxi-
mum number of topics is reached. Cluster-based
tracking requires more adaptation because we al-
low clusters to drift conceptually. Whenever the
maximum number of topics is reached, the con-
tribution of the deletion candidate to its cluster is
reverted and it is removed, freeing space for a new
topic.
</bodyText>
<sectionHeader confidence="0.999652" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.977691541666667">
We evaluate the three algorithms in terms
of effectiveness and efficiency. Starting out
with tracking a small set of topics using the
traditional approach, we evaluate various sim-
ilarity metrics to ensure high effectiveness.
We then conduct scaling experiments on mas-
sive streams in bounded and unbounded space.
Corpora
Traditional tracking datasets are unsuitable to
approach tracking at scale as they consist of only
a few thousand documents and several hundred
topics (Allan, 2002). We created a new data
set consisting of two streams (document and
topic stream). The document stream consists
of 52 million tweets gathered through Twitter’s
streaming API 1. The tweets are order by their
time-stamps. Since we are advocating a high
volume topic stream, we require millions of
topics. To ensure a high number of topics, we
treat the entire English part (4.4 mio articles) of
Wikipedia2 as a proxy for a collection of topics
and turn it into a stream. Each article is considered
to be an unstructured textual representation of a
topic time-stamped by its latest verified update.
</bodyText>
<footnote confidence="0.999903">
1http://stream.twitter.com
2http://en.wikipedia.org/wiki/Wikipedia database
</footnote>
<subsectionHeader confidence="0.373203">
Relevance Judgements
</subsectionHeader>
<bodyText confidence="0.9731246">
The topics we picked range from natural disasters,
political and financial events to news about
celebrities, as seen in table 3. We adopted the
search-guided-annotation process used by NIST
(Fiscus et al., 2002) and followed NIST’s TDT
annotation guidelines. According to the definition
of TDT, a document is relevant to a topic if
it speaks about it (Allan, 2002). In total we
identified 14,436 tweets as relevant to one of 30
topics.
total number of topics 4.4 mio
annotated topics 30
total number of documents 52 mio
documents relevant to 14.5k
one of the 30 annotated topics
</bodyText>
<tableCaption confidence="0.997625">
Table 1: Data set statistics
</tableCaption>
<bodyText confidence="0.928236">
Baseline
We use an exact tracking system as a baseline.
To speed up runtime, we implement an inverted
index in conjunction with term-at-a-time query
execution. Additionally, we provide a trade off
between effectiveness and efficiency by ran-
domly down sampling the Twitter stream. Note
that this closely resembles previous approaches
to scale topic tracking (Ghosh et al., 2013).
</bodyText>
<subsectionHeader confidence="0.704514">
Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.996959">
We evaluate effectiveness by recall and precision
and combine them using F1 scores. Efficiency
is evaluated using two different metrics. We
provide a theoretical upper bound by computing
the number of dot products required for tracking
(Equations 1-4).
</bodyText>
<equation confidence="0.9999415">
DPtraditional = nD * nT (1)
DPLSH−based = (nD +nT)*(k*L)+DPcs (2)
DPcluster−based = (nD + nT) * c + DPcs (3)
DPcs = nD * nC (4)
</equation>
<bodyText confidence="0.505642375">
Variables Definition
nD total number of documents
nT total number of topics
k number of bits per hash
L total number of hash tables
c total number of clusters
nC total number of topics
in all candidate sets
</bodyText>
<tableCaption confidence="0.994954">
Table 2: Definition of variables for equation 1-4
</tableCaption>
<page confidence="0.83644">
1769
</page>
<table confidence="0.995594">
Topic-Title Topic description Number of relevant tweets
Amy Winehouse Amy Winehouse dies 3265
Prince William William and Kate arrive in Canada 1021
Floods in Seoul Floods and landslides in North and South Korea 432
Flight 4896 Flight 4896 crashed 11
Bangladesh-India border Bangladesh and India sign a border pact 4
Goran Hadzic War criminal Goran Hadzic got arrested 2
</table>
<tableCaption confidence="0.999941">
Table 3: Showing 6 example topics plus a short summary of relevant tweets, as well as the number of relevant tweets per topic
</tableCaption>
<bodyText confidence="0.999887380952381">
They therefore indicate performance without
system- or implementation-dependent distortions.
Equations 2 and 3 represent the cost to identify the
candidate set for the LSH- and cluster-based al-
gorithm plus the cost resulting from exhaustively
comparing the candidate sets with the documents
(Equation 4).
Because we compute the dot products for a worst
case scenario, we also provide the runtime in sec-
onds. All run-times are averaged over 5 runs, mea-
sured on the same idle machine. To ensure fair
comparison, all algorithms are implemented in C
using the same libraries, compiler, compiler opti-
mizations and run as a single process using 4 GB
of memory. Because the runtime of the traditional
approach (∼171 days) exceeds our limits, we esti-
mate it based on extrapolating 50 runs using up to
25,000 topics. Note that this extrapolation favours
the efficiency of the baseline system as it ignores
hardware dependent slowdowns when scaling up
the number of topics.
</bodyText>
<subsectionHeader confidence="0.999131">
6.1 Exact tracking
</subsectionHeader>
<bodyText confidence="0.999966454545455">
In our first experiment we track 30 annotated
topics on 52 million tweets using the traditional
approach. We compare various similarity mea-
sures (Table 4) and use the best-performing one
in all following experiments. Our data set dif-
fers from the TREC and TDT corpora, which used
news-wire articles. Allan et al. (2000) report
that the cosine similarity constantly performed as
the best distance function for TDT. The use of
Wikipedia and Twitter causes a different set of
similarity measures to perform best. This results
from the imbalance in average document length
between Wikipedia articles (590 terms) and tweets
(11 terms). The term weights in short tweets
(many only containing a single term) are inflated
by the cosine’s length normalization. Those short
tweets are however not uniquely linkable to target
topics and consequently regarded as non-relevant
by annotators, which explains the drop in per-
formance. The similarity function chosen for all
subsequent experiments is a BM25 weighted dot
product, which we found to perform best.
</bodyText>
<table confidence="0.9600156">
F1 score
tf-idf weighted cosine 0.147
tf-idf weighted dot product 0.149
BM25 weighted cosine 0.208
BM25 weighted dot product 0.217
</table>
<tableCaption confidence="0.982028">
Table 4: Comparing the effectiveness of similarity mea-
sures when matching 30 Wikipedia articles against 52 million
tweets
</tableCaption>
<subsectionHeader confidence="0.9767535">
6.2 Tracking at scale, using Wikipedia and
Twitter
</subsectionHeader>
<bodyText confidence="0.999921266666666">
Previously, we conducted small scale experi-
ments, now we are looking to scale them up,
by tracking 4.4 million Wikipedia articles on 52
million tweets without limiting the number of
topics tracked. The resulting trade-off between
effectiveness and efficiency is shown in Figure
1 and 2. The right-most point corresponds to
exhaustive comparison of every document against
every topic – this results in highest possible ef-
fectiveness (F1 score) and highest computational
cost. All runs use optimal tracking thresholds de-
termined by sweeping them while optimizing on
F1 score as an objective function. We also show
the performance resulting from the traditional
approach when randomly down-sampling the doc-
ument (Twitter) stream, which resembles previous
attempts to scale tracking (Ghosh et al., 2013).
Every point on the LSH-based tracking curve
in Figure 1 and 2 represents a different number
of bits per hash key (varying between 4 and 20)
and tables (ranging from 6 to 200). The points
on the cluster-based tracking curves result from
varying the number of clusters (ranging from 1 to
100,000) and probes. The resulting bucket sizes
span from a few dozen to over a million topics.
As expected, the graphs in Figure 1 closely
resembles those in Figure 2. The two figures also
show that the performance of all three algorithms
is continuously adjustable. Unsurprisingly, LSH-
and cluster-based tracking clearly outperform
</bodyText>
<page confidence="0.991467">
1770
</page>
<figureCaption confidence="0.963222666666667">
Figure 1: Trade-off between efficiency and dot-products for
LSH- and cluster-based tracking as well as a random down-
sampling approach for traditional tracking
Figure 2: Trade-off between efficiency and runtime for
LSH- and cluster-based tracking as well as a random down-
sampling approach for traditional tracking;
</figureCaption>
<bodyText confidence="0.999894521739131">
random document sampling for the traditional ap-
proach, based on their more effective search space
reduction strategies. More surprisingly, we also
observe that cluster-based tracking outperforms
tracking based on LSH in terms of efficiency for
F1 scores between 10% and 20%. To understand
why tracking based on clustering is faster than
randomized tracking, we further investigate
their abilities in cutting down the search space.
Figure 3 presents the candidate set size nec-
essary to find a certain ratio of relevant topics.
The graph also illustrates the impact of probing
multiple clusters. When focusing on a recall up
to 60%, LSH-based tracking requires a signif-
icantly larger candidate set size in comparison
with tracking through clustering. For example,
LSH-based tracking needs to examine 30% of all
topics to reach a recall of 50%, while the cluster
based approach only needs to look at 9%. This
effect diminishes for higher recall values. Fur-
thermore, we observe an impressive performance
gain in recall from 20% to 60%, resulting from
additionally probing the k-closest clusters instead
</bodyText>
<figureCaption confidence="0.9142856">
Figure 3: Comparing the candidate set size with the Recall
of LSH- with cluster-based tracking without the exact evalua-
tion phase; The magnitude of the candidate set size represents
the ratio between the number of candidate topics and the total
number of topics;
</figureCaption>
<bodyText confidence="0.99993792">
of just the closest one. While data dependent
segmentation is expected to outperform LSH in
terms of effectiveness, we were surprised by the
magnitude of its impact on efficiency.
The lack in effectiveness of LSH has a direct
negative implication on its efficiency for tracking.
In order to make up for its suboptimal space seg-
mentation, it requires substantially bigger candi-
date sets to reach the same level of recall as the
cluster-based approach. The size of the candi-
date set is critical because we assume a subsequent
exact comparison phase to lower the false posi-
tive rate. The overhead of both algorithms is out-
weighed by the cost of exact comparison for the
candidate set.
Table 5, which compares the performance of the
three algorithms, reveals a drastic reduction in run-
time of up to 80%, at the cost of only a minor
decrease in F1 score. The differences of 6% and
10% percent in F1 score are statistically not sig-
nificant according to a sign test (p&lt;=0.362 and
p&lt;=0.2). Consequently, both algorithms achieve
substantial runtime reduction, while maintaining a
level of effectiveness that is statistically indistin-
guishable from the traditional (exact) approach.
</bodyText>
<subsectionHeader confidence="0.9572275">
6.3 Tracking Wikipedia on Twitter in
constant space
</subsectionHeader>
<bodyText confidence="0.999902">
Tracking a stream of topics in bounded space is
highly application specific due to the deletion pro-
cedure. We know from previous studies (Nichols
et al., 2012) that a topic’s popularity within Twit-
ter fades away over time. We are interested in
keeping currently active topics and delete those
that attract the least number of recent documents.
This set-up has the interesting aspect that the doc-
</bodyText>
<page confidence="0.94874">
1771
</page>
<table confidence="0.991984">
Algorithm F1 score Dot Products Runtime (sec)
traditional approach 0.217 2.3 * 1014 1.5 * 107
LSH-based tracking 0.196 (-10%) 1.4 * 1014 (-39%) 8.0 * 106 (-46 %)
cluster-based tracking 0.204 (-6%) 3.1 * 1013 (-86%) 2.5 * 106 (-83 %)
</table>
<tableCaption confidence="0.982821">
Table 5: Effectiveness and efficiency of LSH- and cluster-based tracking to the traditional approach
</tableCaption>
<table confidence="0.999739">
Algorithm Space F1 score dot products runtime (sec)
LSH-based tracking unbounded 0.196 1.4 * 1014 8.0 * 106
bounded 0.173 (-12%) 5.1 * 10 (-99%) 4.1 * 104 (-99%)
cluster-based tracking unbounded 0.204 3.1 * 1011 2.5 * 106
104
bounded 0.189 (-7%) 1.8 * 10 (-99%) 3.3 * (-98%)
</table>
<tableCaption confidence="0.999083">
Table 6: Effectiveness and efficiency for tracking in bounded and unbounded space
</tableCaption>
<bodyText confidence="0.99998305">
ument stream dictates the lifespan of each topic
in the topic stream. Table 6 contains the results
of cluster- and LSH-based tracking and compares
them to their bounded versions using the same set
up. Note that the hit in performance is solely
defined by the amount of memory provided and
therefore continuously adjustable.
For this particular experiment, we chose an upper
bound of 25k concurrent topics. The table repre-
sents a substantial drop in runtime, following the
reduced search space, at a fairly low expense in
effectiveness. Based on our observations, we hy-
pothesise that significant topics are more likely to
be discussed during random Twitter chatter than
the average Wikipedia topic. It is interesting to
notice that the runtime also indicates a lower over-
head for LSH-based tracking in comparison with
the cluster-based approach. This difference was
hidden in the unbounded tracking experiments but
carries now more weight.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998918052631579">
We extended traditional topic tracking by demon-
strating that it is possible to track an unbounded
stream of topics in constant space and time. We
also presented two approaches to tracking, based
on LSH and clustering that efficiently scale to a
high number of topics and documents while main-
taining a level of effectiveness that is statistically
indistinguishable from an exact tracking system.
While they trade gains in efficiency against a loss
in effectiveness, we showed that cluster based
tracking does so more efficiently due to more ef-
fective space segmentation, which allows a higher
reduction of the search space. Contrary to com-
mon believes this showed how nearest neighbour
search in data streams based on clustering per-
forms faster than LSH, for the same level of accu-
racy. Furthermore, we showed that standard mea-
sures of similarity (cosine) are sub-optimal when
tracking Wikipedia against Twitter.
</bodyText>
<sectionHeader confidence="0.999374" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996226">
James Allan, Victor Lavrenko, Daniella Malin, and
Russell Swan. 2000. Detections, bounds, and time-
lines: Umass and tdt-3. In Proceedings of Topic De-
tection and Tracking Workshop, pages 167-174.
James Allan, Ron Papka, and Victor Lavrenko. 1998.
On-line new event detection and tracking. In Pro-
ceedings of the 21st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval (SIGIR ’98). ACM, New York,
NY, USA.
James Allan. 2002. Topic Detection and Track-
ing: Event-Based Information Organization. Kluwer
Academic Publishers, Norwell, MA, USA.
Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.
2010. Emerging topic detection on Twitter based on
temporal and social terms evaluation. In Proceedings
of the Tenth International Workshop on Multimedia
Data Mining, pages 1-10. ACM.
H. Becker, M. Naaman, and L. Gravano. 2009. Event
Identification in Social Media. In 12th International
Workshop on the Web and Databases (WebDB’09),
Providence, USA.
Moreno Carullo, Elisabetta Binaghi, Ignazio Gallo and
Nicola Lamberti. 2008. ”Clustering of short com-
mercial documents for the web.” Paper presented at
the meeting of the ICPR.
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings of
the thirty-fourth annual ACM symposium on Theory
of computing (STOC ’02). ACM, New York, NY,
USA.
Eichmann, D. and P. Sirivasan. 1999. ”Filters, Webs
and Answers: The University of Iowa TREC-8 Re-
sults” Eighth Conference on Text Retrieval, NIST,
USA.
</reference>
<page confidence="0.851902">
1772
</page>
<reference confidence="0.999501573170732">
Fiscus, J. G. and Doddington, G. R. 2002. Topic detec-
tion and tracking evaluation overview. Topic detec-
tion and tracking: event-based information organi-
zation, pages 17-31.
Saptarshi Ghosh, Muhammad Bilal Zafar, Parantapa
Bhattacharya, Naveen Sharma, Niloy Ganguly, and
Krishna Gummadi. 2013. On sampling the wisdom
of crowds: random vs. expert sampling of the twit-
ter stream. In Proceedings of the 22nd ACM inter-
national conference on Conference on information
&amp; knowledge management (CIKM-13). New York,
NY, USA.
Aristides Gionis, Piotr Indyk, and Rajeev Motwani.
1999. Similarity Search in High Dimensions via
Hashing. InProceedings of the 25th International
Conference on Very Large Data Bases (VLDB ’99),
San Francisco, CA, USA.
Sayyadi Hassan, Hurst Matthew and Maykov Alexey.
2009. ”Event Detection and Tracking in Social
Streams.” In Proceedings of the ICWSM, CA, USA.
Yihong Hong, Yue Fei, and Jianwu Yang. 2013. Ex-
ploiting topic tracking in real-time tweet streams. In
Proceedings of the 2013 international workshop on
Mining unstructured big data using natural language
processing. ACM, New York, NY, USA.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbours: towards removing the curse of
dimensionality. In Proceedings of the thirtieth an-
nual ACM symposium on Theory of computing
(STOC ’98). ACM, New York, NY, USA.
Jimmy Lin, Rion Snow, and William Morgan. 2011.
Smoothing techniques for adaptive online language
models: topic tracking in tweet streams. In Proceed-
ings of the 17th ACM SIGKDD international con-
ference on Knowledge discovery and data mining
(KDD ’11). ACM, New York, NY, USA, 422-429.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Now Publishers Inc.
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.
2012. Summarizing sporting events using twitter. In-
Proceedings of the 2012 ACM international confer-
ence on Intelligent User Interfaces (IUI ’12). ACM,
New York, NY, USA.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to Twitter. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT ’10). Association for Computational
Linguistics, Stroudsburg, PA, USA.
Sasa Petrovic. 2013. Real-time event detection in mas-
sive streams. Ph.D. thesis, School of Informatics,
University of Edinburgh.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized Algorithms and NLP: Us-
ing Locality Sensitive Hash Functions for High
Speed Noun Clustering. In Proceedings of ACL.
Raymond K. Pon, Alfonso F. Cardenas, David Buttler,
and Terence Critchlow. 2007. Tracking multiple top-
ics for finding interesting articles. In Proceedings of
the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ’07).
ACM, New York, NY, USA.
I. Soboroff, I.Ounis, and J. Lin. 2012. Overview of the
trec-2012 microblog track. In Proceedings of TREC.
Jintao Tang, Ting Wang, Qin Lu, Ji Wang, and Wenjie
Li. 2011. A Wikipedia based semantic graph model
for topic tracking in blogosphere. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence - Volume Three (IJCAI’11).
TDT by NIST - 1998-2004.
http://www.itl.nist.gov/iad/mig/
tests/tdt/resources.html (Last Update: 2008)
Jianshu Weng, Erwin Leonardi, Francis Lee. Event De-
tection in Twitter. 2011. In Proceeding of ICWSM.
AAAI Press.
Xintian Yang, Amol Ghoting, Yiye Ruan, and Srini-
vasan Parthasarathy. 2012. A framework for summa-
rizing and analysing twitter feeds. In Proceedings of
the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ’12).
ACM, New York, NY, USA.
</reference>
<page confidence="0.984071">
1773
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.783722">
<title confidence="0.999961">Tracking unbounded Topic Streams</title>
<author confidence="0.998833">Dominik</author>
<affiliation confidence="0.999274">School of University of</affiliation>
<email confidence="0.966676">@sms.ed.ac.uk</email>
<author confidence="0.988898">Victor</author>
<affiliation confidence="0.9986105">School of University of</affiliation>
<email confidence="0.961476">@inf.ed.ac.uk</email>
<author confidence="0.8742">Miles</author>
<email confidence="0.963093">@bloomberg.net</email>
<abstract confidence="0.999458333333333">Tracking topics on social media streams is non-trivial as the number of topics mentioned grows without bound. This complexity is compounded when we want to track such topics against other fast moving streams. We go beyond traditional small scale topic tracking and consider a stream of topics against another document stream. We introduce two tracking approaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million documents in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Victor Lavrenko</author>
<author>Daniella Malin</author>
<author>Russell Swan</author>
</authors>
<title>Detections, bounds, and timelines: Umass and tdt-3.</title>
<date>2000</date>
<booktitle>In Proceedings of Topic Detection and Tracking Workshop,</booktitle>
<pages>167--174</pages>
<contexts>
<context position="22489" citStr="Allan et al. (2000)" startWordPosition="3601" endWordPosition="3604">raditional approach (∼171 days) exceeds our limits, we estimate it based on extrapolating 50 runs using up to 25,000 topics. Note that this extrapolation favours the efficiency of the baseline system as it ignores hardware dependent slowdowns when scaling up the number of topics. 6.1 Exact tracking In our first experiment we track 30 annotated topics on 52 million tweets using the traditional approach. We compare various similarity measures (Table 4) and use the best-performing one in all following experiments. Our data set differs from the TREC and TDT corpora, which used news-wire articles. Allan et al. (2000) report that the cosine similarity constantly performed as the best distance function for TDT. The use of Wikipedia and Twitter causes a different set of similarity measures to perform best. This results from the imbalance in average document length between Wikipedia articles (590 terms) and tweets (11 terms). The term weights in short tweets (many only containing a single term) are inflated by the cosine’s length normalization. Those short tweets are however not uniquely linkable to target topics and consequently regarded as non-relevant by annotators, which explains the drop in performance. </context>
</contexts>
<marker>Allan, Lavrenko, Malin, Swan, 2000</marker>
<rawString>James Allan, Victor Lavrenko, Daniella Malin, and Russell Swan. 2000. Detections, bounds, and timelines: Umass and tdt-3. In Proceedings of Topic Detection and Tracking Workshop, pages 167-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Ron Papka</author>
<author>Victor Lavrenko</author>
</authors>
<title>On-line new event detection and tracking.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR ’98).</booktitle>
<publisher>ACM,</publisher>
<location>New York, NY, USA.</location>
<marker>Allan, Papka, Lavrenko, 1998</marker>
<rawString>James Allan, Ron Papka, and Victor Lavrenko. 1998. On-line new event detection and tracking. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR ’98). ACM, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
</authors>
<title>Topic Detection and Tracking: Event-Based Information Organization.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="1399" citStr="Allan, 2002" startWordPosition="206" endWordPosition="207">monstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams. 1 Introduction The emergence of massive social media streams has sparked a growing need for systems able to process them. While previous research (Hassan et al., 2009; Becker et al., 2009; Petrovic et al., 2010; Cataldi et al., (2010); Weng et al., (2011); Petrovic 2013) has focused on detecting new topics in unbounded textual streams, less attention was paid to following (tracking) the steadily growing set of topics. Standard topic tracking (Allan, 2002) deals with helping human analysts follow and monitor ongoing events on massive data streams. By pairing topics with relevant documents, topic tracking splits a noisy stream of documents into sub-streams grouped by their target topics. This is a crucial task for financial and security analysts who are interested in pulling together relevant information from unstructured and noisy data streams. Other fields like summarization or topic modeling benefit from topic tracking as a mean to generate their data sources. In todays data streams however, new topics emerge on a continual basis and we are i</context>
<context position="4567" citStr="Allan, 2002" startWordPosition="695" endWordPosition="697">how it is possible to track an unbounded stream of topics in constant time and space, while maintaining a level of effectiveness that is statistically indistinguishable from an exact tracking system • We show how single-pass clustering can outperform locality sensitive hashing in terms of effectiveness and efficiency for identifying nearest neighbours in a stream • We demonstrate that standard measures of similarity are sub-optimal when matching short documents against long documents 2 Related Work Topic or event tracking was first introduced in the Topic Detection and Tracking (TDT) program (Allan, 2002). In TDT, topic tracking involves monitoring a stream of news documents with the intent to identify those documents relevant to a small predefined set of target topics. During the course of TDT, research focused extensively on the effectiveness of tracking systems, neglecting scale and efficiency. The three official data sets only range from 25k to 74k documents with a few hundred topics (Allan, 2002). More recently, the rise of publicly available real-time social media streams triggered new research on topic detection and tracking, intended to apply the technology to those high volume documen</context>
<context position="7237" citStr="Allan, 2002" startWordPosition="1135" endWordPosition="1136">s in relation to the TREC real-time filtering task by relying on a sliding window principle, while focusing on the cold start problem. 3 Topic Tracking 3.1 Traditional Approach Numerous approaches to topic tracking have emerged, spanning from probabilistic retrieval to statistical classification frameworks. While there is no single general approach, we define the traditional approach to tracking from a high-level perspective covering the basic principle of all previous approaches. We do not make any assumptions about the kind of topics, documents or distance functions used. As defined by TDT (Allan, 2002), we assume, we operate on an unbounded document stream with the goal of tracking a fixed set of target topics. Although topics are allowed to drift conceptually and evolve over time, new topics would always trigger the start of a new tracking system. Algorithm 1 Traditional Tracking INPUT: TOPIC-SET {t e T} DOCUMENT-STREAM {d e D} OUTPUT: relevant topic-document pairs {t, d} while documents d in stream D do for all topics t in set T do similarity = computeSimilarity(d,t) if similarity &gt; threshold then emit relevant {t, d} As seen in Algorithm 1, documents arrive one at a time, requiring insta</context>
<context position="18349" citStr="Allan, 2002" startWordPosition="2931" endWordPosition="2932">ntribution of the deletion candidate to its cluster is reverted and it is removed, freeing space for a new topic. 6 Experiments We evaluate the three algorithms in terms of effectiveness and efficiency. Starting out with tracking a small set of topics using the traditional approach, we evaluate various similarity metrics to ensure high effectiveness. We then conduct scaling experiments on massive streams in bounded and unbounded space. Corpora Traditional tracking datasets are unsuitable to approach tracking at scale as they consist of only a few thousand documents and several hundred topics (Allan, 2002). We created a new data set consisting of two streams (document and topic stream). The document stream consists of 52 million tweets gathered through Twitter’s streaming API 1. The tweets are order by their time-stamps. Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia2 as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update. 1htt</context>
</contexts>
<marker>Allan, 2002</marker>
<rawString>James Allan. 2002. Topic Detection and Tracking: Event-Based Information Organization. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Cataldi</author>
<author>Luigi Di Caro</author>
<author>Claudio Schifanella</author>
</authors>
<title>Emerging topic detection on Twitter based on temporal and social terms evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Tenth International Workshop on Multimedia Data Mining,</booktitle>
<pages>1--10</pages>
<publisher>ACM.</publisher>
<marker>Cataldi, Di Caro, Schifanella, 2010</marker>
<rawString>Mario Cataldi, Luigi Di Caro, and Claudio Schifanella. 2010. Emerging topic detection on Twitter based on temporal and social terms evaluation. In Proceedings of the Tenth International Workshop on Multimedia Data Mining, pages 1-10. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Becker</author>
<author>M Naaman</author>
<author>L Gravano</author>
</authors>
<title>Event Identification in Social Media.</title>
<date>2009</date>
<booktitle>In 12th International Workshop on the Web and Databases (WebDB’09),</booktitle>
<location>Providence, USA.</location>
<contexts>
<context position="1127" citStr="Becker et al., 2009" startWordPosition="162" endWordPosition="165">l scale topic tracking and consider a stream of topics against another document stream. We introduce two tracking approaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million documents in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams. 1 Introduction The emergence of massive social media streams has sparked a growing need for systems able to process them. While previous research (Hassan et al., 2009; Becker et al., 2009; Petrovic et al., 2010; Cataldi et al., (2010); Weng et al., (2011); Petrovic 2013) has focused on detecting new topics in unbounded textual streams, less attention was paid to following (tracking) the steadily growing set of topics. Standard topic tracking (Allan, 2002) deals with helping human analysts follow and monitor ongoing events on massive data streams. By pairing topics with relevant documents, topic tracking splits a noisy stream of documents into sub-streams grouped by their target topics. This is a crucial task for financial and security analysts who are interested in pulling tog</context>
</contexts>
<marker>Becker, Naaman, Gravano, 2009</marker>
<rawString>H. Becker, M. Naaman, and L. Gravano. 2009. Event Identification in Social Media. In 12th International Workshop on the Web and Databases (WebDB’09), Providence, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moreno Carullo</author>
<author>Elisabetta Binaghi</author>
<author>Ignazio Gallo</author>
<author>Nicola Lamberti</author>
</authors>
<title>Clustering of short commercial documents for the web.” Paper presented at the meeting of the ICPR.</title>
<date>2008</date>
<contexts>
<context position="14640" citStr="Carullo et al. (2008)" startWordPosition="2337" endWordPosition="2340">ign(cmin,d) else assign(cmin,d) else if d e D then cmin = argmin,{distances(d,c e C)} candidateSet = {t a cmin} for all topics t in candidateSet do if similarity(d,t) &gt; threshold then emit relevant {t, d} While the literature provides a vast diversity of clustering methods for textual documents, our requirements regarding tracking streams of topics naturally reduce the selection to lightweight single-pass algorithms. Yang et al. (2012) provided evidence that in extreme settings simple approaches work well in terms of balancing effectiveness, efficiency and scalability. We identified ArteCM by Carullo et al. (2008), originally intended to cluster documents for the web, as suitable. Algorithm 3 outlines our approach for cluster based tracking. Given an initial set of 4 random centroids, we compare each arriving topic to all centroids. We associate the new topic with the cluster whenever it is close enough. Particularly close documents contribute to a cluster, allowing it to drift towards topic dense regions. If the document is distant to all existing clusters, we spawn a new cluster based on the document. Documents arriving from the document stream are exactly matched against all centroids to determine t</context>
</contexts>
<marker>Carullo, Binaghi, Gallo, Lamberti, 2008</marker>
<rawString>Moreno Carullo, Elisabetta Binaghi, Ignazio Gallo and Nicola Lamberti. 2008. ”Clustering of short commercial documents for the web.” Paper presented at the meeting of the ICPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses S Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the thirty-fourth annual ACM symposium on Theory of computing (STOC ’02).</booktitle>
<publisher>ACM,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11026" citStr="Charikar (2002)" startWordPosition="1764" endWordPosition="1765">est topics are the nearest neighbours to be identified. We explore locality sensitive hashing (LSH), as described by Indyk et al. (1998), to approach high dimensional nearest neighbour search for topic tracking in sub-linear time. LSH, which has been used to speed up NLP applications (Ravichandran et al., 2005), provides hash functions that guarantee that similar documents are more likely to be hashed to the same binary hash key than distant ones. Hash functions capture similarities between vectors in high dimensions and represent them on a low dimensional binary level. We apply the scheme by Charikar (2002), which describes the probabilistic bounds for the cosine similarity between two vectors. Each bit in a hash key represents a documents position with respect to a randomly placed hyperplane. Those planes segment the vector space, forming high dimensional polygon shaped buckets. Documents and topics are placed into a bucket by determining on which side of each the hyperplanes they are positioned. We interpret these buckets as regions of proximity as the collision probability is directly proportional to the cosine similarity between two vectors. Algorithm 2 LSH-based Tracking INPUT: TOPIC-STREAM</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thirty-fourth annual ACM symposium on Theory of computing (STOC ’02). ACM, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Eichmann</author>
<author>P Sirivasan</author>
</authors>
<title>Filters, Webs and Answers:</title>
<date>1999</date>
<booktitle>The University of Iowa TREC-8 Results” Eighth Conference on Text Retrieval,</booktitle>
<location>NIST, USA.</location>
<marker>Eichmann, Sirivasan, 1999</marker>
<rawString>Eichmann, D. and P. Sirivasan. 1999. ”Filters, Webs and Answers: The University of Iowa TREC-8 Results” Eighth Conference on Text Retrieval, NIST, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Fiscus</author>
<author>G R Doddington</author>
</authors>
<title>Topic detection and tracking evaluation overview. Topic detection and tracking: event-based information organization,</title>
<date>2002</date>
<pages>17--31</pages>
<marker>Fiscus, Doddington, 2002</marker>
<rawString>Fiscus, J. G. and Doddington, G. R. 2002. Topic detection and tracking evaluation overview. Topic detection and tracking: event-based information organization, pages 17-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saptarshi Ghosh</author>
<author>Muhammad Bilal Zafar</author>
<author>Parantapa Bhattacharya</author>
<author>Naveen Sharma</author>
<author>Niloy Ganguly</author>
<author>Krishna Gummadi</author>
</authors>
<title>On sampling the wisdom of crowds: random vs. expert sampling of the twitter stream.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management (CIKM-13).</booktitle>
<location>New York, NY, USA.</location>
<contexts>
<context position="5624" citStr="Ghosh et al., 2013" startWordPosition="868" endWordPosition="871">cly available real-time social media streams triggered new research on topic detection and tracking, intended to apply the technology to those high volume document streams. The novel data streams differ from the TDT data sets in their volume and level of noise. To provide realtime applications, traditional methods need to be overhauled to keep computation feasible. It became common practice to limit data sets to cope with the computational effort. Popular strategies involve reducing the number of tracked topics (Lin et al., 2011; Nichols et al., 2012;) as well as sampling the document stream (Ghosh et al., 2013). These approaches have proven to be efficient in cutting down workload but they also limit an application’s performance. Furthermore, Sayyadi et al. (2009) discovered and tracked topics in social streams based on keyword graphs. They applied the sliding window principle to keep the computation feasible, although their data set only contained 18k documents. Yang et al. 2012 tracked topics in tweet streams using language models. To cope with the computational effort they assume a small set of topics of only a few dozen, which are defined in advance. Tang et al. (2011) tracked a single topic on </context>
<context position="20027" citStr="Ghosh et al., 2013" startWordPosition="3194" endWordPosition="3197">In total we identified 14,436 tweets as relevant to one of 30 topics. total number of topics 4.4 mio annotated topics 30 total number of documents 52 mio documents relevant to 14.5k one of the 30 annotated topics Table 1: Data set statistics Baseline We use an exact tracking system as a baseline. To speed up runtime, we implement an inverted index in conjunction with term-at-a-time query execution. Additionally, we provide a trade off between effectiveness and efficiency by randomly down sampling the Twitter stream. Note that this closely resembles previous approaches to scale topic tracking (Ghosh et al., 2013). Evaluation Metrics We evaluate effectiveness by recall and precision and combine them using F1 scores. Efficiency is evaluated using two different metrics. We provide a theoretical upper bound by computing the number of dot products required for tracking (Equations 1-4). DPtraditional = nD * nT (1) DPLSH−based = (nD +nT)*(k*L)+DPcs (2) DPcluster−based = (nD + nT) * c + DPcs (3) DPcs = nD * nC (4) Variables Definition nD total number of documents nT total number of topics k number of bits per hash L total number of hash tables c total number of clusters nC total number of topics in all candid</context>
<context position="24318" citStr="Ghosh et al., 2013" startWordPosition="3881" endWordPosition="3884">f topics tracked. The resulting trade-off between effectiveness and efficiency is shown in Figure 1 and 2. The right-most point corresponds to exhaustive comparison of every document against every topic – this results in highest possible effectiveness (F1 score) and highest computational cost. All runs use optimal tracking thresholds determined by sweeping them while optimizing on F1 score as an objective function. We also show the performance resulting from the traditional approach when randomly down-sampling the document (Twitter) stream, which resembles previous attempts to scale tracking (Ghosh et al., 2013). Every point on the LSH-based tracking curve in Figure 1 and 2 represents a different number of bits per hash key (varying between 4 and 20) and tables (ranging from 6 to 200). The points on the cluster-based tracking curves result from varying the number of clusters (ranging from 1 to 100,000) and probes. The resulting bucket sizes span from a few dozen to over a million topics. As expected, the graphs in Figure 1 closely resembles those in Figure 2. The two figures also show that the performance of all three algorithms is continuously adjustable. Unsurprisingly, LSHand cluster-based trackin</context>
</contexts>
<marker>Ghosh, Zafar, Bhattacharya, Sharma, Ganguly, Gummadi, 2013</marker>
<rawString>Saptarshi Ghosh, Muhammad Bilal Zafar, Parantapa Bhattacharya, Naveen Sharma, Niloy Ganguly, and Krishna Gummadi. 2013. On sampling the wisdom of crowds: random vs. expert sampling of the twitter stream. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management (CIKM-13). New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aristides Gionis</author>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<date>1999</date>
<booktitle>Similarity Search in High Dimensions via Hashing. InProceedings of the 25th International Conference on Very Large Data Bases (VLDB ’99),</booktitle>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="10344" citStr="Gionis et al. (1999)" startWordPosition="1651" endWordPosition="1654">f proximity. Those regions are intend to capture documents that are more likely to be relevant. Ideally, these regions form a hypersphere centred around every topic vector with a radius equal to the maximum distance to relevant documents. The tracking procedure is then reduced to determining whether an incoming document is also enclosed by any of the hyperspheres. 4.1 Approximated Tracking Our first attempt to reach sub-linear execution time uses random segmentation of the vector space using hashing techniques. We frame the tracking process as a nearest neighbour search problem, as defined by Gionis et al. (1999). Documents arriving from a stream are seen as queries and the closest topics are the nearest neighbours to be identified. We explore locality sensitive hashing (LSH), as described by Indyk et al. (1998), to approach high dimensional nearest neighbour search for topic tracking in sub-linear time. LSH, which has been used to speed up NLP applications (Ravichandran et al., 2005), provides hash functions that guarantee that similar documents are more likely to be hashed to the same binary hash key than distant ones. Hash functions capture similarities between vectors in high dimensions and repres</context>
<context position="12689" citStr="Gionis, et al., 1999" startWordPosition="2038" endWordPosition="2041">. To increase collision probability with similar documents, we repeat the hashing process with different hash functions, storing a topic and hash-key tuple in a hash table. On each arrival of a new document the same hash functions are applied and the key is matched against the hash tables, yielding a set of candidate topics. The probabilistic bounds of the hashing scheme guarantee that topics in the candidate set 1767 are on average more likely to be similar to the document than others. We then match each topic in the candidate set against the document to lower the false positive rate of LSH (Gionis, et al., 1999). The number of exact comparisons necessary is reduced to the number of topics in the candidate set. 4.2 Cluster based Tracking LSH based tracking segments the vector-space randomly without consideration of the data’s distribution. In contrast, we now propose a data dependent approach through document clustering. The main motivation for data dependent space segmentation is increased effectiveness resulting from taking the topic distribution within the vector space into account when forming the regions of proximity. We construct these regions by grouping similar topics to form clusters represen</context>
</contexts>
<marker>Gionis, Indyk, Motwani, 1999</marker>
<rawString>Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 1999. Similarity Search in High Dimensions via Hashing. InProceedings of the 25th International Conference on Very Large Data Bases (VLDB ’99), San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayyadi Hassan</author>
<author>Hurst Matthew</author>
<author>Maykov Alexey</author>
</authors>
<title>Event Detection and Tracking in Social Streams.”</title>
<date>2009</date>
<booktitle>In Proceedings of the ICWSM,</booktitle>
<location>CA, USA.</location>
<contexts>
<context position="1106" citStr="Hassan et al., 2009" startWordPosition="158" endWordPosition="161">yond traditional small scale topic tracking and consider a stream of topics against another document stream. We introduce two tracking approaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million documents in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams. 1 Introduction The emergence of massive social media streams has sparked a growing need for systems able to process them. While previous research (Hassan et al., 2009; Becker et al., 2009; Petrovic et al., 2010; Cataldi et al., (2010); Weng et al., (2011); Petrovic 2013) has focused on detecting new topics in unbounded textual streams, less attention was paid to following (tracking) the steadily growing set of topics. Standard topic tracking (Allan, 2002) deals with helping human analysts follow and monitor ongoing events on massive data streams. By pairing topics with relevant documents, topic tracking splits a noisy stream of documents into sub-streams grouped by their target topics. This is a crucial task for financial and security analysts who are inte</context>
</contexts>
<marker>Hassan, Matthew, Alexey, 2009</marker>
<rawString>Sayyadi Hassan, Hurst Matthew and Maykov Alexey. 2009. ”Event Detection and Tracking in Social Streams.” In Proceedings of the ICWSM, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yihong Hong</author>
<author>Yue Fei</author>
<author>Jianwu Yang</author>
</authors>
<title>Exploiting topic tracking in real-time tweet streams.</title>
<date>2013</date>
<booktitle>In Proceedings of the</booktitle>
<publisher>ACM,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6586" citStr="Hong et al. (2013)" startWordPosition="1029" endWordPosition="1032"> 18k documents. Yang et al. 2012 tracked topics in tweet streams using language models. To cope with the computational effort they assume a small set of topics of only a few dozen, which are defined in advance. Tang et al. (2011) tracked a single topic on a few thousand blogs based on semantic graph topic models. Pon et al. (2007) recommend news by tracking multiple topics for a user but their data sets only span several thousand documents and a few topics. Further related work includes the real-time filtering task, introduced as part of TREC’s Microblog Track in 2012 (Soboroff et al., 2012). Hong et al. (2013) explore topic tracking in tweet streams in relation to the TREC real-time filtering task by relying on a sliding window principle, while focusing on the cold start problem. 3 Topic Tracking 3.1 Traditional Approach Numerous approaches to topic tracking have emerged, spanning from probabilistic retrieval to statistical classification frameworks. While there is no single general approach, we define the traditional approach to tracking from a high-level perspective covering the basic principle of all previous approaches. We do not make any assumptions about the kind of topics, documents or dista</context>
</contexts>
<marker>Hong, Fei, Yang, 2013</marker>
<rawString>Yihong Hong, Yue Fei, and Jianwu Yang. 2013. Exploiting topic tracking in real-time tweet streams. In Proceedings of the 2013 international workshop on Mining unstructured big data using natural language processing. ACM, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbours: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC ’98).</booktitle>
<publisher>ACM,</publisher>
<location>New York, NY, USA.</location>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbours: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC ’98). ACM, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Rion Snow</author>
<author>William Morgan</author>
</authors>
<title>Smoothing techniques for adaptive online language models: topic tracking in tweet streams.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ’11).</booktitle>
<pages>422--429</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="5539" citStr="Lin et al., 2011" startWordPosition="853" endWordPosition="856">ocuments with a few hundred topics (Allan, 2002). More recently, the rise of publicly available real-time social media streams triggered new research on topic detection and tracking, intended to apply the technology to those high volume document streams. The novel data streams differ from the TDT data sets in their volume and level of noise. To provide realtime applications, traditional methods need to be overhauled to keep computation feasible. It became common practice to limit data sets to cope with the computational effort. Popular strategies involve reducing the number of tracked topics (Lin et al., 2011; Nichols et al., 2012;) as well as sampling the document stream (Ghosh et al., 2013). These approaches have proven to be efficient in cutting down workload but they also limit an application’s performance. Furthermore, Sayyadi et al. (2009) discovered and tracked topics in social streams based on keyword graphs. They applied the sliding window principle to keep the computation feasible, although their data set only contained 18k documents. Yang et al. 2012 tracked topics in tweet streams using language models. To cope with the computational effort they assume a small set of topics of only a f</context>
</contexts>
<marker>Lin, Snow, Morgan, 2011</marker>
<rawString>Jimmy Lin, Rion Snow, and William Morgan. 2011. Smoothing techniques for adaptive online language models: topic tracking in tweet streams. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ’11). ACM, New York, NY, USA, 422-429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muthukrishnan</author>
</authors>
<title>Data streams: Algorithms and applications.</title>
<date>2005</date>
<publisher>Now Publishers Inc.</publisher>
<contexts>
<context position="3041" citStr="Muthukrishnan, 2005" startWordPosition="462" endWordPosition="463">y across the world; popular examples of unbounded document streams include social media services such as Twitter. Tracking streams of topics allows research tasks like topic-modeling or summarization to be applied to millions of topics, a scale that is several orders of magnitude larger than those of current publications. We present two massive scale topic tracking systems capable of tracking unbounded topic streams. One is based on locality sensitive hashing (LSH) and the other on clustering. Since we operate on two unbounded data sources we are subject to the streaming model of computation (Muthukrishnan, 2005), which requires instant and single-pass decision making in constant time and space. Contrary to expectations, we find that nearest neighbour search on a stream based on clustering performs faster than LSH for the same level of accuracy. This is surprising as LSH is widely believed to be the fastest way of nearest neighbour search. Our experiments reveal how simple single-pass clustering outperforms LSH in terms of effectiveness and efficiency. Our results are general and apply to any setting where we have massive or infinite numbers of topics, matched against unboundedly large document stream</context>
</contexts>
<marker>Muthukrishnan, 2005</marker>
<rawString>S. Muthukrishnan. 2005. Data streams: Algorithms and applications. Now Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Nichols</author>
<author>Jalal Mahmud</author>
<author>Clemens Drews</author>
</authors>
<title>Summarizing sporting events using twitter.</title>
<date>2012</date>
<booktitle>InProceedings of the 2012 ACM international conference on Intelligent User Interfaces (IUI ’12).</booktitle>
<publisher>ACM,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5561" citStr="Nichols et al., 2012" startWordPosition="857" endWordPosition="860">w hundred topics (Allan, 2002). More recently, the rise of publicly available real-time social media streams triggered new research on topic detection and tracking, intended to apply the technology to those high volume document streams. The novel data streams differ from the TDT data sets in their volume and level of noise. To provide realtime applications, traditional methods need to be overhauled to keep computation feasible. It became common practice to limit data sets to cope with the computational effort. Popular strategies involve reducing the number of tracked topics (Lin et al., 2011; Nichols et al., 2012;) as well as sampling the document stream (Ghosh et al., 2013). These approaches have proven to be efficient in cutting down workload but they also limit an application’s performance. Furthermore, Sayyadi et al. (2009) discovered and tracked topics in social streams based on keyword graphs. They applied the sliding window principle to keep the computation feasible, although their data set only contained 18k documents. Yang et al. 2012 tracked topics in tweet streams using language models. To cope with the computational effort they assume a small set of topics of only a few dozen, which are de</context>
<context position="28004" citStr="Nichols et al., 2012" startWordPosition="4475" endWordPosition="4478">c reduction in runtime of up to 80%, at the cost of only a minor decrease in F1 score. The differences of 6% and 10% percent in F1 score are statistically not significant according to a sign test (p&lt;=0.362 and p&lt;=0.2). Consequently, both algorithms achieve substantial runtime reduction, while maintaining a level of effectiveness that is statistically indistinguishable from the traditional (exact) approach. 6.3 Tracking Wikipedia on Twitter in constant space Tracking a stream of topics in bounded space is highly application specific due to the deletion procedure. We know from previous studies (Nichols et al., 2012) that a topic’s popularity within Twitter fades away over time. We are interested in keeping currently active topics and delete those that attract the least number of recent documents. This set-up has the interesting aspect that the doc1771 Algorithm F1 score Dot Products Runtime (sec) traditional approach 0.217 2.3 * 1014 1.5 * 107 LSH-based tracking 0.196 (-10%) 1.4 * 1014 (-39%) 8.0 * 106 (-46 %) cluster-based tracking 0.204 (-6%) 3.1 * 1013 (-86%) 2.5 * 106 (-83 %) Table 5: Effectiveness and efficiency of LSH- and cluster-based tracking to the traditional approach Algorithm Space F1 score </context>
</contexts>
<marker>Nichols, Mahmud, Drews, 2012</marker>
<rawString>Jeffrey Nichols, Jalal Mahmud, and Clemens Drews. 2012. Summarizing sporting events using twitter. InProceedings of the 2012 ACM international conference on Intelligent User Interfaces (IUI ’12). ACM, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to Twitter. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT ’10). Association for Computational Linguistics,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1150" citStr="Petrovic et al., 2010" startWordPosition="166" endWordPosition="169">g and consider a stream of topics against another document stream. We introduce two tracking approaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million documents in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams. 1 Introduction The emergence of massive social media streams has sparked a growing need for systems able to process them. While previous research (Hassan et al., 2009; Becker et al., 2009; Petrovic et al., 2010; Cataldi et al., (2010); Weng et al., (2011); Petrovic 2013) has focused on detecting new topics in unbounded textual streams, less attention was paid to following (tracking) the steadily growing set of topics. Standard topic tracking (Allan, 2002) deals with helping human analysts follow and monitor ongoing events on massive data streams. By pairing topics with relevant documents, topic tracking splits a noisy stream of documents into sub-streams grouped by their target topics. This is a crucial task for financial and security analysts who are interested in pulling together relevant informat</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2010</marker>
<rawString>Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to Twitter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT ’10). Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
</authors>
<title>Real-time event detection in massive streams.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="1211" citStr="Petrovic 2013" startWordPosition="178" endWordPosition="179">e introduce two tracking approaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million documents in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams. 1 Introduction The emergence of massive social media streams has sparked a growing need for systems able to process them. While previous research (Hassan et al., 2009; Becker et al., 2009; Petrovic et al., 2010; Cataldi et al., (2010); Weng et al., (2011); Petrovic 2013) has focused on detecting new topics in unbounded textual streams, less attention was paid to following (tracking) the steadily growing set of topics. Standard topic tracking (Allan, 2002) deals with helping human analysts follow and monitor ongoing events on massive data streams. By pairing topics with relevant documents, topic tracking splits a noisy stream of documents into sub-streams grouped by their target topics. This is a crucial task for financial and security analysts who are interested in pulling together relevant information from unstructured and noisy data streams. Other fields li</context>
</contexts>
<marker>Petrovic, 2013</marker>
<rawString>Sasa Petrovic. 2013. Real-time event detection in massive streams. Ph.D. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10723" citStr="Ravichandran et al., 2005" startWordPosition="1714" endWordPosition="1717">proximated Tracking Our first attempt to reach sub-linear execution time uses random segmentation of the vector space using hashing techniques. We frame the tracking process as a nearest neighbour search problem, as defined by Gionis et al. (1999). Documents arriving from a stream are seen as queries and the closest topics are the nearest neighbours to be identified. We explore locality sensitive hashing (LSH), as described by Indyk et al. (1998), to approach high dimensional nearest neighbour search for topic tracking in sub-linear time. LSH, which has been used to speed up NLP applications (Ravichandran et al., 2005), provides hash functions that guarantee that similar documents are more likely to be hashed to the same binary hash key than distant ones. Hash functions capture similarities between vectors in high dimensions and represent them on a low dimensional binary level. We apply the scheme by Charikar (2002), which describes the probabilistic bounds for the cosine similarity between two vectors. Each bit in a hash key represents a documents position with respect to a randomly placed hyperplane. Those planes segment the vector space, forming high dimensional polygon shaped buckets. Documents and topi</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond K Pon</author>
<author>Alfonso F Cardenas</author>
<author>David Buttler</author>
<author>Terence Critchlow</author>
</authors>
<title>Tracking multiple topics for finding interesting articles.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ’07).</booktitle>
<publisher>ACM,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6300" citStr="Pon et al. (2007)" startWordPosition="980" endWordPosition="983">own workload but they also limit an application’s performance. Furthermore, Sayyadi et al. (2009) discovered and tracked topics in social streams based on keyword graphs. They applied the sliding window principle to keep the computation feasible, although their data set only contained 18k documents. Yang et al. 2012 tracked topics in tweet streams using language models. To cope with the computational effort they assume a small set of topics of only a few dozen, which are defined in advance. Tang et al. (2011) tracked a single topic on a few thousand blogs based on semantic graph topic models. Pon et al. (2007) recommend news by tracking multiple topics for a user but their data sets only span several thousand documents and a few topics. Further related work includes the real-time filtering task, introduced as part of TREC’s Microblog Track in 2012 (Soboroff et al., 2012). Hong et al. (2013) explore topic tracking in tweet streams in relation to the TREC real-time filtering task by relying on a sliding window principle, while focusing on the cold start problem. 3 Topic Tracking 3.1 Traditional Approach Numerous approaches to topic tracking have emerged, spanning from probabilistic retrieval to stati</context>
</contexts>
<marker>Pon, Cardenas, Buttler, Critchlow, 2007</marker>
<rawString>Raymond K. Pon, Alfonso F. Cardenas, David Buttler, and Terence Critchlow. 2007. Tracking multiple topics for finding interesting articles. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ’07). ACM, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Soboroff</author>
<author>I Ounis</author>
<author>J Lin</author>
</authors>
<title>Overview of the trec-2012 microblog track.</title>
<date>2012</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="6566" citStr="Soboroff et al., 2012" startWordPosition="1025" endWordPosition="1028"> data set only contained 18k documents. Yang et al. 2012 tracked topics in tweet streams using language models. To cope with the computational effort they assume a small set of topics of only a few dozen, which are defined in advance. Tang et al. (2011) tracked a single topic on a few thousand blogs based on semantic graph topic models. Pon et al. (2007) recommend news by tracking multiple topics for a user but their data sets only span several thousand documents and a few topics. Further related work includes the real-time filtering task, introduced as part of TREC’s Microblog Track in 2012 (Soboroff et al., 2012). Hong et al. (2013) explore topic tracking in tweet streams in relation to the TREC real-time filtering task by relying on a sliding window principle, while focusing on the cold start problem. 3 Topic Tracking 3.1 Traditional Approach Numerous approaches to topic tracking have emerged, spanning from probabilistic retrieval to statistical classification frameworks. While there is no single general approach, we define the traditional approach to tracking from a high-level perspective covering the basic principle of all previous approaches. We do not make any assumptions about the kind of topics</context>
</contexts>
<marker>Soboroff, Ounis, Lin, 2012</marker>
<rawString>I. Soboroff, I.Ounis, and J. Lin. 2012. Overview of the trec-2012 microblog track. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jintao Tang</author>
<author>Ting Wang</author>
<author>Qin Lu</author>
<author>Ji Wang</author>
<author>Wenjie Li</author>
</authors>
<title>A Wikipedia based semantic graph model for topic tracking in blogosphere.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Three (IJCAI’11).</booktitle>
<contexts>
<context position="6197" citStr="Tang et al. (2011)" startWordPosition="961" endWordPosition="964">ling the document stream (Ghosh et al., 2013). These approaches have proven to be efficient in cutting down workload but they also limit an application’s performance. Furthermore, Sayyadi et al. (2009) discovered and tracked topics in social streams based on keyword graphs. They applied the sliding window principle to keep the computation feasible, although their data set only contained 18k documents. Yang et al. 2012 tracked topics in tweet streams using language models. To cope with the computational effort they assume a small set of topics of only a few dozen, which are defined in advance. Tang et al. (2011) tracked a single topic on a few thousand blogs based on semantic graph topic models. Pon et al. (2007) recommend news by tracking multiple topics for a user but their data sets only span several thousand documents and a few topics. Further related work includes the real-time filtering task, introduced as part of TREC’s Microblog Track in 2012 (Soboroff et al., 2012). Hong et al. (2013) explore topic tracking in tweet streams in relation to the TREC real-time filtering task by relying on a sliding window principle, while focusing on the cold start problem. 3 Topic Tracking 3.1 Traditional Appr</context>
</contexts>
<marker>Tang, Wang, Lu, Wang, Li, 2011</marker>
<rawString>Jintao Tang, Ting Wang, Qin Lu, Ji Wang, and Wenjie Li. 2011. A Wikipedia based semantic graph model for topic tracking in blogosphere. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Three (IJCAI’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>TDT by NIST</author>
</authors>
<date>1998</date>
<note>http://www.itl.nist.gov/iad/mig/ tests/tdt/resources.html (Last Update:</note>
<marker>NIST, 1998</marker>
<rawString>TDT by NIST - 1998-2004. http://www.itl.nist.gov/iad/mig/ tests/tdt/resources.html (Last Update: 2008)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianshu Weng</author>
<author>Erwin Leonardi</author>
<author>Francis Lee</author>
</authors>
<title>Event Detection in Twitter.</title>
<date>2011</date>
<booktitle>In Proceeding of ICWSM.</booktitle>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1195" citStr="Weng et al., (2011)" startWordPosition="174" endWordPosition="177">er document stream. We introduce two tracking approaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million documents in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams. 1 Introduction The emergence of massive social media streams has sparked a growing need for systems able to process them. While previous research (Hassan et al., 2009; Becker et al., 2009; Petrovic et al., 2010; Cataldi et al., (2010); Weng et al., (2011); Petrovic 2013) has focused on detecting new topics in unbounded textual streams, less attention was paid to following (tracking) the steadily growing set of topics. Standard topic tracking (Allan, 2002) deals with helping human analysts follow and monitor ongoing events on massive data streams. By pairing topics with relevant documents, topic tracking splits a noisy stream of documents into sub-streams grouped by their target topics. This is a crucial task for financial and security analysts who are interested in pulling together relevant information from unstructured and noisy data streams.</context>
</contexts>
<marker>Weng, Leonardi, Lee, 2011</marker>
<rawString>Jianshu Weng, Erwin Leonardi, Francis Lee. Event Detection in Twitter. 2011. In Proceeding of ICWSM. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xintian Yang</author>
<author>Amol Ghoting</author>
<author>Yiye Ruan</author>
<author>Srinivasan Parthasarathy</author>
</authors>
<title>A framework for summarizing and analysing twitter feeds.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ’12).</booktitle>
<publisher>ACM,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6000" citStr="Yang et al. 2012" startWordPosition="926" endWordPosition="929">ommon practice to limit data sets to cope with the computational effort. Popular strategies involve reducing the number of tracked topics (Lin et al., 2011; Nichols et al., 2012;) as well as sampling the document stream (Ghosh et al., 2013). These approaches have proven to be efficient in cutting down workload but they also limit an application’s performance. Furthermore, Sayyadi et al. (2009) discovered and tracked topics in social streams based on keyword graphs. They applied the sliding window principle to keep the computation feasible, although their data set only contained 18k documents. Yang et al. 2012 tracked topics in tweet streams using language models. To cope with the computational effort they assume a small set of topics of only a few dozen, which are defined in advance. Tang et al. (2011) tracked a single topic on a few thousand blogs based on semantic graph topic models. Pon et al. (2007) recommend news by tracking multiple topics for a user but their data sets only span several thousand documents and a few topics. Further related work includes the real-time filtering task, introduced as part of TREC’s Microblog Track in 2012 (Soboroff et al., 2012). Hong et al. (2013) explore topic</context>
<context position="14458" citStr="Yang et al. (2012)" startWordPosition="2308" endWordPosition="2311">ument d in T, D do ifdeTthen cmin = argmin,{distances(d, c e C)} if distance(d,cmin) &gt; thrspawn then spawnNewCluster(d → C) else if distance(d,cmin) &lt; thradapt then contribute,assign(cmin,d) else assign(cmin,d) else if d e D then cmin = argmin,{distances(d,c e C)} candidateSet = {t a cmin} for all topics t in candidateSet do if similarity(d,t) &gt; threshold then emit relevant {t, d} While the literature provides a vast diversity of clustering methods for textual documents, our requirements regarding tracking streams of topics naturally reduce the selection to lightweight single-pass algorithms. Yang et al. (2012) provided evidence that in extreme settings simple approaches work well in terms of balancing effectiveness, efficiency and scalability. We identified ArteCM by Carullo et al. (2008), originally intended to cluster documents for the web, as suitable. Algorithm 3 outlines our approach for cluster based tracking. Given an initial set of 4 random centroids, we compare each arriving topic to all centroids. We associate the new topic with the cluster whenever it is close enough. Particularly close documents contribute to a cluster, allowing it to drift towards topic dense regions. If the document i</context>
</contexts>
<marker>Yang, Ghoting, Ruan, Parthasarathy, 2012</marker>
<rawString>Xintian Yang, Amol Ghoting, Yiye Ruan, and Srinivasan Parthasarathy. 2012. A framework for summarizing and analysing twitter feeds. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ’12). ACM, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>