<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.656181" genericHeader="method">
ANALYZING THE STRUCTURE OF ARGUMENTATIVE DISCOURSE
</sectionHeader>
<author confidence="0.896282">
Robin Cohen
</author>
<affiliation confidence="0.888935333333333">
Department of Computer Science
University of Waterloo
Waterloo, Ontario, Canada N2L 3G1
</affiliation>
<bodyText confidence="0.99811575">
Consider a discourse situation where the speaker tries to convince the hearer of a particular point of view.
The first task for the hearer is to understand what it is the speaker wants him to believe — to analyze the
structure of the argument being presented, before judging credibility and eventually responding.
This paper describes a model for the analysis of arguments that includes:
</bodyText>
<listItem confidence="0.9980891">
• a theory of expected coherent structure which is used to limit analysis to the reconstruction of particular
transmission forms;
• a theory of linguistic clues which assigns a functional interpretation to special words and phrases used by
the speaker to indicate the structure of the argument;
• a theory of evidence relationships which includes the demand for pragmatic analysis to accommodate
beliefs not currently held.
The implications of this particular design for dialogue analysis in general are thus:
• structure is an important feature to extract in a representation to control the processing;
• linguistic constructions can be assigned useful interpretations;
• pragmatic analysis is crucial in cases where the participants differ in beliefs.
</listItem>
<sectionHeader confidence="0.986022" genericHeader="method">
1 THE PROBLEM AREA
</sectionHeader>
<bodyText confidence="0.997726225">
Consider the task of designing an &amp;quot;argument understand-
ing system&amp;quot;, a natural language understanding system
(NLUS) where the input is restricted to arguments.
Consider as well arguments constructed in a dialogue
situation, where a speaker (S) tries to convince a hearer
(H) of a particular point of view. The hearer patiently
listens; hence, the input is &amp;quot;one-way communication&amp;quot;.
The argument understanding system therefore plays the
role of the hearer, and tries to analyze the structure of
the argument being presented. This task is isolated as a
necessary first step for a hearer, in order to be a success-
ful participant in a conversation. In other words, the
hearer must have some representation of what it is the
speaker wants him to believe, before judging credibility
and eventually responding.
Note that this language problem is relatively new and
yet feasible. It is distinct from other NLU endeavors,
such as story understanding, which appeal to a stereotype
of content in order to reduce processing. In arguments,
one is never sure what points the speaker will address;
content can&apos;t be stereotyped. However, arguments have a
defining characteristic — they are necessarily goal-orient-
ed. The speaker wants to convince the hearer of some
overall point. Thus, there is an overall logical structure to
the input and this fact may be used by a hearer to control
analysis.
For our model, the representation for the structure of
the argument is restricted to an indication of the claim
and evidence relations between the propositions. The
notion of evidence is discussed in more detail in section
4. A useful starting definition is: &amp;quot;A proposition E is
evidence for a proposition C if there is some rule of
inference such that E is premise to C&apos;s conclusion — in
other words, there is some logical connection between E
and C&amp;quot;.
In order to design an argument understanding system,
what is then required is a computational model for the
analysis of arguments. This in turn necessitates a theory
of argument understanding, as a basis for the model. We
suggest the following three components for the model:
</bodyText>
<listItem confidence="0.9975532">
• a theory of expected coherent structure, used to drive a
restricted processing strategy. Analysis is kept to a
computationally reasonable task, by limiting the input
to be recognized to a characterization of expected
coherent forms of transmission.
</listItem>
<footnote confidence="0.699534666666667">
Copyright1987 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that
the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy
otherwise, or to republish, requires a fee and/or specific permission.
</footnote>
<note confidence="0.366317">
0362-613X/87/010011-24$03.00
</note>
<title confidence="0.1760975">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 11
Robin Cohen Analyzing the Structure of Argumentative Discourse
</title>
<listItem confidence="0.997290272727273">
• a theory of linguistic clue interpretation, including
insight into both the occurrence of clue words and their
possible function in overall discourse. Clue words are
those words and phrases used by the speaker to direct-
ly indicate the structure of the argument to the hearer
(e.g. connectives). Detecting clues can thus also serve
to constrain the processing for the hearer. Moreover, it
is important to have some facility for recognizing and
interpreting clue words, to build a model that is robust
enough to process a wide variety of input.
• a theory of evidence relationships. The most important
</listItem>
<bodyText confidence="0.951753714285714">
observation is that pragmatic analysis is mandatory for
an analysis model in order to recognize beliefs of the
speaker, not currently held by the hearer. Evidence
connections between propositions often appeal to
unstated information not currently in the hearer&apos;s set of
beliefs, but recognizable as an intended support
relation on the part of the speaker.
</bodyText>
<sectionHeader confidence="0.995401" genericHeader="method">
2 RESTRICTED PROCESSING STRATEGY
</sectionHeader>
<bodyText confidence="0.999962788461539">
Consider the following framework for the model. An
argument is considered to be a set of propositions. The
model is then designed to analyze the argument a propo-
sition at a time, incrementally building a representation
for the underlying structure. The representation devel-
oped is a tree of claim and evidence relations comprising
the argument, where a claim node is father to its evidence
sons. In order to assign an interpretation for a given
proposition, one must thus simply assign it a place in the
tree. In this way, one can tell to which propositions it
serves as evidence and from which other propositions it
receives support.
A key design decision is to separate the two main
operations of determining for each proposition (i) where
it fits with respect to the argument so far, and (ii) how it
relates to some prior proposition. The question of how
two propositions relate in this framework is one of veri-
fying that an evidence relation holds between the prop-
ositions. This task is extracted and relegated to an
evidence oracle, which, passed two propositions A and B,
will respond &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot;, as to whether A is evidence
for B.
With the problem of evidence determination factored,
the model must still cope with the question of where a
proposition may fit. This is handled by characterizing
possible coherent transmissions (ordering of prop-
ositions) on the part of the speaker and then limiting
analysis to reception algorithms designed to recognize
these coherent transmissions. In the section below we
illustrate possible coherent strategies from the speaker,
and present the associated reception algorithm required
to recognize the input.
Note that the computational model for the analysis of
arguments is designed with certain aims and limitations.
In particular, the model is to provide for analysis of
&amp;quot;spontaneous discourse&amp;quot;, demanding construction of a
representation for the argument as each new statement is
processed. The following restriction to the processing
model is thus applied: the processor does not weigh all
possible interpretations for a proposition; if the oracle
sends back a yes answer to the question Is P evidence for
Q?, then P is attached as a son to Q in the tree. In other
words, the model simulates a hearer who does not have
the luxury of &amp;quot;looking back&amp;quot; and re-interpreting previous
statements. Moreover, the model aims to provide an
interpretation for the current proposition, so that once an
interpretation has been found (e.g. that P supplies
evidence for Q) that proposition has been processed. The
evidence relation is also taken to be transitive — i.e., if P
is evidence for Q and Q is evidence for R, then P is
evidence for R. (See Section 4 for more detail on the
evidence relation).
</bodyText>
<subsectionHeader confidence="0.992865">
2.1 PRE-ORDER
</subsectionHeader>
<bodyText confidence="0.92228325">
Pre-order transmissions are those where the speaker
consistently presents a claim and then states evidence.
The example below illustrates this form:
EX1:
</bodyText>
<listItem confidence="0.9949368">
1) Jones would make a good president
2) He has lots of experience
3) He&apos;s been on the city board 10 years
4) And he&apos;s very honest
5) He refused bribes while on the force
</listItem>
<bodyText confidence="0.695265">
With the tree representation:
2 /1\ 4
</bodyText>
<sectionHeader confidence="0.445002" genericHeader="method">
37 N. 5
</sectionHeader>
<subsectionHeader confidence="0.998307">
2.2 POST-ORDER
</subsectionHeader>
<bodyText confidence="0.9256805">
Another coherent strategy is post-order, where the
speaker consistently presents evidence and then states
the claim. Consider the comparable example below:
EX2:
</bodyText>
<listItem confidence="0.9955044">
1) Jones has been on the board 10 years
2) He has lots of experience
3) And he&apos;s refused bribes
4) So he&apos;s honest
5) He would really make a good president
</listItem>
<bodyText confidence="0.637729">
With the representation:
</bodyText>
<equation confidence="0.392631">
5N
</equation>
<bodyText confidence="0.915567666666667">
\
In this example, the claim is the same as in EX1, that
Jones would make a good president, but the evidence
precedes the claim in each case — i.e., 1 is evidence for 2;
3 is evidence for 4; and 2 and 4 together are evidence for
the final claim 5.
</bodyText>
<page confidence="0.912974">
12 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.418956">
Robin Cohen Analyzing the Structure of Argumentative Discourse
</note>
<bodyText confidence="0.999828428571429">
The associated reception algorithms for pre- and post-
order are described in detail in Cohen (1981). Both can
be shown to operate in linear time (linear in the number
nodes of the tree) and so are quite efficient. The pre-ord-
er reception essentially finds an interpretation for the
current proposition of an argument by searching for a
father, up the right border of the tree. The post-order
algorithm employs a stack; the current proposition tests
to be father to the top of the stack; then all sons are
popped off the stack, and the resulting tree pushed on
the stack.
We trace the construction of the trees of EX1 and EX2
below, to provide details of the pre-order and post-order
reception algorithms.
</bodyText>
<equation confidence="0.3845535">
EX1: PRE-ORDER
Algorithm:
</equation>
<bodyText confidence="0.724667">
- The current proposition is NEW.
- The proposition immediately prior is LAST.
</bodyText>
<listItem confidence="0.99801825">
1) Try NEW as evidence for LAST.
2) If that fails, try NEW as evidence for each of LAST&apos;s
ancestors, in turn, up to the root of the tree.
3) If the test in 1 succeeds, stop.
</listItem>
<bodyText confidence="0.7637884">
Consider as well a dummy root (D) for the tree for which
all nodes are evidence (to place the first proposition).
evidence
candidates oracle
in order test for NEW response
</bodyText>
<footnote confidence="0.912607285714286">
1,D 2 ev. for 1? yes
2,1,D 3 ev. for 2? yes
3,2,1,D 4 ev. for 3? no
4 ev. for 2? no
4 ev. for 1? yes
4,1,D 5 ev. for 4? yes
EX2: POST-ORDER
</footnote>
<listItem confidence="0.947574363636364">
Algorithm:
- Keep a stack of elements eligible to be evidence for
a current proposition, with the latest one as TOP.
- To interpret the current proposition NEW:
1) Test if TOP is evidence for NEW
2) If yes, then pop TOP (TOP: = TOP — 1) and make it
a son of NEW (build a tree under NEW); then repeat
step 1 with NEW value of TOP.
3) If no, make the tree with NEW as root the new TOP
of stack (push NEW onto stack). In essence, all sons
for a proposition are picked up at once.
</listItem>
<bodyText confidence="0.440956444444444">
evidence
stack test for oracle
elements evidence response
1 1 ev. for 2? yes
tree:1 under 2 2 ev. for 3? no
3, 1 under 2 3 ev. for 4? yes
2 ev. for 4? no
3 under 4, 1 under 2 4 ev. for 5? yes
2 ev. for 5? yes
</bodyText>
<subsectionHeader confidence="0.986068">
2.3 HYBRID
</subsectionHeader>
<bodyText confidence="0.898254333333333">
As a first approximation to a general processing strategy,
consider designing a reception algorithm to accept hybrid
pre- and post-order arguments (i.e., any given sub-argu-
ment may be transmitted in pre- or post-order). An
example of hybrid transmission is EX3 below.
EX3:
</bodyText>
<listItem confidence="0.9961676">
1) Jones would make a good president
2) He has lots of experience
3) He&apos;s been on the board 10 years
4) And he&apos;s refused bribes
5) So he&apos;s honest
</listItem>
<equation confidence="0.509817">
With the representation:
27 1 N
37 N
</equation>
<bodyText confidence="0.947400538461539">
Here, the first sub-argument is in pre-order, the second
one in post-order, and the argument overall is still coher-
ent.
The reception algorithm for accepting hybrid trans-
missions is basically a combination of techniques from
the pre- and post-order receptions. Now, to process a
current proposition both a father and possible sons must
be searched for. But the search is still restricted — certain
propositions get closed off as possible &amp;quot;relatives&amp;quot; to the
current one (e.g., earlier brothers of an ancestor). Thus,
the complexity of the algorithm is still reasonable; it can
also be shown to be linear. Once more, see Cohen
(1981) for further examples.
A full description of the hybrid algorithm is included
below.
L is kept as a pointer to the lowest node of the tree
still eligible to receive evidence. It is initially set to a
dummy root node to which all other nodes succeed as
evidence. Consider as well a labelling where the last
proposition in the stream that succeeds as evidence for a
node is stored as the rightmost son.
For each node NEW in the input stream do:
/* find father */
do while (NEW is not evidence for L
and L is not dummy root)
set L to father of L
</bodyText>
<figure confidence="0.521754083333333">
end;
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 13
Robin Cohen Analyzing the Structure of Argumentative Discourse
/* see if there are any sons to re-attach */
if (rightmost son of L is not evidence for NEW)
/* no sons to re-attach */
then do;
attach NEW to L
set L to NEW
end;
else
do;
</figure>
<bodyText confidence="0.88987125">
/* some son will re-attach; attach all sons of L
which are evidence for NEW below NEW */
do while (rightmost son of L evidence for NEW)
attach rightmost son of L to NEW
remove rightmost son of L from L
end;
attach NEW to L
end;
Note that the hybrid algorithm, in searching for both
sons and father to the current node, must contend with
cases where a proposition is attached to a higher ancestor
and must be re-attached to its immediate father. This
occurs in a framework where the evidence relation is
considered transitive. The kinds of orderings that involve
this &amp;quot;re-location&amp;quot; are of the form: A, C, B — where C is
evidence for B, B evidence for A (hence C also succeeds
as evidence for A when tested).
For EX3:
test for evidence oracle
candidates evidence response
</bodyText>
<equation confidence="0.962816">
1 ev. for D? yes
1, D 2 ev. for 1? yes
2,1,D 3 ev. for 2? yes
3,2,1,D 4 ev. for 3? no
4 ev. for 2? no
4 ev. for 1? yes
4,1,D 5 ev. for 4? no
(3&amp;2 (4 is a case of 5, so can&apos;t
</equation>
<bodyText confidence="0.780993">
&amp;quot;closed off&amp;quot;) assume 5 ev. for 4)
</bodyText>
<listItem confidence="0.4166656">
5 ev. for 1? yes
4 ev. for 5? yes
(testing re-
attach of
sons)
</listItem>
<subsectionHeader confidence="0.994228">
2.4 SUMMARY OF PROCESSING STRATEGY
</subsectionHeader>
<bodyText confidence="0.9999895">
The processing strategy proposed for our model of argu-
ment analysis is designed to produce a selective interpre-
tation. The particular restrictions to processing chosen
for the model are, moreover, both
</bodyText>
<listItem confidence="0.9981745">
• useful for measuring the efficiency of the model, since
expressed in a framework of an algorithm operating on
a tree structure, where complexity of the algorithm can
be studied and
• well-motivated, since based on a characterization of
coherent input.
</listItem>
<bodyText confidence="0.999705590909091">
In fact, it is an expression of a theory of argument coher-
ence that serves to produce a model that is both efficient
and robust (can handle a wide variety of input).
Note that the particular restrictions suggested are
drawn from analysis of a body of examples from rhetoric
texts, together with some &amp;quot;naturally occurring&amp;quot; argu-
ments (letters to the editor in newspapers). The aim is to
characterize coherent transmission forms (ordering of
propositions) that can be understood, without additional
&amp;quot;clue words&amp;quot; to the underlying structure. We feel that
the hybrid model is a good first approximation because
examples of various forms of hybrid were encountered,
and exceptions to the form all involved additional clue
information. This leaves the study of clues and possible
extended transmission forms to the next section. (In
Cohen (1983) a few longer examples are run through the
model to illustrate the forms it can accept and to moti-
vate provision for the recognition of a wide variety of
argument forms. Note that an actual implementation was
not produced. There is now a scaled-down first imple-
mentation (Smedley 1986), which will be discussed
further in section 6.)
</bodyText>
<sectionHeader confidence="0.993113" genericHeader="method">
3 LINGUISTIC CLUES
</sectionHeader>
<bodyText confidence="0.998836">
The second main component of the argument under-
standing model is a theory of linguistic clues. These are
the words and phrases often used by the speaker to
directly indicate the structure of the argument to the
hearer. It is important to specify
</bodyText>
<listItem confidence="0.972535714285714">
• what kinds of clues exist
• the function of these clues in analysis — i.e., what inter-
pretation can be assigned to a proposition that contains
a clue word, and
• (a more difficult question) when clues are necessary to
ensure comprehension of the argument structure by the
hearer.
</listItem>
<bodyText confidence="0.9999638125">
This approach to the study of clue words is much more
detailed than the initial suggestions of Hobbs (1976) on
how to interpret a few connectives such as and in his
framework. It is also distinct from the investigations of
Reichman (1981) and (recently) Grosz and Sidner
(1986). Grosz and Sidner acknowledge the existence of
clues and discuss various discourse structures that can be
formed in the presence of clues. Reichman also gives a
longer list of clue words and the particular conversational
moves they signal. But there is no systematic proposal for
interpreting a clue word that may occur (classification),
and there is little discussion of how to process some of
these more complex discourse structures without clues
(suggesting when clues are necessary). In this section we
clarify more deeply some of the discussion of clues in
Cohen (1983, 1984b).
</bodyText>
<subsectionHeader confidence="0.992855">
3.1 CLUES OF RE-DIRECTION
</subsectionHeader>
<bodyText confidence="0.999939">
One type of clue is expressions that specifically re-direct
the hearer to an earlier part of the argument. Consider:
</bodyText>
<page confidence="0.836117">
14 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.423364">
Robin Cohen Analyzing the Structure of Argumentative Discourse
EX4:
1) The city is a mess
2) The parks are a mess
3) The park benches are broken
4) The grassy areas are parched
5) Returning to city problems, the highways are bad too
With the representation:
1
27\
N
3 4
</figure>
<bodyText confidence="0.999970666666667">
Here, the clue in proposition 5, returning to city problems
signals additional support for proposition 1. In the
absence of a clue, the specifications for the hybrid algo-
rithm would have 5 test to be a son for 4, 2, and 1 (up
the right border of the tree). So, adding clues should
reduce the processing effort of the hearer. (In fact, if
clues are consistently used by the speaker after long
chains of evidence, the processing complexity of the
reception algorithm can reduce from linear to real-time).
</bodyText>
<subsectionHeader confidence="0.9935">
3.2 CONNECTIVES
</subsectionHeader>
<bodyText confidence="0.971468875">
Another popular type of clue word is the connective. We
present a classification or taxonomy of connectives, and
then associate a common interpretation rule within the
claim and evidence framework for each category of the
taxonomy. In this way, the interpretation of any proposi-
tion containing a connective can be determined on the
basis of the clue word&apos;s classification. For example:
EX5:
</bodyText>
<listItem confidence="0.99471025">
1) This city is a disaster area
2) Houses have been demolished
3) Trees have been uprooted
4) As a result, we need national aid
</listItem>
<bodyText confidence="0.995789804878049">
With the representation:
Here, the connective as a result in proposition 4 belongs
to a category known as inference, indicating that there
should be some prior proposition that connects to 4 and
serves as son to 4 — i.e., supplies evidence for &amp;quot;the
result&amp;quot;. In this example, in fact, 1 acts as the son. The
interpretation rules are necessarily default suggestions for
translating the semantic relations between propositions
into our claim and evidence classification. (See further
discussion on the evidence relation (section 4) for possi-
ble exceptions).
The taxonomy and its associated interpretation rules
are presented below.
Each category is a classification of connectives, made
on the basis of semantic meaning of the connective —
e.g., the parallel category would include all words that
extended a list, including next, then, first, secondly, third-
ly, etc. The set of classes was produced by considering
the categories proposed in Quirk et al. (1972), and merg-
ing some classes that had similar semantics and suggested
the same interpretation rule for claim and evidence. The
inference category covers phrases that suggest one
proposition can be inferred from another — e.g., as a
result, because of this, etc. The detail category moves in
the other direction, and includes connectives that specify
further — e.g., in particular, specifically, etc. The summary
category is used for phrases that conclude a list. Refor-
mulation captures clue words that repeat an earlier idea —
e.g., in other words, once more, etc. Finally, contrast
covers the phrases that introduce comparisons, like on the
other hand or but.
In conjunction with the semantic classes, evidence
interpretation rules were then assigned as default inter-
pretations. (See Section 4.3 for pragmatic &amp;quot;overrides&amp;quot; to
the defaults). Note that some words may fall into more
than one category, based on the meaning used — e.g.,
then meaning &apos;next&apos; (in a list of actions) compared with
then meaning &apos;as a result&apos;. Clue interpretation thus
requires a classification process as well.
P is prior proposition; S is the proposition with the
clue
</bodyText>
<sectionHeader confidence="0.442443" genericHeader="method">
CATEGORY RELATION: P to S EXAMPLE
</sectionHeader>
<bodyText confidence="0.98140555">
parallel brother First, Second
inference son As a result
detail father In particular
summary multiple sons In sum
reformulation son (&amp; father) In other words
contrast brother or father On the other hand
The taxonomy is described in detail in Cohen
(1984b). We include discussion of one category here, as
an example. The detail class is one case where connec-
tives with a range of meanings were merged into one
category. The title &amp;quot;detail&amp;quot; suggests that the connective
will further specify some prior •proposition. Included
cases are: for example, in particular, and as another
instance. The interpretation rule assigned to this category
is that the proposition with the connective provides
evidence for the earlier connecting statement. The moti-
vation is that an accumulation of specific cases leads to a
conclusion of a general statement (a form of reasoning
used very often in naturally occurring arguments).
EX6:
</bodyText>
<listItem confidence="0.989509333333333">
1) The people in this town deserve a city-wide holiday
2) For example, Old Man Jones has worked non-stop
since Christmas
3) And Mayor Flood is still recovering from all his
efforts for the tornado relief
4) In short, all of us are tired
</listItem>
<figure confidence="0.8757908">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 15
Robin Cohen Analyzing the Structure of Argumentative Discourse
1
4
2&apos;3
</figure>
<bodyText confidence="0.664822">
2 is son to 1 (detail class); 3 is also son to 1 (brother to
2) (parallel class); 4 is father to 2 and 3 (summary class).
</bodyText>
<subsectionHeader confidence="0.993895">
3.3 THE FUNCTION OF CLUE WORDS
</subsectionHeader>
<bodyText confidence="0.999966357142857">
So far we have discussed two types of clue words that
can occur in conjunction with arguments transmitted
according to the specifications of the hybrid algorithm
presented in section 2 (our characterization for coherent
discourse). We point out that re-direction clues provide
additional information concerning which of the eligible
propositions is related to the current one, and that
connective clues specify the kind of relation that must be
found between the current proposition and one of the
eligible priors.
In certain cases these restrictions will prevent some of
the tests for evidence that would otherwise have been
conducted, thus saving some processing effort. For exam-
ple, consider the following:
</bodyText>
<listItem confidence="0.882024">
EX7:
1) The city is in serious trouble
2) There are some fires going
3) Three separate blazes have broken out
4) In addition, a tornado is passing through
</listItem>
<equation confidence="0.685026666666667">
with the representation:
1
37 2N
</equation>
<bodyText confidence="0.999597833333333">
The clue in 4 prescribes an interpretation for 4 as brother
to some prior proposition. This means that 4 must act as
evidence for some different proposition. Thus, even
though 3 is technically the first proposition to test out
when interpreting a new proposition (NEW evidence for
LAST is the test), in this particular case this option is not
possible. Thus, one round of work for the evidence oracle
is avoided. In fact, for this example, the test &amp;quot;4 evidence
for 2&amp;quot; fails, and the final test of &amp;quot;4 evidence for 1&amp;quot;
succeeds. (Note that the brother relation is tested by way
of son relation to a (common) father. This is because the
model only processes evidence relationships).
</bodyText>
<subsubsectionHeader confidence="0.592835">
3.3.1 THE NECESSITY FOR CLUES
</subsubsectionHeader>
<bodyText confidence="0.999272047619048">
We now examine the use of clue words in conjunction
with transmissions that violate the specifications of the
hybrid base case. The hypothesis is that more complex
transmissions can be accommodated by the argument
analysis model provided there exist clues to assist the
hearer in recognition. In these cases, the clues are there
by necessity. Their function in the discourse is not to
merely add detail on the interpretation of the contained
proposition, but to allow that proposition an interpreta-
tion that would otherwise be denied.
There is an advantage to adhering to the base case of
acceptable argument structures and treating the use of
clues with other transmission forms as exceptional. In the
first place, this provides a framework for detecting one
interpretation for an argument when another possible
interpretation could be generated if further testing
occurred. In other words, in this model a representation
drawn using the rules of the hybrid reception will always
be accepted as the intention of the speaker, unless clues
specifically override possible tests.
To explain, consider the following example:
</bodyText>
<listItem confidence="0.909938285714286">
EX8:
1) The park benches are rotting
2) The parks are a mess
3) The highways are run down
4) (And another problem with the parks is that) the
grass is dying
5) This city is in sad shape
</listItem>
<bodyText confidence="0.998789739130435">
Without the clue phrase in 4, re-directing to proposition
2, to add more evidence out of turn, a coherent represen-
tation could be built just the same, as below:
If the speaker intends 4 to add detail to the parks prob-
lem, he cannot expect the hearer to make this connection
without more information, simply because a more effort-
less interpretation of 4 is possible (and the speaker
should realize that it is this representation that the hearer
will draw).
Another important reason for separating the base case
of acceptable transmissions is to allow for input that can
be characterized as somehow a coherently generated plan
of the speaker. Recall that the proposition analyzer will
continuously call on an evidence oracle to determine if
some proposition A acts as evidence for some other
proposition B. Suppose there were no restrictions in the
testing of evidence relations. Then, tests for evidence
that would be interpreted as positive would return this
response, regardless of when asked. In other words, a
totally random display of propositions would result in the
same representation for the argument as the reception of
a coherently ordered presentation. Consider the follow-
ing example:
</bodyText>
<listItem confidence="0.930853">
EX9:
1) Yogi&apos;s been a shrewd manager
2) He hired a hitter who now bats .400
3) He traded in a pitcher who is now 0 and 12
4) But he got involved in drug deals to the players
5) And that&apos;s inexcusable for a manager
6) He really needs to be axed
</listItem>
<page confidence="0.643435">
16 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.9929678">
Robin Cohen Analyzing the Structure of Argumentative Discourse
with the representation:
6
5
4 2Z3
</figure>
<bodyText confidence="0.918437333333333">
(where 5 and 1 are contrasting evidence to 6).
EX9B: The argument as above, presented in the order::
2,5,3,6,4,1
</bodyText>
<listItem confidence="0.995321333333333">
1) Yogi hired a .400 hitter
2) (And) that&apos;s inexcusable for a manager
3) He traded in a pitcher who is now 0 and 12
4) Yogi really needs to be axed
5) He got involved in drug deals to the players
6) He is a shrewd manager
</listItem>
<bodyText confidence="0.999337636363636">
This argument now appears incoherent. One reason is
that there is contrast overall that must be clearly sepa-
rated. In any case, the ordering does not conform to the
specifications of the hybrid and as such is a candidate for
an unacceptable transmission.
This example suggests that the use of clue words,
though helpful to signal exceptional transmissions, should
still be studied as a systematic process of the speaker to
assist the hearer in comprehension. In EX9B, could any
number of clue words be added to still make this recogni-
zable? Consider the following attempted repair to EX9B:
</bodyText>
<listItem confidence="0.948368111111111">
EX9C:
1) Yogi hired a .400 hitter
2) But he&apos;s done some things inexcusable for a manag-
er
3) Although he also did other smart things like trading
a pitcher who is now 0 and 12
4) No, Yogi really has to be axed
5) He got involved in drug deals to the players
6) Though he still is a shrewd manager
</listItem>
<bodyText confidence="0.999947916666667">
The question is whether an argument of this form would
still be judged coherent. Our preference will be to specify
particular types of exceptional transmissions that may be
judged coherent. To this end, we have tried to isolate a
few specific cases where clues can be used in conjunction
with coherent orderings beyond the specifications of the
hybrid algorithm. These are highlighted in the next sub-
section below.
One more point about the last example above is that it
emphasizes our concern that the analysis of arguments be
done efficiently. In this case, we want to avoid making
tests for evidence relations that could not exist between
certain propositions as part of a coherent input. In other
words, we don&apos;t want the model to simply test out all
possible combinations of evidence relations (even though
this would only be n*n vs. k*n number of operations),
because an input that is not coherent would then be
accepted.
We would also not want to derive computationally a
representation for an argument that involved more
computational effort, if a simpler interpretation of the
same argument could be derived. (Again, the speaker is
to assume that the hearer will not be searching unneces-
sarily). This is illustrated in EX8 above.
</bodyText>
<subsectionHeader confidence="0.363943">
3.3.2 CASES OF NECESSITY
</subsectionHeader>
<bodyText confidence="0.964524666666667">
Three kinds of acceptable extended transmission strate-
gies are studied in Cohen (1983): parallel evidence, multi-
ple evidence (a proposition acting as evidence for several
claims, in restricted conditions) and mixed-mode sub-ar-
guments (with evidence both preceding and following a
claim). We present an example of the parallel
construction below. See Cohen (1984b, 1983) for further
examples.
EX10:
</bodyText>
<listItem confidence="0.991405428571429">
1) The city has problems
2) The parks are a mess
3) The highways are a mess
4) The buildings are a mess
5) Here&apos;s some evidence for the fact that the parks are
a problem: the benches are broken
6) As for the highways, they&apos;re full of potholes
</listItem>
<bodyText confidence="0.700316">
With the representation:
</bodyText>
<page confidence="0.398406">
4
</page>
<bodyText confidence="0.982119333333333">
Here, the argument breaks the rules of hybrid trans-
mission by adding evidence for an earlier brother (propo-
sition 2); however, this shift is signalled with a phrase of
intention in proposition 5, and the hearer may then
expect a parallel expansion of additional support for each
of the earlier brothers, in turn. (Note that without the
clue, the argument structure derived would simply record
all of 2 to 6 as evidence for 1, in the same vein as EX8).
This example illustrates another interesting feature of
clues — the variety of possible surface forms that can
signal the same evidence relation between propositions.
In EX10, the clue in 5 could also have been The problem
with the parks is... or / will now explain why the parks are
such a problem — .... A range of explicitness is thus possi-
ble. In cases other than this parallel construction, in fact,
a signal to a relation between propositions may be advo-
cated by the simple use of an anaphor. For example:
EX11:
</bodyText>
<listItem confidence="0.987169">
1) The mayor hasn&apos;edone much for this city
2) He doesn&apos;t seem to want to do much
3) That man is a complete loser
</listItem>
<bodyText confidence="0.999519714285714">
Here, the phrase that man signals a link to the mayor. It is
difficult to decide whether the phrase qualifies as a clue
word. The problem is determining a &amp;quot;bottom line&amp;quot; — i.e.,
&amp;quot;can&apos;t every sentence be seen to have some clue, from
semantics, to its interpretation within the argument?&amp;quot;.
For now, we do not consider the cases of anaphora as
above.
</bodyText>
<figure confidence="0.332234">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 17
Robin Cohen Analyzing the Structure of Argumentative Discourse
</figure>
<subsectionHeader confidence="0.974277">
3.4 SUMMARY AND FUTURE DIRECTIONS OF CLUE THEORY
</subsectionHeader>
<bodyText confidence="0.990478">
Our theory of clue interpretation so far has outlined the
following principles:
</bodyText>
<listItem confidence="0.998873846153846">
• Clues may occur with expected coherent transmissions
or to signal exceptional cases.
• Connective clues can be assigned common interpreta-
tion rules according to the semantics of the clue.
• To distinguish helpful versus necessary clues, the pref-
erence will always be to recognize the hybrid trans-
mission; if clue rules or semantics force an exceptional
reading, only certain exceptional structures should be
accepted.
• In all cases, a reading that can be derived with less
computational effort will always be taken as the
intended reading.
• The cases where clues are necessary to force a certain
</listItem>
<bodyText confidence="0.999359851851852">
interpretation provide insight into the function of clues;
their use in conjunction with acceptable transmissions
suggests a function of additional processing reductions.
In order to recognize clues and incorporate their inter-
pretation into a larger model that derives argument struc-
ture, we propose a separate clue interpretation module,
interacting with the basic reception algorithm and the
calls to the evidence oracle. Exactly how this module
would function is left as future work. We do have a few
initial insights, to suggest that clue interpretation is not
only quite useful (argued previously in this section) but
feasible.
For connectives, the clue can be recognized from a
classification. Then, determining whether a related
proposition available from the list of eligibles is in fact
related can be tested, according to the restrictions of the
interpretation rule (as suggested in EX7). Further, the
required semantic relation to the prior proposition can be
passed as additional information to the oracle. The prob-
lem is that this oracle must do some kind of search for
connections between facts and axioms of a knowledge
base. How this semantic analysis is achieved depends on
the underlying representation, but additional semantic
constraints should help to restrict operations.
For re-direction clues, a processor would first have to
recognize the appropriate phrase used. Some standard list
(e.g., returning to, on the topic of) may be specified as a
start. Then, the phrase should suggest some particular
semantic content to the prior proposition (e.g., returning
to the parks mentions parks as central). Now it is the
form of representation of the propositions which may
influence what is acceptable on a list of eligibles. If this
semantic processing can be done very broadly, some calls
to the oracle may be avoided, and this would be an
improvement, assuming the oracle&apos;s operations encom-
pass a more extensive search.
An initial definition for evidence offered in Section 1
is: &amp;quot;A proposition P is evidence for a proposition Q if
there is some logical connection from P to Q — i.e., some
rule of inference such that P is premise to Q&apos;s
conclusion&amp;quot;. The main problem in establishing evidence
relations is that not all the premises are stated. For exam-
ple, one common rule of inference used in arguments is
Modus Ponens, of the form: P Q, P therefore Q. The
way this rule is most often used, the speaker will simply
state P and Q and leave out the major premise &amp;quot;P Q&amp;quot;,
expecting the hearer to be able to fill in the unstated
connection to recognize the evidence relation from P to
Q. Omitting certain premises is referred to as Modus
Brevis and studied in Sadock (1977).
We list below the rule of inference frames included for
our model. Each rule has a slot for major premise, minor
premise, and conclusion, to be filled by stated or unstated
propositions, in recognizing an evidence relationship.
</bodyText>
<figure confidence="0.793552666666667">
RULE OF INFERENCE FRAMES:
Major Minor Conclusion
Modus Ponens P Q P
Modus Tollens P Q —Q —P
Modus Tollendo Ponens P v —Q Q
Modus Ponendo Tollens PvQ Q —p
</figure>
<bodyText confidence="0.943529">
The form most often used is Modus Ponens. When the
major premise is missing, this is the rule of inference to
consider as the intended link from P to Q.
EX12:
</bodyText>
<listItem confidence="0.978847">
1) The Jays had a fantastic team this year
2) All their players had averages over .250
fill:
3) If a team has all players with averages over .250,
then that team is fantastic
</listItem>
<bodyText confidence="0.999408285714286">
The common form for arguments, then, is one where the
hearer must supply missing statements in order to estab-
lish the connections for the representation of the argu-
ment.
There are several possible Modus Brevis forms for
each frame above. The possible missing parts are classi-
fied below for the case of Modus Ponens.
</bodyText>
<sectionHeader confidence="0.534946" genericHeader="method">
MODUS BREVIS FORMS (MODUS PONENS):
</sectionHeader>
<figure confidence="0.972384714285714">
Given
Premises Conclusion
Normal P
Missing Minor
Missing Major Q (most popular form)
Only Major P-4-() (assume rest)
Only Minor (assume rest)
</figure>
<footnote confidence="0.284784">
4.1. OVERVIEW OF ORACLE&apos;S PROCESSING
</footnote>
<sectionHeader confidence="0.937813" genericHeader="method">
4 EVIDENCE DETERMINATION
</sectionHeader>
<bodyText confidence="0.999708142857143">
The third main component of the argument analysis
model is a theory of evidence, to govern the verification
of evidence relations between propositions.
It is important to demonstrate that the part of processing
relegated to the evidence oracle within the overall model
is not insurmountable, to defend the model as useful. In
this section we examine more closely the operation of the
</bodyText>
<page confidence="0.962059">
18 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.304251">
Robin Cohen Analyzing the Structure of Argumentative Discourse
</figure>
<bodyText confidence="0.993314142857143">
oracle, opening up the &amp;quot;black box&amp;quot; just enough to
suggest how it operates.
In general, the oracle is asked to test two propositions
to be in an evidence relationship, responding &amp;quot;yes&amp;quot; or
&amp;quot;no&amp;quot; to a question of the form: &amp;quot;is P evidence for Q?&amp;quot;.
We sketch the operation of the oracle for the example
below:
</bodyText>
<equation confidence="0.888071">
EX13:
1) Joey is a dangerous
2) He is a shark
</equation>
<bodyText confidence="0.985308">
Assuming some resolution of anaphora, etc., a crude
representation of the example in terms of predicates and
arguments is:
</bodyText>
<equation confidence="0.811246">
P: Shark(j); Q: Dangerous(j) (j: Joey).
</equation>
<bodyText confidence="0.923989083333333">
A Modus Ponens template would be of the form: S(j),
for-all (x) ( S(x) D(x) ), therefore D(j) (S:is-shark,
D:is-dangerous). (Note that we are not addressing
certain questions here such as the significance that is a
shark is definitional, while is dangerous is really assertion-
al).
Recall that the argument analysis model seeks to
recognize intended argument structures. So, in this exam-
ple the hearer can at least recognize that &amp;quot;P is evidence
for Q&amp;quot; would follow through if All sharks are dangerous
were believed (i.e., for-all(x) (S(x) D(x) ).
It is thus proposed that the oracle:
</bodyText>
<listItem confidence="0.89094075">
1. identify missing premises (the Modus Brevis form of
the argument being presented);
2. verify plausibility of these missing premises (that
they could be intended by the speaker to be believed
by the hearer); and
3. conclude that an evidence relation holds if the miss-
ing premises are plausible.
For step 2, we try to specify more precisely in the
remainder of this section the kind of tests the hearer can
apply. A summary is provided below:
a) Identify the missing premise within a knowledge
base of shared knowledge.
b) Identify a &amp;quot;relaxed version&amp;quot; of the missing premise
within own private knowledge.
c) Identify the missing premise within a model of the
speaker&apos;s beliefs.
d) Judge the beliefs of a hypothetical third party
(which could be simplified if the bottom line is
&amp;quot;believe it, unless there&apos;s reason to strongly doubt,
from within one&apos;s own beliefs&amp;quot;).
</listItem>
<bodyText confidence="0.999936976190477">
We have found in simulations of the model on a varie-
ty of examples that most of the tests for evidence can be
answered through (a) and (b). We hypothesize that,
given a specification of a knowledge base, the search for
connections between propositions can be controlled. This
hypothesis would be best verified with an implementation
of the oracle and extensive analysis of examples, and
could be the focus of the next stage of our implementa-
tion (beyond Smedley (1986)). (The two large examples
dissected in Cohen (1983) do have this property.)
In addition to the problem that the major premise may
be unstated is the problem that this premise should really
be tempered by the beliefs of the speaker. In other
words, the missing major premise that the hearer must fill
in is really of the form: H believes that S wants H to
believe (P Q). In other words, this premise is not
necessarily one of the hearer&apos;s beliefs. It is important to
emphasize the importance of pragmatic processing in
establishing evidence relations. The tree of claim and
evidence relations built as a representation for the argu-
ment is really an indication of the plan of the speaker, in
the sense that each evidence relation recorded is the
hearer&apos;s conception of a support connection intended by
the speaker.
Note that it is difficult to specify how a plan of a
speaker is determined during analysis. What we are advo-
cating is to interpret the intention of each proposition of
the argument, the other propositions for which it
provides evidence. The result of processing the entire
discourse is not a complete plan of the speaker, in the
sense that each of the &amp;quot;steps&amp;quot; could be executed and the
top level goal (convince the hearer of some overall point)
would then follow. It is more an indication of the moti-
vation behind each utterance towards the ultimate goal of
convincing the hearer. The difficulties in plan inference
for discourse are discussed in more detail in Grosz and
Sidner (1986), and are in fact a topic of our current
concern (see discussion of future work in Section 6).
There is in fact a whole spectrum of problems the
hearer must face in recognizing evidence relationships
between propositions. The four main tests for the hearer
can be described as:
</bodyText>
<listItem confidence="0.957642166666667">
• use logic,
• relax the logic,
• stereotype the speaker,
• judge plausibility (reason as a &amp;quot;hypothetical person&amp;quot;).
We illustrate these possible operations with examples
below.
</listItem>
<subsectionHeader confidence="0.999496">
4.2 LOGIC AND RELAXED LOGIC
</subsectionHeader>
<bodyText confidence="0.9272795">
In example EX14, all the premises of the Modus Ponens
argument are present. The hearer should realize that 1
and 2 are evidence for the claim in 3. Then EX15 illus-
trates the more typical case of missing major premise. If
the hearer fills in All machine candidates win, the
connection from 1 to 2 can be seen. The problem is that
the speaker probably believes something more along the
lines of: Most machine candidates win. And yet, one
couldn&apos;t record a Modus Ponens relation in the argument
with a quantifier such as most instead of for all. Thus, the
hearer must use some relaxation to the rules of logic to
recognize the evidence relation. The detection of
evidence through &amp;quot;relaxed logic&amp;quot; can be accomplished by
having the hearer judge the unstated connection as a
plausible generalization, based on a few known cases.
For example, if the hearer tries to recognize an evidence
relation from 1 to 2 in EX16 below, by filling in All
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 19
Robin Cohen Analyzing the Structure of Argumentative Discourse
sharks are dangerous, and the hearer doesn&apos;t believe this
&amp;quot;axiom&amp;quot; but knows of a few sharks that are dangerous,
he may reason that the missing major premise is reason-
able.
EX14:
</bodyText>
<listItem confidence="0.953994333333333">
1) Aristotle is a man
2) All men are mortal
3) So Aristotle is mortal
EX15:
1) Bilandic will win
2) He&apos;s the machine candidate
</listItem>
<equation confidence="0.791223333333333">
EX16:
I) Joey is a shark
2) So, he is dangerous
</equation>
<bodyText confidence="0.999984909090909">
The idea of recognizing an intended connection from
some other conversant based on one&apos;s own beliefs is not
necessarily simple to implement. There has been some
recent work by Pollack (1986) that suggests a more
concrete foundation for this operation. Pollack discusses
the problem of inferring a questioner Q&apos;s plan from his
discourse. A first process has the responder R ascribing
to Q a belief about some connection (&amp;quot;conditional gener-
ation relation&amp;quot;) that she herself believes true. Occa-
sionally, R will need to recognize a connection that is not
one of her beliefs. Then Pollack suggests there is a rule
where &amp;quot;R ascribes to Q a belief about a relation between
act-types that is a slight variation of one she herself has&amp;quot;.
In particular, one slight difference possible has Q believ-
ing a stronger conditional generation relation, which is
missing one of the required conditions.
This related research is quoted here, not to argue that
this problem is solved, but to acknowledge that it is
important to specify this &amp;quot;relaxed&amp;quot; connection between
one&apos;s own beliefs and those attributed to another. Some
groundwork is being laid with more formal descriptions
of plans such as Pollack&apos;s.
</bodyText>
<subsectionHeader confidence="0.989937">
4.3 DIFFERENCE IN BELIEFS
</subsectionHeader>
<bodyText confidence="0.97961337037037">
The other type of problem faced by a hearer in recogniz-
ing evidence relations arises because of difference in
beliefs between speaker and hearer. As mentioned, the
hearer is actually discerning intended relations on the
part of the speaker, and must often reason outside his
personal framework of knowledge. Again, an issue is
raised of how to discern intentions from discourse. Some
work has been done at the level of one utterance (e.g.,
Allen 1979). We are mostly concerned with advocating
the inclusion of reasoning beyond one&apos;s own beliefs,
without a full theory of how to infer another person&apos;s
beliefs. Instead, we advocate a simplified framework,
discerning evidence relations and allowing a connection
to be drawn as intended if it is plausible to the hearer.
For future work, we are studying how to specify this
process more precisely. (See also Grosz and Sidner
1986).
The point is that the hearer is still able to recognize
connections he does not believe. In EX17, the hearer
should be able to understand an evidence relation from 1
to 2, upon filling in a missing premise of the form &amp;quot;If a
person stands for apple pie and Mom then he is great&amp;quot;. If
the hearer does not believe this statement himself, he
may still consider it to be a reasonable belief of the
speaker; having stereotyped knowledge of the speaker&apos;s
views may thus be useful.
EX17:
</bodyText>
<listItem confidence="0.889616">
1) Reagan is great
2) He stands for apple pie and Mom
</listItem>
<bodyText confidence="0.982210571428571">
Finally, if the hearer is testing a possible evidence
relation between two propositions, does not believe the
missing premise, and has no prior knowledge of the
speaker, the best option available is to try to judge the
plausibility of the unstated information. Essentially, the
hearer must postulate new facts (which he may not be
sure he wants to also believe) and consider relationships
between these facts as plausible or not. It is in this sense
that he adopts a &amp;quot;hypothetical person&apos;s&amp;quot; beliefs. Note
that it is often the case that one will accept new facts
unless something from one&apos;s own beliefs suggests a
contradiction. In this sense, the judgement of plausibility
does relate back to the hearer&apos;s own beliefs.
An example with an implausible missing connection is
EX18 below. If the hearer tests 2 as possible evidence for
1, a major premise of the form &amp;quot;All sharks like tap
dancing&amp;quot; would establish the relation. But the hearer
would not regard this as a plausible belief of the speaker,
and so would fail to recognize an evidence relation
between the two propositions.
EX18:
</bodyText>
<listItem confidence="0.943405">
1) Joey likes to tap dance
2) He is a shark
</listItem>
<bodyText confidence="0.995568181818182">
The problem of judging plausibility is difficult. To
make this process more computationally tractable, one
suggestion is to incorporate into the model some tracking
of mutual belief between speaker and hearer. (See Cohen
(1978) for further discussion on the use of mutual belief
in natural language processing.) Then, certain tests for
evidence relations can be blocked in the oracle, based on
mutual belief.
For instance, rules can be postulated regarding the use
of claims and evidence that are mutually believed. Two
sample rules are:
</bodyText>
<listItem confidence="0.93604925">
(i) &amp;quot;If P is mutually believed, it can&apos;t be used as
claim&amp;quot;.
(ii) &amp;quot;If —P is mutually believed, P can&apos;t be used as
evidence&amp;quot;.
</listItem>
<bodyText confidence="0.7019112">
In addition, some of the default interpretation rules
associated with the taxonomy of linguistic clues can be
overruled by pragmatic considerations. The idea is to
possibly override the default semantic interpretations of
evidence relations otherwise specified, by testing whether
</bodyText>
<page confidence="0.705312">
20 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.238679">
Robin Cohen Analyzing the Structure of Argumentative Discourse
</note>
<bodyText confidence="0.999697153846154">
propositions are already mutually believed. Note that the
idea of a &amp;quot;pragmatic override&amp;quot; is also employed in
Gazdar (1979) for the problem of determining presuppo-
sitions. The importance of pragmatic processing for argu-
ment analysis is once more emphasized, as it is a critical
component to the difficult procedure of judging plausibil-
ity.
While considering mutual belief will help to eliminate
some potentially difficult tests for the oracle, the model
would require a more detailed specification of the main-
tenance and use of mutual belief. This is left as a topic
for future work. Some current ideas are explored in more
detail in Cohen (1985).
</bodyText>
<subsectionHeader confidence="0.985102">
4.4 SUMMARY OF EVIDENCE THEORY
</subsectionHeader>
<bodyText confidence="0.99998452631579">
The &amp;quot;theory&amp;quot; of evidence relationships, developed to
specify the operation of the evidence oracle component
of the argument analysis model, really presents insight
into the problems relevant to evidence relations, rather
than offering solutions. Still, the fundamental question of
how connections between propositions can be verified
has not been studied to any extent by other researchers.
It is extremely worthwhile to acknowledge that it is not
sufficient to indicate what relations do occur, without
also suggesting how these relations could be established
during analysis.
In addition, we have provided some insight into the
more general question of how to accommodate a possible
difference in beliefs between conversants in a natural
language dialogue processing system. We also suggest
that a less sophisticated oracle can be constructed that
merely searches known facts and axioms, possibly includ-
ing relaxations, to handle a large amount of naturally
occurring arguments.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="related work">
5 RELATED WORK
</sectionHeader>
<subsectionHeader confidence="0.862696">
5.1 ARGUMENT UNDERSTANDING
</subsectionHeader>
<bodyText confidence="0.999966906976744">
Other researchers in natural language have studied argu-
ments, in particular. However, the focus of the research
in each case has been different. Birnbaum and the group
at Yale (Birnbaum et al. 1980, McGuire et al. 1981)
study two-way communication, developing an argument
graph to display the points raised by both conversants.
This graph is then used to determine the best moves on
the part of an adversary, to challenge the position of the
other conversant. Thus, the question of what responses
to generate is investigated. On the other hand, there is
little insight into how a hearer can detect the points being
raised by the speaker, to construct this argument graph.
Our focus has thus been on this preliminary problem to
argument understanding.
Archbold (Archbold 1976, Archbold and Hobbs
1980) is most concerned with evaluative arguments,
those with strong underlying ideologies. For example,
Lenin&apos;s speeches are appropriate sample input. Thus, the
difficult question of recognizing differing opinions is a
focus to Archbold&apos;s investigations. In addition, he studies
text rather than discourse, allowing for a deeper review
(re-reading) of the input in order to derive an analysis.
Weiner (1979) describes a representation for argu-
ments that is also a tree structure, with a variety of links
possible. His main concern is to characterize types of
argument structures, for use in the generation of explana-
tions. There is thus little attention on the problems
encountered in deriving argument structures during anal-
ysis.
Weiner&apos;s (1979) model for the structure of explana-
tion bears some resemblance to the representation
described for arguments here. Weiner claims that natural
explanation can be regarded as a series of transforma-
tions of an underlying tree structure that represents the
abstract form of the argument being developed. The
ways in which support can be supplied are listed more
extensively, including examples, alternatives, etc. How
the tree can be built up relies on tracking a node that is
&amp;quot;in focus&amp;quot;. The •fact that determining the relations
between statements may make use of clue words is
mentioned briefly as well. Basically, some of the features
we advocate appear in this research. But we are trying to
provide more insight into operational questions such as:
</bodyText>
<listItem confidence="0.99943">
• How do you determine the (best) relation between
propositions?
• How is the focus set? and
• When are clue words likely to occur?
</listItem>
<bodyText confidence="0.967494085714286">
By contrast, Weiner concentrates on how to generate
explanations using his precisely specified characteriza-
tion.
Reichman (1981) is concerned with a larger problem
of producing a model of discourse (not just arguments),
but her approach should handle arguments as well. The
core of the model is an ATN grammar for parsing and
generation, coupled with a representation of &amp;quot;context
spaces&amp;quot; containing conversational moves. The conversa-
tional moves provide a classification of larger compo-
nents of discourse (not just single propositions). For
example, there is an extensive study of a &amp;quot;challenge&amp;quot;
operation. Since Reichman&apos;s aims are broader than ours,
the lower level issues we address of verifying evidence
and studying the necessity for clue words are not consid-
ered for the model. Moreover, there is an intentional
lack of concern with pragmatic processing, another
crucial feature our model. Instead, Reichman presents a
model for the analysis of a variety of two-person inter-
actions.
In sum, our efforts in argument understanding are
worthwhile because we focus on the necessary first step
in argument analysis — determining the intended struc-
ture, or &amp;quot;what the argument is about&amp;quot;. We study the
possible structural relations between propositions, and
investigate the difficult issue of verifying evidence
relationships. The importance of pragmatic analysis to
recognize whole classes of arguments that involve differ-
ing beliefs is stressed in our work. And finally, the use
and interpretation of clue words is addressed. It is worth
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 21
Robin Cohen Analyzing the Structure of Argumentative Discourse
noting that the differences in our existing studies can
possibly be exploited by pooling efforts and suggesting a
powerful general model for argument analysis.
</bodyText>
<subsectionHeader confidence="0.997669">
5.2 REFERENCE RESOLUTION AND FOCUS
</subsectionHeader>
<bodyText confidence="0.999948272727272">
Some of the work on using focus for reference resolution
contains similarities to the model presented here for
analyzing argument structure. Sidner (1979) maintains a
focus stack of possible items in focus and an alternate
focus list to support shifts of focus. The candidates for
resolving references are thus restricted and ordered. The
point is that these restrictions are drawn from a charac-
terization of coherent discourse, the same approach
taken for our control of processing. Grosz (1977)
presents a model of focus spaces which may be used for
several purposes, including the resolution of definite
noun phrase resolution. The spaces are organized into a
hierarchy, thus similar to our tree representation for
argument structure. Both active and open spaces are
tracked, similar to our tracking candidates eligible to be
relatives to the current proposition.
Because of similarities in the representations and tech-
niques for controlling search for interpretations, it is
worth investigating as future work the precise relation-
ship among coherence, reference resolution and focus
determination for dialogues (some of this is being done
(Grosz and Sidner 1986)).
</bodyText>
<subsectionHeader confidence="0.994776">
5.3 PSYCHOLOGICAL RESEARCH
</subsectionHeader>
<bodyText confidence="0.999972964285714">
Although our model is not designed according to psycho-
logical studies of discourse comprehension, there are
some interesting parallels with existing psychological
research. Labov and Fanshel (1977) investigate thera-
peutic discourse, dialogue between a psychologist and his
patient. The research describes several properties of the
arguments advanced by the patients including: the use of
poor logic, the tendency to omit premises in arguments, a
variety of transmission forms (claims before and after),
and the existence of statements about the structure of the
argument (clues). Since our characterization of input
provides for all these forms, it strengthens our case for
having a robust model.
Geva (1981) investigates the usefulness of flowchart-
ing text structure to assist students in comprehending the
underlying structure. The top-down influence of building
and using a representation is mentioned as important.
The fact that many texts do not follow a strict linear
ordering of connections between statements again
confirms our concern with a variety of possible coherent
transmissions.
In brief, discovering psychological experiments that
agree with the constraints of our model serve to defend
its design. Further, some suggestions we make about
computational measures of discourse processing may
serve to inspire new experiments into the nature of
human processing. So, the relationship with psychology
should be a two-way exchange, and suggests future work.
</bodyText>
<sectionHeader confidence="0.994876" genericHeader="conclusions">
6 USEFULNESS OF THE THEORY
</sectionHeader>
<bodyText confidence="0.999949314814815">
The computational model for the analysis of arguments,
as described in the previous sections, is built on a theory
of argument understanding; it, in turn, can be used as the
basis for an implementation of an argument understand-
ing system. One suggested real-life application area is a
complaint bureau for department stores. Future work
could include a full implementation of the model, and
fine-tuning the design by selecting a particular applica-
tion area for arguments.
Although there is no complete implementation of the
model to date, an overview of a possible design is
presented here, to indicate how the various components
of the model could come together into one integrated
&amp;quot;system&amp;quot;. (Note that an initial implementation of the
model does exist nOw, written in Prolog (Smedley 1986).
But this program merely tests the various reception algo-
rithms described in section 2. The evidence oracle is
replaced by a &amp;quot;query the user&amp;quot; facility. Nonetheless, the
groundwork is in place for a future implementation that
tests the other components of the model).
In Figure 1, there are three main modules: the Propo-
sition Analyzer, Clue Interpreter, and Evidence Oracle.
The Proposition Analyzer takes as input the argument
itself and produces a representation of its underlying
structure. For each proposition of the argument the
Proposition Analyzer attempts to assign it a location in
the representation tree, indicating to which other propo-
sition it relates (provides evidence for or receives
evidence from). The Proposition Analyzer may call on
the Clue Interpreter in the presence of clues, to assist in
the interpretation. In addition, once an eligible relative to
the current proposition is selected, the decision of wheth-
er an evidence relation exists is made by the Evidence
Oracle, which is passed the two propositions and
responds with a yes or no answer. The Evidence Oracle
has available, a knowledge base of shared facts and, if
.possible a model of the speaker. Moreover, if certain
beliefs of the speaker can be extracted during the tests
for unstated premises, the model of the speaker may even
be updated by the Evidence Oracle, to aid in the process-
ing of later propositions.
In the absence of an implementation, the model can
still be defended as a useful prescription of analysis of
arguments. This is accomplished in Cohen (1983) by
hand simulations of a variety of examples, to demon-
strate robustness, and analysis of the complexity of the
processing algorithms, to demonstrate efficiency.
Another argument for the usefulness of the argument
analysis model is that the theories developed for the
model may be applied to the solution of other language
understanding problems. As a result, the study of argu-
ments may be viewed as a worthwhile exercise in the
study of language. Some examples of the wider applica-
bility of the model are:
</bodyText>
<listItem confidence="0.6048445">
• It has been shown that extracting the underlying struc-
ture of discourse is useful to study the complexity of
</listItem>
<page confidence="0.90747">
22 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.9564091">
Robin Cohen Analyzing the Structure of Argumentative Discourse
■■■■•
■•••
PROPOSITION ANALYZER
•••••
••■•■•■ •■•••
argument representation
— argument input
EVIDENCE ORACLE CLUE INTERPRETER — —&gt; data flow
11/ —&gt; control flow
</figure>
<figureCaption confidence="0.9154225">
model of speaker knowledge base
Figure 1. System design.
</figureCaption>
<bodyText confidence="0.968146695652174">
analysis. In the model, the separation of where and
how propositions relate has provided a means of moni-
toring the number of calls to an inference engine, apart
from the more difficult measurement of the process of
actually filling in missing inferences. Hopefully, some
characterization of structures for language problems
other than arguments would be extremely beneficial.
• It has been shown that certain linguistic constructions
serve a function in facilitating the analysis process for
the hearer. Developing common interpretation rules
for various &amp;quot;linguistic clues&amp;quot; should continue for
several language understanding tasks.
• Some insight has been offered into how communication
can proceed despite differing beliefs of speaker and
hearer. The ideas outlined for recognizing beliefs simi-
lar to one&apos;s own, for judging plausible generalizations,
should extend to other problems where reasoning
beyond one&apos;s current beliefs is required.
In addition, very few researchers seem concerned with
truly &amp;quot;low-level&amp;quot; operations, determining not just
&amp;quot;what&apos;s a good representation for discourse&amp;quot; but also
how this representation can be derived, the specification
of some algorithm for processing. It is in this domain that
we feel our research is making a contribution.
For future work, we are currently developing a model
for discourse analysis in general, based on the principles
of this model&apos;s design. A hypothesis worth investigating
from the existing model is that the resulting represen-
tation serves to outline both the linguistic structure of the
discourse and the intentional structure (the speaker&apos;s
intentions behind utterances). The idea is that determin-
ing evidence relations in an argumentative discourse may
best be described as uncovering the intended uses of
utterances (e.g., speaker utters P in order to get hearer to
believe Q), hence reflecting the plan of the speaker. But
this main function of deriving intentional structure must
be performed in conjunction with testing &amp;quot;logical&amp;quot;
connections between propositions, and recognizing
clues, thus isolating linguistic structure (or grouping into
segments). We are interested in specifying a processing
model for discourse understanding that operates at the
level of individual utterances, in the manner of the argu-
ment model, to gain insight into how to derive linguistic
and intentional structure simultaneously. This research is
of significance to the current work of Grosz and Sidner
(1986).
</bodyText>
<sectionHeader confidence="0.99841" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.9998826">
This work was supported in part by the Natural Sciences
and Engineering Research Council of Canada. I am
grateful to the anonymous referees for their valuable
comments and to Ray Perrault for his initial supervision
of this research.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999749597402597">
Archbold, A. 1976 A Study of Some Argument Forms in a Persuasion
Dialogue. ISI internal document, Marina del Rey, California.
Archbold, A. and Hobbs, J. 1980 Notes on &amp;quot;The Analysis of Evalua-
tive Argumentation in Text&amp;quot;: A Report on On-Going Work. SRI
unpublished draft, Menlo Park, California.
Birnbaum, L.; Flowers, M.; and McGuire, R. 1980 Towards an Al
Model of Argumentation. Proceedings of American Association for
Artificial Intelligence (AAAI) Conference: 313-315.
Cohen, P. 1978 On Knowing What to Say: Planning Speech Acts.
Technical Report No. 118, Computer Science Department, Univer-
sity of Toronto, Toronto, Ontario, Canada.
Cohen, R. 1980 Understanding Arguments. Proceedings of Canadian
Society for Computational Studies of Intelligence (CSCS1) Conference:
272-279.
Cohen, R. 1981 Investigation of Processing Strategies for the Struc-
tural Analysis of Arguments. Proceedings of the Association for
Computational Linguistics (ACL) Conference: 71-75.
Cohen, R. 1983 A Computational Model for the Analysis of Argu-
ments. Ph.D. thesis, Technical Report No. 151, Computer Systems
Research Group, University of Toronto, Toronto, Ontario, Canada.
Cohen, R. 1984a A Theory of Discourse Coherence for Argument
Understanding. Proceedings of Canadian Society for Computational
Studies of Intelligence(CSCS1) Conference: 6-10.
Cohen, R. 1984b A Computational Theory of the Function of Clue
Words in Argument Understanding. Proceedings of 10th Interna-
tional Conference on Computational Linguistics (COLING 84):
251-258.
Cohen, R. 1985 The Need for Pragmatics in Natural Language Under-
standing. Proceedings of CSCS1-sponsored Theoretical Advances in
Natural Language Understanding Conference.
Gazdar, G. 1979 Pragmatics: Implicature, Presupposition and Logical
Form. Academic Press, New York, New York.
Geva, E. 1981 Flowcharting Expository Texts and Reading Compre-
hension. Paper presented at Annual Meeting of American Educa-
tional Research Association.
Grosz, B. 1977 The Representation and Use of Focus in Dialogue
Understanding. SRI Technical Note No. 151, Menlo Park, Califor-
nia.
Grosz, B. and Sidner, C. 1986 The Structures of Discourse Structure.
Report No. 6097. Bolt, Beranek and Newman (BBN) Cambridge,
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 23
Robin Cohen Analyzing the Structure of Argumentative Discourse
Massachusetts. (also Report No. CSLI-85-39; Attention, Intentions,
and the Structure of Discourse, Computational Linguistics 12(3):
175-204.
Hobbs, J. 1976 A Computational Approach to Discourse Analysis.
Research Report No. 76-2. City University of New York Depart-
ment of Computer, New York, New York.
Labov, W. and Fanshel, D. 1977 Therapeutic Discourse. Academic
Press, New York, New York.
McCarty, L. and Sridharan, N.S. 1981 A Computational Theory of
Legal Argumentation. Research Report LRP-TR-13, Rutgers
University Lab for Computer Science, New Brunswick, New Jersey.
McGuire, R., Birnbaum, L. and Flowers, M. 1981 Opportunistic Proc-
essing in Arguments. Proceedings of the International Joint Conference
on Artificial Intelligence.bf one,.(1JCA I) Conference: 58-60.
Pollack, M. 1986 A Model of Plan Inference that Distinguishes
Between the Beliefs of Actors and Observers. Proceedings of the
Association for Computational Linguistics (A CL) Conference: 207-214.
Quirk, R. et. al. 1972 A Grammar of Contemporary English. Longmans
Co., London, England.
Reichman, R. 1981 Plain Speaking: A Theory and Grammar of Spon-
taneous Discourse. Report No. 4681, Bolt, Beranek and Newman
(BBN), Cambridge, Massachusetts..
Sadock, J. 1977 Modus Brevis: The Truncated Argument. Papers from
the 13th Regional Meeting, Chicago Linguistics Society: 545-554.
Sidner, C. 1979 Towards a Computational Theory of Definite
Anaphora Comprehension in English Discourse. Al Lab Report
TR-537, Massacusetts Institute of Technology (MIT), Cambridge,
Massachusetts.
Smedley, T. 1986 An Implementation of a Computational Model for
the Analysis of Arguments: An Introduction to the First Attempt.
Research Report CS-86-26, University of Waterloo Department of
Computer Science, Waterloo, Ontario, Canada.
Weiner, J. 1979 BLAH, A System which Explains Its Reasoning. Tech-
nical Report TR 79-14, University of New Hampshire Computer
Science Department, Durham, New Hampshire.
</reference>
<page confidence="0.977012">
24 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.998992">ANALYZING THE STRUCTURE OF ARGUMENTATIVE DISCOURSE</title>
<author confidence="0.999996">Robin Cohen</author>
<affiliation confidence="0.999951">Department of Computer University of Waterloo</affiliation>
<address confidence="0.995909">Waterloo, Ontario, Canada N2L 3G1</address>
<abstract confidence="0.989863566757493">Consider a discourse situation where the speaker tries to convince the hearer of a particular point of view. The first task for the hearer is to understand what it is the speaker wants him to believe — to analyze the structure of the argument being presented, before judging credibility and eventually responding. This paper describes a model for the analysis of arguments that includes: • a theory of expected coherent structure which is used to limit analysis to the reconstruction of particular transmission forms; • a theory of linguistic clues which assigns a functional interpretation to special words and phrases used by the speaker to indicate the structure of the argument; • a theory of evidence relationships which includes the demand for pragmatic analysis to accommodate beliefs not currently held. The implications of this particular design for dialogue analysis in general are thus: • structure is an important feature to extract in a representation to control the processing; • linguistic constructions can be assigned useful interpretations; • pragmatic analysis is crucial in cases where the participants differ in beliefs. 1 THE PROBLEM AREA Consider the task of designing an &amp;quot;argument understanding system&amp;quot;, a natural language understanding system the input is restricted to arguments. Consider as well arguments constructed in a dialogue situation, where a speaker (S) tries to convince a hearer a particular point of view. The hearer patiently listens; hence, the input is &amp;quot;one-way communication&amp;quot;. The argument understanding system therefore plays the role of the hearer, and tries to analyze the structure of the argument being presented. This task is isolated as a necessary first step for a hearer, in order to be a successful participant in a conversation. In other words, the hearer must have some representation of what it is the speaker wants him to believe, before judging credibility and eventually responding. Note that this language problem is relatively new and yet feasible. It is distinct from other NLU endeavors, such as story understanding, which appeal to a stereotype of content in order to reduce processing. In arguments, one is never sure what points the speaker will address; content can&apos;t be stereotyped. However, arguments have a defining characteristic — they are necessarily goal-oriented. The speaker wants to convince the hearer of some overall point. Thus, there is an overall logical structure to the input and this fact may be used by a hearer to control analysis. For our model, the representation for the structure of the argument is restricted to an indication of the claim and evidence relations between the propositions. The notion of evidence is discussed in more detail in section 4. A useful starting definition is: &amp;quot;A proposition E is evidence for a proposition C if there is some rule of inference such that E is premise to C&apos;s conclusion — in other words, there is some logical connection between E and C&amp;quot;. In order to design an argument understanding system, what is then required is a computational model for the analysis of arguments. This in turn necessitates a theory of argument understanding, as a basis for the model. We suggest the following three components for the model: a theory of expected coherent structure, to drive a restricted processing strategy. Analysis is kept to a computationally reasonable task, by limiting the input to be recognized to a characterization of expected coherent forms of transmission. the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that copies are not made for direct commercial advantage and the and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/010011-24$03.00 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 11 Robin Cohen Analyzing the Structure of Argumentative Discourse a theory of linguistic clue interpretation, insight into both the occurrence of clue words and their possible function in overall discourse. Clue words are those words and phrases used by the speaker to directly indicate the structure of the argument to the hearer (e.g. connectives). Detecting clues can thus also serve to constrain the processing for the hearer. Moreover, it is important to have some facility for recognizing and interpreting clue words, to build a model that is robust enough to process a wide variety of input. a theory of evidence relationships. most important observation is that pragmatic analysis is mandatory for an analysis model in order to recognize beliefs of the speaker, not currently held by the hearer. Evidence connections between propositions often appeal to unstated information not currently in the hearer&apos;s set of beliefs, but recognizable as an intended support relation on the part of the speaker. PROCESSING STRATEGY Consider the following framework for the model. An argument is considered to be a set of propositions. The model is then designed to analyze the argument a proposition at a time, incrementally building a representation for the underlying structure. The representation developed is a tree of claim and evidence relations comprising argument, where a claim node is its evidence order to assign an interpretation for a given proposition, one must thus simply assign it a place in the tree. In this way, one can tell to which propositions it serves as evidence and from which other propositions it receives support. A key design decision is to separate the two main of determining for each proposition (i) fits with respect to the argument so far, and (ii) relates to some prior proposition. The question of how two propositions relate in this framework is one of verifying that an evidence relation holds between the propositions. This task is extracted and relegated to an oracle, passed two propositions A and will respond &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot;, as to whether A is evidence With the problem of evidence determination factored, the model must still cope with the question of where a proposition may fit. This is handled by characterizing possible coherent transmissions (ordering of propositions) on the part of the speaker and then limiting to algorithms to recognize these coherent transmissions. In the section below we illustrate possible coherent strategies from the speaker, and present the associated reception algorithm required to recognize the input. Note that the computational model for the analysis of arguments is designed with certain aims and limitations. In particular, the model is to provide for analysis of &amp;quot;spontaneous discourse&amp;quot;, demanding construction of a representation for the argument as each new statement is processed. The following restriction to the processing model is thus applied: the processor does not weigh all possible interpretations for a proposition; if the oracle back a yes answer to the question P evidence for P is attached as a son to Q in the tree. In other words, the model simulates a hearer who does not have the luxury of &amp;quot;looking back&amp;quot; and re-interpreting previous statements. Moreover, the model aims to provide an interpretation for the current proposition, so that once an interpretation has been found (e.g. that P supplies evidence for Q) that proposition has been processed. The evidence relation is also taken to be transitive — i.e., if P evidence for Q and Q is evidence for P is for Section 4 for more detail on the evidence relation). 2.1 PRE-ORDER Pre-order transmissions are those where the speaker consistently presents a claim and then states evidence. The example below illustrates this form: EX1: 1) Jones would make a good president 2) He has lots of experience 3) He&apos;s been on the city board 10 years 4) And he&apos;s very honest 5) He refused bribes while on the force With the tree representation: 4 N. 5 2.2 POST-ORDER Another coherent strategy is post-order, where the speaker consistently presents evidence and then states the claim. Consider the comparable example below: EX2: 1) Jones has been on the board 10 years 2) He has lots of experience 3) And he&apos;s refused bribes 4) So he&apos;s honest 5) He would really make a good president With the representation: \ In this example, the claim is the same as in EX1, that would make a good president, the evidence precedes the claim in each case — i.e., 1 is evidence for 2; 3 is evidence for 4; and 2 and 4 together are evidence for the final claim 5. 12 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 Robin Cohen Analyzing the Structure of Argumentative Discourse The associated reception algorithms for preand postorder are described in detail in Cohen (1981). Both can be shown to operate in linear time (linear in the number nodes of the tree) and so are quite efficient. The pre-order reception essentially finds an interpretation for the current proposition of an argument by searching for a father, up the right border of the tree. The post-order algorithm employs a stack; the current proposition tests to be father to the top of the stack; then all sons are popped off the stack, and the resulting tree pushed on the stack. trace the construction of the trees of EX1 and below, to provide details of the pre-order and post-order reception algorithms. Algorithm: current proposition is proposition immediately prior is LAST. 1) Try NEW as evidence for LAST. If that fails, try evidence for each of LAST&apos;s ancestors, in turn, up to the root of the tree. 3) If the test in 1 succeeds, stop. Consider as well a dummy root (D) for the tree for which all nodes are evidence (to place the first proposition). evidence candidates oracle in order test for NEW response ev. for 1? yes 2,1,D 3 ev. for 2? yes 3,2,1,D 4 ev. for 3? no 4 ev. for 2? no 4 ev. for 1? yes 4,1,D 5 ev. for 4? yes EX2: POST-ORDER Algorithm: - Keep a stack of elements eligible to be evidence for a current proposition, with the latest one as TOP. - To interpret the current proposition NEW: Test if evidence for If then pop TOP (TOP: = TOP make it son of (build a under repeat 1 with NEW value of If no, the tree with NEW as root the new of stack (push NEW onto stack). In essence, all sons for a proposition are picked up at once. evidence stack test for oracle elements evidence response 1 1 ev. for 2? yes tree:1 under 2 2 ev. for 3? no 3, 1 under 2 3 ev. for 4? yes 2 ev. for 4? no 3 under 4, 1 under 2 4 ev. for 5? yes 2 ev. for 5? yes As a first approximation to a general processing strategy, consider designing a reception algorithm to accept hybrid preand post-order arguments (i.e., any given sub-argument may be transmitted in preor post-order). An example of hybrid transmission is EX3 below. EX3: 1) Jones would make a good president 2) He has lots of experience 3) He&apos;s been on the board 10 years 4) And he&apos;s refused bribes 5) So he&apos;s honest With the representation: 1N Here, the first sub-argument is in pre-order, the second one in post-order, and the argument overall is still coherent. The reception algorithm for accepting hybrid transmissions is basically a combination of techniques from the preand post-order receptions. Now, to process a current proposition both a father and possible sons must be searched for. But the search is still restricted — certain propositions get closed off as possible &amp;quot;relatives&amp;quot; to the current one (e.g., earlier brothers of an ancestor). Thus, the complexity of the algorithm is still reasonable; it can also be shown to be linear. Once more, see Cohen (1981) for further examples. A full description of the hybrid algorithm is included below. L is kept as a pointer to the lowest node of the tree still eligible to receive evidence. It is initially set to a dummy root node to which all other nodes succeed as evidence. Consider as well a labelling where the last proposition in the stream that succeeds as evidence for a node is stored as the rightmost son. For each node NEW in the input stream do: /* find father */ do while (NEW is not evidence for L and L is not dummy root) father of end; Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 13 Robin Cohen Analyzing the Structure of Argumentative Discourse /* see if there are any sons to re-attach */ if (rightmost son of L is not evidence for NEW) /* no sons to re-attach */ then do; attach NEW to L set L to NEW end; else do; /* some son will re-attach; attach all sons of L which are evidence for NEW below NEW */ do while (rightmost son of L evidence for NEW) attach rightmost son of L to NEW remove rightmost son of L from L end; NEW to end; Note that the hybrid algorithm, in searching for both sons and father to the current node, must contend with cases where a proposition is attached to a higher ancestor and must be re-attached to its immediate father. This occurs in a framework where the evidence relation is considered transitive. The kinds of orderings that involve this &amp;quot;re-location&amp;quot; are of the form: A, C, B — where C is evidence for B, B evidence for A (hence C also succeeds as evidence for A when tested). For EX3: test for evidence oracle candidates evidence response for D? yes 1, D 2 ev. for 1? yes ev. for 2? yes 3,2,1,D 4 ev. for 3? no for 2? no 4 ev. for 1? yes 4,1,D 5 ev. for 4? no (3&amp;2 (4 is a case of 5, so can&apos;t &amp;quot;closed off&amp;quot;) assume 5 ev. for 4) 5 ev. for 1? yes 4 ev. for 5? yes reattach of sons) 2.4 SUMMARY OF PROCESSING STRATEGY The processing strategy proposed for our model of argument analysis is designed to produce a selective interpretation. The particular restrictions to processing chosen for the model are, moreover, both • useful for measuring the efficiency of the model, since expressed in a framework of an algorithm operating on a tree structure, where complexity of the algorithm can be studied and • well-motivated, since based on a characterization of coherent input. In fact, it is an expression of a theory of argument coherence that serves to produce a model that is both efficient and robust (can handle a wide variety of input). Note that the particular restrictions suggested are drawn from analysis of a body of examples from rhetoric texts, together with some &amp;quot;naturally occurring&amp;quot; arguments (letters to the editor in newspapers). The aim is to characterize coherent transmission forms (ordering of propositions) that can be understood, without additional &amp;quot;clue words&amp;quot; to the underlying structure. We feel that the hybrid model is a good first approximation because examples of various forms of hybrid were encountered, and exceptions to the form all involved additional clue information. This leaves the study of clues and possible transmission forms to the next section. Cohen (1983) a few longer examples are run through the model to illustrate the forms it can accept and to motivate provision for the recognition of a wide variety of argument forms. Note that an actual implementation was not produced. There is now a scaled-down first implementation (Smedley 1986), which will be discussed further in section 6.) CLUES The second main component of the argument understanding model is a theory of linguistic clues. These are the words and phrases often used by the speaker to directly indicate the structure of the argument to the hearer. It is important to specify • what kinds of clues exist • the function of these clues in analysis — i.e., what interpretation can be assigned to a proposition that contains a clue word, and • (a more difficult question) when clues are necessary to ensure comprehension of the argument structure by the hearer. This approach to the study of clue words is much more detailed than the initial suggestions of Hobbs (1976) on to interpret a few connectives such as his framework. It is also distinct from the investigations of Reichman (1981) and (recently) Grosz and Sidner (1986). Grosz and Sidner acknowledge the existence of clues and discuss various discourse structures that can be formed in the presence of clues. Reichman also gives a longer list of clue words and the particular conversational moves they signal. But there is no systematic proposal for interpreting a clue word that may occur (classification), and there is little discussion of how to process some of these more complex discourse structures without clues (suggesting when clues are necessary). In this section we clarify more deeply some of the discussion of clues in</abstract>
<note confidence="0.6947794">Cohen (1983, 1984b). 3.1 CLUES OF RE-DIRECTION One type of clue is expressions that specifically re-direct the hearer to an earlier part of the argument. Consider: Linguistics, Volume 13, Numbers 1-2, January-June 1987</note>
<author confidence="0.587674">Robin Cohen Analyzing the Structure of Argumentative Discourse</author>
<abstract confidence="0.991972379310346">EX4: 1) The city is a mess 2) The parks are a mess 3) The park benches are broken 4) The grassy areas are parched 5) Returning to city problems, the highways are bad too With the representation: 1 N 3 4 the clue in proposition 5, to city problems signals additional support for proposition 1. In the absence of a clue, the specifications for the hybrid algorithm would have 5 test to be a son for 4, 2, and 1 (up the right border of the tree). So, adding clues should reduce the processing effort of the hearer. (In fact, if clues are consistently used by the speaker after long chains of evidence, the processing complexity of the reception algorithm can reduce from linear to real-time). 3.2 CONNECTIVES Another popular type of clue word is the connective. We present a classification or taxonomy of connectives, and then associate a common interpretation rule within the claim and evidence framework for each category of the taxonomy. In this way, the interpretation of any proposition containing a connective can be determined on the basis of the clue word&apos;s classification. For example: EX5: 1) This city is a disaster area 2) Houses have been demolished 3) Trees have been uprooted 4) As a result, we need national aid With the representation: the connective a result proposition 4 belongs to a category known as inference, indicating that there should be some prior proposition that connects to 4 and serves as son to 4 — i.e., supplies evidence for &amp;quot;the result&amp;quot;. In this example, in fact, 1 acts as the son. The interpretation rules are necessarily default suggestions for translating the semantic relations between propositions into our claim and evidence classification. (See further discussion on the evidence relation (section 4) for possible exceptions). The taxonomy and its associated interpretation rules are presented below. Each category is a classification of connectives, made on the basis of semantic meaning of the connective — e.g., the parallel category would include all words that a list, including then, first, secondly, third- The set of classes was produced by considering the categories proposed in Quirk et al. (1972), and merging some classes that had similar semantics and suggested the same interpretation rule for claim and evidence. The inference category covers phrases that suggest one can be inferred from another — e.g., a because of this, The detail category moves in the other direction, and includes connectives that specify — e.g., particular, specifically, The summary is used for phrases that conclude a list. Reformulation captures clue words that repeat an earlier idea — other words, once more, Finally, contrast the phrases that introduce comparisons, like the hand In conjunction with the semantic classes, evidence rules were then assigned as default interpretations. (See Section 4.3 for pragmatic &amp;quot;overrides&amp;quot; to the defaults). Note that some words may fall into more than one category, based on the meaning used — e.g., &apos;next&apos; (in a list of actions) compared with &apos;as a result&apos;. Clue interpretation thus requires a classification process as well. prior proposition; S is the proposition with the clue RELATION: P to parallel brother First, Second inference son As a result detail father In particular summary multiple sons In sum reformulation son (&amp; father) In other words contrast brother or father On the other hand The taxonomy is described in detail in Cohen (1984b). We include discussion of one category here, as example. The is one case where connectives with a range of meanings were merged into one category. The title &amp;quot;detail&amp;quot; suggests that the connective will further specify some prior •proposition. Included are: example, in particular, another interpretation rule assigned to this category is that the proposition with the connective provides evidence for the earlier connecting statement. The motivation is that an accumulation of specific cases leads to a conclusion of a general statement (a form of reasoning used very often in naturally occurring arguments). EX6: 1) The people in this town deserve a city-wide holiday 2) For example, Old Man Jones has worked non-stop since Christmas 3) And Mayor Flood is still recovering from all his efforts for the tornado relief 4) In short, all of us are tired Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 15 Robin Cohen Analyzing the Structure of Argumentative Discourse 1 4 2 is son to 1 (detail class); 3 is also son to 1 (brother to 2) (parallel class); 4 is father to 2 and 3 (summary class). 3.3 THE FUNCTION OF CLUE WORDS So far we have discussed two types of clue words that can occur in conjunction with arguments transmitted according to the specifications of the hybrid algorithm presented in section 2 (our characterization for coherent discourse). We point out that re-direction clues provide additional information concerning which of the eligible propositions is related to the current one, and that connective clues specify the kind of relation that must be found between the current proposition and one of the eligible priors. In certain cases these restrictions will prevent some of the tests for evidence that would otherwise have been conducted, thus saving some processing effort. For example, consider the following: EX7: 1) The city is in serious trouble 2) There are some fires going 3) Three separate blazes have broken out 4) In addition, a tornado is passing through with the representation: 1 2N The clue in 4 prescribes an interpretation for 4 as brother to some prior proposition. This means that 4 must act as evidence for some different proposition. Thus, even though 3 is technically the first proposition to test out when interpreting a new proposition (NEW evidence for the test), in this particular case this option is not possible. Thus, one round of work for the evidence oracle is avoided. In fact, for this example, the test &amp;quot;4 evidence for 2&amp;quot; fails, and the final test of &amp;quot;4 evidence for 1&amp;quot; succeeds. (Note that the brother relation is tested by way of son relation to a (common) father. This is because the model only processes evidence relationships). 3.3.1 THE NECESSITY FOR CLUES We now examine the use of clue words in conjunction with transmissions that violate the specifications of the hybrid base case. The hypothesis is that more complex transmissions can be accommodated by the argument analysis model provided there exist clues to assist the hearer in recognition. In these cases, the clues are there by necessity. Their function in the discourse is not to merely add detail on the interpretation of the contained proposition, but to allow that proposition an interpretation that would otherwise be denied. There is an advantage to adhering to the base case of acceptable argument structures and treating the use of clues with other transmission forms as exceptional. In the first place, this provides a framework for detecting one interpretation for an argument when another possible interpretation could be generated if further testing occurred. In other words, in this model a representation drawn using the rules of the hybrid reception will always be accepted as the intention of the speaker, unless clues specifically override possible tests. To explain, consider the following example: EX8: 1) The park benches are rotting 2) The parks are a mess 3) The highways are run down 4) (And another problem with the parks is that) the grass is dying 5) This city is in sad shape Without the clue phrase in 4, re-directing to proposition 2, to add more evidence out of turn, a coherent representation could be built just the same, as below: If the speaker intends 4 to add detail to the parks problem, he cannot expect the hearer to make this connection without more information, simply because a more effortless interpretation of 4 is possible (and the speaker should realize that it is this representation that the hearer will draw). Another important reason for separating the base case of acceptable transmissions is to allow for input that can be characterized as somehow a coherently generated plan of the speaker. Recall that the proposition analyzer will continuously call on an evidence oracle to determine if some proposition A acts as evidence for some other proposition B. Suppose there were no restrictions in the testing of evidence relations. Then, tests for evidence that would be interpreted as positive would return this response, regardless of when asked. In other words, a totally random display of propositions would result in the same representation for the argument as the reception of a coherently ordered presentation. Consider the following example: EX9: 1) Yogi&apos;s been a shrewd manager 2) He hired a hitter who now bats .400 3) He traded in a pitcher who is now 0 and 12 4) But he got involved in drug deals to the players 5) And that&apos;s inexcusable for a manager 6) He really needs to be axed 16 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 Robin Cohen Analyzing the Structure of Argumentative Discourse with the representation: 6 5 (where 5 and 1 are contrasting evidence to 6). EX9B: The argument as above, presented in the order:: 2,5,3,6,4,1 1) Yogi hired a .400 hitter 2) (And) that&apos;s inexcusable for a manager 3) He traded in a pitcher who is now 0 and 12 4) Yogi really needs to be axed 5) He got involved in drug deals to the players 6) He is a shrewd manager This argument now appears incoherent. One reason is that there is contrast overall that must be clearly separated. In any case, the ordering does not conform to the specifications of the hybrid and as such is a candidate for an unacceptable transmission. This example suggests that the use of clue words, though helpful to signal exceptional transmissions, should still be studied as a systematic process of the speaker to the hearer in comprehension. In any number of clue words be added to still make this recognizable? Consider the following attempted repair to EX9B: EX9C: 1) Yogi hired a .400 hitter 2) But he&apos;s done some things inexcusable for a manager 3) Although he also did other smart things like trading a pitcher who is now 0 and 12 4) No, Yogi really has to be axed 5) He got involved in drug deals to the players 6) Though he still is a shrewd manager The question is whether an argument of this form would still be judged coherent. Our preference will be to specify particular types of exceptional transmissions that may be judged coherent. To this end, we have tried to isolate a few specific cases where clues can be used in conjunction with coherent orderings beyond the specifications of the hybrid algorithm. These are highlighted in the next subsection below. One more point about the last example above is that it emphasizes our concern that the analysis of arguments be done efficiently. In this case, we want to avoid making tests for evidence relations that could not exist between certain propositions as part of a coherent input. In other words, we don&apos;t want the model to simply test out all possible combinations of evidence relations (even though this would only be n*n vs. k*n number of operations), because an input that is not coherent would then be accepted. We would also not want to derive computationally a representation for an argument that involved more computational effort, if a simpler interpretation of the same argument could be derived. (Again, the speaker is to assume that the hearer will not be searching unnecessarily). This is illustrated in EX8 above. 3.3.2 CASES OF NECESSITY Three kinds of acceptable extended transmission strateare studied in Cohen (1983): evidence, multievidence proposition acting as evidence for several in restricted conditions) and sub-arevidence both preceding and following a claim). We present an example of the parallel construction below. See Cohen (1984b, 1983) for further examples. EX10: 1) The city has problems 2) The parks are a mess 3) The highways are a mess 4) The buildings are a mess 5) Here&apos;s some evidence for the fact that the parks are a problem: the benches are broken 6) As for the highways, they&apos;re full of potholes With the representation: 4 Here, the argument breaks the rules of hybrid transmission by adding evidence for an earlier brother (proposition 2); however, this shift is signalled with a phrase of intention in proposition 5, and the hearer may then expect a parallel expansion of additional support for each of the earlier brothers, in turn. (Note that without the clue, the argument structure derived would simply record all of 2 to 6 as evidence for 1, in the same vein as EX8). This example illustrates another interesting feature of clues — the variety of possible surface forms that can signal the same evidence relation between propositions. EX10, the clue in 5 could also have been problem the parks is... / now explain why the parks are a problem — .... range of explicitness is thus possible. In cases other than this parallel construction, in fact, a signal to a relation between propositions may be advocated by the simple use of an anaphor. For example: EX11: 1) The mayor hasn&apos;edone much for this city 2) He doesn&apos;t seem to want to do much 3) That man is a complete loser the phrase man a link to mayor. is difficult to decide whether the phrase qualifies as a clue word. The problem is determining a &amp;quot;bottom line&amp;quot; — i.e., &amp;quot;can&apos;t every sentence be seen to have some clue, from semantics, to its interpretation within the argument?&amp;quot;. For now, we do not consider the cases of anaphora as above. Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 17 Robin Cohen Analyzing the Structure of Argumentative Discourse 3.4 SUMMARY AND FUTURE DIRECTIONS OF CLUE THEORY Our theory of clue interpretation so far has outlined the following principles: • Clues may occur with expected coherent transmissions or to signal exceptional cases. • Connective clues can be assigned common interpretation rules according to the semantics of the clue. • To distinguish helpful versus necessary clues, the preference will always be to recognize the hybrid transmission; if clue rules or semantics force an exceptional reading, only certain exceptional structures should be accepted. • In all cases, a reading that can be derived with less computational effort will always be taken as the intended reading. • The cases where clues are necessary to force a certain interpretation provide insight into the function of clues; their use in conjunction with acceptable transmissions suggests a function of additional processing reductions. In order to recognize clues and incorporate their interpretation into a larger model that derives argument structure, we propose a separate clue interpretation module, interacting with the basic reception algorithm and the calls to the evidence oracle. Exactly how this module would function is left as future work. We do have a few initial insights, to suggest that clue interpretation is not only quite useful (argued previously in this section) but feasible. For connectives, the clue can be recognized from a classification. Then, determining whether a proposition available from the list of eligibles is in fact related can be tested, according to the restrictions of the interpretation rule (as suggested in EX7). Further, the required semantic relation to the prior proposition can be passed as additional information to the oracle. The problem is that this oracle must do some kind of search for connections between facts and axioms of a knowledge base. How this semantic analysis is achieved depends on the underlying representation, but additional semantic constraints should help to restrict operations. For re-direction clues, a processor would first have to recognize the appropriate phrase used. Some standard list to, on the topic of) be specified as a start. Then, the phrase should suggest some particular content to the prior proposition (e.g., the parks central). Now it is the form of representation of the propositions which may influence what is acceptable on a list of eligibles. If this semantic processing can be done very broadly, some calls to the oracle may be avoided, and this would be an improvement, assuming the oracle&apos;s operations encompass a more extensive search. An initial definition for evidence offered in Section 1 is: &amp;quot;A proposition P is evidence for a proposition Q if there is some logical connection from P to Q — i.e., some rule of inference such that P is premise to Q&apos;s conclusion&amp;quot;. The main problem in establishing evidence relations is that not all the premises are stated. For example, one common rule of inference used in arguments is Modus Ponens, of the form: P Q, P therefore Q. way this rule is most often used, the speaker will simply state P and Q and leave out the major premise &amp;quot;P Q&amp;quot;, expecting the hearer to be able to fill in the unstated connection to recognize the evidence relation from P to Q. Omitting certain premises is referred to as Modus Brevis and studied in Sadock (1977). list below the rule of inference for our model. Each rule has a slot for major premise, minor premise, and conclusion, to be filled by stated or unstated propositions, in recognizing an evidence relationship.</abstract>
<title confidence="0.792301142857143">RULE OF INFERENCE FRAMES: Major Minor Conclusion Modus Ponens P Q P Tollens P Q —Q Modus Tollendo Ponens P v —Q Q Ponendo Tollens PvQ Q The form most often used is Modus Ponens. When the</title>
<abstract confidence="0.9465162">major premise is missing, this is the rule of inference to consider as the intended link from P to Q. EX12: 1) The Jays had a fantastic team this year All players had averages over .250 fill: If a team has with averages over .250, then that team is fantastic The common form for arguments, then, is one where the hearer must supply missing statements in order to establish the connections for the representation of the argument. There are several possible Modus Brevis forms for each frame above. The possible missing parts are classified below for the case of Modus Ponens.</abstract>
<title confidence="0.918204">MODUS BREVIS FORMS (MODUS PONENS): Given Premises Conclusion Normal P Missing Minor</title>
<degree confidence="0.774174">Major (most popular form) Major P-4-() rest) Minor rest</degree>
<abstract confidence="0.991235039848197">4.1. OVERVIEW OF ORACLE&apos;S PROCESSING DETERMINATION The third main component of the argument analysis model is a theory of evidence, to govern the verification of evidence relations between propositions. It is important to demonstrate that the part of processing relegated to the evidence oracle within the overall model is not insurmountable, to defend the model as useful. In section we examine more closely the operation the Linguistics, Volume 13, Numbers 1-2, January-June 1987 Robin Cohen Analyzing the Structure of Argumentative Discourse oracle, opening up the &amp;quot;black box&amp;quot; just enough to suggest how it operates. In general, the oracle is asked to test two propositions to be in an evidence relationship, responding &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot; to a question of the form: &amp;quot;is P evidence for Q?&amp;quot;. We sketch the operation of the oracle for the example below: EX13: 1) Joey is a dangerous 2) He is a shark Assuming some resolution of anaphora, etc., a crude representation of the example in terms of predicates and arguments is: P: Shark(j); Q: Dangerous(j) (j: Joey). A Modus Ponens template would be of the form: S(j), for-all (x) ( S(x) D(x) ), therefore D(j) D:is-dangerous). (Note that we are not addressing questions here such as the significance that a definitional, while dangerous really assertional). Recall that the argument analysis model seeks to recognize intended argument structures. So, in this example the hearer can at least recognize that &amp;quot;P is evidence Q&amp;quot; would follow through if sharks are dangerous were believed (i.e., for-all(x) (S(x) D(x) It is thus proposed that the oracle: 1. identify missing premises (the Modus Brevis form of the argument being presented); 2. verify plausibility of these missing premises (that they could be intended by the speaker to be believed by the hearer); and 3. conclude that an evidence relation holds if the missing premises are plausible. For step 2, we try to specify more precisely in the remainder of this section the kind of tests the hearer can apply. A summary is provided below: a) Identify the missing premise within a knowledge base of shared knowledge. b) Identify a &amp;quot;relaxed version&amp;quot; of the missing premise within own private knowledge. c) Identify the missing premise within a model of the speaker&apos;s beliefs. d) Judge the beliefs of a hypothetical third party (which could be simplified if the bottom line is &amp;quot;believe it, unless there&apos;s reason to strongly doubt, from within one&apos;s own beliefs&amp;quot;). We have found in simulations of the model on a variety of examples that most of the tests for evidence can be answered through (a) and (b). We hypothesize that, given a specification of a knowledge base, the search for connections between propositions can be controlled. This hypothesis would be best verified with an implementation of the oracle and extensive analysis of examples, and could be the focus of the next stage of our implementation (beyond Smedley (1986)). (The two large examples dissected in Cohen (1983) do have this property.) In addition to the problem that the major premise may be unstated is the problem that this premise should really be tempered by the beliefs of the speaker. In other words, the missing major premise that the hearer must fill in is really of the form: H believes that S wants H to believe (P Q). In other words, this premise is necessarily one of the hearer&apos;s beliefs. It is important to emphasize the importance of pragmatic processing in establishing evidence relations. The tree of claim and evidence relations built as a representation for the argument is really an indication of the plan of the speaker, in the sense that each evidence relation recorded is the hearer&apos;s conception of a support connection intended by the speaker. Note that it is difficult to specify how a plan of a speaker is determined during analysis. What we are advocating is to interpret the intention of each proposition of the argument, the other propositions for which it provides evidence. The result of processing the entire discourse is not a complete plan of the speaker, in the sense that each of the &amp;quot;steps&amp;quot; could be executed and the top level goal (convince the hearer of some overall point) would then follow. It is more an indication of the motivation behind each utterance towards the ultimate goal of convincing the hearer. The difficulties in plan inference for discourse are discussed in more detail in Grosz and Sidner (1986), and are in fact a topic of our current concern (see discussion of future work in Section 6). There is in fact a whole spectrum of problems the hearer must face in recognizing evidence relationships between propositions. The four main tests for the hearer be described • use logic, • relax the logic, • stereotype the speaker, • judge plausibility (reason as a &amp;quot;hypothetical person&amp;quot;). We illustrate these possible operations with examples below. 4.2 LOGIC AND RELAXED LOGIC In example EX14, all the premises of the Modus Ponens argument are present. The hearer should realize that 1 and 2 are evidence for the claim in 3. Then EX15 illustrates the more typical case of missing major premise. If hearer fills in machine candidates win, connection from 1 to 2 can be seen. The problem is that the speaker probably believes something more along the of: machine candidates win. yet, one couldn&apos;t record a Modus Ponens relation in the argument a quantifier such as of all. the hearer must use some relaxation to the rules of logic to recognize the evidence relation. The detection of evidence through &amp;quot;relaxed logic&amp;quot; can be accomplished by having the hearer judge the unstated connection as a plausible generalization, based on a few known cases. For example, if the hearer tries to recognize an evidence from 1 to 2 in by filling in Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 19 Robin Cohen Analyzing the Structure of Argumentative Discourse are dangerous, the hearer doesn&apos;t believe this &amp;quot;axiom&amp;quot; but knows of a few sharks that are dangerous, he may reason that the missing major premise is reasonable. EX14: 1) Aristotle is a man 2) All men are mortal 3) So Aristotle is mortal EX15: 1) Bilandic will win 2) He&apos;s the machine candidate EX16: I) Joey is a shark 2) So, he is dangerous The idea of recognizing an intended connection from some other conversant based on one&apos;s own beliefs is not necessarily simple to implement. There has been some recent work by Pollack (1986) that suggests a more concrete foundation for this operation. Pollack discusses the problem of inferring a questioner Q&apos;s plan from his discourse. A first process has the responder R ascribing to Q a belief about some connection (&amp;quot;conditional generation relation&amp;quot;) that she herself believes true. Occasionally, R will need to recognize a connection that is not one of her beliefs. Then Pollack suggests there is a rule where &amp;quot;R ascribes to Q a belief about a relation between act-types that is a slight variation of one she herself has&amp;quot;. In particular, one slight difference possible has Q believing a stronger conditional generation relation, which is missing one of the required conditions. This related research is quoted here, not to argue that this problem is solved, but to acknowledge that it is important to specify this &amp;quot;relaxed&amp;quot; connection between one&apos;s own beliefs and those attributed to another. Some groundwork is being laid with more formal descriptions of plans such as Pollack&apos;s. 4.3 DIFFERENCE IN BELIEFS The other type of problem faced by a hearer in recognizing evidence relations arises because of difference in beliefs between speaker and hearer. As mentioned, the hearer is actually discerning intended relations on the part of the speaker, and must often reason outside his personal framework of knowledge. Again, an issue is raised of how to discern intentions from discourse. Some work has been done at the level of one utterance (e.g., Allen 1979). We are mostly concerned with advocating the inclusion of reasoning beyond one&apos;s own beliefs, without a full theory of how to infer another person&apos;s beliefs. Instead, we advocate a simplified framework, discerning evidence relations and allowing a connection to be drawn as intended if it is plausible to the hearer. For future work, we are studying how to specify this process more precisely. (See also Grosz and Sidner 1986). The point is that the hearer is still able to recognize connections he does not believe. In EX17, the hearer should be able to understand an evidence relation from 1 to 2, upon filling in a missing premise of the form &amp;quot;If a person stands for apple pie and Mom then he is great&amp;quot;. If the hearer does not believe this statement himself, he may still consider it to be a reasonable belief of the speaker; having stereotyped knowledge of the speaker&apos;s views may thus be useful. EX17: 1) Reagan is great 2) He stands for apple pie and Mom Finally, if the hearer is testing a possible evidence relation between two propositions, does not believe the missing premise, and has no prior knowledge of the speaker, the best option available is to try to judge the plausibility of the unstated information. Essentially, the hearer must postulate new facts (which he may not be sure he wants to also believe) and consider relationships between these facts as plausible or not. It is in this sense that he adopts a &amp;quot;hypothetical person&apos;s&amp;quot; beliefs. Note that it is often the case that one will accept new facts unless something from one&apos;s own beliefs suggests a contradiction. In this sense, the judgement of plausibility does relate back to the hearer&apos;s own beliefs. An example with an implausible missing connection is EX18 below. If the hearer tests 2 as possible evidence for 1, a major premise of the form &amp;quot;All sharks like tap dancing&amp;quot; would establish the relation. But the hearer would not regard this as a plausible belief of the speaker, and so would fail to recognize an evidence relation between the two propositions. EX18: 1) Joey likes to tap dance 2) He is a shark The problem of judging plausibility is difficult. To make this process more computationally tractable, one suggestion is to incorporate into the model some tracking of mutual belief between speaker and hearer. (See Cohen (1978) for further discussion on the use of mutual belief in natural language processing.) Then, certain tests for evidence relations can be blocked in the oracle, based on mutual belief. For instance, rules can be postulated regarding the use of claims and evidence that are mutually believed. Two sample rules are: (i) &amp;quot;If P is mutually believed, it can&apos;t be used claim&amp;quot;. (ii) &amp;quot;If —P is mutually believed, P can&apos;t be used as evidence&amp;quot;. In addition, some of the default interpretation rules associated with the taxonomy of linguistic clues can be overruled by pragmatic considerations. The idea is to possibly override the default semantic interpretations of evidence relations otherwise specified, by testing whether Computational Volume 13, Numbers 1-2, January-June 1987 Robin Cohen Analyzing the Structure of Argumentative Discourse propositions are already mutually believed. Note that the idea of a &amp;quot;pragmatic override&amp;quot; is also employed in Gazdar (1979) for the problem of determining presuppositions. The importance of pragmatic processing for argument analysis is once more emphasized, as it is a critical component to the difficult procedure of judging plausibility. While considering mutual belief will help to eliminate some potentially difficult tests for the oracle, the model would require a more detailed specification of the maintenance and use of mutual belief. This is left as a topic for future work. Some current ideas are explored in more detail in Cohen (1985). 4.4 SUMMARY OF EVIDENCE THEORY The &amp;quot;theory&amp;quot; of evidence relationships, developed to specify the operation of the evidence oracle component of the argument analysis model, really presents insight into the problems relevant to evidence relations, rather than offering solutions. Still, the fundamental question of how connections between propositions can be verified has not been studied to any extent by other researchers. It is extremely worthwhile to acknowledge that it is not sufficient to indicate what relations do occur, without also suggesting how these relations could be established during analysis. In addition, we have provided some insight into the more general question of how to accommodate a possible difference in beliefs between conversants in a natural language dialogue processing system. We also suggest that a less sophisticated oracle can be constructed that merely searches known facts and axioms, possibly including relaxations, to handle a large amount of naturally occurring arguments. 5 RELATED WORK 5.1 ARGUMENT UNDERSTANDING Other researchers in natural language have studied arguments, in particular. However, the focus of the research in each case has been different. Birnbaum and the group at Yale (Birnbaum et al. 1980, McGuire et al. 1981) study two-way communication, developing an argument graph to display the points raised by both conversants. This graph is then used to determine the best moves on the part of an adversary, to challenge the position of the other conversant. Thus, the question of what responses to generate is investigated. On the other hand, there is little insight into how a hearer can detect the points being raised by the speaker, to construct this argument graph. Our focus has thus been on this preliminary problem to argument understanding. Archbold (Archbold 1976, Archbold and Hobbs 1980) is most concerned with evaluative arguments, those with strong underlying ideologies. For example, Lenin&apos;s speeches are appropriate sample input. Thus, the difficult question of recognizing differing opinions is a focus to Archbold&apos;s investigations. In addition, he studies text rather than discourse, allowing for a deeper review (re-reading) of the input in order to derive an analysis. Weiner (1979) describes a representation for arguments that is also a tree structure, with a variety of links possible. His main concern is to characterize types of argument structures, for use in the generation of explanations. There is thus little attention on the problems encountered in deriving argument structures during analysis. Weiner&apos;s (1979) model for the structure of explanation bears some resemblance to the representation described for arguments here. Weiner claims that natural explanation can be regarded as a series of transformations of an underlying tree structure that represents the abstract form of the argument being developed. The ways in which support can be supplied are listed more extensively, including examples, alternatives, etc. How the tree can be built up relies on tracking a node that is &amp;quot;in focus&amp;quot;. The •fact that determining the relations between statements may make use of clue words is mentioned briefly as well. Basically, some of the features we advocate appear in this research. But we are trying to provide more insight into operational questions such as: • How do you determine the (best) relation between propositions? • How is the focus set? and • When are clue words likely to occur? By contrast, Weiner concentrates on how to generate explanations using his precisely specified characterization. Reichman (1981) is concerned with a larger problem of producing a model of discourse (not just arguments), but her approach should handle arguments as well. The core of the model is an ATN grammar for parsing and generation, coupled with a representation of &amp;quot;context spaces&amp;quot; containing conversational moves. The conversational moves provide a classification of larger components of discourse (not just single propositions). For example, there is an extensive study of a &amp;quot;challenge&amp;quot; operation. Since Reichman&apos;s aims are broader than ours, the lower level issues we address of verifying evidence and studying the necessity for clue words are not considered for the model. Moreover, there is an intentional lack of concern with pragmatic processing, another crucial feature our model. Instead, Reichman presents a model for the analysis of a variety of two-person interactions. In sum, our efforts in argument understanding are worthwhile because we focus on the necessary first step in argument analysis — determining the intended structure, or &amp;quot;what the argument is about&amp;quot;. We study the possible structural relations between propositions, and investigate the difficult issue of verifying evidence relationships. The importance of pragmatic analysis to recognize whole classes of arguments that involve differing beliefs is stressed in our work. And finally, the use and interpretation of clue words is addressed. It is worth Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 21 Robin Cohen Analyzing the Structure of Argumentative Discourse noting that the differences in our existing studies can possibly be exploited by pooling efforts and suggesting a powerful general model for argument analysis. 5.2 REFERENCE RESOLUTION AND FOCUS Some of the work on using focus for reference resolution contains similarities to the model presented here for analyzing argument structure. Sidner (1979) maintains a focus stack of possible items in focus and an alternate focus list to support shifts of focus. The candidates for resolving references are thus restricted and ordered. The point is that these restrictions are drawn from a characterization of coherent discourse, the same approach taken for our control of processing. Grosz (1977) presents a model of focus spaces which may be used for several purposes, including the resolution of definite noun phrase resolution. The spaces are organized into a hierarchy, thus similar to our tree representation for argument structure. Both active and open spaces are tracked, similar to our tracking candidates eligible to be relatives to the current proposition. Because of similarities in the representations and techniques for controlling search for interpretations, it is worth investigating as future work the precise relationship among coherence, reference resolution and focus determination for dialogues (some of this is being done (Grosz and Sidner 1986)). 5.3 PSYCHOLOGICAL RESEARCH Although our model is not designed according to psychological studies of discourse comprehension, there are some interesting parallels with existing psychological research. Labov and Fanshel (1977) investigate therapeutic discourse, dialogue between a psychologist and his patient. The research describes several properties of the arguments advanced by the patients including: the use of poor logic, the tendency to omit premises in arguments, a variety of transmission forms (claims before and after), and the existence of statements about the structure of the argument (clues). Since our characterization of input provides for all these forms, it strengthens our case for having a robust model. Geva (1981) investigates the usefulness of flowcharting text structure to assist students in comprehending the underlying structure. The top-down influence of building and using a representation is mentioned as important. The fact that many texts do not follow a strict linear ordering of connections between statements again confirms our concern with a variety of possible coherent transmissions. In brief, discovering psychological experiments that agree with the constraints of our model serve to defend its design. Further, some suggestions we make about computational measures of discourse processing may serve to inspire new experiments into the nature of human processing. So, the relationship with psychology should be a two-way exchange, and suggests future work. OF THE THEORY The computational model for the analysis of arguments, as described in the previous sections, is built on a theory of argument understanding; it, in turn, can be used as the basis for an implementation of an argument understanding system. One suggested real-life application area is a complaint bureau for department stores. Future work could include a full implementation of the model, and fine-tuning the design by selecting a particular application area for arguments. Although there is no complete implementation of the model to date, an overview of a possible design is presented here, to indicate how the various components of the model could come together into one integrated &amp;quot;system&amp;quot;. (Note that an initial implementation of the model does exist nOw, written in Prolog (Smedley 1986). But this program merely tests the various reception algorithms described in section 2. The evidence oracle is replaced by a &amp;quot;query the user&amp;quot; facility. Nonetheless, the groundwork is in place for a future implementation that tests the other components of the model). In Figure 1, there are three main modules: the Proposition Analyzer, Clue Interpreter, and Evidence Oracle. The Proposition Analyzer takes as input the argument itself and produces a representation of its underlying structure. For each proposition of the argument the Proposition Analyzer attempts to assign it a location in the representation tree, indicating to which other proposition it relates (provides evidence for or receives evidence from). The Proposition Analyzer may call on the Clue Interpreter in the presence of clues, to assist in the interpretation. In addition, once an eligible relative to the current proposition is selected, the decision of whether an evidence relation exists is made by the Evidence Oracle, which is passed the two propositions and responds with a yes or no answer. The Evidence Oracle has available, a knowledge base of shared facts and, if .possible a model of the speaker. Moreover, if certain beliefs of the speaker can be extracted during the tests for unstated premises, the model of the speaker may even be updated by the Evidence Oracle, to aid in the processing of later propositions. In the absence of an implementation, the model can still be defended as a useful prescription of analysis of arguments. This is accomplished in Cohen (1983) by hand simulations of a variety of examples, to demonstrate robustness, and analysis of the complexity of the processing algorithms, to demonstrate efficiency. Another argument for the usefulness of the argument analysis model is that the theories developed for the model may be applied to the solution of other language understanding problems. As a result, the study of arguments may be viewed as a worthwhile exercise in the study of language. Some examples of the wider applicability of the model are: • It has been shown that extracting the underlying strucof discourse is useful to study the complexity 22 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 Robin Cohen Analyzing the Structure of Argumentative Discourse ■■■■• ■••• PROPOSITION ANALYZER ••••• ••■•■•■ •■••• argument representation — argument input EVIDENCE ORACLE CLUE INTERPRETER — —&gt; data flow —&gt; control flow 11/ model of speaker knowledge base Figure 1. System design. analysis. In the model, the separation of where and how propositions relate has provided a means of monitoring the number of calls to an inference engine, apart from the more difficult measurement of the process of actually filling in missing inferences. Hopefully, some characterization of structures for language problems other than arguments would be extremely beneficial. • It has been shown that certain linguistic constructions serve a function in facilitating the analysis process for the hearer. Developing common interpretation rules for various &amp;quot;linguistic clues&amp;quot; should continue for several language understanding tasks. • Some insight has been offered into how communication can proceed despite differing beliefs of speaker and hearer. The ideas outlined for recognizing beliefs similar to one&apos;s own, for judging plausible generalizations, should extend to other problems where reasoning beyond one&apos;s current beliefs is required. In addition, very few researchers seem concerned with truly &amp;quot;low-level&amp;quot; operations, determining not just &amp;quot;what&apos;s a good representation for discourse&amp;quot; but also how this representation can be derived, the specification of some algorithm for processing. It is in this domain that we feel our research is making a contribution. For future work, we are currently developing a model for discourse analysis in general, based on the principles of this model&apos;s design. A hypothesis worth investigating from the existing model is that the resulting representation serves to outline both the linguistic structure of the discourse and the intentional structure (the speaker&apos;s intentions behind utterances). The idea is that determining evidence relations in an argumentative discourse may best be described as uncovering the intended uses of utterances (e.g., speaker utters P in order to get hearer to believe Q), hence reflecting the plan of the speaker. But this main function of deriving intentional structure must be performed in conjunction with testing &amp;quot;logical&amp;quot; connections between propositions, and recognizing clues, thus isolating linguistic structure (or grouping into segments). We are interested in specifying a processing model for discourse understanding that operates at the level of individual utterances, in the manner of the argument model, to gain insight into how to derive linguistic and intentional structure simultaneously. This research is of significance to the current work of Grosz and Sidner (1986). ACKNOWLEDGMENTS This work was supported in part by the Natural Sciences and Engineering Research Council of Canada. I am grateful to the anonymous referees for their valuable comments and to Ray Perrault for his initial supervision of this research.</abstract>
<note confidence="0.96659468115942">REFERENCES Archbold, A. 1976 A Study of Some Argument Forms in a Persuasion Dialogue. ISI internal document, Marina del Rey, California. Archbold, A. and Hobbs, J. 1980 Notes on &amp;quot;The Analysis of Evalua- Argumentation in Text&amp;quot;: A Report on On-Going Work. unpublished draft, Menlo Park, California. L.; Flowers, M.; and McGuire, R. 1980 Towards an of Argumentation. of American Association for Intelligence Cohen, P. 1978 On Knowing What to Say: Planning Speech Acts. Technical Report No. 118, Computer Science Department, University of Toronto, Toronto, Ontario, Canada. R. 1980 Understanding Arguments. of Canadian Society for Computational Studies of Intelligence (CSCS1) Conference: 272-279. Cohen, R. 1981 Investigation of Processing Strategies for the Struc- Analysis of Arguments. of the Association for Linguistics Cohen, R. 1983 A Computational Model for the Analysis of Arguments. Ph.D. thesis, Technical Report No. 151, Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada. Cohen, R. 1984a A Theory of Discourse Coherence for Argument of Canadian Society for Computational of Intelligence(CSCS1) Conference: Cohen, R. 1984b A Computational Theory of the Function of Clue in Argument Understanding. of 10th Interna- Conference on Computational Linguistics 84): 251-258. Cohen, R. 1985 The Need for Pragmatics in Natural Language Underof CSCS1-sponsored Theoretical Advances in Natural Language Understanding Conference. G. 1979 Implicature, Presupposition and Logical Press, New York, New York. Geva, E. 1981 Flowcharting Expository Texts and Reading Comprehension. Paper presented at Annual Meeting of American Educational Research Association. Grosz, B. 1977 The Representation and Use of Focus in Dialogue Note No. 151, Menlo Park, California. Grosz, B. and Sidner, C. 1986 The Structures of Discourse Structure. No. 6097. Bolt, Beranek and Newman Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 23 Robin Cohen Analyzing the Structure of Argumentative Discourse Massachusetts. (also Report No. CSLI-85-39; Attention, Intentions, the Structure of Discourse, Linguistics 175-204. Hobbs, J. 1976 A Computational Approach to Discourse Analysis. Research Report No. 76-2. City University of New York Department of Computer, New York, New York. W. and Fanshel, D. 1977 Discourse. Press, New York, New York. McCarty, L. and Sridharan, N.S. 1981 A Computational Theory of Legal Argumentation. Research Report LRP-TR-13, Rutgers University Lab for Computer Science, New Brunswick, New Jersey. McGuire, R., Birnbaum, L. and Flowers, M. 1981 Opportunistic Procin Arguments. of the International Joint Conference Artificial Intelligence.bf one,.(1JCA I) 58-60. Pollack, M. 1986 A Model of Plan Inference that the Beliefs of Actors and Observers. of the for Computational Linguistics CL) R. et. al. 1972 Grammar of Contemporary English. Co., London, England. Reichman, R. 1981 Plain Speaking: A Theory and Grammar of Spontaneous Discourse. Report No. 4681, Bolt, Beranek and Newman (BBN), Cambridge, Massachusetts.. J. 1977 Modus Brevis: The Truncated Argument. from 13th Regional Meeting, Chicago Linguistics Society: Sidner, C. 1979 Towards a Computational Theory of Anaphora Comprehension in English Discourse. Al Lab Report</note>
<affiliation confidence="0.994382">Massacusetts Institute of Technology</affiliation>
<address confidence="0.609112">Massachusetts. Smedley, T. 1986 An Implementation of a Computational Model for</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Archbold</author>
</authors>
<title>A Study of Some Argument Forms in a Persuasion Dialogue. ISI internal document,</title>
<date>1976</date>
<location>Marina del Rey, California.</location>
<contexts>
<context position="49796" citStr="Archbold 1976" startWordPosition="8401" endWordPosition="8402"> group at Yale (Birnbaum et al. 1980, McGuire et al. 1981) study two-way communication, developing an argument graph to display the points raised by both conversants. This graph is then used to determine the best moves on the part of an adversary, to challenge the position of the other conversant. Thus, the question of what responses to generate is investigated. On the other hand, there is little insight into how a hearer can detect the points being raised by the speaker, to construct this argument graph. Our focus has thus been on this preliminary problem to argument understanding. Archbold (Archbold 1976, Archbold and Hobbs 1980) is most concerned with evaluative arguments, those with strong underlying ideologies. For example, Lenin&apos;s speeches are appropriate sample input. Thus, the difficult question of recognizing differing opinions is a focus to Archbold&apos;s investigations. In addition, he studies text rather than discourse, allowing for a deeper review (re-reading) of the input in order to derive an analysis. Weiner (1979) describes a representation for arguments that is also a tree structure, with a variety of links possible. His main concern is to characterize types of argument structures</context>
</contexts>
<marker>Archbold, 1976</marker>
<rawString>Archbold, A. 1976 A Study of Some Argument Forms in a Persuasion Dialogue. ISI internal document, Marina del Rey, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Archbold</author>
<author>J Hobbs</author>
</authors>
<title>Notes on &amp;quot;The Analysis of Evaluative Argumentation in Text&amp;quot;: A Report on On-Going Work. SRI unpublished draft,</title>
<date>1980</date>
<location>Menlo Park, California.</location>
<contexts>
<context position="49822" citStr="Archbold and Hobbs 1980" startWordPosition="8403" endWordPosition="8406">(Birnbaum et al. 1980, McGuire et al. 1981) study two-way communication, developing an argument graph to display the points raised by both conversants. This graph is then used to determine the best moves on the part of an adversary, to challenge the position of the other conversant. Thus, the question of what responses to generate is investigated. On the other hand, there is little insight into how a hearer can detect the points being raised by the speaker, to construct this argument graph. Our focus has thus been on this preliminary problem to argument understanding. Archbold (Archbold 1976, Archbold and Hobbs 1980) is most concerned with evaluative arguments, those with strong underlying ideologies. For example, Lenin&apos;s speeches are appropriate sample input. Thus, the difficult question of recognizing differing opinions is a focus to Archbold&apos;s investigations. In addition, he studies text rather than discourse, allowing for a deeper review (re-reading) of the input in order to derive an analysis. Weiner (1979) describes a representation for arguments that is also a tree structure, with a variety of links possible. His main concern is to characterize types of argument structures, for use in the generatio</context>
</contexts>
<marker>Archbold, Hobbs, 1980</marker>
<rawString>Archbold, A. and Hobbs, J. 1980 Notes on &amp;quot;The Analysis of Evaluative Argumentation in Text&amp;quot;: A Report on On-Going Work. SRI unpublished draft, Menlo Park, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Birnbaum</author>
<author>M Flowers</author>
<author>R McGuire</author>
</authors>
<title>Towards an Al Model of Argumentation.</title>
<date>1980</date>
<booktitle>Proceedings of American Association for Artificial Intelligence (AAAI) Conference:</booktitle>
<pages>313--315</pages>
<contexts>
<context position="49219" citStr="Birnbaum et al. 1980" startWordPosition="8305" endWordPosition="8308">rovided some insight into the more general question of how to accommodate a possible difference in beliefs between conversants in a natural language dialogue processing system. We also suggest that a less sophisticated oracle can be constructed that merely searches known facts and axioms, possibly including relaxations, to handle a large amount of naturally occurring arguments. 5 RELATED WORK 5.1 ARGUMENT UNDERSTANDING Other researchers in natural language have studied arguments, in particular. However, the focus of the research in each case has been different. Birnbaum and the group at Yale (Birnbaum et al. 1980, McGuire et al. 1981) study two-way communication, developing an argument graph to display the points raised by both conversants. This graph is then used to determine the best moves on the part of an adversary, to challenge the position of the other conversant. Thus, the question of what responses to generate is investigated. On the other hand, there is little insight into how a hearer can detect the points being raised by the speaker, to construct this argument graph. Our focus has thus been on this preliminary problem to argument understanding. Archbold (Archbold 1976, Archbold and Hobbs 19</context>
</contexts>
<marker>Birnbaum, Flowers, McGuire, 1980</marker>
<rawString>Birnbaum, L.; Flowers, M.; and McGuire, R. 1980 Towards an Al Model of Argumentation. Proceedings of American Association for Artificial Intelligence (AAAI) Conference: 313-315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cohen</author>
</authors>
<title>On Knowing What to Say: Planning Speech Acts.</title>
<date>1978</date>
<tech>Technical Report No. 118,</tech>
<institution>Computer Science Department, University of Toronto,</institution>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="46470" citStr="Cohen (1978)" startWordPosition="7883" endWordPosition="7884">sible missing connection is EX18 below. If the hearer tests 2 as possible evidence for 1, a major premise of the form &amp;quot;All sharks like tap dancing&amp;quot; would establish the relation. But the hearer would not regard this as a plausible belief of the speaker, and so would fail to recognize an evidence relation between the two propositions. EX18: 1) Joey likes to tap dance 2) He is a shark The problem of judging plausibility is difficult. To make this process more computationally tractable, one suggestion is to incorporate into the model some tracking of mutual belief between speaker and hearer. (See Cohen (1978) for further discussion on the use of mutual belief in natural language processing.) Then, certain tests for evidence relations can be blocked in the oracle, based on mutual belief. For instance, rules can be postulated regarding the use of claims and evidence that are mutually believed. Two sample rules are: (i) &amp;quot;If P is mutually believed, it can&apos;t be used as claim&amp;quot;. (ii) &amp;quot;If —P is mutually believed, P can&apos;t be used as evidence&amp;quot;. In addition, some of the default interpretation rules associated with the taxonomy of linguistic clues can be overruled by pragmatic considerations. The idea is to p</context>
</contexts>
<marker>Cohen, 1978</marker>
<rawString>Cohen, P. 1978 On Knowing What to Say: Planning Speech Acts. Technical Report No. 118, Computer Science Department, University of Toronto, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cohen</author>
</authors>
<title>Understanding Arguments.</title>
<date>1980</date>
<booktitle>Proceedings of Canadian Society for Computational Studies of Intelligence (CSCS1) Conference:</booktitle>
<pages>272--279</pages>
<marker>Cohen, 1980</marker>
<rawString>Cohen, R. 1980 Understanding Arguments. Proceedings of Canadian Society for Computational Studies of Intelligence (CSCS1) Conference: 272-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cohen</author>
</authors>
<title>Investigation of Processing Strategies for the Structural Analysis of Arguments.</title>
<date>1981</date>
<booktitle>Proceedings of the Association for Computational Linguistics (ACL) Conference:</booktitle>
<pages>71--75</pages>
<contexts>
<context position="9150" citStr="Cohen (1981)" startWordPosition="1489" endWordPosition="1490">ts of experience 3) And he&apos;s refused bribes 4) So he&apos;s honest 5) He would really make a good president With the representation: 5N \ In this example, the claim is the same as in EX1, that Jones would make a good president, but the evidence precedes the claim in each case — i.e., 1 is evidence for 2; 3 is evidence for 4; and 2 and 4 together are evidence for the final claim 5. 12 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 Robin Cohen Analyzing the Structure of Argumentative Discourse The associated reception algorithms for pre- and postorder are described in detail in Cohen (1981). Both can be shown to operate in linear time (linear in the number nodes of the tree) and so are quite efficient. The pre-order reception essentially finds an interpretation for the current proposition of an argument by searching for a father, up the right border of the tree. The post-order algorithm employs a stack; the current proposition tests to be father to the top of the stack; then all sons are popped off the stack, and the resulting tree pushed on the stack. We trace the construction of the trees of EX1 and EX2 below, to provide details of the pre-order and post-order reception algori</context>
<context position="12131" citStr="Cohen (1981)" startWordPosition="2050" endWordPosition="2051">-argument is in pre-order, the second one in post-order, and the argument overall is still coherent. The reception algorithm for accepting hybrid transmissions is basically a combination of techniques from the pre- and post-order receptions. Now, to process a current proposition both a father and possible sons must be searched for. But the search is still restricted — certain propositions get closed off as possible &amp;quot;relatives&amp;quot; to the current one (e.g., earlier brothers of an ancestor). Thus, the complexity of the algorithm is still reasonable; it can also be shown to be linear. Once more, see Cohen (1981) for further examples. A full description of the hybrid algorithm is included below. L is kept as a pointer to the lowest node of the tree still eligible to receive evidence. It is initially set to a dummy root node to which all other nodes succeed as evidence. Consider as well a labelling where the last proposition in the stream that succeeds as evidence for a node is stored as the rightmost son. For each node NEW in the input stream do: /* find father */ do while (NEW is not evidence for L and L is not dummy root) set L to father of L end; Computational Linguistics, Volume 13, Numbers 1-2, J</context>
</contexts>
<marker>Cohen, 1981</marker>
<rawString>Cohen, R. 1981 Investigation of Processing Strategies for the Structural Analysis of Arguments. Proceedings of the Association for Computational Linguistics (ACL) Conference: 71-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cohen</author>
</authors>
<title>A Computational Model for the Analysis of Arguments.</title>
<date>1983</date>
<tech>Ph.D. thesis, Technical Report No. 151,</tech>
<institution>Computer Systems Research Group, University of Toronto,</institution>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="15354" citStr="Cohen (1983)" startWordPosition="2636" endWordPosition="2637">om analysis of a body of examples from rhetoric texts, together with some &amp;quot;naturally occurring&amp;quot; arguments (letters to the editor in newspapers). The aim is to characterize coherent transmission forms (ordering of propositions) that can be understood, without additional &amp;quot;clue words&amp;quot; to the underlying structure. We feel that the hybrid model is a good first approximation because examples of various forms of hybrid were encountered, and exceptions to the form all involved additional clue information. This leaves the study of clues and possible extended transmission forms to the next section. (In Cohen (1983) a few longer examples are run through the model to illustrate the forms it can accept and to motivate provision for the recognition of a wide variety of argument forms. Note that an actual implementation was not produced. There is now a scaled-down first implementation (Smedley 1986), which will be discussed further in section 6.) 3 LINGUISTIC CLUES The second main component of the argument understanding model is a theory of linguistic clues. These are the words and phrases often used by the speaker to directly indicate the structure of the argument to the hearer. It is important to specify •</context>
<context position="17086" citStr="Cohen (1983" startWordPosition="2929" endWordPosition="2930">nd (recently) Grosz and Sidner (1986). Grosz and Sidner acknowledge the existence of clues and discuss various discourse structures that can be formed in the presence of clues. Reichman also gives a longer list of clue words and the particular conversational moves they signal. But there is no systematic proposal for interpreting a clue word that may occur (classification), and there is little discussion of how to process some of these more complex discourse structures without clues (suggesting when clues are necessary). In this section we clarify more deeply some of the discussion of clues in Cohen (1983, 1984b). 3.1 CLUES OF RE-DIRECTION One type of clue is expressions that specifically re-direct the hearer to an earlier part of the argument. Consider: 14 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 Robin Cohen Analyzing the Structure of Argumentative Discourse EX4: 1) The city is a mess 2) The parks are a mess 3) The park benches are broken 4) The grassy areas are parched 5) Returning to city problems, the highways are bad too With the representation: 1 27\ N 3 4 Here, the clue in proposition 5, returning to city problems signals additional support for proposition 1.</context>
<context position="29296" citStr="Cohen (1983)" startWordPosition="4978" endWordPosition="4979">ply test out all possible combinations of evidence relations (even though this would only be n*n vs. k*n number of operations), because an input that is not coherent would then be accepted. We would also not want to derive computationally a representation for an argument that involved more computational effort, if a simpler interpretation of the same argument could be derived. (Again, the speaker is to assume that the hearer will not be searching unnecessarily). This is illustrated in EX8 above. 3.3.2 CASES OF NECESSITY Three kinds of acceptable extended transmission strategies are studied in Cohen (1983): parallel evidence, multiple evidence (a proposition acting as evidence for several claims, in restricted conditions) and mixed-mode sub-arguments (with evidence both preceding and following a claim). We present an example of the parallel construction below. See Cohen (1984b, 1983) for further examples. EX10: 1) The city has problems 2) The parks are a mess 3) The highways are a mess 4) The buildings are a mess 5) Here&apos;s some evidence for the fact that the parks are a problem: the benches are broken 6) As for the highways, they&apos;re full of potholes With the representation: 4 Here, the argument</context>
<context position="39135" citStr="Cohen (1983)" startWordPosition="6634" endWordPosition="6635">e is &amp;quot;believe it, unless there&apos;s reason to strongly doubt, from within one&apos;s own beliefs&amp;quot;). We have found in simulations of the model on a variety of examples that most of the tests for evidence can be answered through (a) and (b). We hypothesize that, given a specification of a knowledge base, the search for connections between propositions can be controlled. This hypothesis would be best verified with an implementation of the oracle and extensive analysis of examples, and could be the focus of the next stage of our implementation (beyond Smedley (1986)). (The two large examples dissected in Cohen (1983) do have this property.) In addition to the problem that the major premise may be unstated is the problem that this premise should really be tempered by the beliefs of the speaker. In other words, the missing major premise that the hearer must fill in is really of the form: H believes that S wants H to believe (P Q). In other words, this premise is not necessarily one of the hearer&apos;s beliefs. It is important to emphasize the importance of pragmatic processing in establishing evidence relations. The tree of claim and evidence relations built as a representation for the argument is really an ind</context>
<context position="58353" citStr="Cohen (1983)" startWordPosition="9729" endWordPosition="9730">ence relation exists is made by the Evidence Oracle, which is passed the two propositions and responds with a yes or no answer. The Evidence Oracle has available, a knowledge base of shared facts and, if .possible a model of the speaker. Moreover, if certain beliefs of the speaker can be extracted during the tests for unstated premises, the model of the speaker may even be updated by the Evidence Oracle, to aid in the processing of later propositions. In the absence of an implementation, the model can still be defended as a useful prescription of analysis of arguments. This is accomplished in Cohen (1983) by hand simulations of a variety of examples, to demonstrate robustness, and analysis of the complexity of the processing algorithms, to demonstrate efficiency. Another argument for the usefulness of the argument analysis model is that the theories developed for the model may be applied to the solution of other language understanding problems. As a result, the study of arguments may be viewed as a worthwhile exercise in the study of language. Some examples of the wider applicability of the model are: • It has been shown that extracting the underlying structure of discourse is useful to study </context>
</contexts>
<marker>Cohen, 1983</marker>
<rawString>Cohen, R. 1983 A Computational Model for the Analysis of Arguments. Ph.D. thesis, Technical Report No. 151, Computer Systems Research Group, University of Toronto, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cohen</author>
</authors>
<title>A Theory of Discourse Coherence for Argument Understanding.</title>
<date>1984</date>
<booktitle>Proceedings of Canadian Society for Computational Studies of Intelligence(CSCS1) Conference:</booktitle>
<pages>6--10</pages>
<contexts>
<context position="21007" citStr="Cohen (1984" startWordPosition="3573" endWordPosition="3574"> to the defaults). Note that some words may fall into more than one category, based on the meaning used — e.g., then meaning &apos;next&apos; (in a list of actions) compared with then meaning &apos;as a result&apos;. Clue interpretation thus requires a classification process as well. P is prior proposition; S is the proposition with the clue CATEGORY RELATION: P to S EXAMPLE parallel brother First, Second inference son As a result detail father In particular summary multiple sons In sum reformulation son (&amp; father) In other words contrast brother or father On the other hand The taxonomy is described in detail in Cohen (1984b). We include discussion of one category here, as an example. The detail class is one case where connectives with a range of meanings were merged into one category. The title &amp;quot;detail&amp;quot; suggests that the connective will further specify some prior •proposition. Included cases are: for example, in particular, and as another instance. The interpretation rule assigned to this category is that the proposition with the connective provides evidence for the earlier connecting statement. The motivation is that an accumulation of specific cases leads to a conclusion of a general statement (a form of reas</context>
<context position="29571" citStr="Cohen (1984" startWordPosition="5018" endWordPosition="5019"> involved more computational effort, if a simpler interpretation of the same argument could be derived. (Again, the speaker is to assume that the hearer will not be searching unnecessarily). This is illustrated in EX8 above. 3.3.2 CASES OF NECESSITY Three kinds of acceptable extended transmission strategies are studied in Cohen (1983): parallel evidence, multiple evidence (a proposition acting as evidence for several claims, in restricted conditions) and mixed-mode sub-arguments (with evidence both preceding and following a claim). We present an example of the parallel construction below. See Cohen (1984b, 1983) for further examples. EX10: 1) The city has problems 2) The parks are a mess 3) The highways are a mess 4) The buildings are a mess 5) Here&apos;s some evidence for the fact that the parks are a problem: the benches are broken 6) As for the highways, they&apos;re full of potholes With the representation: 4 Here, the argument breaks the rules of hybrid transmission by adding evidence for an earlier brother (proposition 2); however, this shift is signalled with a phrase of intention in proposition 5, and the hearer may then expect a parallel expansion of additional support for each of the earlier</context>
</contexts>
<marker>Cohen, 1984</marker>
<rawString>Cohen, R. 1984a A Theory of Discourse Coherence for Argument Understanding. Proceedings of Canadian Society for Computational Studies of Intelligence(CSCS1) Conference: 6-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cohen</author>
</authors>
<title>A Computational Theory of the Function of Clue Words in Argument Understanding.</title>
<date>1984</date>
<booktitle>Proceedings of 10th International Conference on Computational Linguistics (COLING</booktitle>
<volume>84</volume>
<pages>251--258</pages>
<contexts>
<context position="21007" citStr="Cohen (1984" startWordPosition="3573" endWordPosition="3574"> to the defaults). Note that some words may fall into more than one category, based on the meaning used — e.g., then meaning &apos;next&apos; (in a list of actions) compared with then meaning &apos;as a result&apos;. Clue interpretation thus requires a classification process as well. P is prior proposition; S is the proposition with the clue CATEGORY RELATION: P to S EXAMPLE parallel brother First, Second inference son As a result detail father In particular summary multiple sons In sum reformulation son (&amp; father) In other words contrast brother or father On the other hand The taxonomy is described in detail in Cohen (1984b). We include discussion of one category here, as an example. The detail class is one case where connectives with a range of meanings were merged into one category. The title &amp;quot;detail&amp;quot; suggests that the connective will further specify some prior •proposition. Included cases are: for example, in particular, and as another instance. The interpretation rule assigned to this category is that the proposition with the connective provides evidence for the earlier connecting statement. The motivation is that an accumulation of specific cases leads to a conclusion of a general statement (a form of reas</context>
<context position="29571" citStr="Cohen (1984" startWordPosition="5018" endWordPosition="5019"> involved more computational effort, if a simpler interpretation of the same argument could be derived. (Again, the speaker is to assume that the hearer will not be searching unnecessarily). This is illustrated in EX8 above. 3.3.2 CASES OF NECESSITY Three kinds of acceptable extended transmission strategies are studied in Cohen (1983): parallel evidence, multiple evidence (a proposition acting as evidence for several claims, in restricted conditions) and mixed-mode sub-arguments (with evidence both preceding and following a claim). We present an example of the parallel construction below. See Cohen (1984b, 1983) for further examples. EX10: 1) The city has problems 2) The parks are a mess 3) The highways are a mess 4) The buildings are a mess 5) Here&apos;s some evidence for the fact that the parks are a problem: the benches are broken 6) As for the highways, they&apos;re full of potholes With the representation: 4 Here, the argument breaks the rules of hybrid transmission by adding evidence for an earlier brother (proposition 2); however, this shift is signalled with a phrase of intention in proposition 5, and the hearer may then expect a parallel expansion of additional support for each of the earlier</context>
</contexts>
<marker>Cohen, 1984</marker>
<rawString>Cohen, R. 1984b A Computational Theory of the Function of Clue Words in Argument Understanding. Proceedings of 10th International Conference on Computational Linguistics (COLING 84): 251-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cohen</author>
</authors>
<title>The Need for Pragmatics in Natural Language Understanding.</title>
<date>1985</date>
<booktitle>Proceedings of CSCS1-sponsored Theoretical Advances in Natural Language Understanding Conference.</booktitle>
<contexts>
<context position="47966" citStr="Cohen (1985)" startWordPosition="8118" endWordPosition="8119"> believed. Note that the idea of a &amp;quot;pragmatic override&amp;quot; is also employed in Gazdar (1979) for the problem of determining presuppositions. The importance of pragmatic processing for argument analysis is once more emphasized, as it is a critical component to the difficult procedure of judging plausibility. While considering mutual belief will help to eliminate some potentially difficult tests for the oracle, the model would require a more detailed specification of the maintenance and use of mutual belief. This is left as a topic for future work. Some current ideas are explored in more detail in Cohen (1985). 4.4 SUMMARY OF EVIDENCE THEORY The &amp;quot;theory&amp;quot; of evidence relationships, developed to specify the operation of the evidence oracle component of the argument analysis model, really presents insight into the problems relevant to evidence relations, rather than offering solutions. Still, the fundamental question of how connections between propositions can be verified has not been studied to any extent by other researchers. It is extremely worthwhile to acknowledge that it is not sufficient to indicate what relations do occur, without also suggesting how these relations could be established during</context>
</contexts>
<marker>Cohen, 1985</marker>
<rawString>Cohen, R. 1985 The Need for Pragmatics in Natural Language Understanding. Proceedings of CSCS1-sponsored Theoretical Advances in Natural Language Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Pragmatics: Implicature, Presupposition and Logical Form.</title>
<date>1979</date>
<publisher>Academic Press,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="47443" citStr="Gazdar (1979)" startWordPosition="8032" endWordPosition="8033">as claim&amp;quot;. (ii) &amp;quot;If —P is mutually believed, P can&apos;t be used as evidence&amp;quot;. In addition, some of the default interpretation rules associated with the taxonomy of linguistic clues can be overruled by pragmatic considerations. The idea is to possibly override the default semantic interpretations of evidence relations otherwise specified, by testing whether 20 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 Robin Cohen Analyzing the Structure of Argumentative Discourse propositions are already mutually believed. Note that the idea of a &amp;quot;pragmatic override&amp;quot; is also employed in Gazdar (1979) for the problem of determining presuppositions. The importance of pragmatic processing for argument analysis is once more emphasized, as it is a critical component to the difficult procedure of judging plausibility. While considering mutual belief will help to eliminate some potentially difficult tests for the oracle, the model would require a more detailed specification of the maintenance and use of mutual belief. This is left as a topic for future work. Some current ideas are explored in more detail in Cohen (1985). 4.4 SUMMARY OF EVIDENCE THEORY The &amp;quot;theory&amp;quot; of evidence relationships, deve</context>
</contexts>
<marker>Gazdar, 1979</marker>
<rawString>Gazdar, G. 1979 Pragmatics: Implicature, Presupposition and Logical Form. Academic Press, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Geva</author>
</authors>
<title>Flowcharting Expository Texts and Reading Comprehension. Paper presented at</title>
<date>1981</date>
<journal>Annual Meeting of American Educational Research Association.</journal>
<contexts>
<context position="55216" citStr="Geva (1981)" startWordPosition="9232" endWordPosition="9233"> there are some interesting parallels with existing psychological research. Labov and Fanshel (1977) investigate therapeutic discourse, dialogue between a psychologist and his patient. The research describes several properties of the arguments advanced by the patients including: the use of poor logic, the tendency to omit premises in arguments, a variety of transmission forms (claims before and after), and the existence of statements about the structure of the argument (clues). Since our characterization of input provides for all these forms, it strengthens our case for having a robust model. Geva (1981) investigates the usefulness of flowcharting text structure to assist students in comprehending the underlying structure. The top-down influence of building and using a representation is mentioned as important. The fact that many texts do not follow a strict linear ordering of connections between statements again confirms our concern with a variety of possible coherent transmissions. In brief, discovering psychological experiments that agree with the constraints of our model serve to defend its design. Further, some suggestions we make about computational measures of discourse processing may s</context>
</contexts>
<marker>Geva, 1981</marker>
<rawString>Geva, E. 1981 Flowcharting Expository Texts and Reading Comprehension. Paper presented at Annual Meeting of American Educational Research Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
</authors>
<title>The Representation and Use of Focus in Dialogue Understanding.</title>
<date>1977</date>
<tech>SRI Technical Note No. 151,</tech>
<location>Menlo Park, California.</location>
<contexts>
<context position="53808" citStr="Grosz (1977)" startWordPosition="9023" endWordPosition="9024">y pooling efforts and suggesting a powerful general model for argument analysis. 5.2 REFERENCE RESOLUTION AND FOCUS Some of the work on using focus for reference resolution contains similarities to the model presented here for analyzing argument structure. Sidner (1979) maintains a focus stack of possible items in focus and an alternate focus list to support shifts of focus. The candidates for resolving references are thus restricted and ordered. The point is that these restrictions are drawn from a characterization of coherent discourse, the same approach taken for our control of processing. Grosz (1977) presents a model of focus spaces which may be used for several purposes, including the resolution of definite noun phrase resolution. The spaces are organized into a hierarchy, thus similar to our tree representation for argument structure. Both active and open spaces are tracked, similar to our tracking candidates eligible to be relatives to the current proposition. Because of similarities in the representations and techniques for controlling search for interpretations, it is worth investigating as future work the precise relationship among coherence, reference resolution and focus determina</context>
</contexts>
<marker>Grosz, 1977</marker>
<rawString>Grosz, B. 1977 The Representation and Use of Focus in Dialogue Understanding. SRI Technical Note No. 151, Menlo Park, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>The Structures of Discourse Structure.</title>
<date>1986</date>
<booktitle>Beranek and Newman (BBN) Cambridge, Computational Linguistics, Volume 13, Numbers 1-2, January-June</booktitle>
<tech>Report No. 6097. Bolt,</tech>
<pages>23</pages>
<contexts>
<context position="16512" citStr="Grosz and Sidner (1986)" startWordPosition="2835" endWordPosition="2838">cture of the argument to the hearer. It is important to specify • what kinds of clues exist • the function of these clues in analysis — i.e., what interpretation can be assigned to a proposition that contains a clue word, and • (a more difficult question) when clues are necessary to ensure comprehension of the argument structure by the hearer. This approach to the study of clue words is much more detailed than the initial suggestions of Hobbs (1976) on how to interpret a few connectives such as and in his framework. It is also distinct from the investigations of Reichman (1981) and (recently) Grosz and Sidner (1986). Grosz and Sidner acknowledge the existence of clues and discuss various discourse structures that can be formed in the presence of clues. Reichman also gives a longer list of clue words and the particular conversational moves they signal. But there is no systematic proposal for interpreting a clue word that may occur (classification), and there is little discussion of how to process some of these more complex discourse structures without clues (suggesting when clues are necessary). In this section we clarify more deeply some of the discussion of clues in Cohen (1983, 1984b). 3.1 CLUES OF RE-</context>
<context position="40582" citStr="Grosz and Sidner (1986)" startWordPosition="6883" endWordPosition="6886">s determined during analysis. What we are advocating is to interpret the intention of each proposition of the argument, the other propositions for which it provides evidence. The result of processing the entire discourse is not a complete plan of the speaker, in the sense that each of the &amp;quot;steps&amp;quot; could be executed and the top level goal (convince the hearer of some overall point) would then follow. It is more an indication of the motivation behind each utterance towards the ultimate goal of convincing the hearer. The difficulties in plan inference for discourse are discussed in more detail in Grosz and Sidner (1986), and are in fact a topic of our current concern (see discussion of future work in Section 6). There is in fact a whole spectrum of problems the hearer must face in recognizing evidence relationships between propositions. The four main tests for the hearer can be described as: • use logic, • relax the logic, • stereotype the speaker, • judge plausibility (reason as a &amp;quot;hypothetical person&amp;quot;). We illustrate these possible operations with examples below. 4.2 LOGIC AND RELAXED LOGIC In example EX14, all the premises of the Modus Ponens argument are present. The hearer should realize that 1 and 2 ar</context>
<context position="44579" citStr="Grosz and Sidner 1986" startWordPosition="7554" endWordPosition="7557">reason outside his personal framework of knowledge. Again, an issue is raised of how to discern intentions from discourse. Some work has been done at the level of one utterance (e.g., Allen 1979). We are mostly concerned with advocating the inclusion of reasoning beyond one&apos;s own beliefs, without a full theory of how to infer another person&apos;s beliefs. Instead, we advocate a simplified framework, discerning evidence relations and allowing a connection to be drawn as intended if it is plausible to the hearer. For future work, we are studying how to specify this process more precisely. (See also Grosz and Sidner 1986). The point is that the hearer is still able to recognize connections he does not believe. In EX17, the hearer should be able to understand an evidence relation from 1 to 2, upon filling in a missing premise of the form &amp;quot;If a person stands for apple pie and Mom then he is great&amp;quot;. If the hearer does not believe this statement himself, he may still consider it to be a reasonable belief of the speaker; having stereotyped knowledge of the speaker&apos;s views may thus be useful. EX17: 1) Reagan is great 2) He stands for apple pie and Mom Finally, if the hearer is testing a possible evidence relation be</context>
<context position="54478" citStr="Grosz and Sidner 1986" startWordPosition="9122" endWordPosition="9125">sed for several purposes, including the resolution of definite noun phrase resolution. The spaces are organized into a hierarchy, thus similar to our tree representation for argument structure. Both active and open spaces are tracked, similar to our tracking candidates eligible to be relatives to the current proposition. Because of similarities in the representations and techniques for controlling search for interpretations, it is worth investigating as future work the precise relationship among coherence, reference resolution and focus determination for dialogues (some of this is being done (Grosz and Sidner 1986)). 5.3 PSYCHOLOGICAL RESEARCH Although our model is not designed according to psychological studies of discourse comprehension, there are some interesting parallels with existing psychological research. Labov and Fanshel (1977) investigate therapeutic discourse, dialogue between a psychologist and his patient. The research describes several properties of the arguments advanced by the patients including: the use of poor logic, the tendency to omit premises in arguments, a variety of transmission forms (claims before and after), and the existence of statements about the structure of the argument</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B. and Sidner, C. 1986 The Structures of Discourse Structure. Report No. 6097. Bolt, Beranek and Newman (BBN) Cambridge, Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 23</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robin Cohen</author>
</authors>
<title>Analyzing the Structure of Argumentative Discourse Massachusetts.</title>
<journal>Attention, Intentions, and the Structure of Discourse, Computational Linguistics</journal>
<tech>also Report No. CSLI-85-39;</tech>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<marker>Cohen, </marker>
<rawString>Robin Cohen Analyzing the Structure of Argumentative Discourse Massachusetts. (also Report No. CSLI-85-39; Attention, Intentions, and the Structure of Discourse, Computational Linguistics 12(3): 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>A Computational Approach to Discourse Analysis.</title>
<date>1976</date>
<tech>Research Report No. 76-2.</tech>
<institution>City University of New York Department of Computer,</institution>
<location>New York, New York.</location>
<contexts>
<context position="16342" citStr="Hobbs (1976)" startWordPosition="2808" endWordPosition="2809">nt of the argument understanding model is a theory of linguistic clues. These are the words and phrases often used by the speaker to directly indicate the structure of the argument to the hearer. It is important to specify • what kinds of clues exist • the function of these clues in analysis — i.e., what interpretation can be assigned to a proposition that contains a clue word, and • (a more difficult question) when clues are necessary to ensure comprehension of the argument structure by the hearer. This approach to the study of clue words is much more detailed than the initial suggestions of Hobbs (1976) on how to interpret a few connectives such as and in his framework. It is also distinct from the investigations of Reichman (1981) and (recently) Grosz and Sidner (1986). Grosz and Sidner acknowledge the existence of clues and discuss various discourse structures that can be formed in the presence of clues. Reichman also gives a longer list of clue words and the particular conversational moves they signal. But there is no systematic proposal for interpreting a clue word that may occur (classification), and there is little discussion of how to process some of these more complex discourse struc</context>
</contexts>
<marker>Hobbs, 1976</marker>
<rawString>Hobbs, J. 1976 A Computational Approach to Discourse Analysis. Research Report No. 76-2. City University of New York Department of Computer, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Labov</author>
<author>D Fanshel</author>
</authors>
<title>Therapeutic Discourse.</title>
<date>1977</date>
<publisher>Academic Press,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="54705" citStr="Labov and Fanshel (1977)" startWordPosition="9152" endWordPosition="9155"> tracked, similar to our tracking candidates eligible to be relatives to the current proposition. Because of similarities in the representations and techniques for controlling search for interpretations, it is worth investigating as future work the precise relationship among coherence, reference resolution and focus determination for dialogues (some of this is being done (Grosz and Sidner 1986)). 5.3 PSYCHOLOGICAL RESEARCH Although our model is not designed according to psychological studies of discourse comprehension, there are some interesting parallels with existing psychological research. Labov and Fanshel (1977) investigate therapeutic discourse, dialogue between a psychologist and his patient. The research describes several properties of the arguments advanced by the patients including: the use of poor logic, the tendency to omit premises in arguments, a variety of transmission forms (claims before and after), and the existence of statements about the structure of the argument (clues). Since our characterization of input provides for all these forms, it strengthens our case for having a robust model. Geva (1981) investigates the usefulness of flowcharting text structure to assist students in compreh</context>
</contexts>
<marker>Labov, Fanshel, 1977</marker>
<rawString>Labov, W. and Fanshel, D. 1977 Therapeutic Discourse. Academic Press, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L McCarty</author>
<author>N S Sridharan</author>
</authors>
<title>A Computational Theory of Legal Argumentation.</title>
<date>1981</date>
<tech>Research Report LRP-TR-13,</tech>
<institution>Rutgers University Lab for Computer Science,</institution>
<location>New Brunswick, New Jersey.</location>
<marker>McCarty, Sridharan, 1981</marker>
<rawString>McCarty, L. and Sridharan, N.S. 1981 A Computational Theory of Legal Argumentation. Research Report LRP-TR-13, Rutgers University Lab for Computer Science, New Brunswick, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McGuire</author>
<author>L Birnbaum</author>
<author>M Flowers</author>
</authors>
<title>Opportunistic Processing in Arguments.</title>
<date>1981</date>
<booktitle>Proceedings of the International Joint Conference on Artificial Intelligence.bf one,.(1JCA I) Conference:</booktitle>
<pages>58--60</pages>
<contexts>
<context position="49241" citStr="McGuire et al. 1981" startWordPosition="8309" endWordPosition="8312">nto the more general question of how to accommodate a possible difference in beliefs between conversants in a natural language dialogue processing system. We also suggest that a less sophisticated oracle can be constructed that merely searches known facts and axioms, possibly including relaxations, to handle a large amount of naturally occurring arguments. 5 RELATED WORK 5.1 ARGUMENT UNDERSTANDING Other researchers in natural language have studied arguments, in particular. However, the focus of the research in each case has been different. Birnbaum and the group at Yale (Birnbaum et al. 1980, McGuire et al. 1981) study two-way communication, developing an argument graph to display the points raised by both conversants. This graph is then used to determine the best moves on the part of an adversary, to challenge the position of the other conversant. Thus, the question of what responses to generate is investigated. On the other hand, there is little insight into how a hearer can detect the points being raised by the speaker, to construct this argument graph. Our focus has thus been on this preliminary problem to argument understanding. Archbold (Archbold 1976, Archbold and Hobbs 1980) is most concerned </context>
</contexts>
<marker>McGuire, Birnbaum, Flowers, 1981</marker>
<rawString>McGuire, R., Birnbaum, L. and Flowers, M. 1981 Opportunistic Processing in Arguments. Proceedings of the International Joint Conference on Artificial Intelligence.bf one,.(1JCA I) Conference: 58-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pollack</author>
</authors>
<title>A Model of Plan Inference that Distinguishes Between the Beliefs of Actors and Observers.</title>
<date>1986</date>
<booktitle>Proceedings of the Association for Computational Linguistics (A CL) Conference:</booktitle>
<pages>207--214</pages>
<contexts>
<context position="42678" citStr="Pollack (1986)" startWordPosition="7245" endWordPosition="7246">1987 19 Robin Cohen Analyzing the Structure of Argumentative Discourse sharks are dangerous, and the hearer doesn&apos;t believe this &amp;quot;axiom&amp;quot; but knows of a few sharks that are dangerous, he may reason that the missing major premise is reasonable. EX14: 1) Aristotle is a man 2) All men are mortal 3) So Aristotle is mortal EX15: 1) Bilandic will win 2) He&apos;s the machine candidate EX16: I) Joey is a shark 2) So, he is dangerous The idea of recognizing an intended connection from some other conversant based on one&apos;s own beliefs is not necessarily simple to implement. There has been some recent work by Pollack (1986) that suggests a more concrete foundation for this operation. Pollack discusses the problem of inferring a questioner Q&apos;s plan from his discourse. A first process has the responder R ascribing to Q a belief about some connection (&amp;quot;conditional generation relation&amp;quot;) that she herself believes true. Occasionally, R will need to recognize a connection that is not one of her beliefs. Then Pollack suggests there is a rule where &amp;quot;R ascribes to Q a belief about a relation between act-types that is a slight variation of one she herself has&amp;quot;. In particular, one slight difference possible has Q believing </context>
</contexts>
<marker>Pollack, 1986</marker>
<rawString>Pollack, M. 1986 A Model of Plan Inference that Distinguishes Between the Beliefs of Actors and Observers. Proceedings of the Association for Computational Linguistics (A CL) Conference: 207-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
</authors>
<title>A Grammar of Contemporary English. Longmans Co.,</title>
<date>1972</date>
<location>London, England.</location>
<marker>Quirk, 1972</marker>
<rawString>Quirk, R. et. al. 1972 A Grammar of Contemporary English. Longmans Co., London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichman</author>
</authors>
<title>Plain Speaking: A Theory and Grammar of Spontaneous Discourse.</title>
<date>1981</date>
<tech>Report No. 4681,</tech>
<location>Bolt, Beranek and Newman (BBN), Cambridge, Massachusetts..</location>
<contexts>
<context position="16473" citStr="Reichman (1981)" startWordPosition="2831" endWordPosition="2832">r to directly indicate the structure of the argument to the hearer. It is important to specify • what kinds of clues exist • the function of these clues in analysis — i.e., what interpretation can be assigned to a proposition that contains a clue word, and • (a more difficult question) when clues are necessary to ensure comprehension of the argument structure by the hearer. This approach to the study of clue words is much more detailed than the initial suggestions of Hobbs (1976) on how to interpret a few connectives such as and in his framework. It is also distinct from the investigations of Reichman (1981) and (recently) Grosz and Sidner (1986). Grosz and Sidner acknowledge the existence of clues and discuss various discourse structures that can be formed in the presence of clues. Reichman also gives a longer list of clue words and the particular conversational moves they signal. But there is no systematic proposal for interpreting a clue word that may occur (classification), and there is little discussion of how to process some of these more complex discourse structures without clues (suggesting when clues are necessary). In this section we clarify more deeply some of the discussion of clues i</context>
<context position="51573" citStr="Reichman (1981)" startWordPosition="8680" endWordPosition="8681">ernatives, etc. How the tree can be built up relies on tracking a node that is &amp;quot;in focus&amp;quot;. The •fact that determining the relations between statements may make use of clue words is mentioned briefly as well. Basically, some of the features we advocate appear in this research. But we are trying to provide more insight into operational questions such as: • How do you determine the (best) relation between propositions? • How is the focus set? and • When are clue words likely to occur? By contrast, Weiner concentrates on how to generate explanations using his precisely specified characterization. Reichman (1981) is concerned with a larger problem of producing a model of discourse (not just arguments), but her approach should handle arguments as well. The core of the model is an ATN grammar for parsing and generation, coupled with a representation of &amp;quot;context spaces&amp;quot; containing conversational moves. The conversational moves provide a classification of larger components of discourse (not just single propositions). For example, there is an extensive study of a &amp;quot;challenge&amp;quot; operation. Since Reichman&apos;s aims are broader than ours, the lower level issues we address of verifying evidence and studying the nece</context>
</contexts>
<marker>Reichman, 1981</marker>
<rawString>Reichman, R. 1981 Plain Speaking: A Theory and Grammar of Spontaneous Discourse. Report No. 4681, Bolt, Beranek and Newman (BBN), Cambridge, Massachusetts..</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sadock</author>
</authors>
<title>Modus Brevis: The Truncated Argument. Papers from the 13th Regional Meeting, Chicago Linguistics Society:</title>
<date>1977</date>
<pages>545--554</pages>
<contexts>
<context position="34861" citStr="Sadock (1977)" startWordPosition="5912" endWordPosition="5913">nnection from P to Q — i.e., some rule of inference such that P is premise to Q&apos;s conclusion&amp;quot;. The main problem in establishing evidence relations is that not all the premises are stated. For example, one common rule of inference used in arguments is Modus Ponens, of the form: P Q, P therefore Q. The way this rule is most often used, the speaker will simply state P and Q and leave out the major premise &amp;quot;P Q&amp;quot;, expecting the hearer to be able to fill in the unstated connection to recognize the evidence relation from P to Q. Omitting certain premises is referred to as Modus Brevis and studied in Sadock (1977). We list below the rule of inference frames included for our model. Each rule has a slot for major premise, minor premise, and conclusion, to be filled by stated or unstated propositions, in recognizing an evidence relationship. RULE OF INFERENCE FRAMES: Major Minor Conclusion Modus Ponens P Q P Modus Tollens P Q —Q —P Modus Tollendo Ponens P v —Q Q Modus Ponendo Tollens PvQ Q —p The form most often used is Modus Ponens. When the major premise is missing, this is the rule of inference to consider as the intended link from P to Q. EX12: 1) The Jays had a fantastic team this year 2) All their p</context>
</contexts>
<marker>Sadock, 1977</marker>
<rawString>Sadock, J. 1977 Modus Brevis: The Truncated Argument. Papers from the 13th Regional Meeting, Chicago Linguistics Society: 545-554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sidner</author>
</authors>
<title>Towards a Computational Theory of Definite Anaphora Comprehension</title>
<date>1979</date>
<booktitle>in English Discourse. Al Lab Report TR-537, Massacusetts Institute of Technology (MIT),</booktitle>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="53466" citStr="Sidner (1979)" startWordPosition="8968" endWordPosition="8969">volve differing beliefs is stressed in our work. And finally, the use and interpretation of clue words is addressed. It is worth Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 21 Robin Cohen Analyzing the Structure of Argumentative Discourse noting that the differences in our existing studies can possibly be exploited by pooling efforts and suggesting a powerful general model for argument analysis. 5.2 REFERENCE RESOLUTION AND FOCUS Some of the work on using focus for reference resolution contains similarities to the model presented here for analyzing argument structure. Sidner (1979) maintains a focus stack of possible items in focus and an alternate focus list to support shifts of focus. The candidates for resolving references are thus restricted and ordered. The point is that these restrictions are drawn from a characterization of coherent discourse, the same approach taken for our control of processing. Grosz (1977) presents a model of focus spaces which may be used for several purposes, including the resolution of definite noun phrase resolution. The spaces are organized into a hierarchy, thus similar to our tree representation for argument structure. Both active and </context>
</contexts>
<marker>Sidner, 1979</marker>
<rawString>Sidner, C. 1979 Towards a Computational Theory of Definite Anaphora Comprehension in English Discourse. Al Lab Report TR-537, Massacusetts Institute of Technology (MIT), Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Smedley</author>
</authors>
<title>An Implementation of a Computational Model for the Analysis of Arguments: An Introduction to the First Attempt.</title>
<date>1986</date>
<tech>Research Report CS-86-26,</tech>
<institution>University of Waterloo Department of Computer Science,</institution>
<location>Waterloo, Ontario, Canada.</location>
<contexts>
<context position="15639" citStr="Smedley 1986" startWordPosition="2685" endWordPosition="2686">o the underlying structure. We feel that the hybrid model is a good first approximation because examples of various forms of hybrid were encountered, and exceptions to the form all involved additional clue information. This leaves the study of clues and possible extended transmission forms to the next section. (In Cohen (1983) a few longer examples are run through the model to illustrate the forms it can accept and to motivate provision for the recognition of a wide variety of argument forms. Note that an actual implementation was not produced. There is now a scaled-down first implementation (Smedley 1986), which will be discussed further in section 6.) 3 LINGUISTIC CLUES The second main component of the argument understanding model is a theory of linguistic clues. These are the words and phrases often used by the speaker to directly indicate the structure of the argument to the hearer. It is important to specify • what kinds of clues exist • the function of these clues in analysis — i.e., what interpretation can be assigned to a proposition that contains a clue word, and • (a more difficult question) when clues are necessary to ensure comprehension of the argument structure by the hearer. This</context>
<context position="39083" citStr="Smedley (1986)" startWordPosition="6626" endWordPosition="6627">ird party (which could be simplified if the bottom line is &amp;quot;believe it, unless there&apos;s reason to strongly doubt, from within one&apos;s own beliefs&amp;quot;). We have found in simulations of the model on a variety of examples that most of the tests for evidence can be answered through (a) and (b). We hypothesize that, given a specification of a knowledge base, the search for connections between propositions can be controlled. This hypothesis would be best verified with an implementation of the oracle and extensive analysis of examples, and could be the focus of the next stage of our implementation (beyond Smedley (1986)). (The two large examples dissected in Cohen (1983) do have this property.) In addition to the problem that the major premise may be unstated is the problem that this premise should really be tempered by the beliefs of the speaker. In other words, the missing major premise that the hearer must fill in is really of the form: H believes that S wants H to believe (P Q). In other words, this premise is not necessarily one of the hearer&apos;s beliefs. It is important to emphasize the importance of pragmatic processing in establishing evidence relations. The tree of claim and evidence relations built a</context>
<context position="56796" citStr="Smedley 1986" startWordPosition="9475" endWordPosition="9476">s for an implementation of an argument understanding system. One suggested real-life application area is a complaint bureau for department stores. Future work could include a full implementation of the model, and fine-tuning the design by selecting a particular application area for arguments. Although there is no complete implementation of the model to date, an overview of a possible design is presented here, to indicate how the various components of the model could come together into one integrated &amp;quot;system&amp;quot;. (Note that an initial implementation of the model does exist nOw, written in Prolog (Smedley 1986). But this program merely tests the various reception algorithms described in section 2. The evidence oracle is replaced by a &amp;quot;query the user&amp;quot; facility. Nonetheless, the groundwork is in place for a future implementation that tests the other components of the model). In Figure 1, there are three main modules: the Proposition Analyzer, Clue Interpreter, and Evidence Oracle. The Proposition Analyzer takes as input the argument itself and produces a representation of its underlying structure. For each proposition of the argument the Proposition Analyzer attempts to assign it a location in the rep</context>
</contexts>
<marker>Smedley, 1986</marker>
<rawString>Smedley, T. 1986 An Implementation of a Computational Model for the Analysis of Arguments: An Introduction to the First Attempt. Research Report CS-86-26, University of Waterloo Department of Computer Science, Waterloo, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weiner</author>
</authors>
<title>BLAH, A System which Explains Its Reasoning.</title>
<date>1979</date>
<tech>Technical Report TR 79-14,</tech>
<institution>University of New Hampshire Computer Science Department,</institution>
<location>Durham, New Hampshire.</location>
<contexts>
<context position="50225" citStr="Weiner (1979)" startWordPosition="8463" endWordPosition="8464"> detect the points being raised by the speaker, to construct this argument graph. Our focus has thus been on this preliminary problem to argument understanding. Archbold (Archbold 1976, Archbold and Hobbs 1980) is most concerned with evaluative arguments, those with strong underlying ideologies. For example, Lenin&apos;s speeches are appropriate sample input. Thus, the difficult question of recognizing differing opinions is a focus to Archbold&apos;s investigations. In addition, he studies text rather than discourse, allowing for a deeper review (re-reading) of the input in order to derive an analysis. Weiner (1979) describes a representation for arguments that is also a tree structure, with a variety of links possible. His main concern is to characterize types of argument structures, for use in the generation of explanations. There is thus little attention on the problems encountered in deriving argument structures during analysis. Weiner&apos;s (1979) model for the structure of explanation bears some resemblance to the representation described for arguments here. Weiner claims that natural explanation can be regarded as a series of transformations of an underlying tree structure that represents the abstract</context>
</contexts>
<marker>Weiner, 1979</marker>
<rawString>Weiner, J. 1979 BLAH, A System which Explains Its Reasoning. Technical Report TR 79-14, University of New Hampshire Computer Science Department, Durham, New Hampshire.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>