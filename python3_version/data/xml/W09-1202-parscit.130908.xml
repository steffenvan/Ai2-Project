<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000227">
<title confidence="0.999509">
An Iterative Approach for Joint
Dependency Parsing and Semantic Role Labeling
</title>
<author confidence="0.998865">
Qifeng Dai
</author>
<affiliation confidence="0.999533">
Department of Computer Sci-
ence, University of Science and
Technology of China, Hefei,
</affiliation>
<address confidence="0.421423">
China
</address>
<email confidence="0.859764">
daiqifeng001@126.com
</email>
<author confidence="0.989423">
Enhong Chen
</author>
<affiliation confidence="0.999544">
Department of Computer Sci-
ence, University of Science and
Technology of China, Hefei,
</affiliation>
<address confidence="0.461365">
China
</address>
<email confidence="0.949429">
cheneh@ustc.edu.cn
</email>
<author confidence="0.993183">
Liu Shi
</author>
<affiliation confidence="0.999572666666667">
Department of Computer Sci-
ence, University of Science and
Technology of China, Hefei,
</affiliation>
<address confidence="0.469625">
China
</address>
<email confidence="0.968504">
shiliu@ustc.edu
</email>
<sectionHeader confidence="0.980852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999679230769231">
We propose a system to carry out the joint pars-
ing of syntactic and semantic dependencies in
multiple languages for our participation in the
shared task of CoNLL-2009. We present an it-
erative approach for dependency parsing and
semantic role labeling. We have participated in
the closed challenge, and our system achieves
73.98% on labeled macro F1 for the complete
problem, 77.11% on labeled attachment score
for syntactic dependencies, and 70.78% on la-
beled F1 for semantic dependencies. The cur-
rent experimental results show that our method
effectively improves system performance.
</bodyText>
<sectionHeader confidence="0.995172" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859225806452">
In this paper we describe the system submitted to
the closed challenge of the CoNLL-2009 shared
task on joint parsing of syntactic and semantic de-
pendencies in multiple languages.
Give a sentence, the task of dependency parsing
is to identify the syntactic head of each word in the
sentence and classify the relation between the de-
pendent and its head. The task of semantic role
labeling is to label the senses of predicates in the
sentence and labeling the semantic role of each
word in the sentence relative to each predicate.
The difficulty of this shared task is to perform
joint task on dependency parsing and semantic role
labeling. We split the shared task into four sub-
problems: syntactic dependency parsing, syntactic
dependency label classification, word sense disam-
biguation, and semantic role labeling. And we pro-
pose a novel iterative approach to perform the joint
task. In the first step, the system performs depend-
ency parsing and semantic role labeling in a pipe-
lined manner and the four sub-problems extract
features based on the known information. In the
iterative step, the system performs the four tasks in
a pipelined manner but uses features extracted
from the previous parsing result.
The remainder of the paper is structured as fol-
lows. Section 2 presents the technical details of our
system. Section 3 presents experimental results and
the performance analysis. Section 4 looks into a
few issues concerning our forthcoming work for
this shared task, and concludes the paper.
</bodyText>
<sectionHeader confidence="0.854985" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.999595">
This section briefly describes the main components
of our system: a) system flow; b) syntactic parsing;
c) semantic role labeling; d) an iterative approach
to perform joint syntactic-semantic parsing.
</bodyText>
<subsectionHeader confidence="0.993045">
2.1 System flow
</subsectionHeader>
<bodyText confidence="0.999961">
As many systems did in CoNLL Shared Task 2008,
the most direct way for such task is pipeline ap-
proach. First, Split the system into four subtasks:
syntactic dependency parsing, syntactic depend-
ency relation labeling, predicate sense labeling and
semantic role labeling. Then, execute them one by
one. In our system, we extend this pipeline system
to an iterative system so that it can do a joint label-
ing to improve the performance.
Our iterative system is based on the pipeline
system. For the first iteration (original step), we
use the pipeline system to parse and label the
</bodyText>
<page confidence="0.775824">
19
</page>
<note confidence="0.9482105">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 19–24,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958571428572">
whole sentence. For the rest iterations (iterative
step), we use another pipeline system to parse and
label it. The structure of this pipeline is the same as
the original one, but each subtask can have much
more features than the original subtask. Because
the whole sentence has been labeled in the original
step, all information is available for every subtask.
For example, when doing syntactic dependency
relation labeling, we can add some features about
sense and semantic role. It seems like using syntac-
tic results to do semantic labeling, then using se-
mantic results to improve syntactic labeling. This
is the core idea of our joint system. Figure 1 shows
the main flow of our system.
</bodyText>
<subsectionHeader confidence="0.998633">
2.2 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999930333333333">
In the dependency parsing step, we split the task
into two sub-problems: syntactic dependency pars-
ing and syntactic dependency relation labeling.
In the syntactic dependency parsing stage,
MSTParser1, a dependency parser that searches for
maximum spanning trees over directed graphs, is
applied. Due to the differences between the seven
languages, we use different parameters to train a
parsing model. Specifically, as Czech and German
languages are none-projective and the others are
projective, we train Czech and German languages
with parameter “none-projective” and the others
with “projective”.
On the syntactic dependency label classification
step, we used the max-entropy classification algo-
rithm to train the model. This step contains two
processes. In the first process the sub-problem
trains the model with the following basic features:
</bodyText>
<listItem confidence="0.999103357142857">
• FORM1: FORM of the head.
• LEMMA1: LEMMA of the head.
• STEM1 (English only): STEM of the head.
• POS1: POS of the head.
• IS_PRED1: the value of FILLPRED of the
head.
• FEAT1: FEAT of the head.
• LM_STEM1 (English only): the left-most
modifier’s STEM of head.
• LM_POS1: the left-most modifier’s POS
of head.
• L_NUM1: number of the head’s left modi-
fiers.
• RM_STEM1 (English only): the right-
most modifier’s STEM of head.
• RM_POS1: the right-most modifier’s POS
of head.
• M_NUM1: number of modifiers of the
head.
• SUFFIX1 (English only): suffix of the
head.
• FORM2: FORM of the dependent.
• LEMMA2: LEMMA of the dependent.
• STEM2 (English only): STEM of the de-
pendent.
• POS2: POS of the dependent.
• IS PRED2: the value of FILLPRED of the
dependent.
</listItem>
<equation confidence="0.387437">
1 http://sourceforge.net/projects/mstparser
Start
</equation>
<bodyText confidence="0.959428571428571">
Set count = iterate times
Set isIterStep = false
Get fea-
tures:
this step
return the
feature of
system
judge by
the type of
sub task
and the
parameter
isIterStep.
</bodyText>
<figureCaption confidence="0.995288">
Figure 1. The main flow of iteration system
</figureCaption>
<figure confidence="0.996839571428571">
N
Semantic role labeling
Syntactic dependency
parsing
Syntactic dependency
relation labeling
Predicate sense label-
ing
count --
isIterStep = true
count = 0
Y
End
20
</figure>
<listItem confidence="0.999676954545455">
• FEAT2: FEAT of the dependent.
• LM_STEM2 (English only): the left-most
modifier’s STEM of dependent.
• LM_POS2: the left-most modifier’s POS
of dependent.
• L_NUM2: number of the dependent’s left
modifiers.
• RM_STEM2 (English only): the right-
most modifier’s STEM of dependent.
• RM_POS2: the right-most modifier’s POS
of dependent.
• M_NUM2: number of modifiers of the de-
pendent.
• SUFFIX2 (English only): suffix of the de-
pendent.
• DEP_PATH_ROOT_POS2: POS list from
dependent to tree’s root through the syn-
tactic dependency path.
• DEP_PATH_ROOT_LEN2: length from
dependent to tree’s root through the syn-
tactic dependency path.
• POSITION: The position of the word with
</listItem>
<bodyText confidence="0.8686648">
respect to its predicate. It has three values,
“before”, “is” and “after”, for the predicate.
In the iterative step, in addition to the features
mentioned above, the sub-task trains the model
with the following features:
</bodyText>
<listItem confidence="0.9918855">
• DEP PATH ROOT POS1: POS list from
head to tree’s root through the syntactic
dependency path.
• DEP_PATH_ROOT_REL1: length from
dependent to tree’s root through the syn-
tactic dependency path.
• PRED_POS: POS list of all predicates in
the sentence.
• FORM2 + DEP_PATH_REL: component
of FORM2 and the POS list from head to
the dependent through the syntactic de-
pendency path.
• POSITION + FORM2
• STEM1 + FORM2 (English only)
• STEM1 + STEM2 (English only)
• POSITION + POS2
• ROLE LIST2: list of APRED when the
_
dependent is a predicate.
• ROLE: list of APRED and PRED when
the head is predicate.
• L_ROLE: the nearest semantic role in its
left side when head is a predicate.
• R_ROLE: the nearest semantic role in its
right side when head is a predicate.
• IS_ROLE1: whether dependent is a se-
mantic role of head when head is a predi-
cate.
</listItem>
<subsectionHeader confidence="0.987807">
2.3 Semantic role labeling
</subsectionHeader>
<bodyText confidence="0.9998056">
Unlike CoNLL-2008 shared task, this shared task
does not need to identify predicates. So the main
task of this step is to label the sense of each predi-
cate and label the semantic role for each predicate.
When labeling the sense of each predicate, we
build a classification model for each predicate. As
the senses of different predicates are usually unre-
lated even if they have the same sense label, this
makes it difficult for us to use only one classifier to
label them. But this approach leads to another issue.
The set of predicates in the training set cannot
cover all predicates. For new predicates in the test
set, no classification model can be found for them,
and we build a most common sense for them. The
features we used are as follow:
</bodyText>
<listItem confidence="0.9988123">
• DEPREL1: DEPREL of the predicate.
• STEM1
• POS1
• RM_STEM1 (English only)
• RM POS1
_
• FORM2
• POS2
• SUFFIX2
• VOICE (English only): VOICE of predi-
cate.
• POSITION + POS2
• L_POS1 + POS1 + R_POS1: component
of left word’s POS and predicate POS and
right word’s POS.
• FORM2 + DEP PATH REL
_ _
• DEP PATH ROOT POS1
_ _ _
• DEP_PATH_ROOT_REL1
</listItem>
<bodyText confidence="0.9985816">
When labeling the semantic role, we use a simi-
lar approach as we did in CoNLL Shared Task
2008. However, as the frames information is not
supplied for all languages, we do not use it in this
task. The features we use are as follows:
</bodyText>
<listItem confidence="0.979601210526316">
• DEPREL1
• STEM1 (English only)
• POS1
• RM_STEM1 (English only)
• RM_POS1
21
• FORM2
• POS2
• SUFFIX2
• VOICE2 (English only)
• POSITION
• DEP PATH REL
_ _
• DEP PATH POS
_ _
• SENSE2
• SENSE2 + VOICE2
• POSITION + VOICE2
• DEP PATH LEN
</listItem>
<bodyText confidence="0.999051142857143">
model, “HEAD1” is golden. The classifier will
build a model directly and let “HEAD2” equal to
“HEAD1”. However, in the iterative step,
“HEAD1” is not golden, but such model makes it
impossible to change the results.. The iterative step
will be useless.
We design a simple method to avoid this issue.
</bodyText>
<listItem confidence="0.92766">
• Firstly, split the training set into N (N&gt;1)
subsets.
• Secondly, for each subset, use the left N-1
subsets to build an original sub-model (use
features in the pipeline step).
• Thirdly, use each sub-model to label the
corresponding subset.
• Lastly, use these labeled N subsets to ex-
tract samples (use features in the iterative
step) for building the iterative model.
</listItem>
<bodyText confidence="0.999990043478261">
In this way, the “HEAD1” is not golden any
more. And for each sub-task, we can use the simi-
lar method to build the original model and the it-
erative model.
Moreover, in our system, we only build the it-
erative models for syntactic dependency relation
labeling and semantic role labeling. For syntactic
dependency parsing, we use an approach with very
high time and space complexity, so it is not added
to the iterative step. Thus, its results will not be
changed in the iterative step. For sense labeling,
we build classification models for every predicate.
There are too many models and each model con-
tains only a few classes. We think they are not
suitable for building the iterative model. But, as its
previous sub-task (syntactic dependency relation
labeling) is added to the iterative step, it is useful
to add it to the iterative step. Though we do not
build an iterative model for sense labeling, we can
directly use its pipeline model. This is another ad-
vantage of our iterative model: if one subtask is not
suitable for doing iterative labeling/parsing, we can
use its pipeline model instead.
</bodyText>
<sectionHeader confidence="0.993253" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999823857142857">
We have tested our system with the test set and
obtained official results as shown in Table 1. We
have tried to find how the iterative step influences
syntactic dependency parsing and semantic role
labeling. For syntactic dependency parsing and
semantic role labeling, we do experiments on the
test set.
</bodyText>
<listItem confidence="0.59156">
• DEP_PATH_ROOT_REL1
</listItem>
<bodyText confidence="0.999975333333333">
Moreover, we build an iterative model in this
shared task. When doing an iterative labeling, the
previous labeling results are known. So we can
design some new features for checking the previ-
ous results in a global view. The features we add
for the iterative model are as follows:
</bodyText>
<listItem confidence="0.995037333333333">
• SENSE1: SENSE of the predicate.
• SENSE1 + VOICE1: component of the
SENSE + VOICE of predicate.
• VOICE1 + FORM1: component of VOICE
and FORM.
• ROLE_LIST1: list of APRED of predicate.
</listItem>
<subsectionHeader confidence="0.922321">
2.4 Iterative Approach
</subsectionHeader>
<bodyText confidence="0.999493695652174">
As described above, some subtasks have two
groups of features. One is for the pipeline model,
and the other is for the iterative model. The usage
of these two types of model is the same. The only
difference is that they use different features. The
iterative model can get more information, so they
can use more features. These additional features
can contain some joint and global (like frame and
global structure) information. The performance
may be improved because the viewer is extended.
Some structural error and semantic conflict can be
fixed.
Although the usage of the two types of model is
the same, there are some differences when building
the models.
In the iterative step, all information is available
for doing parsing and labeling. For example, when
doing syntactic dependency relation labeling in the
iterative step, the fields “HEAD”, “DEPREL”,
“PRED” and “APREDs” are filled by the pervious
iteration. So all these information can be used in
the iterative step. This will cause one issue: use
“HEAD1” to label “HEAD2”. When training the
</bodyText>
<page confidence="0.578246">
22
</page>
<table confidence="0.998116666666667">
Macro F1 Score
Average 73.98
Catalan 72.09
Chinese 72.72
Czech 67.14
English 81.89
German 75.00
Japanese 80.89
Spanish 68.14
</table>
<tableCaption confidence="0.935775">
Table 1. The Macro F1 Score of every languages and
the average value.
</tableCaption>
<subsectionHeader confidence="0.99874">
3.1 Syntactic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.99997232">
Dependency Parsing can be split into two sub-
problems: syntactic dependency parsing and syn-
tactic dependency label classification. We use the
iterative method on syntactic dependency label
classification. We do experiments on the test set.
On the test set, we do two group experiments. In
the first group, we build a subtest to test this sub-
task only. All other information is given, and we
just label the dependency relation. The results are
shown in Table 2. The row of “Initial step” shows
the results of this sub task in the original step. The
left two rows show the results in the iterative step
with iterating once and twice. The table shows that
the iterative approach improves the performance.
Especially for Catalan, the performance increases
by 2.89%.
Certainly, in the whole system, this subtask can-
not get golden information about sense and seman-
tic roles. So we test it in the whole system (joint
test) on the test set in the second group of experi-
ments. As shown in Table 3, the iterative step is
not as good as previous test. But it is still useful for
some languages. The reason that some languages
have no improvements on the iterative step is that
the result of the initial step is not so good.
</bodyText>
<subsectionHeader confidence="0.998886">
3.2 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.999990545454545">
Like syntactic dependency parsing, we do two tests
on Semantic Role Labeling. This result is not con-
sistent with the official data because we have add-
ed some features of the subtask. The results of
subtest can be found in Table 4. And Table 5
shows the results of the joint test. These two
groups of results show that the advantage of the
iterative step is not as good as that of syntactic de-
pendency labeling in subtest. But it improves the
performance for most languages. The iterative step
improves the performance in both two tests.
</bodyText>
<subsectionHeader confidence="0.999693">
3.3 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999981857142857">
From the experimental results, we can see that the
effect of each part of the iterative step depends on
the overall labeling result of the previous step. And
the labeling effect varies with different languages.
Iterative approach can improve the performance of
the system but it strongly depends on the initial
labeling result.
</bodyText>
<sectionHeader confidence="0.987634" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99999135">
This paper has presented a simple discriminative
system submitted to the CoNLL-2009 shared task
to address the learning task of syntactic and seman-
tic dependencies. The paper first describes how to
carry out syntactic dependency parsing and seman-
tic role labeling, and then a new iterative approach
is presented for joint parsing. The experimental
results show that the iterative process can improve
the labeling accuracy on syntactic and semantic
analysis. However, this approach probably depends
on the accuracy of the initial labeling results. The
results of the initial labeling results will affect the
effect of the iterative process.
Because of time constraints and inadequate ex-
perimental environment, our first results do not
meet our expectation, and the effect of the iterative
step is not so clear. Next, we will strive to refine
our approach to produce good results for the syn-
tactic dependency parsing, since it has a great im-
pact on the final parsing results.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999926222222222">
The authors would like to thank the reviewers for
their helpful comments. This work was supported
by National Natural Science Foundation of China
(No.60573077, No.60775037) and the National
High Technology Research and Development Pro-
gram of China (863 Program) (grant no.
2009AA01Z123). We also thank the High-
Performance Center of USTC for providing us
with the experimental platform.
</bodyText>
<page confidence="0.681238">
23
</page>
<table confidence="0.97782">
Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 93.64 95.66 95.01 88.10 88.10 96.79 92.98 96.41 89.71* 98.17 95.48
Iteration 1 94.60 98.56* 96.08* 88.59 88.29 97.31* 94.57* 96.63* 89.31 98.34 98.30
Iteration 2 94.65 98.55 96.08* 88.68* 88.45* 97.29 94.56 96.63* 89.53 98.35* 98.33*
</table>
<tableCaption confidence="0.9931695">
Table 2. The subtest result of Labeled Syntactic Accuracy of each language and the average performance value
on test set. (* denotes the best score for the system)
</tableCaption>
<table confidence="0.99859475">
Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 74.02 77.75 73.81 58.69* 55.50* 84.75 78.85 82.45 66.27* 90.45* 71.64
Iteration 1 73.90 77.82 73.86* 58.17 54.95 84.81 78.95 82.51* 65.78 90.43 71.68
Iteration 2 73.94 77.85* 73.86* 58.31 55.13 84.82* 79.02* 82.46 65.85 90.45* 71.69*
</table>
<tableCaption confidence="0.995314">
Table 3. The joint test result of Labeled Syntactic Accuracy of each language and the average performance value
on test set. (* denotes the best score for the system)
</tableCaption>
<table confidence="0.99740525">
Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 83.83 88.56 85.86 88.08 86.20* 86.23 82.09 80.98 78.82 74.32* 87.45
Iteration 1 84.34 89.02* 87.14* 87.88 86.09 86.66 82.07 83.66* 79.28* 74.06 87.59
Iteration 2 84.36 89.02* 87.01 88.10* 86.17 86.78* 82.34* 83.15 79.18 74.06 87.81*
</table>
<tableCaption confidence="0.9259465">
Table 4. The sub test result of Semantic Labeled F1 of each language and the average performance value on test
set. (* denotes the best score for the system)
</tableCaption>
<table confidence="0.98909575">
Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 70.01 66.87 71.63 75.50 75.71 78.97 69.87 67.50 58.47 70.91* 64.64
Iteration 1 70.15 67.12 71.98 75.54 75.68 79.40 70.17* 68.08* 58.55* 70.69 64.32
Iteration 2 70.20 67.33* 71.99* 75.65* 75.90* 79.47* 69.98 67.98 58.33 70.70 64.65*
</table>
<tableCaption confidence="0.841151">
Table 5. The joint test result of Semantic Labeled F1 of each language and the average performance value on test
set. (* denotes the best score for the system)
</tableCaption>
<sectionHeader confidence="0.985777" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898333333333">
Jan Haji6, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Antonia Martí, Lluís
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Št6pánek, Pavel Straiiák, Mihai Surdeanu,
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009
Shared Task: Syntactic and Semantic Dependencies
in Multiple Languages. Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2009). Boulder, Colorado, USA. June 4-
5. pp. 3-22.
Mariona Taulé, Maria Antònia Martí and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC-2008). Marrakech, Morocco.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143-172.
Jan Haji6, Jarmila Panevová, Eva Haji6ová, Petr Sgall,
Petr Pajas, Jan Št6pánek, Jiri Havelka, Marie Miku-
lová and Zden&amp; Žabokrtský. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data
Consortium, Philadelphia, Pennsylvania, USA. ISBN
1-58563-370-4. LDC Cat. No. LDC2006T01. URL:
http://ldc.upenn.edu.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Lluís Màrquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Padó and Manfred Pinkal. 2006.
The SALSA Corpus: a German Corpus Resource for
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and
Evaluation (LREC-2006). Genoa, Italy.
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida.
2002. Construction of a Japanese Relevance-tagged
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. pp. 2008-2013.
McDonald, Ryan. 2006. Discriminative learning and
Spanning Tree Algorithms for Dependency parsing.
Ph.D. thesis, University of Pennyslvania.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical. Report CMU-CS-99-108.
</reference>
<page confidence="0.95252">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.400734">
<title confidence="0.99554">An Iterative Approach for Dependency Parsing and Semantic Role Labeling</title>
<author confidence="0.908693">Qifeng</author>
<affiliation confidence="0.995865666666667">of Computer ence, University of Science Technology of China,</affiliation>
<email confidence="0.998963">daiqifeng001@126.com</email>
<author confidence="0.705348">Enhong</author>
<affiliation confidence="0.997901666666667">of Computer ence, University of Science Technology of China,</affiliation>
<email confidence="0.839977">cheneh@ustc.edu.cn</email>
<author confidence="0.692042">Liu</author>
<affiliation confidence="0.999236">of Computer ence, University of Science Technology of China,</affiliation>
<email confidence="0.999911">shiliu@ustc.edu</email>
<abstract confidence="0.999424357142857">We propose a system to carry out the joint parsing of syntactic and semantic dependencies in multiple languages for our participation in the shared task of CoNLL-2009. We present an iterative approach for dependency parsing and semantic role labeling. We have participated in the closed challenge, and our system achieves 73.98% on labeled macro F1 for the complete problem, 77.11% on labeled attachment score for syntactic dependencies, and 70.78% on labeled F1 for semantic dependencies. The current experimental results show that our method effectively improves system performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Jan Haji6</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Maria Antonia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Št6pánek, Pavel Straiiák, Mihai Surdeanu, Nianwen Xue and Yi Zhang.</title>
<date>2009</date>
<booktitle>The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009).</booktitle>
<pages>3--22</pages>
<location>Boulder, Colorado, USA.</location>
<marker>Haji6, Ciaramita, Johansson, Kawahara, 2009</marker>
<rawString>Jan Haji6, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antonia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Št6pánek, Pavel Straiiák, Mihai Surdeanu, Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009). Boulder, Colorado, USA. June 4-5. pp. 3-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taulé</author>
</authors>
<title>Maria Antònia Martí and Marta Recasens.</title>
<date>2008</date>
<booktitle>Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008).</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Taulé, 2008</marker>
<rawString>Mariona Taulé, Maria Antònia Martí and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008). Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<pages>15--1</pages>
<marker>Xue, Palmer, 2009</marker>
<rawString>Nianwen Xue and Martha Palmer. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Haji6</author>
<author>Jarmila Panevová</author>
</authors>
<title>Eva Haji6ová, Petr Sgall, Petr Pajas, Jan Št6pánek, Jiri Havelka, Marie Mikulová and Zden&amp; Žabokrtský.</title>
<date>2006</date>
<booktitle>The Prague Dependency Treebank 2.0. CD-ROM. Linguistic Data Consortium,</booktitle>
<tech>ISBN 1-58563-370-4. LDC Cat. No. LDC2006T01. URL: http://ldc.upenn.edu.</tech>
<location>Philadelphia, Pennsylvania, USA.</location>
<marker>Haji6, Panevová, 2006</marker>
<rawString>Jan Haji6, Jarmila Panevová, Eva Haji6ová, Petr Sgall, Petr Pajas, Jan Št6pánek, Jiri Havelka, Marie Mikulová and Zden&amp; Žabokrtský. 2006. The Prague Dependency Treebank 2.0. CD-ROM. Linguistic Data Consortium, Philadelphia, Pennsylvania, USA. ISBN 1-58563-370-4. LDC Cat. No. LDC2006T01. URL: http://ldc.upenn.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluís Màrquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, Màrquez, Nivre, 2008</marker>
<rawString>Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Padó</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA Corpus: a German Corpus Resource for Lexical Semantics.</title>
<date>2006</date>
<booktitle>Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006).</booktitle>
<location>Genoa, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Padó, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Padó and Manfred Pinkal. 2006. The SALSA Corpus: a German Corpus Resource for Lexical Semantics. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Koiti Hasida</author>
</authors>
<title>Construction of a Japanese Relevance-tagged Corpus.</title>
<date>2002</date>
<booktitle>Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002). Las</booktitle>
<pages>2008--2013</pages>
<location>Palmas,</location>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 2002. Construction of a Japanese Relevance-tagged Corpus. Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002). Las Palmas, Spain. pp. 2008-2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative learning and Spanning Tree Algorithms for Dependency parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennyslvania.</institution>
<marker>McDonald, 2006</marker>
<rawString>McDonald, Ryan. 2006. Discriminative learning and Spanning Tree Algorithms for Dependency parsing. Ph.D. thesis, University of Pennyslvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical. Report CMU-CS-99-108.</tech>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical. Report CMU-CS-99-108.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>