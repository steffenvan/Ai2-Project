<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.796429">
Linear Text Segmentation Using Affinity Propagation
</title>
<author confidence="0.990104">
Anna Kazantseva
</author>
<affiliation confidence="0.998049666666667">
School of Electrical Engineering
and Computer Science,
University of Ottawa
</affiliation>
<email confidence="0.995442">
ankazant@site.uottawa.ca
</email>
<sectionHeader confidence="0.997351" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996860380952381">
This paper presents a new algorithm for lin-
ear text segmentation. It is an adaptation of
Affinity Propagation, a state-of-the-art clus-
tering algorithm in the framework of factor
graphs. Affinity Propagation for Segmenta-
tion, or APS, receives a set of pairwise simi-
larities between data points and produces seg-
ment boundaries and segment centres – data
points which best describe all other data points
within the segment. APS iteratively passes
messages in a cyclic factor graph, until conver-
gence. Each iteration works with information
on all available similarities, resulting in high-
quality results. APS scales linearly for realistic
segmentation tasks. We derive the algorithm
from the original Affinity Propagation formu-
lation, and evaluate its performance on topi-
cal text segmentation in comparison with two
state-of-the art segmenters. The results sug-
gest that APS performs on par with or outper-
forms these two very competitive baselines.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999213583333333">
In complex narratives, it is typical for the topic to
shift continually. Some shifts are gradual, others –
more abrupt. Topical text segmentation identifies the
more noticeable topic shifts. A topical segmenter’s
output is a very simple picture of the document’s
structure. Segmentation is a useful intermediate step
in such applications as subjectivity analysis (Stoy-
anov and Cardie, 2008), automatic summarization
(Haghighi and Vanderwende, 2009), question an-
swering (Oh, Myaeng, and Jang, 2007) and others.
That is why improved quality of text segmentation
can benefit other language-processing tasks.
</bodyText>
<author confidence="0.75994">
Stan Szpakowicz
</author>
<affiliation confidence="0.9927156">
School of Electrical Engineering
and Computer Science,
University of Ottawa &amp;
Institute of Computer Science,
Polish Academy of Sciences
</affiliation>
<email confidence="0.973161">
szpak@site.uottawa.ca
</email>
<bodyText confidence="0.999879176470588">
We present Affinity Propagation for Segmenta-
tion (APS), an adaptation of a state-of-the-art clus-
tering algorithm, Affinity Propagation (Frey and
Dueck, 2007; Givoni and Frey, 2009).1 The origi-
nal AP algorithm considerably improved exemplar-
based clustering both in terms of speed and the qual-
ity of solutions. That is why we chose to adapt it to
segmentation. At its core, APS is suitable for seg-
menting any sequences of data, but we present it in
the context of segmenting documents. APS takes as
input a matrix of pairwise similarities between sen-
tences and, for each sentence, a preference value
which indicates an a priori belief in how likely
a sentence is to be chosen as a segment centre.
APS outputs segment assignments and segment cen-
tres – data points which best explain all other points
in a segment. The algorithm attempts to maximize
net similarity – the sum of similarities between all
data points and their respective segment centres.
APS operates by iteratively passing messages in
a factor graph (Kschischang, Frey, and Loeliger,
2001) until a good set of segments emerges. Each
iteration considers all similarities – takes into ac-
count all available information. An iteration in-
cludes sending at most O(N2) messages. For the
majority of realistic segmentation tasks, however,
the upper bound is O(MN) messages, where M
is a constant. This is more computationally ex-
pensive than the requirements of locally informed
segmentation algorithms such as those based on
HMM or CRF (see Section 2), but for a globally-
informed algorithm the requirements are very rea-
sonable. APS is an instance of loopy-belief propaga-
tion (belief propagation on cyclic graphs) which has
</bodyText>
<footnote confidence="0.9617355">
1An implementation of APS in Java, and the data sets, can be
downloaded at hwww.site.uottawa.ca/∼ankazanti.
</footnote>
<page confidence="0.924648">
284
</page>
<note confidence="0.958153">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284–293,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999841666666667">
been used to achieved state-of-the-art performance
in error-correcting decoding, image processing and
data compression. Theoretically, such algorithms
are not guaranteed to converge or to maximize the
objective function. Yet in practice they often achieve
competitive results.
APS works on an already pre-compiled similaritiy
matrix, so it offers flexibility in the choice of simi-
larity metrics. The desired number of segments can
be set by adjusting preferences.
We evaluate the performance of APS on three
tasks: finding topical boundaries in transcripts of
course lectures (Malioutov and Barzilay, 2006),
identifying sections in medical textbooks (Eisen-
stein and Barzilay, 2008) and identifying chapter
breaks in novels. We compare APS with two recent
systems: the Minimum Cut segmenter (Malioutov
and Barzilay, 2006) and the Bayesian segmenter
(Eisenstein and Barzilay, 2008). The comparison
is based on the WindowDiff metric (Pevzner and
Hearst, 2002). APS matches or outperforms these
very competitive baselines.
Section 2 of the paper outlines relevant research
on topical text segmentation. Section 3 briefly cov-
ers the framework of factor graphs and outlines the
original Affinity Propagation algorithm for cluster-
ing. Section 4 contains the derivation of the new
update messages for APSeg. Section 5 describes the
experimental setting, Section 6 reports the results,
Section 7 discusses conclusions and future work.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999919730158731">
This sections discusses selected text segmentation
methods and positions the proposed APS algorithm
in that context.
Most research on automatic text segmentation re-
volves around a simple idea: when the topic shifts,
so does the vocabulary (Youmans, 1991). We can
roughly subdivide existing approaches into two cat-
egories: locally informed and globally informed.
Locally informed segmenters attempt to identify
topic shifts by considering only a small portion of
complete document. A classical approach is Text-
Tiling (Hearst, 1997). It consists of sliding two ad-
jacent windows through text and measuring lexical
similarity between them. Drops in similarity corre-
spond to topic shifts. Other examples include text
segmentation using Hidden Markov Models (Blei
and Moreno, 2001) or Conditional Random Fields
(Lafferty, McCallum, and Pereira, 2001). Locally
informed methods are often very efficient because
of lean memory and CPU time requirements. Due to
a limited view of the document, however, they can
easily be thrown off by short inconsequential digres-
sions in narration.
Globally informed methods consider “the big pic-
ture” when determining the most likely location of
segment boundaries. Choi (2000) applies divisive
clustering to segmentation. Malioutov and Barzilay
(2006) show that the knowledge about long-range
similarities between sentences improves segmenta-
tion quality. They cast segmentation as a graph-
cutting problem. The document is represented as a
graph: nodes are sentences and edges are weighted
using a measure of lexical similarity. The graph is
cut in a way which maximizes the net edge weight
within each segment and minimizes the net weight
of severed edges. Such Minimum Cut segmentation
resembles APS the most among others mentioned in
this paper. The main difference between the two is
in different objective functions.
Another notable direction in text segmentation
uses generative models to find segment boundaries.
Eisenstein and Barzilay (2008) treat words in a sen-
tence as draws from a multinomial language model.
Segment boundaries are assigned so as to maximize
the likelihood of observing the complete sequence.
Misra et al. (2009) use a Latent Dirichlet alloca-
tion topic model (Blei, Ng, and Jordan, 2003) to find
coherent segment boundaries. Such methods output
segment boundaries and suggest lexical distribution
associated with each segment. Generative models
tend to perform well, but are less flexible than the
similarity-based models when it comes to incorpo-
rating new kinds of information.
Globally informed models generally perform bet-
ter, especially on more challenging datasets such as
speech recordings, but they have – unsurprisingly –
higher memory and CPU time requirements.
The APS algorithm described in this paper com-
bines several desirable properties. It is unsupervised
and, unlike most other segmenters, does not require
specifying the desired number of segments as an in-
put parameter. On each iteration it takes into account
the information about a large portion of the docu-
</bodyText>
<page confidence="0.996498">
285
</page>
<bodyText confidence="0.999847833333333">
ment (or all of it). Because APS operates on a pre-
compiled matrix of pair-wise sentence similarities, it
is easy to incorporate new kinds of information, such
as synonymy or adjacency. It also provides some in-
formation as to what the segment is about, because
each segment is associated with a segment centre.
</bodyText>
<sectionHeader confidence="0.731618" genericHeader="method">
3 Factor graphs and affinity propagation
</sectionHeader>
<subsectionHeader confidence="0.8458385">
for clustering
3.1 Factor graphs and the max-sum algorithm
</subsectionHeader>
<bodyText confidence="0.9990211">
The APS algorithm is an instance of belief propa-
gation on a cyclic factor graph. In order to explain
the derivation of the algorithm, we will first briefly
introduce factor graphs as a framework.
Many computational problems can be reduced to
maximizing the value of a multi-variate function
F(x1, ... , xn) which can be approximated by a sum
of simpler functions. In Equation 1, H is a set of
discrete indices and fh is a local function with argu-
ments Xh C {x1, ... , xn}:
</bodyText>
<equation confidence="0.9951385">
�F(x1, ... , xn) = fh(Xh) (1)
hEH
</equation>
<bodyText confidence="0.999264590909091">
Factor graphs offer a concise graphical represen-
tation for such problems. A global function F which
can be decomposed into a sum of M local function
fh can be represented as a bi-partite graph with M
function nodes and N variable nodes (M = |H|).
Figure 1 shows an example of a factor graph for
F(x1, x2, x3, x4) = f1(x1, x2, x3) + f2(x2, x3, x4).
The factor (or function) nodes are dark squares, the
variable nodes are light circles.
The well-known max-sum algorithm (Bishop,
2006) seeks a configuration of variables which max-
imizes the objective function. It finds the maximum
in acyclic factor graphs, but in graphs with cycles
neither convergence nor optimality are guaranteed
(Pearl, 1982). Yet in practice good approximations
can be achieved. The max-sum algorithm amounts
to propagating messages from function nodes to
variable nodes and from variable nodes to function
nodes. A message sent from a variable node x to a
function node f is computed as a sum of the incom-
ing messages from all neighbours of x other than f
(the sum is computed for each possible value of x):
</bodyText>
<equation confidence="0.958908333333333">
�µx→f = µf&apos;→x (2)
f&apos;EN(x)\f
Figure 1: Factor graph for F(x1, x2, x3, x4)
= f1(x1, x2, x3) + f2(x2, x3, x4).
f1 f2
x1 x2 x3 x4
</equation>
<bodyText confidence="0.998108571428571">
N(x) is the set of all function nodes which are x’s
neighbours. The message reflects the evidence about
the distribution of x from all functions which have x
as an argument, except for the function correspond-
ing to the receiving node f.
A message µf→x from function f to variable x is
computed as follows:
</bodyText>
<equation confidence="0.943343666666667">
�µf→x =max (f(x1, ... , xm) +
N(f)\xx&apos;EN(f)\x
(3)
</equation>
<bodyText confidence="0.985353272727273">
N(f) is the set of all variable nodes which are f’s
neighbours. The message reflects the evidence about
the distribution of x from function f and its neigh-
bours other than x.
A common message-passing schedule on cyclic
factor graphs is flooding: iteratively passing all
variable-to-function messages, then all function-to-
variable messages. Upon convergence, the summary
message reflecting final beliefs about the maximiz-
ing configuration of variables is computed as a sum
of all incoming function-to-variable messages.
</bodyText>
<subsectionHeader confidence="0.999094">
3.2 Affinity Propagation
</subsectionHeader>
<bodyText confidence="0.999914411764706">
The APS algorithm described in this paper is a mod-
ification of the original Affinity Propagation algo-
rithm intended for exemplar-based clustering (Frey
and Dueck, 2007; Givoni and Frey, 2009). This sec-
tion describes the binary variable formulation pro-
posed by Givoni and Frey, and lays the groundwork
for deriving the new update messages (Section 4).
Affinity Propagation for exemplar-based cluster-
ing is formulated as follows: to cluster N data
points, one must specify a matrix of pairwise sim-
ilarities {SIM(i,j)}iJE{1,...,N},i=,4j and a set of
self-similarities (so-called preferences) SIM(j, j)
which reflect a priori beliefs in how likely each data
point is to be selected as an exemplar. Preference
values occupy the diagonal of the similarity matrix.
The algorithm then assigns each data point to an ex-
emplar so as to maximize net similarity – the sum of
</bodyText>
<equation confidence="0.557335">
µx&apos;→f)
</equation>
<page confidence="0.975724">
286
</page>
<figureCaption confidence="0.977936">
Figure 2: Factor graph for affinity propagation.
</figureCaption>
<figure confidence="0.53389025">
E1 Ej EN
I1
Ii
IN
</figure>
<bodyText confidence="0.9838573">
similarities between all points and their respective
exemplars; this is expressed by Equation 7. Figure 2
shows a schematic factor graph for this problem,
with N2 binary variables. cij = 1 iff point j is an
exemplar for point i. Function nodes Ej enforce a
coherence constraint: a data point cannot exemplify
another point unless it is an exemplar for itself:
Ej(c1j,...,cNj) = ⎧ −oo if cjj = 0 n cij = 1
⎨⎪ for some i =� j
⎪⎩ 0 otherwise
</bodyText>
<equation confidence="0.405821">
(4)
</equation>
<bodyText confidence="0.998801333333333">
An I node encodes a single-cluster constraint: each
data point must belong to exactly one exemplar –
and therefore to one cluster:
</bodyText>
<equation confidence="0.975131666666667">
( −oo if Pj cij =� 1
Ii(ci1, . . . , ciN) = (5)
0 otherwise
</equation>
<bodyText confidence="0.96566275">
An S node encodes user-defined similarities
between data-points and candidate exemplars
(SIM(i, j) is the similarity between points i and
j):
</bodyText>
<equation confidence="0.9619755">
(
= SIM (i, j) if cij = 1
Sij (cij) (6)
0 otherwise
</equation>
<bodyText confidence="0.999541777777778">
Equation 7 shows the objective function which we
want to maximize: a sum of similarities between
data points and their exemplars, subject to the two
constraints (coherence and single-cluster per point).
According to Equation 3, the computation of a sin-
gle factor-to-variable message involves maximizing
over 2n configurations. E and I, however, are bi-
nary constraints and evaluate to −oo for most con-
figurations. This drastically reduces the number of
configurations which can maximize the message val-
ues. Given this simple fact, Givoni and Frey (2009)
show how to reduce the necessary update messages
to only two types of scalar ones: availabilities (α)
and responsibilities (ρ).2
A responsibility message ρij, sent from a variable
node cij to function node Ej, reflects the evidence
of how likely j is to be an exemplar for i given all
other potential exemplars:
</bodyText>
<equation confidence="0.9912985">
ρij = SIM(i, j) − m=,4ax(SIM(i, k) + αik) (8)
kj
</equation>
<bodyText confidence="0.999496111111111">
An availability message αij, sent from a function
node Ej to a variable node cij, reflects how likely
point j is to be an exemplar for i given the evidence
from all other data points:
(9)
Let γij(l) be the message value corresponding to set-
ting variable cij to l, l E {0, 1}. Instead of sending
two-valued messages (corresponding to the two pos-
sible values of the binary variables), we can send
the difference for the two possible configurations:
γij = γij(1) − γij(0) – effectively, a log-likelihood
ratio.
2Normally, each iteration of the algorithm sends five types
of two-valued messages: to and from functions E and I and
a message from functions S. Fortunately, the messages sent
to and from E factors to the variable nodes subsume the three
other message types and it is not necessary to compute them
explicitly. See (Givoni and Frey, 2009, p.195) for details.
</bodyText>
<equation confidence="0.5913684">
X max[ρkj, 0] if i = j
k7�j
min[0, ρjj + X max[ρkj, 0]] if i =� j
k/∈ji,j}
⎧
⎨⎪⎪⎪
⎪⎪⎪⎩
αij =
S11 S1j S1N
Si1 Sij SiN
cN1 cNj cNN
c11 c1j c1N
ci1 cij ciN
SN1 SNj SNN
XS(c11, ... , cNN) = Si,j(cij) + X Ii(ci1, ... , ciN)
i,j i
(7)
X Ej(c1j,...,cNj)
+
j
</equation>
<page confidence="0.76353">
287
</page>
<figureCaption confidence="0.983453">
Figure 3: Examples of valid configuration of hidden
variables {cij} for clustering and segmentation.
</figureCaption>
<figure confidence="0.978008">
(a) Clustering (b) Segmentation
</figure>
<bodyText confidence="0.99831725">
The algorithm converges when the set of points
labelled as exemplars remains unchanged for a pre-
determined number of iterations. When the al-
gorithm terminates, messages to each variable are
added together. A positive final message indicates
that the most likely value of a variable cij is 1 (point
j is an exemplar for i), a negative message indicates
that it is 0 (j is not an exemplar for i).
</bodyText>
<sectionHeader confidence="0.98815" genericHeader="method">
4 Affinity Propagation for Segmentation
</sectionHeader>
<bodyText confidence="0.987565444444444">
This section explains how we adapt the Affinity
Propagation clustering algorithm to segmentation.
In this setting, sentences are data points and we
refer to exemplars as segment centres. Given a doc-
ument, we want to assign each sentence to a segment
centre so as to maximize net similarity.
The new formulation relies on the same underly-
ing factor graph (Figure 2). A binary variable node
cij is set to 1 iff sentence j is the segment centre for
sentence i. When clustering is the objective, a clus-
ter may consist of points coming from anywhere in
the data sequence. When segmentation is the ob-
jective, a segment must consist of a solid block of
points around the segment centre. Figure 3 shows,
for a toy problem with 5 data points, possible valid
configurations of variables {cij} for clustering (3a)
and for segmentation (3b).
To formalize this new linearity requirement, we
elaborate Equation 4 into Equation 10. Ej evaluates
to −oo in three cases. Case 1 is the original coher-
ence constraint. Case 2 states that no point k may
be in the segment with a centre is j, if k lies before
the start of the segment (the sequence c(s−1)j = 0,
csj = 1 necessarily corresponds to the start of the
segment). Case 3 handles analogously the end of
the segment.
−oo 1. if cjj = 0 n cij = 1 for some i =� j
</bodyText>
<listItem confidence="0.829983">
2. if cjj = 1 n csj = 1 n c(s−1)j = 0
nckj=1 for some s &lt; j, k &lt; s − 1
3. if cjj = 1 n cej = 1 n c(e+1)j = 0
nckj = 1 for some e &gt; j, k &gt; e + 1
0 otherwise
</listItem>
<bodyText confidence="0.993677">
The E function nodes are the only changed part of
the factor graph, so we only must re-derive α mes-
sages (availabilities) sent from factors E to variable
nodes. A function-to-variable message is computed
as shown in Equation 11 (elaborated Equation 3),
and the only incoming messages to E nodes are re-
sponsibilities (ρ messages):
</bodyText>
<equation confidence="0.9884762">
Xµf→x = max (f(x1, ... , xm) + µx,→f) =
N(f)\x x,EN(f)\x
Xmax ((Ej(c1j, ... , cNj) +
cij, i=,4j
cij, i=,4j
</equation>
<bodyText confidence="0.9994755">
We need to compute the message values for the
two possible settings of binary variables – denoted
as αij(1) and αij(0) – and propagate the difference
αij = αij(1) - αij(0).
Consider the case of factor Ej sending an α mes-
sage to the variable node cjj (i.e., i = j). If cjj = 0
then point j is not its own segment centre and the
only valid configuration is to set all other cij to 0:
</bodyText>
<equation confidence="0.992437">
Xαjj(0) = max (Ej(c1j,...,cNj) +
cij,i=&apos;4j
cij,i�=j
X=
i7�j
</equation>
<bodyText confidence="0.991286777777778">
To compute αij(1) (point j is its own segment
centre), we only must maximize over configurations
which will not correspond to cases 2 and 3 in Equa-
tion 10 (other assignments are trivially non-optimal
because they would evaluate Ej to −oo). Let the
start of a segment be s, 1 &lt; s &lt; j and the end of
the segment be e, j + 1 &lt; e &lt; N. We only need to
consider configurations such that all points between
s and e are in the segment while all others are not.
</bodyText>
<figure confidence="0.7532465">
Ej = ⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
ρij(cij)))
ρij(cij))
ρij(0)
</figure>
<page confidence="0.9735055">
288
289
</page>
<equation confidence="0.956350463636364">
s − 1
X
k=1
Xi− 1
k=s
pkj(0) +
pkj(1)]+
(15)
αij, i&lt;j(1) = imax
s=1 [
X j
k=i+1
Xe
k=j+1
XN
k=e+1
pkj(0)]
pkj(1) +
pkj(1) + N max
e=j [
j−1
X
k=s
s− 1
X
k=1
pkj(0) +
pkj(1)]+
(16)
αij, i&gt;j(1) = jmax
s=1 [
Xi− 1
k=j
Xe
k=i+1
XN
k=e+1
pkj(0)]
pkj(1) +
pkj(1) + N max
e=i [
Xαij(0) = max(
k/∈i,j
pkj(0), (17)
j−1
X
k=s
Xi− 1
k=1
s− 1
X
k=i+1
pkj(0) +
pkj(1)]+
pkj(0) + j
s=i+1[
max
Xe
k=j+1
XN
k=e+1
pkj(0)])
pkj(1) +
N
pjj(1) + max[
e=j
pkj(0))
XN
k=i+1
j−1 X
k=s
Xs − 1
k=1
pkj(1)]+ (13)
pkj(0) +
N
max[
e=j
pkj(1) +
pkj(0)]
Xe
k=j+1
XN
k=e+1
Xe
k=j+1
pkj)
Xαij(0) = max(
k/∈i,j
pkj(0), (18)
s − 1
X
k=1
j−1
X
k=s
j
max[
s=1
pkj(0) +
pkj(1)]+
Xi− 1
k=e+1
Xe
k=j+1
pkj(1) +
pkj(0)]
The following picture shows a valid configuration.3
1 s
j e N
</equation>
<bodyText confidence="0.408314">
To compute the message αij(1), i = j, we have:
</bodyText>
<equation confidence="0.803502285714286">
αjj(1) = jmax
s=1 [
Subtracting Equation 12 from Equation 13, we get:
αjj = αjj(1) − αjj(0) = (14)
N
pkj) + max(
e=j
</equation>
<bodyText confidence="0.997739538461539">
Now, consider the case of factor Ej sending an α
message to a variable node cij other than segment
exemplar j (i.e., i =� j). Two subcases are possible:
point i may lie before the segment centre j (i &lt; j),
or it may lie after the segment centre (i &gt; j).
The configurations which may maximize αij(1)
(the message value for setting the hidden variable
to 1) necessarily conform to two conditions: point
j is labelled as a segment centre (cjj = 1) and all
points lying between i and j are in the segment.
This corresponds to Equation 15 for i &lt; j and to
Equation 16 for i &gt; j. Pictorial examples of corre-
sponding valid configurations precede the equations.
</bodyText>
<figure confidence="0.527631">
1 s i j
e N
</figure>
<footnote confidence="0.957311333333333">
3Variables cij set to 1 are shown as shaded circles, to 0 – as
white circles. Normally, variables form a column in the factor
graph; we transpose them to save space.
</footnote>
<equation confidence="0.758202">
1 s
j i e
N
</equation>
<bodyText confidence="0.9979145">
To compute the message value for setting the
hidden variable cij to 0, we again distinguish
between i &lt; j and i &gt; j and consider whether cjj
= 1 or cjj = 0 (point j is / is not a segment centre).
For cjj = 0 the only optimal configuration is cij = 0
for all i =� j. For cjj = 1 the set of possible optimal
configurations is determined by the position of point
i with respect to point j. Following the same logic
as in the previous cases we get Equation 17 for
i &lt; j and Equation 18 for i &gt; j.
</bodyText>
<figure confidence="0.83089275">
1 i s j
e N
1 s
j e i
N
pjj(1) + i−1
max[
e=j
</figure>
<figureCaption confidence="0.793766666666667">
Due to space constraints, we will omit the details
of subtracting Equation 17 from 15 and Equation 18
from 16. The final update rules for both i &lt; j and
</figureCaption>
<figure confidence="0.603183833333333">
j−1 X
k=s
j
max(
s=1
Algorithm 1 Affinity Propagation for Segmentation
</figure>
<listItem confidence="0.96676">
1: input: 1) a set of pairwise similarities {SIM(i, j)}(i,j)E{1,...,N}2, SIM(i, j) E R; 2) a set of prefer-
ences (self-similarities) {SIM(i, i)}iE{1,...,N} indicating a priori likelihood of point i being a segment
centre
2: initialization: ∀i, j : αij = 0 (set all availabilities to 0)
3: repeat
4: iteratively update responsibilities (p) and availabilities (α)
∀i, j : pij = SIM(i, j) + m7�ax(SIM(i, k) − αik)
kj
6: until convergence
7: compute the final configuration of variables: ∀i, j j is the exemplar for i iff pij + αij &gt; 0
8: output: exemplar assignments
</listItem>
<figure confidence="0.992609838235294">
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
pkj) if i = j
j
max(
s=1
j−1 �
k=s
N
pkj) + max(
e=j
e
k=j+1
e
k=j+1
pkj,
pkj +
pkj +
N
max
e=j
i
min [max
s=1
�i− 1
k=s
� j
k=i+1
j
min
s=i+1
∀i,j : αij =
pkj +
pkj] if i &lt; j
�i− 1
k=s
i
max
s=1
�s− 1
k=i+1
j−1 �
k=s
pkj,
pkj +
pkj +
min[jmax
s=1
N
max
e=i
e
k=i+1
�i− 1
k=j
e
k=i+1
pkj] if i &gt; j
pkj +
N
max
e=i
i−1
min
e=j
�i− 1
k=e+1
</figure>
<bodyText confidence="0.990931483870968">
i &gt; j appear in Algorithm 1, where we summarize
the whole process.
The equations look cumbersome but they are triv-
ial to compute. Every summand corresponds to find-
ing the most likely start or end of the segment, tak-
ing into account fixed information. When computing
messages for any given sender node, we can remem-
ber the maximizing values for neighbouring recipi-
ent nodes. For example, after computing the avail-
ability message from factor Ej to cij, we must only
consider one more responsibility value when com-
puting the message from Ej to variable c(i+1)j. The
cost of computing a message is thus negligible.
When the matrix is fully specified, each iteration
requires passing 2N2 messages, so the algorithm
runs in O(N2) time and requires O(N2) memory
(to store the similarities, the availabilities and the
responsibilities). When performing segmentation,
however, the user generally has some idea about
the average or maximum segment length. In such
more realistic cases, the input matrix of similarities
is sparse – it is constructed by sliding a window of
size M. M usually needs to be at least twice the
maximum segment length or thrice the average seg-
ment length. Each iteration, then, involves sending
2MN messages and the storage requirements are
also O(MN).
As is common in loopy belief propagation algo-
rithms, both availability and responsibility messages
are dampened to avoid overshooting and oscillating.
The dampening factor is A where 0.5 ≤ A &lt; 1.
</bodyText>
<note confidence="0.3446145">
newMsg = A ∗ oldMsg + (1 − A)newMsg (19)
The APS algorithm is unsupervised. It only benefits
</note>
<page confidence="0.988504">
290
</page>
<bodyText confidence="0.999853272727273">
from a small development set to fine-tune a few pa-
rameters: preference values and the dampening fac-
tor. APS does not require (nor allow) specifying the
number of segments beforehand. The granularity of
segmentation is adjusted through preference values;
this reflect how likely each sentence is to be selected
as a segment centre. (This translates into the cost of
adding a segment.)
Because each message only requires the knowl-
edge about one column or row of the matrix, the al-
gorithm can be easily parallelized.
</bodyText>
<sectionHeader confidence="0.988721" genericHeader="method">
5 Experimental Setting
</sectionHeader>
<bodyText confidence="0.998660016129032">
Datasets. We evaluate the performance of the
APS algorithm on three datasets. The first, com-
piled by Malioutov and Barzilay (2006), consists
of manually transcribed and segmented lectures on
Artificial Intelligence, 3 development files and 19
test files. The second dataset consists of 227 chap-
ters from medical textbooks (Eisenstein and Barzi-
lay, 2008), 5 of which we use for development. In
this dataset the gold standard segment boundaries
correspond to section breaks specified by the au-
thors. The third dataset consists of 85 works of fic-
tion downloaded from Project Gutenberg, 3 of which
are used for development. The segment boundaries
correspond to chapter breaks or to breaks between
individual stories. They were inserted automatically
using HTML markup in the downloaded files.
The datasets exhibit different characteristics. The
lecture dataset and the fiction dataset are challeng-
ing because they are less cohesive than medical text-
books. The textbooks are cognitively more difficult
to process and the authors rely on repetition of ter-
minology to facilitate comprehension. Since lexical
repetition is the main source of information for text
segmentation, we expect a higher performance on
this dataset. Transcribed speech, on the other hand,
is considerably less cohesive. The lecturer makes an
effort to speak in “plain language” and to be com-
prehensible, relying less on terminology. The use of
pronouns is very common, as is the use of examples.
Repeated use of the same words is also uncom-
mon in fiction. In addition, the dataset was compiled
automatically using HTML markup. The markup
is not always reliable and occasionally the e-book
proofreaders skip it altogether, which potentially
adds noise to the dataset.
Baselines. We compare the performance of
APS with that of two state-of-the-art segmenters: the
Minimum Cut segmenter (Malioutov and Barzilay,
2006) and the Bayesian segmenter (Eisenstein and
Barzilay, 2008). The authors have made Java imple-
mentations publicly available. For the Minimum Cut
segmenter, we select the best parameters using the
script included with that distribution. The Bayesian
segmenter automatically estimates all necessary pa-
rameters from the data.
Preprocessing and the choice of similarity met-
ric. As described in Section 4, the APS algorithm
takes as inputs a matrix of pairwise similarities be-
tween sentences in the document and also, for each
sentence, a preference value.
This paper focuses on comparing globally in-
formed segmentation algorithms, and leaves for fu-
ture work the exploration of best similarity metrics.
To allow fair comparison, then, we use the same
metric as the Minimum Cut segmenter, cosine sim-
ilarity. Each sentence is represented as a vector of
token-type frequencies. Following (Malioutov and
Barzilay, 2006), the frequency vectors are smoothed
by adding counts of words from the adjacent sen-
tences and then weighted using a tf.idf metric (for
details, see ibid.) The similarity between sentence
vectors s1 and s2 is computed as follows:
</bodyText>
<equation confidence="0.997743666666667">
s1 • s2
cos(s1, s2) = (20)
||s1 ||x ||s2||
</equation>
<bodyText confidence="0.999765">
The representation used by the Bayesian segmenter
is too different to be incorporated into our model di-
rectly, but ultimately it is based on the distribution
of unigrams in documents. This is close enough to
our representation to allow fair comparison.
The fiction dataset consists of books: novels or
collections of short stories. Fiction is known to ex-
hibit less lexical cohesion. That is why – when
working on this dataset – we work at the paragraph
level: the similarity is measured not between sen-
tences but between paragraphs. We use this repre-
sentation with all three segmenters.
All parameters have been fine-tuned on the devel-
opment portions of the datasets. For APS algorithm
per se we needed to set three parameters: the size of
the sliding window for similarity computations, the
dampening factor A and the preference values. The
</bodyText>
<page confidence="0.990295">
291
</page>
<table confidence="0.99938975">
BayesSeg MinCutSeg APS
AI 0.443 0.437 0.404
Clinical 0.353 0.382 0.371
Fiction 0.377 0.381 0.350
</table>
<tableCaption confidence="0.702096333333333">
Table 1: Results of segmenting the three datasets us-
ing the Bayesian segmenter, the Minimum Cut seg-
menter and APS.
</tableCaption>
<bodyText confidence="0.999563235294118">
parameters for the similarity metric (best variation
of tf.idf, the window size and the decay factor for
smoothing) were set using the script provided in the
Minimum Cut segmenter’s distribution.
Evaluation metric. We have measured the per-
formance of the segmenters with the WindowDiff
metric (Pevzner and Hearst, 2002). It is computed
by sliding a window through reference and through
segmentation output and, at each window position,
comparing the number of reference breaks to the
number of breaks inserted by the segmenter (hypo-
thetical breaks). It is a penalty measure which re-
ports the number of windows where the reference
and hypothetical breaks do not match, normalized
by the total number of windows. In Equation 21,
ref and hyp denote the number of reference and hy-
pothetical segment breaks within a window.
</bodyText>
<equation confidence="0.485206">
(|ref − hyp |=� 0) (21)
</equation>
<sectionHeader confidence="0.991207" genericHeader="evaluation">
6 Experimental Results and Discussion
</sectionHeader>
<bodyText confidence="0.999882848484848">
Table 1 compares the performance of the three seg-
menters using WindowDiff values. On the lecture
and fiction datasets, the APS segmenter outperforms
the others by a small margin, around 8% over the
better of the two. It is second-best on the clinical
textbook dataset. According to a one-tailed paired
t-test with 95% confidence cut-off, the improvement
is statistically significant only on the fiction dataset.
All datasets are challenging and the baselines are
very competitive, so drawing definitive conclusions
is difficult. Still, we can be fairly confident that
APS performs at least as well as the other two seg-
menters. It also has certain advantages.
One important difference between APS and the
other segmenters is that APS does not require the
number of segments as an input parameter. This is
very helpful, because such information is generally
unavailable in any realistic deployment setting. The
parameters are fine-tuned to maximize WindowDiff
values, so this results in high-precision, low-recall
segment assignments; that is because WindowDiff
favours missing boundaries over near-hits.
APS also outputs segment centres, thus providing
some information about a segment’s topic. We have
not evaluated how descriptive the segment centres
are; this is left for future work.
APS performs slightly better than the other seg-
menters but not by much. We hypothesize that one
of the reasons is that APS relies on the presence of
descriptive segment centres which are not necessar-
ily present for large, coarse-grained segments such
as chapters in novels. It is possible for APS to have
an advantage performing fine-grained segmentation.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999845074074074">
In this paper we have presented APS – a new algo-
rithm for linear text segmentation. APS takes into
account the global structure of the document and
outputs segment boundaries and segment centres. It
scales linearly in the number of input sentences, per-
forms competitively with the state-of-the-art and is
easy to implement. We also provide a Java imple-
mentation of the APS segmenter.
We consider two main directions for future work:
using more informative similarity metrics and mak-
ing the process of segmentation hierarchical. We
chose to use cosine similarity primarily to allow fair
comparison and to judge the algorithm itself, in iso-
lation from the information it uses. Cosine similarity
is a very simple metric which cannot provide an ad-
equate picture of topic fluctuations in documents. It
is likely that dictionary-based or corpus-based simi-
larity measures would yield a major improvement in
performance.
Reliance on descriptive segment centres may
handicap APS’s performance when looking for
coarse-grained segments. One possible remedy is to
look for shorter segments first and then merge them.
One can also modify the algorithm to perform hier-
archical segmentation: consider net similarity with
low-level segment centres as well as with high-level
ones. We plan to explore both possibilities.
</bodyText>
<equation confidence="0.945883">
N−k
1
winDiff =
N
− k
i=1
</equation>
<page confidence="0.995833">
292
</page>
<sectionHeader confidence="0.999099" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996312">
We thank Inmar Givoni for explaining the details
of binary Affinity Propagation and for comment-
ing on our early ideas in this project. Many thanks
to Yongyi Mao for a helpful discussion on the use
Affinity Propagation for text segmentation.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999911479452055">
Bishop, Christopher M. 2006. Pattern Recognition and
Machine Learning. Springer.
Blei, David and Pedro Moreno. 2001. Topic Segmenta-
tion with an Aspect Hidden Markov Model. In Pro-
ceedings of the 24th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 343–348. ACM Press.
Blei, David M., Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Choi, Freddy Y. Y. 2000. Advances in Domain Inde-
pendent Linear Text Segmentation. In Proceedings of
NAACL, pages 26–33.
Eisenstein, Jacob and Regina Barzilay. 2008. Bayesian
Unsupervised Topic Segmentation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 334–343, Honolulu,
Hawaii, October.
Frey, Brendan J. and Delbert Dueck. 2007. Clustering
by Passing Messages Between Data Points. Science,
315:972–976.
Givoni, Inmar E. and Brendan J. Frey. 2009. A Binary
Variable Model for Affinity Propagation. Neural Com-
putation, 21:1589–1600.
Haghighi, Aria and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-Document Summariza-
tion. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 362–370, Boulder, Colorado, June.
Hearst, Marti A. 1997. TextTiling: segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23:33–64, March.
Kschischang, Frank R., Brendan J. Frey, and Hans-A
Loeliger. 2001. Factor graphs and the sum-product
algorithm. In IEEE Transactions on Information The-
ory, Vol 47, No 2, pages 498–519, February.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of ICML-01, pages 282–289.
Malioutov, Igor and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25–
32, Sydney, Australia, July.
Misra, Hemant, Franc¸ois Yvon, Joemon M. Jose, and
Olivier Capp´e. 2009. Text segmentation via topic
modeling: an analytical study. In 18th ACM Con-
ference on Information and Knowledge Management,
pages 1553–1556.
Oh, Hyo-Jung, Sung Hyon Myaeng, and Myung-Gil
Jang. 2007. Semantic passage segmentation based on
sentence topics for question answering. Information
Sciences, an International Journal, 177:3696–3717,
September.
Pearl, Judea. 1982. Reverend Bayes on inference en-
gines: A distributed hierarchical approach. In Pro-
ceedings of the American Association of Artificial In-
telligence National Conference on AI, pages 133–136,
Pittsburgh, PA.
Pevzner, Lev and Marti A. Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 28(1):19–36.
Stoyanov, Veselin and Claire Cardie. 2008. Topic identi-
fication for fine-grained opinion analysis. In COLING
’08 Proceedings of the 22nd International Conference
on Computational Linguistics - Volume 1, pages 817–
824.
Youmans, Gilbert. 1991. A new tool for discourse anal-
ysis: The vocabulary-management profile. Language,
67(4):763–789.
</reference>
<page confidence="0.998967">
293
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896955">
<title confidence="0.99814">Linear Text Segmentation Using Affinity Propagation</title>
<author confidence="0.992499">Anna</author>
<affiliation confidence="0.998831666666667">School of Electrical and Computer University of</affiliation>
<email confidence="0.915745">ankazant@site.uottawa.ca</email>
<abstract confidence="0.999581909090909">This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentaor receives a set of pairwise similarities between data points and produces segboundaries and segment data points which best describe all other data points the segment. passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highresults. linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formulation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results sugthat on par with or outperforms these two very competitive baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="9776" citStr="Bishop, 2006" startWordPosition="1513" endWordPosition="1514">1, H is a set of discrete indices and fh is a local function with arguments Xh C {x1, ... , xn}: �F(x1, ... , xn) = fh(Xh) (1) hEH Factor graphs offer a concise graphical representation for such problems. A global function F which can be decomposed into a sum of M local function fh can be represented as a bi-partite graph with M function nodes and N variable nodes (M = |H|). Figure 1 shows an example of a factor graph for F(x1, x2, x3, x4) = f1(x1, x2, x3) + f2(x2, x3, x4). The factor (or function) nodes are dark squares, the variable nodes are light circles. The well-known max-sum algorithm (Bishop, 2006) seeks a configuration of variables which maximizes the objective function. It finds the maximum in acyclic factor graphs, but in graphs with cycles neither convergence nor optimality are guaranteed (Pearl, 1982). Yet in practice good approximations can be achieved. The max-sum algorithm amounts to propagating messages from function nodes to variable nodes and from variable nodes to function nodes. A message sent from a variable node x to a function node f is computed as a sum of the incoming messages from all neighbours of x other than f (the sum is computed for each possible value of x): �µx</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Pedro Moreno</author>
</authors>
<title>Topic Segmentation with an Aspect Hidden Markov Model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>343--348</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="6116" citStr="Blei and Moreno, 2001" startWordPosition="914" endWordPosition="917">entation revolves around a simple idea: when the topic shifts, so does the vocabulary (Youmans, 1991). We can roughly subdivide existing approaches into two categories: locally informed and globally informed. Locally informed segmenters attempt to identify topic shifts by considering only a small portion of complete document. A classical approach is TextTiling (Hearst, 1997). It consists of sliding two adjacent windows through text and measuring lexical similarity between them. Drops in similarity correspond to topic shifts. Other examples include text segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). Locally informed methods are often very efficient because of lean memory and CPU time requirements. Due to a limited view of the document, however, they can easily be thrown off by short inconsequential digressions in narration. Globally informed methods consider “the big picture” when determining the most likely location of segment boundaries. Choi (2000) applies divisive clustering to segmentation. Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation q</context>
</contexts>
<marker>Blei, Moreno, 2001</marker>
<rawString>Blei, David and Pedro Moreno. 2001. Topic Segmentation with an Aspect Hidden Markov Model. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 343–348. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="7582" citStr="Blei, Ng, and Jordan, 2003" startWordPosition="1139" endWordPosition="1143">t within each segment and minimizes the net weight of severed edges. Such Minimum Cut segmentation resembles APS the most among others mentioned in this paper. The main difference between the two is in different objective functions. Another notable direction in text segmentation uses generative models to find segment boundaries. Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. Segment boundaries are assigned so as to maximize the likelihood of observing the complete sequence. Misra et al. (2009) use a Latent Dirichlet allocation topic model (Blei, Ng, and Jordan, 2003) to find coherent segment boundaries. Such methods output segment boundaries and suggest lexical distribution associated with each segment. Generative models tend to perform well, but are less flexible than the similarity-based models when it comes to incorporating new kinds of information. Globally informed models generally perform better, especially on more challenging datasets such as speech recordings, but they have – unsurprisingly – higher memory and CPU time requirements. The APS algorithm described in this paper combines several desirable properties. It is unsupervised and, unlike mos</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David M., Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in Domain Independent Linear Text Segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="6545" citStr="Choi (2000)" startWordPosition="981" endWordPosition="982">measuring lexical similarity between them. Drops in similarity correspond to topic shifts. Other examples include text segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). Locally informed methods are often very efficient because of lean memory and CPU time requirements. Due to a limited view of the document, however, they can easily be thrown off by short inconsequential digressions in narration. Globally informed methods consider “the big picture” when determining the most likely location of segment boundaries. Choi (2000) applies divisive clustering to segmentation. Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. They cast segmentation as a graphcutting problem. The document is represented as a graph: nodes are sentences and edges are weighted using a measure of lexical similarity. The graph is cut in a way which maximizes the net edge weight within each segment and minimizes the net weight of severed edges. Such Minimum Cut segmentation resembles APS the most among others mentioned in this paper. The main difference between t</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Choi, Freddy Y. Y. 2000. Advances in Domain Independent Linear Text Segmentation. In Proceedings of NAACL, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian Unsupervised Topic Segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>334--343</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="4581" citStr="Eisenstein and Barzilay, 2008" startWordPosition="684" endWordPosition="688">in error-correcting decoding, image processing and data compression. Theoretically, such algorithms are not guaranteed to converge or to maximize the objective function. Yet in practice they often achieve competitive results. APS works on an already pre-compiled similaritiy matrix, so it offers flexibility in the choice of similarity metrics. The desired number of segments can be set by adjusting preferences. We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisenstein and Barzilay, 2008) and identifying chapter breaks in novels. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). The comparison is based on the WindowDiff metric (Pevzner and Hearst, 2002). APS matches or outperforms these very competitive baselines. Section 2 of the paper outlines relevant research on topical text segmentation. Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algorithm for clustering. Section 4 contains the derivation of the new update m</context>
<context position="7317" citStr="Eisenstein and Barzilay (2008)" startWordPosition="1095" endWordPosition="1098">ences improves segmentation quality. They cast segmentation as a graphcutting problem. The document is represented as a graph: nodes are sentences and edges are weighted using a measure of lexical similarity. The graph is cut in a way which maximizes the net edge weight within each segment and minimizes the net weight of severed edges. Such Minimum Cut segmentation resembles APS the most among others mentioned in this paper. The main difference between the two is in different objective functions. Another notable direction in text segmentation uses generative models to find segment boundaries. Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. Segment boundaries are assigned so as to maximize the likelihood of observing the complete sequence. Misra et al. (2009) use a Latent Dirichlet allocation topic model (Blei, Ng, and Jordan, 2003) to find coherent segment boundaries. Such methods output segment boundaries and suggest lexical distribution associated with each segment. Generative models tend to perform well, but are less flexible than the similarity-based models when it comes to incorporating new kinds of information. Globally informed models generally perform</context>
<context position="24541" citStr="Eisenstein and Barzilay, 2008" startWordPosition="4242" endWordPosition="4246">eflect how likely each sentence is to be selected as a segment centre. (This translates into the cost of adding a segment.) Because each message only requires the knowledge about one column or row of the matrix, the algorithm can be easily parallelized. 5 Experimental Setting Datasets. We evaluate the performance of the APS algorithm on three datasets. The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. The second dataset consists of 227 chapters from medical textbooks (Eisenstein and Barzilay, 2008), 5 of which we use for development. In this dataset the gold standard segment boundaries correspond to section breaks specified by the authors. The third dataset consists of 85 works of fiction downloaded from Project Gutenberg, 3 of which are used for development. The segment boundaries correspond to chapter breaks or to breaks between individual stories. They were inserted automatically using HTML markup in the downloaded files. The datasets exhibit different characteristics. The lecture dataset and the fiction dataset are challenging because they are less cohesive than medical textbooks. T</context>
<context position="26128" citStr="Eisenstein and Barzilay, 2008" startWordPosition="4490" endWordPosition="4493">ffort to speak in “plain language” and to be comprehensible, relying less on terminology. The use of pronouns is very common, as is the use of examples. Repeated use of the same words is also uncommon in fiction. In addition, the dataset was compiled automatically using HTML markup. The markup is not always reliable and occasionally the e-book proofreaders skip it altogether, which potentially adds noise to the dataset. Baselines. We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). The authors have made Java implementations publicly available. For the Minimum Cut segmenter, we select the best parameters using the script included with that distribution. The Bayesian segmenter automatically estimates all necessary parameters from the data. Preprocessing and the choice of similarity metric. As described in Section 4, the APS algorithm takes as inputs a matrix of pairwise similarities between sentences in the document and also, for each sentence, a preference value. This paper focuses on comparing globally informed segmentation algorithms, and leaves for future work the ex</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Eisenstein, Jacob and Regina Barzilay. 2008. Bayesian Unsupervised Topic Segmentation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 334–343, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan J Frey</author>
<author>Delbert Dueck</author>
</authors>
<title>Clustering by Passing Messages Between Data Points.</title>
<date>2007</date>
<journal>Science,</journal>
<pages>315--972</pages>
<contexts>
<context position="2072" citStr="Frey and Dueck, 2007" startWordPosition="295" endWordPosition="298">p in such applications as subjectivity analysis (Stoyanov and Cardie, 2008), automatic summarization (Haghighi and Vanderwende, 2009), question answering (Oh, Myaeng, and Jang, 2007) and others. That is why improved quality of text segmentation can benefit other language-processing tasks. Stan Szpakowicz School of Electrical Engineering and Computer Science, University of Ottawa &amp; Institute of Computer Science, Polish Academy of Sciences szpak@site.uottawa.ca We present Affinity Propagation for Segmentation (APS), an adaptation of a state-of-the-art clustering algorithm, Affinity Propagation (Frey and Dueck, 2007; Givoni and Frey, 2009).1 The original AP algorithm considerably improved exemplarbased clustering both in terms of speed and the quality of solutions. That is why we chose to adapt it to segmentation. At its core, APS is suitable for segmenting any sequences of data, but we present it in the context of segmenting documents. APS takes as input a matrix of pairwise similarities between sentences and, for each sentence, a preference value which indicates an a priori belief in how likely a sentence is to be chosen as a segment centre. APS outputs segment assignments and segment centres – data po</context>
<context position="11566" citStr="Frey and Dueck, 2007" startWordPosition="1810" endWordPosition="1813">idence about the distribution of x from function f and its neighbours other than x. A common message-passing schedule on cyclic factor graphs is flooding: iteratively passing all variable-to-function messages, then all function-tovariable messages. Upon convergence, the summary message reflecting final beliefs about the maximizing configuration of variables is computed as a sum of all incoming function-to-variable messages. 3.2 Affinity Propagation The APS algorithm described in this paper is a modification of the original Affinity Propagation algorithm intended for exemplar-based clustering (Frey and Dueck, 2007; Givoni and Frey, 2009). This section describes the binary variable formulation proposed by Givoni and Frey, and lays the groundwork for deriving the new update messages (Section 4). Affinity Propagation for exemplar-based clustering is formulated as follows: to cluster N data points, one must specify a matrix of pairwise similarities {SIM(i,j)}iJE{1,...,N},i=,4j and a set of self-similarities (so-called preferences) SIM(j, j) which reflect a priori beliefs in how likely each data point is to be selected as an exemplar. Preference values occupy the diagonal of the similarity matrix. The algor</context>
</contexts>
<marker>Frey, Dueck, 2007</marker>
<rawString>Frey, Brendan J. and Delbert Dueck. 2007. Clustering by Passing Messages Between Data Points. Science, 315:972–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inmar E Givoni</author>
<author>Brendan J Frey</author>
</authors>
<title>A Binary Variable Model for Affinity Propagation. Neural Computation,</title>
<date>2009</date>
<pages>21--1589</pages>
<contexts>
<context position="2096" citStr="Givoni and Frey, 2009" startWordPosition="299" endWordPosition="302"> as subjectivity analysis (Stoyanov and Cardie, 2008), automatic summarization (Haghighi and Vanderwende, 2009), question answering (Oh, Myaeng, and Jang, 2007) and others. That is why improved quality of text segmentation can benefit other language-processing tasks. Stan Szpakowicz School of Electrical Engineering and Computer Science, University of Ottawa &amp; Institute of Computer Science, Polish Academy of Sciences szpak@site.uottawa.ca We present Affinity Propagation for Segmentation (APS), an adaptation of a state-of-the-art clustering algorithm, Affinity Propagation (Frey and Dueck, 2007; Givoni and Frey, 2009).1 The original AP algorithm considerably improved exemplarbased clustering both in terms of speed and the quality of solutions. That is why we chose to adapt it to segmentation. At its core, APS is suitable for segmenting any sequences of data, but we present it in the context of segmenting documents. APS takes as input a matrix of pairwise similarities between sentences and, for each sentence, a preference value which indicates an a priori belief in how likely a sentence is to be chosen as a segment centre. APS outputs segment assignments and segment centres – data points which best explain </context>
<context position="11590" citStr="Givoni and Frey, 2009" startWordPosition="1814" endWordPosition="1817">ibution of x from function f and its neighbours other than x. A common message-passing schedule on cyclic factor graphs is flooding: iteratively passing all variable-to-function messages, then all function-tovariable messages. Upon convergence, the summary message reflecting final beliefs about the maximizing configuration of variables is computed as a sum of all incoming function-to-variable messages. 3.2 Affinity Propagation The APS algorithm described in this paper is a modification of the original Affinity Propagation algorithm intended for exemplar-based clustering (Frey and Dueck, 2007; Givoni and Frey, 2009). This section describes the binary variable formulation proposed by Givoni and Frey, and lays the groundwork for deriving the new update messages (Section 4). Affinity Propagation for exemplar-based clustering is formulated as follows: to cluster N data points, one must specify a matrix of pairwise similarities {SIM(i,j)}iJE{1,...,N},i=,4j and a set of self-similarities (so-called preferences) SIM(j, j) which reflect a priori beliefs in how likely each data point is to be selected as an exemplar. Preference values occupy the diagonal of the similarity matrix. The algorithm then assigns each d</context>
<context position="13719" citStr="Givoni and Frey (2009)" startWordPosition="2180" endWordPosition="2183"> points i and j): ( = SIM (i, j) if cij = 1 Sij (cij) (6) 0 otherwise Equation 7 shows the objective function which we want to maximize: a sum of similarities between data points and their exemplars, subject to the two constraints (coherence and single-cluster per point). According to Equation 3, the computation of a single factor-to-variable message involves maximizing over 2n configurations. E and I, however, are binary constraints and evaluate to −oo for most configurations. This drastically reduces the number of configurations which can maximize the message values. Given this simple fact, Givoni and Frey (2009) show how to reduce the necessary update messages to only two types of scalar ones: availabilities (α) and responsibilities (ρ).2 A responsibility message ρij, sent from a variable node cij to function node Ej, reflects the evidence of how likely j is to be an exemplar for i given all other potential exemplars: ρij = SIM(i, j) − m=,4ax(SIM(i, k) + αik) (8) kj An availability message αij, sent from a function node Ej to a variable node cij, reflects how likely point j is to be an exemplar for i given the evidence from all other data points: (9) Let γij(l) be the message value corresponding to s</context>
</contexts>
<marker>Givoni, Frey, 2009</marker>
<rawString>Givoni, Inmar E. and Brendan J. Frey. 2009. A Binary Variable Model for Affinity Propagation. Neural Computation, 21:1589–1600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring Content Models for Multi-Document Summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1585" citStr="Haghighi and Vanderwende, 2009" startWordPosition="228" endWordPosition="231">segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines. 1 Introduction In complex narratives, it is typical for the topic to shift continually. Some shifts are gradual, others – more abrupt. Topical text segmentation identifies the more noticeable topic shifts. A topical segmenter’s output is a very simple picture of the document’s structure. Segmentation is a useful intermediate step in such applications as subjectivity analysis (Stoyanov and Cardie, 2008), automatic summarization (Haghighi and Vanderwende, 2009), question answering (Oh, Myaeng, and Jang, 2007) and others. That is why improved quality of text segmentation can benefit other language-processing tasks. Stan Szpakowicz School of Electrical Engineering and Computer Science, University of Ottawa &amp; Institute of Computer Science, Polish Academy of Sciences szpak@site.uottawa.ca We present Affinity Propagation for Segmentation (APS), an adaptation of a state-of-the-art clustering algorithm, Affinity Propagation (Frey and Dueck, 2007; Givoni and Frey, 2009).1 The original AP algorithm considerably improved exemplarbased clustering both in terms</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Haghighi, Aria and Lucy Vanderwende. 2009. Exploring Content Models for Multi-Document Summarization. In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--33</pages>
<contexts>
<context position="5871" citStr="Hearst, 1997" startWordPosition="879" endWordPosition="880">ports the results, Section 7 discusses conclusions and future work. 2 Related Work This sections discusses selected text segmentation methods and positions the proposed APS algorithm in that context. Most research on automatic text segmentation revolves around a simple idea: when the topic shifts, so does the vocabulary (Youmans, 1991). We can roughly subdivide existing approaches into two categories: locally informed and globally informed. Locally informed segmenters attempt to identify topic shifts by considering only a small portion of complete document. A classical approach is TextTiling (Hearst, 1997). It consists of sliding two adjacent windows through text and measuring lexical similarity between them. Drops in similarity correspond to topic shifts. Other examples include text segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). Locally informed methods are often very efficient because of lean memory and CPU time requirements. Due to a limited view of the document, however, they can easily be thrown off by short inconsequential digressions in narration. Globally informed methods consider “the big picture” wh</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, Marti A. 1997. TextTiling: segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23:33–64, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank R Kschischang</author>
<author>Brendan J Frey</author>
<author>Hans-A Loeliger</author>
</authors>
<title>Factor graphs and the sum-product algorithm.</title>
<date>2001</date>
<journal>In IEEE Transactions on Information Theory,</journal>
<volume>47</volume>
<pages>498--519</pages>
<contexts>
<context position="2966" citStr="Kschischang, Frey, and Loeliger, 2001" startWordPosition="446" endWordPosition="450">ces of data, but we present it in the context of segmenting documents. APS takes as input a matrix of pairwise similarities between sentences and, for each sentence, a preference value which indicates an a priori belief in how likely a sentence is to be chosen as a segment centre. APS outputs segment assignments and segment centres – data points which best explain all other points in a segment. The algorithm attempts to maximize net similarity – the sum of similarities between all data points and their respective segment centres. APS operates by iteratively passing messages in a factor graph (Kschischang, Frey, and Loeliger, 2001) until a good set of segments emerges. Each iteration considers all similarities – takes into account all available information. An iteration includes sending at most O(N2) messages. For the majority of realistic segmentation tasks, however, the upper bound is O(MN) messages, where M is a constant. This is more computationally expensive than the requirements of locally informed segmentation algorithms such as those based on HMM or CRF (see Section 2), but for a globallyinformed algorithm the requirements are very reasonable. APS is an instance of loopy-belief propagation (belief propagation o</context>
</contexts>
<marker>Kschischang, Frey, Loeliger, 2001</marker>
<rawString>Kschischang, Frank R., Brendan J. Frey, and Hans-A Loeliger. 2001. Factor graphs and the sum-product algorithm. In IEEE Transactions on Information Theory, Vol 47, No 2, pages 498–519, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="6184" citStr="Lafferty, McCallum, and Pereira, 2001" startWordPosition="922" endWordPosition="926">c shifts, so does the vocabulary (Youmans, 1991). We can roughly subdivide existing approaches into two categories: locally informed and globally informed. Locally informed segmenters attempt to identify topic shifts by considering only a small portion of complete document. A classical approach is TextTiling (Hearst, 1997). It consists of sliding two adjacent windows through text and measuring lexical similarity between them. Drops in similarity correspond to topic shifts. Other examples include text segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). Locally informed methods are often very efficient because of lean memory and CPU time requirements. Due to a limited view of the document, however, they can easily be thrown off by short inconsequential digressions in narration. Globally informed methods consider “the big picture” when determining the most likely location of segment boundaries. Choi (2000) applies divisive clustering to segmentation. Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. They cast segmentation as a graphcutting problem. The docume</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML-01, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum Cut Model for Spoken Lecture Segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4506" citStr="Malioutov and Barzilay, 2006" startWordPosition="675" endWordPosition="678">putational Linguistics been used to achieved state-of-the-art performance in error-correcting decoding, image processing and data compression. Theoretically, such algorithms are not guaranteed to converge or to maximize the objective function. Yet in practice they often achieve competitive results. APS works on an already pre-compiled similaritiy matrix, so it offers flexibility in the choice of similarity metrics. The desired number of segments can be set by adjusting preferences. We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisenstein and Barzilay, 2008) and identifying chapter breaks in novels. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). The comparison is based on the WindowDiff metric (Pevzner and Hearst, 2002). APS matches or outperforms these very competitive baselines. Section 2 of the paper outlines relevant research on topical text segmentation. Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algo</context>
<context position="6620" citStr="Malioutov and Barzilay (2006)" startWordPosition="988" endWordPosition="991">arity correspond to topic shifts. Other examples include text segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). Locally informed methods are often very efficient because of lean memory and CPU time requirements. Due to a limited view of the document, however, they can easily be thrown off by short inconsequential digressions in narration. Globally informed methods consider “the big picture” when determining the most likely location of segment boundaries. Choi (2000) applies divisive clustering to segmentation. Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. They cast segmentation as a graphcutting problem. The document is represented as a graph: nodes are sentences and edges are weighted using a measure of lexical similarity. The graph is cut in a way which maximizes the net edge weight within each segment and minimizes the net weight of severed edges. Such Minimum Cut segmentation resembles APS the most among others mentioned in this paper. The main difference between the two is in different objective functions. Another notable direction in te</context>
<context position="24318" citStr="Malioutov and Barzilay (2006)" startWordPosition="4210" endWordPosition="4213">une a few parameters: preference values and the dampening factor. APS does not require (nor allow) specifying the number of segments beforehand. The granularity of segmentation is adjusted through preference values; this reflect how likely each sentence is to be selected as a segment centre. (This translates into the cost of adding a segment.) Because each message only requires the knowledge about one column or row of the matrix, the algorithm can be easily parallelized. 5 Experimental Setting Datasets. We evaluate the performance of the APS algorithm on three datasets. The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. The second dataset consists of 227 chapters from medical textbooks (Eisenstein and Barzilay, 2008), 5 of which we use for development. In this dataset the gold standard segment boundaries correspond to section breaks specified by the authors. The third dataset consists of 85 works of fiction downloaded from Project Gutenberg, 3 of which are used for development. The segment boundaries correspond to chapter breaks or to breaks between individual stories. They were inserte</context>
<context position="26069" citStr="Malioutov and Barzilay, 2006" startWordPosition="4482" endWordPosition="4485">nd, is considerably less cohesive. The lecturer makes an effort to speak in “plain language” and to be comprehensible, relying less on terminology. The use of pronouns is very common, as is the use of examples. Repeated use of the same words is also uncommon in fiction. In addition, the dataset was compiled automatically using HTML markup. The markup is not always reliable and occasionally the e-book proofreaders skip it altogether, which potentially adds noise to the dataset. Baselines. We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). The authors have made Java implementations publicly available. For the Minimum Cut segmenter, we select the best parameters using the script included with that distribution. The Bayesian segmenter automatically estimates all necessary parameters from the data. Preprocessing and the choice of similarity metric. As described in Section 4, the APS algorithm takes as inputs a matrix of pairwise similarities between sentences in the document and also, for each sentence, a preference value. This paper focuses on comparing globally informed</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Malioutov, Igor and Regina Barzilay. 2006. Minimum Cut Model for Spoken Lecture Segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 25– 32, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hemant Misra</author>
<author>Franc¸ois Yvon</author>
<author>Joemon M Jose</author>
<author>Olivier Capp´e</author>
</authors>
<title>Text segmentation via topic modeling: an analytical study.</title>
<date>2009</date>
<booktitle>In 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>1553--1556</pages>
<marker>Misra, Yvon, Jose, Capp´e, 2009</marker>
<rawString>Misra, Hemant, Franc¸ois Yvon, Joemon M. Jose, and Olivier Capp´e. 2009. Text segmentation via topic modeling: an analytical study. In 18th ACM Conference on Information and Knowledge Management, pages 1553–1556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyo-Jung Oh</author>
<author>Sung Hyon Myaeng</author>
<author>Myung-Gil Jang</author>
</authors>
<title>Semantic passage segmentation based on sentence topics for question answering.</title>
<date>2007</date>
<journal>Information Sciences, an International Journal,</journal>
<pages>177--3696</pages>
<contexts>
<context position="1633" citStr="Oh, Myaeng, and Jang, 2007" startWordPosition="235" endWordPosition="239">segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines. 1 Introduction In complex narratives, it is typical for the topic to shift continually. Some shifts are gradual, others – more abrupt. Topical text segmentation identifies the more noticeable topic shifts. A topical segmenter’s output is a very simple picture of the document’s structure. Segmentation is a useful intermediate step in such applications as subjectivity analysis (Stoyanov and Cardie, 2008), automatic summarization (Haghighi and Vanderwende, 2009), question answering (Oh, Myaeng, and Jang, 2007) and others. That is why improved quality of text segmentation can benefit other language-processing tasks. Stan Szpakowicz School of Electrical Engineering and Computer Science, University of Ottawa &amp; Institute of Computer Science, Polish Academy of Sciences szpak@site.uottawa.ca We present Affinity Propagation for Segmentation (APS), an adaptation of a state-of-the-art clustering algorithm, Affinity Propagation (Frey and Dueck, 2007; Givoni and Frey, 2009).1 The original AP algorithm considerably improved exemplarbased clustering both in terms of speed and the quality of solutions. That is </context>
</contexts>
<marker>Oh, Myaeng, Jang, 2007</marker>
<rawString>Oh, Hyo-Jung, Sung Hyon Myaeng, and Myung-Gil Jang. 2007. Semantic passage segmentation based on sentence topics for question answering. Information Sciences, an International Journal, 177:3696–3717, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Reverend Bayes on inference engines: A distributed hierarchical approach.</title>
<date>1982</date>
<booktitle>In Proceedings of the American Association of Artificial Intelligence National Conference on AI,</booktitle>
<pages>133--136</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="9988" citStr="Pearl, 1982" startWordPosition="1545" endWordPosition="1546"> function F which can be decomposed into a sum of M local function fh can be represented as a bi-partite graph with M function nodes and N variable nodes (M = |H|). Figure 1 shows an example of a factor graph for F(x1, x2, x3, x4) = f1(x1, x2, x3) + f2(x2, x3, x4). The factor (or function) nodes are dark squares, the variable nodes are light circles. The well-known max-sum algorithm (Bishop, 2006) seeks a configuration of variables which maximizes the objective function. It finds the maximum in acyclic factor graphs, but in graphs with cycles neither convergence nor optimality are guaranteed (Pearl, 1982). Yet in practice good approximations can be achieved. The max-sum algorithm amounts to propagating messages from function nodes to variable nodes and from variable nodes to function nodes. A message sent from a variable node x to a function node f is computed as a sum of the incoming messages from all neighbours of x other than f (the sum is computed for each possible value of x): �µx→f = µf&apos;→x (2) f&apos;EN(x)\f Figure 1: Factor graph for F(x1, x2, x3, x4) = f1(x1, x2, x3) + f2(x2, x3, x4). f1 f2 x1 x2 x3 x4 N(x) is the set of all function nodes which are x’s neighbours. The message reflects the </context>
</contexts>
<marker>Pearl, 1982</marker>
<rawString>Pearl, Judea. 1982. Reverend Bayes on inference engines: A distributed hierarchical approach. In Proceedings of the American Association of Artificial Intelligence National Conference on AI, pages 133–136, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A Critique and Improvement of an Evaluation Metric for Text Segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="4856" citStr="Pevzner and Hearst, 2002" startWordPosition="726" endWordPosition="729">x, so it offers flexibility in the choice of similarity metrics. The desired number of segments can be set by adjusting preferences. We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisenstein and Barzilay, 2008) and identifying chapter breaks in novels. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). The comparison is based on the WindowDiff metric (Pevzner and Hearst, 2002). APS matches or outperforms these very competitive baselines. Section 2 of the paper outlines relevant research on topical text segmentation. Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algorithm for clustering. Section 4 contains the derivation of the new update messages for APSeg. Section 5 describes the experimental setting, Section 6 reports the results, Section 7 discusses conclusions and future work. 2 Related Work This sections discusses selected text segmentation methods and positions the proposed APS algorithm in that context</context>
<context position="28628" citStr="Pevzner and Hearst, 2002" startWordPosition="4894" endWordPosition="4897">window for similarity computations, the dampening factor A and the preference values. The 291 BayesSeg MinCutSeg APS AI 0.443 0.437 0.404 Clinical 0.353 0.382 0.371 Fiction 0.377 0.381 0.350 Table 1: Results of segmenting the three datasets using the Bayesian segmenter, the Minimum Cut segmenter and APS. parameters for the similarity metric (best variation of tf.idf, the window size and the decay factor for smoothing) were set using the script provided in the Minimum Cut segmenter’s distribution. Evaluation metric. We have measured the performance of the segmenters with the WindowDiff metric (Pevzner and Hearst, 2002). It is computed by sliding a window through reference and through segmentation output and, at each window position, comparing the number of reference breaks to the number of breaks inserted by the segmenter (hypothetical breaks). It is a penalty measure which reports the number of windows where the reference and hypothetical breaks do not match, normalized by the total number of windows. In Equation 21, ref and hyp denote the number of reference and hypothetical segment breaks within a window. (|ref − hyp |=� 0) (21) 6 Experimental Results and Discussion Table 1 compares the performance of th</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Pevzner, Lev and Marti A. Hearst. 2002. A Critique and Improvement of an Evaluation Metric for Text Segmentation. Computational Linguistics, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
</authors>
<title>Topic identification for fine-grained opinion analysis.</title>
<date>2008</date>
<booktitle>In COLING ’08 Proceedings of the 22nd International Conference on Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>817--824</pages>
<contexts>
<context position="1527" citStr="Stoyanov and Cardie, 2008" startWordPosition="221" endWordPosition="225">lation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines. 1 Introduction In complex narratives, it is typical for the topic to shift continually. Some shifts are gradual, others – more abrupt. Topical text segmentation identifies the more noticeable topic shifts. A topical segmenter’s output is a very simple picture of the document’s structure. Segmentation is a useful intermediate step in such applications as subjectivity analysis (Stoyanov and Cardie, 2008), automatic summarization (Haghighi and Vanderwende, 2009), question answering (Oh, Myaeng, and Jang, 2007) and others. That is why improved quality of text segmentation can benefit other language-processing tasks. Stan Szpakowicz School of Electrical Engineering and Computer Science, University of Ottawa &amp; Institute of Computer Science, Polish Academy of Sciences szpak@site.uottawa.ca We present Affinity Propagation for Segmentation (APS), an adaptation of a state-of-the-art clustering algorithm, Affinity Propagation (Frey and Dueck, 2007; Givoni and Frey, 2009).1 The original AP algorithm co</context>
</contexts>
<marker>Stoyanov, Cardie, 2008</marker>
<rawString>Stoyanov, Veselin and Claire Cardie. 2008. Topic identification for fine-grained opinion analysis. In COLING ’08 Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, pages 817– 824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilbert Youmans</author>
</authors>
<title>A new tool for discourse analysis: The vocabulary-management profile.</title>
<date>1991</date>
<journal>Language,</journal>
<volume>67</volume>
<issue>4</issue>
<contexts>
<context position="5595" citStr="Youmans, 1991" startWordPosition="839" endWordPosition="840">ext segmentation. Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algorithm for clustering. Section 4 contains the derivation of the new update messages for APSeg. Section 5 describes the experimental setting, Section 6 reports the results, Section 7 discusses conclusions and future work. 2 Related Work This sections discusses selected text segmentation methods and positions the proposed APS algorithm in that context. Most research on automatic text segmentation revolves around a simple idea: when the topic shifts, so does the vocabulary (Youmans, 1991). We can roughly subdivide existing approaches into two categories: locally informed and globally informed. Locally informed segmenters attempt to identify topic shifts by considering only a small portion of complete document. A classical approach is TextTiling (Hearst, 1997). It consists of sliding two adjacent windows through text and measuring lexical similarity between them. Drops in similarity correspond to topic shifts. Other examples include text segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). Locally </context>
</contexts>
<marker>Youmans, 1991</marker>
<rawString>Youmans, Gilbert. 1991. A new tool for discourse analysis: The vocabulary-management profile. Language, 67(4):763–789.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>