<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.999578333333333">
Taking into Account the Differences between Actively and Passively
Acquired Data: The Case of Active Learning with Support Vector Machines
for Imbalanced Datasets
</title>
<author confidence="0.988387">
Michael Bloodgood∗
</author>
<affiliation confidence="0.85038">
Human Language Technology
Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.814329">
Baltimore, MD 21211 USA
</address>
<email confidence="0.999">
bloodgood@jhu.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999163">
Actively sampled data can have very different
characteristics than passively sampled data.
Therefore, it’s promising to investigate using
different inference procedures during AL than
are used during passive learning (PL). This
general idea is explored in detail for the fo-
cused case of AL with cost-weighted SVMs
for imbalanced data, a situation that arises for
many HLT tasks. The key idea behind the
proposed InitPA method for addressing im-
balance is to base cost models during AL on
an estimate of overall corpus imbalance com-
puted via a small unbiased sample rather than
the imbalance in the labeled training data,
which is the leading method used during PL.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999479923076923">
Recently there has been considerable interest in us-
ing active learning (AL) to reduce HLT annotation
burdens. Actively sampled data can have differ-
ent characteristics than passively sampled data and
therefore, this paper proposes modifying algorithms
used to infer models during AL. Since most AL re-
search assumes the same learning algorithms will be
used during AL as during passive learning1 (PL),
this paper opens up a new thread of AL research that
accounts for the differences between passively and
actively sampled data.
The specific case focused on in this paper is
that of AL with SVMs (AL-SVM) for imbalanced
</bodyText>
<footnote confidence="0.9005145">
∗ This research was conducted while the first author was a
PhD student at the University of Delaware.
1Passive learning refers to the typical supervised learning
setup where the learner does not actively select its training data.
</footnote>
<author confidence="0.465938">
K. Vijay-Shanker
</author>
<affiliation confidence="0.55353275">
Computer and Information
Sciences Department
University of Delaware
Newark, DE 19716 USA
</affiliation>
<email confidence="0.880284">
vijay@cis.udel.edu
</email>
<bodyText confidence="0.999778090909091">
datasets2. Collectively, the factors: interest in AL,
widespread class imbalance for many HLT tasks, in-
terest in using SVMs, and PL research showing that
SVM performance can be improved substantially by
addressing imbalance, indicate the importance of the
case of AL with SVMs with imbalanced data.
Extensive PL research has shown that learning
algorithms’ performance degrades for imbalanced
datasets and techniques have been developed that
prevent this degradation. However, to date, rela-
tively little work has addressed imbalance during AL
(see Section 2). In contrast to previous work, this
paper advocates that the AL scenario brings out the
need to modify PL approaches to dealing with im-
balance. In particular, a new method is developed
for cost-weighted SVMs that estimates a cost model
based on overall corpus imbalance rather than the
imbalance in the so far labeled training data. Sec-
tion 2 discusses related work, Section 3 discusses
the experimental setup, Section 4 presents the new
method called InitPA, Section 5 evaluates InitPA,
and Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997957">
A problem with imbalanced data is that the class
boundary (hyperplane) learned by SVMs can be too
close to the positive (pos) examples and then recall
suffers. Many approaches have been presented for
overcoming this problem in the PL setting. Many
require substantially longer training times or ex-
</bodyText>
<footnote confidence="0.99671975">
2This paper focuses on the fundamental case of binary clas-
sification where class imbalance arises because the positive ex-
amples are rarer than the negative examples, a situation that nat-
urally arises for many HLT tasks.
</footnote>
<page confidence="0.924156">
137
</page>
<note confidence="0.3729405">
Proceedings of NAACL HLT 2009: Short Papers, pages 137–140,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996395">
tra training data to tune parameters and thus are
not ideal for use during AL. Cost-weighted SVMs
(cwSVMs), on the other hand, are a promising ap-
proach for use with AL: they impose no extra train-
ing overhead. cwSVMs introduce unequal cost fac-
tors so the optimization problem solved becomes:
</bodyText>
<equation confidence="0.9481825">
Minimize:
1
Subject to:
∀k : yk [~w · ~xk + b] ≥ 1 − ξk, (2)
</equation>
<bodyText confidence="0.984630846153846">
where (~w, b) represents the learned hyperplane, ~xk
is the feature vector for example k, yk is the label
for example k, ξk = max(0,1 − yk(~wk · ~xk + b))
is the slack variable for example k, and C+ and C−
are user-defined cost factors.
The most important part for this paper are the cost
factors C+ and C−. The ratio C+ C− quantifies the
importance of reducing slack error on pos train ex-
amples relative to reducing slack error on negative
(neg) train examples. The value of the ratio is cru-
cial for balancing the precision recall tradeoff well.
(Morik et al., 1999) showed that during PL, set-
ting les
</bodyText>
<equation confidence="0.644461">
C± = # of pos training egamples is an effec-
</equation>
<bodyText confidence="0.999972">
tive heuristic. Section 4 explores using this heuris-
tic during AL and explains a modified heuristic that
could work better during AL.
(Ertekin et al., 2007) propose using the balancing
of training data that occurs as a result of AL-SVM
to handle imbalance and do not use any further mea-
sures to address imbalance. (Zhu and Hovy, 2007)
used resampling to address imbalance and based the
amount of resampling, which is the analog of our
cost model, on the amount of imbalance in the cur-
rent set of labeled train data, as PL approaches do.
In contrast, the InitPA approach in Section 4 bases
its cost models on overall (unlabeled) corpus imbal-
ance rather than the amount of imbalance in the cur-
rent set of labeled data.
</bodyText>
<sectionHeader confidence="0.997927" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999955">
We use relation extraction (RE) and text classifica-
tion (TC) datasets and SVMlight (Joachims, 1999)
for training the SVMs. For RE, we use AImed,
previously used to train protein interaction extrac-
tion systems ((Giuliano et al., 2006)). As in previ-
ous work, we cast RE as a binary classification task
</bodyText>
<figureCaption confidence="0.988399">
Figure 1: Hyperplane B was trained with a higher C+
</figureCaption>
<bodyText confidence="0.956790333333333">
C−
ratio than hyperplane A was trained with.
(14.94% of the examples in AImed are positive). We
use the KGC kernel from (Giuliano et al., 2006), one
of the highest-performing systems on AImed to date
and perform 10-fold cross validation. For TC, we
use the Reuters-21578 ModApte split. Since a doc-
ument may belong to more than one category, each
category is treated as a separate binary classification
problem, as in (Joachims, 1998). As in (Joachims,
1998), we use the ten largest categories, which have
imbalances ranging from 1.88% to 29.96%.
</bodyText>
<sectionHeader confidence="0.996638" genericHeader="method">
4 AL-SVM Methods for Addressing Class
Imbalance
</sectionHeader>
<bodyText confidence="0.996268842105263">
The key question when using cwSVMs is how to set
the ratio C+ C− . Increasing it will typically shift the
learned hyperplane so recall is increased and preci-
sion is decreased (see Figure 1 for a hypothetical ex-
ample). Let PA= C+ C−.3 How should the PA be set
during AL-SVM?
We propose two approaches: one sets the PA
based on the level of imbalance in the labeled train-
ing data and one aims to set the PA based on an es-
timate of overall corpus imbalance, which can dras-
tically differ from the level of imbalance in actively
sampled training data. The first method is called
CurrentPA, depicted in Figure 2. Note that in step
0 of the loop, PA is set based on the distribution of
positive and negative examples in the current set of
labeled data. However, observe that during AL the
ratio # neg labeled examples in the current set of la-
# pos labeled examples
beled data gets skewed from the ratio in the entire
</bodyText>
<footnote confidence="0.876994">
3PA stands for positive amplification and gives us a concise
way to denote the fraction C+
</footnote>
<figure confidence="0.5396143125">
C− , which doesn’t have a standard
name.
2k~w
k
2 + C+ E
i:yi=+1
� ξi (1)
ξi + C−
i:yi=−1
138
Input:
L = small initial set of labeled data
U = large pool of unlabeled data
Loop until stopping criterion is met:
0. Set PA = |{x∈Labeled:f(x)=−1}|
|{x∈L:f(x)=+1}|
</figure>
<bodyText confidence="0.991314">
where f is the function we desire to learn.
</bodyText>
<listItem confidence="0.964368142857143">
1. Train an SVM with C+ and C− set such
that C+C− = PA and obtain hyperplane h .4
2. batch ← select k points from U that are
closest to h and request their labels.5
3. U = U − batch .
4. L = L ∪ batch.
End Loop
</listItem>
<figureCaption confidence="0.997607">
Figure 2: The CurrentPA algorithm
</figureCaption>
<figure confidence="0.790136">
Empirical Evidence of CurrentPA creating a Skewed Distribution (Fold Avg)
Number of Points for which Annotations Have Been Requested
</figure>
<figureCaption confidence="0.888804857142857">
Figure 3: Illustration of AL skewing the distribution of
pos/neg points on AImed.
corpus because AL systematically selects the exam-
ples that are closest to the current model’s hyper-
plane and this tends to select more positive exam-
ples than random selection would select (see also
(Ertekin et al., 2007)).
</figureCaption>
<bodyText confidence="0.999453555555556">
Empirical evidence of this distribution skew is il-
lustrated in Figure 3. The trend toward balanced
datasets during AL could mislead and cause us to
underestimate the PA.
Therefore, our next algorithm aims to set the PA
based on the ratio of neg to pos instances in the en-
tire corpus. However, since we don’t have labels for
the entire corpus, we don’t know this ratio. But by
using a small initial sample of labeled data, we can
</bodyText>
<footnote confidence="0.996976">
4We use SVMlight’s default value for C−.
5In our experiments, batch size is 20.
</footnote>
<table confidence="0.3828505">
AImed Average F Measure versus Number of Annotations
Number of Points for which Annotations Have Been Requested
</table>
<figureCaption confidence="0.9973025">
Figure 4: AImed learning curves. y-axis is from 20% to
60%.
</figureCaption>
<bodyText confidence="0.9999156">
estimate this ratio with high confidence. This esti-
mate can then be used for setting the PA throughout
the AL process. We call this method of setting the
PA based on a small initial set of labeled data the
InitPA method. It is like CurrentPA except we move
Step 0 to be executed one time before the loop and
then use that same PA value on each iteration of the
AL loop.
To guide what size to make the initial set of la-
beled data, one can determine the sample size re-
quired to estimate the proportion of positives in a
finite population to within sampling error e with a
desired level of confidence using standard statisti-
cal techniques found in many college-level statistics
references such as (Berenson et al., 1988). For ex-
ample, carrying out the computations on the AImed
dataset shows that a size of 100 enables us to be
95% confident that our proportion estimate is within
0.0739 of the true proportion. In our experiments,
we used an initial labeled set of size 100.
</bodyText>
<sectionHeader confidence="0.995088" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.997014727272727">
In addition to InitPA and CurrentPA, we also imple-
mented the methods from (Ertekin et al., 2007; Zhu
and Hovy, 2007). We implemented oversampling by
duplicating points and by BootOS (Zhu and Hovy,
2007). To avoid cluttering the graphs, we only show
the highest-performing oversampling variant, which
was by duplicating points. Learning curves are pre-
sented in Figures 4 and 5.
Note InitPA is the highest-performing method for
all datasets, especially in the practically important
area of where the learning curves begin to plateau.
</bodyText>
<figure confidence="0.972213096774194">
Ratio of # of Negative to # of Positive Points
5.5
4.5
3.5
2.5
1.5
0
5
4
3
2
500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Ratio with CurrentPA
Ratio with Entire Set
Performance (F Measure)
60
55
50
45
40
35
30
25
200 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
InitPA
Oversampling(Zhu and Hovy,2007)
CurrentPA
EHG2007(Ertekin et al, 2007)
139
Reuters Average F Measure versus Number of Annotations
Number of Points for which Annotations Have Been Requested
</figure>
<figureCaption confidence="0.9951045">
Figure 5: Reuters learning curves. y-axis is from 76% to
83%.
</figureCaption>
<bodyText confidence="0.999984333333333">
This area is important because this is around where
we would want to stop AL (Bloodgood and Vijay-
Shanker, 2009).
Observe that the gains of InitPA over CurrentPA
are smaller for Reuters. For some Reuters cate-
gories, InitPA and CurrentPA have nearly identical
performance. Applying the models learned by Cur-
rentPA at each round of AL on the data used to
train the model reveals that the recall on the train-
ing data is nearly 100% for those categories where
InitPA/CurrentPA perform similarly. Increasing the
relative penalty for slack error on positive training
points will not have much impact if (nearly) all of
the pos train points are already classified correctly.
Thus, in situations where models are already achiev-
ing nearly 100% recall on their train data, InitPA is
not expected to outperform CurrentPA.
The hyperplanes learned during AL-SVM serve
two purposes: sampling - they govern which unla-
beled points will be selected for human annotation,
and predicting - when AL stops, the most recently
learned hyperplane is used for classifying test data.
Although all AL-SVM approaches we’re aware of
use the same hyperplane at each round of AL for
both of these purposes, this is not required. We com-
pared InitPA with hybrid approaches where hyper-
planes trained using an InitPA cost model are used
for sampling and hyperplanes trained using a Cur-
rentPA cost model are used for predicting, and vice-
versa, and found that InitPA performed better than
both of these hybrid approaches. This indicates that
the InitPA cost model yields hyperplanes that are
better for both sampling and predicting.
</bodyText>
<sectionHeader confidence="0.999285" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999987944444444">
We’ve made the case for the importance of AL-SVM
for imbalanced datasets and showed that the AL sce-
nario calls for modifications to PL approaches to ad-
dressing imbalance. For AL-SVM, the key idea be-
hind InitPA is to base cost models on an estimate of
overall corpus imbalance rather than the class imbal-
ance in the so far labeled data. The practical utility
of the InitPA method was demonstrated empirically;
situations where InitPA won’t help that much were
made clear; and analysis showed that the sources of
InitPA’s gains were from both better sampling and
better predictive models.
InitPA is an instantiation of a more general idea
of not using the same inference algorithms during
AL as during PL but instead modifying inference al-
gorithms to suit esoteric characteristics of actively
sampled data. This is an idea that has seen relatively
little exploration and is ripe for further investigation.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9943645">
Mark L. Berenson, David M. Levine, and David Rind-
skopf. 1988. Applied Statistics. Prentice-Hall, Engle-
wood Cliffs, NJ.
Michael Bloodgood and K. Vijay-Shanker. 2009. A
method for stopping active learning based on stabiliz-
ing predictions and the need for user-adjustable stop-
ping. In CoNLL.
Seyda Ertekin, Jian Huang, L´eon Bottou, and C. Lee
Giles. 2007. Learning on the border: active learning
in imbalanced data classification. In CIKM.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In EACL.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In ECML, pages 137–142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods –
Support Vector Learning, pages 169–184.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning with a
knowledge-based approach - a case study in intensive
care monitoring. In ICML, pages 268–277.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In EMNLP-CoNLL.
</reference>
<figure confidence="0.994901142857143">
Performance (F Measure)
83
82
81
80
79
78
77
76
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
InitPA
Oversampling(Zhu and Hovy,2007)
CurrentPA
EHG2007(Ertekin et al, 2007)
</figure>
<page confidence="0.914783">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608061">
<title confidence="0.996700666666667">Taking into Account the Differences between Actively and Acquired Data: The Case of Active Learning with Support Vector for Imbalanced Datasets</title>
<author confidence="0.861117">Human Language</author>
<affiliation confidence="0.866862">Center of Johns Hopkins</affiliation>
<address confidence="0.999222">Baltimore, MD 21211</address>
<email confidence="0.999811">bloodgood@jhu.edu</email>
<abstract confidence="0.9978739375">Actively sampled data can have very different characteristics than passively sampled data. Therefore, it’s promising to investigate using different inference procedures during AL than are used during passive learning (PL). This general idea is explored in detail for the focused case of AL with cost-weighted SVMs for imbalanced data, a situation that arises for many HLT tasks. The key idea behind the proposed InitPA method for addressing imbalance is to base cost models during AL on an estimate of overall corpus imbalance computed via a small unbiased sample rather than the imbalance in the labeled training data, which is the leading method used during PL.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark L Berenson</author>
<author>David M Levine</author>
<author>David Rindskopf</author>
</authors>
<title>Applied Statistics. Prentice-Hall,</title>
<date>1988</date>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="9772" citStr="Berenson et al., 1988" startWordPosition="1671" endWordPosition="1674">oughout the AL process. We call this method of setting the PA based on a small initial set of labeled data the InitPA method. It is like CurrentPA except we move Step 0 to be executed one time before the loop and then use that same PA value on each iteration of the AL loop. To guide what size to make the initial set of labeled data, one can determine the sample size required to estimate the proportion of positives in a finite population to within sampling error e with a desired level of confidence using standard statistical techniques found in many college-level statistics references such as (Berenson et al., 1988). For example, carrying out the computations on the AImed dataset shows that a size of 100 enables us to be 95% confident that our proportion estimate is within 0.0739 of the true proportion. In our experiments, we used an initial labeled set of size 100. 5 Evaluation In addition to InitPA and CurrentPA, we also implemented the methods from (Ertekin et al., 2007; Zhu and Hovy, 2007). We implemented oversampling by duplicating points and by BootOS (Zhu and Hovy, 2007). To avoid cluttering the graphs, we only show the highest-performing oversampling variant, which was by duplicating points. Lear</context>
</contexts>
<marker>Berenson, Levine, Rindskopf, 1988</marker>
<rawString>Mark L. Berenson, David M. Levine, and David Rindskopf. 1988. Applied Statistics. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>K Vijay-Shanker</author>
</authors>
<title>A method for stopping active learning based on stabilizing predictions and the need for user-adjustable stopping. In CoNLL.</title>
<date>2009</date>
<marker>Bloodgood, Vijay-Shanker, 2009</marker>
<rawString>Michael Bloodgood and K. Vijay-Shanker. 2009. A method for stopping active learning based on stabilizing predictions and the need for user-adjustable stopping. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seyda Ertekin</author>
<author>Jian Huang</author>
<author>L´eon Bottou</author>
<author>C Lee Giles</author>
</authors>
<title>Learning on the border: active learning in imbalanced data classification.</title>
<date>2007</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="4858" citStr="Ertekin et al., 2007" startWordPosition="786" endWordPosition="789">example k, and C+ and C− are user-defined cost factors. The most important part for this paper are the cost factors C+ and C−. The ratio C+ C− quantifies the importance of reducing slack error on pos train examples relative to reducing slack error on negative (neg) train examples. The value of the ratio is crucial for balancing the precision recall tradeoff well. (Morik et al., 1999) showed that during PL, setting les C± = # of pos training egamples is an effective heuristic. Section 4 explores using this heuristic during AL and explains a modified heuristic that could work better during AL. (Ertekin et al., 2007) propose using the balancing of training data that occurs as a result of AL-SVM to handle imbalance and do not use any further measures to address imbalance. (Zhu and Hovy, 2007) used resampling to address imbalance and based the amount of resampling, which is the analog of our cost model, on the amount of imbalance in the current set of labeled train data, as PL approaches do. In contrast, the InitPA approach in Section 4 bases its cost models on overall (unlabeled) corpus imbalance rather than the amount of imbalance in the current set of labeled data. 3 Experimental Setup We use relation ex</context>
<context position="8371" citStr="Ertekin et al., 2007" startWordPosition="1420" endWordPosition="1423">obtain hyperplane h .4 2. batch ← select k points from U that are closest to h and request their labels.5 3. U = U − batch . 4. L = L ∪ batch. End Loop Figure 2: The CurrentPA algorithm Empirical Evidence of CurrentPA creating a Skewed Distribution (Fold Avg) Number of Points for which Annotations Have Been Requested Figure 3: Illustration of AL skewing the distribution of pos/neg points on AImed. corpus because AL systematically selects the examples that are closest to the current model’s hyperplane and this tends to select more positive examples than random selection would select (see also (Ertekin et al., 2007)). Empirical evidence of this distribution skew is illustrated in Figure 3. The trend toward balanced datasets during AL could mislead and cause us to underestimate the PA. Therefore, our next algorithm aims to set the PA based on the ratio of neg to pos instances in the entire corpus. However, since we don’t have labels for the entire corpus, we don’t know this ratio. But by using a small initial sample of labeled data, we can 4We use SVMlight’s default value for C−. 5In our experiments, batch size is 20. AImed Average F Measure versus Number of Annotations Number of Points for which Annotati</context>
<context position="10136" citStr="Ertekin et al., 2007" startWordPosition="1736" endWordPosition="1739">ze required to estimate the proportion of positives in a finite population to within sampling error e with a desired level of confidence using standard statistical techniques found in many college-level statistics references such as (Berenson et al., 1988). For example, carrying out the computations on the AImed dataset shows that a size of 100 enables us to be 95% confident that our proportion estimate is within 0.0739 of the true proportion. In our experiments, we used an initial labeled set of size 100. 5 Evaluation In addition to InitPA and CurrentPA, we also implemented the methods from (Ertekin et al., 2007; Zhu and Hovy, 2007). We implemented oversampling by duplicating points and by BootOS (Zhu and Hovy, 2007). To avoid cluttering the graphs, we only show the highest-performing oversampling variant, which was by duplicating points. Learning curves are presented in Figures 4 and 5. Note InitPA is the highest-performing method for all datasets, especially in the practically important area of where the learning curves begin to plateau. Ratio of # of Negative to # of Positive Points 5.5 4.5 3.5 2.5 1.5 0 5 4 3 2 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 Ratio with CurrentPA Ratio with Entir</context>
</contexts>
<marker>Ertekin, Huang, Bottou, Giles, 2007</marker>
<rawString>Seyda Ertekin, Jian Huang, L´eon Bottou, and C. Lee Giles. 2007. Learning on the border: active learning in imbalanced data classification. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Lorenza Romano</author>
</authors>
<title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="5673" citStr="Giuliano et al., 2006" startWordPosition="927" endWordPosition="930">g to address imbalance and based the amount of resampling, which is the analog of our cost model, on the amount of imbalance in the current set of labeled train data, as PL approaches do. In contrast, the InitPA approach in Section 4 bases its cost models on overall (unlabeled) corpus imbalance rather than the amount of imbalance in the current set of labeled data. 3 Experimental Setup We use relation extraction (RE) and text classification (TC) datasets and SVMlight (Joachims, 1999) for training the SVMs. For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al., 2006)). As in previous work, we cast RE as a binary classification task Figure 1: Hyperplane B was trained with a higher C+ C− ratio than hyperplane A was trained with. (14.94% of the examples in AImed are positive). We use the KGC kernel from (Giuliano et al., 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. For TC, we use the Reuters-21578 ModApte split. Since a document may belong to more than one category, each category is treated as a separate binary classification problem, as in (Joachims, 1998). As in (Joachims, 1998), we use the ten largest</context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Exploiting shallow linguistic information for relation extraction from biomedical literature. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with suport vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In ECML,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="6225" citStr="Joachims, 1998" startWordPosition="1025" endWordPosition="1026">protein interaction extraction systems ((Giuliano et al., 2006)). As in previous work, we cast RE as a binary classification task Figure 1: Hyperplane B was trained with a higher C+ C− ratio than hyperplane A was trained with. (14.94% of the examples in AImed are positive). We use the KGC kernel from (Giuliano et al., 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. For TC, we use the Reuters-21578 ModApte split. Since a document may belong to more than one category, each category is treated as a separate binary classification problem, as in (Joachims, 1998). As in (Joachims, 1998), we use the ten largest categories, which have imbalances ranging from 1.88% to 29.96%. 4 AL-SVM Methods for Addressing Class Imbalance The key question when using cwSVMs is how to set the ratio C+ C− . Increasing it will typically shift the learned hyperplane so recall is increased and precision is decreased (see Figure 1 for a hypothetical example). Let PA= C+ C−.3 How should the PA be set during AL-SVM? We propose two approaches: one sets the PA based on the level of imbalance in the labeled training data and one aims to set the PA based on an estimate of overall co</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with suport vector machines: Learning with many relevant features. In ECML, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods – Support Vector Learning,</booktitle>
<pages>169--184</pages>
<contexts>
<context position="5539" citStr="Joachims, 1999" startWordPosition="907" endWordPosition="908">ult of AL-SVM to handle imbalance and do not use any further measures to address imbalance. (Zhu and Hovy, 2007) used resampling to address imbalance and based the amount of resampling, which is the analog of our cost model, on the amount of imbalance in the current set of labeled train data, as PL approaches do. In contrast, the InitPA approach in Section 4 bases its cost models on overall (unlabeled) corpus imbalance rather than the amount of imbalance in the current set of labeled data. 3 Experimental Setup We use relation extraction (RE) and text classification (TC) datasets and SVMlight (Joachims, 1999) for training the SVMs. For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al., 2006)). As in previous work, we cast RE as a binary classification task Figure 1: Hyperplane B was trained with a higher C+ C− ratio than hyperplane A was trained with. (14.94% of the examples in AImed are positive). We use the KGC kernel from (Giuliano et al., 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. For TC, we use the Reuters-21578 ModApte split. Since a document may belong to more than one category, each c</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods – Support Vector Learning, pages 169–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Morik</author>
<author>Peter Brockhausen</author>
<author>Thorsten Joachims</author>
</authors>
<title>Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring.</title>
<date>1999</date>
<booktitle>In ICML,</booktitle>
<pages>268--277</pages>
<contexts>
<context position="4623" citStr="Morik et al., 1999" startWordPosition="743" endWordPosition="746"> 1 Subject to: ∀k : yk [~w · ~xk + b] ≥ 1 − ξk, (2) where (~w, b) represents the learned hyperplane, ~xk is the feature vector for example k, yk is the label for example k, ξk = max(0,1 − yk(~wk · ~xk + b)) is the slack variable for example k, and C+ and C− are user-defined cost factors. The most important part for this paper are the cost factors C+ and C−. The ratio C+ C− quantifies the importance of reducing slack error on pos train examples relative to reducing slack error on negative (neg) train examples. The value of the ratio is crucial for balancing the precision recall tradeoff well. (Morik et al., 1999) showed that during PL, setting les C± = # of pos training egamples is an effective heuristic. Section 4 explores using this heuristic during AL and explains a modified heuristic that could work better during AL. (Ertekin et al., 2007) propose using the balancing of training data that occurs as a result of AL-SVM to handle imbalance and do not use any further measures to address imbalance. (Zhu and Hovy, 2007) used resampling to address imbalance and based the amount of resampling, which is the analog of our cost model, on the amount of imbalance in the current set of labeled train data, as PL</context>
</contexts>
<marker>Morik, Brockhausen, Joachims, 1999</marker>
<rawString>Katharina Morik, Peter Brockhausen, and Thorsten Joachims. 1999. Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring. In ICML, pages 268–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Eduard Hovy</author>
</authors>
<title>Active learning for word sense disambiguation with methods for addressing the class imbalance problem.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="5036" citStr="Zhu and Hovy, 2007" startWordPosition="818" endWordPosition="821"> slack error on pos train examples relative to reducing slack error on negative (neg) train examples. The value of the ratio is crucial for balancing the precision recall tradeoff well. (Morik et al., 1999) showed that during PL, setting les C± = # of pos training egamples is an effective heuristic. Section 4 explores using this heuristic during AL and explains a modified heuristic that could work better during AL. (Ertekin et al., 2007) propose using the balancing of training data that occurs as a result of AL-SVM to handle imbalance and do not use any further measures to address imbalance. (Zhu and Hovy, 2007) used resampling to address imbalance and based the amount of resampling, which is the analog of our cost model, on the amount of imbalance in the current set of labeled train data, as PL approaches do. In contrast, the InitPA approach in Section 4 bases its cost models on overall (unlabeled) corpus imbalance rather than the amount of imbalance in the current set of labeled data. 3 Experimental Setup We use relation extraction (RE) and text classification (TC) datasets and SVMlight (Joachims, 1999) for training the SVMs. For RE, we use AImed, previously used to train protein interaction extrac</context>
<context position="10157" citStr="Zhu and Hovy, 2007" startWordPosition="1740" endWordPosition="1743">e the proportion of positives in a finite population to within sampling error e with a desired level of confidence using standard statistical techniques found in many college-level statistics references such as (Berenson et al., 1988). For example, carrying out the computations on the AImed dataset shows that a size of 100 enables us to be 95% confident that our proportion estimate is within 0.0739 of the true proportion. In our experiments, we used an initial labeled set of size 100. 5 Evaluation In addition to InitPA and CurrentPA, we also implemented the methods from (Ertekin et al., 2007; Zhu and Hovy, 2007). We implemented oversampling by duplicating points and by BootOS (Zhu and Hovy, 2007). To avoid cluttering the graphs, we only show the highest-performing oversampling variant, which was by duplicating points. Learning curves are presented in Figures 4 and 5. Note InitPA is the highest-performing method for all datasets, especially in the practically important area of where the learning curves begin to plateau. Ratio of # of Negative to # of Positive Points 5.5 4.5 3.5 2.5 1.5 0 5 4 3 2 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 Ratio with CurrentPA Ratio with Entire Set Performance (F </context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>Jingbo Zhu and Eduard Hovy. 2007. Active learning for word sense disambiguation with methods for addressing the class imbalance problem. In EMNLP-CoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>