<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001924">
<title confidence="0.995929">
Exploiting Domain Structure for Named Entity Recognition
</title>
<author confidence="0.999133">
Jing Jiang and ChengXiang Zhai
</author>
<affiliation confidence="0.9987615">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.81386">
Urbana, IL 61801
</address>
<email confidence="0.998182">
{jiang4,czhai}@cs.uiuc.edu
</email>
<sectionHeader confidence="0.999603" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99955709375">
Named Entity Recognition (NER) is the task of
identifying and classifying phrases that denote cer-
tain types of named entities (NEs), such as per-
sons, organizations and locations in news articles,
and genes, proteins and chemicals in biomedical lit-
erature. NER is a fundamental task in many natural
language processing applications, such as question
answering, machine translation, text mining, and in-
formation retrieval (Srihari and Li, 1999; Huang and
Vogel, 2002).
Existing approaches to NER are mostly based on
supervised learning. They can often achieve high
accuracy provided that a large annotated training set
similar to the test data is available (Borthwick, 1999;
Zhou and Su, 2002; Florian et al., 2003; Klein et al.,
2003; Finkel et al., 2005). Unfortunately, when the
test data has some difference from the training data,
these approaches tend to not perform well. For ex-
ample, Ciaramita and Altun (2005) reported a per-
formance degradation of a named entity recognizer
trained on CoNLL 2003 Reuters corpus, where the
F1 measure dropped from 0.908 when tested on a
similar Reuters set to 0.643 when tested on a Wall
Street Journal set. The degradation can be expected
to be worse if the training data and the test data are
more different.
The performance degradation indicates that exist-
ing approaches adapt poorly to new domains. We
believe one reason for this poor adaptability is that
these approaches have not considered the fact that,
depending on the genre or domain of the text, the
entities to be recognized may have different mor-
</bodyText>
<sectionHeader confidence="0.66447" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999303516129032">
Named Entity Recognition (NER) is a
fundamental task in text mining and nat-
ural language understanding. Current ap-
proaches to NER (mostly based on super-
vised learning) perform well on domains
similar to the training domain, but they
tend to adapt poorly to slightly different
domains. We present several strategies
for exploiting the domain structure in the
training data to learn a more robust named
entity recognizer that can perform well on
a new domain. First, we propose a sim-
ple yet effective way to automatically rank
features based on their generalizabilities
across domains. We then train a classifier
with strong emphasis on the most general-
izable features. This emphasis is imposed
by putting a rank-based prior on a logis-
tic regression model. We further propose
a domain-aware cross validation strategy
to help choose an appropriate parameter
for the rank-based prior. We evaluated
the proposed method with a task of recog-
nizing named entities (genes) in biology
text involving three species. The exper-
iment results show that the new domain-
aware approach outperforms a state-of-
the-art baseline method in adapting to new
domains, especially when there is a great
difference between the new domain and
the training domain.
</bodyText>
<page confidence="0.986381">
74
</page>
<note confidence="0.9953725">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 74–81,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999979755813954">
phological properties or occur in different contexts.
Indeed, since most existing learning-based NER ap-
proaches explore a large feature space, without regu-
larization, a learned NE recognizer can easily overfit
the training domain.
Domain overfitting is a serious problem in NER
because we often need to tag entities in completely
new domains. Given any new test domain, it is gen-
erally quite expensive to obtain a large amount of
labeled entity examples in that domain. As a result,
in many real applications, we must train on data that
do not fully resemble the test data.
This problem is especially serious in recognizing
entities, in particular gene names, from biomedical
literature. Gene names of one species can be quite
different from those of another species syntactically
due to their different naming conventions. For exam-
ple, some biological species such as yeast use sym-
bolic gene names like tL(CAA)G3, while some other
species such as fly use descriptive gene names like
wingless.
In this paper, we present several strategies for ex-
ploiting the domain structure in the training data to
learn a more robust named entity recognizer that can
perform well on a new domain. Our work is mo-
tivated by the fact that in many real applications,
the training data available to us naturally falls into
several domains that are similar in some aspects but
different in others. For example, in biomedical lit-
erature, the training data can be naturally grouped
by the biological species being discussed, while for
news articles, the training data can be divided by
the genre, the time, or the news agency of the arti-
cles. Our main idea is to exploit such domain struc-
ture in the training data to identify generalizable fea-
tures which, presumably, are more useful for rec-
ognizing named entities in a new domain. Indeed,
named entities across different domains often share
certain common features, and it is these common
features that are suitable for adaptation to new do-
mains; features that only work for a particular do-
main would not be as useful as those working for
multiple domains. In biomedical literature, for ex-
ample, surrounding words such as expression and
encode are strong indicators of gene mentions, re-
gardless of the specific biological species being dis-
cussed, whereas species-specific name characteris-
tics (e.g., prefix = “-less”) would clearly not gener-
alize well, and may even hurt the performance on a
new domain. Similarly, in news articles, the part-of-
speeches of surrounding words such as “followed by
a verb” are more generalizable indicators of name
mentions than capitalization, which might be mis-
leading if the genre of the new domain is different;
an extreme case is when every letter in the new do-
main is capitalized.
Based on these intuitions, we regard a feature as
generalizable if it is useful for NER in all training
domains, and propose a generalizability-based fea-
ture ranking method, in which we first rank the fea-
tures within each training domain, and then combine
the rankings to promote the features that are ranked
high in all domains. We further propose a rank-
based prior on logistic regression models, which
puts more emphasis on the more generalizable fea-
tures during the learning stage in a principled way.
Finally, we present a domain-aware validation strat-
egy for setting an appropriate parameter value for
the rank-based prior. We evaluated our method on
a biomedical literature data set with annotated gene
names from three species, fly, mouse, and yeast, by
treating one species as the new domain and the other
two as the training domains. The experiment results
show that the proposed method outperforms a base-
line method that represents the state-of-the-art NER
techniques.
The rest of the paper is organized as follows: In
Section 2, we introduce a feature ranking method
based on the generalizability of features across do-
mains. In Section 3, we briefly introduce the logistic
regression models for NER. We then propose a rank-
based prior on logistic regression models and de-
scribe the domain-aware validation strategy in Sec-
tion 4. The experiment results are presented in Sec-
tion 5. Finally we discuss related work in Section 6
and conclude our work in Section 7.
</bodyText>
<sectionHeader confidence="0.967213" genericHeader="introduction">
2 Generalizability-Based Feature Ranking
</sectionHeader>
<bodyText confidence="0.999713571428571">
We take a commonly used approach and treat NER
as a sequential tagging problem (Borthwick, 1999;
Zhou and Su, 2002; Finkel et al., 2005). Each token
is assigned the tag I if it is part of an NE and the tag
O otherwise. Let x denote the feature vector for a
token, and let y denote the tag for x. We first com-
pute the probability p(ylx) for each token, using a
</bodyText>
<page confidence="0.997866">
75
</page>
<bodyText confidence="0.99994394">
learned classifier. We then apply Viterbi algorithm
to assign the most likely tag sequence to a sequence
of tokens, i.e., a sentence. The features we use fol-
low the common practice in NER, including surface
word features, orthographic features, POS tags, sub-
strings, and contextual features in a local window of
size 5 around the target token (Finkel et al., 2005).
As in any learning problem, feature selection
may affect the NER performance significantly. In-
deed, a very likely cause of the domain overfit-
ting problem may be that the learned NE recog-
nizer has picked up some non-generalizable fea-
tures, which are not useful for a new domain. Below,
we present a generalizability-based feature ranking
method, which favors more generalizable features.
Formally, we assume that the training examples
are divided into m subsets T1, T2,... , Tm, corre-
sponding to m different domains D1, D2, ... , Dm.
We further assume that the test set Tm+1 is from
a new domain Dm+1, and this new domain shares
some common features of the m training domains.
Note that these are reasonable assumptions that re-
flect the situation in real problems.
We use generalizability to denote the amount of
contribution a feature can make to the classification
accuracy on any domain. Thus, a feature with high
generalizability should be useful for classification
on any domain. To identify the highly generalizable
features, we must then compare their contributions
to classification among different domains.
Suppose in each individual domain, the features
can be ranked by their contributions to the classifi-
cation accuracy. There are different feature ranking
methods based on different criteria. Without loss of
generality, let us use rT : F → {1, 2, ... , |F|} to
denote a ranking function that maps a feature f E F
to a rank rT (f) based on a set of training examples
T, where F is the set of all features, and the rank de-
notes the position of the feature in the final ranked
list. The smaller the rank rT (f) is, the more impor-
tant the feature f is in the training set T. For the m
training domains, we thus have m ranking functions
rT1, rT2, ... , rTm.
To identify the generalizable features across the m
different domains, we propose to combine the m in-
dividual domain ranking functions in the following
way. The idea is to give high ranks to features that
are useful in all training domains . To achieve this
goal, we first define a scoring function s : F → R
as follows:
</bodyText>
<equation confidence="0.996150666666667">
m 1
s(f) = min rTi(f). (1)
i=1
</equation>
<bodyText confidence="0.99999459375">
We then rank the features in decreasing order of their
scores using the above scoring function. This is es-
sentially to rank features according to their maxi-
mum rank maxi rTi(f) among the m domains. Let
function rSeR return the rank of a feature in this com-
bined, generalizability-based ranked list.
The original ranking function rT used for indi-
vidual domain feature ranking can use different cri-
teria such as information gain or x2 statistic (Yang
and Pedersen, 1997). In our experiments, we used a
ranking function based on the model parameters of
the classifier, which we will explain in Section 5.2.
Next, we need to incorporate this preference for
generalizable features into the classifier. Note that
because this generalizability-based feature ranking
method is independent of the learning algorithm, it
can be applied on top of any classifier. In this work,
we choose the logistic regression classifier. One way
to incorporate the feature ranking into the classifier
is to select the top-k features, where k is chosen by
cross validation. There are two potential problems
with this hard feature selection approach. First, once
k features are selected, they are treated equally dur-
ing the learning stage, resulting in a loss of the pref-
erence among these k features. Second, this incre-
mental feature selection approach does not consider
the correlation among features. We propose an al-
ternative way to incorporate the feature ranking into
the classifier, where the preference for generalizable
features is transformed into a non-uniform prior over
the feature parameters in the model. This can be re-
garded as a soft feature selection approach.
</bodyText>
<sectionHeader confidence="0.987975" genericHeader="method">
3 Logistic Regression for NER
</sectionHeader>
<bodyText confidence="0.9989295">
In binary logistic regression models, the probability
of an observation x being classified as I is
</bodyText>
<equation confidence="0.999870833333333">
p(I |x, )3) = exp00 + E|F|
i=1 Axi) (2)
1 + exp(00 + �|F|
i=1 Oixi)
exp()3 · x&apos;) = (3)
1 + exp()3 · x&apos;),
</equation>
<page confidence="0.728496">
76
</page>
<bodyText confidence="0.9996126">
where β0 is the bias weight, βi (1 G i G F )
are the weights for the features, and x&apos; is the aug-
mented feature vector with x0 = 1. The weight vec-
tor ,3 can be learned from the training examples by
a maximum likelihood estimator. It is worth point-
ing out that logistic regression has a close relation
with maximum entropy models. Indeed, when the
features in a maximum entropy model are defined as
conjunctions of a feature on observations only and
a Kronecker delta of a class label, which is a com-
mon practice in NER, the maximum entropy model
is equivalent to a logistic regression model (Finkel
et al., 2005). Thus the logistic regression method we
use for NER is essentially the same as the maximum
entropy models used for NER in previous work.
To avoid overfitting, a zero mean Gaussian prior
on the weights is usually used (Chen and Rosenfeld,
1999; Bender et al., 2003), and a maximum a poste-
rior (MAP) estimator is used to maximize the poste-
rior probability:
</bodyText>
<equation confidence="0.954355">
p(yjJxj,)3), (4)
</equation>
<bodyText confidence="0.990523">
where yj is the true class label for xj, N is the num-
ber of training examples, and
</bodyText>
<equation confidence="0.993562">
1exp(− β2 i2). (5)
�2πσ22σi
i
</equation>
<bodyText confidence="0.999814333333333">
In previous work, σi are set uniformly to the same
value for all features, because there is in general no
additional prior knowledge about the features.
</bodyText>
<sectionHeader confidence="0.999454" genericHeader="method">
4 Rank-Based Prior
</sectionHeader>
<bodyText confidence="0.999979454545455">
Instead of using the same σi for all features, we pro-
pose a rank-based non-uniform Gaussian prior on
the weights of the features so that more general-
izable features get higher prior variances (i.e., low
prior strength) and features on the bottom of the list
get low prior variances (i.e., high prior strength).
Since the prior has a zero mean, such a prior would
force features on the bottom of the ranked list, which
have the least generalizability, to have near-zero
weights, but allow more generalizable features to be
assigned higher weights during the training stage.
</bodyText>
<subsectionHeader confidence="0.990303">
4.1 Transformation Function
</subsectionHeader>
<bodyText confidence="0.997465714285714">
We need to find a transformation function h :
11, 2, ... , JFJJ → R+ so that we can set σ2i =
h(rgen(fi)), where rgen(fi) is the rank of feature
fi in the generalizability-based ranked feature list,
as defined in Section 2. We choose the following
h function because it has the desired properties as
described above:
</bodyText>
<equation confidence="0.9635685">
a
h(r) = r1/b, (6)
</equation>
<bodyText confidence="0.993617210526316">
where a and b (a, b &gt; 0) are parameters that control
the degree of the confidence in the generalizability-
based ranked feature list. Note that a corresponds to
the prior variance assigned to the top-most feature in
the ranked list. When b is small, the prior variance
drops rapidly as the rank r increases, giving only a
small number of top features high prior variances.
When b is larger, there will be less discrimination
among the features. When b approaches infinity, the
prior becomes a uniform prior with the variance set
to a for all features. If we set a small threshold τ on
the variance, then we can derive that at least m =
(a )b features have a prior variance greater than τ.
�
Thus b is proportional to the logarithm of the number
of features that are assigned a variance greater than
the threshold τ when a is fixed. Figure 1 shows the
h function when a is set to 20 and b is set to a set of
different values.
</bodyText>
<figure confidence="0.8548765">
0 200 400 600 800 1000
r
</figure>
<figureCaption confidence="0.999708">
Figure 1: Transformation Function h(r) = 20
</figureCaption>
<bodyText confidence="0.726277">
r1/b
</bodyText>
<sectionHeader confidence="0.5681835" genericHeader="method">
4.2 Parameter Setting using Domain-Aware
Validation
</sectionHeader>
<bodyText confidence="0.994014">
We need to set the appropriate values for the param-
eters a and b. For parameter a, we use the following
</bodyText>
<figure confidence="0.993101272727273">
h(r)
25
20
15
10
5
0
b = 2
b = 4
b = 6
b = ∞
</figure>
<equation confidence="0.822369125">
,3ˆ = arg max
p(,C3)
�
N
H
j=1
p(M = H �F1
i=1
</equation>
<page confidence="0.961211">
77
</page>
<bodyText confidence="0.9951428">
simple strategy to obtain an estimation. We first train
a logistic regression model on all the training data
using a Gaussian prior with a fixed variance (set to
1 in our experiments). We then find the maximum
weight
</bodyText>
<equation confidence="0.999559666666667">
|F |
βmax = max |βi |(7)
i=1
</equation>
<bodyText confidence="0.999990866666667">
in this trained model. Finally we set a = β2max. Our
reasoning is that since a is the variance of the prior
for the best feature, a is related to the “permissible
range” of β for the best feature, and βmax gives us a
way for adjusting a according to the empirical range
of βi’s.
As we pointed out in Section 4.1, when a is fixed,
parameter b controls the number of top features that
are given a relatively high prior variance, and hence
implicitly controls the number of top features to
choose for the classifier to put the most weights on.
To select an appropriate value of b, we can use a
held-out validation set to tune the parameter value
b. Here we present a validation strategy that exploits
the domain structure in the training data to set the
parameter b for a new domain. Note that in regular
validation, both the training set and the validation
set contain examples from all training domains. As
a result, the average performance on the validation
set may be dominated by domains in which the NEs
are easy to classify. Since our goal is to build a clas-
sifier that performs well on new domains, we should
pay more attention to hard domains that have lower
classification accuracy. We should therefore exam-
ine the performance of the classifier on each training
domain individually in the validation stage to gain
an insight into the appropriate value of b for a new
domain, which has an equal chance of being similar
to any of the training domains.
Our domain-aware validation strategy first finds
the optimal value of b for each training domain. For
each subset Ti of the training data belonging to do-
main Di, we divide it into a training set Tit and a val-
idation set Tiv. Then for each domain Di, we train a
classifier on the training sets of all domains, that is,
we train on Um j=1 Tjt . We then test the classifier on
Tiv. We try a set of different values of b with a fixed
value of a, and choose the optimal b that gives the
best performance on Tiv. Let this optimal value of b
for domain Di be bi.
Given bi (1 ≤ i ≤ m), we can choose an appropri-
ate value of bm+1 for an unknown test domain Dm+1
based on the assumption that Dm+1 is a mixture of
all the training domains. bm+1 is then chosen to be
a weighted average of bi, (1 ≤ i ≤ m):
</bodyText>
<equation confidence="0.999136">
bm+1 = �m λibi, (8)
i=1
</equation>
<bodyText confidence="0.9999866">
where λi indicates how similar Dm+1 is to Di. In
many cases, the test domain Dm+1 is completely
unknown. In this case, the best we can do is to set
λi = 1/m for all i, that is, to assume that Dm+1 is
an even mixture of all training domains.
</bodyText>
<sectionHeader confidence="0.996652" genericHeader="method">
5 Empirical Evaluation
</sectionHeader>
<subsectionHeader confidence="0.883409">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.998069107142857">
We evaluated our domain-aware approach to NER
on the problem of gene recognition in biomedical
literature. The data we used is from BioCreAtIvE
Task 1B (Hirschman et al., 2005). We chose this
data set because it contains three subsets of MED-
LINE abstracts with gene names from three species
(fly, mouse, and yeast), while no other existing an-
notated NER data set has such explicit domain struc-
ture. The original BioCreAtIvE 1B data was not
provided with every gene annotated, but for each ab-
stract, a list of genes that were mentioned in the ab-
stract was given. A gene synonym list was also given
for each species. We used a simple string matching
method with slight relaxation to tag the gene men-
tions in the abstracts. We took 7500 sentences from
each species for our experiments, where half of the
sentences contain gene mentions. We further split
the 7500 sentences of each species into two sets,
5000 for training and 2500 for testing.
We conducted three sets of experiments, each
combining the 5000-sentence training data of two
species as training data, and the 2500-sentence test
data of the third species as test data. The 2500-
sentence test data of the training species was used
for validation. We call these three sets of experi-
ments F+M⇒Y, F+Y⇒M, and M+Y⇒F.
we use FEX1 for feature extraction and BBR2 for
logistic regression in our experiments.
</bodyText>
<footnote confidence="0.9999955">
1http://l2r.cs.uiuc.edu/ cogcomp/asoftware.php?skey=FEX
2http://www.stat.rutgers.edu/ madigan/BBR/
</footnote>
<page confidence="0.997447">
78
</page>
<subsectionHeader confidence="0.999505">
5.2 Comparison with Baseline Method
</subsectionHeader>
<bodyText confidence="0.99955334">
Because the data set was generated by our automatic
tagging procedure using the given gene lists, there is
no previously reported performance on this data set
for us to compare with. Therefore, to see whether
using the domain structure in the training data can
really help the adaptation to new domains, we com-
pared our method with a state-of-the-art baseline
method based on logistic regression. It uses a Gaus-
sian prior with zero mean and uniform variance on
all model parameters. It also employs 5-fold regular
cross validation to pick the optimal variance for the
prior. Regular feature selection is also considered
in the baseline method, where the features are first
ranked according to some criterion, and then cross
validation is used to select the top-k features. We
tested three popular regular feature ranking meth-
ods: feature frequency (F), information gain (IG),
and k2 statistic (CHI). These methods were dis-
cussed in (Yang and Pedersen, 1997). However, with
any of the three feature ranking criteria, cross valida-
tion showed that selecting all features gave the best
average validation performance. Therefore, the best
baseline method which we compare our method with
uses all features. We call the baseline method BL.
In our method, the generalizability-based feature
ranking requires a first step of feature ranking within
each training domain. While we could also use F,
IG or CHI to rank features in each domain, to make
our method self-contained, we used the following
strategy. We first train a logistic regression model
on each domain using a zero-mean Gaussian prior
with variance set to 1. Then, features are ranked
in decreasing order of the absolute values of their
weights. The rationale is that, in general, features
with higher weights in the logistic regression model
are more important. With this ranking within each
training domain, we then use the generalizability-
based feature ranking method to combine the m
domain-specific rankings. The obtained ranked fea-
ture list is used to construct the rank-based prior,
where the parameters a and b are set in the way as
discussed in Section 4.2. We call our method DOM.
In Table 1, we show the precision, recall, and F1
measures of our domain-aware method (DOM) and
the baseline method (BL) in all three sets of exper-
iments. We see that the domain-aware method out-
performs the baseline method in all three cases when
F1 is used as the primary performance measure. In
F+YAM and M+Y==�-F, both precision and recall are
also improved over the baseline method.
</bodyText>
<table confidence="0.999641285714286">
Exp Method P R F1
F+MAY BL 0.557 0.466 0.508
DOM 0.575 0.516 0.544
F+YAM BL 0.571 0.335 0.422
DOM 0.582 0.381 0.461
M+Y=:&gt;.F BL 0.583 0.097 0.166
DOM 0.591 0.139 0.225
</table>
<tableCaption confidence="0.838266">
Table 1: Comparison of the domain-aware method
and the baseline method, where in the domain-aware
method, b = 0.5b1 + 0.5b2
</tableCaption>
<bodyText confidence="0.999942">
Note that the absolute performance shown in Ta-
ble 1 is lower than the state-of-the-art performance
of gene recognition (Finkel et al., 2005).3 One rea-
son is that we explicitly excluded the test domain
from the training data, while most previous work on
gene recognition was conducted on a test set drawn
from the same collection as the training data. An-
other reason is that we used simple string match-
ing to generate the data set, which introduced noise
to the data because gene names often have irregular
lexical variants.
</bodyText>
<subsectionHeader confidence="0.944589">
5.3 Comparison with Regular Feature Ranking
</subsectionHeader>
<figure confidence="0.89149975">
Methods
F+M⇒Y
1 2 3 4 5 6 7 8 9 10
b
</figure>
<figureCaption confidence="0.824072333333333">
Figure 2: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
F+MAY
</figureCaption>
<footnote confidence="0.963509">
3Our baseline method performed comparably to the state-of-
the-art systems on the standard BioCreAtIvE 1A data.
</footnote>
<figure confidence="0.954239">
F1
0.65
0.55
0.45
0.35
0.6
0.5
0.4
DOM
F
IG
CHI
BL
79
F+Y⇒M
1 2 3 4 5 6 7 8 9 10
b
</figure>
<figureCaption confidence="0.9779955">
Figure 3: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
</figureCaption>
<figure confidence="0.8795255">
F+YAM
M+Y⇒F
</figure>
<figureCaption confidence="0.919179333333333">
Figure 4: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
M+YAF
</figureCaption>
<bodyText confidence="0.999993095238095">
To further understand how our method improved
the performance, we compared the generalizability-
based feature ranking method with the three regular
feature ranking methods, F, IG, and CHI, that were
used in the baseline method. To make fair compar-
ison, for the regular feature ranking methods, we
also used the rank-based prior transformation as de-
scribed in Section 4 to incorporate the preference for
top-ranked features. Figure 2, Figure 3 and Figure 4
show the performance of different feature ranking
methods in the three sets of experiments as the pa-
rameter b for the rank-based prior changes. As we
pointed out in Section 4, b is proportional to the log-
arithm of the number of “effective features”.
From the figures, we clearly see that the curve for
the generalizability-based ranking method DOM is
always above the curves of the other methods, indi-
cating that when the same amount of top features are
being emphasized by the prior, the features selected
by DOM give better performance on a new domain
than the features selected by the other methods. This
suggests that the top-ranked features in DOM are in-
deed more suitable for adaptation to new domains
than the top features ranked by the other methods.
The figures also show that the ranking method
DOM achieved better performance than the baseline
over a wide range of b values, especially in F+YAM
and M+YAF, whereas for methods F, IG and CHI,
the performance quickly converged to the baseline
performance as b increased.
It is interesting to note the comparison between F
and IG (or CHI). In general, when the test data is
similar to the training data, IG (or CHI) is advanta-
geous over F (Yang and Pedersen, 1997). However,
in this case when the test domain is different from
the training domains, F shows advantages for adap-
tation. A possible explanation is that frequent fea-
tures are in general less likely to be domain-specific,
and therefore feature frequency can also be used as a
criterion to select generalizable features and to filter
out domain-specific features, although it is still not
as effective as the method we proposed.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999897588235294">
The NER problem has been extensively studied in
the NLP community. Most existing work has fo-
cused on supervised learning approaches, employ-
ing models such as HMMs (Zhou and Su, 2002),
MEMMs (Bender et al., 2003; Finkel et al., 2005),
and CRFs (McCallum and Li, 2003). Collins and
Singer (1999) proposed an unsupervised method for
named entity classification based on the idea of co-
training. Ando and Zhang (2005) proposed a semi-
supervised learning method to exploit unlabeled data
for building more robust NER systems. In all these
studies, the evaluation is conducted on unlabeled
data similar to the labeled data.
Recently there have been some studies on adapt-
ing NER systems to new domains employing tech-
niques such as active learning and semi-supervised
learning (Shen et al., 2004; Mohit and Hwa, 2005),
</bodyText>
<figure confidence="0.997205076923077">
F1
0.45
0.35
0.5
0.4
0.3
DOM
F
IG
CHI
BL
0.3
0.25
0.2
0.15
0.1
DOM
F
IG
CHI
BL
0
1 2 3 4 5 6 7 8 9 10
b
F1
0.05
</figure>
<page confidence="0.976464">
80
</page>
<bodyText confidence="0.999882363636364">
or incorporating external lexical knowledge (Cia-
ramita and Altun, 2005). However, there has not
been any study on exploiting the domain structure
contained in the training examples themselves to
build generalizable NER systems. We focus on
the domain structure in the training data to build
a classifier that relies more on features generaliz-
able across different domains to avoid overfitting the
training domains. As our method is orthogonal to
most of the aforementioned work, they can be com-
bined to further improve the performance.
</bodyText>
<sectionHeader confidence="0.991816" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999882461538461">
Named entity recognition is an important problem
that can help many text mining and natural lan-
guage processing tasks such as information extrac-
tion and question answering. Currently NER faces
a poor domain adaptability problem when the test
data is not from the same domain as the training
data. We present several strategies to exploit the
domain structure in the training data to improve the
performance of the learned NER classifier on a new
domain. Our results show that the domain-aware
strategies we proposed improved the performance
over a baseline method that represents the state-of-
the-art NER techniques.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999854">
This work was in part supported by the National
Science Foundation under award numbers 0425852,
0347933, and 0428472. We would like to thank
Bruce Schatz, Xin He, Qiaozhu Mei, Xu Ling, and
some other BeeSpace project members for useful
discussions. We would like to thank Mark Sammons
for his help with FEX. We would also like to thank
the anonymous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999724322033899">
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for text
chunking. In Proceedings of ACL-2005.
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proceedings of CoNLL-2003.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, School of Com-
puter Science, Carnegie Mellon University.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Workshop on Advances
in Structured Learning for Text and Speech Processing
(NIPS-2005).
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP/VLC-1999.
Jenny Finkel, Shipra Dingare, Christopher D. Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics, 6(Suppl 1):S5.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Proceedings of CoNLL-2003.
Lynette Hirschman, Marc Colosimo, Alexander Morgan,
and Alexander Yeh. 2005. Overview of BioCreAtIvE
task 1B: normailized gene lists. BMC Bioinformatics,
6(Suppl 1):S11.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI-2002.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recogni-
tion with character-level models. In Proceedings of
CoNLL-2003.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL-2003.
Behrang Mohit and Rebecca Hwa. 2005. Syntax-based
semi-supervised named entity tagging. In Proceedings
of ACL-2005.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of ACL-
2004.
Rohini Srihari and Wei Li. 1999. Information extraction
supported question answering. In TREC-8.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML-1997.
Guodong Zhou and Jian Su. 2002. Named entity recog-
nition using an HMM-based chunk tagger. In Proceed-
ings of ACL-2002.
</reference>
<page confidence="0.999263">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917758">
<title confidence="0.999942">Exploiting Domain Structure for Named Entity Recognition</title>
<author confidence="0.995155">Jiang</author>
<affiliation confidence="0.9988795">Department of Computer University of Illinois at</affiliation>
<address confidence="0.923861">Urbana, IL</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A highperformance semi-supervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005.</booktitle>
<contexts>
<context position="26420" citStr="Ando and Zhang (2005)" startWordPosition="4543" endWordPosition="4546">e feature frequency can also be used as a criterion to select generalizable features and to filter out domain-specific features, although it is still not as effective as the method we proposed. 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), F1 0.45 0.35 0.5 0.4 0.3 DOM F IG CHI BL 0.3 0.25 0.2 0.15 0.1 DOM F IG CHI BL 0 1 2 3 4 5 6 7 8 9 10 b F1 0.05 80 or incorporating external lexical knowledge (Ciaramita and Altun, 2005). However, there </context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A highperformance semi-supervised learning method for text chunking. In Proceedings of ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Maximum entropy models for named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<contexts>
<context position="13064" citStr="Bender et al., 2003" startWordPosition="2190" endWordPosition="2193">at logistic regression has a close relation with maximum entropy models. Indeed, when the features in a maximum entropy model are defined as conjunctions of a feature on observations only and a Kronecker delta of a class label, which is a common practice in NER, the maximum entropy model is equivalent to a logistic regression model (Finkel et al., 2005). Thus the logistic regression method we use for NER is essentially the same as the maximum entropy models used for NER in previous work. To avoid overfitting, a zero mean Gaussian prior on the weights is usually used (Chen and Rosenfeld, 1999; Bender et al., 2003), and a maximum a posterior (MAP) estimator is used to maximize the posterior probability: p(yjJxj,)3), (4) where yj is the true class label for xj, N is the number of training examples, and 1exp(− β2 i2). (5) �2πσ22σi i In previous work, σi are set uniformly to the same value for all features, because there is in general no additional prior knowledge about the features. 4 Rank-Based Prior Instead of using the same σi for all features, we propose a rank-based non-uniform Gaussian prior on the weights of the features so that more generalizable features get higher prior variances (i.e., low prio</context>
<context position="26218" citStr="Bender et al., 2003" startWordPosition="4510" endWordPosition="4513">e test domain is different from the training domains, F shows advantages for adaptation. A possible explanation is that frequent features are in general less likely to be domain-specific, and therefore feature frequency can also be used as a criterion to select generalizable features and to filter out domain-specific features, although it is still not as effective as the method we proposed. 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), F</context>
</contexts>
<marker>Bender, Och, Ney, 2003</marker>
<rawString>Oliver Bender, Franz Josef Och, and Hermann Ney. 2003. Maximum entropy models for named entity recognition. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="892" citStr="Borthwick, 1999" startWordPosition="128" endWordPosition="129">g and classifying phrases that denote certain types of named entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degra</context>
<context position="7548" citStr="Borthwick, 1999" startWordPosition="1223" endWordPosition="1224">he paper is organized as follows: In Section 2, we introduce a feature ranking method based on the generalizability of features across domains. In Section 3, we briefly introduce the logistic regression models for NER. We then propose a rankbased prior on logistic regression models and describe the domain-aware validation strategy in Section 4. The experiment results are presented in Section 5. Finally we discuss related work in Section 6 and conclude our work in Section 7. 2 Generalizability-Based Feature Ranking We take a commonly used approach and treat NER as a sequential tagging problem (Borthwick, 1999; Zhou and Su, 2002; Finkel et al., 2005). Each token is assigned the tag I if it is part of an NE and the tag O otherwise. Let x denote the feature vector for a token, and let y denote the tag for x. We first compute the probability p(ylx) for each token, using a 75 learned classifier. We then apply Viterbi algorithm to assign the most likely tag sequence to a sequence of tokens, i.e., a sentence. The features we use follow the common practice in NER, including surface word features, orthographic features, POS tags, substrings, and contextual features in a local window of size 5 around the ta</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="13042" citStr="Chen and Rosenfeld, 1999" startWordPosition="2186" endWordPosition="2189">t is worth pointing out that logistic regression has a close relation with maximum entropy models. Indeed, when the features in a maximum entropy model are defined as conjunctions of a feature on observations only and a Kronecker delta of a class label, which is a common practice in NER, the maximum entropy model is equivalent to a logistic regression model (Finkel et al., 2005). Thus the logistic regression method we use for NER is essentially the same as the maximum entropy models used for NER in previous work. To avoid overfitting, a zero mean Gaussian prior on the weights is usually used (Chen and Rosenfeld, 1999; Bender et al., 2003), and a maximum a posterior (MAP) estimator is used to maximize the posterior probability: p(yjJxj,)3), (4) where yj is the true class label for xj, N is the number of training examples, and 1exp(− β2 i2). (5) �2πσ22σi i In previous work, σi are set uniformly to the same value for all features, because there is in general no additional prior knowledge about the features. 4 Rank-Based Prior Instead of using the same σi for all features, we propose a rank-based non-uniform Gaussian prior on the weights of the features so that more generalizable features get higher prior var</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Named-entity recognition in novel domains with external lexical knowledge.</title>
<date>2005</date>
<booktitle>In Workshop on Advances in Structured Learning for Text and Speech Processing (NIPS-2005).</booktitle>
<contexts>
<context position="1137" citStr="Ciaramita and Altun (2005)" startWordPosition="168" endWordPosition="171">natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degradation indicates that existing approaches adapt poorly to new domains. We believe one reason for this poor adaptability is that these approaches have not considered the fact that, depending on the genre or domain of the text, the entities to be </context>
<context position="27003" citStr="Ciaramita and Altun, 2005" startWordPosition="4652" endWordPosition="4656">e idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), F1 0.45 0.35 0.5 0.4 0.3 DOM F IG CHI BL 0.3 0.25 0.2 0.15 0.1 DOM F IG CHI BL 0 1 2 3 4 5 6 7 8 9 10 b F1 0.05 80 or incorporating external lexical knowledge (Ciaramita and Altun, 2005). However, there has not been any study on exploiting the domain structure contained in the training examples themselves to build generalizable NER systems. We focus on the domain structure in the training data to build a classifier that relies more on features generalizable across different domains to avoid overfitting the training domains. As our method is orthogonal to most of the aforementioned work, they can be combined to further improve the performance. 7 Conclusion and Future Work Named entity recognition is an important problem that can help many text mining and natural language proce</context>
</contexts>
<marker>Ciaramita, Altun, 2005</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2005. Named-entity recognition in novel domains with external lexical knowledge. In Workshop on Advances in Structured Learning for Text and Speech Processing (NIPS-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC-1999.</booktitle>
<contexts>
<context position="26301" citStr="Collins and Singer (1999)" startWordPosition="4524" endWordPosition="4527">adaptation. A possible explanation is that frequent features are in general less likely to be domain-specific, and therefore feature frequency can also be used as a criterion to select generalizable features and to filter out domain-specific features, although it is still not as effective as the method we proposed. 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), F1 0.45 0.35 0.5 0.4 0.3 DOM F IG CHI BL 0.3 0.25 0.2 0.15 0.1 DOM F IG CHI BL 0 1 2</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of EMNLP/VLC-1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Finkel</author>
<author>Shipra Dingare</author>
<author>Christopher D Manning</author>
<author>Malvina Nissim</author>
<author>Beatrice Alex</author>
<author>Claire Grover</author>
</authors>
<title>Exploring the boundaries: Gene and protein identification in biomedical text.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<pages>1--5</pages>
<contexts>
<context position="975" citStr="Finkel et al., 2005" startWordPosition="142" endWordPosition="145">uch as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degradation indicates that existing approaches adapt poorly to new domains. We believe o</context>
<context position="7589" citStr="Finkel et al., 2005" startWordPosition="1229" endWordPosition="1232"> Section 2, we introduce a feature ranking method based on the generalizability of features across domains. In Section 3, we briefly introduce the logistic regression models for NER. We then propose a rankbased prior on logistic regression models and describe the domain-aware validation strategy in Section 4. The experiment results are presented in Section 5. Finally we discuss related work in Section 6 and conclude our work in Section 7. 2 Generalizability-Based Feature Ranking We take a commonly used approach and treat NER as a sequential tagging problem (Borthwick, 1999; Zhou and Su, 2002; Finkel et al., 2005). Each token is assigned the tag I if it is part of an NE and the tag O otherwise. Let x denote the feature vector for a token, and let y denote the tag for x. We first compute the probability p(ylx) for each token, using a 75 learned classifier. We then apply Viterbi algorithm to assign the most likely tag sequence to a sequence of tokens, i.e., a sentence. The features we use follow the common practice in NER, including surface word features, orthographic features, POS tags, substrings, and contextual features in a local window of size 5 around the target token (Finkel et al., 2005). As in a</context>
<context position="12799" citStr="Finkel et al., 2005" startWordPosition="2144" endWordPosition="2147"> x&apos;), 76 where β0 is the bias weight, βi (1 G i G F ) are the weights for the features, and x&apos; is the augmented feature vector with x0 = 1. The weight vector ,3 can be learned from the training examples by a maximum likelihood estimator. It is worth pointing out that logistic regression has a close relation with maximum entropy models. Indeed, when the features in a maximum entropy model are defined as conjunctions of a feature on observations only and a Kronecker delta of a class label, which is a common practice in NER, the maximum entropy model is equivalent to a logistic regression model (Finkel et al., 2005). Thus the logistic regression method we use for NER is essentially the same as the maximum entropy models used for NER in previous work. To avoid overfitting, a zero mean Gaussian prior on the weights is usually used (Chen and Rosenfeld, 1999; Bender et al., 2003), and a maximum a posterior (MAP) estimator is used to maximize the posterior probability: p(yjJxj,)3), (4) where yj is the true class label for xj, N is the number of training examples, and 1exp(− β2 i2). (5) �2πσ22σi i In previous work, σi are set uniformly to the same value for all features, because there is in general no addition</context>
<context position="22912" citStr="Finkel et al., 2005" startWordPosition="3945" endWordPosition="3948">rms the baseline method in all three cases when F1 is used as the primary performance measure. In F+YAM and M+Y==�-F, both precision and recall are also improved over the baseline method. Exp Method P R F1 F+MAY BL 0.557 0.466 0.508 DOM 0.575 0.516 0.544 F+YAM BL 0.571 0.335 0.422 DOM 0.582 0.381 0.461 M+Y=:&gt;.F BL 0.583 0.097 0.166 DOM 0.591 0.139 0.225 Table 1: Comparison of the domain-aware method and the baseline method, where in the domain-aware method, b = 0.5b1 + 0.5b2 Note that the absolute performance shown in Table 1 is lower than the state-of-the-art performance of gene recognition (Finkel et al., 2005).3 One reason is that we explicitly excluded the test domain from the training data, while most previous work on gene recognition was conducted on a test set drawn from the same collection as the training data. Another reason is that we used simple string matching to generate the data set, which introduced noise to the data because gene names often have irregular lexical variants. 5.3 Comparison with Regular Feature Ranking Methods F+M⇒Y 1 2 3 4 5 6 7 8 9 10 b Figure 2: Comparison between regular feature ranking and generalizability-based feature ranking on F+MAY 3Our baseline method performed</context>
<context position="26240" citStr="Finkel et al., 2005" startWordPosition="4514" endWordPosition="4517">erent from the training domains, F shows advantages for adaptation. A possible explanation is that frequent features are in general less likely to be domain-specific, and therefore feature frequency can also be used as a criterion to select generalizable features and to filter out domain-specific features, although it is still not as effective as the method we proposed. 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), F1 0.45 0.35 0.5 0.4 0.</context>
</contexts>
<marker>Finkel, Dingare, Manning, Nissim, Alex, Grover, 2005</marker>
<rawString>Jenny Finkel, Shipra Dingare, Christopher D. Manning, Malvina Nissim, Beatrice Alex, and Claire Grover. 2005. Exploring the boundaries: Gene and protein identification in biomedical text. BMC Bioinformatics, 6(Suppl 1):S5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<contexts>
<context position="933" citStr="Florian et al., 2003" startWordPosition="134" endWordPosition="137">e certain types of named entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degradation indicates that existing approaches</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Marc Colosimo</author>
<author>Alexander Morgan</author>
<author>Alexander Yeh</author>
</authors>
<title>Overview of BioCreAtIvE task 1B: normailized gene lists.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<pages>1--11</pages>
<contexts>
<context position="18626" citStr="Hirschman et al., 2005" startWordPosition="3238" endWordPosition="3241"> on the assumption that Dm+1 is a mixture of all the training domains. bm+1 is then chosen to be a weighted average of bi, (1 ≤ i ≤ m): bm+1 = �m λibi, (8) i=1 where λi indicates how similar Dm+1 is to Di. In many cases, the test domain Dm+1 is completely unknown. In this case, the best we can do is to set λi = 1/m for all i, that is, to assume that Dm+1 is an even mixture of all training domains. 5 Empirical Evaluation 5.1 Experimental Setup We evaluated our domain-aware approach to NER on the problem of gene recognition in biomedical literature. The data we used is from BioCreAtIvE Task 1B (Hirschman et al., 2005). We chose this data set because it contains three subsets of MEDLINE abstracts with gene names from three species (fly, mouse, and yeast), while no other existing annotated NER data set has such explicit domain structure. The original BioCreAtIvE 1B data was not provided with every gene annotated, but for each abstract, a list of genes that were mentioned in the abstract was given. A gene synonym list was also given for each species. We used a simple string matching method with slight relaxation to tag the gene mentions in the abstracts. We took 7500 sentences from each species for our experi</context>
</contexts>
<marker>Hirschman, Colosimo, Morgan, Yeh, 2005</marker>
<rawString>Lynette Hirschman, Marc Colosimo, Alexander Morgan, and Alexander Yeh. 2005. Overview of BioCreAtIvE task 1B: normailized gene lists. BMC Bioinformatics, 6(Suppl 1):S11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Stephan Vogel</author>
</authors>
<title>Improved named entity translation and bilingual named entity extraction.</title>
<date>2002</date>
<booktitle>In ICMI-2002.</booktitle>
<contexts>
<context position="686" citStr="Huang and Vogel, 2002" startWordPosition="94" endWordPosition="97">ang and ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 {jiang4,czhai}@cs.uiuc.edu 1 Introduction Named Entity Recognition (NER) is the task of identifying and classifying phrases that denote certain types of named entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when t</context>
</contexts>
<marker>Huang, Vogel, 2002</marker>
<rawString>Fei Huang and Stephan Vogel. 2002. Improved named entity translation and bilingual named entity extraction. In ICMI-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Joseph Smarr</author>
<author>Huy Nguyen</author>
<author>Christopher D Manning</author>
</authors>
<title>Named entity recognition with character-level models.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<contexts>
<context position="953" citStr="Klein et al., 2003" startWordPosition="138" endWordPosition="141">ed entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degradation indicates that existing approaches adapt poorly to new</context>
</contexts>
<marker>Klein, Smarr, Nguyen, Manning, 2003</marker>
<rawString>Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D. Manning. 2003. Named entity recognition with character-level models. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<contexts>
<context position="26274" citStr="McCallum and Li, 2003" startWordPosition="4520" endWordPosition="4523"> F shows advantages for adaptation. A possible explanation is that frequent features are in general less likely to be domain-specific, and therefore feature frequency can also be used as a criterion to select generalizable features and to filter out domain-specific features, although it is still not as effective as the method we proposed. 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), F1 0.45 0.35 0.5 0.4 0.3 DOM F IG CHI BL 0.3 0.25 0.2 0.1</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Rebecca Hwa</author>
</authors>
<title>Syntax-based semi-supervised named entity tagging.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005.</booktitle>
<contexts>
<context position="26815" citStr="Mohit and Hwa, 2005" startWordPosition="4607" endWordPosition="4610">Ms (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), F1 0.45 0.35 0.5 0.4 0.3 DOM F IG CHI BL 0.3 0.25 0.2 0.15 0.1 DOM F IG CHI BL 0 1 2 3 4 5 6 7 8 9 10 b F1 0.05 80 or incorporating external lexical knowledge (Ciaramita and Altun, 2005). However, there has not been any study on exploiting the domain structure contained in the training examples themselves to build generalizable NER systems. We focus on the domain structure in the training data to build a classifier that relies more on features generalizable across different domains to avoid overfitting the training domains. As our method is orthogonal to most of the aforementioned work, they</context>
</contexts>
<marker>Mohit, Hwa, 2005</marker>
<rawString>Behrang Mohit and Rebecca Hwa. 2005. Syntax-based semi-supervised named entity tagging. In Proceedings of ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
<author>ChewLim Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL2004.</booktitle>
<contexts>
<context position="26793" citStr="Shen et al., 2004" startWordPosition="4603" endWordPosition="4606"> and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), F1 0.45 0.35 0.5 0.4 0.3 DOM F IG CHI BL 0.3 0.25 0.2 0.15 0.1 DOM F IG CHI BL 0 1 2 3 4 5 6 7 8 9 10 b F1 0.05 80 or incorporating external lexical knowledge (Ciaramita and Altun, 2005). However, there has not been any study on exploiting the domain structure contained in the training examples themselves to build generalizable NER systems. We focus on the domain structure in the training data to build a classifier that relies more on features generalizable across different domains to avoid overfitting the training domains. As our method is orthogonal to most of the afo</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and ChewLim Tan. 2004. Multi-criteria-based active learning for named entity recognition. In Proceedings of ACL2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohini Srihari</author>
<author>Wei Li</author>
</authors>
<title>Information extraction supported question answering.</title>
<date>1999</date>
<booktitle>In TREC-8.</booktitle>
<contexts>
<context position="662" citStr="Srihari and Li, 1999" startWordPosition="90" endWordPosition="93">ty Recognition Jing Jiang and ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 {jiang4,czhai}@cs.uiuc.edu 1 Introduction Named Entity Recognition (NER) is the task of identifying and classifying phrases that denote certain types of named entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure d</context>
</contexts>
<marker>Srihari, Li, 1999</marker>
<rawString>Rohini Srihari and Wei Li. 1999. Information extraction supported question answering. In TREC-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of ICML-1997.</booktitle>
<contexts>
<context position="10768" citStr="Yang and Pedersen, 1997" startWordPosition="1793" endWordPosition="1796">res that are useful in all training domains . To achieve this goal, we first define a scoring function s : F → R as follows: m 1 s(f) = min rTi(f). (1) i=1 We then rank the features in decreasing order of their scores using the above scoring function. This is essentially to rank features according to their maximum rank maxi rTi(f) among the m domains. Let function rSeR return the rank of a feature in this combined, generalizability-based ranked list. The original ranking function rT used for individual domain feature ranking can use different criteria such as information gain or x2 statistic (Yang and Pedersen, 1997). In our experiments, we used a ranking function based on the model parameters of the classifier, which we will explain in Section 5.2. Next, we need to incorporate this preference for generalizable features into the classifier. Note that because this generalizability-based feature ranking method is independent of the learning algorithm, it can be applied on top of any classifier. In this work, we choose the logistic regression classifier. One way to incorporate the feature ranking into the classifier is to select the top-k features, where k is chosen by cross validation. There are two potenti</context>
<context position="20904" citStr="Yang and Pedersen, 1997" startWordPosition="3608" endWordPosition="3611"> state-of-the-art baseline method based on logistic regression. It uses a Gaussian prior with zero mean and uniform variance on all model parameters. It also employs 5-fold regular cross validation to pick the optimal variance for the prior. Regular feature selection is also considered in the baseline method, where the features are first ranked according to some criterion, and then cross validation is used to select the top-k features. We tested three popular regular feature ranking methods: feature frequency (F), information gain (IG), and k2 statistic (CHI). These methods were discussed in (Yang and Pedersen, 1997). However, with any of the three feature ranking criteria, cross validation showed that selecting all features gave the best average validation performance. Therefore, the best baseline method which we compare our method with uses all features. We call the baseline method BL. In our method, the generalizability-based feature ranking requires a first step of feature ranking within each training domain. While we could also use F, IG or CHI to rank features in each domain, to make our method self-contained, we used the following strategy. We first train a logistic regression model on each domain </context>
<context position="25568" citStr="Yang and Pedersen, 1997" startWordPosition="4402" endWordPosition="4405">ods. This suggests that the top-ranked features in DOM are indeed more suitable for adaptation to new domains than the top features ranked by the other methods. The figures also show that the ranking method DOM achieved better performance than the baseline over a wide range of b values, especially in F+YAM and M+YAF, whereas for methods F, IG and CHI, the performance quickly converged to the baseline performance as b increased. It is interesting to note the comparison between F and IG (or CHI). In general, when the test data is similar to the training data, IG (or CHI) is advantageous over F (Yang and Pedersen, 1997). However, in this case when the test domain is different from the training domains, F shows advantages for adaptation. A possible explanation is that frequent features are in general less likely to be domain-specific, and therefore feature frequency can also be used as a criterion to select generalizable features and to filter out domain-specific features, although it is still not as effective as the method we proposed. 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HM</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of ICML-1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
</authors>
<title>Named entity recognition using an HMM-based chunk tagger.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-2002.</booktitle>
<contexts>
<context position="911" citStr="Zhou and Su, 2002" startWordPosition="130" endWordPosition="133"> phrases that denote certain types of named entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degradation indicates th</context>
<context position="7567" citStr="Zhou and Su, 2002" startWordPosition="1225" endWordPosition="1228">ized as follows: In Section 2, we introduce a feature ranking method based on the generalizability of features across domains. In Section 3, we briefly introduce the logistic regression models for NER. We then propose a rankbased prior on logistic regression models and describe the domain-aware validation strategy in Section 4. The experiment results are presented in Section 5. Finally we discuss related work in Section 6 and conclude our work in Section 7. 2 Generalizability-Based Feature Ranking We take a commonly used approach and treat NER as a sequential tagging problem (Borthwick, 1999; Zhou and Su, 2002; Finkel et al., 2005). Each token is assigned the tag I if it is part of an NE and the tag O otherwise. Let x denote the feature vector for a token, and let y denote the tag for x. We first compute the probability p(ylx) for each token, using a 75 learned classifier. We then apply Viterbi algorithm to assign the most likely tag sequence to a sequence of tokens, i.e., a sentence. The features we use follow the common practice in NER, including surface word features, orthographic features, POS tags, substrings, and contextual features in a local window of size 5 around the target token (Finkel </context>
<context position="26190" citStr="Zhou and Su, 2002" startWordPosition="4505" endWordPosition="4508">wever, in this case when the test domain is different from the training domains, F shows advantages for adaptation. A possible explanation is that frequent features are in general less likely to be domain-specific, and therefore feature frequency can also be used as a criterion to select generalizable features and to filter out domain-specific features, although it is still not as effective as the method we proposed. 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2</context>
</contexts>
<marker>Zhou, Su, 2002</marker>
<rawString>Guodong Zhou and Jian Su. 2002. Named entity recognition using an HMM-based chunk tagger. In Proceedings of ACL-2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>