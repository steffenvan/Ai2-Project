<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001841">
<title confidence="0.9942015">
Can you summarize this? Identifying correlates of input difficulty for
generic multi-document summarization
</title>
<author confidence="0.99692">
Ani Nenkova Annie Louis
</author>
<affiliation confidence="0.8225325">
University of Pennsylvania University of Pennsylvania
Philadelphia, PA 19104, USA Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.999437">
nenkova@seas.upenn.edu lannie@seas.upenn.edu
</email>
<sectionHeader confidence="0.995675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998207357142857">
Different summarization requirements could
make the writing of a good summary more dif-
ficult, or easier. Summary length and the char-
acteristics of the input are such constraints in-
fluencing the quality of a potential summary.
In this paper we report the results of a quanti-
tative analysis on data from large-scale evalu-
ations of multi-document summarization, em-
pirically confirming this hypothesis. We fur-
ther show that features measuring the cohe-
siveness of the input are highly correlated with
eventual summary quality and that it is possi-
ble to use these as features to predict the diffi-
culty of new, unseen, summarization inputs.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961555555556">
In certain situations even the best automatic sum-
marizers or professional writers can find it hard to
write a good summary of a set of articles. If there
is no clear topic shared across the input articles, or
if they follow the development of the same event in
time for a longer period, it could become difficult
to decide what information is most representative
and should be conveyed in a summary. Similarly,
length requirements could pre-determine summary
quality—a short outline of a story might be confus-
ing and unclear but a page long discussion might
give an excellent overview of the same issue.
Even systems that perform well on average pro-
duce summaries of poor quality for some inputs. For
this reason, understanding what aspects of the in-
put make it difficult for summarization becomes an
interesting and important issue that has not been ad-
dressed in the summarization community untill now.
</bodyText>
<page confidence="0.97914">
825
</page>
<bodyText confidence="0.999889086956522">
In information retrieval, for example, the variable
system performance has been recognized as a re-
search challenge and numerous studies on identify-
ing query difficulty have been carried out (most re-
cently (Cronen-Townsend et al., 2002; Yom-Tov et
al., 2005; Carmel et al., 2006)).
In this paper we present results supporting the hy-
potheses that input topicality cohesiveness and sum-
mary length are among the factors that determine
summary quality regardless of the choice of summa-
rization strategy (Section 2). The data used for the
analyses comes from the annual Document Under-
standing Conference (DUC) in which various sum-
marization approaches are evaluated on common
data, with new test sets provided each year.
In later sections we define a suite of features cap-
turing aspects of the topicality cohesiveness of the
input (Section 3) and relate these to system perfor-
mance, identifying reliable correlates of input diffi-
culty (Section 4). Finally, in Section 5, we demon-
strate that the features can be used to build a clas-
sifier predicting summarization input difficulty with
accuracy considerably above chance level.
</bodyText>
<sectionHeader confidence="0.9893745" genericHeader="method">
2 Preliminary analysis and distinctions:
DUC 2001
</sectionHeader>
<bodyText confidence="0.99939875">
Generic multi-document summarization was fea-
tured as a task at the Document Understanding Con-
ference (DUC) in four years, 2001 through 2004.
In our study we use the DUC 2001 multi-document
task submissions as development data for in-depth
analysis and feature selection. There were 29 in-
put sets and 12 automatic summarizers participating
in the evaluation that year. Summaries of different
</bodyText>
<note confidence="0.818605">
Proceedings of ACL-08: HLT, pages 825–833,
</note>
<page confidence="0.547818">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999975866666667">
lengths were produced by each system: 50, 100, 200
and 400 words. Each summary was manually eval-
uated to determine the extent to which its content
overlaped with that of a human model, giving a cov-
erage score. The content comparison was performed
on a subsentence level and was based on elementary
discourse units in the model summary.1
The coverage scores are taken as an indicator of
difficultly of the input: systems achieve low cover-
age for difficult sets and higher coverage for easy
sets. Since we are interested in identifying charac-
teristics of generally difficult inputs rather than in
discovering what types of inputs might be difficult
for one given system, we use the average system
score per set as indicator of general difficulty.
</bodyText>
<subsectionHeader confidence="0.998686">
2.1 Analysis of variance
</subsectionHeader>
<bodyText confidence="0.999977961538462">
Before attempting to derive characteristics of inputs
difficult for summarization, we first confirm that in-
deed expected performance is influenced by the in-
put itself. We performed analysis of variance for
DUC 2001 data, with automatic system coverage
score as the dependent variable, to gain some insight
into the factors related to summarization difficulty.
The results of the ANOVA with input set, summa-
rizer identity and summary length as factors, as well
as the interaction between these, are shown in Ta-
ble 1.
As expected, summarizer identity is a significant
factor: some summarization strategies/systems are
more effective than others and produce summaries
with higher coverage score. More interestingly, the
input set and summary length factors are also highly
significant and explain more of the variability in
coverage scores than summarizer identity does, as
indicated by the larger values of the F statistic.
Length The average automatic summarizer cov-
erage scores increase steadily as length requirements
are relaxed, going up from 0.50 for 50-word sum-
maries to 0.76 for 400-word summaries as shown in
Table 2 (second row). The general trend we observe
is that on average systems are better at producing
summaries when more space is available. The dif-
</bodyText>
<footnote confidence="0.979293">
1The routinely used tool for automatic evaluation ROUGE
was adopted exactly because it was demonstrated it is highly
correlated with the manual DUC coverage scores (Lin and
Hovy, 2003a; Lin, 2004).
</footnote>
<table confidence="0.99946175">
Type 50 100 200 400
Human 1.00 1.17 1.38 1.29
Automatic 0.50 0.55 0.70 0.76
Baseline 0.41 0.46 0.52 0.57
</table>
<tableCaption confidence="0.985422333333333">
Table 2: Average human, system and baseline coverage
scores for different summary lengths of N words. N =
50, 100, 200, and 400.
</tableCaption>
<bodyText confidence="0.999763378378378">
ferences are statistically significant2 only between
50-word and 200- and 400-word summaries and be-
tween 100-word and 400-word summaries. The fact
that summary quality improves with increasing sum-
mary length has been observed in prior studies as
well (Radev and Tam, 2003; Lin and Hovy, 2003b;
Kolluru and Gotoh, 2005) but generally little atten-
tion has been paid to this fact in system development
and no specific user studies are available to show
what summary length might be most suitable for
specific applications. In later editions of the DUC
conference, only summaries of 100 words were pro-
duced, focusing development efforts on one of the
more demanding length restrictions. The interaction
between summary length and summarizer is small
but significant (Table 1), with certain summariza-
tion strategies more successful at particular sum-
mary lengths than at others.
Improved performance as measured by increase
in coverage scores is observed for human summa-
rizers as well (shown in the first row of Table 2).
Even the baseline systems (first n words of the most
recent article in the input or first sentences from
different input articles) show improvement when
longer summaries are allowed (performance shown
in the third row of the table). It is important to
notice that the difference between automatic sys-
tem and baseline performance increases as the sum-
mary length increases—the difference between sys-
tems and baselines coverage scores is around 0.1
for the shorter 50- and 100-word summaries but 0.2
for the longer summaries. This fact has favorable
implications for practical system developments be-
cause it indicates that in applications where some-
what longer summaries are appropriate, automati-
cally produced summaries will be much more infor-
mative than a baseline summary.
</bodyText>
<footnote confidence="0.83557">
2One-sided t-test, 95% level of significance.
</footnote>
<page confidence="0.995849">
826
</page>
<table confidence="0.996893142857143">
Factor DF Sum of squares Expected mean squares F stat Pr(&gt; F)
input 28 150.702 5.382 59.4227 0
summarizer 11 34.316 3.120 34.4429 0
length 3 16.082 5.361 59.1852 0
input:summarizer 306 65.492 0.214 2.3630 0
input:length 84 36.276 0.432 4.7680 0
summarizer:length 33 6.810 0.206 2.2784 0
</table>
<tableCaption confidence="0.999879">
Table 1: Analysis of variance for coverage scores of automatic systems with input, summarizer, and length as factors.
</tableCaption>
<bodyText confidence="0.999924607142857">
Input The input set itself is a highly significant
factor that influences the coverage scores that sys-
tems obtain: some inputs are handled by the systems
better than others. Moreover, the input interacts both
with the summarizers and the summary length.
This is an important finding for several reasons.
First, in system evaluations such as DUC the inputs
for summarization are manually selected by anno-
tators. There is no specific attempt to ensure that
the inputs across different years have on average the
same difficulty. Simply assuming this to be the case
could be misleading: it is possible in a given year to
have “easier” input test set compared to a previous
year. Then system performance across years can-
not be meaningfully compared, and higher system
scores would not be indicative of system improve-
ment between the evaluations.
Second, in summarization applications there is
some control over the input for summarization. For
example, related documents that need to summa-
rized could be split into smaller subsets that are more
amenable to summarization or routed to an appropri-
ate summarization system than can handle this kind
of input using a different strategy, as done for in-
stance in (McKeown et al., 2002).
Because of these important implications we inves-
tigate input characteristics and define various fea-
tures distinguishing easy inputs from difficult ones.
</bodyText>
<subsectionHeader confidence="0.965377">
2.2 Difficulty for people and machines
</subsectionHeader>
<bodyText confidence="0.99995025">
Before proceeding to the analysis of input difficulty
in multi-document summarization, it is worth men-
tioning that our study is primarily motivated by sys-
tem development needs and consequently the focus
is on finding out what inputs are easy or difficult
for automatic systems. Different factors might make
summarization difficult for people. In order to see to
what extent the notion of summarization input dif-
</bodyText>
<table confidence="0.994193">
summary length correlation
50 0.50
100 0.57*
200 0.77**
400 0.70**
</table>
<tableCaption confidence="0.988677333333333">
Table 3: Pearson correlation between average human and
system coverage scores on the DUC 2001 dataset. Sig-
nificance levels: *p &lt; 0.05 and **p &lt; 0.00001.
</tableCaption>
<bodyText confidence="0.999796266666667">
ficulty is shared between machines and people, we
computed the correlation between the average sys-
tem and average human coverage score at a given
summary length for all DUC 2001 test sets (shown
in Table 3). The correlation is highest for 200-word
summaries, 0.77, which is also highly significant.
For shorter summaries the correlation between hu-
man and system performance is not significant.
In the remaining part of the paper we deal ex-
clusively with difficulty as defined by system per-
formance, which differs from difficulty for people
summarizing the same material as evidenced by the
correlations in Table 3. We do not attempt to draw
conclusions about any cognitively relevant factors
involved in summarizing.
</bodyText>
<subsectionHeader confidence="0.999255">
2.3 Type of summary and difficulty
</subsectionHeader>
<bodyText confidence="0.993072454545455">
In DUC 2001, annotators prepared test sets from five
possible predefined input categories:3.
Single event (3 sets) Documents describing a single
event over a timeline (e.g. The Exxon Valdez
oil spill).
3Participants in the evaluation were aware of the different
categories of input and indeed some groups developed systems
that handled different types of input employing different strate-
gies (McKeown et al., 2001). In later years, the idea of multi-
strategy summarization has been further explored by (Lacatusu
et al., 2006)
</bodyText>
<page confidence="0.985566">
827
</page>
<bodyText confidence="0.995212985915493">
Subject (6 sets) Documents discussing a single
topic (e.g. Mad cow disease)
Biographical (2 sets) All documents in the input
provide information about the same person
(e.g. Elizabeth Taylor)
Multiple distinct events (12 sets) The documents
discuss different events of the same type (e.g.
different occasions of police misconduct).
Opinion (6 sets) Each document describes a differ-
ent perspective to a common topic (e.g. views
of the senate, congress, public, lawyers etc on
the decision by the senate to count illegal aliens
in the 1990 census).
Figure 1 shows the average system coverage score
for the different input types. The more topically co-
hesive input types such as biographical, single event
and subject, which are more focused on a single en-
tity or news item and narrower in scope, are eas-
ier for systems. The average system coverage score
for them is higher than for the non-cohesive sets
such as multiple distinct events and opinion sets, re-
gardless of summary length. The difference is even
more apparently clear when the scores are plotted af-
ter grouping input types into cohesive (biographical,
single event and subject) and non-cohesive (multi-
ple events and opinion). Such grouping also gives
the necessary power to perform statistical test for
significance, confirming the difference in coverage
scores for the two groups. This is not surprising: a
summary of documents describing multiple distinct
events of the same type is likely to require higher
degree of generalization and abstraction. Summa-
rizing opinions would in addition be highly subjec-
tive. A summary of a cohesive set meanwhile would
contain facts directly from the input and it would be
easier to determine which information is important.
The example human summaries for set D32 (single
event) and set D19 (opinions) shown below give an
idea of the potential difficulties automatic summa-
rizers have to deal with. set D32 On 24 March 1989,
the oil tanker Exxon Valdez ran aground on a reef near
Valdez, Alaska, spilling 8.4 million gallons of crude oil
into Prince William Sound. In two days, the oil spread
over 100 miles with a heavy toll on wildlife. Cleanup
proceeded at a slow pace, and a plan for cleaning 364
miles of Alaskan coastline was released. In June, the
tanker was refloated. By early 1990, only 5 to 9 percent of
spilled oil was recovered. A federal jury indicted Exxon
on five criminal charges and the Valdez skipper was guilty
of negligent discharge of oil.
set D19 Congress is debating whether or not to count ille-
gal aliens in the 1990 census. Congressional House seats
are apportioned to the states and huge sums of federal
money are allocated based on census population. Cali-
fornia, with an estimated half of all illegal aliens, will be
greatly affected. Those arguing for inclusion say that the
Constitution does not mention “citizens”, but rather, in-
structs that House apportionment be based on the “whole
number ofpersons” residing in the various states. Those
opposed say that the framers were unaware of this issue.
“Illegal aliens” did not exist in the U.S. until restrictive
immigration laws were passed in 1875.
The manual set-type labels give an intuitive idea
of what factors might be at play but it is desirable to
devise more specific measures to predict difficulty.
Do such measures exist? Is there a way to automati-
cally distinguish cohesive (easy) from non-cohesive
(difficult) sets? In the next section we define a num-
ber of features that aim to capture the cohesiveness
of an input set and show that some of them are in-
deed significantly related to set difficulty.
</bodyText>
<sectionHeader confidence="0.999741" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999950772727273">
We implemented 14 features for our analysis of in-
put set difficulty. The working hypothesis is that co-
hesive sets with clear topics are easier to summarize
and the features we define are designed to capture
aspects of input cohesiveness.
Number of sentences in the input, calculated
over all articles in the input set. Shorter inputs
should be easier as there will be less information loss
between the summary and the original material.
Vocabulary size of the input set, equal to the
number of unique words in the input. Smaller vo-
cabularies would be characteristic of easier sets.
Percentage of words used only once in the input.
The rationale behind this feature is that cohesive in-
put sets contain news articles dealing with a clearly
defined topic, so words will be reused across docu-
ments. Sets that cover disparate events and opinions
are likely to contain more words that appear in the
input only once.
Type-token ratio is a measure of the lexical vari-
ation in an input set and is equal to the input vo-
cabulary size divided by the number of words in the
</bodyText>
<page confidence="0.994701">
828
</page>
<figureCaption confidence="0.999781">
Figure 1: Average system coverage scores for summaries in a category
</figureCaption>
<bodyText confidence="0.997469090909091">
input. A high type-token ratio indicates there is little
(lexical) repetition in the input, a possible side-effect
of non-cohesiveness.
Entropy of the input set. Let X be a discrete ran-
dom variable taking values from the finite set V =
{w1, ..., wnj where V is the vocabulary of the in-
put set and wi are the words that appear in the input.
The probability distribution p(w) = Pr(X = w)
can be easily calculated using frequency counts from
the input. The entropy of the input set is equal to the
entropy of X:
</bodyText>
<equation confidence="0.899983333333333">
i=n
H(X) = − p(wi)log2 p(wi) (1)
i=1
</equation>
<bodyText confidence="0.986922918367347">
Average, minimum and maximum cosine over-
lap between the news articles in the input. Repeti-
tion in the input is often exploited as an indicator of
importance by different summarization approaches
(Luhn, 1958; Barzilay et al., 1999; Radev et al.,
2004; Nenkova et al., 2006). The more similar the
different documents in the input are to each other,
the more likely there is repetition across documents
at various granularities.
Cosine similarity between the document vector
representations is probably the easiest and most
commonly used among the various similarity mea-
sures. We use tf*idf weights in the vector represen-
tations, with term frequency (tf) normalized by the
total number of words in the document in order to re-
move bias resulting from high frequencies by virtue
of higher document length alone.
The cosine similarity between two (document
representation) vectors v1 and v2 is given by cosO =
v1.v2 A value of 0 indicates that the vectors are
||v1 |v2||
orthogonal and dissimilar, a value of 1 indicates per-
fectly similar documents in terms of the words con-
tained in them.
To compute the cosine overlap features, we find
the pairwise cosine similarity between each two
documents in an input set and compute their aver-
age. The minimum and maximum overlap features
are also computed as an indication of the overlap
bounds. We expect cohesive inputs to be composed
of similar documents, hence the cosine overlaps in
these sets of documents must be higher than those in
non-cohesive inputs.
KL divergence Another measure of relatedness
of the documents comprising an input set is the dif-
ference in word distributions in the input compared
to the word distribution in a large collection of di-
verse texts. If the input is found to be largely dif-
ferent from a generic collection, it is plausible to as-
sume that the input is not a random collection of ar-
ticles but rather is defined by a clear topic discussed
within and across the articles. It is reasonable to ex-
pect that the higher the divergence is, the easier it is
to define what is important in the article and hence
the easier it is to produce a good summary.
For computing the distribution of words in a gen-
eral background corpus, we used all the inputs sets
from DUC years 2001 to 2006. The divergence mea-
sure we used is the Kullback Leibler divergence, or
</bodyText>
<page confidence="0.997036">
829
</page>
<bodyText confidence="0.999982833333333">
relative entropy, between the input (I) and collection
language models. Let pinp(w) be the probability of
the word w in the input and pcoll(w) be the proba-
bility of the word occurring in the large background
collection. Then the relative entropy between the in-
put and the collection is given by
</bodyText>
<equation confidence="0.976048">
11 KL divergence = pinp(w)
wEI pinp(w)log2 pcoll(w) (2)
</equation>
<bodyText confidence="0.958525913043478">
Low KL divergence from a random background
collection may be characteristic of highly non-
cohesive inputs consisting of unrelated documents.
Number of topic signature terms for the input
set. The idea of topic signature terms was intro-
duced by Lin and Hovy (Lin and Hovy, 2000) in the
context of single document summarization, and was
later used in several multi-document summarization
systems (Conroy et al., 2006; Lacatusu et al., 2004;
Gupta et al., 2007).
Lin and Hovy’s idea was to automatically iden-
tify words that are descriptive for a cluster of docu-
ments on the same topic, such as the input to a multi-
document summarizer. We will call this cluster T.
Since the goal is to find descriptive terms for the
cluster, a comparison collection of documents not
on the topic is also necessary (we will call this back-
ground collection NT).
Given T and NT, the likelihood ratio statistic
(Dunning, 1994) is used to identify the topic signa-
ture terms. The probabilistic model of the data al-
lows for statistical inference in order to decide which
terms t are associated with T more strongly than
with NT than one would expect by chance.
More specifically, there are two possibilities for
the distribution of a term t: either it is very indicative
of the topic of cluster T, and appears more often in
T than in documents from NT, or the term t is not
topical and appears with equal frequency across both
T and NT. These two alternatives can be formally
written as the following hypotheses:
H1: P(t|T) = P(t|NT) = p (t is not a descrip-
tive term for the input)
H2: P(t|T) = p1 and P(t|NT) = p2 and p1 &gt;
p2 (t is a descriptive term)
In order to compute the likelihood of each hypoth-
esis given the collection of the background docu-
ments and the topic cluster, we view them as a se-
quence of words wi: w1w2 ... wN. The occurrence
of a given word t, wi = t, can thus be viewed a
Bernoulli trial with probability p of success, with
success occurring when wi = t and failure other-
wise.
The probability of observing the term t appearing
k times in N trials is given by the binomial distribu-
tion
</bodyText>
<equation confidence="0.659303333333333">
b(k, N, p) = I k pk (1 − p)N−k (3)
We can now compute
\
</equation>
<bodyText confidence="0.90138875">
Likelihood of the data given H1
λ = (4 )
Likelihood of the data given H2
which is equal to
</bodyText>
<equation confidence="0.9972">
b(ct,N,p)
λ = (5)
b(cT, NT, p1) ∗ b(cNT, NNT, p2)
</equation>
<bodyText confidence="0.997499185185185">
The maximum likelihood estimates for the proba-
bilities can be computed directly. p = ctN, where ct is
equal to the number of times term t appeared in the
entire corpus T+NT, and N is the number of words
in the entire corpus. Similarly, p1 = NTcT, where cT
is the number of times term t occurred in T and NT
is the number of all words in T. p2 = cNT
NNT , where
cNT is the number of times term t occurred in NT
and NNT is the total number of words in NT.
−2logλ has a well-know distribution: χ2. Bigger
values of −2logλ indicate that the likelihood of the
data under H2 is higher, and the χ2 distribution can
be used to determine when it is significantly higher
(−2logλ exceeding 10 gives a significance level of
0.001 and is the cut-off we used).
For terms for which the computed −2logλ is
higher than 10, we can infer that they occur more
often with the topic T than in a general corpus NT,
and we can dub them “topic signature terms”.
Percentage of signature terms in vocabulary
The number of signature terms gives the total count
of topic signatures over all the documents in the in-
put. However, the number of documents in an input
set and the size of the individual documents across
different sets are not the same. It is therefore possi-
ble that the mere count feature is biased to the length
</bodyText>
<page confidence="0.990004">
830
</page>
<bodyText confidence="0.999824421052632">
and number of documents in the input set. To ac-
count for this, we add the percentage of topic words
in the vocabulary as a feature.
Average, minimum and maximum topic sig-
nature overlap between the documents in the in-
put. Cosine similarity measures the overlap between
two documents based on all the words appearing in
them. A more refined document representation can
be defined by assuming the document vectors con-
tain only the topic signature words rather than all
words. A high overlap of topic words across two
documents is indicative of shared topicality. The
average, minimum and maximum pairwise cosine
overlap between the tf*idf weighted topic signature
vectors of the two documents are used as features
for predicting input cohesiveness. If the overlap is
large, then the topic is similar across the two docu-
ments and hence their combination will yield a co-
hesive input.
</bodyText>
<sectionHeader confidence="0.980402" genericHeader="method">
4 Feature selection
</sectionHeader>
<bodyText confidence="0.987344333333333">
Table 4 shows the results from a one-sided t-test
comparing the values of the various features for
the easy and difficult input set classes. The com-
parisons are for summary length of 100 words be-
cause in later years only such summaries were evalu-
ated. The binary easy/difficult classes were assigned
based on the average system coverage score for the
given set, with half of the sets assigned to each class.
In addition to the t-tests we also calculated Pear-
son’s correlation (shown in Table 5) between the fea-
tures and the average system coverage score for each
set. In the correlation analysis the input sets are not
classified into easy or difficult but rather the real val-
ued coverage scores are used directly. Overall, the
features that were identified by the t-test as most de-
scriptive of the differences between easy and diffi-
cult inputs were also the ones with higher correla-
tions with real-valued coverage scores.
Our expectations in defining the features are con-
firmed by the correlation results. For example, sys-
tems have low coverage scores for sets with high-
entropy vocabularies as indicated by the negative
and high by absolute value correlation (-0.4256).
Sets with high entropy are those in which there is
little repetition within and across different articles,
and for which it is subsequently difficult to deter-
feature
</bodyText>
<table confidence="0.6982074">
KL divergence*
% of sig. terms in vocab*
average cosine overlap*
vocabulary size*
set entropy*
average sig. term overlap*
max cosine overlap
max topic signature overlap
number of sentences
min topic signature overlap
number of signature terms
min cosine overlap
% of words used only once
type-token ratio
*Significant at a 95% confidence level(p &lt; 0.05)
</table>
<tableCaption confidence="0.996027666666667">
Table 4: Comparison of non-cohesive (average system
coverage score &lt; median average system score) vs cohe-
sive sets for summary length of 100 words
</tableCaption>
<bodyText confidence="0.9999662">
mine what is the most important content. On the
other hand, sets characterized by bigger KL diver-
gence are easier—there the distribution of words is
skewed compared to a general collection of articles,
with important topic words occurring more often.
Easy to summarize sets are characterized by low
entropy, small vocabulary, high average cosine and
average topic signature overlaps, high KL diver-
gence and a high percentage of the vocabulary con-
sists of topic signature terms.
</bodyText>
<sectionHeader confidence="0.963645" genericHeader="method">
5 Classification results
</sectionHeader>
<bodyText confidence="0.9979935625">
We used the 192 sets from multi-document summa-
rization DUC evaluations in 2002 (55 generic sets),
2003 (30 generic summary sets and 7 viewpoint sets)
and 2004 (50 generic and 50 biography sets) to train
and test a logistic regression classifier. The sets from
all years were pooled together and evenly divided
into easy and difficult inputs based on the average
system coverage score for each set.
Table 6 shows the results from 10-fold cross val-
idation. SIG is a classifier based on the six features
identified as significant in distinguishing easy from
difficult inputs based on a t-test comparison (Ta-
ble 4). SIG+yt has two additional features: the year
and the type of summarization input (generic, view-
point and biographical). ALL is a classifier based on
all 14 features defined in the previous section, and
</bodyText>
<table confidence="0.9990124">
t-stat p-value
-2.4725 0.01
-2.0956 0.02
-2.1227 0.02
1.9378 0.03
2.0288 0.03
-1.8803 0.04
-1.6968 0.05
-1.6380 0.06
1.4780 0.08
-0.9540 0.17
0.8057 0.21
-0.2654 0.39
0.2497 0.40
0.2343 0.41
</table>
<page confidence="0.953677">
831
</page>
<tableCaption confidence="0.99699">
Table 5: Correlation between coverage score and feature
values for the 29 DUC’01 100-word summaries.
</tableCaption>
<table confidence="0.9959528">
features accuracy P R F
SIG 56.25% 0.553 0.600 0.576
SIG+yt 69.27% 0.696 0.674 0.684
ALL 61.45% 0.615 0.589 0.600
ALL+yt 65.10% 0.643 0.663 0.653
</table>
<tableCaption confidence="0.962178333333333">
Table 6: Logistic regression classification results (accu-
racy, precision, recall and f-measure) for balanced data of
100-word summaries from DUC’02 through DUC’04.
</tableCaption>
<bodyText confidence="0.999343571428571">
ALL+yt also includes the year and task features.
Classification accuracy is considerably higher
than the 50% random baseline. Using all features
yields better accuracy (61%) than using solely the
6 significant features (accuracy of 56%). In both
cases, adding the year and task leads to extra 3%
net improvement. The best overall results are for
the SIG+yt classifier with net improvement over the
baseline equal to 20%. At the same time, it should
be taken into consideration that the amount of train-
ing data for our experiments is small: a total of 192
sets. Despite this, the measures of input cohesive-
ness capture enough information to result in a clas-
sifier with above-baseline performance.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999993613636364">
We have addressed the question of what makes the
writing of a summary for a multi-document input
difficult. Summary length is a significant factor,
with all summarizers (people, machines and base-
lines) performing better at longer summary lengths.
An exploratory analysis of DUC 2001 indicated that
systems produce better summaries for cohesive in-
puts dealing with a clear topic (single event, subject
and biographical sets) while non-cohesive sets about
multiple events and opposing opinions are consis-
tently of lower quality. We defined a number of fea-
tures aimed at capturing input cohesiveness, ranging
from simple features such as input length and size
to more sophisticated measures such as input set en-
tropy, KL divergence from a background corpus and
topic signature terms based on log-likelihood ratio.
Generally, easy to summarize sets are character-
ized by low entropy, small vocabulary, high average
cosine and average topic signature overlaps, high
KL divergence and a high percentage of the vocab-
ulary consists of topic signature terms. Experiments
with a logistic regression classifier based on the fea-
tures further confirms that input cohesiveness is pre-
dictive of the difficulty it will pose to automatic sum-
marizers.
Several important notes can be made. First, it is
important to develop strategies that can better handle
non-cohesive inputs, reducing fluctuations in sys-
tem performance. Most current systems are devel-
oped with the expectation they can handle any input
but this is evidently not the case and more attention
should be paid to the issue. Second, the interpre-
tations of year to year evaluations can be affected.
As demonstrated, the properties of the input have a
considerable influence on summarization quality. If
special care is not taken to ensure that the difficulty
of inputs in different evaluations is kept more or less
the same, results from the evaluations are not com-
parable and we cannot make general claims about
progress and system improvements between evalua-
tions. Finally, the presented results are clearly just a
beginning in understanding of summarization diffi-
culty. A more complete characterization of summa-
rization input will be necessary in the future.
</bodyText>
<sectionHeader confidence="0.998913" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9667075">
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan
</reference>
<figure confidence="0.996421933333333">
feature correlation
set entropy -0.4256
KL divergence 0.3663
vocabulary size -0.3610
% of sig. terms in vocab 0.3277
average sig. term overlap 0.2860
number of sentences -0.2511
max topic signature overlap 0.2416
average cosine overlap 0.2244
number of signature terms -0.1880
max cosine overlap 0.1337
min topic signature overlap 0.0401
min cosine overlap 0.0308
type-token ratio -0.0276
% of words used only once -0.0025
</figure>
<page confidence="0.97537">
832
</page>
<reference confidence="0.987298666666667">
Pelleg. 2006. What makes a query difficult? In SI-
GIR ’06: Proceedings of the 29th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 390–397.
John Conroy, Judith Schlesinger, and Dianne O’Leary.
2006. Topic-focused multi-document summarization
using an approximate oracle score. In Proceedings of
ACL, companion volume.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
2002. Predicting query performance. In Proceedings
of the 25th Annual International ACM SIGIR confer-
ence on Research and Development in Information Re-
trieval (SIGIR 2002), pages 299–306.
Ted Dunning. 1994. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61–74.
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 2007.
Measuring importance and query relevance in topic-
focused multi-document summarization. In ACL’07,
companion volume.
BalaKrishna Kolluru and Yoshihiko Gotoh. 2005. On
the subjectivity of human authored short summaries.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
Finley Lacatusu, Andrew Hickl, Sanda Harabagiu, and
Luke Nezda. 2004. Lite gistexter at duc2004. In Pro-
ceedings of the 4th Document Understanding Confer-
ence (DUC’04).
F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley,
B. Rink, P. Wang, and L. Taylor. 2006. Lcc’s gistexter
at duc 2006: Multi-strategy multi-document summa-
rization. In DUC’06.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of the 18th conference on Computa-
tional linguistics, pages 495–501.
Chin-Yew Lin and Eduard Hovy. 2003a. Automatic eval-
uation of summaries using n-gram co-occurance statis-
tics. In Proceedings ofHLT-NAACL 2003.
Chin-Yew Lin and Eduard Hovy. 2003b. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL 03
on Text summarization workshop, pages 73–80.
Chin-Yew Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In ACL Text Summarization
Workshop.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBMJournal of Research and Development,
2(2):159–165.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivassiloglou,
B. Schiffman, and S. Teufel. 2001. Columbia multi-
document summarization: Approach and evaluation.
In DUC’01.
Kathleen McKeown, Regina Barzilay, David Evans,
Vasleios Hatzivassiloglou, Judith Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with columbia’s newsblaster. In Pro-
ceedings of the 2nd Human Language Technologies
Conference HLT-02.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that influ-
ence summarization. In Proceedings of SIGIR.
Dragomir Radev and Daniel Tam. 2003. Single-
document and multi-document summary evaluation
via relative utility. In Poster session, International
Conference on Information and Knowledge Manage-
ment (CIKM’03).
Dragomir Radev, Hongyan Jing, Malgorzata Sty, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40:919–938.
Elad Yom-Tov, Shai Fine, David Carmel, and Adam Dar-
low. 2005. Learning to estimate query difficulty: in-
cluding applications to missing content detection and
distributed information retrieval. In SIGIR ’05: Pro-
ceedings of the 28th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 512–519.
</reference>
<page confidence="0.999155">
833
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961193">
<title confidence="0.9941705">Can you summarize this? Identifying correlates of input difficulty for generic multi-document summarization</title>
<author confidence="0.983641">Ani Nenkova Annie Louis</author>
<affiliation confidence="0.999964">University of Pennsylvania University of Pennsylvania</affiliation>
<address confidence="0.999164">Philadelphia, PA 19104, USA Philadelphia, PA 19104, USA</address>
<email confidence="0.999593">nenkova@seas.upenn.edulannie@seas.upenn.edu</email>
<abstract confidence="0.999214133333333">Different summarization requirements could make the writing of a good summary more difficult, or easier. Summary length and the characteristics of the input are such constraints influencing the quality of a potential summary. In this paper we report the results of a quantitative analysis on data from large-scale evaluations of multi-document summarization, empirically confirming this hypothesis. We further show that features measuring the cohesiveness of the input are highly correlated with eventual summary quality and that it is possible to use these as features to predict the difficulty of new, unseen, summarization inputs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17086" citStr="Barzilay et al., 1999" startWordPosition="2776" endWordPosition="2779">ut set. Let X be a discrete random variable taking values from the finite set V = {w1, ..., wnj where V is the vocabulary of the input set and wi are the words that appear in the input. The probability distribution p(w) = Pr(X = w) can be easily calculated using frequency counts from the input. The entropy of the input set is equal to the entropy of X: i=n H(X) = − p(wi)log2 p(wi) (1) i=1 Average, minimum and maximum cosine overlap between the news articles in the input. Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn, 1958; Barzilay et al., 1999; Radev et al., 2004; Nenkova et al., 2006). The more similar the different documents in the input are to each other, the more likely there is repetition across documents at various granularities. Cosine similarity between the document vector representations is probably the easiest and most commonly used among the various similarity measures. We use tf*idf weights in the vector representations, with term frequency (tf) normalized by the total number of words in the document in order to remove bias resulting from high frequencies by virtue of higher document length alone. The cosine similarity </context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>Regina Barzilay, Kathleen McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Carmel</author>
<author>Elad Yom-Tov</author>
<author>Adam Darlow</author>
<author>Dan Pelleg</author>
</authors>
<title>What makes a query difficult? In</title>
<date>2006</date>
<booktitle>SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>390--397</pages>
<contexts>
<context position="2131" citStr="Carmel et al., 2006" startWordPosition="332" endWordPosition="335">excellent overview of the same issue. Even systems that perform well on average produce summaries of poor quality for some inputs. For this reason, understanding what aspects of the input make it difficult for summarization becomes an interesting and important issue that has not been addressed in the summarization community untill now. 825 In information retrieval, for example, the variable system performance has been recognized as a research challenge and numerous studies on identifying query difficulty have been carried out (most recently (Cronen-Townsend et al., 2002; Yom-Tov et al., 2005; Carmel et al., 2006)). In this paper we present results supporting the hypotheses that input topicality cohesiveness and summary length are among the factors that determine summary quality regardless of the choice of summarization strategy (Section 2). The data used for the analyses comes from the annual Document Understanding Conference (DUC) in which various summarization approaches are evaluated on common data, with new test sets provided each year. In later sections we define a suite of features capturing aspects of the topicality cohesiveness of the input (Section 3) and relate these to system performance, i</context>
</contexts>
<marker>Carmel, Yom-Tov, Darlow, Pelleg, 2006</marker>
<rawString>David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg. 2006. What makes a query difficult? In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 390–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Conroy</author>
<author>Judith Schlesinger</author>
<author>Dianne O’Leary</author>
</authors>
<title>Topic-focused multi-document summarization using an approximate oracle score.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL, companion</booktitle>
<pages>volume.</pages>
<marker>Conroy, Schlesinger, O’Leary, 2006</marker>
<rawString>John Conroy, Judith Schlesinger, and Dianne O’Leary. 2006. Topic-focused multi-document summarization using an approximate oracle score. In Proceedings of ACL, companion volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Cronen-Townsend</author>
<author>Yun Zhou</author>
<author>W Bruce Croft</author>
</authors>
<title>Predicting query performance.</title>
<date>2002</date>
<booktitle>In Proceedings of the 25th Annual International ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR</booktitle>
<pages>299--306</pages>
<contexts>
<context position="2087" citStr="Cronen-Townsend et al., 2002" startWordPosition="324" endWordPosition="327">nd unclear but a page long discussion might give an excellent overview of the same issue. Even systems that perform well on average produce summaries of poor quality for some inputs. For this reason, understanding what aspects of the input make it difficult for summarization becomes an interesting and important issue that has not been addressed in the summarization community untill now. 825 In information retrieval, for example, the variable system performance has been recognized as a research challenge and numerous studies on identifying query difficulty have been carried out (most recently (Cronen-Townsend et al., 2002; Yom-Tov et al., 2005; Carmel et al., 2006)). In this paper we present results supporting the hypotheses that input topicality cohesiveness and summary length are among the factors that determine summary quality regardless of the choice of summarization strategy (Section 2). The data used for the analyses comes from the annual Document Understanding Conference (DUC) in which various summarization approaches are evaluated on common data, with new test sets provided each year. In later sections we define a suite of features capturing aspects of the topicality cohesiveness of the input (Section </context>
</contexts>
<marker>Cronen-Townsend, Zhou, Croft, 2002</marker>
<rawString>Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2002. Predicting query performance. In Proceedings of the 25th Annual International ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2002), pages 299–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="20433" citStr="Dunning, 1994" startWordPosition="3354" endWordPosition="3355">text of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). Lin and Hovy’s idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer. We will call this cluster T. Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection NT). Given T and NT, the likelihood ratio statistic (Dunning, 1994) is used to identify the topic signature terms. The probabilistic model of the data allows for statistical inference in order to decide which terms t are associated with T more strongly than with NT than one would expect by chance. More specifically, there are two possibilities for the distribution of a term t: either it is very indicative of the topic of cluster T, and appears more often in T than in documents from NT, or the term t is not topical and appears with equal frequency across both T and NT. These two alternatives can be formally written as the following hypotheses: H1: P(t|T) = P(t</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>Ani Nenkova</author>
<author>Dan Jurafsky</author>
</authors>
<title>Measuring importance and query relevance in topicfocused multi-document summarization.</title>
<date>2007</date>
<booktitle>In ACL’07, companion volume.</booktitle>
<contexts>
<context position="19989" citStr="Gupta et al., 2007" startWordPosition="3273" endWordPosition="3276">e large background collection. Then the relative entropy between the input and the collection is given by 11 KL divergence = pinp(w) wEI pinp(w)log2 pcoll(w) (2) Low KL divergence from a random background collection may be characteristic of highly noncohesive inputs consisting of unrelated documents. Number of topic signature terms for the input set. The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). Lin and Hovy’s idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer. We will call this cluster T. Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection NT). Given T and NT, the likelihood ratio statistic (Dunning, 1994) is used to identify the topic signature terms. The probabilistic model of the data allows for statistical inference in order to decide which terms t are as</context>
</contexts>
<marker>Gupta, Nenkova, Jurafsky, 2007</marker>
<rawString>Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 2007. Measuring importance and query relevance in topicfocused multi-document summarization. In ACL’07, companion volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BalaKrishna Kolluru</author>
<author>Yoshihiko Gotoh</author>
</authors>
<title>On the subjectivity of human authored short summaries.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="6324" citStr="Kolluru and Gotoh, 2005" startWordPosition="1002" endWordPosition="1005">d with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). Type 50 100 200 400 Human 1.00 1.17 1.38 1.29 Automatic 0.50 0.55 0.70 0.76 Baseline 0.41 0.46 0.52 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been paid to this fact in system development and no specific user studies are available to show what summary length might be most suitable for specific applications. In later editions of the DUC conference, only summaries of 100 words were produced, focusing development efforts on one of the more demanding length restrictions. The interaction between summary length and summarizer is small but significant (Table 1), with certain summarization strategies more successful at particular summary lengths than at others. Improved performance as measured by increase </context>
</contexts>
<marker>Kolluru, Gotoh, 2005</marker>
<rawString>BalaKrishna Kolluru and Yoshihiko Gotoh. 2005. On the subjectivity of human authored short summaries. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
<author>Sanda Harabagiu</author>
<author>Luke Nezda</author>
</authors>
<title>Lite gistexter at duc2004.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th Document Understanding Conference (DUC’04).</booktitle>
<contexts>
<context position="19968" citStr="Lacatusu et al., 2004" startWordPosition="3269" endWordPosition="3272">he word occurring in the large background collection. Then the relative entropy between the input and the collection is given by 11 KL divergence = pinp(w) wEI pinp(w)log2 pcoll(w) (2) Low KL divergence from a random background collection may be characteristic of highly noncohesive inputs consisting of unrelated documents. Number of topic signature terms for the input set. The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). Lin and Hovy’s idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer. We will call this cluster T. Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection NT). Given T and NT, the likelihood ratio statistic (Dunning, 1994) is used to identify the topic signature terms. The probabilistic model of the data allows for statistical inference in order to decide</context>
</contexts>
<marker>Lacatusu, Hickl, Harabagiu, Nezda, 2004</marker>
<rawString>Finley Lacatusu, Andrew Hickl, Sanda Harabagiu, and Luke Nezda. 2004. Lite gistexter at duc2004. In Proceedings of the 4th Document Understanding Conference (DUC’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lacatusu</author>
<author>A Hickl</author>
<author>K Roberts</author>
<author>Y Shi</author>
<author>J Bensley</author>
<author>B Rink</author>
<author>P Wang</author>
<author>L Taylor</author>
</authors>
<title>Lcc’s gistexter at duc 2006: Multi-strategy multi-document summarization.</title>
<date>2006</date>
<booktitle>In DUC’06.</booktitle>
<contexts>
<context position="11575" citStr="Lacatusu et al., 2006" startWordPosition="1840" endWordPosition="1843">bout any cognitively relevant factors involved in summarizing. 2.3 Type of summary and difficulty In DUC 2001, annotators prepared test sets from five possible predefined input categories:3. Single event (3 sets) Documents describing a single event over a timeline (e.g. The Exxon Valdez oil spill). 3Participants in the evaluation were aware of the different categories of input and indeed some groups developed systems that handled different types of input employing different strategies (McKeown et al., 2001). In later years, the idea of multistrategy summarization has been further explored by (Lacatusu et al., 2006) 827 Subject (6 sets) Documents discussing a single topic (e.g. Mad cow disease) Biographical (2 sets) All documents in the input provide information about the same person (e.g. Elizabeth Taylor) Multiple distinct events (12 sets) The documents discuss different events of the same type (e.g. different occasions of police misconduct). Opinion (6 sets) Each document describes a different perspective to a common topic (e.g. views of the senate, congress, public, lawyers etc on the decision by the senate to count illegal aliens in the 1990 census). Figure 1 shows the average system coverage score </context>
</contexts>
<marker>Lacatusu, Hickl, Roberts, Shi, Bensley, Rink, Wang, Taylor, 2006</marker>
<rawString>F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley, B. Rink, P. Wang, and L. Taylor. 2006. Lcc’s gistexter at duc 2006: Multi-strategy multi-document summarization. In DUC’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="19808" citStr="Lin and Hovy, 2000" startWordPosition="3245" endWordPosition="3248"> entropy, between the input (I) and collection language models. Let pinp(w) be the probability of the word w in the input and pcoll(w) be the probability of the word occurring in the large background collection. Then the relative entropy between the input and the collection is given by 11 KL divergence = pinp(w) wEI pinp(w)log2 pcoll(w) (2) Low KL divergence from a random background collection may be characteristic of highly noncohesive inputs consisting of unrelated documents. Number of topic signature terms for the input set. The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). Lin and Hovy’s idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer. We will call this cluster T. Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection NT). Given T and NT, the likelihood ratio </context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th conference on Computational linguistics, pages 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurance statistics.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<contexts>
<context position="5757" citStr="Lin and Hovy, 2003" startWordPosition="907" endWordPosition="910">scores than summarizer identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). Type 50 100 200 400 Human 1.00 1.17 1.38 1.29 Automatic 0.50 0.55 0.70 0.76 Baseline 0.41 0.46 0.52 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention h</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003a. Automatic evaluation of summaries using n-gram co-occurance statistics. In Proceedings ofHLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The potential and limitations of automatic sentence extraction for summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 03 on Text summarization workshop,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="5757" citStr="Lin and Hovy, 2003" startWordPosition="907" endWordPosition="910">scores than summarizer identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). Type 50 100 200 400 Human 1.00 1.17 1.38 1.29 Automatic 0.50 0.55 0.70 0.76 Baseline 0.41 0.46 0.52 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention h</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003b. The potential and limitations of automatic sentence extraction for summarization. In Proceedings of the HLT-NAACL 03 on Text summarization workshop, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In ACL Text Summarization Workshop.</booktitle>
<contexts>
<context position="5770" citStr="Lin, 2004" startWordPosition="911" endWordPosition="912">r identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). Type 50 100 200 400 Human 1.00 1.17 1.38 1.29 Automatic 0.50 0.55 0.70 0.76 Baseline 0.41 0.46 0.52 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been paid </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In ACL Text Summarization Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBMJournal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="17063" citStr="Luhn, 1958" startWordPosition="2774" endWordPosition="2775">y of the input set. Let X be a discrete random variable taking values from the finite set V = {w1, ..., wnj where V is the vocabulary of the input set and wi are the words that appear in the input. The probability distribution p(w) = Pr(X = w) can be easily calculated using frequency counts from the input. The entropy of the input set is equal to the entropy of X: i=n H(X) = − p(wi)log2 p(wi) (1) i=1 Average, minimum and maximum cosine overlap between the news articles in the input. Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn, 1958; Barzilay et al., 1999; Radev et al., 2004; Nenkova et al., 2006). The more similar the different documents in the input are to each other, the more likely there is repetition across documents at various granularities. Cosine similarity between the document vector representations is probably the easiest and most commonly used among the various similarity measures. We use tf*idf weights in the vector representations, with term frequency (tf) normalized by the total number of words in the document in order to remove bias resulting from high frequencies by virtue of higher document length alone.</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBMJournal of Research and Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>R Barzilay</author>
<author>D Evans</author>
<author>V Hatzivassiloglou</author>
<author>B Schiffman</author>
<author>S Teufel</author>
</authors>
<title>Columbia multidocument summarization: Approach and evaluation.</title>
<date>2001</date>
<booktitle>In DUC’01.</booktitle>
<contexts>
<context position="11465" citStr="McKeown et al., 2001" startWordPosition="1822" endWordPosition="1825">rizing the same material as evidenced by the correlations in Table 3. We do not attempt to draw conclusions about any cognitively relevant factors involved in summarizing. 2.3 Type of summary and difficulty In DUC 2001, annotators prepared test sets from five possible predefined input categories:3. Single event (3 sets) Documents describing a single event over a timeline (e.g. The Exxon Valdez oil spill). 3Participants in the evaluation were aware of the different categories of input and indeed some groups developed systems that handled different types of input employing different strategies (McKeown et al., 2001). In later years, the idea of multistrategy summarization has been further explored by (Lacatusu et al., 2006) 827 Subject (6 sets) Documents discussing a single topic (e.g. Mad cow disease) Biographical (2 sets) All documents in the input provide information about the same person (e.g. Elizabeth Taylor) Multiple distinct events (12 sets) The documents discuss different events of the same type (e.g. different occasions of police misconduct). Opinion (6 sets) Each document describes a different perspective to a common topic (e.g. views of the senate, congress, public, lawyers etc on the decisio</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Schiffman, Teufel, 2001</marker>
<rawString>K. McKeown, R. Barzilay, D. Evans, V. Hatzivassiloglou, B. Schiffman, and S. Teufel. 2001. Columbia multidocument summarization: Approach and evaluation. In DUC’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasleios Hatzivassiloglou</author>
<author>Judith Klavans</author>
<author>Ani Nenkova</author>
<author>Carl Sable</author>
<author>Barry Schiffman</author>
<author>Sergey Sigelman</author>
</authors>
<title>Tracking and summarizing news on a daily basis with columbia’s newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd Human Language Technologies Conference HLT-02.</booktitle>
<contexts>
<context position="9474" citStr="McKeown et al., 2002" startWordPosition="1509" endWordPosition="1512">given year to have “easier” input test set compared to a previous year. Then system performance across years cannot be meaningfully compared, and higher system scores would not be indicative of system improvement between the evaluations. Second, in summarization applications there is some control over the input for summarization. For example, related documents that need to summarized could be split into smaller subsets that are more amenable to summarization or routed to an appropriate summarization system than can handle this kind of input using a different strategy, as done for instance in (McKeown et al., 2002). Because of these important implications we investigate input characteristics and define various features distinguishing easy inputs from difficult ones. 2.2 Difficulty for people and machines Before proceeding to the analysis of input difficulty in multi-document summarization, it is worth mentioning that our study is primarily motivated by system development needs and consequently the focus is on finding out what inputs are easy or difficult for automatic systems. Different factors might make summarization difficult for people. In order to see to what extent the notion of summarization inpu</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen McKeown, Regina Barzilay, David Evans, Vasleios Hatzivassiloglou, Judith Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and summarizing news on a daily basis with columbia’s newsblaster. In Proceedings of the 2nd Human Language Technologies Conference HLT-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
<author>Kathleen McKeown</author>
</authors>
<title>A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="17129" citStr="Nenkova et al., 2006" startWordPosition="2784" endWordPosition="2787"> taking values from the finite set V = {w1, ..., wnj where V is the vocabulary of the input set and wi are the words that appear in the input. The probability distribution p(w) = Pr(X = w) can be easily calculated using frequency counts from the input. The entropy of the input set is equal to the entropy of X: i=n H(X) = − p(wi)log2 p(wi) (1) i=1 Average, minimum and maximum cosine overlap between the news articles in the input. Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn, 1958; Barzilay et al., 1999; Radev et al., 2004; Nenkova et al., 2006). The more similar the different documents in the input are to each other, the more likely there is repetition across documents at various granularities. Cosine similarity between the document vector representations is probably the easiest and most commonly used among the various similarity measures. We use tf*idf weights in the vector representations, with term frequency (tf) normalized by the total number of words in the document in order to remove bias resulting from high frequencies by virtue of higher document length alone. The cosine similarity between two (document representation) vecto</context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Daniel Tam</author>
</authors>
<title>Singledocument and multi-document summary evaluation via relative utility.</title>
<date>2003</date>
<booktitle>In Poster session, International Conference on Information and Knowledge Management (CIKM’03).</booktitle>
<contexts>
<context position="6277" citStr="Radev and Tam, 2003" startWordPosition="994" endWordPosition="997">it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). Type 50 100 200 400 Human 1.00 1.17 1.38 1.29 Automatic 0.50 0.55 0.70 0.76 Baseline 0.41 0.46 0.52 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been paid to this fact in system development and no specific user studies are available to show what summary length might be most suitable for specific applications. In later editions of the DUC conference, only summaries of 100 words were produced, focusing development efforts on one of the more demanding length restrictions. The interaction between summary length and summarizer is small but significant (Table 1), with certain summarization strategies more successful at particular summary lengths than at others</context>
</contexts>
<marker>Radev, Tam, 2003</marker>
<rawString>Dragomir Radev and Daniel Tam. 2003. Singledocument and multi-document summary evaluation via relative utility. In Poster session, International Conference on Information and Knowledge Management (CIKM’03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Sty</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>40--919</pages>
<contexts>
<context position="17106" citStr="Radev et al., 2004" startWordPosition="2780" endWordPosition="2783">rete random variable taking values from the finite set V = {w1, ..., wnj where V is the vocabulary of the input set and wi are the words that appear in the input. The probability distribution p(w) = Pr(X = w) can be easily calculated using frequency counts from the input. The entropy of the input set is equal to the entropy of X: i=n H(X) = − p(wi)log2 p(wi) (1) i=1 Average, minimum and maximum cosine overlap between the news articles in the input. Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn, 1958; Barzilay et al., 1999; Radev et al., 2004; Nenkova et al., 2006). The more similar the different documents in the input are to each other, the more likely there is repetition across documents at various granularities. Cosine similarity between the document vector representations is probably the easiest and most commonly used among the various similarity measures. We use tf*idf weights in the vector representations, with term frequency (tf) normalized by the total number of words in the document in order to remove bias resulting from high frequencies by virtue of higher document length alone. The cosine similarity between two (documen</context>
</contexts>
<marker>Radev, Jing, Sty, Tam, 2004</marker>
<rawString>Dragomir Radev, Hongyan Jing, Malgorzata Sty, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elad Yom-Tov</author>
<author>Shai Fine</author>
<author>David Carmel</author>
<author>Adam Darlow</author>
</authors>
<title>Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.</title>
<date>2005</date>
<booktitle>In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>512--519</pages>
<contexts>
<context position="2109" citStr="Yom-Tov et al., 2005" startWordPosition="328" endWordPosition="331">cussion might give an excellent overview of the same issue. Even systems that perform well on average produce summaries of poor quality for some inputs. For this reason, understanding what aspects of the input make it difficult for summarization becomes an interesting and important issue that has not been addressed in the summarization community untill now. 825 In information retrieval, for example, the variable system performance has been recognized as a research challenge and numerous studies on identifying query difficulty have been carried out (most recently (Cronen-Townsend et al., 2002; Yom-Tov et al., 2005; Carmel et al., 2006)). In this paper we present results supporting the hypotheses that input topicality cohesiveness and summary length are among the factors that determine summary quality regardless of the choice of summarization strategy (Section 2). The data used for the analyses comes from the annual Document Understanding Conference (DUC) in which various summarization approaches are evaluated on common data, with new test sets provided each year. In later sections we define a suite of features capturing aspects of the topicality cohesiveness of the input (Section 3) and relate these to</context>
</contexts>
<marker>Yom-Tov, Fine, Carmel, Darlow, 2005</marker>
<rawString>Elad Yom-Tov, Shai Fine, David Carmel, and Adam Darlow. 2005. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 512–519.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>