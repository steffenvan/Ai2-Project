<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014930">
<title confidence="0.999312">
A Method for Unsupervised Broad-Coverage
Lexical Error Detection and Correction
</title>
<author confidence="0.99919">
Nai-Lung Tsao David Wible
</author>
<affiliation confidence="0.850902333333333">
Graduate Institute of Learning and Instruction Graduate Institute of Learning and Instruction
National Central University National Central University
Jhongli City, Taoyuan County 32001, Taiwan Jhongli City, Taoyuan County 32001, Taiwan
</affiliation>
<email confidence="0.992007">
beaktsao@gmail.com Wible45@yahoo.com
</email>
<sectionHeader confidence="0.998551" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999531666666667">
We describe and motivate an unsupervised
lexical error detection and correction algo-
rithm and its application in a tool called Lex-
bar appearing as a query box on the Web
browser toolbar or as a search engine inter-
face. Lexbar accepts as user input candidate
strings of English to be checked for accept-
ability and, where errors are detected, offers
corrections. We introduce the notion of hy-
brid n-gram and extract these from BNC as
the knowledgebase against which to compare
user input. An extended notion of edit dis-
tance is used to identify most likely candi-
dates for correcting detected errors. Results
are illustrated with four types of errors.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972538461538">
We describe and motivate an unsupervised lexical
error detection and correction algorithm and its
application in a tool called Lexbar appearing as a
query box in a web-based corpus search engine or
on the Web browser toolbar. The tool is intended
as a proxy for search engines in the common prac-
tice where users put search engines to use as error
checkers. A problem with this use of search en-
gines like Google is that such searches commonly
provide false positives, hits for strings that contain
errors. Lexbar accepts as user input candidate
strings of English to be checked for acceptability
and, where errors are detected, offers corrections.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999960366666666">
Among the many works on error detection, re-
cently unsupervised error detection approaches
have been proposed, such as [Chodorow and Lea-
cock, 2000] and [Quixal and Badia 2008]. These
use contextual features and statistical word asso-
ciation measurement to decide if the detected bi-
gram or trigram is an error or not. To our
knowledge, such unsupervised methods have not
been applied in error correction. [Gamon et al
2008] and [Felice and Pulman 2008] propose un-
supervised approaches to build a probabilistic
model for detecting errors (prepositions and arti-
cles) and providing correct answers. They also
typically focus on a particular type of error, usu-
ally limited to a specific word class such as prepo-
sition errors, often in a pre-determined
paradigmatic slot. Our approach reported here is
unsupervised in both detection and correction and
is not tailored to a specific target error subtype or
targeted to a specific position in a string. More
generally the family of error types suitable for this
approach are lexical or lexico-grammatical errors
since detection and correction are based on pat-
terns of word use detected statistically. At the core
of our approach is a bank of what we call “hybrid
n-grams” extracted from BNC to serve as the tar-
get knowledge against which learner input is
compared for detection and correction. We illus-
trate the single algorithm with results on four dif-
ferent categories of errors.
</bodyText>
<sectionHeader confidence="0.93313" genericHeader="method">
3 Overview of the Algorithm
</sectionHeader>
<bodyText confidence="0.999405833333333">
The Lexbar application consists of two main
components: (1) the target language knowledge-
base of hybrid n-grams that serves as the standard
against which learner production is examined for
errors, and (2) the error detection and correction
algorithm that uses this knowledgebase to evalu-
</bodyText>
<page confidence="0.992233">
51
</page>
<note confidence="0.815634">
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 51–54,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999817108108108">
ate learner production through matching and edit
distance. Relatively broad coverage is achieved
from one algorithm since no specific error type is
targeted but violations of word behaviors patterns.
Typically, n-grams are contiguous sequences of
lemmas or specific word forms. Using traditional
n-grams and string matching against them as a
means of error detection leads to weak precision
since the absence of a specific n-gram in a stan-
dard corpus does not render it an error. To address
this limitation, we extend the notion of n-gram to
include in the string not only lemmas or word
forms but parts-of-speech as well. For example,
the chunk point of view can be part of a longer
string from my point of view. Here, the preposition
from is non-substitutable whereas the possessive
pronoun my can be replaced by others of the same
POS (his/her/your/etc.). Hence, replacing the one
results in an error (*in my point of view1) while
replacing the other is fine (from her/his/their/our
point of view). The purpose of hybrid n-grams is
to introduce the flexibility to capture the appropri-
ate level of abstraction for each slot in a lexical
chunk. Hybrid n-grams permit any combination of
word forms, lemmas, POSs in a string (see details
below). Thus the hybrid n-gram for from my point
of view is from [dps] point of view2.
For a string of input submitted for error check-
ing, the algorithm first does a matching operation
between the input string and the hybrid n-gram
bank. The second step for input is finding hybrid
n-grams which nearly match the input, using edit
distance to measure nearness or similarity. Hybrid
n-grams with a distance of 1 or less from the input
string are candidates as correction suggestions and
are ranked, least distant from the input string
ranked as top correction suggestion.
</bodyText>
<sectionHeader confidence="0.992201" genericHeader="method">
4 The Knowledgebase: Hybrid N-grams
</sectionHeader>
<bodyText confidence="0.999972428571428">
As mentioned in Section 3, a hybrid n-gram bank
will be needed. In our model, each slot has four
levels of representation to choose from: word
form (enjoys but not enjoy or enjoying, etc);
lemma (representing all word forms of that lex-
eme, e.g., enjoy, enjoys, and enjoyed, etc); de-
tailed POS (CLAWS5 with 46 different POSs);
</bodyText>
<footnote confidence="0.992695666666667">
1 We use * to represent the error part in n-gram string.
2 We use [] to represent POS categories. [dps] is the
CLAWS5 tag for possessive pronoun.
</footnote>
<bodyText confidence="0.999982063829788">
rough POS (9 different POSs)3. The main chal-
lenge is to extract hybrid n-grams which are the
optimum combination of representations for each
slot to represent a lexical chunk or pattern. One
key to this is a pruning method (described below).
Clearly, compared with traditional n-gram extrac-
tion, the size of our hybrid n-gram bank size will
be extremely large if we save all the combinations
that can be generated for each n-gram. Consider-
ing the example from my point of view and setting
point as the target word, if we only extract hybrid
5-gram strings for it, we will get 2*44=512 (two
forms of noun point and four forms of others) dif-
ferent hybrid 5-grams. This entails many disad-
vantages, for example in storage space and
processing time. Therefore, we apply several
pruning approaches to keep only useful hybrid n-
grams in the bank. Another motivation for pruning
the bank is to reach optimum recall and precision.
The choice of which hybrid n-grams to retain in or
discard from the bank directly determines which
input strings would be judged as errors and what
candidate corrections would be generated for er-
rors. We illustrate the effects of pruning below.
The first criterion for pruning is frequency.
Only hybrid n-grams with a frequency greater
than the threshold are saved. The second criterion
is called subset pruning. There will be overlap
among different hybrid n-grams. For example, the
chunk from my point of view could be represented
by dozens of hybrid n-grams. Two of them are: (1)
from [dps] point of view, and (2) from my point of
view. Notice an input string from her point of view
would match (1) but not (2). Here the optimum n-
gram is (1) because it includes all cases covered
by (2) but other acceptable ones as well. Crucially,
it is not the case that the more general hybrid n-
gram will always yield the more optimum results,
however. This must be determined case by case.
Consider the first slot in the same chunk from my
point of view. The following two versions could
represent that chunk: (3) from [dps] point of view
and (4) [prp] [dps] point of view4. Notice here,
however, that it will be the more specific rather
than the more inclusive version that is to be pre-
ferred. (3) specifies the exact preposition for the
chunk whereas (4) would accept any preposition
</bodyText>
<footnote confidence="0.967244666666667">
3 Rough POS includes verb, noun, adj, adv, conj, interj, prep,
pron, vm0.
4 [prp] is the CLAWS5 tag for preposition.
</footnote>
<page confidence="0.997338">
52
</page>
<bodyText confidence="0.998419538461538">
(or [prp]) occurring in the first slot. But indeed
from is not freely substitutable in this chunk (cf
*in my point of view). Thus in each slot in each
chunk, pruning checks each potential hybrid n-
gram against the target corpus to determine statis-
tically the n-grams that capture the optimum de-
gree of substitutability or frozenness for each slot.
This creates an extremely flexible means of
representing the knowledgebase. Consider verb
complement selection. In examples such as They
enjoy swimming, the level of generalization is dif-
ferent for the governing verb slot (enjoy) on the
one hand and the complement (swimming) on the
other. The right generalization for the complement
is a specific verb form but not specific to any one
verb. This slot is captured under the CLAWS5
POS [vvg] 5 , thus permitting enjoy swim-
ming/reading/sleeping, but not enjoy to
swim/swam and so on. Unlike the complement,
the governing verb slot here is a specific lexeme
(enjoy swimming but not hope swimming; cf hope
to swim) and moreover, it permits that lexeme in
any of its word forms (enjoy/enjoying/enjoyed
swimming). A hybrid n-gram representation has
the power to capture these different levels of gen-
eralization and restriction in one representation.
Here is how pruning is done. First, we set a fil-
ter factor ε, where 0&lt;ε&lt;1. Assume x and y are
two hybrid n-grams and len(x)=len(y). If x c y and
≥ ε6, we will eliminate y from bank. For
example, for the two 5-grams x=from [dps] point
of view and y=[prp] [dps] point of view, obviously
x cy because from is a kind of [prp] (preposition).
If we set the filter factor ε=80% and
will be not included in the hybrid n-gram bank.
For example from 100M-word BNC, before prun-
ing, there are 110K hybrid n-grams containing
target lemma point. After pruning, there are only
5K useful hybrid n-grams left.
</bodyText>
<sectionHeader confidence="0.9994575" genericHeader="method">
5 The Edit Distance Algorithm for Error
Detection and Correction
</sectionHeader>
<subsectionHeader confidence="0.942949">
5.1 Error Detection
</subsectionHeader>
<bodyText confidence="0.999777">
We apply a simple edit distance for error detection
by comparing user input n-grams and standard
</bodyText>
<footnote confidence="0.376697">
5 [vvg] is the CLAWS5 tag for gerund.
</footnote>
<page confidence="0.989111">
6
</page>
<bodyText confidence="0.994175081081081">
x means the frequency of x in BNC.
hybrid n-gram in the bank. The approaches are
briefly summarized and short examples given in
the following:
Step 1: POS tag the user input string and get all
hybrid n-grams that can represent that string. For
example, a user inputs in my point of view and
then [prp] my point of view, [prp] [dps] point of
view, in [dps] point of view, in my point of
[nn1]... etc. will be generated. Let C denote the
entire set of hybrid n-grams generated from an
instance of user input.
Step 2: Search all hybrid n-grams in the target
knowledgebase containing point or view, which
are the content words in user input. Let S denote
all of the target hybrid n-grams containing point
or view.
Step 3: Compute the edit distance d between
every element in C and S. If 3 d=0 in (C, S), we
assume the user input n-gram is correct. If ∀ d&gt;1
in (C, S), our system will ignore this case and pro-
vide nothing. If 3 d=1, we assume the user input
might be wrong and the system will enter the error
correction procedure.
For efficiency’s sake in Step 2, the hybrid n-
grams are indexed by content words. We use
Levenshtein’s edit distance algorithm [Leven-
shtein 1996] in Step 3. It indicates the difference
between user input and standard n-grams in three
ways: “substitute relation,” i.e., two n-grams are
the same length and identical except for one slot.
“Delete relation” and “insert relation” hold be-
tween two different length n-grams. In this paper
we consider only the “substitute relation,” such as
in my point of view and from my point of view.
This limits edit distance computing to pairs of n-
grams of the same length (e.g. 5-gram to 5-gram).
</bodyText>
<subsectionHeader confidence="0.97865">
5.2 Error Correction
</subsectionHeader>
<bodyText confidence="0.999958833333333">
The system identifies correction candidates from S
as those with edit distance d=1 from some mem-
ber(s) in C. Once the system gets several correc-
tion candidates for an input string whose edit
distances from user input are 1, we have to decide
the ranking of the correct candidates by a value
called weighted edit distance. Weighted edit dis-
tance can identify more appropriate correct n-
grams for the user. Imagine a case where an n-
gram from C and an n-gram from S show a substi-
tution relation. Assume u is the differing element
in the C n-gram and v is its counterpart in the S n-
</bodyText>
<equation confidence="0.972036">
x y
x
&gt;ε, y
y
</equation>
<page confidence="0.995023">
53
</page>
<bodyText confidence="0.6478675">
gram. Weighted edit distance between these two is
computed by the following rules:
Rule 1: If u and v are both word-forms and are
different word-forms of the same lemma (for ex-
ample enjoyed and enjoying), given distance α.
Rule 2: If u and v are both members of
CLAWS5 POS and their rough POS are the same,
given distance 07.
</bodyText>
<figureCaption confidence="0.85165525">
Rule 3: If u and v are both function words, give
distance y.
Rule 4: If u and v are both content word, give
distance 8.
</figureCaption>
<bodyText confidence="0.999964555555556">
We set α&lt;0 and y&lt;8. Correct candidate with
lower weighted distance makes itself more appro-
priate for suggestion. For example, before weight-
ing, the error string pay attention on gets two
distance 1 correct candidates pay attention to and
focus attention on. Weighting will give pay atten-
tion to a lower weighted distance because on and
to are function words whereas focus and pay are
content words.
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="method">
6 Experimental Result
</sectionHeader>
<bodyText confidence="0.993145">
Four types of errors shown in Table 1 are exam-
ined for our detection and correction algorithm.
</bodyText>
<table confidence="0.917794363636364">
Error string Algorithm result Correction sug-
gested to user
Preposition
have a look *of have a look at have a look at
I am interested [pnp] be interested in I am interested in
*of
*in my point of from [dps] point of view from my point of
view view
pay attention pay attention to pay attention to
*on
pay attention to
We can discuss we [vm0] discuss it we can discuss it
*about. we can discuss
[noun]
we can discuss
{adv}
we [vm0] discuss
[noun]
we [vm0] discuss [av0]
Adjectival participles
He is [pnp] be confused [prp] He is confused
*confusing with with
I am [pnp] be interested in I am interested in
*interesting in
I am *exciting [pnp] be excited [prp] I am excited about
about
Verb form
He wants he wants [vvt] He wants to read
*reading.
he want [vvt]
I enjoy *to i enjoy [vvg] I enjoy reading
read.
i enjoy [vvg]
</table>
<tableCaption confidence="0.720233333333333">
7 Recall we use two levels of POS tagging in our hybrid n-
grams: 1. The detailed one is CLAWS5 with 46 tags. 2. The
rough or simple tag set of 9 tags.
</tableCaption>
<bodyText confidence="0.974541642857143">
let them *to let them [vvi] let them stay
stay.
let them [vvi]
make him *to make him [vvi] make him leave
leave
make him [vvi]
must let them [vm0] let them [vvi] must let them stay
*to stay
spend time to spend time [vvg] spend time under-
understand standing
will make him will make [pnp] [vvi] will make him
*to leave leave
Missing be
I* afraid of be afraid of be afraid of
</bodyText>
<reference confidence="0.944302285714286">
[adv]afraid of
[adj] afraid of
[adv] afraid of
[av0] afraid of
They* aware of be aware of be aware of
[adv]aware of
[av0] aware of
</reference>
<tableCaption confidence="0.977299">
Table 1: Four error types and their examples with cor-
rect suggestions.
</tableCaption>
<sectionHeader confidence="0.998931" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99998375">
We propose an algorithm for unsupervised lexical
error detection and correction and apply it to a
user tool called Lexbar. This is a work-in-progress
report, and we have not yet run full testing with a
large data set, such as a learner corpus. However
the early stage experimental results show promise,
especially its broad coverage over different error
types compared to error-specific approaches.
</bodyText>
<sectionHeader confidence="0.999076" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99702225">
The work described in this paper was partially
supported by the grants from the National Science
Council, Taiwan (Project Nos. 96-2524-S-008-
003- and 98-2511-S-008-002-MY2)
</bodyText>
<sectionHeader confidence="0.994696" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.998713388888889">
Martin Chodorow and Claudia Leacock 2000. An un-
supervised method for detecting grammatical errors.
Proceedings of the 1st conference of NAACL, pages
140–147.
Rachele De Felice and Stephen G. Pulman 2008.
Automatic detection of preposition errors in learner
writing. CALICO AALL Workshop.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende 2008. Us-
ing Contextual Speller Techniques and Language
Modeling for ESL Error Correction. Proceedings of
IJCNLP.
V. I. Levenshtein 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10:707–710.
Martí Quixal and Toni Badia 2008. Exploiting unsu-
pervised techniques to predict EFL learner errors.
CALICO AALL Workshop.
</reference>
<page confidence="0.999015">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885365">
<title confidence="0.99934">A Method for Unsupervised Lexical Error Detection and Correction</title>
<author confidence="0.998929">Nai-Lung Tsao David Wible</author>
<affiliation confidence="0.9996325">Graduate Institute of Learning and Instruction Graduate Institute of Learning and Instruction National Central University National Central University</affiliation>
<address confidence="0.922796">Jhongli City, Taoyuan County 32001, Taiwan Jhongli City, Taoyuan County 32001, Taiwan</address>
<email confidence="0.975437">beaktsao@gmail.comWible45@yahoo.com</email>
<abstract confidence="0.998782375">We describe and motivate an unsupervised lexical error detection and correction algorithm and its application in a tool called Lexbar appearing as a query box on the Web browser toolbar or as a search engine interface. Lexbar accepts as user input candidate strings of English to be checked for acceptability and, where errors are detected, offers corrections. We introduce the notion of hybrid n-gram and extract these from BNC as the knowledgebase against which to compare user input. An extended notion of edit distance is used to identify most likely candidates for correcting detected errors. Results are illustrated with four types of errors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>spend time under- standing will make him *to leave will make [pnp] [vvi] will make him leave Missing be</title>
<marker></marker>
<rawString>spend time under- standing will make him *to leave will make [pnp] [vvi] will make him leave Missing be</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors.</title>
<date>2000</date>
<booktitle>Proceedings of the 1st conference of NAACL,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="1859" citStr="Chodorow and Leacock, 2000" startWordPosition="288" endWordPosition="292">search engine or on the Web browser toolbar. The tool is intended as a proxy for search engines in the common practice where users put search engines to use as error checkers. A problem with this use of search engines like Google is that such searches commonly provide false positives, hits for strings that contain errors. Lexbar accepts as user input candidate strings of English to be checked for acceptability and, where errors are detected, offers corrections. 2 Related Work Among the many works on error detection, recently unsupervised error detection approaches have been proposed, such as [Chodorow and Leacock, 2000] and [Quixal and Badia 2008]. These use contextual features and statistical word association measurement to decide if the detected bigram or trigram is an error or not. To our knowledge, such unsupervised methods have not been applied in error correction. [Gamon et al 2008] and [Felice and Pulman 2008] propose unsupervised approaches to build a probabilistic model for detecting errors (prepositions and articles) and providing correct answers. They also typically focus on a particular type of error, usually limited to a specific word class such as preposition errors, often in a pre-determined </context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Martin Chodorow and Claudia Leacock 2000. An unsupervised method for detecting grammatical errors. Proceedings of the 1st conference of NAACL, pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
<author>Stephen G Pulman</author>
</authors>
<title>Automatic detection of preposition errors in learner writing.</title>
<date>2008</date>
<journal>CALICO AALL Workshop.</journal>
<marker>De Felice, Pulman, 2008</marker>
<rawString>Rachele De Felice and Stephen G. Pulman 2008. Automatic detection of preposition errors in learner writing. CALICO AALL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W B Dolan</author>
<author>D Belenko</author>
<author>L Vanderwende</author>
</authors>
<title>Using Contextual Speller Techniques and Language Modeling for ESL Error Correction.</title>
<date>2008</date>
<booktitle>Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="2133" citStr="Gamon et al 2008" startWordPosition="335" endWordPosition="338">, hits for strings that contain errors. Lexbar accepts as user input candidate strings of English to be checked for acceptability and, where errors are detected, offers corrections. 2 Related Work Among the many works on error detection, recently unsupervised error detection approaches have been proposed, such as [Chodorow and Leacock, 2000] and [Quixal and Badia 2008]. These use contextual features and statistical word association measurement to decide if the detected bigram or trigram is an error or not. To our knowledge, such unsupervised methods have not been applied in error correction. [Gamon et al 2008] and [Felice and Pulman 2008] propose unsupervised approaches to build a probabilistic model for detecting errors (prepositions and articles) and providing correct answers. They also typically focus on a particular type of error, usually limited to a specific word class such as preposition errors, often in a pre-determined paradigmatic slot. Our approach reported here is unsupervised in both detection and correction and is not tailored to a specific target error subtype or targeted to a specific position in a string. More generally the family of error types suitable for this approach are lexi</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. B. Dolan, D. Belenko, and L. Vanderwende 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error Correction. Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>V I</author>
</authors>
<title>Levenshtein 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady,</title>
<pages>10--707</pages>
<marker>I, </marker>
<rawString>V. I. Levenshtein 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10:707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martí Quixal</author>
<author>Toni Badia</author>
</authors>
<title>Exploiting unsupervised techniques to predict EFL learner errors.</title>
<date>2008</date>
<journal>CALICO AALL Workshop.</journal>
<contexts>
<context position="1887" citStr="Quixal and Badia 2008" startWordPosition="294" endWordPosition="297">er toolbar. The tool is intended as a proxy for search engines in the common practice where users put search engines to use as error checkers. A problem with this use of search engines like Google is that such searches commonly provide false positives, hits for strings that contain errors. Lexbar accepts as user input candidate strings of English to be checked for acceptability and, where errors are detected, offers corrections. 2 Related Work Among the many works on error detection, recently unsupervised error detection approaches have been proposed, such as [Chodorow and Leacock, 2000] and [Quixal and Badia 2008]. These use contextual features and statistical word association measurement to decide if the detected bigram or trigram is an error or not. To our knowledge, such unsupervised methods have not been applied in error correction. [Gamon et al 2008] and [Felice and Pulman 2008] propose unsupervised approaches to build a probabilistic model for detecting errors (prepositions and articles) and providing correct answers. They also typically focus on a particular type of error, usually limited to a specific word class such as preposition errors, often in a pre-determined paradigmatic slot. Our appro</context>
</contexts>
<marker>Quixal, Badia, 2008</marker>
<rawString>Martí Quixal and Toni Badia 2008. Exploiting unsupervised techniques to predict EFL learner errors. CALICO AALL Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>