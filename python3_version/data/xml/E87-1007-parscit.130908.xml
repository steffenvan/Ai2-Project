<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.874741">
HOW TO DETECT GRAMMATICAL ERRORS IN A TEXT WITHOUT PARSING IT
Eric Steven Atwell
Artificial Intelligence Group
Department of Computer Studies
Leeds University, Leeds LS2 9JT, U.K.
</title>
<email confidence="0.931673">
(EARN/BITNET: eric%leeds.ai@ac.uk)
</email>
<sectionHeader confidence="0.962799" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.991140095238095">
The Constituent Likelihood Automatic Word-tagging
System (CLAWS) was originally designed for the low-level
grammatical analysis of the million-word LOB Corpus of
English text samples. CLAWS does not attempt a full parse,
but uses a first-order Markov model of language to assign
word-class labels to words. CLAWS can be modified to
detect grammatical errors, essentially by flagging unlikely
word-class transitions in the input text. This may seem to be
an intuitively implausible and theoretically inadequate model
of natural language syntax, but nevertheless it can
successfully pinpoint most grammatical errors in a text
Several modifications to CLAWS have been explored. The
resulting system cannot detect all errors in typed documents;
but then neither do far more complex systems, which attempt
a full parse, requiring much greater computation.
Checking Grammar In Texts
A number of researchers have experimented with ways to
cope with granunatically ill-formed English input (for
example, [Carbonell and Hayes 83], [Chamiak 83], [Granger
83], [Hayes and Mouradian 81], [Heidorn et al 821, [Jensen
et al 83], [Kwasny and Sondheimer 81], [Weischedel and
Black 80], [Weischedel and Sondheimer 83]). However, the
majority of these systems are designed for Natural Language
interfaces to software systems, and so can assume a
restricted vocabulary and syntax; for example, the system
discussed by [Fass 83] had a vocabulary of less than 50
words. This may be justifiable for a NL front-end to a
computer system such as a Database Query system, since
even an artificial subset of English may be more acceptable
to users than a formal command or query language.
However, for automated text-checking in Word Processing,
we cannot reasonably ask the WP user to restrict their
English text in this way. This means that WP text-checking
systems must be extremely robust, capable of analysing a
very wide range of lexical and syntactic constructs.
Otherwise, the grammar checker is liable to flag many
constructs which are in fact acceptable to humans, but
happen not to be included in the system&apos;s limited grammar.
A system which not only performs syntactic analysis of text,
but also pinpoints grammatical errors, must be assessed
along two orthogonal scales rather than a single &apos;accuracy&apos;
measure:
</bodyText>
<figure confidence="0.4524265">
RECALL =
&amp;quot;number of words/constructs correctly flagged as errors&amp;quot;
divided by
&amp;quot;total number of &apos;true&apos; errors that should be flagged*
PRECISION =
&amp;quot;number of words/constructs correctly flagged as errors&amp;quot;
divided by
&amp;quot;total number of words/constructs flagged by the system&amp;quot;
</figure>
<bodyText confidence="0.999978727272727">
It is easy to optimise one of these performance measures
at the expense of the other. flagging (nearly) ALL words in a
text will guarantee optimal recall (i.e. (nearly) all actual
errors will be flagged) but at a low precision; and
conversely, reducing the number of words flagged to nearly
zero should raise the precision but lower the recall. The
problem is to balance this trade-off to arrive at recall AND
precision levels acceptable to WP users. A system which
can accept a limited subset of English (and reject (or flag as
erroneous) anything else) may have a reasonable recall rate;
that is, most of the &apos;true&apos; errors will probably be included in
the rejected text. However, the precision rate is liable to be
unacceptable to the WP user: large amounts of the input text
will effectively be marked as potentially erroneous, with no
indication of where&apos; within this text the actual errors lie. One
way to deal with this problem is to increase the size and
power of the parser and underlying grammar to deal with
something nearer the whole gamut of English syntax; this is
the approach taken by IBM&apos;s EPISTLE project (see [Heidorn
et al 821, [Jensen et al 83]). Unfortunately, this can lead to a
very large and computationally expensive system: [Heidorn
et al 82] reported that the EPISTLE system required a 4Mb
virtual machine (although a more efficient implementation
under development should require less memory).
The UNIX Writer&apos;s Workbench collection of programs
(see [Cherry and Macdonald 83], [Cherry et al 83]) is
probably the most widely-used system for WP text-checking
(and also one of the most widely-used NLP systems overall -
see [Atwell 86], [Hubert 85]). This system includes a
number of separate programs to check for different types of
faults, including misspellings, cliches, and certain stylistic
infelicities such as overly long (or short) sentences.
However, it lacks a general-purpose grammar checker, the
nearest program is a tool to filter out doubled words (as in &amp;quot;I
signed the the contract&amp;quot;). Although there is a program
PARTS which assigns a part of speech tag to each word in
the text (as a precursor to the stylistic analysis programs),
this program uses a set of localized heuristic rules to
disambiguate words according to context; and these rules are
based on the underlying assumption that the input sentences
are grammatically well-formed. So, there is no clear way to
modify PARTS to flag grammatical errors, unless we
introduce a radically different mechanism for disambiguating
word-tags according to context.
</bodyText>
<page confidence="0.998354">
38
</page>
<sectionHeader confidence="0.586447" genericHeader="categories and subject descriptors">
LOB and CLAWS
</sectionHeader>
<bodyText confidence="0.999949691176471">
One such alternative word-tag disambiguation mechanism
was developed for the analysis of the Lancaster-Oslo/Bergen
(LOB) Corpus. The LOB Corpus is a million-word
collection of English text samples, used for experimentation
and inspiration in computational linguistics and related
studies (see for example [Leech et al 83a], [Atwell
forthcoming b]). CLAWS, the Constituent-Likelihood
Automatic Word-tagging System ([Leech et al 83b1, [Atwell
et al 84]), was developed to annotate the raw text with basic
grammatical information, to make it more useful for
linguistic research; CLAWS did not attempt a full parse of
each sentence, but simply marked each word with a
grammatical code from a set of 133 WORDTAGS. The
word-tagged LOB Corpus is now available to other
researchers (see [Johansson et al 86]).
CLAWS was originally implemented in Pascal, but it is
currently being recoded in C and in POPLOG Prolog.
CLAWS can deal with Unrestricted English text input
including &amp;quot;noisy&amp;quot; or ill-formed sentences, because it is based
on Constituent Likelihood Grammar, a novel probabilistic
approach to grammatical description and analysis described
in [Atwell 83]. A Constituent Likelihood Grammar is used
to calculate likelihoods for competing putative analysis; not
only does this tell us which is the &apos;best&apos; analysis, but it also
shows how &apos;good&apos; this analysis is. For assigning word-tags
to words, a simple Markovian model can be used instead of
a probabilistic rewrite-rule system (such as a probabilistic
context-free grammar); this greatly simplifies processing.
CLAWS first uses a dictionary, suffixlist and other default
routines to assign a set of putative tags to each word; then,
for each sequence of ambiguously-tagged words, the
likelihood of every possible combination or &apos;chain&apos; of tags is
evaluated, and the best chain is chosen. The likelihood of
each chain of tags is evaluated as a product of all the &apos;links&apos;
(tag-pair-likelihoods) in the sequence; tag-pair likelihood is a
function of the frequency of that sequence of two tags in a
sample of tagged text, compared to the frequency of each of
the two tags individually.
An important advantage of this simple Markovian model
is that word-tagging is done without parsing: there is no
need to work out higher-level constituent-structure trees
before assigning unambiguous word-tags to words. Despite
its simplicity, this technique is surprisingly robust and
successful: CLAWS has been used to analyse a wide variety
of Unrestricted English, including extracts form newspapers,
novels, diaries, learned journals, E.E.C. regulations, etc., with
a consistent accuracy of c96%. Although the system did not
have parse trees available in deciding word-classes, only
c4% of words in the LOB Corpus had to have their assigned
wordtag corrected by manual editing (see [Atwell 81, 82]).
Another important advantage of the simple Markovian
model is that it is relatively straightforward to transfer the
model from English to other Natural Languages. The basic
statistical model remains, only the dictionary and Markovian
tag-pair frequency table need to be replaced. We are
experimenting with the possibility of (partially) automating
even this process - see [Atwell 86a, 86b, forthcoming c],
[Atwell and Drakos 87].
The general Constituent Likelihood approach to
grammatical analysis, and CLAWS in particular, can be used
to analyse text including ill-formed syntax. More
importantly, it can also be adapted to flag syntactic errors in
texts; unlike other techniques for error-detection, these
modifications of CLAWS lead to only limited increases in
processing requirements. In fact, various different types of
modification are possible, yielding varying degrees of
success in error-detection. Several different techniques have
been explored.
</bodyText>
<subsectionHeader confidence="0.680493">
Error Likelihoods
</subsectionHeader>
<bodyText confidence="0.924929461538461">
A very simple adaptation of CLAWS (simple in theory at
least) is to augment the tag-pair frequency table with a tag-
pair error likelihood table. As in the original system,
CLAWS uses the tag-pair frequency table and the
Constituent Likelihood formulae to find the best word-tag for
each word. Having found the best tag for each word, every
cooccurring pair of tags in the analysis is re-assessed: the
ERROR-LIKELIHOOD of each tag-pair is checked. Error-
likelihood is a measure of how frequently a given tag-pair
occurs in an error as compared to how frequently it occurs in
valid text. For example, if the user types
... my farther was ...
CLAWS will yield the word-tag analysis
</bodyText>
<subsectionHeader confidence="0.440724">
PP$ RBR BEDZ
</subsectionHeader>
<bodyText confidence="0.997754705882353">
which means &lt;possessive personal pronoun&gt;,
&lt;comparative adverb&gt;, &lt;past singular BE&gt;. This analysis is
then passed to the checking module, which uses tag-pair
frequency statistics extracted from copious samples of error-
full texts. These should show that tag-pairs &lt;PPS RBR&gt; and
&lt;RBR BEDZ&gt; often occur where there is a typing error, and
rarely occur in grammatically correct constructs; so an error
can be flagged at the corresponding point in the text.
Although the adjustment to the model is theoretically
simple, the tag-pair error likelihood frequency figures
required could only be gleaned by human analysis of huge
amounts of error-full text. Our initial efforts to collect an
Error Corpus convinced us that this approach was
impractical because of the time and effort required to collect
the necessary data. In any case, an alternative technique
which managed without a separate table of tag-pair error
likelihoods turns out to be quite successful.
Low Absolute Likelihoods
This alternative technique involved using CLAWS
unmodified to choose the best tag for each word, as before,
and then measuring ABSOLUTE LIKELIHOODS of tag-
pairs. Instead of a separate tag-pair error likelihood table to
assess the grammaticality, the same tag-pair frequency table
is used for tag-assignment and error-detection. The tag-pair
frequency table gives frequencies for grammatically well-
formed text, so the second module simply assumes that if a
low-likelihood tag pair occurs in the input text, it indicates a
grammatical error. In the example above, tag-pairs &lt;PPS
RBR&gt; and &lt;RBR BEDZ&gt; have low likelihoods (as they
occur only rarely in grammatically well-formed text), so an
error can be diagnosed.
Figure 1 is a fuller example of this approach to error
diagnosis. This shows the analysis of a short text; please
note that the text was constructed for illustration purposes
</bodyText>
<page confidence="0.996578">
39
</page>
<bodyText confidence="0.999658872340426">
only, and the characters mentioned bear no resemblance to
real living people! The text contains many mis-typed words,
but these mistakes would not be detected by a conventional
spelling-checker, since the error-forms happen to coincide
with other legal English words; the only way that these
errors can be detected is by noticing that the resultant
phrases and clauses are ungrammatical. The grammar-
checking program first divides the input text into words.
Note that this is not entirely trivial: for example, enclitics
such as I&apos;ll, won&apos;t are split into two words 1 + will
+ a&apos;r. The left-hand column in Figure 1 shows the sequence
of words in the sample text, one word per line. The second
column shows the grammatical tag chosen using the
Constituent Likelihood model as best in the given context.
The third column shows the absolute likelihood of the
chosen grammatical tag; this likelihood is normalised relative
to a threshold, so that values greater than one constitute
&amp;quot;acceptable&amp;quot; grammatical analyses, whereas values less than
one are indicative of unacceptably improbable grammar.
Whenever the absolute likelihood value falls below this
acceptability threshold, the flag ERROR? is output in the
fourth column, to draw visual attention to the putative error.
Thus, for example, the first word in the text, my, is tagged
PP$ (possessive personal pronoun), and this tag has a
normalised absolute likelihood of over 15, which is
acceptable; the second word, farther, is tagged RBR
(comparative adverb), but this time the absolute likelihood is
below one (0.264271), so the word is flagged as a putative
ERROR?
This technique is extremely primitive, yet appears to
work fairly well. There is no longer any need to gather
error-likelihoods from an Error Corpus. However, the
definition of what constitutes a &amp;quot;low&amp;quot; likelihood is not
straightforward. On the whole, there is a reasonably clear
correlation between words marked ERROR? and actual
mistakes, so clearly low values can be taken as diagnostic of
errors, once the question of what constitutes &amp;quot;lowness&amp;quot; has
been defined rigorously. In the example, the acceptability
level is defined in terms of a simple threshold: likelihoods
are normalised so values below 1.000003 are deemed too
low to be acceptable. The appropriate normalisation scaling
factor was found empirically. Unfortunately, a threshold at
this level would mean some minor troughs would not be
flagged, e.g. clever in / stole a meat clever,... (which was
tagged ii (adjective) but should have been the noun cleaver)
has a normalised likelihood of 4.516465; tame in the
gruesome tame of Eroc Auwell... (which was also tagged JJ
(adjective) but should have been the noun tale ) also has a
normalised likelihood of 4.516465; and the phrase won day
(which should have been one day ) involves a normalised
likelihood of 4.060886 (although this is, strictly speaking,
associated with day rather than won, an error flag would be
sufficiently close to the actual error to draw the user&apos;s
attention to it). However, if we raised the threshold (or
alternatively changed the normalisation function so that these
normalised likelihoods are below 1.000000), then more
words would be flagged, lowering the precision of error
diagnosis. In some cases, error diagnosis would be
&amp;quot;blurred&amp;quot;, since sometimes words immediately before and/or
after the error also have low likelihoods; for example, was in
my farther was very crawl.., has a likelihood of 1.216545.
Worse, some error flags would appear in completely
inappropriate places, with no true errors in the immediate
context; for example, the exclamation mark at the end of he
won&apos;t get away with this! has a likelihood of 4.185351 and
so would probably be flagged as an error if the threshold
were raised.
Another way to define a trough would be as a local
minimum, that is, a point on where points immediately
before and after have higher likelihood values; even a trough
with a quite high value is flagged this way so long as
surrounding points are even higher. This would catch clever,
tame and won day mentioned above. However, strictly
speaking several other words not currently flagged in Figure
1 are also local minima, for example my in perhaps my
friends would ... and 4&amp;quot; in he bald at me if I ... So, this
definition is liable to cause a greater number of &apos;red herring&apos;
valid words to be erroneously flagged as putative mistakes,
again leading to a worse precision.
Once an optimal threshold or other computational
definition of low likelihood has been chosen, it is a simple
matter to amend the output routine to produce output in a
simplified format acceptable to Word Processor users,
without grammatical tags or likelihood ratings but with
putative errors flagged. However, even with an optimal
measure of lowness, the success rate is unlikely to be
perfect. The model deliberately incorporates only
rudimentary knowledge about English: a lexicon of words
and their wordtags, and a tag-pair frequency matrix
embodying knowledge of tag cooccurrence likelihoods.
Certain types of error are unlikely to be detected without
some further knowledge. One limited augmentation to this
simple model involves the addition of error tags to the
analysis procedure.
</bodyText>
<subsectionHeader confidence="0.47676">
Error-Tags
</subsectionHeader>
<bodyText confidence="0.99998855">
A rather more sophisticated technique for taking syntactic
context into account involves adding ERROR-TAGS to
lexical entries. These are the tags of any similar words
(where these are different from the word&apos;s own tags). In the
analysis phase, the system must then choose the best tag
(from error-tag(s) and &apos;own&apos; tag(s)) according to syntactic
context, still using the unmodified CLAWS Constituent-
Likelihood model. For example, in the sentence 1 am very
hit. an error can be diagnosed if the system works out that
the tags of input word hit ( NN, VB, VBD, and VBN -
&lt;singular common noun&gt;, &lt;verb infinitive&gt;, &lt;verb past
tense&gt;, &lt;verb past participle&gt;) are all much less likely in the
given context than LI (&lt;adjective&gt;), known to be the tag of a
similar word ( hot ). So, a rather more sophisticated error-
detection system includes knowledge not just about tags of
words, but also about what alternative word-classes would be
plausible if the input was an error. This information consists
in an additional field in lexicon entries: each dictionary entry
must hold (i) the word itself, (ii) the word&apos;s own tags, and
(iii) the error-tags associated with the word. For example:
</bodyText>
<sectionHeader confidence="0.793903" genericHeader="general terms">
WORD TAG(S) ERROR-TAG(S)
</sectionHeader>
<bodyText confidence="0.993293529411765">
form NN 1N# RI#
hit NN VB VBD VBN
prophecy NN VB#
Note that error-tags are marked with # to distinguish
them from own tags. CLAWS then chooses the best tag for
each word as usual. However, in the final output, instead of
each word being marked with the chosen word-tag, words
associated with an ERROR TAG are flagged as potential
errors.
To illustrate why error-tags might help in error diagnosis,
notice that dense in I maid several dense in his ... does not
have a below-threshold absolute likelihood, and so is not
flagged as a putative error. An error-tag based system could
calculate that the best sequence of tags (allowing error-tags)
for the word sequence several dense in his ... is [AP NNS#
IN PPM (&lt;post-determiner&gt;, &lt;plural common noun&gt;,
&lt;preposition&gt;, &lt;possessive personal pronoun&gt;). Since NNS
is an error-tag, an error is flagged. However, the simpler
absolute likelihood based model does not allow for the
option of choosing NNS as the tag for dense, and is forced
to choose the best of the &apos;own&apos; tags; this in turn causes a
mistagging of in as NNU (&lt;abbreviated unit of
measurement&gt;, since [JJ NNU] (&lt;adjective&gt; &lt;abbreviated
unit of measurement&gt;) is likelier than [11 IN] (&lt;adjective&gt;
&lt;preposition&gt;). Furthermore, [ll NNU] turns out not to be
an exceptionally unusual tag cooccurrence. The point of all
this is that, without error-tags, the the system may mistag
words immediately before or after error-words, and this
mistagging may well distort the absolute likelihoods used for
error diagnosis.
This error-tag-based technique was originally proposed
and illustrated in [Atwell 83]. The method has been tested
with a small test lexicon, but we have yet to build a
complete dictionary with error-tags for all words. Adding
error tags to a large lexicon is a non-trivial research task;
and adding error-tags to the analysis stage increases
computation, since there are more tags to choose between for
each word. So far, we have not found conclusive evidence
that the success rate is increased significantly; this requires
further investigation. Also to be more fully investigated is
how to take account of other relevant factors in error
diagnosis, in addition to error-tags.
Full Cohorts
In theory at least, the Constituent-Likelihood method
could be generalised to take account of all relevant
contextual factors, not just syntactic bonding. This could be
done by generating COHORTS for each input word, and
then choosing the cohort-member word which fits the context
best. For example, if the sentence you were very hit were
input, the following cohorts would be generated:
you yew ewe
were where wear
very vary veery
hit hot hut hat
(the term &amp;quot;cohort&amp;quot; is adapted from [Marslen-Wilson 85]
with a slight modification of meaning). Cohorts of similar
words can be discovered from the spelling-check dictionary
using the same algorithm employed to suggest corrections
for misspellings in current systems; these techniques are
fairly well-understood (see, for example, [Yannakoudakis
and Fawthrop], [Veronis 871, (Borland 851). Next, each
member of a cohort is assigned a relative likelihood rating,
taking into account relevant factors including:
i) the degree of similarity to the word actually typed (this
measure would be available anyway, as it has to be
calculated during cohort generation; the actual word typed
gets a similarity factor of 1, and other members of the cohort
get appropriate lower weights)
ii) the &apos;degree of fit&apos; in the given syntactic context
(measured as the syntactic constituent likelihood bond
between the tag(s) of each cohort member and the tag(s) of
the words before and after, using the CLAWS constituent
likelihood formulae);
iii) the frequency of usage in general English (common
words like &amp;quot;you&amp;quot; and &amp;quot;very&amp;quot; get a high weighting factor, rare
words like &amp;quot;ewe&amp;quot;, &amp;quot;yew&amp;quot;, and &amp;quot;veery&amp;quot; get a much lower
weighting; word relative frequency figures can be gleaned
from statistical studies of large Corpora, such as [Hofland
and Johansson 82], [Francis and Kucera 82], [Carroll et al
71]);
iv) if a cohort member occurs in a grammatical idiom or
preferred collocation with surrounding words, then its
relative weighting is increased (e.g. in the context &amp;quot;fish and
...&amp;quot;, chips gets a higher collocation weighting than chops);
collocation preferences can also be elicited from studies of
large corpora using techniques such as those of [Sinclair et
al 70];
v) domain-dependent lexical preferences should ideally be
taken into account, for example in an electronics manual
current should get a higher domain weighting than currant.
All these factors are multiplied (using appropriate
weightings) to yield a relative likelihood rating for each
member of the cohort. The cohort-member with the highest
rating is (probably) the intended word; if the word actually
typied is different, an error can be diagnosed, and
furthermore a correction can be offered to the user.
Unfortunately, although this approach may seem sensible
in theory, in practice it would require a huge R&amp;D effort to
gather the statistical information needed to drive such a
system, and the resulting model would be computationally
complex and expensive. It would be more sensible to try to
incorporate only those features which contribute significantly
to increased error-detection, and ignore all other factors.
This means we must test the existing error-detection system
extensively, and analyse the failures to try to discover what
additional knowledge would be useful to the system.
Error Corpus
The error-likelihood and full-cohort techniques would
appear to give the best error-detection rates, but require vast
computations to build a general-purpose system from scratch.
The error-tag technique also requires a substantial research
effort to build a large general-purpose lexicon. A version of
the Constituent Likelihood Automatic Word-tagging System
modified to use the ABSOLUTE LIKELIHOOD method of
error-detection has been more extensively tested; this system
cannot detect all grammatical errors, but appears to be quite
successful with certain classes of errors. To test alternative
prototypes, we are building up an ERROR CORPUS of texts
containing errors. The LOB Corpus includes many errors
</bodyText>
<page confidence="0.998727">
41
</page>
<bodyText confidence="0.975970693693694">
which appeared in the original published texts; these are
marked SIC in the text, and noted in the Manual which
comes with the Corpus files, [Johansson et al 781. The
initial Error Corpus consisted in these errors, and it is being
added to from other sources (see Acknowledgements below).
The errors in the Error Corpus can be (manually) classified
according to the kind of processing required for detection
(the examples below starts with a LOB line reference
number):
A: non-word error-forms, where the error can be found
by simple dictionary-lookup; for example,
A21 115 As the news pours in from around the world,
beleagured (SIC) Berlin this weekend is a city on a razor&apos;s
edge.
B: error-forms involving valid English words in an
invalid grammatical context, the kind of error the CLAWS-
based approach could be expected to detect (these may be
due to spelling or typing or grammatical mistakes by the
typist, but this is irrelevant here: the classification is
according to the type of processing required by the detection
program); for example
E18 121 Unlike an oil refinery one cannot grumble much
about the fumes, smell and industrial dirt, generally, for little
comes out of the chimney except possibly invisible gasses.
(SIC)
C: error-forms which are valid English words, but in an
abnormal grammatical/semantic context, which a CLAWS-
type system would not detect, but which could conceivably
be caught by a very sophisticated parser; for example,
breaking &apos;long-distance&apos; number agreement rules as in
Al5 170 It is, however, reported that the tariff on textiles
and cars imported from the Common Market are (SIC) to be
reduced by 10 per cent.
D: lexically and syntactically valid error-forms which
would require &amp;quot;intelligent&apos; semantic analysis for detection;
for example,
PI7 189 She did not imagine that he would pay her a visit
except in Frank&apos;s interest, and when she hurried into the
room where her mother was Dying in vain to learn the
reason of his visit, her first words were of her fiancee. (SIC)
or
K29 35 He had then sown (SIC) her up with a needle, and,
after a time she had come back to him cured and able to
bear more children.
Collection and detailed analysis of texts for this Error
Corpus is still in progress at the time of writing; but one
important early impression is that different sources show
widely different distributions of error-classes. For example,
a sample of 150 errors from three different sources shows
the following distribution:
i) Published (and hence manually proofread) text:
A: 52% B: 28% C: 8% D: 12%
ii) essays by 11- and 12-year-old children:
A: 36% B: 38% C: 16% D: 10%
non-native English speakers:
A: 4% B: 48% C: 12% D: 36%
Because of this great variation, precision and recall rates
are also liable to vary greatly according to text source. In a
production version of the system, the &apos;unusualness&apos; threshold
(or other measure) used to decide when to flag putative
errors will be chosen by the user, so that users can optimise
precision or recall. It is not clear how this kind of user-
customisation could be built into other WP text-checking
systems; but it is an obvious side-benefit of a Constituent
Likelihood based system.
Conclusions
The figures above indicate that a CLAWS-based
granunar-checker would be particularly useful to non-native
English speakers; but even for this class of users, precision
and recall are imperfect. The CLAWS-based system is
inadequate on its own, but should properly be used as one
tool amongst many; for example as an augmentation to the
Writer&apos;s Workbench collection of text-critiquing and
proofreading programs, or in conjunction with other English
Language Teaching tools such as a computerised ELT
dictionary (such as those discussed by [Akkennan et al 85]
or [Atwell forthcoming a]. Other systems for dealing with
syntactically ill-formed English attempt a full grammatical
parse of each input sentence, and in addition require error-
recovery routines of varying degrees of sophistication. This
involves much more processing than the CLAWS-based
system; and yet even these systems fail to diagnose all errors
in a text. Clearly, the Constituent-Likelihood error-detection
technique is ideally suited to applications where fast
processing and relatively small computing requirements are
of paramount importance, and for users who find imperfect
error-detection better than none at all. I freely admit that the
system has not yet been comprehensively tested on a wide
variety of WP users; as with all Al research systems, a lot af
work still has to be done to engineer a generally-acceptable
commercial product. We are currently looking for sponsors
and collaborators for this research: anyone interested in
developing the prototype into a robust system (for example,
to be integrated into a WP system) is invited to contact the
author!
ACKNOWLEDGEMENTS
This paper was originally produced in 1986 as
Department of Computer Studies Research Report no212,
Leeds University. I gratefully acknowledge the help of
supervisors, colleagues and friends at the Universities of
Lancaster and Â°Leeds. The original CLAWS system was
developed by Ian Marshall, Roger Garside, Geoffrey Leech
and myself at Lancaster University, for a project funded by
the Social Science Research Council. Stephen Elliott spent a
lot of time building up the Error Corpus and testing variants
of the error-detection system, funded by an ICL Research
Associateship. Pauline McCrorie and Matthias Wong
worked on the POPLOG prolog and C versions of CLAWS.
Various other colleagues have also offered advice and
encouragement, particularly Geoffrey Sampson, Stuart
Roberts, Chris Paice, Lita Taylor, Andrew Beale, Susan
</bodyText>
<page confidence="0.993558">
42
</page>
<note confidence="0.623735">
Blackwell, and Barbara Booth. Linguistics; Proceedings of the ICAME Conference on the
</note>
<bodyText confidence="0.734959">
use of computer corpora in English Language Research,
Nijmegen, Netherlands Rodopi.
</bodyText>
<sectionHeader confidence="0.831014" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.847538617021277">
Akkerman, Erik, Pieter Masereeuw, and Willem Meijs 1985
Designing a computerized lexicon for linguistic purposes
Rodopi, Amsterdam
Atwell, Eric Steven 1981 LOB Corpus Tagging Project:
Manual Pre-edit Handbook. Departments of Computer
Studies and Linguistics, University of Lancaster
Atwell, Eric Steven 1982 LOB Corpus Tagging Project:
Manual Postedit Handbook (A mini-grammar of LOB
Corpus English, examining the types of error commonly
made during automatic (computational) analysis of ordinary
written English.) Departments of Computer Studies and
Linguistics, University of Lancaster
Atwell, Eric Steven 1983 &amp;quot;Constituent-Likelihood Grammar&amp;quot;
in Newsletter of the International Computer Archive of
Modern English (ICAME NEWS) 7: 34-67, Norwegian
Computing Centre for the Humanities, Bergen University
Atwell, Eric Steven 1986a Extracting a Natural Language
grammar from raw text Department of Computer Studies
Research Report no.208, University of Leeds
Atwell, Eric Steven 1986b, &amp;quot;A parsing expert system which
learns from corpus analysis&amp;quot; in Willem Meijs (ed) Corpus
Linguistics and Beyond: Proceedings of the Seventh
International Conference on English Language Research on
Computerised Corpora, Amsterdam, Netherlands Rodopi,
Amsterdam
Atwell, Eric Steven 1986c &amp;quot;Beyond the micro: advanced
software for research and teaching from computer science
and artificial intelligence&amp;quot; in Leech, Geoffrey and Candlin,
Christopher (eds.) Computers in English language teaching
and research: selected papers from the British Council
Symposium on computers in English language education and
research, Lancaster, England 167-183, Longman
Atwell, Eric Steven (forthcoming a) &amp;quot;A lexical database for
English learners and users: the Oxford Advanced Learner&apos;s
Dictionary&amp;quot; to appear in Proceedings of ICDBHSS87, the
1987 International Conference on DataBases in the
Humanities and Social Sciences, Montgomery, Alabama,
USA
Atwell, Eric Steven (forthcoming b) &amp;quot;Transforming a Parsed
Corpus into a Corpus Parsee&apos;, to appear in Proceedings of
the 1987 ICAME 8th International Conference on English
Language Research on Computerised Corpora, Helsinki,
Finland
Atwell, Eric Steven (forthcoming c) &amp;quot;An Expert System for
the Automatic Discovery of Particles&amp;quot; to appear in
Proceedings of the 1987 International Conference on the
Study of Particles, Berlin, East Germany
</bodyText>
<reference confidence="0.831518725274725">
Atwell, Eric Steven, Geoffrey Leech and Roger Garside
1984, &amp;quot;Analysis of the LOB Corpus: progress and
prospects&amp;quot;, in Jan Aarts and Willem Meijs (ed), Corpus
E Atwell and N Dmkos, &amp;quot;Pattern Recognition Applied to the
Acquisition of a Grammatical Classification System from
Unrestricted English Text&amp;quot; to appear in the Proceedings of
the Association for Computational Linguistics Third
European Chapter Conference, 1987 (forthcoming).
Borland International Inc. 1985 Turbo Lightning: Owner&apos;s
Handbook Borland International, Scotts Valley, California
USA
Carbonell, Jaime and Philip Hayes 1983 &amp;quot;Recovery strategies
for parsing extragranunatical language&amp;quot; in American Journal
of Computational Linguistics 9(3-4): 123-146
Carroll, John, Peter Davies, and Barry Richman 1971 The
American Heritage word frequency book Houghton Mifflin /
American Heritage
Chamiak, Eugene 1983 &amp;quot;A parser with something for
everyone&amp;quot; in Margaret King (ed) Parsing Natural Language
Academic Press, London
Cherry, L, Fox, M, Frase, L, Gingrich, P, Keenan, S. and
Macdonald, N 1983 &amp;quot;Computer aids for text analysis&amp;quot; in Bell
Laboratories Records, May/June: 10-16
Cherry, Lorinda and Macdonald, Nina 1983 &amp;quot;The Writer&apos;s
WorkBench software&amp;quot; in BYTE October: 241-248
Fass, Dan, and Yorick Wilks 1983 &amp;quot;Preference semantics,
ill-formedness, and metaphor&amp;quot; in American Journal of
Computational Linguistics 9(3-4): 178-187
Francis, W Nelson, and Henry Kucera 1982 Frequency
analysis of English usage: lexicon and grammar Houghton
Mifflin
Granger, Richard 1983 &amp;quot;The NOMAD system: expectation-
based detection and correction of errors during understanding
of syntactically and semantically ill-formed text&amp;quot; in American
Journal of Computational Linguistics 9(3-4): 188-196
Hayes, Philip I, and G V Mouradian 1981 &amp;quot;Flexible Parsing&amp;quot;
in American Journal of Computational Linguistics 7(4):
232-242
Heidom, G E, Jensen, K, Miller, L A, Byrd, R J, and
Chodorow, M S, 1982 &amp;quot;The EPISTLE text-critiquing
system&amp;quot; in IBM Systems Journal 21(3): 305-326
Hofland, Knut and Stig Johansson 1982 Word frequencies in
British and American English Longman
Hubert, Henry 1985 Computers and Composition: an
annotated bibliography, English Education 534 Resource
Report, University of British Columbia
Jensen, K, Heidom, G E, Miller, L A, and Ravin, Y 1983
&amp;quot;Parse fitting and prose fixing: getting a hold on ill-
formedness&amp;quot; in American Journal of Computational
Linguistics 9(3-4): 147-160
Johansson, Stig, Geoffrey Leech and Helen Goodluck 1978
Manual of information to accompany the Lancaster-
Oslo/Bergen Corpus of British English, for use with digital
computers Department of English, Oslo University
Johansson, Stig, Eric Atwell, Roger Garside, and Geoffrey
Leech 1986 The Tagged LOB Corpus Norwegian Computing
Centre for the Humanities, University of Bergen, Norway.
Kwasny, S and Norman Sondheimer 1981 &amp;quot;Relaxation
techniques for parsing grammatically ill-formed input in
natural language understanding systems&amp;quot; in American
Journal of Computational Linguistics 7(2): 99-108
Leech, Geoffrey, Roger Garside, and Eric Steven Atwell
1983a, &amp;quot;Recent developments in the use of computer corpora
in English language research&amp;quot; in Transactions of the
Philological Society 1983: 23-40.
Leech, Geoffrey, Roger Garside, and Eric Steven Atwell
1983b, &amp;quot;The Automatic Grammatical Tagging of the LOB
Corpus&amp;quot; in Newsletter of the International Computer Archive
of Modern English (ICAME NEWS) 7: 13-33, Norwegian
Computing Centre for the Humanities, Bergen University
Marslen-Wilson, W D 1985 &amp;quot;Aspects of human speech
understanding&amp;quot; in Fallside, Frank and Woods, William (eds.)
Computer speech processing, Prentice-Hall
Sinclair, John, Jones, S, and Daley, R 1970 English lexical
studies, Report to OSTI on project OLP/08; Dept of English,
Birmingham University
Veronis, Jean 1987 &amp;quot;Correction of phonographic errors in
natural language processing&amp;quot; in Oakman, Robert and
Pantonial, Barbara (eds.) ICCH87: Proceedings of the Eighth
International Conference on Computers and the Humanities,
Department of Computer Science, University of South
Carolina
Weischedel, Ralph, and John Black 1980 &amp;quot;Responding
intelligently to unparsable inputs&amp;quot; in American Journal of
Computational Linguistics 6(2) 97-109
Weischedel, Ralph, and Norman Sondheimer 1983 &amp;quot;Meta-
rules as a basis for processing ill-formed input&amp;quot; in American
Journal of Computational Linguistics 9(3-4):161-177
Yannakoudakis, E J, and Fawthrop, D 1983 &amp;quot;The rules of
spelling errors&amp;quot; in Information processing and management
19(2): 87-99
</reference>
<page confidence="0.997775">
44
</page>
<table confidence="0.991153776699028">
Figure I. Sample output with low Likelihoods flagged. with IN 35.186770
this 127 2L792427
my PPS 15.297639 4.185351
farther RBR 0.264271 ERROR? PPM 90.897396
was BEDZ 1.216545 stole VBD 135.815263
very QL 22.137197 a AT 39.564677
crawl NN 0.259613 ERROR? meat NN 191.684559
103.174992 clever 12 4.516465
he PPM 90.897396 24.477376
bald ii 0.271961 ERROR? and CC 82.096986
at IN 17.237397 PPM. 25.834909
me 9910 29.279452 maid NN 0.059657 ERROR?
if CS 11.400905 several AP 2.055110
PP1A 71.313009
dense ii 8.725460
dud 13 0.2/1961 ERROR?
in NNU 31945608
anything PN 0.088535 ERROR? his PPS 0.306138 ERROR?
wrong .21 1.682160
hid VBD 0.099010 ERROR?
24.477376
with IN 34.451138
and CC 82096986
It 993 9.309486
sometimes RB 29.179920
11.826017
he P91A 9.921162
It 993 62.337141
would MD 64.525545
must MD 46.875000
hot .21 0.230232 ERROR?
have HV 41513082
and CC 24.663050
hurt VB 0.527257 ERROR?
bit NN 20.025340
a AT 45.661755
me 9910 0.062710 ERROR?
lit VBD 0.037789 ERROR?
18.500350
22.778418
until CS 29.873133
SOO NN 9.189478
PPM 71313009
the All 4.149936
was BEDZ 95.448591
gruesome NN 160.254821
90 QL 22.137197
tame 31 4316445
week NN 0.289613 ERROR?
of IN 17.237397
and CC 42.917870
Erne NN 54.835271
miserable NN 20.028340
Attwell NN 26.254356
that CS 15.439211
PP1A 71.313009 appeared VBN 8387370
In NNU 4.870130
wanted VBD 135315263
all ABN 0.265393 ERROR?
to TO 25.445266
the ATI 3.499841
due JI 0.216826 ERROR?
Palmy NNS 40.467490
21.911547 70.542572
finally RB 36.564715
perhaps RB 36364715
45.403013
my PPS 5.473606
won VBD 25.409130
friends PINS 44.477694
day NN 4.060886
would MD 15.005662
84.114626
learnt VBN 0.237220 ERROR?
PPM 36.536284
to TO 34.470793
decided VBD 135.815263
spell NN 0.061250 ERROR?
to TO 28445266
my PPS 0.545207 ERROR?
got VBD 0.102690 ERROR?
name NN 51.946085
my PPS 30.396041
correctly 21 4.516465
won VBD 0.099010 ERROR?
at IN 17.237397
back RP 21.849187
last AP 10.850327
on IN 10.259310
3.437432
him P910 29.279452
3.242075
PP1A 4.764065
MD 64.525545 Figure 1. Sample output with low Likelihoods flagged.
mike NN 0.123308 ERROR?
him 9930 0.062710 ERROR?
Pay VB 10.708766
1.396258
he PP3A 4.764065
will MD 64.525545
n&apos;t XNOT 95.159151
get VS 0.145558 ERROR?
away RB 29.196041
</table>
<page confidence="0.995726">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999489">HOW TO DETECT GRAMMATICAL ERRORS IN A TEXT WITHOUT PARSING IT</title>
<author confidence="0.999989">Eric Steven Atwell</author>
<affiliation confidence="0.99992">Artificial Intelligence Group Department of Computer Studies</affiliation>
<address confidence="0.722305">Leeds University, Leeds LS2 9JT, U.K.</address>
<email confidence="0.912229">(EARN/BITNET:eric%leeds.ai@ac.uk)</email>
<abstract confidence="0.989444867620751">The Constituent Likelihood Automatic Word-tagging System (CLAWS) was originally designed for the low-level grammatical analysis of the million-word LOB Corpus of English text samples. CLAWS does not attempt a full parse, but uses a first-order Markov model of language to assign word-class labels to words. CLAWS can be modified to detect grammatical errors, essentially by flagging unlikely word-class transitions in the input text. This may seem to be an intuitively implausible and theoretically inadequate model of natural language syntax, but nevertheless it can successfully pinpoint most grammatical errors in a text Several modifications to CLAWS have been explored. The system cannot detect in typed documents; but then neither do far more complex systems, which attempt a full parse, requiring much greater computation. Checking Grammar In Texts A number of researchers have experimented with ways to cope with granunatically ill-formed English input (for example, [Carbonell and Hayes 83], [Chamiak 83], [Granger 83], [Hayes and Mouradian 81], [Heidorn et al 821, [Jensen et al 83], [Kwasny and Sondheimer 81], [Weischedel and Black 80], [Weischedel and Sondheimer 83]). However, the majority of these systems are designed for Natural Language interfaces to software systems, and so can assume a restricted vocabulary and syntax; for example, the system discussed by [Fass 83] had a vocabulary of less than 50 words. This may be justifiable for a NL front-end to a computer system such as a Database Query system, since even an artificial subset of English may be more acceptable to users than a formal command or query language. However, for automated text-checking in Word Processing, we cannot reasonably ask the WP user to restrict their English text in this way. This means that WP text-checking systems must be extremely robust, capable of analysing a very wide range of lexical and syntactic constructs. Otherwise, the grammar checker is liable to flag many constructs which are in fact acceptable to humans, but happen not to be included in the system&apos;s limited grammar. A system which not only performs syntactic analysis of text, but also pinpoints grammatical errors, must be assessed along two orthogonal scales rather than a single &apos;accuracy&apos; measure: RECALL = &amp;quot;number of words/constructs correctly flagged as errors&amp;quot; divided by &amp;quot;total number of &apos;true&apos; errors that should be flagged* PRECISION = &amp;quot;number of words/constructs correctly flagged as errors&amp;quot; divided by &amp;quot;total number of words/constructs flagged by the system&amp;quot; easy to optimise one of these performance measures at the expense of the other. flagging (nearly) ALL words in a text will guarantee optimal recall (i.e. (nearly) all actual errors will be flagged) but at a low precision; and conversely, reducing the number of words flagged to nearly zero should raise the precision but lower the recall. The problem is to balance this trade-off to arrive at recall AND precision levels acceptable to WP users. A system which can accept a limited subset of English (and reject (or flag as erroneous) anything else) may have a reasonable recall rate; that is, most of the &apos;true&apos; errors will probably be included in rejected text. However, the precision rate is liable to unacceptable to the WP user: large amounts of the input text will effectively be marked as potentially erroneous, with no indication of where&apos; within this text the actual errors lie. One way to deal with this problem is to increase the size and power of the parser and underlying grammar to deal with something nearer the whole gamut of English syntax; this is the approach taken by IBM&apos;s EPISTLE project (see [Heidorn et al 821, [Jensen et al 83]). Unfortunately, this can lead to a very large and computationally expensive system: [Heidorn et al 82] reported that the EPISTLE system required a 4Mb virtual machine (although a more efficient implementation under development should require less memory). The UNIX Writer&apos;s Workbench collection of programs (see [Cherry and Macdonald 83], [Cherry et al 83]) is probably the most widely-used system for WP text-checking (and also one of the most widely-used NLP systems overall see [Atwell 86], [Hubert 85]). This system includes a number of separate programs to check for different types of faults, including misspellings, cliches, and certain stylistic infelicities such as overly long (or short) sentences. However, it lacks a general-purpose grammar checker, the nearest program is a tool to filter out doubled words (as in &amp;quot;I signed the the contract&amp;quot;). Although there is a program PARTS which assigns a part of speech tag to each word in the text (as a precursor to the stylistic analysis programs), this program uses a set of localized heuristic rules to disambiguate words according to context; and these rules are based on the underlying assumption that the input sentences are grammatically well-formed. So, there is no clear way to modify PARTS to flag grammatical errors, unless we introduce a radically different mechanism for disambiguating word-tags according to context. 38 LOB and CLAWS One such alternative word-tag disambiguation mechanism was developed for the analysis of the Lancaster-Oslo/Bergen (LOB) Corpus. The LOB Corpus is a million-word collection of English text samples, used for experimentation and inspiration in computational linguistics and related studies (see for example [Leech et al 83a], [Atwell forthcoming b]). CLAWS, the Constituent-Likelihood Automatic Word-tagging System ([Leech et al 83b1, [Atwell et al 84]), was developed to annotate the raw text with basic grammatical information, to make it more useful for linguistic research; CLAWS did not attempt a full parse of each sentence, but simply marked each word with a grammatical code from a set of 133 WORDTAGS. The word-tagged LOB Corpus is now available to other researchers (see [Johansson et al 86]). CLAWS was originally implemented in Pascal, but it is currently being recoded in C and in POPLOG Prolog. CLAWS can deal with Unrestricted English text input including &amp;quot;noisy&amp;quot; or ill-formed sentences, because it is based on Constituent Likelihood Grammar, a novel probabilistic approach to grammatical description and analysis described in [Atwell 83]. A Constituent Likelihood Grammar is used to calculate likelihoods for competing putative analysis; not only does this tell us which is the &apos;best&apos; analysis, but it also shows how &apos;good&apos; this analysis is. For assigning word-tags to words, a simple Markovian model can be used instead of a probabilistic rewrite-rule system (such as a probabilistic context-free grammar); this greatly simplifies processing. CLAWS first uses a dictionary, suffixlist and other default routines to assign a set of putative tags to each word; then, each sequence of ambiguously-tagged words, likelihood of every possible combination or &apos;chain&apos; of tags is evaluated, and the best chain is chosen. The likelihood of each chain of tags is evaluated as a product of all the &apos;links&apos; (tag-pair-likelihoods) in the sequence; tag-pair likelihood is a function of the frequency of that sequence of two tags in a sample of tagged text, compared to the frequency of each of the two tags individually. An important advantage of this simple Markovian model is that word-tagging is done without parsing: there is no need to work out higher-level constituent-structure trees before assigning unambiguous word-tags to words. Despite its simplicity, this technique is surprisingly robust and successful: CLAWS has been used to analyse a wide variety of Unrestricted English, including extracts form newspapers, novels, diaries, learned journals, E.E.C. regulations, etc., with a consistent accuracy of c96%. Although the system did not have parse trees available in deciding word-classes, only c4% of words in the LOB Corpus had to have their assigned wordtag corrected by manual editing (see [Atwell 81, 82]). Another important advantage of the simple Markovian model is that it is relatively straightforward to transfer the model from English to other Natural Languages. The basic statistical model remains, only the dictionary and Markovian tag-pair frequency table need to be replaced. We are experimenting with the possibility of (partially) automating even this process see [Atwell 86a, 86b, forthcoming c], [Atwell and Drakos 87]. The general Constituent Likelihood approach to grammatical analysis, and CLAWS in particular, can be used to analyse text including ill-formed syntax. More importantly, it can also be adapted to flag syntactic errors in texts; unlike other techniques for error-detection, these modifications of CLAWS lead to only limited increases in processing requirements. In fact, various different types of modification are possible, yielding varying degrees of success in error-detection. Several different techniques have been explored. Error Likelihoods A very simple adaptation of CLAWS (simple in theory at least) is to augment the tag-pair frequency table with a taglikelihood As in the original system, CLAWS uses the tag-pair frequency table and the Constituent Likelihood formulae to find the best word-tag for each word. Having found the best tag for each word, every cooccurring pair of tags in the analysis is re-assessed: the ERROR-LIKELIHOOD of each tag-pair is checked. Errorlikelihood is a measure of how frequently a given tag-pair occurs in an error as compared to how frequently it occurs in valid text. For example, if the user types farther was ... CLAWS will yield the word-tag analysis PP$ RBR BEDZ which means &lt;possessive personal pronoun&gt;, &lt;comparative adverb&gt;, &lt;past singular BE&gt;. This analysis is then passed to the checking module, which uses tag-pair frequency statistics extracted from copious samples of errorfull texts. These should show that tag-pairs &lt;PPS RBR&gt; and BEDZ&gt; often where there is a typing error, and rarely occur in grammatically correct constructs; so an error can be flagged at the corresponding point in the text. Although the adjustment to the model is theoretically simple, the tag-pair error likelihood frequency figures required could only be gleaned by human analysis of huge amounts of error-full text. Our initial efforts to collect an Corpus us that this approach was impractical because of the time and effort required to collect the necessary data. In any case, an alternative technique which managed without a separate table of tag-pair error likelihoods turns out to be quite successful. Low Absolute Likelihoods This alternative technique involved using CLAWS unmodified to choose the best tag for each word, as before, and then measuring ABSOLUTE LIKELIHOODS of tagpairs. Instead of a separate tag-pair error likelihood table to assess the grammaticality, the same tag-pair frequency table is used for tag-assignment and error-detection. The tag-pair frequency table gives frequencies for grammatically wellformed text, so the second module simply assumes that if a low-likelihood tag pair occurs in the input text, it indicates a grammatical error. In the example above, tag-pairs &lt;PPS RBR&gt; and &lt;RBR BEDZ&gt; have low likelihoods (as they occur only rarely in grammatically well-formed text), so an error can be diagnosed. Figure 1 is a fuller example of this approach to error diagnosis. This shows the analysis of a short text; please note that the text was constructed for illustration purposes 39 only, and the characters mentioned bear no resemblance to real living people! The text contains many mis-typed words, but these mistakes would not be detected by a conventional spelling-checker, since the error-forms happen to coincide with other legal English words; the only way that these errors can be detected is by noticing that the resultant phrases and clauses are ungrammatical. The grammarchecking program first divides the input text into words. Note that this is not entirely trivial: for example, enclitics as won&apos;t split into two words + will + a&apos;r. The left-hand column in Figure 1 shows the sequence of words in the sample text, one word per line. The second column shows the grammatical tag chosen using the Constituent Likelihood model as best in the given context. The third column shows the absolute likelihood of the chosen grammatical tag; this likelihood is normalised relative threshold, that values greater than one constitute &amp;quot;acceptable&amp;quot; grammatical analyses, whereas values less than one are indicative of unacceptably improbable grammar. Whenever the absolute likelihood value falls below this threshold, the flag output in the fourth column, to draw visual attention to the putative error. for example, the first word in the text, tagged personal pronoun), and this tag has a normalised absolute likelihood of over 15, which is the second word, tagged (comparative adverb), but this time the absolute likelihood is below one (0.264271), so the word is flagged as a putative ERROR? This technique is extremely primitive, yet appears to work fairly well. There is no longer any need to gather error-likelihoods from an Error Corpus. However, the definition of what constitutes a &amp;quot;low&amp;quot; likelihood is not straightforward. On the whole, there is a reasonably clear between words marked actual so clearly low values taken as diagnostic of errors, once the question of what constitutes &amp;quot;lowness&amp;quot; has been defined rigorously. In the example, the acceptability level is defined in terms of a simple threshold: likelihoods are normalised so values below 1.000003 are deemed too low to be acceptable. The appropriate normalisation scaling factor was found empirically. Unfortunately, a threshold at this level would mean some minor troughs would not be e.g. / a meat clever,... was ii (adjective) but should have been the noun a normalised likelihood of 4.516465; tame of Eroc Auwell... was also tagged JJ but should have been the noun ) has a likelihood of 4.516465; and the phrase day should have been day ) a normalised likelihood of 4.060886 (although this is, strictly speaking, with day rather than error flag would be sufficiently close to the actual error to draw the user&apos;s attention to it). However, if we raised the threshold (or alternatively changed the normalisation function so that these normalised likelihoods are below 1.000000), then more words would be flagged, lowering the precision of error diagnosis. In some cases, error diagnosis would be &amp;quot;blurred&amp;quot;, since sometimes words immediately before and/or the error also have low likelihoods; for example, farther was very crawl.., a likelihood of 1.216545. Worse, some error flags would appear in completely inappropriate places, with no true errors in the immediate for example, the exclamation mark at the end of get away with this! a likelihood of 4.185351 and so would probably be flagged as an error if the threshold were raised. Another way to define a trough would be as a local minimum, that is, a point on where points immediately before and after have higher likelihood values; even a trough with a quite high value is flagged this way so long as points are even higher. This would catch day above. However, strictly speaking several other words not currently flagged in Figure are also local minima, for example in perhaps my would ... 4&amp;quot; in bald at me if I ... this definition is liable to cause a greater number of &apos;red herring&apos; valid words to be erroneously flagged as putative mistakes, again leading to a worse precision. Once an optimal threshold or other computational definition of low likelihood has been chosen, it is a simple matter to amend the output routine to produce output in a simplified format acceptable to Word Processor users, without grammatical tags or likelihood ratings but with putative errors flagged. However, even with an optimal measure of lowness, the success rate is unlikely to be perfect. The model deliberately incorporates only rudimentary knowledge about English: a lexicon of words and their wordtags, and a tag-pair frequency matrix embodying knowledge of tag cooccurrence likelihoods. Certain types of error are unlikely to be detected without some further knowledge. One limited augmentation to this model involves the addition of tags to analysis procedure. Error-Tags rather sophisticated technique for taking syntactic context into account involves adding ERROR-TAGS to lexical entries. These are the tags of any similar words (where these are different from the word&apos;s own tags). In the analysis phase, the system must then choose the best tag (from error-tag(s) and &apos;own&apos; tag(s)) according to syntactic context, still using the unmodified CLAWS Constituentmodel. For example, in the sentence am very error can be diagnosed if the system works out that tags of input word ( VB, VBD, and VBN - &lt;singular common noun&gt;, &lt;verb infinitive&gt;, &lt;verb past tense&gt;, &lt;verb past participle&gt;) are all much less likely in the given context than LI (&lt;adjective&gt;), known to be the tag of a word ( ). a rather more sophisticated errordetection system includes knowledge not just about tags of words, but also about what alternative word-classes would be plausible if the input was an error. This information consists in an additional field in lexicon entries: each dictionary entry hold (i) the word itself, (ii) the word&apos;s and (iii) the error-tags associated with the word. For example: WORD TAG(S) ERROR-TAG(S) form NN 1N# RI# hit NN VB VBD VBN prophecy NN VB# Note that error-tags are marked with # to distinguish from CLAWS then chooses the best tag for each word as usual. However, in the final output, instead of each word being marked with the chosen word-tag, words associated with an ERROR TAG are flagged as potential errors. To illustrate why error-tags might help in error diagnosis, that maid several dense in his ... not have a below-threshold absolute likelihood, and so is not flagged as a putative error. An error-tag based system could calculate that the best sequence of tags (allowing error-tags) the word sequence dense in his ... [AP NNS# IN PPM (&lt;post-determiner&gt;, &lt;plural common noun&gt;, &lt;preposition&gt;, &lt;possessive personal pronoun&gt;). Since NNS is an error-tag, an error is flagged. However, the simpler absolute likelihood based model does not allow for the of choosing NNS as the tag for is forced to choose the best of the &apos;own&apos; tags; this in turn causes a of NNU (&lt;abbreviated unit of measurement&gt;, since [JJ NNU] (&lt;adjective&gt; &lt;abbreviated unit of measurement&gt;) is likelier than [11 IN] (&lt;adjective&gt; &lt;preposition&gt;). Furthermore, [ll NNU] turns out not to be an exceptionally unusual tag cooccurrence. The point of all this is that, without error-tags, the the system may mistag words immediately before or after error-words, and this mistagging may well distort the absolute likelihoods used for error diagnosis. This error-tag-based technique was originally proposed and illustrated in [Atwell 83]. The method has been tested with a small test lexicon, but we have yet to build a complete dictionary with error-tags for all words. Adding error tags to a large lexicon is a non-trivial research task; and adding error-tags to the analysis stage increases computation, since there are more tags to choose between for each word. So far, we have not found conclusive evidence that the success rate is increased significantly; this requires further investigation. Also to be more fully investigated is how to take account of other relevant factors in error diagnosis, in addition to error-tags. Full Cohorts In theory at least, the Constituent-Likelihood method could be generalised to take account of all relevant contextual factors, not just syntactic bonding. This could be done by generating COHORTS for each input word, and then choosing the cohort-member word which fits the context For example, if the sentence were very hit input, the following cohorts would be generated: you yew ewe were where wear very vary veery hit hot hut hat (the term &amp;quot;cohort&amp;quot; is adapted from [Marslen-Wilson 85] with a slight modification of meaning). Cohorts of similar can from the spelling-check dictionary using the same algorithm employed to suggest corrections for misspellings in current systems; these techniques are fairly well-understood (see, for example, [Yannakoudakis and Fawthrop], [Veronis 871, (Borland 851). Next, each of a cohort is assigned a likelihood rating, taking into account relevant factors including: i) the degree of similarity to the word actually typed (this measure would be available anyway, as it has to be calculated during cohort generation; the actual word typed gets a similarity factor of 1, and other members of the cohort get appropriate lower weights) ii) the &apos;degree of fit&apos; in the given syntactic context as the constituent likelihood bond between the tag(s) of each cohort member and the tag(s) of the words before and after, using the CLAWS constituent likelihood formulae); iii) the frequency of usage in general English (common words like &amp;quot;you&amp;quot; and &amp;quot;very&amp;quot; get a high weighting factor, rare words like &amp;quot;ewe&amp;quot;, &amp;quot;yew&amp;quot;, and &amp;quot;veery&amp;quot; get a much lower weighting; word relative frequency figures can be gleaned from statistical studies of large Corpora, such as [Hofland and Johansson 82], [Francis and Kucera 82], [Carroll et al 71]); iv) if a cohort member occurs in a grammatical idiom or preferred collocation with surrounding words, then its relative weighting is increased (e.g. in the context &amp;quot;fish and chips a higher collocation weighting than collocation preferences can also be elicited from studies of large corpora using techniques such as those of [Sinclair et al 70]; v) domain-dependent lexical preferences should ideally be taken into account, for example in an electronics manual get a higher domain weighting than All these factors are multiplied (using appropriate weightings) to yield a relative likelihood rating for each member of the cohort. The cohort-member with the highest rating is (probably) the intended word; if the word actually typied is different, an error can be diagnosed, and furthermore a correction can be offered to the user. Unfortunately, although this approach may seem sensible in theory, in practice it would require a huge R&amp;D effort to gather the statistical information needed to drive such a system, and the resulting model would be computationally complex and expensive. It would be more sensible to try to incorporate only those features which contribute significantly to increased error-detection, and ignore all other factors. This means we must test the existing error-detection system extensively, and analyse the failures to try to discover what additional knowledge would be useful to the system. Error Corpus The error-likelihood and full-cohort techniques would appear to give the best error-detection rates, but require vast computations to build a general-purpose system from scratch. The error-tag technique also requires a substantial research effort to build a large general-purpose lexicon. A version of the Constituent Likelihood Automatic Word-tagging System to ABSOLUTE LIKELIHOOD method of error-detection has been more extensively tested; this system detect errors, but appears to be quite successful with certain classes of errors. To test alternative prototypes, we are building up an ERROR CORPUS of texts containing errors. The LOB Corpus includes many errors 41 which appeared in the original published texts; these are marked SIC in the text, and noted in the Manual which comes with the Corpus files, [Johansson et al 781. The initial Error Corpus consisted in these errors, and it is being added to from other sources (see Acknowledgements below). The errors in the Error Corpus can be (manually) classified according to the kind of processing required for detection (the examples below starts with a LOB line reference number): A: non-word error-forms, where the error can be found by simple dictionary-lookup; for example, A21 115 As the news pours in from around the world, beleagured (SIC) Berlin this weekend is a city on a razor&apos;s edge. B: error-forms involving valid English words in an invalid grammatical context, the kind of error the CLAWSbased approach could be expected to detect (these may be due to spelling or typing or grammatical mistakes by the typist, but this is irrelevant here: the classification is according to the type of processing required by the detection program); for example E18 121 Unlike an oil refinery one cannot grumble much about the fumes, smell and industrial dirt, generally, for little comes out of the chimney except possibly invisible gasses. (SIC) C: error-forms which are valid English words, but in an abnormal grammatical/semantic context, which a CLAWSsystem would not detect, but which be caught by a very sophisticated parser; for example, breaking &apos;long-distance&apos; number agreement rules as in Al5 170 It is, however, reported that the tariff on textiles and cars imported from the Common Market are (SIC) to be reduced by 10 per cent. D: lexically and syntactically valid error-forms which would require &amp;quot;intelligent&apos; semantic analysis for detection; for example, PI7 189 She did not imagine that he would pay her a visit except in Frank&apos;s interest, and when she hurried into the room where her mother was Dying in vain to learn the reason of his visit, her first words were of her fiancee. (SIC) or K29 35 He had then sown (SIC) her up with a needle, and, after a time she had come back to him cured and able to bear more children. Collection and detailed analysis of texts for this Error Corpus is still in progress at the time of writing; but one important early impression is that different sources show widely different distributions of error-classes. For example, a sample of 150 errors from three different sources shows the following distribution: i) Published (and hence manually proofread) text: A: 52% B: 28% C: 8% D: 12% ii) essays by 11and 12-year-old children: A: 36% B: 38% C: 16% D: 10% non-native English speakers: A: 4% B: 48% C: 12% D: 36% Because of this great variation, precision and recall rates are also liable to vary greatly according to text source. In a production version of the system, the &apos;unusualness&apos; threshold (or other measure) used to decide when to flag putative errors will be chosen by the user, so that users can optimise precision or recall. It is not clear how this kind of usercustomisation could be built into other WP text-checking systems; but it is an obvious side-benefit of a Constituent Likelihood based system. Conclusions The figures above indicate that a CLAWS-based granunar-checker would be particularly useful to non-native English speakers; but even for this class of users, precision and recall are imperfect. The CLAWS-based system is inadequate on its own, but should properly be used as one tool amongst many; for example as an augmentation to the Writer&apos;s Workbench collection of text-critiquing and proofreading programs, or in conjunction with other English Language Teaching tools such as a computerised ELT dictionary (such as those discussed by [Akkennan et al 85] or [Atwell forthcoming a]. Other systems for dealing with syntactically ill-formed English attempt a full grammatical parse of each input sentence, and in addition require errorrecovery routines of varying degrees of sophistication. This involves much more processing than the CLAWS-based and yet even these systems fail to diagnose in a text. Clearly, the Constituent-Likelihood error-detection technique is ideally suited to applications where fast processing and relatively small computing requirements are of paramount importance, and for users who find imperfect error-detection better than none at all. I freely admit that the system has not yet been comprehensively tested on a wide variety of WP users; as with all Al research systems, a lot af work still has to be done to engineer a generally-acceptable commercial product. We are currently looking for sponsors and collaborators for this research: anyone interested in developing the prototype into a robust system (for example, to be integrated into a WP system) is invited to contact the author! ACKNOWLEDGEMENTS This paper was originally produced in 1986 as Department of Computer Studies Research Report no212, Leeds University. I gratefully acknowledge the help of supervisors, colleagues and friends at the Universities of Lancaster and Â°Leeds. The original CLAWS system was developed by Ian Marshall, Roger Garside, Geoffrey Leech and myself at Lancaster University, for a project funded by the Social Science Research Council. Stephen Elliott spent a lot of time building up the Error Corpus and testing variants of the error-detection system, funded by an ICL Research Associateship. Pauline McCrorie and Matthias Wong worked on the POPLOG prolog and C versions of CLAWS.</abstract>
<note confidence="0.49232925">Various other colleagues have also offered advice and encouragement, particularly Geoffrey Sampson, Stuart Roberts, Chris Paice, Lita Taylor, Andrew Beale, Susan 42</note>
<author confidence="0.640346">Proceedings of the ICAME Conference on the</author>
<affiliation confidence="0.894969">use of computer corpora in English Language Research,</affiliation>
<address confidence="0.928828">Netherlands</address>
<email confidence="0.526628">REFERENCES</email>
<note confidence="0.680863">Akkerman, Erik, Pieter Masereeuw, and Willem Meijs 1985</note>
<title confidence="0.7521512">Designing a computerized lexicon for linguistic purposes Rodopi, Amsterdam Eric Steven 1981 Corpus Tagging Project: Pre-edit Handbook. of Computer Studies and Linguistics, University of Lancaster</title>
<author confidence="0.557563">Eric Steven Corpus Tagging Project</author>
<affiliation confidence="0.6567116">Manual Postedit Handbook (A mini-grammar of LOB Corpus English, examining the types of error commonly made during automatic (computational) analysis of ordinary English.) of Computer Studies and Linguistics, University of Lancaster</affiliation>
<address confidence="0.876673">Atwell, Eric Steven 1983 &amp;quot;Constituent-Likelihood Grammar&amp;quot;</address>
<note confidence="0.5072195">of the International Computer Archive of English (ICAME NEWS) 34-67, Norwegian</note>
<affiliation confidence="0.893486">Computing Centre for the Humanities, Bergen University</affiliation>
<author confidence="0.893776">Eric Steven a a Natural Language</author>
<note confidence="0.58068165">from raw text of Computer Studies Research Report no.208, University of Leeds Atwell, Eric Steven 1986b, &amp;quot;A parsing expert system which from corpus analysis&amp;quot; in Willem Meijs (ed) Linguistics and Beyond: Proceedings of the Seventh International Conference on English Language Research on Corpora, Amsterdam, Netherlands Amsterdam Atwell, Eric Steven 1986c &amp;quot;Beyond the micro: advanced software for research and teaching from computer science and artificial intelligence&amp;quot; in Leech, Geoffrey and Candlin, Computers in English language teaching and research: selected papers from the British Council Symposium on computers in English language education and Lancaster, England Longman Atwell, Eric Steven (forthcoming a) &amp;quot;A lexical database for English learners and users: the Oxford Advanced Learner&apos;s to appear in of ICDBHSS87, the 1987 International Conference on DataBases in the Humanities and Social Sciences, Montgomery, Alabama,</note>
<address confidence="0.943864">USA</address>
<abstract confidence="0.758341">Atwell, Eric Steven (forthcoming b) &amp;quot;Transforming a Parsed into a Corpus Parsee&apos;, to appear in of the 1987 ICAME 8th International Conference on English</abstract>
<affiliation confidence="0.874208">Language Research on Computerised Corpora, Helsinki,</affiliation>
<address confidence="0.707561">Finland</address>
<note confidence="0.716379769230769">Atwell, Eric Steven (forthcoming c) &amp;quot;An Expert System for the Automatic Discovery of Particles&amp;quot; to appear in Proceedings of the 1987 International Conference on the Study of Particles, Berlin, East Germany Atwell, Eric Steven, Geoffrey Leech and Roger Garside 1984, &amp;quot;Analysis of the LOB Corpus: progress and in Jan Aarts and Willem Meijs (ed), E Atwell and N Dmkos, &amp;quot;Pattern Recognition Applied to the Acquisition of a Grammatical Classification System from Unrestricted English Text&amp;quot; to appear in the Proceedings of the Association for Computational Linguistics Third European Chapter Conference, 1987 (forthcoming). International Inc. 1985 Lightning: Owner&apos;s</note>
<address confidence="0.84781">International, Scotts Valley, California USA</address>
<note confidence="0.7657615">Carbonell, Jaime and Philip Hayes 1983 &amp;quot;Recovery strategies parsing extragranunatical language&amp;quot; in Journal Computational Linguistics 123-146 John, Peter Davies, and Barry Richman 1971</note>
<title confidence="0.6875304">Heritage word frequency book Mifflin / American Heritage Chamiak, Eugene 1983 &amp;quot;A parser with something for in Margaret King (ed) Natural Language Academic Press, London</title>
<author confidence="0.742908">L Cherry</author>
<author confidence="0.742908">M Fox</author>
<author confidence="0.742908">L Frase</author>
<author confidence="0.742908">P Gingrich</author>
<author confidence="0.742908">S Keenan</author>
<note confidence="0.985508">N 1983 &amp;quot;Computer aids for text analysis&amp;quot; in Records, 10-16 Cherry, Lorinda and Macdonald, Nina 1983 &amp;quot;The Writer&apos;s software&amp;quot; in 241-248 Fass, Dan, and Yorick Wilks 1983 &amp;quot;Preference semantics, and metaphor&amp;quot; in Journal of Linguistics 178-187 W Nelson, and Henry Kucera 1982</note>
<title confidence="0.5434895">of English usage: lexicon and grammar Mifflin</title>
<author confidence="0.235815">Richard The NOMAD system expectation- Granger</author>
<abstract confidence="0.771165">based detection and correction of errors during understanding syntactically and semantically ill-formed text&amp;quot; in</abstract>
<note confidence="0.771821117647059">of Computational Linguistics 188-196 Hayes, Philip I, and G V Mouradian 1981 &amp;quot;Flexible Parsing&amp;quot; American Journal of Computational Linguistics 232-242 Heidom, G E, Jensen, K, Miller, L A, Byrd, R J, and Chodorow, M S, 1982 &amp;quot;The EPISTLE text-critiquing in Systems Journal 305-326 Knut and Stig Johansson 1982 frequencies in and American English Henry 1985 and Composition: an bibliography, Education 534 Resource Report, University of British Columbia Jensen, K, Heidom, G E, Miller, L A, and Ravin, Y 1983 &amp;quot;Parse fitting and prose fixing: getting a hold on illin Journal of Computational 147-160 Johansson, Stig, Geoffrey Leech and Helen Goodluck 1978</note>
<title confidence="0.48398">Manual of information to accompany the Lancaster-</title>
<author confidence="0.472294">OsloBergen Corpus of British English</author>
<author confidence="0.472294">for use with digital</author>
<affiliation confidence="0.980647">of English, Oslo University</affiliation>
<address confidence="0.728289">Johansson, Stig, Eric Atwell, Roger Garside, and Geoffrey</address>
<note confidence="0.32021475">1986 Tagged LOB Corpus Computing Centre for the Humanities, University of Bergen, Norway. Kwasny, S and Norman Sondheimer 1981 &amp;quot;Relaxation techniques for parsing grammatically ill-formed input in language understanding systems&amp;quot; American of Computational Linguistics 99-108 Leech, Geoffrey, Roger Garside, and Eric Steven Atwell 1983a, &amp;quot;Recent developments in the use of computer corpora English language research&amp;quot; in of the Society 23-40. Leech, Geoffrey, Roger Garside, and Eric Steven Atwell 1983b, &amp;quot;The Automatic Grammatical Tagging of the LOB</note>
<title confidence="0.389109">in of the International Computer Archive</title>
<author confidence="0.54627">Modern English</author>
<affiliation confidence="0.997039">Computing Centre for the Humanities, Bergen University</affiliation>
<address confidence="0.897377">Marslen-Wilson, W D 1985 &amp;quot;Aspects of human speech</address>
<note confidence="0.27995425">understanding&amp;quot; in Fallside, Frank and Woods, William (eds.) speech processing, John, Jones, S, and Daley, R 1970 lexical to OSTI on project OLP/08; Dept of English,</note>
<affiliation confidence="0.99601">Birmingham University</affiliation>
<author confidence="0.540898666666667">Jean Correction of phonographic errors in natural language processing in Oakman Veronis</author>
<author confidence="0.540898666666667">Robert</author>
<author confidence="0.540898666666667">Barbara Proceedings of the Eighth</author>
<affiliation confidence="0.713820333333333">International Conference on Computers and the Humanities, Department of Computer Science, University of South Carolina</affiliation>
<address confidence="0.733728">Weischedel, Ralph, and John Black 1980 &amp;quot;Responding</address>
<abstract confidence="0.651346212765958">to unparsable inputs&amp;quot; in Journal of Linguistics 97-109 Ralph, and Norman Sondheimer 1983 &amp;quot;Metaas a basis for processing ill-formed input&amp;quot; in of Computational Linguistics Yannakoudakis, E J, and Fawthrop, D 1983 &amp;quot;The rules of errors&amp;quot; Information processing and management 19(2): 87-99 44 Figure I. Sample output with low Likelihoods flagged. with IN 35.186770 2L792427 this 127 my PPS 15.297639 4.185351 farther RBR 0.264271 ERROR? PPM 90.897396 was BEDZ 1.216545 stole VBD 135.815263 very QL 22.137197 a AT 39.564677 crawl NN 0.259613 ERROR? meat NN 191.684559 103.174992 clever 12 4.516465 he PPM 90.897396 24.477376 bald ii 0.271961 ERROR? and CC 82.096986 at IN 17.237397 PPM. 25.834909 me 9910 29.279452 maid NN 0.059657 ERROR? if CS 11.400905 several AP 2.055110 PP1A 71.313009 dense ii 8.725460 dud 13 0.2/1961 ERROR? in NNU 31945608 anything PN 0.088535 ERROR? his PPS 0.306138 ERROR? wrong .21 1.682160 hid VBD 0.099010 ERROR? 24.477376 with IN 34.451138 and CC 82096986 It 993 9.309486 sometimes RB 29.179920 11.826017 he P91A 9.921162 It 993 62.337141 would MD 64.525545 must MD 46.875000 hot .21 0.230232 ERROR? have HV 41513082 and CC 24.663050 hurt VB 0.527257 ERROR? bit NN 20.025340 a AT 45.661755 me 9910 0.062710 ERROR? lit VBD 0.037789 ERROR?</abstract>
<address confidence="0.907239777777778">18.500350 22.778418 until CS 29.873133 SOO NN 9.189478 PPM 71313009 the All 4.149936 was BEDZ 95.448591 gruesome NN 160.254821 90 QL 22.137197</address>
<phone confidence="0.379676">tame 31 4316445</phone>
<note confidence="0.893679733333333">week NN 0.289613 ERROR? of IN 17.237397 and CC 42.917870 Erne NN 54.835271 miserable NN 20.028340 Attwell NN 26.254356 that CS 15.439211 PP1A 71.313009 appeared VBN 8387370 In NNU 4.870130 wanted VBD 135315263 all ABN 0.265393 ERROR? to TO 25.445266 the ATI 3.499841 due JI 0.216826 ERROR? Palmy NNS 40.467490</note>
<phone confidence="0.6129">21.911547 70.542572</phone>
<address confidence="0.875333111111111">finally RB 36.564715 perhaps RB 36364715 45.403013 my PPS 5.473606 won VBD 25.409130 friends PINS 44.477694 day NN 4.060886 would MD 15.005662 84.114626</address>
<note confidence="0.8023435">learnt VBN 0.237220 ERROR? PPM 36.536284 to TO 34.470793 decided VBD 135.815263 spell NN 0.061250 ERROR? to TO 28445266</note>
<abstract confidence="0.3845475">my PPS 0.545207 ERROR? got VBD 0.102690 ERROR? name NN 51.946085 my PPS 30.396041 correctly 21 4.516465 won VBD 0.099010 ERROR? at IN 17.237397 back RP 21.849187 last AP 10.850327 on IN 10.259310 3.437432 him P910 29.279452 3.242075 PP1A 4.764065 MD 64.525545 Figure 1. Sample output with low Likelihoods flagged. mike NN 0.123308 ERROR? him 9930 0.062710 ERROR? Pay VB 10.708766 1.396258 he PP3A 4.764065 will MD 64.525545 n&apos;t XNOT 95.159151 get VS 0.145558 ERROR? away RB 29.196041</abstract>
<intro confidence="0.334931">45</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Steven Atwell</author>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
</authors>
<title>Analysis of the LOB Corpus: progress and prospects&amp;quot;,</title>
<date>1984</date>
<booktitle>in Jan Aarts and Willem Meijs (ed), Corpus</booktitle>
<marker>Atwell, Leech, Garside, 1984</marker>
<rawString>Atwell, Eric Steven, Geoffrey Leech and Roger Garside 1984, &amp;quot;Analysis of the LOB Corpus: progress and prospects&amp;quot;, in Jan Aarts and Willem Meijs (ed), Corpus</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Atwell</author>
<author>N Dmkos</author>
</authors>
<title>Pattern Recognition Applied to the Acquisition of a Grammatical Classification System from Unrestricted English Text&amp;quot; to appear</title>
<date>1987</date>
<booktitle>in the Proceedings of the Association for Computational Linguistics Third European Chapter Conference,</booktitle>
<marker>Atwell, Dmkos, 1987</marker>
<rawString>E Atwell and N Dmkos, &amp;quot;Pattern Recognition Applied to the Acquisition of a Grammatical Classification System from Unrestricted English Text&amp;quot; to appear in the Proceedings of the Association for Computational Linguistics Third European Chapter Conference, 1987 (forthcoming).</rawString>
</citation>
<citation valid="true">
<title>Turbo Lightning: Owner&apos;s Handbook Borland International, Scotts Valley,</title>
<date>1985</date>
<institution>Borland International Inc.</institution>
<location>California USA</location>
<marker>1985</marker>
<rawString>Borland International Inc. 1985 Turbo Lightning: Owner&apos;s Handbook Borland International, Scotts Valley, California USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Philip Hayes</author>
</authors>
<title>Recovery strategies for parsing extragranunatical language&amp;quot;</title>
<date>1983</date>
<journal>in American Journal of Computational Linguistics</journal>
<volume>9</volume>
<issue>3</issue>
<pages>123--146</pages>
<marker>Carbonell, Hayes, 1983</marker>
<rawString>Carbonell, Jaime and Philip Hayes 1983 &amp;quot;Recovery strategies for parsing extragranunatical language&amp;quot; in American Journal of Computational Linguistics 9(3-4): 123-146</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Peter Davies</author>
<author>Barry Richman</author>
</authors>
<title>The American Heritage word frequency book Houghton Mifflin /</title>
<date>1971</date>
<publisher>American Heritage</publisher>
<marker>Carroll, Davies, Richman, 1971</marker>
<rawString>Carroll, John, Peter Davies, and Barry Richman 1971 The American Heritage word frequency book Houghton Mifflin / American Heritage</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Chamiak</author>
</authors>
<title>A parser with something for everyone&amp;quot; in Margaret King (ed) Parsing Natural Language</title>
<date>1983</date>
<publisher>Academic Press,</publisher>
<location>London</location>
<marker>Chamiak, 1983</marker>
<rawString>Chamiak, Eugene 1983 &amp;quot;A parser with something for everyone&amp;quot; in Margaret King (ed) Parsing Natural Language Academic Press, London</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cherry</author>
<author>M Fox</author>
<author>L Frase</author>
<author>P Gingrich</author>
<author>S Keenan</author>
<author>N Macdonald</author>
</authors>
<title>Computer aids for text analysis&amp;quot; in Bell Laboratories Records,</title>
<date>1983</date>
<pages>10--16</pages>
<location>May/June:</location>
<marker>Cherry, Fox, Frase, Gingrich, Keenan, Macdonald, 1983</marker>
<rawString>Cherry, L, Fox, M, Frase, L, Gingrich, P, Keenan, S. and Macdonald, N 1983 &amp;quot;Computer aids for text analysis&amp;quot; in Bell Laboratories Records, May/June: 10-16</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorinda Cherry</author>
<author>Macdonald</author>
</authors>
<title>The Writer&apos;s WorkBench software&amp;quot;</title>
<date>1983</date>
<booktitle>in BYTE October:</booktitle>
<pages>241--248</pages>
<location>Nina</location>
<marker>Cherry, Macdonald, 1983</marker>
<rawString>Cherry, Lorinda and Macdonald, Nina 1983 &amp;quot;The Writer&apos;s WorkBench software&amp;quot; in BYTE October: 241-248</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
<author>Yorick Wilks</author>
</authors>
<title>Preference semantics, ill-formedness, and metaphor&amp;quot;</title>
<date>1983</date>
<journal>in American Journal of Computational Linguistics</journal>
<volume>9</volume>
<issue>3</issue>
<pages>178--187</pages>
<marker>Fass, Wilks, 1983</marker>
<rawString>Fass, Dan, and Yorick Wilks 1983 &amp;quot;Preference semantics, ill-formedness, and metaphor&amp;quot; in American Journal of Computational Linguistics 9(3-4): 178-187</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>Frequency analysis of English usage: lexicon and grammar</title>
<date>1982</date>
<location>Houghton Mifflin</location>
<marker>Francis, Kucera, 1982</marker>
<rawString>Francis, W Nelson, and Henry Kucera 1982 Frequency analysis of English usage: lexicon and grammar Houghton Mifflin</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Granger</author>
</authors>
<title>The NOMAD system: expectationbased detection and correction of errors during understanding of syntactically and semantically ill-formed text&amp;quot;</title>
<date>1983</date>
<journal>in American Journal of Computational Linguistics</journal>
<volume>9</volume>
<issue>3</issue>
<pages>188--196</pages>
<marker>Granger, 1983</marker>
<rawString>Granger, Richard 1983 &amp;quot;The NOMAD system: expectationbased detection and correction of errors during understanding of syntactically and semantically ill-formed text&amp;quot; in American Journal of Computational Linguistics 9(3-4): 188-196</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip I Hayes</author>
<author>G V Mouradian</author>
</authors>
<title>Flexible Parsing&amp;quot;</title>
<date>1981</date>
<journal>in American Journal of Computational Linguistics</journal>
<volume>7</volume>
<issue>4</issue>
<pages>232--242</pages>
<marker>Hayes, Mouradian, 1981</marker>
<rawString>Hayes, Philip I, and G V Mouradian 1981 &amp;quot;Flexible Parsing&amp;quot; in American Journal of Computational Linguistics 7(4): 232-242</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidom</author>
<author>K Jensen</author>
<author>L A Miller</author>
<author>R J Byrd</author>
<author>M S Chodorow</author>
</authors>
<title>The EPISTLE text-critiquing system&amp;quot;</title>
<date>1982</date>
<journal>in IBM Systems Journal</journal>
<volume>21</volume>
<issue>3</issue>
<pages>305--326</pages>
<marker>Heidom, Jensen, Miller, Byrd, Chodorow, 1982</marker>
<rawString>Heidom, G E, Jensen, K, Miller, L A, Byrd, R J, and Chodorow, M S, 1982 &amp;quot;The EPISTLE text-critiquing system&amp;quot; in IBM Systems Journal 21(3): 305-326</rawString>
</citation>
<citation valid="true">
<authors>
<author>Knut Hofland</author>
<author>Stig Johansson</author>
</authors>
<date>1982</date>
<booktitle>Word frequencies in British and</booktitle>
<publisher>American English Longman</publisher>
<marker>Hofland, Johansson, 1982</marker>
<rawString>Hofland, Knut and Stig Johansson 1982 Word frequencies in British and American English Longman</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Hubert</author>
</authors>
<title>Computers and Composition: an annotated bibliography,</title>
<date>1985</date>
<journal>English Education</journal>
<tech>Resource Report,</tech>
<volume>534</volume>
<institution>University of British Columbia</institution>
<marker>Hubert, 1985</marker>
<rawString>Hubert, Henry 1985 Computers and Composition: an annotated bibliography, English Education 534 Resource Report, University of British Columbia</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
<author>G E Heidom</author>
<author>L A Miller</author>
<author>Y Ravin</author>
</authors>
<title>Parse fitting and prose fixing: getting a hold on illformedness&amp;quot;</title>
<date>1983</date>
<journal>in American Journal of Computational Linguistics</journal>
<volume>9</volume>
<issue>3</issue>
<pages>147--160</pages>
<marker>Jensen, Heidom, Miller, Ravin, 1983</marker>
<rawString>Jensen, K, Heidom, G E, Miller, L A, and Ravin, Y 1983 &amp;quot;Parse fitting and prose fixing: getting a hold on illformedness&amp;quot; in American Journal of Computational Linguistics 9(3-4): 147-160</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig Johansson</author>
</authors>
<title>Geoffrey Leech and Helen Goodluck</title>
<date>1978</date>
<institution>Department of English, Oslo University</institution>
<marker>Johansson, 1978</marker>
<rawString>Johansson, Stig, Geoffrey Leech and Helen Goodluck 1978 Manual of information to accompany the LancasterOslo/Bergen Corpus of British English, for use with digital computers Department of English, Oslo University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig Johansson</author>
<author>Eric Atwell</author>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
</authors>
<title>The Tagged LOB Corpus Norwegian Computing Centre for the Humanities,</title>
<date>1986</date>
<institution>University of Bergen,</institution>
<marker>Johansson, Atwell, Garside, Leech, 1986</marker>
<rawString>Johansson, Stig, Eric Atwell, Roger Garside, and Geoffrey Leech 1986 The Tagged LOB Corpus Norwegian Computing Centre for the Humanities, University of Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kwasny</author>
<author>Norman Sondheimer</author>
</authors>
<title>Relaxation techniques for parsing grammatically ill-formed input in natural language understanding systems&amp;quot;</title>
<date>1981</date>
<journal>in American Journal of Computational Linguistics</journal>
<volume>7</volume>
<issue>2</issue>
<pages>99--108</pages>
<marker>Kwasny, Sondheimer, 1981</marker>
<rawString>Kwasny, S and Norman Sondheimer 1981 &amp;quot;Relaxation techniques for parsing grammatically ill-formed input in natural language understanding systems&amp;quot; in American Journal of Computational Linguistics 7(2): 99-108</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
</authors>
<title>and Eric Steven Atwell 1983a, &amp;quot;Recent developments in the use of computer corpora in English language research&amp;quot;</title>
<date>1983</date>
<journal>in Transactions of the Philological Society</journal>
<pages>23--40</pages>
<marker>Leech, Garside, 1983</marker>
<rawString>Leech, Geoffrey, Roger Garside, and Eric Steven Atwell 1983a, &amp;quot;Recent developments in the use of computer corpora in English language research&amp;quot; in Transactions of the Philological Society 1983: 23-40.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
</authors>
<title>Eric Steven Atwell 1983b, &amp;quot;The Automatic Grammatical Tagging of the LOB Corpus&amp;quot;</title>
<date>1985</date>
<booktitle>in Newsletter of the International Computer Archive of Modern English (ICAME NEWS) 7: 13-33, Norwegian Computing Centre for the Humanities, Bergen University Marslen-Wilson, W D</booktitle>
<editor>in Fallside, Frank and Woods, William (eds.)</editor>
<marker>Leech, Garside, 1985</marker>
<rawString>Leech, Geoffrey, Roger Garside, and Eric Steven Atwell 1983b, &amp;quot;The Automatic Grammatical Tagging of the LOB Corpus&amp;quot; in Newsletter of the International Computer Archive of Modern English (ICAME NEWS) 7: 13-33, Norwegian Computing Centre for the Humanities, Bergen University Marslen-Wilson, W D 1985 &amp;quot;Aspects of human speech understanding&amp;quot; in Fallside, Frank and Woods, William (eds.) Computer speech processing, Prentice-Hall</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
<author>S Jones</author>
<author>R Daley</author>
</authors>
<title>English lexical studies, Report to OSTI on project OLP/08; Dept of English,</title>
<date>1970</date>
<location>Birmingham University</location>
<marker>Sinclair, Jones, Daley, 1970</marker>
<rawString>Sinclair, John, Jones, S, and Daley, R 1970 English lexical studies, Report to OSTI on project OLP/08; Dept of English, Birmingham University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Veronis</author>
</authors>
<title>Correction of phonographic errors in natural language processing&amp;quot;</title>
<date>1987</date>
<booktitle>ICCH87: Proceedings of the Eighth International Conference on Computers and the Humanities,</booktitle>
<editor>in Oakman, Robert and Pantonial, Barbara (eds.)</editor>
<institution>Department of Computer Science, University of South Carolina</institution>
<marker>Veronis, 1987</marker>
<rawString>Veronis, Jean 1987 &amp;quot;Correction of phonographic errors in natural language processing&amp;quot; in Oakman, Robert and Pantonial, Barbara (eds.) ICCH87: Proceedings of the Eighth International Conference on Computers and the Humanities, Department of Computer Science, University of South Carolina</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>John Black</author>
</authors>
<title>Responding intelligently to unparsable inputs&amp;quot;</title>
<date>1980</date>
<journal>in American Journal of Computational Linguistics</journal>
<volume>6</volume>
<issue>2</issue>
<pages>97--109</pages>
<marker>Weischedel, Black, 1980</marker>
<rawString>Weischedel, Ralph, and John Black 1980 &amp;quot;Responding intelligently to unparsable inputs&amp;quot; in American Journal of Computational Linguistics 6(2) 97-109</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Norman Sondheimer</author>
</authors>
<title>Metarules as a basis for processing ill-formed input&amp;quot;</title>
<date>1983</date>
<journal>in American Journal of Computational Linguistics</journal>
<pages>9--3</pages>
<marker>Weischedel, Sondheimer, 1983</marker>
<rawString>Weischedel, Ralph, and Norman Sondheimer 1983 &amp;quot;Metarules as a basis for processing ill-formed input&amp;quot; in American Journal of Computational Linguistics 9(3-4):161-177</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Yannakoudakis</author>
<author>D Fawthrop</author>
</authors>
<title>The rules of spelling errors&amp;quot; in Information processing and management</title>
<date>1983</date>
<volume>19</volume>
<issue>2</issue>
<pages>87--99</pages>
<marker>Yannakoudakis, Fawthrop, 1983</marker>
<rawString>Yannakoudakis, E J, and Fawthrop, D 1983 &amp;quot;The rules of spelling errors&amp;quot; in Information processing and management 19(2): 87-99</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>