<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.995845">
The Effect of Corpus Size on Case Frame Acquisition
for Discourse Analysis
</title>
<author confidence="0.992392">
Ryohei Sasano
</author>
<affiliation confidence="0.994922">
Graduate School of Informatics,
Kyoto University
</affiliation>
<email confidence="0.996674">
sasano@i.kyoto-u.ac.jp
</email>
<author confidence="0.991069">
Daisuke Kawahara
</author>
<affiliation confidence="0.9524085">
National Institute of Information
and Communications Technology
</affiliation>
<email confidence="0.995435">
dk@nict.go.jp
</email>
<author confidence="0.985836">
Sadao Kurohashi
</author>
<affiliation confidence="0.995245">
Graduate School of Informatics,
Kyoto University
</affiliation>
<email confidence="0.996988">
kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.998591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888666666667">
This paper reports the effect of corpus size on
case frame acquisition for discourse analysis
in Japanese. For this study, we collected a
Japanese corpus consisting of up to 100 bil-
lion words, and constructed case frames from
corpora of six different sizes. Then, we ap-
plied these case frames to syntactic and case
structure analysis, and zero anaphora resolu-
tion. We obtained better results by using case
frames constructed from larger corpora; the
performance was not saturated even with a
corpus size of 100 billion words.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945115384616">
Very large corpora obtained from the Web have
been successfully utilized for many natural lan-
guage processing (NLP) applications, such as prepo-
sitional phrase (PP) attachment, other-anaphora res-
olution, spelling correction, confusable word set dis-
ambiguation and machine translation (Volk, 2001;
Modjeska et al., 2003; Lapata and Keller, 2005; At-
terer and Sch¨utze, 2006; Brants et al., 2007).
Most of the previous work utilized only the sur-
face information of the corpora, such as n-grams,
co-occurrence counts, and simple surface syntax.
This may be because these studies did not require
structured knowledge, and for such studies, the size
of currently available corpora is considered to have
been almost enough. For instance, while Brants et
al. (2007) reported that translation quality continued
to improve with increasing corpus size for training
language models at even size of 2 trillion tokens, the
increase became small at the corpus size of larger
than 30 billion tokens.
However, for more complex NLP tasks, such as
case structure analysis and zero anaphora resolution,
it is necessary to obtain more structured knowledge,
such as semantic case frames, which describe the
cases each predicate has and the types of nouns that
can fill a case slot. Note that case frames offer not
only the knowledge of the relationships between a
predicate and its particular case slot, but also the
knowledge of the relationships among a predicate
and its multiple case slots. To obtain such knowl-
edge, very large corpora seem to be necessary; how-
ever it is still unknown how much corpora would be
required to obtain good coverage.
For examples, Kawahara and Kurohashi pro-
posed a method for constructing wide-coverage case
frames from large corpora (Kawahara and Kuro-
hashi, 2006b), and a model for syntactic and case
structure analysis of Japanese that based upon case
frames (Kawahara and Kurohashi, 2006a). How-
ever, they did not demonstrate whether the coverage
of case frames was wide enough for these tasks and
how dependent the performance of the model was on
the corpus size for case frame construction.
This paper aims to address these questions. We
collect a very large Japanese corpus consisting of
about 100 billion words, or 1.6 billion unique sen-
tences from the Web. Subsets of the corpus are ran-
domly selected to obtain corpora of different sizes
ranging from 1.6 million to 1.6 billion sentences.
We construct case frames from each corpus and ap-
ply them to syntactic and case structure analysis, and
zero anaphora resolution, in order to investigate the
</bodyText>
<page confidence="0.968102">
521
</page>
<note confidence="0.891003">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521–529,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.982647">
relationships between the corpus size and the perfor-
mance of these analyses.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999466">
Many NLP tasks have successfully utilized very
large corpora, most of which were acquired from
the Web (Kilgarriff and Grefenstette, 2003). Volk
(2001) proposed a method for resolving PP attach-
ment ambiguities based upon Web data. Modjeska
et al. (2003) used the Web for resolving nominal
anaphora. Lapata and Keller (2005) investigated the
performance of web-based models for a wide range
of NLP tasks, such as MT candidate selection, ar-
ticle generation, and countability detection. Nakov
and Hearst (2008) solved relational similarity prob-
lems using the Web as a corpus.
With respect to the effect of corpus size on NLP
tasks, Banko and Brill (2001a) showed that for
content sensitive spelling correction, increasing the
training data size improved the accuracy. Atterer
and Sch¨utze (2006) investigated the effect of cor-
pus size in combining supervised and unsupervised
learning for two types of attachment decision; they
found that the combined system only improved the
performance of the parser for small training sets.
Brants et al. (2007) varied the amount of language
model training data from 13 million to 2 trillion to-
kens and applied these models to machine transla-
tion systems. They reported that translation qual-
ity continued to improve with increasing corpus size
for training language models at even size of 2 tril-
lion tokens. Suzuki and Isozaki (2008) provided ev-
idence that the use of more unlabeled data in semi-
supervised learning could improve the performance
of NLP tasks, such as POS tagging, syntactic chunk-
ing, and named entities recognition.
There are several methods to extract useful infor-
mation from very large corpora. Search engines,
such as Google and Altavista, are often used to ob-
tain Web counts (e.g. (Nakov and Hearst, 2005;
Gledson and Keane, 2008)). However, search en-
gines are not designed for NLP research and the re-
ported hit counts are subject to uncontrolled vari-
ations and approximations. Therefore, several re-
searchers have collected corpora from the Web by
themselves. For English, Banko and Brill (2001b)
collected a corpus with 1 billion words from vari-
ety of English texts. Liu and Curran (2006) created
a Web corpus for English that contained 10 billion
words and showed that for content-sensitive spelling
correction the Web corpus results were better than
using a search engine. Halacsy et al. (2004) created
a corpus with 1 billion words for Hungarian from
the Web by downloading 18 million pages. Others
utilize publicly available corpus such as the North
American News Corpus (NANC) and the Gigaword
Corpus (Graff, 2003). For instance, McClosky et al.
(2006) proposed a simple method of self-training a
two phase parser-reranker system using NANC.
As for Japanese, Kawahara and Kurohashi
(2006b) collected 23 million pages and created a
corpus with approximately 20 billion words. Google
released Japanese n-gram constructed from 20 bil-
lion Japanese sentences (Kudo and Kazawa, 2007).
Several news wires are publicly available consisting
of tens of million sentences. Kotonoha project is
now constructing a balanced corpus of the present-
day written Japanese consisting of 50 million words
(Maekawa, 2006).
</bodyText>
<sectionHeader confidence="0.713915" genericHeader="method">
3 Construction of Case Frames
</sectionHeader>
<bodyText confidence="0.9999655">
Case frames describe the cases each predicate has
and what nouns can fill the case slots. In this study,
case frames we construct case frames from raw cor-
pora by using the method described in (Kawahara
and Kurohashi, 2006b). This section illustrates the
methodology for constructing case frames.
</bodyText>
<subsectionHeader confidence="0.9996">
3.1 Basic Method
</subsectionHeader>
<bodyText confidence="0.999905642857143">
After parsing a large corpus by a Japanese parser
KNP1, we construct case frames from modifier-head
examples in the resulting parses. The problems for
case frame construction are syntactic and semantic
ambiguities. In other words, the resulting parses in-
evitably contain errors and predicate senses are in-
trinsically ambiguous. To cope with these problems,
we construct case frames from reliable modifier-
head examples.
First, we extract modifier-head examples that had
no syntactic ambiguity, and assemble them by cou-
pling a predicate and its closest case component.
That is, we assemble the examples not by predi-
cates, such as tsumu (load/accumulate), but by cou-
</bodyText>
<footnote confidence="0.987873">
1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
</footnote>
<page confidence="0.997799">
522
</page>
<tableCaption confidence="0.999661">
Table 1: Examples of Constructed Case Frames.
</tableCaption>
<bodyText confidence="0.997546714285714">
Case slot Examples Generalized examples with rate
tsumu (1) ga (nominative) he, driver, friend, · · · [CT:PERSON]:0.45, [NE:PERSON]:0.08, · · ·
(load) wo (accusative) baggage, luggage, hay, · · · [CT:ARTIFACT]:0.31, ···
ni (dative) car, truck, vessel, seat, · · · [CT:VEHICLE]:0.32, · · ·
tsumu (2) ga (nominative) player, children, party, · · · [CT:PERSON]:0.40, [NE:PERSON]:0.12, · · ·
(accumulate) wo (accusative) experience, knowledge, · · · [CT:ABSTRACT]:0.47, · · ·
... ... ...
hanbai (1) ga (nominative) company, Microsoft, firm, · · · [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, ···
(sell) wo (accusative) goods, product, ticket, · · · [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, · · ·
ni (dative) customer, company, user, · · · [CT:PERSON]:0.28, · · ·
de (locative) shop, bookstore, site · · · [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, · · ·
... ... ...
ples, such as nimotsu-wo tsumu (load baggage) and
keiken-wo tsumu (accumulate experience). Such
couples are considered to play an important role
for constituting sentence meanings. We call the as-
sembled examples as basic case frames. In order
to remove inappropriate examples, we introduce a
threshold α and use only examples that appeared no
less than α times in the corpora.
Then, we cluster the basic case frames to merge
similar case frames. For example, since nimotsu-
wo tsumu (load baggage) and busshi-wo tsumu (load
supplies) are similar, they are merged. The similar-
ity is measured by using a Japanese thesaurus (The
National Language Institute for Japanese Language,
2004). Table 1 shows examples of constructed case
frames.
</bodyText>
<subsectionHeader confidence="0.999935">
3.2 Generalization of Examples
</subsectionHeader>
<bodyText confidence="0.9999575">
When we use hand-crafted case frames, the data
sparseness problem is serious; by using case frames
automatically constructed from a large corpus, it was
alleviated to some extent but not eliminated. For in-
stance, there are thousands of named entities (NEs)
that cannot be covered intrinsically. To deal with
this problem, we generalize the examples of the case
slots. Kawahara and Kurohashi also generalized ex-
amples but only for a few types. In this study, we
generalize case slot examples based upon common
noun categories and NE classes.
First, we generalize the examples based upon the
categories that tagged by the Japanese morpholog-
ical analyzer JUMAN2. In JUMAN, about 20 cat-
egories are defined and tagged to common nouns.
For example, ringo (apple), inu (dog) and byoin
</bodyText>
<footnote confidence="0.742231">
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
</footnote>
<tableCaption confidence="0.995378">
Table 2: Definition of NE in IREX.
</tableCaption>
<table confidence="0.973334444444444">
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o’clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
</table>
<bodyText confidence="0.9994845625">
(hospital) are tagged as FOOD, ANIMAL and FA-
CILITY, respectively. For each category, we calcu-
late the ratio of the categorized example among all
case slot examples, and add it to the case slot (e.g.
[CT:FOOD]:0.07).
We also generalize the examples based upon NE
classes. We use a common standard NE defini-
tion for Japanese provided by the IREX (1999).
We first recognize NEs in the source corpus by
using an NE recognizer (Sasano and Kurohashi,
2008); and then construct case frames from the NE-
recognized corpus. Similar to the categories, for
each NE class, we calculate the NE ratio among all
the case slot examples, and add it to the case slot
(e.g. [NE:PERSON]:0.12). The generalized exam-
ples are also included in Table 1.
</bodyText>
<sectionHeader confidence="0.991342" genericHeader="method">
4 Discourse Analysis with Case Frames
</sectionHeader>
<bodyText confidence="0.999656">
In order to investigate the effect of corpus size
on complex NLP tasks, we apply the constructed
cases frames to an integrated probabilistic model
for Japanese syntactic and case structure analysis
(Kawahara and Kurohashi, 2006a) and a probabilis-
tic model for Japanese zero anaphora resolution
(Sasano et al., 2008). In this section, we briefly de-
scribe these models.
</bodyText>
<page confidence="0.99144">
523
</page>
<subsectionHeader confidence="0.9428745">
4.1 Model for Syntactic and Case Structure
Analysis
</subsectionHeader>
<bodyText confidence="0.999965636363636">
Kawahara and Kurohashi (2006a) proposed an in-
tegrated probabilistic model for Japanese syntactic
and case structure analysis based upon case frames.
Case structure analysis recognizes predicate argu-
ment structures. Their model gives a probability to
each possible syntactic structure T and case struc-
ture L of the input sentence 5, and outputs the syn-
tactic and case structure that have the highest proba-
bility. That is to say, the system selects the syntactic
structure Tbest and the case structure Lbest that max-
imize the probability P(T, L|5):
</bodyText>
<equation confidence="0.9944015">
(Tbest, Lbest) = argmax P(T, L|5)
(T,L)
= argmax P(T, L, 5) (1)
(T,L)
</equation>
<bodyText confidence="0.997681">
The last equation is derived because P(5) is con-
stant. P(T, L, 5) is defined as the product of a prob-
ability for generating a clause Ci as follows:
</bodyText>
<equation confidence="0.991356">
P(T, L, 5) = ∏ P(Ci|bhi) (2)
i=1..n
</equation>
<bodyText confidence="0.999441733333333">
where n is the number of clauses in 5, and bhi is
Ci’s modifying bunsetsu3. P(Ci|bhi) is approxi-
mately decomposed into the product of several gen-
erative probabilities such as P(A(sj) = 1|CFl, sj)
and P(nj|CFl, sj, A(sj) = 1), where the function
A(sj) returns 1 if a case slot sj is filled with an input
case component; otherwise 0. P(A(sj)=1|CFl, sj)
denotes the probability that the case slot sj is filled
with an input case component, and is estimated from
resultant case structure analysis of a large raw cor-
pus. P(nj|CFl, sj, A(sj) = 1) denotes the proba-
bility of generating a content part nj from a filled
case slot sj in a case frame CFl, and is calculated
by using case frames. For details see (Kawahara and
Kurohashi, 2006a).
</bodyText>
<subsectionHeader confidence="0.963667">
4.2 Model for Zero Anaphora Resolution
</subsectionHeader>
<bodyText confidence="0.995583666666667">
Anaphora resolution is one of the most important
techniques for discourse analysis. In English, overt
pronouns such as she and definite noun phrases such
as the company are anaphors that refer to preced-
ing entities (antecedents). On the other hand, in
3In Japanese, bunsetsu is a basic unit of dependency, con-
sisting of one or more content words and the following zero or
more function words. It corresponds to a base phrase in English.
Japanese, anaphors are often omitted; these omis-
sions are called zero pronouns. Zero anaphora res-
olution is the integrated task of zero pronoun detec-
tion and zero pronoun resolution.
We proposed a probabilistic model for Japanese
zero anaphora resolution based upon case frames
(Sasano et al., 2008). This model first resolves
coreference and identifies discourse entities; then
gives a probability to each possible case frame CF
and case assignment CA when target predicate v,
input case components ICC and existing discourse
entities ENT are given, and outputs the case frame
and case assignment that have the highest probabil-
ity. That is to say, this model selects the case frame
CFbest and the case assignment CAbest that maxi-
mize the probability P(CF, CA|v, ICC, ENT):
</bodyText>
<equation confidence="0.992449333333333">
(CFbest, CAbest)
= argmax P(CF, CA|v, ICC, ENT) (3)
(CF,CA)
</equation>
<bodyText confidence="0.960812285714286">
P(CF, CA|v, ICC, ENT) is approximately de-
composed into the product of several probabilities.
Case frames are used for calculating P(nj|CFl,
sj, A(sj) = 1), the probability of generating a con-
tent part nj from a case slot sj in a case frame
CFl, and P(nj|CFl, sj, A&apos;(sj)=1), the probability
of generating a content part nj of a zero pronoun,
where the function A&apos;(sj) returns 1 if a case slot sj
is filled with an antecedent of a zero pronoun; other-
wise 0.
P(nj|CFl, sj, A&apos;(sj)=1) is similar to P(nj|CFl,
sj, A(sj)=1) and estimated from the frequencies of
case slot examples in case frames. However, while
A&apos;(sj)=1 means sj is not filled with an overt argu-
ment but filled with an antecedent of zero pronoun,
case frames are constructed from overt predicate ar-
gument pairs. Therefore, the content part nj is often
not included in the case slot examples. To cope with
this problem, this model also utilizes generalized ex-
amples to estimate P(nj|CFl, sj, A(sj) = 1). For
details see (Sasano et al., 2008).
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999902">
5.1 Construction of Case Frames
</subsectionHeader>
<bodyText confidence="0.999614333333333">
In order to investigate the effect of corpus size,
we constructed case frames from corpora of dif-
ferent sizes. We first collected Japanese sentences
</bodyText>
<page confidence="0.998249">
524
</page>
<tableCaption confidence="0.999409">
Table 4: Statistics of the Constructed Case Frames.
</tableCaption>
<table confidence="0.976275">
Corpus size (sentences) 1.6M 6.3M 25M 100M 400M 1.6G
# of predicate 2460 6134 13532 27226 42739 65679
(type) verb 2039 4895 10183 19191 28523 41732
adjective 154 326 617 1120 1641 2318
noun with copula 267 913 2732 6915 12575 21629
</table>
<note confidence="0.588131166666667">
average # of case frames for a predicate 15.9 12.2 13.3 16.1 20.5 25.3
average # of case slots for a case frame 2.95 3.44 3.88 4.21 4.69 5.08
average # of examples for a case slot 4.89 10.2 19.5 34.0 67.2 137.6
average # of unique examples for a case slot 1.19 1.85 3.06 4.42 6.81 9.64
average # of generalized examples for a case slot 0.14 0.24 0.37 0.49 0.67 0.84
File size(byte) 8.9M 20M 56M 147M 369M 928M
</note>
<tableCaption confidence="0.981146">
Table 3: Corpus Sizes and Thresholds.
</tableCaption>
<table confidence="0.99809175">
Corpus size for case 1.6M 6.3M 25M 100M 400M 1.6G
frame construction
(sentences)
Threshold α 2 3 4 5 7 10
introduced in Sec. 3.1 1.6M 3.2M 6.3M 13M 25M 50M
Corpus size to
estimate generative
probability (sentences)
</table>
<bodyText confidence="0.999606111111111">
from the Web using the method proposed by Kawa-
hara and Kurohashi (2006b). We acquired approx-
imately 6 billion Japanese sentences consisting of
approximately 100 billion words from 100 million
Japanese web pages. After discarding duplicate sen-
tences, which may have been extracted from mirror
sites, we acquired a corpus comprising of 1.6 bil-
lion (1.6G) unique Japanese sentences consisting of
approximately 25 billion words. The average num-
ber of characters and words in each sentence was
28.3, 15.6, respectively. Then we randomly selected
subsets of the corpus for five different sizes; 1.6M,
6.3M, 25M, 100M, and 400M sentences to obtain
corpora of different sizes.
We constructed case frames from each corpus. We
employed JUMAN and KNP to parse each corpus.
We changed the threshold α introduced in Section
3.1 depending upon the size of the corpus as shown
in Table 3. Completing the case frame construc-
tion took about two weeks using 600 CPUs. Ta-
ble 4 shows the statistics for the constructed case
frames. The number of predicates, the average num-
ber of examples and unique examples for a case slot,
and whole file size were confirmed to be heavily de-
pendent upon the corpus size. However, the average
number of case frames for a predicate and case slots
for a case frame did not.
</bodyText>
<subsectionHeader confidence="0.998216">
5.2 Coverage of Constructed Case Frames
5.2.1 Setting
</subsectionHeader>
<bodyText confidence="0.999964473684211">
In order to investigate the coverage of the resul-
tant case frames, we used a syntactic relation, case
structure, and anaphoric relation annotated corpus
consisting of 186 web documents (979 sentences).
This corpus was manually annotated using the same
criteria as Kawahara et al. (2004). There were 2,390
annotated relationships between predicates and their
direct (not omitted) case components and 837 zero
anaphoric relations in the corpus.
We used two evaluation metrics depending upon
whether the target case component was omitted or
not. For the overt case component of a predicate, we
judged the target component was covered by case
frames if the target component itself was included in
the examples for one of the corresponding case slots
of the case frame. For the omitted case component,
we checked not only the target component itself but
also all mentions that refer to the same entity as the
target component.
</bodyText>
<subsectionHeader confidence="0.976037">
5.2.2 Coverage of Case Frames
</subsectionHeader>
<bodyText confidence="0.998369307692308">
Figure 1 shows the coverage of case frames for
the overt argument, which would have tight relations
with case structure analysis. The lower line shows
the coverage without considering generalized exam-
ples, the middle line shows the coverage considering
generalized NE examples, and the upper line shows
the coverage considering all generalized examples.
Figure 2 shows the coverage of case frames for
the omitted argument, which would have tight rela-
tions with zero anaphora resolution. The upper line
shows the coverage considering all generalized ex-
amples, which is considered to be the upper bound
of performance for the zero anaphora resolution sys-
</bodyText>
<page confidence="0.988372">
525
</page>
<figure confidence="0.838228">
1M 10M 100M 1000M
Corpus Size (Number of Sentences)
1M 10M 100M 1000M
Corpus Size (Number of Sentences)
</figure>
<figureCaption confidence="0.99989">
Figure 2: Coverage of CF (omitted argument).
</figureCaption>
<bodyText confidence="0.997690291666666">
tem described in Section 4.2. Comparing with Fig-
ure 1, we found two characteristics. First, the lower
and middle lines of Figure 2 were located lower than
the corresponding lines in Figure 1. This would re-
flect that some frequently omitted case components
are not described in the case frames because the case
frames were constructed from only overt predicate
argument pairs. Secondly, the effect of generalized
NE examples was more evident for the omitted ar-
gument reflecting the important role of NEs in zero
anaphora resolution.
Both figures show that the coverage was improved
by using larger corpora and there was no saturation
even when the largest corpus of 1.6 billion sentences
was used. When the largest corpus and all general-
ized examples were used, the case frames achieved a
coverage of almost 90% for both the overt and omit-
ted argument.
Figure 3 shows the coverage of case frames for
each predicate type, which was calculated for both
overt and omitted argument considering all general-
ized examples. The case frames for verbs achieved
a coverage of 93.0%. There were 189 predicate-
argument pairs that were not included case frames;
</bodyText>
<figure confidence="0.4138725">
1M 10M 100M 1000M
Corpus Size (Number of Sentences)
</figure>
<figureCaption confidence="0.999752">
Figure 3: Coverage of CF for Each Predicate Type.
</figureCaption>
<bodyText confidence="0.999903933333333">
11 pairs of them were due to lack of the case frame
of target predicate itself, and the others were due
to lack of the corresponding example. For adjec-
tive, the coverage was 78.8%. The main cause of
the lower coverage would be that the predicate argu-
ment relations concerning adjectives that were used
in restrictive manner, such as “oishii sushi” (deli-
cious sushi), were not used for case frame construc-
tion, although such relations were also the target of
the coverage evaluation. For noun with copula, the
coverage was only 54.5%. However, most predicate
argument relations concerning nouns with copula
were easily recognized from syntactic preference,
and thus the low coverage would not quite affect the
performance of discourse analysis.
</bodyText>
<subsectionHeader confidence="0.9991365">
5.3 Syntactic and Case Structure Analysis
5.3.1 Accuracy of Syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.968011722222222">
We investigated the effect of corpus size for syn-
tactic analysis described in Section 4.1. We used
hand-annotated 759 web sentences, which was used
by Kawahara and Kurohashi (2007). We evaluated
the resultant syntactic structures with regard to de-
pendency accuracy, the proportion of correct depen-
dencies out of all dependencies4.
Figure 4 shows the accuracy of syntactic struc-
tures. We conducted these experiments with case
frames constructed from corpora of different sizes.
We also changed the corpus size to estimate gen-
erative probability of a case slot in Section 4.1 de-
pending upon the size of the corpus for case frame
construction as shown in Table 3. Figure 4 also in-
4Note that Kawahara and Kurohashi (2007) exclude the de-
pendency between the last two bunsetsu, since Japanese is head-
final and thus the second last bunsetsu unambiguously depends
on the last bunsetsu.
</bodyText>
<figure confidence="0.983569333333333">
+NE,CT match
+ NE match
exact match
0.683
0.649
0.897
</figure>
<figureCaption confidence="0.997698">
Figure 1: Coverage of CF (overt argument).
</figureCaption>
<figure confidence="0.974765882352941">
Coverage
0.8
0.6
0.4
0.2
0.0
1.0
+NE,CT match
+ NE match
exact match
0.608
0.472
0.892
adjective
verb
noun with copula
0.930
0.788
0.545
Coverage 1.0
0.8
0.6
0.4
0.2
0.0
Coverage 1.0
0.8
0.6
0.4
0.2
0.0
526
1M 10M 100M 1000M
Corpus Size (Number of Sentences)
</figure>
<figureCaption confidence="0.988158">
Figure 4: Accuracy of Syntactic Analysis. (McNemar’s
test results are also shown under each data point.)
</figureCaption>
<bodyText confidence="0.999746730769231">
cludes McNemar’s test results. For instance, the dif-
ference between the corpus size of 1.6G and 100M
sentences is significant at the 90% level (p = 0.1),
but not significant at the 99% level (p = 0.01).
In Figure 4, ‘w/o case frames’ shows the accu-
racy of the rule-based syntactic parser KNP that does
not use case frames. Since the model described
in Section 4.1 assumes the existence of reasonable
case frames, when we used case frames constructed
from very small corpus, such as 1.6M and 6.3M sen-
tences, the accuracy was lower than that of the rule-
based syntactic parser. Moreover, when we tested
the model described in Section 4.1 without any case
frames, the accuracy was 0.885.
We confirmed that better performance was ob-
tained by using case frames constructed from larger
corpora, and the accuracy of 0.8945 was achieved
by using the case frames constructed from 1.6G sen-
tences. However the effect of the corpus size was
limited. This is because there are various causes
of dependency error and the case frame sparseness
problem is not serious for syntactic analysis.
We considered that generalized examples can
benefit for the accuracy of syntactic analysis, and
tried several models that utilize these examples.
However, we cannot confirm any improvement.
</bodyText>
<subsectionHeader confidence="0.994723">
5.3.2 Accuracy of Case Structure Analysis
</subsectionHeader>
<bodyText confidence="0.99992175">
We conducted case structure analysis on 215 web
sentences in order to investigate the effect of cor-
pus size for case structure analysis. The case mark-
ers of topic marking phrases and clausal modifiers
</bodyText>
<footnote confidence="0.916744">
5It corresponds to 0.877 in Kawahara and Kurohashi’s
(2007) evaluation metrics.
</footnote>
<table confidence="0.2836265">
1M 10M 100M 1000M
Corpus Size (Number of Sentences)
</table>
<figureCaption confidence="0.980772">
Figure 5: Accuracy of Case Structure Analysis.
</figureCaption>
<tableCaption confidence="0.9675925">
Table 5: Corpus Sizes for Case Frame Construction and
Time for Syntactic and Case Structure Analysis.
</tableCaption>
<table confidence="0.9420665">
Corpus size 1.6M 6.3M 25M 100M 400M 1.6G
Time (sec.) 850 1244 1833 2696 3783 5553
</table>
<bodyText confidence="0.9997674">
were evaluated by comparing them with the gold
standard in the corpus. Figure 5 shows the experi-
mental results. We confirmed that the accuracy of
case structure analysis strongly depends on corpus
size for case frame construction.
</bodyText>
<subsectionHeader confidence="0.970627">
5.3.3 Analysis Speed
</subsectionHeader>
<bodyText confidence="0.99999">
Table 5 shows the time for analyzing syntactic
and case structure of 759 web sentences. Although
the time for analysis became longer by using case
frames constructed from a larger corpus, the growth
rate was smaller than the growth rate of the size for
case frames described in Table 4.
Since there is enough increase in accuracy of case
structure analysis, we can say that case frames con-
structed larger corpora are desirable for case struc-
ture analysis.
</bodyText>
<subsectionHeader confidence="0.991336">
5.4 Zero Anaphora Resolution
5.4.1 Accuracy of Zero Anaphora Resolution
</subsectionHeader>
<bodyText confidence="0.999457181818182">
We used an anaphoric relation annotated corpus
consisting of 186 web documents (979 sentences)
to evaluate zero anaphora resolution. We used first
51 documents for test and used the other 135 doc-
uments for calculating several probabilities. In the
51 test documents, 233 zero anaphora relations were
annotated between one of the mentions of the an-
tecedent and corresponding predicate that had zero
pronoun.
In order to concentrate on evaluation for zero
anaphora resolution, we used the correct mor-
</bodyText>
<figure confidence="0.993946523809524">
1.6M
with case frames
w/o case frames
6.3M
25M
p &lt; 0.1
6.3M
1.6M
p &lt; 0.1
6.3M
1.6M
100M
p &lt; 0.1
25M
p &lt; 0.01
6.3M
1.6M
400M
0.894
1.6G
p &lt; 0.1
100M
25M
p &lt; 0.01
6.3M
1.6M
0.784
Accuracy 0.900
0.800
0.700
0.600
0.500
0.400
Accuracy 0.896
0.894
0.892
0.890
0.888
0.886
527
1M 10M 100M 1000M
Corpus Size (Number of Sentences)
</figure>
<figureCaption confidence="0.999941">
Figure 6: F-measure of Zero Anaphora Resolution.
</figureCaption>
<bodyText confidence="0.977981777777778">
phemes, named entities, syntactic structures and
coreference relations that were manually annotated.
Since correct coreference relations were given, the
number of created entities was the same between the
gold standard and the system output because zero
anaphora resolution did not create new entities.
The experimental results are shown in Figure 6, in
which F-measure was calculated by:
# of correctly recognized zero anaphora
</bodyText>
<equation confidence="0.783302333333333">
R = # of zero anaphora annotated in corpus ,
# of correctly recognized zero anaphora
P=
# of system outputted zero anaphora ,
2
F= 1/R + 1/P .
</equation>
<bodyText confidence="0.999991428571429">
The upper line shows the performance using all
generalized examples, the middle line shows the
performance using only generalized NEs, and the
lower line shows the performance without using
any generalized examples. While generalized cat-
egories much improved the F-measure, generalized
NEs contributed little. This tendency is similar to
that of coverage of case frames for omitted argument
shown in Figure 2. Unlike syntactic and case struc-
ture analysis, the performance for the zero anaphora
resolution is quite low when using case frames con-
structed from small corpora, and we can say case
frames constructed from larger corpora are essential
for zero anaphora resolution.
</bodyText>
<subsectionHeader confidence="0.884775">
5.4.2 Analysis Speed
</subsectionHeader>
<bodyText confidence="0.997447">
Table 6 shows the time for resolving zero
anaphora in 51 web documents consisting of 278
sentences. The time for analysis became longer by
using case frames constructed from larger corpora,
</bodyText>
<tableCaption confidence="0.9247275">
Table 6: Corpus Sizes for Case Frame Construction and
Time for Zero Anaphora Resolution.
</tableCaption>
<table confidence="0.943814">
Corpus size 1.6M 6.3M 25M 100M 400M 1.6G
Time (sec.) 538 545 835 1040 1646 2219
</table>
<bodyText confidence="0.974466">
which tendency is similar to the growth of the time
for analyzing syntactic and case structure.
</bodyText>
<subsectionHeader confidence="0.796713">
5.5 Discussion
</subsectionHeader>
<bodyText confidence="0.9999662">
Experimental results of both case structure analy-
sis and zero anaphora resolution show the effective-
ness of a larger corpus in case frame acquisition for
Japanese discourse analysis. Up to the corpus size
of 1.6 billion sentences, or 100 billion words, these
experimental results still show a steady increase in
performance. That is, we can say that the corpus
size of 1.6 billion sentences is not enough to obtain
case frames of sufficient coverage.
These results suggest that increasing corpus size
is more essential for acquiring structured knowledge
than for acquiring unstructured statistics of a corpus,
such as n-grams, and co-occurrence counts; and for
complex NLP tasks such as case structure analysis
and zero anaphora resolution, the currently available
corpus size is not sufficient.
Therefore, to construct more wide-coverage case
frames by using a larger corpus and reveal how much
corpora would be required to obtain sufficient cov-
erage is considered as future work.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999902588235294">
This paper has reported the effect of corpus size
on case frame acquisition for syntactic and case
structure analysis, and zero anaphora resolution in
Japanese. We constructed case frames from cor-
pora of six different sizes ranging from 1.6 million
to 1.6 billion sentences; and then applied these case
frames to Japanese syntactic and case structure anal-
ysis, and zero anaphora resolution. Experimental re-
sults showed better results were obtained using case
frames constructed from larger corpora, and the per-
formance showed no saturation even when the cor-
pus size was 1.6 billion sentences.
The findings suggest that increasing corpus size
is more essential for acquiring structured knowledge
than for acquiring surface statistics of a corpus; and
for complex NLP tasks the currently available cor-
pus size is not sufficient.
</bodyText>
<figure confidence="0.995308916666667">
+NE,CT match
+ NE match
exact match
0.417
0.330
0.313
F-measure 0.50
0.40
0.30
0.20
0.10
0.00
</figure>
<page confidence="0.986236">
528
</page>
<sectionHeader confidence="0.997583" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999931617977528">
Michaela Atterer and Hinrich Sch¨utze. 2006. The ef-
fect of corpus size in combining supervised and un-
supervised training for disambiguation. In Proc. of
COLING-ACL’06, pages 25–32.
Michele Banko and Eric Brill. 2001a. Mitigating the
paucity-of-data problem: Exploring the effect of train-
ing corpus size on classifier performance for natural
language processing. In Proc. of HLT’01.
Michele Banko and Eric Brill. 2001b. Scaling to very
very large corpora for natural language disambigua-
tion. In Proc. of ACL’01, pages 26–33.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proc. of EMNLP-CoNLL’07,
pages 858–867.
Ann Gledson and John Keane. 2008. Using web-search
results to measure word-group similarity. In Proc. of
COLING’08, pages 281–288.
David Graff. 2003. English Gigaword. Technical Report
LDC2003T05, Linguistic Data Consortium, Philadel-
phia, PA USA.
Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras
Rung, Istvan Szakadat, and Vikto Tron. 2004. Creat-
ing open language resources for Hungarian. In Proc.
of LREC’04, pages 203–210.
IREX Committee, editor. 1999. Proc. of the IREX Work-
shop.
Daisuke Kawahara and Sadao Kurohashi. 2006a. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proc. of HLT-
NAACL’06, pages 176–183.
Daisuke Kawahara and Sadao Kurohashi. 2006b.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC’06, pages
1344–1347.
Daisuke Kawahara and Sadao Kurohashi. 2007.
Probabilistic coordination disambiguation in a fully-
lexicalized Japanese parser. In Proc. of EMNLP-
CoNLL’07, pages 306–314.
Daisuke Kawahara, Ryohei Sasano, and Sadao Kuro-
hashi. 2004. Toward text understanding: Integrat-
ing relevance-tagged corpora and automatically con-
structed case frames. In Proc. of LREC’04, pages
1833–1836.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistic, 29(3):333–347.
Taku Kudo and Hideto Kazawa. 2007. Web Japanese N-
gram version 1, published by Gengo Shigen Kyokai.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2:1:1–31.
Vinci Liu and James R. Curran. 2006. Web text corpus
for natural language processing. In Proc. of EACL’06,
pages 233–240.
Kikuo Maekawa. 2006. Kotonoha, the corpus develop-
ment project of the National Institute for Japanese lan-
guage. In Proc. of the 13th NIJL International Sympo-
sium, pages 55–62.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proc. of
HLT-NAACL’06, pages 152–159.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proc. of EMNLP-2003,
pages 176–183.
Preslav Nakov and Marti Hearst. 2005. A study of using
search engine page hits as a proxy for n-gram frequen-
cies. In Proc. of RANLP’05.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. of ACL-HLT’08, pages 452–460.
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural lan-
guage processing. In Proc. of IJCNLP’08, pages 607–
612.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proc. of
COLING’08, pages 769–776.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL-HLT’08,
pages 665–673.
The National Language Institute for Japanese Language.
2004. Bunruigoihyo. Dainippon Tosho, (In Japanese).
Martin Volk. 2001. Exploiting the WWW as a corpus
to resolve PP attachment ambiguities. In Proc. of the
Corpus Linguistics, pages 601–606.
</reference>
<page confidence="0.998591">
529
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.020810">
<title confidence="0.9971875">The Effect of Corpus Size on Case Frame for Discourse Analysis</title>
<author confidence="0.33838">Ryohei</author>
<affiliation confidence="0.31789">Graduate School of</affiliation>
<abstract confidence="0.630032304347826">Kyoto sasano@i.kyoto-u.ac.jp Daisuke National Institute of and Communications dk@nict.go.jp Sadao Graduate School of Kyoto kuro@i.kyoto-u.ac.jp Abstract This paper reports the effect of corpus size on case frame acquisition for discourse analysis in Japanese. For this study, we collected a Japanese corpus consisting of up to 100 billion words, and constructed case frames from corpora of six different sizes. Then, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michaela Atterer</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>The effect of corpus size in combining supervised and unsupervised training for disambiguation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL’06,</booktitle>
<pages>25--32</pages>
<marker>Atterer, Sch¨utze, 2006</marker>
<rawString>Michaela Atterer and Hinrich Sch¨utze. 2006. The effect of corpus size in combining supervised and unsupervised training for disambiguation. In Proc. of COLING-ACL’06, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Mitigating the paucity-of-data problem: Exploring the effect of training corpus size on classifier performance for natural language processing.</title>
<date>2001</date>
<booktitle>In Proc. of HLT’01.</booktitle>
<contexts>
<context position="4393" citStr="Banko and Brill (2001" startWordPosition="688" endWordPosition="691">lly utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve </context>
<context position="5791" citStr="Banko and Brill (2001" startWordPosition="912" endWordPosition="915">misupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-tr</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001a. Mitigating the paucity-of-data problem: Exploring the effect of training corpus size on classifier performance for natural language processing. In Proc. of HLT’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proc. of ACL’01,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="4393" citStr="Banko and Brill (2001" startWordPosition="688" endWordPosition="691">lly utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve </context>
<context position="5791" citStr="Banko and Brill (2001" startWordPosition="912" endWordPosition="915">misupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-tr</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001b. Scaling to very very large corpora for natural language disambiguation. In Proc. of ACL’01, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL’07,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="1284" citStr="Brants et al., 2007" startWordPosition="184" endWordPosition="187">e structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural language processing (NLP) applications, such as prepositional phrase (PP) attachment, other-anaphora resolution, spelling correction, confusable word set disambiguation and machine translation (Volk, 2001; Modjeska et al., 2003; Lapata and Keller, 2005; Atterer and Sch¨utze, 2006; Brants et al., 2007). Most of the previous work utilized only the surface information of the corpora, such as n-grams, co-occurrence counts, and simple surface syntax. This may be because these studies did not require structured knowledge, and for such studies, the size of currently available corpora is considered to have been almost enough. For instance, while Brants et al. (2007) reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens, the increase became small at the corpus size of larger than 30 billion tokens. However, </context>
<context position="4788" citStr="Brants et al. (2007)" startWordPosition="747" endWordPosition="750">selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens. Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semisupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proc. of EMNLP-CoNLL’07, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Gledson</author>
<author>John Keane</author>
</authors>
<title>Using web-search results to measure word-group similarity.</title>
<date>2008</date>
<booktitle>In Proc. of COLING’08,</booktitle>
<pages>281--288</pages>
<contexts>
<context position="5527" citStr="Gledson and Keane, 2008" startWordPosition="870" endWordPosition="873"> machine translation systems. They reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens. Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semisupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion </context>
</contexts>
<marker>Gledson, Keane, 2008</marker>
<rawString>Ann Gledson and John Keane. 2008. Using web-search results to measure word-group similarity. In Proc. of COLING’08, pages 281–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English Gigaword.</title>
<date>2003</date>
<tech>Technical Report LDC2003T05,</tech>
<institution>Linguistic Data Consortium,</institution>
<location>Philadelphia, PA USA.</location>
<contexts>
<context position="6317" citStr="Graff, 2003" startWordPosition="1000" endWordPosition="1001"> have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news wires are publicly available consisting of tens of million sentences. Kotonoha project is now constructing a balanced corpus of the presentday written Japanese consisting of 50 million words (Maekawa, 2006). 3 Construction</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English Gigaword. Technical Report LDC2003T05, Linguistic Data Consortium, Philadelphia, PA USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Halacsy</author>
<author>Andras Kornai</author>
<author>Laszlo Nemeth</author>
<author>Andras Rung</author>
<author>Istvan Szakadat</author>
<author>Vikto Tron</author>
</authors>
<title>Creating open language resources for Hungarian.</title>
<date>2004</date>
<booktitle>In Proc. of LREC’04,</booktitle>
<pages>203--210</pages>
<contexts>
<context position="6094" citStr="Halacsy et al. (2004)" startWordPosition="962" endWordPosition="965">e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news</context>
</contexts>
<marker>Halacsy, Kornai, Nemeth, Rung, Szakadat, Tron, 2004</marker>
<rawString>Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras Rung, Istvan Szakadat, and Vikto Tron. 2004. Creating open language resources for Hungarian. In Proc. of LREC’04, pages 203–210.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Proc. of the IREX Workshop.</booktitle>
<editor>IREX Committee, editor.</editor>
<contexts>
<context position="11088" citStr="(1999)" startWordPosition="1736" endWordPosition="1736">ition of NE in IREX. NE class Examples ORGANIZATION NHK Symphony Orchestra PERSON Kawasaki Kenjiro LOCATION Rome, Sinuiju ARTIFACT Nobel Prize DATE July 17, April this year TIME twelve o’clock noon MONEY sixty thousand dollars PERCENT 20%, thirty percents (hospital) are tagged as FOOD, ANIMAL and FACILITY, respectively. For each category, we calculate the ratio of the categorized example among all case slot examples, and add it to the case slot (e.g. [CT:FOOD]:0.07). We also generalize the examples based upon NE classes. We use a common standard NE definition for Japanese provided by the IREX (1999). We first recognize NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008); and then construct case frames from the NErecognized corpus. Similar to the categories, for each NE class, we calculate the NE ratio among all the case slot examples, and add it to the case slot (e.g. [NE:PERSON]:0.12). The generalized examples are also included in Table 1. 4 Discourse Analysis with Case Frames In order to investigate the effect of corpus size on complex NLP tasks, we apply the constructed cases frames to an integrated probabilistic model for Japanese syntactic and case struct</context>
</contexts>
<marker>1999</marker>
<rawString>IREX Committee, editor. 1999. Proc. of the IREX Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis.</title>
<date>2006</date>
<booktitle>In Proc. of HLTNAACL’06,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="2665" citStr="Kawahara and Kurohashi, 2006" startWordPosition="408" endWordPosition="412">mantic case frames, which describe the cases each predicate has and the types of nouns that can fill a case slot. Note that case frames offer not only the knowledge of the relationships between a predicate and its particular case slot, but also the knowledge of the relationships among a predicate and its multiple case slots. To obtain such knowledge, very large corpora seem to be necessary; however it is still unknown how much corpora would be required to obtain good coverage. For examples, Kawahara and Kurohashi proposed a method for constructing wide-coverage case frames from large corpora (Kawahara and Kurohashi, 2006b), and a model for syntactic and case structure analysis of Japanese that based upon case frames (Kawahara and Kurohashi, 2006a). However, they did not demonstrate whether the coverage of case frames was wide enough for these tasks and how dependent the performance of the model was on the corpus size for case frame construction. This paper aims to address these questions. We collect a very large Japanese corpus consisting of about 100 billion words, or 1.6 billion unique sentences from the Web. Subsets of the corpus are randomly selected to obtain corpora of different sizes ranging from 1.6 m</context>
<context position="6490" citStr="Kawahara and Kurohashi (2006" startWordPosition="1024" endWordPosition="1027">s. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news wires are publicly available consisting of tens of million sentences. Kotonoha project is now constructing a balanced corpus of the presentday written Japanese consisting of 50 million words (Maekawa, 2006). 3 Construction of Case Frames Case frames describe the cases each predicate has and what nouns can fill the case slots. In this study, case frames we construct case frames from raw corpor</context>
<context position="11730" citStr="Kawahara and Kurohashi, 2006" startWordPosition="1839" endWordPosition="1842">ognize NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008); and then construct case frames from the NErecognized corpus. Similar to the categories, for each NE class, we calculate the NE ratio among all the case slot examples, and add it to the case slot (e.g. [NE:PERSON]:0.12). The generalized examples are also included in Table 1. 4 Discourse Analysis with Case Frames In order to investigate the effect of corpus size on complex NLP tasks, we apply the constructed cases frames to an integrated probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006a) and a probabilistic model for Japanese zero anaphora resolution (Sasano et al., 2008). In this section, we briefly describe these models. 523 4.1 Model for Syntactic and Case Structure Analysis Kawahara and Kurohashi (2006a) proposed an integrated probabilistic model for Japanese syntactic and case structure analysis based upon case frames. Case structure analysis recognizes predicate argument structures. Their model gives a probability to each possible syntactic structure T and case structure L of the input sentence 5, and outputs the syntactic and case structure that have the highest prob</context>
<context position="13458" citStr="Kawahara and Kurohashi, 2006" startWordPosition="2140" endWordPosition="2143"> decomposed into the product of several generative probabilities such as P(A(sj) = 1|CFl, sj) and P(nj|CFl, sj, A(sj) = 1), where the function A(sj) returns 1 if a case slot sj is filled with an input case component; otherwise 0. P(A(sj)=1|CFl, sj) denotes the probability that the case slot sj is filled with an input case component, and is estimated from resultant case structure analysis of a large raw corpus. P(nj|CFl, sj, A(sj) = 1) denotes the probability of generating a content part nj from a filled case slot sj in a case frame CFl, and is calculated by using case frames. For details see (Kawahara and Kurohashi, 2006a). 4.2 Model for Zero Anaphora Resolution Anaphora resolution is one of the most important techniques for discourse analysis. In English, overt pronouns such as she and definite noun phrases such as the company are anaphors that refer to preceding entities (antecedents). On the other hand, in 3In Japanese, bunsetsu is a basic unit of dependency, consisting of one or more content words and the following zero or more function words. It corresponds to a base phrase in English. Japanese, anaphors are often omitted; these omissions are called zero pronouns. Zero anaphora resolution is the integrat</context>
<context position="16990" citStr="Kawahara and Kurohashi (2006" startWordPosition="2748" endWordPosition="2752"> frame 2.95 3.44 3.88 4.21 4.69 5.08 average # of examples for a case slot 4.89 10.2 19.5 34.0 67.2 137.6 average # of unique examples for a case slot 1.19 1.85 3.06 4.42 6.81 9.64 average # of generalized examples for a case slot 0.14 0.24 0.37 0.49 0.67 0.84 File size(byte) 8.9M 20M 56M 147M 369M 928M Table 3: Corpus Sizes and Thresholds. Corpus size for case 1.6M 6.3M 25M 100M 400M 1.6G frame construction (sentences) Threshold α 2 3 4 5 7 10 introduced in Sec. 3.1 1.6M 3.2M 6.3M 13M 25M 50M Corpus size to estimate generative probability (sentences) from the Web using the method proposed by Kawahara and Kurohashi (2006b). We acquired approximately 6 billion Japanese sentences consisting of approximately 100 billion words from 100 million Japanese web pages. After discarding duplicate sentences, which may have been extracted from mirror sites, we acquired a corpus comprising of 1.6 billion (1.6G) unique Japanese sentences consisting of approximately 25 billion words. The average number of characters and words in each sentence was 28.3, 15.6, respectively. Then we randomly selected subsets of the corpus for five different sizes; 1.6M, 6.3M, 25M, 100M, and 400M sentences to obtain corpora of different sizes. W</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006a. A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. In Proc. of HLTNAACL’06, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using highperformance computing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC’06,</booktitle>
<pages>1344--1347</pages>
<contexts>
<context position="2665" citStr="Kawahara and Kurohashi, 2006" startWordPosition="408" endWordPosition="412">mantic case frames, which describe the cases each predicate has and the types of nouns that can fill a case slot. Note that case frames offer not only the knowledge of the relationships between a predicate and its particular case slot, but also the knowledge of the relationships among a predicate and its multiple case slots. To obtain such knowledge, very large corpora seem to be necessary; however it is still unknown how much corpora would be required to obtain good coverage. For examples, Kawahara and Kurohashi proposed a method for constructing wide-coverage case frames from large corpora (Kawahara and Kurohashi, 2006b), and a model for syntactic and case structure analysis of Japanese that based upon case frames (Kawahara and Kurohashi, 2006a). However, they did not demonstrate whether the coverage of case frames was wide enough for these tasks and how dependent the performance of the model was on the corpus size for case frame construction. This paper aims to address these questions. We collect a very large Japanese corpus consisting of about 100 billion words, or 1.6 billion unique sentences from the Web. Subsets of the corpus are randomly selected to obtain corpora of different sizes ranging from 1.6 m</context>
<context position="6490" citStr="Kawahara and Kurohashi (2006" startWordPosition="1024" endWordPosition="1027">s. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news wires are publicly available consisting of tens of million sentences. Kotonoha project is now constructing a balanced corpus of the presentday written Japanese consisting of 50 million words (Maekawa, 2006). 3 Construction of Case Frames Case frames describe the cases each predicate has and what nouns can fill the case slots. In this study, case frames we construct case frames from raw corpor</context>
<context position="11730" citStr="Kawahara and Kurohashi, 2006" startWordPosition="1839" endWordPosition="1842">ognize NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008); and then construct case frames from the NErecognized corpus. Similar to the categories, for each NE class, we calculate the NE ratio among all the case slot examples, and add it to the case slot (e.g. [NE:PERSON]:0.12). The generalized examples are also included in Table 1. 4 Discourse Analysis with Case Frames In order to investigate the effect of corpus size on complex NLP tasks, we apply the constructed cases frames to an integrated probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006a) and a probabilistic model for Japanese zero anaphora resolution (Sasano et al., 2008). In this section, we briefly describe these models. 523 4.1 Model for Syntactic and Case Structure Analysis Kawahara and Kurohashi (2006a) proposed an integrated probabilistic model for Japanese syntactic and case structure analysis based upon case frames. Case structure analysis recognizes predicate argument structures. Their model gives a probability to each possible syntactic structure T and case structure L of the input sentence 5, and outputs the syntactic and case structure that have the highest prob</context>
<context position="13458" citStr="Kawahara and Kurohashi, 2006" startWordPosition="2140" endWordPosition="2143"> decomposed into the product of several generative probabilities such as P(A(sj) = 1|CFl, sj) and P(nj|CFl, sj, A(sj) = 1), where the function A(sj) returns 1 if a case slot sj is filled with an input case component; otherwise 0. P(A(sj)=1|CFl, sj) denotes the probability that the case slot sj is filled with an input case component, and is estimated from resultant case structure analysis of a large raw corpus. P(nj|CFl, sj, A(sj) = 1) denotes the probability of generating a content part nj from a filled case slot sj in a case frame CFl, and is calculated by using case frames. For details see (Kawahara and Kurohashi, 2006a). 4.2 Model for Zero Anaphora Resolution Anaphora resolution is one of the most important techniques for discourse analysis. In English, overt pronouns such as she and definite noun phrases such as the company are anaphors that refer to preceding entities (antecedents). On the other hand, in 3In Japanese, bunsetsu is a basic unit of dependency, consisting of one or more content words and the following zero or more function words. It corresponds to a base phrase in English. Japanese, anaphors are often omitted; these omissions are called zero pronouns. Zero anaphora resolution is the integrat</context>
<context position="16990" citStr="Kawahara and Kurohashi (2006" startWordPosition="2748" endWordPosition="2752"> frame 2.95 3.44 3.88 4.21 4.69 5.08 average # of examples for a case slot 4.89 10.2 19.5 34.0 67.2 137.6 average # of unique examples for a case slot 1.19 1.85 3.06 4.42 6.81 9.64 average # of generalized examples for a case slot 0.14 0.24 0.37 0.49 0.67 0.84 File size(byte) 8.9M 20M 56M 147M 369M 928M Table 3: Corpus Sizes and Thresholds. Corpus size for case 1.6M 6.3M 25M 100M 400M 1.6G frame construction (sentences) Threshold α 2 3 4 5 7 10 introduced in Sec. 3.1 1.6M 3.2M 6.3M 13M 25M 50M Corpus size to estimate generative probability (sentences) from the Web using the method proposed by Kawahara and Kurohashi (2006b). We acquired approximately 6 billion Japanese sentences consisting of approximately 100 billion words from 100 million Japanese web pages. After discarding duplicate sentences, which may have been extracted from mirror sites, we acquired a corpus comprising of 1.6 billion (1.6G) unique Japanese sentences consisting of approximately 25 billion words. The average number of characters and words in each sentence was 28.3, 15.6, respectively. Then we randomly selected subsets of the corpus for five different sizes; 1.6M, 6.3M, 25M, 100M, and 400M sentences to obtain corpora of different sizes. W</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006b. Case frame compilation from the web using highperformance computing. In Proc. of LREC’06, pages 1344–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Probabilistic coordination disambiguation in a fullylexicalized Japanese parser.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL’07,</booktitle>
<pages>306--314</pages>
<contexts>
<context position="22269" citStr="Kawahara and Kurohashi (2007)" startWordPosition="3613" endWordPosition="3616">t used for case frame construction, although such relations were also the target of the coverage evaluation. For noun with copula, the coverage was only 54.5%. However, most predicate argument relations concerning nouns with copula were easily recognized from syntactic preference, and thus the low coverage would not quite affect the performance of discourse analysis. 5.3 Syntactic and Case Structure Analysis 5.3.1 Accuracy of Syntactic Analysis We investigated the effect of corpus size for syntactic analysis described in Section 4.1. We used hand-annotated 759 web sentences, which was used by Kawahara and Kurohashi (2007). We evaluated the resultant syntactic structures with regard to dependency accuracy, the proportion of correct dependencies out of all dependencies4. Figure 4 shows the accuracy of syntactic structures. We conducted these experiments with case frames constructed from corpora of different sizes. We also changed the corpus size to estimate generative probability of a case slot in Section 4.1 depending upon the size of the corpus for case frame construction as shown in Table 3. Figure 4 also in4Note that Kawahara and Kurohashi (2007) exclude the dependency between the last two bunsetsu, since Ja</context>
</contexts>
<marker>Kawahara, Kurohashi, 2007</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2007. Probabilistic coordination disambiguation in a fullylexicalized Japanese parser. In Proc. of EMNLPCoNLL’07, pages 306–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Toward text understanding: Integrating relevance-tagged corpora and automatically constructed case frames.</title>
<date>2004</date>
<booktitle>In Proc. of LREC’04,</booktitle>
<pages>1833--1836</pages>
<contexts>
<context position="18547" citStr="Kawahara et al. (2004)" startWordPosition="3006" endWordPosition="3009">rames. The number of predicates, the average number of examples and unique examples for a case slot, and whole file size were confirmed to be heavily dependent upon the corpus size. However, the average number of case frames for a predicate and case slots for a case frame did not. 5.2 Coverage of Constructed Case Frames 5.2.1 Setting In order to investigate the coverage of the resultant case frames, we used a syntactic relation, case structure, and anaphoric relation annotated corpus consisting of 186 web documents (979 sentences). This corpus was manually annotated using the same criteria as Kawahara et al. (2004). There were 2,390 annotated relationships between predicates and their direct (not omitted) case components and 837 zero anaphoric relations in the corpus. We used two evaluation metrics depending upon whether the target case component was omitted or not. For the overt case component of a predicate, we judged the target component was covered by case frames if the target component itself was included in the examples for one of the corresponding case slots of the case frame. For the omitted case component, we checked not only the target component itself but also all mentions that refer to the s</context>
</contexts>
<marker>Kawahara, Sasano, Kurohashi, 2004</marker>
<rawString>Daisuke Kawahara, Ryohei Sasano, and Sadao Kurohashi. 2004. Toward text understanding: Integrating relevance-tagged corpora and automatically constructed case frames. In Proc. of LREC’04, pages 1833–1836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Introduction to the special issue on the web as corpus.</title>
<date>2003</date>
<journal>Computational Linguistic,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="3881" citStr="Kilgarriff and Grefenstette, 2003" startWordPosition="604" endWordPosition="607">ranging from 1.6 million to 1.6 billion sentences. We construct case frames from each corpus and apply them to syntactic and case structure analysis, and zero anaphora resolution, in order to investigate the 521 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521–529, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data s</context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Adam Kilgarriff and Gregory Grefenstette. 2003. Introduction to the special issue on the web as corpus. Computational Linguistic, 29(3):333–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Hideto Kazawa</author>
</authors>
<title>Web Japanese Ngram version 1, published by Gengo Shigen Kyokai.</title>
<date>2007</date>
<contexts>
<context position="6680" citStr="Kudo and Kazawa, 2007" startWordPosition="1052" endWordPosition="1055">search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news wires are publicly available consisting of tens of million sentences. Kotonoha project is now constructing a balanced corpus of the presentday written Japanese consisting of 50 million words (Maekawa, 2006). 3 Construction of Case Frames Case frames describe the cases each predicate has and what nouns can fill the case slots. In this study, case frames we construct case frames from raw corpora by using the method described in (Kawahara and Kurohashi, 2006b). This section illustrates the methodology for constructing case frames. 3.1 Basic Method After parsing a large corpus by a </context>
</contexts>
<marker>Kudo, Kazawa, 2007</marker>
<rawString>Taku Kudo and Hideto Kazawa. 2007. Web Japanese Ngram version 1, published by Gengo Shigen Kyokai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<pages>2--1</pages>
<contexts>
<context position="1234" citStr="Lapata and Keller, 2005" startWordPosition="175" endWordPosition="178">en, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural language processing (NLP) applications, such as prepositional phrase (PP) attachment, other-anaphora resolution, spelling correction, confusable word set disambiguation and machine translation (Volk, 2001; Modjeska et al., 2003; Lapata and Keller, 2005; Atterer and Sch¨utze, 2006; Brants et al., 2007). Most of the previous work utilized only the surface information of the corpora, such as n-grams, co-occurrence counts, and simple surface syntax. This may be because these studies did not require structured knowledge, and for such studies, the size of currently available corpora is considered to have been almost enough. For instance, while Brants et al. (2007) reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens, the increase became small at the corpu</context>
<context position="4066" citStr="Lapata and Keller (2005)" startWordPosition="634" endWordPosition="637">estigate the 521 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521–529, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; the</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2:1:1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinci Liu</author>
<author>James R Curran</author>
</authors>
<title>Web text corpus for natural language processing.</title>
<date>2006</date>
<booktitle>In Proc. of EACL’06,</booktitle>
<pages>233--240</pages>
<contexts>
<context position="5886" citStr="Liu and Curran (2006)" startWordPosition="929" endWordPosition="932">c chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (</context>
</contexts>
<marker>Liu, Curran, 2006</marker>
<rawString>Vinci Liu and James R. Curran. 2006. Web text corpus for natural language processing. In Proc. of EACL’06, pages 233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
</authors>
<title>Kotonoha, the corpus development project of the National Institute for Japanese language.</title>
<date>2006</date>
<booktitle>In Proc. of the 13th NIJL International Symposium,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="6901" citStr="Maekawa, 2006" startWordPosition="1087" endWordPosition="1088"> Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news wires are publicly available consisting of tens of million sentences. Kotonoha project is now constructing a balanced corpus of the presentday written Japanese consisting of 50 million words (Maekawa, 2006). 3 Construction of Case Frames Case frames describe the cases each predicate has and what nouns can fill the case slots. In this study, case frames we construct case frames from raw corpora by using the method described in (Kawahara and Kurohashi, 2006b). This section illustrates the methodology for constructing case frames. 3.1 Basic Method After parsing a large corpus by a Japanese parser KNP1, we construct case frames from modifier-head examples in the resulting parses. The problems for case frame construction are syntactic and semantic ambiguities. In other words, the resulting parses ine</context>
</contexts>
<marker>Maekawa, 2006</marker>
<rawString>Kikuo Maekawa. 2006. Kotonoha, the corpus development project of the National Institute for Japanese language. In Proc. of the 13th NIJL International Symposium, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL’06,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="6355" citStr="McClosky et al. (2006)" startWordPosition="1004" endWordPosition="1007"> the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news wires are publicly available consisting of tens of million sentences. Kotonoha project is now constructing a balanced corpus of the presentday written Japanese consisting of 50 million words (Maekawa, 2006). 3 Construction of Case Frames Case frames describe t</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proc. of HLT-NAACL’06, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalia N Modjeska</author>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Using the web in machine learning for other-anaphora resolution.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP-2003,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="1209" citStr="Modjeska et al., 2003" startWordPosition="171" endWordPosition="174">six different sizes. Then, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural language processing (NLP) applications, such as prepositional phrase (PP) attachment, other-anaphora resolution, spelling correction, confusable word set disambiguation and machine translation (Volk, 2001; Modjeska et al., 2003; Lapata and Keller, 2005; Atterer and Sch¨utze, 2006; Brants et al., 2007). Most of the previous work utilized only the surface information of the corpora, such as n-grams, co-occurrence counts, and simple surface syntax. This may be because these studies did not require structured knowledge, and for such studies, the size of currently available corpora is considered to have been almost enough. For instance, while Brants et al. (2007) reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens, the increase </context>
<context position="3996" citStr="Modjeska et al. (2003)" startWordPosition="623" endWordPosition="626">se structure analysis, and zero anaphora resolution, in order to investigate the 521 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521–529, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervis</context>
</contexts>
<marker>Modjeska, Markert, Nissim, 2003</marker>
<rawString>Natalia N. Modjeska, Katja Markert, and Malvina Nissim. 2003. Using the web in machine learning for other-anaphora resolution. In Proc. of EMNLP-2003, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>A study of using search engine page hits as a proxy for n-gram frequencies.</title>
<date>2005</date>
<booktitle>In Proc. of RANLP’05.</booktitle>
<contexts>
<context position="5501" citStr="Nakov and Hearst, 2005" startWordPosition="866" endWordPosition="869"> applied these models to machine translation systems. They reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens. Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semisupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) create</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. A study of using search engine page hits as a proxy for n-gram frequencies. In Proc. of RANLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti A Hearst</author>
</authors>
<title>Solving relational similarity problems using the web as a corpus.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT’08,</booktitle>
<pages>452--460</pages>
<contexts>
<context position="4250" citStr="Nakov and Hearst (2008)" startWordPosition="662" endWordPosition="665">mputational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 mil</context>
</contexts>
<marker>Nakov, Hearst, 2008</marker>
<rawString>Preslav Nakov and Marti A. Hearst. 2008. Solving relational similarity problems using the web as a corpus. In Proc. of ACL-HLT’08, pages 452–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Japanese named entity recognition using structural natural language processing.</title>
<date>2008</date>
<booktitle>In Proc. of IJCNLP’08,</booktitle>
<pages>607--612</pages>
<contexts>
<context position="11188" citStr="Sasano and Kurohashi, 2008" startWordPosition="1750" endWordPosition="1753">ON Kawasaki Kenjiro LOCATION Rome, Sinuiju ARTIFACT Nobel Prize DATE July 17, April this year TIME twelve o’clock noon MONEY sixty thousand dollars PERCENT 20%, thirty percents (hospital) are tagged as FOOD, ANIMAL and FACILITY, respectively. For each category, we calculate the ratio of the categorized example among all case slot examples, and add it to the case slot (e.g. [CT:FOOD]:0.07). We also generalize the examples based upon NE classes. We use a common standard NE definition for Japanese provided by the IREX (1999). We first recognize NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008); and then construct case frames from the NErecognized corpus. Similar to the categories, for each NE class, we calculate the NE ratio among all the case slot examples, and add it to the case slot (e.g. [NE:PERSON]:0.12). The generalized examples are also included in Table 1. 4 Discourse Analysis with Case Frames In order to investigate the effect of corpus size on complex NLP tasks, we apply the constructed cases frames to an integrated probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006a) and a probabilistic model for Japanese zero anaphora re</context>
</contexts>
<marker>Sasano, Kurohashi, 2008</marker>
<rawString>Ryohei Sasano and Sadao Kurohashi. 2008. Japanese named entity recognition using structural natural language processing. In Proc. of IJCNLP’08, pages 607– 612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for japanese zero anaphora resolution.</title>
<date>2008</date>
<booktitle>In Proc. of COLING’08,</booktitle>
<pages>769--776</pages>
<contexts>
<context position="11818" citStr="Sasano et al., 2008" startWordPosition="1853" endWordPosition="1856"> construct case frames from the NErecognized corpus. Similar to the categories, for each NE class, we calculate the NE ratio among all the case slot examples, and add it to the case slot (e.g. [NE:PERSON]:0.12). The generalized examples are also included in Table 1. 4 Discourse Analysis with Case Frames In order to investigate the effect of corpus size on complex NLP tasks, we apply the constructed cases frames to an integrated probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006a) and a probabilistic model for Japanese zero anaphora resolution (Sasano et al., 2008). In this section, we briefly describe these models. 523 4.1 Model for Syntactic and Case Structure Analysis Kawahara and Kurohashi (2006a) proposed an integrated probabilistic model for Japanese syntactic and case structure analysis based upon case frames. Case structure analysis recognizes predicate argument structures. Their model gives a probability to each possible syntactic structure T and case structure L of the input sentence 5, and outputs the syntactic and case structure that have the highest probability. That is to say, the system selects the syntactic structure Tbest and the case s</context>
<context position="14237" citStr="Sasano et al., 2008" startWordPosition="2267" endWordPosition="2270">as she and definite noun phrases such as the company are anaphors that refer to preceding entities (antecedents). On the other hand, in 3In Japanese, bunsetsu is a basic unit of dependency, consisting of one or more content words and the following zero or more function words. It corresponds to a base phrase in English. Japanese, anaphors are often omitted; these omissions are called zero pronouns. Zero anaphora resolution is the integrated task of zero pronoun detection and zero pronoun resolution. We proposed a probabilistic model for Japanese zero anaphora resolution based upon case frames (Sasano et al., 2008). This model first resolves coreference and identifies discourse entities; then gives a probability to each possible case frame CF and case assignment CA when target predicate v, input case components ICC and existing discourse entities ENT are given, and outputs the case frame and case assignment that have the highest probability. That is to say, this model selects the case frame CFbest and the case assignment CAbest that maximize the probability P(CF, CA|v, ICC, ENT): (CFbest, CAbest) = argmax P(CF, CA|v, ICC, ENT) (3) (CF,CA) P(CF, CA|v, ICC, ENT) is approximately decomposed into the produc</context>
<context position="15772" citStr="Sasano et al., 2008" startWordPosition="2529" endWordPosition="2532">a case slot sj is filled with an antecedent of a zero pronoun; otherwise 0. P(nj|CFl, sj, A&apos;(sj)=1) is similar to P(nj|CFl, sj, A(sj)=1) and estimated from the frequencies of case slot examples in case frames. However, while A&apos;(sj)=1 means sj is not filled with an overt argument but filled with an antecedent of zero pronoun, case frames are constructed from overt predicate argument pairs. Therefore, the content part nj is often not included in the case slot examples. To cope with this problem, this model also utilizes generalized examples to estimate P(nj|CFl, sj, A(sj) = 1). For details see (Sasano et al., 2008). 5 Experiments 5.1 Construction of Case Frames In order to investigate the effect of corpus size, we constructed case frames from corpora of different sizes. We first collected Japanese sentences 524 Table 4: Statistics of the Constructed Case Frames. Corpus size (sentences) 1.6M 6.3M 25M 100M 400M 1.6G # of predicate 2460 6134 13532 27226 42739 65679 (type) verb 2039 4895 10183 19191 28523 41732 adjective 154 326 617 1120 1641 2318 noun with copula 267 913 2732 6915 12575 21629 average # of case frames for a predicate 15.9 12.2 13.3 16.1 20.5 25.3 average # of case slots for a case frame 2.9</context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2008</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2008. A fully-lexicalized probabilistic model for japanese zero anaphora resolution. In Proc. of COLING’08, pages 769–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT’08,</booktitle>
<pages>665--673</pages>
<contexts>
<context position="5110" citStr="Suzuki and Isozaki (2008)" startWordPosition="801" endWordPosition="804">improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens. Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semisupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In Proceedings of ACL-HLT’08, pages 665–673.</rawString>
</citation>
<citation valid="true">
<title>The National Language Institute for Japanese Language.</title>
<date>2004</date>
<journal>Bunruigoihyo. Dainippon Tosho, (In Japanese).</journal>
<contexts>
<context position="6094" citStr="(2004)" startWordPosition="965" endWordPosition="965"> Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from variety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news</context>
<context position="18547" citStr="(2004)" startWordPosition="3009" endWordPosition="3009">r of predicates, the average number of examples and unique examples for a case slot, and whole file size were confirmed to be heavily dependent upon the corpus size. However, the average number of case frames for a predicate and case slots for a case frame did not. 5.2 Coverage of Constructed Case Frames 5.2.1 Setting In order to investigate the coverage of the resultant case frames, we used a syntactic relation, case structure, and anaphoric relation annotated corpus consisting of 186 web documents (979 sentences). This corpus was manually annotated using the same criteria as Kawahara et al. (2004). There were 2,390 annotated relationships between predicates and their direct (not omitted) case components and 837 zero anaphoric relations in the corpus. We used two evaluation metrics depending upon whether the target case component was omitted or not. For the overt case component of a predicate, we judged the target component was covered by case frames if the target component itself was included in the examples for one of the corresponding case slots of the case frame. For the omitted case component, we checked not only the target component itself but also all mentions that refer to the s</context>
</contexts>
<marker>2004</marker>
<rawString>The National Language Institute for Japanese Language. 2004. Bunruigoihyo. Dainippon Tosho, (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>Exploiting the WWW as a corpus to resolve PP attachment ambiguities.</title>
<date>2001</date>
<booktitle>In Proc. of the Corpus Linguistics,</booktitle>
<pages>601--606</pages>
<contexts>
<context position="1186" citStr="Volk, 2001" startWordPosition="169" endWordPosition="170"> corpora of six different sizes. Then, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural language processing (NLP) applications, such as prepositional phrase (PP) attachment, other-anaphora resolution, spelling correction, confusable word set disambiguation and machine translation (Volk, 2001; Modjeska et al., 2003; Lapata and Keller, 2005; Atterer and Sch¨utze, 2006; Brants et al., 2007). Most of the previous work utilized only the surface information of the corpora, such as n-grams, co-occurrence counts, and simple surface syntax. This may be because these studies did not require structured knowledge, and for such studies, the size of currently available corpora is considered to have been almost enough. For instance, while Brants et al. (2007) reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillio</context>
<context position="3894" citStr="Volk (2001)" startWordPosition="608" endWordPosition="609">ion sentences. We construct case frames from each corpus and apply them to syntactic and case structure analysis, and zero anaphora resolution, in order to investigate the 521 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521–529, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved </context>
</contexts>
<marker>Volk, 2001</marker>
<rawString>Martin Volk. 2001. Exploiting the WWW as a corpus to resolve PP attachment ambiguities. In Proc. of the Corpus Linguistics, pages 601–606.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>