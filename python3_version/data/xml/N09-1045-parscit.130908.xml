<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001060">
<title confidence="0.998076">
Improving the Arabic Pronunciation Dictionary for Phone and Word
Recognition with Linguistically-Based Pronunciation Rules
</title>
<author confidence="0.961349">
Fadi Biadsy* and Nizar Habash† and Julia Hirschberg*
</author>
<affiliation confidence="0.993389">
*Department of Computer Science, Columbia University, New York, USA
</affiliation>
<email confidence="0.87859">
{fadi,julia}@cs.columbia.edu
</email>
<affiliation confidence="0.785259">
†Center for Computational Learning Systems, Columbia University, New York, USA
</affiliation>
<email confidence="0.994541">
habash@ccls.columbia.edu
</email>
<sectionHeader confidence="0.995475" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.978525105263158">
In this paper, we show that linguistically mo-
tivated pronunciation rules can improve phone
and word recognition results for Modern Stan-
dard Arabic (MSA). Using these rules and
the MADA morphological analysis and dis-
ambiguation tool, multiple pronunciations per
word are automatically generated to build two
pronunciation dictionaries; one for training
and another for decoding. We demonstrate
that the use of these rules can significantly
improve both MSA phone recognition and
MSA word recognition accuracies over a base-
line system using pronunciation rules typi-
cally employed in previous work on MSA Au-
tomatic Speech Recognition (ASR). We ob-
tain a significant improvement in absolute ac-
curacy in phone recognition of 3.77%–7.29%
and a significant improvement of 4.1% in ab-
solute accuracy in ASR.
</bodyText>
<sectionHeader confidence="0.999098" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998433942307692">
The correspondence between orthography and pro-
nunciation in Modern Standard Arabic (MSA) falls
somewhere between that of languages such as Span-
ish and Finnish, which have an almost one-to-one
mapping between letters and sounds, and languages
such as English and French, which exhibit a more
complex letter-to-sound mapping (El-Imam, 2004).
The more complex this mapping is, the more diffi-
cult the language is for Automatic Speech Recogni-
tion (ASR).
An essential component of an ASR system is its
pronunciation dictionary (lexicon), which maps the
orthographic representation of words to their pho-
netic or phonemic pronunciation variants. For lan-
guages with complex letter-to-sound mappings, such
dictionaries are typically written by hand. However,
for morphologically rich languages, such as MSA,1
pronunciation dictionaries are difficult to create by
hand, because of the large number of word forms,
each of which has a large number of possible pro-
nunciations. Fortunately, the relationship between
orthography and pronunciation is relatively regu-
lar and well understood for MSA. Moreover, re-
cent automatic techniques for morphological anal-
ysis and disambiguation (MADA) can also be useful
in automating part of the dictionary creation process
(Habash and Rambow, 2005; Habash and Rambow,
2007) Nonetheless, most documented Arabic ASR
systems appear to handle only a subset of Arabic
phonetic phenomena; very few use morphological
disambiguation tools.
In Section 2, we briefly describe related work, in-
cluding the baseline system we use. In Section 3, we
outline the linguistic phenomena we believe are crit-
ical to improving MSA pronunciation dictionaries.
In Section 4, we describe the pronunciation rules we
have developed based upon these linguistic phenom-
ena. In Section 5, we describe how these rules are
used, together with MADA, to build our pronuncia-
tion dictionaries for training and decoding automat-
ically. In Section 6, we present results of our eval-
uations of our phone- and word-recognition systems
(XPR and XWR) on MSA comparing these systems
to two baseline systems, BASEPR and BASEWR.
1MSA words have fourteen features: part-of-speech, person,
number, gender, voice, aspect, determiner proclitic, conjunctive
proclitic, particle proclitic, pronominal enclitic, nominal case,
nunation, idafa (possessed), and mood. MSA features are real-
ized using both concatenative (affixes and stems) and templatic
(root and patterns) morphology with a variety of morphological
and phonological adjustments that appear in word orthography
and interact with orthographic variations.
</bodyText>
<page confidence="0.970918">
397
</page>
<note confidence="0.8909375">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 397–405,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9925945">
We conclude in Section 7 and identify directions for
future research.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955619047619">
Most recent work on ASR for MSA uses a sin-
gle pronunciation dictionary constructed by map-
ping every undiacritized word in the training cor-
pus to all of the diacritized Buckwalter analyses and
the diacritized versions of this word in the Arabic
Treebank (Maamouri et al., 2003; Afify et al., 2005;
Messaoudi et al., 2006; Soltau et al., 2007). In these
papers, each diacritized word is converted to a sin-
gle pronunciation with a one-to-one mapping using
“very few” unspecified rules. None of these systems
use morphological disambiguation to determine the
most likely pronunciation of the word given its con-
text. Vergyri et al. (2008) do use morphological in-
formation to predict word pronunciation. They se-
lect the top choice from the MADA (Morphological
Analysis and Disambiguation for Arabic) system for
each word to train their acoustic models. For the test
lexicon they used the undiacritized orthography, as
well as all diacritizations found for each word in the
training data as possible pronunciation variants. We
use this system as our baseline for comparison.
</bodyText>
<sectionHeader confidence="0.80528" genericHeader="method">
3 Arabic Orthography and Pronunciation
</sectionHeader>
<bodyText confidence="0.9999518">
MSA is written in a morpho-phonemic orthographic
representation using the Arabic script, an alphabet
accented with optional diacritical marks.2 MSA has
34 phonemes (28 consonants, 3 long vowels and 3
short vowels). The Arabic script has 36 basic let-
ters (ignoring ligatures) and 9 diacritics. Most Ara-
bic letters have a one-to-one mapping to an MSA
phoneme; however, there are a small number of
common exceptions (Habash et al., 2007; El-Imam,
2004) which we summarize next.
</bodyText>
<subsectionHeader confidence="0.996183">
3.1 Optional Diacritics
</subsectionHeader>
<bodyText confidence="0.75989475">
Arabic script commonly uses nine optional diacrit-
ics: (a) three short-vowel diacritics representing the
vowels /a/, /u/ and /i/; (b) one long-vowel diacritic
(Dagger Alif ‘) representing the long vowel /A/ in a
</bodyText>
<footnote confidence="0.668972">
2We provide Arabic script orthographic transliteration in
the Buckwalter transliteration scheme (Buckwalter, 2004). For
Modern Standard Arabic phonological transcription, we use a
variant of the Buckwalter transliteration with the following ex-
ceptions: glottal stops are represented as /G/ and long vowels as
/A/, /U/ and /I/. All Arabic script diacritics are phonologically
spelled out.
</footnote>
<bodyText confidence="0.999656352941176">
small number of words; (c) three nunation diacrit-
ics (F /an/, N /un/, K /in/) representing a combina-
tion of a short vowel and the nominal indefiniteness
marker /n/ in MSA; (d) one consonant lengthening
diacritic (called Shadda —) which repeats/elongates
the previous consonant (e.g., kat—ab is pronounced
/kattab/); and (e) one diacritic for marking when
there is no diacritic (called Sukun o).
Arabic diacritics can only appear after a let-
ter. Word-initial diacritics (in practice, only short
vowels) are handled by adding an extra Alif1A
(also called Hamzat-Wasl) at the beginning of the
word. Sentence/utterance initial Hamzat-Wasl is
pronounced like a glottal stop preceding the short
vowel; however, the sentence medial Hamzat-Wasl
is silent except for the short vowel. For exam-
ple, Ainkataba kitAbN is /Ginkataba kitAbun/ but
kitAbN Ainkataba is /kitAbun inkataba/. A ‘real’
Hamza (glottal stop) is always pronounced as a glot-
tal stop. The Hamzat-Wasl appears most commonly
as the Alif of the definite article Al. It also appears
in specific words and word classes such as relative
pronouns (e.g., Aly ‘who’ and verbs in pattern VII
(Ain1a2a3).
Arabic short vowel diacritics are used together
with the glide consonant letters w and y to denote
the long vowels /U/ (as uw) and /I/ (iy). This makes
these two letters ambiguous.
Diacritics are largely restricted to religious texts
and Arabic language school textbooks. In other
texts, fewer than 1.5% of words contain a diacritic.
Some diacritics are lexical (where word meaning
varies) and others are inflectional (where nominal
case or verbal mood varies). Inflectional diacritics
are typically word final. Since nominal case, verbal
mood and nunation have all disappeared in spoken
dialectal Arabic, Arabic speakers do not always pro-
duce these inflections correctly or at all.
Much work has been done on automatic Arabic
diacritization (Vergyri and Kirchhoff, 2004; Anan-
thakrishnan et al., 2005; Zitouni et al., 2006; Habash
and Rambow, 2007). In this paper, we use the
MADA (Morphological Analysis and Disambigua-
tion for Arabic) system to diacritize Arabic (Habash
and Rambow, 2005; Habash and Rambow, 2007).
MADA, which uses the Buckwalter Arabic mor-
phological Analyzer databases (Buckwalter, 2004),
provides the necessary information to determine
Hamzat-Wasl through morphologically tagging the
definite article; in most other cases it outputs the spe-
cial symbol “{” for Hamzat-Wasl.
</bodyText>
<page confidence="0.997229">
398
</page>
<subsectionHeader confidence="0.999722">
3.2 Hamza Spelling
</subsectionHeader>
<bodyText confidence="0.9999871875">
The consonant Hamza (glottal stop /G/) has multi-
ple forms in Arabic script:_′,1&gt;,1&lt;,_,&amp;,ò
}, 1|. There are complex rules for Hamza spelling
that primarily depend on its vocalic context. For ex-
ample, ò } is used word medially and finally when
preceded or followed by an /i/ vowel. Similarly, the
Hamza form 1  |is used when the Hamza is followed
by the long vowel /A/.
Hamza spelling is further complicated by the fact
that Arabic writers often replace hamzated letters
with the un-hamzated form (I &gt; → A A) or use a
two-letter spelling, e.g. ò } → cì Y ′. Due to
this variation, the un-hamzated forms (particularly
for 1 &gt; and 1 &lt;) are ignored in Arabic ASR evalua-
tion. The MADA system regularizes most of these
spelling variations as part of its analysis.
</bodyText>
<subsectionHeader confidence="0.998193">
3.3 Morpho-phonemic Spelling
</subsectionHeader>
<bodyText confidence="0.997622">
Arabic script includes a small number of mor-
phemic/lexical phenomena, some very common:
</bodyText>
<listItem confidence="0.966131769230769">
• Ta-Marbuta The Ta-Marbuta (p) is typically a
feminine ending. It appears word-finally, op-
tionally followed by a diacritic. In MSA it
is pronounced as /t/ when followed by a di-
acritic; otherwise it is silent. For example,
maktabapN ‘a library’ is pronounced / mak-
tabatun/.
• Alif-Maqsura The Alif-Maqsura (Y ) is a silent
derivational marker, which always follows a
short vowel /a/ at the end of a word. For ex-
ample, rawaY ‘to tell a story’ is pronounced
/rawa/.
• Definite Article The Arabic definite article is
a proclitic that assimilates to the first conso-
nant in the noun it modifies if this consonant
is alveolar or dental (except for j). These are
the so-called Sun Letters: t, v, d, *, r, z, s, $,
S, D, T, Z, l, and n. For example, the word
Al$ams ‘the sun’ is pronounced /a$$ams/ not
*/al$ams/. The definite article does not assimi-
late to the other consonants, the Moon Letters.
For example, the word Alqamar ‘the moon’ is
pronounced /alqamar/ not */aqqamar/.
• Silent Letters A silent Alif appears in the mor-
pheme +uwA /U/ which indicates masculine
plural conjugation in verbs. Another silent Alif
</listItem>
<bodyText confidence="0.972884333333333">
appears after some nunated nouns, e.g., ki-
taAbAF /kitAban/. In some poetic readings,
this Alif can be produced as the long vowel
/A/: /kitAbA/. Finally, a common odd spelling
is that of the proper name Eamrw /Eamr/
‘Amr’where the final w is silent.
</bodyText>
<sectionHeader confidence="0.956425" genericHeader="method">
4 Pronunciation Rules
</sectionHeader>
<bodyText confidence="0.999993909090909">
As noted in Section 3, diacritization alone does not
predict actual pronunciation in MSA. In this section
we describe a set of rules based on MSA phonol-
ogy which will extend a diacritized word to a set
of possible pronunciations. It should be noted that
even MSA-trained speakers, such as broadcast news
anchors, may not follow the “proper” pronunciation
according to Arabic syntax and phonology. So we
attempt to accommodate these pronunciation vari-
ants in our pronunciation dictionary.
The following rules are applied on each dia-
critized word.3 These rules are divided into four
categories: (I) a shared set of rules used in all
systems compared (BASEPR, BASEWR, XPR
and XWR);4 (II) a set of rules in BASEPR and
BASEWR which we modified for XPR and XWR;
(III) a first set of new rules devised for our systems
XPR and XWR; and (IV) a second set of new rules
that generate additional pronunciation variants.
Below we indicate, for each rule, how many words
in the training corpus (335,324 words) had their
pronunciation affected by the rule.
</bodyText>
<listItem confidence="0.963268909090909">
I. Shared Pronunciation Rules
1. Dagger Alif: ‘ → /A/
(e.g., h‘*A → hA*A) (This rule affected 1.8%
of all the words in our training data)
2. Madda:  |→ /G A/
(e.g., Al|n → AlGAn) (affected 1.9%)
3. Nunation: AF → /a n/, F → /a n/, /K/ → /i n/,
N → /u n/
(e.g., kutubAF → kutuban) (affected 9.7%)
4. Hamza: All Hamza forms: ′, }, &amp;, &lt;, &gt; → /G/
(e.g., &gt;kala → Gakala) (affected 21.3%)
</listItem>
<footnote confidence="0.994466166666667">
3Our script that generates the pronunciation dictio-
naries from MADA output can be downloaded from
www.cs.columbia.edu/speech/software.cgi.
4We have attempted to replicate the baseline pronunciation
rules for (Vergyri et al., 2008) based on published work and
personal communications with the authors.
</footnote>
<page confidence="0.994217">
399
</page>
<figure confidence="0.698069">
5. Ta-Marbuta: p —* /t/
(e.g., madrasapa —* madrasata) (affected
15.3%)
</figure>
<bodyText confidence="0.9573605">
As a post-processing step in all systems, we re-
move the Sukun diacritic and convert every letter X
to phoneme /X/. In XPR and XWR, we also remove
short vowels that precede or succeed a long vowel.
</bodyText>
<listItem confidence="0.982051578947368">
II. Modified Pronunciation Rules
1. Alif-Maqsura: Y —* /a/
(e.g., salomY —* saloma) (affected 4.2%)
(Baseline: Y —* /A/)
2. Shadda: Shadda is always removed
(e.g., ba$—ara —* ba$ara) (affected 23.8%)
(Baseline: the consonant was doubled)
3. U and I: uwo —* /U/, iyo —* /I/
(e.g., makotuwob —* makotUb) (affected
25.07%) (Baseline: same rule but it inaccu-
rately interacted with the baseline Shadda rule)
III. New Pronunciation Rules
1. Waw Al-jamaa: suffixes uwoA —* /U/
(e.g., katabuwoA —* katabU) (affected 0.4%)
2. Definite Article: Al —* /a l/ (if tagged as Al+
by MADA)
(e.g., wAlkitAba —* walkitAba) (affected
30.0%)
3. Hamzat-Wasl: { is always removed.
(affected 3.0%)
4. “Al” in relative pronouns: Al —* /a l/
(affected 1.3%)
5. Sun letters: if the definite article (Al) is fol-
lowed by a sun letter, remove the l.
(e.g., Al$amsu —* A$amsu) (affected 8.1%)
IV. New Pronunciation Rules Generating Addi-
tional Variants
• Ta-Marbuta: if a word ends with Ta-Marbuta
(p) followed by any diacritic, remove the Ta-
Marbuta and its diacritic. Apply the rules above
(I-III) on the modified word and add the output
pronunciation.
(e.g., marbwTapF —* marbwTa) (affected
15.3%)
• Case ending: if a word ends with a short vowel
(a, u, i), remove the short vowel. Apply rules
(I-III) on the modified word, and add the output
pronunciation
</listItem>
<bodyText confidence="0.7283">
(e.g., yaktubu —* yaktub (affected 60.9%)
</bodyText>
<sectionHeader confidence="0.904686" genericHeader="method">
5 Building the Pronunciation Dictionaries
</sectionHeader>
<bodyText confidence="0.999989377777778">
As noted above, pronunciation dictionaries map
words to one or more phonetically expressed pro-
nunciation variants. These dictionaries are used
for training and decoding in ASR systems. Typi-
cally, most data available to train large vocabulary
ASR systems is orthographically (not phonetically)
transcribed. There are two well-known alternatives
for training acoustic models in ASR: (1) bootstrap
training, when some phonetically annotated data is
available, and (2) flat-start, when such data is not
available (Young et al., 2006). In flat-start training,
for example, the pronunciation dictionary is used
to map the orthographic transcription of the train-
ing data to a sequence of phonetic labels to train
the initial monophone models. Next, the dictionary
is employed again to produce networks of possible
pronunciations which can be used in forced align-
ment to obtain the most likely phone sequence that
matches the acoustic data. Finally, the monophone
acoustic models are re-estimated. In our work, we
refer to this dictionary as the training pronuncia-
tion dictionary. The second usage of the pronun-
ciation dictionary is to generate the pronunciation
models while decoding. We refer to this dictionary
as the decoding pronunciation dictionary.
For languages like English, no distinction be-
tween decoding and training pronunciation dictio-
naries is necessary. However, as noted in Section
3, short vowels and other diacritic markers are typi-
cally not orthographically represented in MSA texts.
Thus ASR systems typically do not output fully di-
acritized transcripts. Diacritization is generally not
necessary to make the transcript readable by Arabic-
literate readers. Therefore, entries in the decod-
ing pronunciation dictionary consist of undiacritized
words that are mapped to a set of phonetically-
represented diacritizations. However, every entry in
the training pronunciation dictionary is a fully dia-
critized word mapped to a set of possible context-
dependent pronunciations. Particularly in the train-
ing step, contextual information for each word is
available from the transcript, so, for our work, we
can use the MADA morphological tagger to obtain
the most likely diacritics. As a result, the speech
signal is mapped to a more accurate representation
</bodyText>
<page confidence="0.984154">
400
</page>
<bodyText confidence="0.999774923076923">
of the training transcript, which we hypothesize will
lead to a better estimation of the acoustic models.
As noted in Section 1, pronunciation dictionaries
for ASR systems are usually written by hand. How-
ever, Arabic’s morphological richness makes it dif-
ficult to create a pronunciation dictionary by hand
since there are a very large number of word forms,
each of which has a large number of possible pro-
nunciations. The relatively regular relationship be-
tween orthography and pronunciation and tools for
morphological analysis and disambiguation such as
MADA, however, make it possible to create such
dictionaries automatically with some success.5
</bodyText>
<subsectionHeader confidence="0.900406">
5.1 Training Pronunciation Dictionary
</subsectionHeader>
<bodyText confidence="0.999991392857143">
In this section, we describe an automatic approach
to building a pronunciation dictionary for MSA that
covers all words in the orthographic transcripts of
the training data. First, for each word in each ut-
terance, we run MADA to disambiguate the word
based on its context in the transcript. MADA outputs
all possible fully-diacritized morphological analy-
ses, ranked by their likelihood, the MADA confi-
dence score.6 We thus obtain a fully-diacritized or-
thographic transcription for training. Second, we
map the highest-ranked diacritization of each word
to a set of pronunciations, which we obtain from the
pronunciation rules described in Section 4. Since
MADA may not always rank the best analysis as its
top choice, we also run the pronunciation rules on
the second best choice returned by MADA, when
the difference between the top two choices is less
than a threshold determined empirically (in our im-
plementation we chose 0.2). In Figure 1, the training
pronunciation dictionary maps the 2nd column (the
entry keys) to the 3rd column.
We generate the baseline training pronunciation
dictionary using only the baseline rules from Section
4. This dictionary also makes use of MADA, but it
maps the MADA-diacritized word to only one pro-
nunciation. The baseline training dictionary maps
the 2nd column (the entry keys) to only one pronun-
ciation in the 3rd column in Figure 1.
</bodyText>
<footnote confidence="0.995520285714286">
5The MADA system (Habash and Rambow, 2005; Habash
and Rambow, 2007) reports 4.8% diacritic error rate (DER) on
all diacritics and 2.2% (DER) when ignoring the last (inflec-
tional) diacritic.
6In our training data, only about 1% of all words are not
diacritized because of lack of coverage in the morphological
analysis component.
</footnote>
<figureCaption confidence="0.99875">
Figure 1: Mapping an undiacritized word to MADA out-
puts to possible pronunciations.
</figureCaption>
<subsectionHeader confidence="0.985726">
5.2 Decoding Pronunciation Dictionary
</subsectionHeader>
<bodyText confidence="0.999962733333333">
The decoding pronunciation dictionary is used in
ASR to build the pronunciation models while decod-
ing. Since, as noted above, it is standard to produce
unvocalized transcripts when recognizing MSA, we
must map word pronunciations to unvocalized ortho-
graphic output. Therefore, for each diacritized word
in our training pronunciation dictionary, we remove
diacritic markers and replace Hamzat-Wasl ({), &lt;,
and &gt; by the letter ‘A’, and then map the modified
word to the set of pronunciations for that word. For
example, in Figure 1 the undiacritized word mdrsp
in the 1st column is mapped to the pronunciations
in the 3rd column. The baseline decoding pronun-
ciation dictionary is constructed similarly from the
baseline training pronunciation dictionary.
</bodyText>
<sectionHeader confidence="0.998736" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.937397333333334">
To determine whether our pronunciation rules are
useful in speech processing applications, we eval-
uated their impact on two tasks, automatic phone
recognition and ASR. For our experiments, we used
the broadcast news TDT4 corpus (Arabic Set 1), di-
vided into 47.61 hours of speech (89 news shows)
for training and 5.18 hours (11 shows); test and
training shows were selected at random. Both train-
ing and test data were segmented based on silence
and non-speech segments and down-sampled to
8Khz.7 This segmentation produced 20,707 speech
segments for our training data and 2,255 segments
for testing.
7One of our goals is phone recognition telephone conversa-
tion for Arabic dialect identifaction, hence the down-sampling.
</bodyText>
<figure confidence="0.998522333333334">
Pronunciation Variants
/m adrasati/
/m adrasat/
/m adrasa/
2nd
Pronunciations for MADA
/m u d a r i s a t i/
/mudarisat/
/mudarisa/
2nd
Pronunciations for MADA
Undiacritized Orth. Diacritized MADA
mdrsp
!&amp;quot;#$
mudar&amp;quot;isapi
madorasapi
&amp; &apos;!&apos;&amp;quot;(#&apos;$
&amp; &apos;!)&amp;quot;&apos;#*$
</figure>
<page confidence="0.989084">
401
</page>
<subsectionHeader confidence="0.970714">
6.1 Acoustic Models
</subsectionHeader>
<bodyText confidence="0.999857">
Our monophone acoustic models are built using 3-
state continuous HMMs without state-skipping with
a mixture of 12 Gaussians per state. We extract
standard MFCC (Mel Frequency Cepstral Coeffi-
cients) features from 25 ms frames, with a frame
shift of 10 ms. Each feature vector is 39D: 13 fea-
tures (12 cepstral features plus energy), 13 deltas,
and 13 double-deltas. The features are normalized
using cepstral mean normalization. For our ASR
experiments, tied context-dependent cross-word tri-
phone HMMs are created with the same settings as
monophones. The acoustic models are speaker- and
gender-independent, trained using ML (maximum
likelihood) with flat-start.8 We build our framework
using the HMM Toolkit (HTK) (Young et al., 2006).
</bodyText>
<subsectionHeader confidence="0.993791">
6.2 Phone Recognition Evaluation
</subsectionHeader>
<bodyText confidence="0.95601656097561">
We hypothesize that improved pronunciation rules
will have a profound impact on phone recognition
accuracy. To compare our phone recognition (XPR)
system with the baseline (BASEPR), we train two
phone recognizers using HTK. The BASEPR rec-
ognizer uses the training-pronunciation dictionary
generated using the baseline rules; the XPR sys-
tem uses a pronunciation dictionary generated using
these rules plus our modified and new rules (cf. Sec-
tion 5). The two systems are identical except for
their pronunciation dictionaries.
We evaluate the two systems under two condi-
tions: (1) phone recognition with a bigram phone
language model (LM)9 and (2) phone recognition
with an open-loop phone recognizer, such that any
phoneme can follow any other phoneme with a uni-
form distribution. Results of this evaluation are pre-
sented in Table 1.
Ideally, we would like to compare the perfor-
mance of these systems against a common MSA
phonetically-transcribed gold standard. Unfortu-
nately, to our knowledge, such a data set does not
exist. So we approximate such a gold standard
on a blind test set through forced alignment, us-
ing the trained acoustic models and pronunciation
8Since our focus is a comparison of different approaches to
pronunciation modeling on Arabic recognition tasks, we have
not experimented with different features, parameters, and differ-
ent machine learning approaches (such as discriminative train-
ing and/or the combination of both).
9The bigram phoneme LM of each phone recognizer is
trained on the phonemes obtained from forced aligning the
training transcript to the speech data using that recognizer’s
training pronunciation dictionary and acoustic models.
dictionaries. Since our choice of acoustic model
(of BASEPR or XPR) and pronunciation dictionary
(again of BASEPR or XPR) can bias our results,
we consider four gold variants (GV) with differ-
ent combinations of acoustic model and pronunci-
ation dictionary, to set expected lower and upper
bounds. These combinations are represented in Ta-
ble 1 as GV1–4, where the source of acoustic mod-
els is BASEPR or XPR and source of pronuncia-
tion rules are BASEPR, XPR or XPR and BASEPR
combined. These GV are described in more detail
below, as we describe our results.
Since BASEPR system uses a pronunciation dic-
tionary with a one-to-one mapping of orthography
to phones, the GV1 phone sequence for any test
utterance’s orthographical transcript according to
BASEPR can be obtained directly from the ortho-
graphic transcript. Note that if, in fact, GV1 does
represent the true gold standard (i.e., the correct
phone sequence for the test utterances) then if XPR
obtains a lower phone error rate using this gold stan-
dard than BASEPR does, we can conclude that in
fact XPR’s acoustic models are better estimated.
This is in fact the case. In Table 1, first line, we
see that XPR under both conditions (open-loop and
bigram LM) significantly (p-value &lt; 2.2e−16) out-
performs the corresponding BASEPR phone recog-
nizer using GV1.10
If GV1 does not accurately represent the phone
sequences of the test data, then there must be some
phones in the GV1 sequences that should be deleted,
inserted, or substituted. On the hypothesis that our
training-pronunciation dictionary might improve the
BASEPR assignments, we enrich the baseline pro-
nunciation dictionary with XPR’s dictionary. Now,
we force-align the orthographic transcript using
this extended pronunciation dictionary, still using
BASEPR’s acoustic models, with the acoustic sig-
nal. We denote the output phone sequences as GV2.
If a pronunciation generated using the BASEPR dic-
tionary was already correct (in GV1) according to
the acoustic signal, this forced alignment process
still has the option of choosing it. We hypothesize
that the result, GV2, is a more accurate represen-
tation of the true phone sequences in the test data,
since it should be able to model the acoustic sig-
nal more accurately. On GV2, as on GV1, we see
that XPR, under both conditions, significantly (p-
</bodyText>
<footnote confidence="0.987127333333333">
10Throughout this discussion we use paired t-tests to measure
significant difference, where the sample values are the phone
recognizer accuracies on the utterances.
</footnote>
<page confidence="0.990457">
402
</page>
<table confidence="0.999433333333333">
Gold Variants Open-loop (Accuracy) Bigram Phone LM (Accuracy)
GV Acoustic Model of Pron. Dict. of BASEPR XPR BASEPR XPR
1 BASEPR BASEPR 37.40 39.21 41.56 45.17
2 BASEPR BASEPR+XPR 38.64 42.41 43.44 50.73
3 XPR XPR 37.06 42.38 42.21 51.41
4 XPR BASEPR+XPR 37.47 42.74 42.59 51.51
</table>
<tableCaption confidence="0.9691255">
Table 1: Comparing the effect of BASEPR and XPR pronunciation rules, alone and in combination, using 4 Gold
Variants under two conditions (Open-loop and LM)
</tableCaption>
<bodyText confidence="0.999868509433963">
value &lt; 2.2e − 16) outperforms the corresponding
BASEPR phone recognizers (see Table 1, second
line).
We also compared the performance of the two
systems using upper bound variants. For GV3 we
used the forced alignment of the orthographic tran-
scription using only XPR’s pronuncation dictionary
with XPR’s acoustic models. In GV4 we combine
the pronunciation dictionary of XPR with BASEPR
dictionary and use XPR’s acoustic models. Unsur-
prisingly, we find that the XPR recognizer signifi-
cantly (p-value &lt;2.2e − 16) outperforms BASEPR
when using these two variants under both conditions
(see Table 1, third and fourth lines).
The results presented in Table 1 compare the ro-
bustness of the acoustic models as well as the pro-
nunciation components of the two systems. We also
want to evaluate the accuracy of our pronunciation
predictions in representing the actual acoustic sig-
nal. One way to do this is to see how often the forced
alignment process choose phone sequences using
the BASEPR pronunciation dictionary as opposed
to XPR’s. We forced aligned the test transcript —
using the XPR acoustic models and only the XPR
pronunciation dictionary — with the acoustic sig-
nal. We then compare the output sequences to the
output of the forced alignment process where the
combined pronunciations from BASEPR+XPR and
the XPR acoustic models were used. We find that
the difference between the two is only 1.03% (with
246,376 phones, 557 deletions, 1696 substitutions,
and 277 insertions). Thus, adding the BASEPR rules
to XPR does not appear to contribute a great deal to
the representation chosen by forced alignment. In
a similar experiment, we use the BASEPR acous-
tic models instead of the XPR models and compare
the results of using BASEPR-pronunciation dictio-
nary with the combination of XPR+BASEPR’s dic-
tionaries for forced alignment. Interestingly, in this
experiment we do find a significantly larger differ-
ence between the two outputs 17.04% (with 233,787
phones, 1404 deletions, 14013 substitutions, and
27040 insertions). We can hypothesize from these
experiments that the baseline pronunciation dictio-
nary alone is not sufficient to represent the acoustic
signal accurately, since large numbers of phonemes
are edited when adding the XPR pronunciations. In
contrast, adding the BASEPR’s pronunciation dic-
tionary to XPR’s shows a relatively small percent-
age of edits, which suggests that the XPR pronun-
ciation dictionary extends and covers more accu-
rately the pronunciations already contained in the
BASEPR dictionary.
</bodyText>
<subsectionHeader confidence="0.996331">
6.3 Speech Recognition Evaluation
</subsectionHeader>
<bodyText confidence="0.999941666666667">
We have also conducted an ASR experiment to eval-
uate the usefulness of our pronunciation rules for
this application.11 We employ the baseline pro-
nunciation rules to generate the baseline training
and decoding pronunciation dictionaries. Using
these dictionaries, we build the baseline ASR sys-
tem (BASEWR). Using our extended pronunciation
rules, we generate our dictionaries and train our
ASR system (XWR). Both systems have the same
model settings, as described in Section 6.1. They
also share the same language model (LM), a trigram
LM trained on the undiacritized transcripts of the
training data and a subset of Arabic gigawords (ap-
proximately 281 million words, in total), using the
SRILM toolkit (Stolcke, 2002).
Table 2 presents the comparison of BASEWR
with the XWR system. In Section 5.1, we noted that
the top two choices from MADA may be included in
the XWR pronunciation dictionary when the differ-
ence in MADA confidence scores for these two is
less than a given threshold. So we analyze the im-
pact of including this second MADA option in both
the training and decoding dictionaries on ASR re-
sults. In all cases, whether the second MADA choice
</bodyText>
<footnote confidence="0.671996666666667">
11It should be noted that we have not attempted to build a
state-of-the-art Arabic speech recognizer; our goal is purely to
evaluate our approach to pronunciation modeling for Arabic.
</footnote>
<page confidence="0.998407">
403
</page>
<bodyText confidence="0.999974769230769">
is included or not, XWR significantly (p-values &lt;
8.1e-15) outperforms BASEWR. Our best results are
obtained when we include the top first and second
MADA option in the decoding pronunciation dictio-
nary but only the top MADA choice in the training
pronunciation dictionary. The difference between
this version of XWR and an XWR version which
includes the top second MADA choice in the train-
ing dictionary is significant (p-value = 0.017).
To evaluate the impact of the set of rules that gen-
erate additional pronunciation variants (described in
Section 4 - IV) on word recognition, we built a
system, denoted as XWR_I-III, that uses only the
first three sets of rules (I–III) and compared its per-
formance to that of both BASEWR and the corre-
sponding XWR system. As shown in Table 2, we
observe that XWR_I-III significantly outperforms
BASEWR in 2.27 (p-value &lt; 2.2e-16). Also, the
corresponding XWR that uses all the rules (includ-
ing IV set) significantly outperforms XWR_I-III in
1.24 (p-value &lt; 2.2e-16).
The undiacritized vocabulary size used in our ex-
periment was 34,511. We observe that 6.38% of
the words in the test data were out of vocabulary
(OOV), which may partly explain our low absolute
recognition accuracy. The dictionary size statistics
(for entries generated from the training data only)
used in these experiments are shown in Table 3. We
have done some error analysis to understand the rea-
son behind high absolute error rate for both systems.
We observe that many of the test utterances are very
noisy. We wanted to see whether XWR still out-
performs BASEWR if we remove these utterances.
Removing all utterances for which BASEWR ob-
tains an accuracy of less than 25%, we are left with
1720/2255 utterances. On these remaining utter-
ances, the BASEWR accuracy is 64.4% and XWR’s
accuracy is 67.23% — a significant difference de-
spite the bias in favor of BASEWR.
</bodyText>
<sectionHeader confidence="0.991326" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.998515">
In this paper we have shown that the use of more
linguistically motivated pronunciation rules can im-
prove phone recognition and word recognition re-
sults for MSA. We have described some of the pho-
netic, phonological, and morphological features of
MSA that are rarely modeled in ASR systems and
have developed a set of pronunciation rules that en-
capsulate these features. We have demonstrated how
the use of these rules can significantly improve both
MSA phone recognition and MSA word recognition
</bodyText>
<table confidence="0.996947857142857">
System Acc Corr Del Sub Ins
BASEWR 52.78 65.36 360 12297 4598
XWR_I–III (1 TD/DD) 55.05 66.84 324 11791 4308
XWR (1 TD/DD) 56.29 69.06 274 11031 4665
XWR (2 TD, 2 DD) 56.28 69.12 274 11008 4694
XWR (2 TD, 1 DD) 55.53 68.55 285 11206 4759
XWR (1 TD, 2 DD) 56.88 69.42 284 10891 4579
</table>
<tableCaption confidence="0.980251857142857">
Table 2: Comparing the performance of BASEWR to
XWR, where the top 1 or 2 MADA options are included
in the training dictionary (TD) and decoding dictionary
(DD). XWR I–III uses only the first three sets of pro-
nunciation rules in Section 4. Accuracy = (100 - WER);
Correct is Accuracy without counting insertions (%). To-
tal number of words is 36,538.
</tableCaption>
<table confidence="0.999828714285714">
Dictionary # entries PPW
BASEPR TD 45,117 1
BASEPR DD 44,383 1.3
XPR TD (MADA top 1) 80,200 1.78
XPR TD (MADA top 1 and 2) 128,663 2.85
XWR DD (MADA top 1) 71,853 2.08
XWR DD (MADA top 1 and 2) 105,402 3.05
</table>
<tableCaption confidence="0.7208805">
Table 3: Dictionary sizes generated fom the training data
only (PPW: pronunciations per word, TD: Training pro-
nunciation dictionary, DD: Decoding pronunciation dic-
tionary).
</tableCaption>
<bodyText confidence="0.999766">
accuracy by a series of experiments comparing our
XPR and XWR systems to the corresponding base-
line systems BASEPR and BASEWR. We obtain an
improvement in absolute accuracy in phone recogni-
tion of 3.77%–7.29% and a significant improvement
of 4.1% in absolute accuracy in ASR.
In future work, we will address several issues
which appear to hurt our recognition accuracy, such
as handling the words that MADA fails to analyze.
We also will develop a similar approach to handling
dialectical Arabic speech using the MAGEAD mor-
phological analyzer (Habash and Rambow, 2006).
A larger goal is to employ the MSA and dialectical
phone recognizers to aid in spoken Arabic dialect
identification using phonotactic modeling (see (Bi-
adsy et al., 2009)).
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9648235">
We are very thankful to Dimitra Vergyri for providing us
with the details of the baseline pronunciation dictionary
and for her useful suggestions. This material is based
upon work supported by the Defense Advanced Research
Projects Agency (DARPA) under Contract No. HR0011-
06-C-0023 (approved for public release, distribution un-
limited). Any opinions, findings and conclusions or rec-
ommendations expressed in this material are those of
the authors and do not necessarily reflect the views of
DARPA.
</bodyText>
<page confidence="0.998751">
404
</page>
<sectionHeader confidence="0.99589" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999947584415585">
M. Afify, L. Nguyen, B. Xiang, S. Abdou, and
J. Makhoul. 2005. Arabic broadcast news transcrip-
tion using a one million word vocalized vocabulary. In
Proceedings ofInterspeech 2005, pages 1637–1640.
S. Ananthakrishnan, S. Narayanan, and S. Bangalore.
2005. Automatic diacritization of arabic transcripts
for asr. In Proceedings ofICON, Kanpur, India.
F. Biadsy, J. Hirschberg, and N. Habash. 2009. Spo-
ken Arabic Dialect Identification Using Phonotactic
Modeling. In Proceedings of EACL 2009 Workshop
on Computational Approaches to Semitic Languages,
Athens, Greece.
T. Buckwalter. 2004. Buckwalter Arabic Morphologi-
cal Analyzer Version 2.0. Linguistic Data Consortium,
University of Pennsylvania, 2002. LDC Cat alog No.:
LDC2004L02, ISBN 1-58563-324-0.
Y. A. El-Imam. 2004. Phonetization of Arabic: rules
and algorithms. In Computer Speech and Language
18, pages 339–373.
N. Habash and O. Rambow. 2005. Arabic Tokeniza-
tion, Part-of-Speech Tagging and Morphological Dis-
ambiguation in One Fell Swoop. In Proceedings of the
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL’05), pages 573–580.
N. Habash and O. Rambow. 2006. MAGEAD: A Mor-
phological Analyzer and Generator for the Arabic Di-
alects. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 681–688, Sydney, Australia.
N. Habash and O. Rambow. 2007. Arabic Diacritization
through Full Morphological Tagging. In Proceedings
of the 8th Meeting of the North American Chapter of
the Association for Computational Linguistics/Human
Language Technologies Conference (HLT-NAACL07).
N. Habash, A. Soudi, and T. Buckwalter. 2007. On
Arabic Transliteration. In A. van den Bosch and
A. Soudi, editors, Arabic Computational Morphology:
Knowledge-based and Empirical Methods. Springer.
M. Maamouri, A. Bies, H. Jin, and T. Buckwalter.
2003. Arabic treebank: Part 1 v 2.0. Distributed by
the Linguistic Data Consortium. LDC Catalog No.:
LDC2003T06.
A. Messaoudi, J. L. Gauvain, and L. Lamel. 2006. Ara-
bic broadcast news transcription using a one million
word vocalized vocabulary. In Proceedings of ICASP
2006, volume 1, pages 1093–1096.
H. Soltau, G. Saon, D. Povey, L. Mangu, B. Kingsbury,
J. Kuo, M. Omar, and G. Zweig. 2007. The IBM 2006
GALE Arabic ASR System. In Proceedings ofICASP
2007.
S. Stolcke. 2002. Tokenization, Morphological Anal-
ysis, and Part-of-Speech Tagging for Arabic in One
Fell Swoop. In Proceedings ofICSLP 2002, volume 2,
pages 901–904.
D. Vergyri and K. Kirchhoff. 2004. Automatic Dia-
critization of Arabic for Acoustic Modeling in Speech
Recognition. In Ali Farghaly and Karine Megerdoo-
mian, editors, COLING 2004 Workshop on Computa-
tional Approaches to Arabic Script-based Languages,
pages 66–73, Geneva, Switzerland.
D. Vergyri, A. Mandal, W. Wang, A. Stolcke, J. Zheng,
M. Graciarena, D. Rybach, C. Gollan, R. Schluter,
K. Kirchhoff, A. Faria, and N. Morgan. 2008. Devel-
opment of the SRI/Nightingale Arabic ASR system. In
Proceedings ofInterspeech 2008, pages 1437–1440.
S. Young, G. Evermann, M. Gales, D. Kershaw,
G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev,
and P. Woodland. 2006. The HTK Book, version 3.4:
htk.eng.cam.ac.uk. Cambridge University Engineer-
ing Department.
I. Zitouni, J. S. Sorensen, and R. Sarikaya. 2006. Maxi-
mum Entropy Based Restoration of Arabic Diacritics.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 577–584, Sydney, Australia.
</reference>
<page confidence="0.999032">
405
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.517526">
<title confidence="0.9745435">Improving the Arabic Pronunciation Dictionary for Phone and Recognition with Linguistically-Based Pronunciation Rules</title>
<author confidence="0.630112">of Computer Science</author>
<author confidence="0.630112">Columbia University</author>
<author confidence="0.630112">New York</author>
<affiliation confidence="0.794827">for Computational Learning Systems, Columbia University, New York,</affiliation>
<email confidence="0.999912">habash@ccls.columbia.edu</email>
<abstract confidence="0.99762175">In this paper, we show that linguistically motivated pronunciation rules can improve phone and word recognition results for Modern Standard Arabic (MSA). Using these rules and the MADA morphological analysis and disambiguation tool, multiple pronunciations per word are automatically generated to build two pronunciation dictionaries; one for training and another for decoding. We demonstrate that the use of these rules can significantly improve both MSA phone recognition and MSA word recognition accuracies over a baseline system using pronunciation rules typically employed in previous work on MSA Automatic Speech Recognition (ASR). We obtain a significant improvement in absolute accuracy in phone recognition of 3.77%–7.29% and a significant improvement of 4.1% in absolute accuracy in ASR.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Afify</author>
<author>L Nguyen</author>
<author>B Xiang</author>
<author>S Abdou</author>
<author>J Makhoul</author>
</authors>
<title>Arabic broadcast news transcription using a one million word vocalized vocabulary.</title>
<date>2005</date>
<booktitle>In Proceedings ofInterspeech</booktitle>
<pages>1637--1640</pages>
<contexts>
<context position="4360" citStr="Afify et al., 2005" startWordPosition="639" endWordPosition="642">nd interact with orthographic variations. 397 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 397–405, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We conclude in Section 7 and identify directions for future research. 2 Related Work Most recent work on ASR for MSA uses a single pronunciation dictionary constructed by mapping every undiacritized word in the training corpus to all of the diacritized Buckwalter analyses and the diacritized versions of this word in the Arabic Treebank (Maamouri et al., 2003; Afify et al., 2005; Messaoudi et al., 2006; Soltau et al., 2007). In these papers, each diacritized word is converted to a single pronunciation with a one-to-one mapping using “very few” unspecified rules. None of these systems use morphological disambiguation to determine the most likely pronunciation of the word given its context. Vergyri et al. (2008) do use morphological information to predict word pronunciation. They select the top choice from the MADA (Morphological Analysis and Disambiguation for Arabic) system for each word to train their acoustic models. For the test lexicon they used the undiacritized</context>
</contexts>
<marker>Afify, Nguyen, Xiang, Abdou, Makhoul, 2005</marker>
<rawString>M. Afify, L. Nguyen, B. Xiang, S. Abdou, and J. Makhoul. 2005. Arabic broadcast news transcription using a one million word vocalized vocabulary. In Proceedings ofInterspeech 2005, pages 1637–1640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ananthakrishnan</author>
<author>S Narayanan</author>
<author>S Bangalore</author>
</authors>
<title>Automatic diacritization of arabic transcripts for asr.</title>
<date>2005</date>
<booktitle>In Proceedings ofICON, Kanpur,</booktitle>
<contexts>
<context position="8221" citStr="Ananthakrishnan et al., 2005" startWordPosition="1238" endWordPosition="1242">biguous. Diacritics are largely restricted to religious texts and Arabic language school textbooks. In other texts, fewer than 1.5% of words contain a diacritic. Some diacritics are lexical (where word meaning varies) and others are inflectional (where nominal case or verbal mood varies). Inflectional diacritics are typically word final. Since nominal case, verbal mood and nunation have all disappeared in spoken dialectal Arabic, Arabic speakers do not always produce these inflections correctly or at all. Much work has been done on automatic Arabic diacritization (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005; Zitouni et al., 2006; Habash and Rambow, 2007). In this paper, we use the MADA (Morphological Analysis and Disambiguation for Arabic) system to diacritize Arabic (Habash and Rambow, 2005; Habash and Rambow, 2007). MADA, which uses the Buckwalter Arabic morphological Analyzer databases (Buckwalter, 2004), provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the special symbol “{” for Hamzat-Wasl. 398 3.2 Hamza Spelling The consonant Hamza (glottal stop /G/) has multiple forms in Arabic script:_′,1&gt;,1&lt;,</context>
</contexts>
<marker>Ananthakrishnan, Narayanan, Bangalore, 2005</marker>
<rawString>S. Ananthakrishnan, S. Narayanan, and S. Bangalore. 2005. Automatic diacritization of arabic transcripts for asr. In Proceedings ofICON, Kanpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Biadsy</author>
<author>J Hirschberg</author>
<author>N Habash</author>
</authors>
<title>Spoken Arabic Dialect Identification Using Phonotactic Modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009 Workshop on Computational Approaches to Semitic Languages,</booktitle>
<location>Athens, Greece.</location>
<marker>Biadsy, Hirschberg, Habash, 2009</marker>
<rawString>F. Biadsy, J. Hirschberg, and N. Habash. 2009. Spoken Arabic Dialect Identification Using Phonotactic Modeling. In Proceedings of EACL 2009 Workshop on Computational Approaches to Semitic Languages, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Buckwalter Arabic Morphological Analyzer Version 2.0. Linguistic Data Consortium,</title>
<date>2004</date>
<booktitle>LDC Cat alog No.: LDC2004L02, ISBN</booktitle>
<pages>1--58563</pages>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="5996" citStr="Buckwalter, 2004" startWordPosition="893" endWordPosition="894">owels). The Arabic script has 36 basic letters (ignoring ligatures) and 9 diacritics. Most Arabic letters have a one-to-one mapping to an MSA phoneme; however, there are a small number of common exceptions (Habash et al., 2007; El-Imam, 2004) which we summarize next. 3.1 Optional Diacritics Arabic script commonly uses nine optional diacritics: (a) three short-vowel diacritics representing the vowels /a/, /u/ and /i/; (b) one long-vowel diacritic (Dagger Alif ‘) representing the long vowel /A/ in a 2We provide Arabic script orthographic transliteration in the Buckwalter transliteration scheme (Buckwalter, 2004). For Modern Standard Arabic phonological transcription, we use a variant of the Buckwalter transliteration with the following exceptions: glottal stops are represented as /G/ and long vowels as /A/, /U/ and /I/. All Arabic script diacritics are phonologically spelled out. small number of words; (c) three nunation diacritics (F /an/, N /un/, K /in/) representing a combination of a short vowel and the nominal indefiniteness marker /n/ in MSA; (d) one consonant lengthening diacritic (called Shadda —) which repeats/elongates the previous consonant (e.g., kat—ab is pronounced /kattab/); and (e) on</context>
<context position="8527" citStr="Buckwalter, 2004" startWordPosition="1287" endWordPosition="1288"> typically word final. Since nominal case, verbal mood and nunation have all disappeared in spoken dialectal Arabic, Arabic speakers do not always produce these inflections correctly or at all. Much work has been done on automatic Arabic diacritization (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005; Zitouni et al., 2006; Habash and Rambow, 2007). In this paper, we use the MADA (Morphological Analysis and Disambiguation for Arabic) system to diacritize Arabic (Habash and Rambow, 2005; Habash and Rambow, 2007). MADA, which uses the Buckwalter Arabic morphological Analyzer databases (Buckwalter, 2004), provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the special symbol “{” for Hamzat-Wasl. 398 3.2 Hamza Spelling The consonant Hamza (glottal stop /G/) has multiple forms in Arabic script:_′,1&gt;,1&lt;,_,&amp;,ò }, 1|. There are complex rules for Hamza spelling that primarily depend on its vocalic context. For example, ò } is used word medially and finally when preceded or followed by an /i/ vowel. Similarly, the Hamza form 1 |is used when the Hamza is followed by the long vowel /A/. Hamza spelling is furth</context>
</contexts>
<marker>Buckwalter, 2004</marker>
<rawString>T. Buckwalter. 2004. Buckwalter Arabic Morphological Analyzer Version 2.0. Linguistic Data Consortium, University of Pennsylvania, 2002. LDC Cat alog No.: LDC2004L02, ISBN 1-58563-324-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A El-Imam</author>
</authors>
<title>Phonetization of Arabic: rules and algorithms.</title>
<date>2004</date>
<journal>In Computer Speech and Language</journal>
<volume>18</volume>
<pages>339--373</pages>
<contexts>
<context position="1537" citStr="El-Imam, 2004" startWordPosition="217" endWordPosition="218">g pronunciation rules typically employed in previous work on MSA Automatic Speech Recognition (ASR). We obtain a significant improvement in absolute accuracy in phone recognition of 3.77%–7.29% and a significant improvement of 4.1% in absolute accuracy in ASR. 1 Introduction The correspondence between orthography and pronunciation in Modern Standard Arabic (MSA) falls somewhere between that of languages such as Spanish and Finnish, which have an almost one-to-one mapping between letters and sounds, and languages such as English and French, which exhibit a more complex letter-to-sound mapping (El-Imam, 2004). The more complex this mapping is, the more difficult the language is for Automatic Speech Recognition (ASR). An essential component of an ASR system is its pronunciation dictionary (lexicon), which maps the orthographic representation of words to their phonetic or phonemic pronunciation variants. For languages with complex letter-to-sound mappings, such dictionaries are typically written by hand. However, for morphologically rich languages, such as MSA,1 pronunciation dictionaries are difficult to create by hand, because of the large number of word forms, each of which has a large number of </context>
<context position="5621" citStr="El-Imam, 2004" startWordPosition="840" endWordPosition="841">und for each word in the training data as possible pronunciation variants. We use this system as our baseline for comparison. 3 Arabic Orthography and Pronunciation MSA is written in a morpho-phonemic orthographic representation using the Arabic script, an alphabet accented with optional diacritical marks.2 MSA has 34 phonemes (28 consonants, 3 long vowels and 3 short vowels). The Arabic script has 36 basic letters (ignoring ligatures) and 9 diacritics. Most Arabic letters have a one-to-one mapping to an MSA phoneme; however, there are a small number of common exceptions (Habash et al., 2007; El-Imam, 2004) which we summarize next. 3.1 Optional Diacritics Arabic script commonly uses nine optional diacritics: (a) three short-vowel diacritics representing the vowels /a/, /u/ and /i/; (b) one long-vowel diacritic (Dagger Alif ‘) representing the long vowel /A/ in a 2We provide Arabic script orthographic transliteration in the Buckwalter transliteration scheme (Buckwalter, 2004). For Modern Standard Arabic phonological transcription, we use a variant of the Buckwalter transliteration with the following exceptions: glottal stops are represented as /G/ and long vowels as /A/, /U/ and /I/. All Arabic s</context>
</contexts>
<marker>El-Imam, 2004</marker>
<rawString>Y. A. El-Imam. 2004. Phonetization of Arabic: rules and algorithms. In Computer Speech and Language 18, pages 339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>573--580</pages>
<contexts>
<context position="2469" citStr="Habash and Rambow, 2005" startWordPosition="354" endWordPosition="357">ages with complex letter-to-sound mappings, such dictionaries are typically written by hand. However, for morphologically rich languages, such as MSA,1 pronunciation dictionaries are difficult to create by hand, because of the large number of word forms, each of which has a large number of possible pronunciations. Fortunately, the relationship between orthography and pronunciation is relatively regular and well understood for MSA. Moreover, recent automatic techniques for morphological analysis and disambiguation (MADA) can also be useful in automating part of the dictionary creation process (Habash and Rambow, 2005; Habash and Rambow, 2007) Nonetheless, most documented Arabic ASR systems appear to handle only a subset of Arabic phonetic phenomena; very few use morphological disambiguation tools. In Section 2, we briefly describe related work, including the baseline system we use. In Section 3, we outline the linguistic phenomena we believe are critical to improving MSA pronunciation dictionaries. In Section 4, we describe the pronunciation rules we have developed based upon these linguistic phenomena. In Section 5, we describe how these rules are used, together with MADA, to build our pronunciation dict</context>
<context position="8409" citStr="Habash and Rambow, 2005" startWordPosition="1269" endWordPosition="1272">ere word meaning varies) and others are inflectional (where nominal case or verbal mood varies). Inflectional diacritics are typically word final. Since nominal case, verbal mood and nunation have all disappeared in spoken dialectal Arabic, Arabic speakers do not always produce these inflections correctly or at all. Much work has been done on automatic Arabic diacritization (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005; Zitouni et al., 2006; Habash and Rambow, 2007). In this paper, we use the MADA (Morphological Analysis and Disambiguation for Arabic) system to diacritize Arabic (Habash and Rambow, 2005; Habash and Rambow, 2007). MADA, which uses the Buckwalter Arabic morphological Analyzer databases (Buckwalter, 2004), provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the special symbol “{” for Hamzat-Wasl. 398 3.2 Hamza Spelling The consonant Hamza (glottal stop /G/) has multiple forms in Arabic script:_′,1&gt;,1&lt;,_,&amp;,ò }, 1|. There are complex rules for Hamza spelling that primarily depend on its vocalic context. For example, ò } is used word medially and finally when preceded or followed by an /i/</context>
<context position="18768" citStr="Habash and Rambow, 2005" startWordPosition="2962" endWordPosition="2965">DA, when the difference between the top two choices is less than a threshold determined empirically (in our implementation we chose 0.2). In Figure 1, the training pronunciation dictionary maps the 2nd column (the entry keys) to the 3rd column. We generate the baseline training pronunciation dictionary using only the baseline rules from Section 4. This dictionary also makes use of MADA, but it maps the MADA-diacritized word to only one pronunciation. The baseline training dictionary maps the 2nd column (the entry keys) to only one pronunciation in the 3rd column in Figure 1. 5The MADA system (Habash and Rambow, 2005; Habash and Rambow, 2007) reports 4.8% diacritic error rate (DER) on all diacritics and 2.2% (DER) when ignoring the last (inflectional) diacritic. 6In our training data, only about 1% of all words are not diacritized because of lack of coverage in the morphological analysis component. Figure 1: Mapping an undiacritized word to MADA outputs to possible pronunciations. 5.2 Decoding Pronunciation Dictionary The decoding pronunciation dictionary is used in ASR to build the pronunciation models while decoding. Since, as noted above, it is standard to produce unvocalized transcripts when recognizi</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>N. Habash and O. Rambow. 2005. Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 573–580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>MAGEAD: A Morphological Analyzer and Generator for the Arabic Dialects.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>681--688</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="34178" citStr="Habash and Rambow, 2006" startWordPosition="5451" endWordPosition="5454">ion dictionary, DD: Decoding pronunciation dictionary). accuracy by a series of experiments comparing our XPR and XWR systems to the corresponding baseline systems BASEPR and BASEWR. We obtain an improvement in absolute accuracy in phone recognition of 3.77%–7.29% and a significant improvement of 4.1% in absolute accuracy in ASR. In future work, we will address several issues which appear to hurt our recognition accuracy, such as handling the words that MADA fails to analyze. We also will develop a similar approach to handling dialectical Arabic speech using the MAGEAD morphological analyzer (Habash and Rambow, 2006). A larger goal is to employ the MSA and dialectical phone recognizers to aid in spoken Arabic dialect identification using phonotactic modeling (see (Biadsy et al., 2009)). Acknowledgments We are very thankful to Dimitra Vergyri for providing us with the details of the baseline pronunciation dictionary and for her useful suggestions. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011- 06-C-0023 (approved for public release, distribution unlimited). Any opinions, findings and conclusions or recommendations expressed in </context>
</contexts>
<marker>Habash, Rambow, 2006</marker>
<rawString>N. Habash and O. Rambow. 2006. MAGEAD: A Morphological Analyzer and Generator for the Arabic Dialects. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages 681–688, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic Diacritization through Full Morphological Tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th Meeting of the North American Chapter of the Association for Computational Linguistics/Human Language Technologies Conference (HLT-NAACL07).</booktitle>
<contexts>
<context position="2495" citStr="Habash and Rambow, 2007" startWordPosition="358" endWordPosition="361">to-sound mappings, such dictionaries are typically written by hand. However, for morphologically rich languages, such as MSA,1 pronunciation dictionaries are difficult to create by hand, because of the large number of word forms, each of which has a large number of possible pronunciations. Fortunately, the relationship between orthography and pronunciation is relatively regular and well understood for MSA. Moreover, recent automatic techniques for morphological analysis and disambiguation (MADA) can also be useful in automating part of the dictionary creation process (Habash and Rambow, 2005; Habash and Rambow, 2007) Nonetheless, most documented Arabic ASR systems appear to handle only a subset of Arabic phonetic phenomena; very few use morphological disambiguation tools. In Section 2, we briefly describe related work, including the baseline system we use. In Section 3, we outline the linguistic phenomena we believe are critical to improving MSA pronunciation dictionaries. In Section 4, we describe the pronunciation rules we have developed based upon these linguistic phenomena. In Section 5, we describe how these rules are used, together with MADA, to build our pronunciation dictionaries for training and </context>
<context position="8269" citStr="Habash and Rambow, 2007" startWordPosition="1247" endWordPosition="1250">ous texts and Arabic language school textbooks. In other texts, fewer than 1.5% of words contain a diacritic. Some diacritics are lexical (where word meaning varies) and others are inflectional (where nominal case or verbal mood varies). Inflectional diacritics are typically word final. Since nominal case, verbal mood and nunation have all disappeared in spoken dialectal Arabic, Arabic speakers do not always produce these inflections correctly or at all. Much work has been done on automatic Arabic diacritization (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005; Zitouni et al., 2006; Habash and Rambow, 2007). In this paper, we use the MADA (Morphological Analysis and Disambiguation for Arabic) system to diacritize Arabic (Habash and Rambow, 2005; Habash and Rambow, 2007). MADA, which uses the Buckwalter Arabic morphological Analyzer databases (Buckwalter, 2004), provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the special symbol “{” for Hamzat-Wasl. 398 3.2 Hamza Spelling The consonant Hamza (glottal stop /G/) has multiple forms in Arabic script:_′,1&gt;,1&lt;,_,&amp;,ò }, 1|. There are complex rules for Hamza s</context>
<context position="18794" citStr="Habash and Rambow, 2007" startWordPosition="2966" endWordPosition="2969">etween the top two choices is less than a threshold determined empirically (in our implementation we chose 0.2). In Figure 1, the training pronunciation dictionary maps the 2nd column (the entry keys) to the 3rd column. We generate the baseline training pronunciation dictionary using only the baseline rules from Section 4. This dictionary also makes use of MADA, but it maps the MADA-diacritized word to only one pronunciation. The baseline training dictionary maps the 2nd column (the entry keys) to only one pronunciation in the 3rd column in Figure 1. 5The MADA system (Habash and Rambow, 2005; Habash and Rambow, 2007) reports 4.8% diacritic error rate (DER) on all diacritics and 2.2% (DER) when ignoring the last (inflectional) diacritic. 6In our training data, only about 1% of all words are not diacritized because of lack of coverage in the morphological analysis component. Figure 1: Mapping an undiacritized word to MADA outputs to possible pronunciations. 5.2 Decoding Pronunciation Dictionary The decoding pronunciation dictionary is used in ASR to build the pronunciation models while decoding. Since, as noted above, it is standard to produce unvocalized transcripts when recognizing MSA, we must map word p</context>
</contexts>
<marker>Habash, Rambow, 2007</marker>
<rawString>N. Habash and O. Rambow. 2007. Arabic Diacritization through Full Morphological Tagging. In Proceedings of the 8th Meeting of the North American Chapter of the Association for Computational Linguistics/Human Language Technologies Conference (HLT-NAACL07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>A Soudi</author>
<author>T Buckwalter</author>
</authors>
<title>On Arabic Transliteration.</title>
<date>2007</date>
<booktitle>Arabic Computational Morphology: Knowledge-based and Empirical Methods.</booktitle>
<editor>In A. van den Bosch and A. Soudi, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="5605" citStr="Habash et al., 2007" startWordPosition="836" endWordPosition="839">ll diacritizations found for each word in the training data as possible pronunciation variants. We use this system as our baseline for comparison. 3 Arabic Orthography and Pronunciation MSA is written in a morpho-phonemic orthographic representation using the Arabic script, an alphabet accented with optional diacritical marks.2 MSA has 34 phonemes (28 consonants, 3 long vowels and 3 short vowels). The Arabic script has 36 basic letters (ignoring ligatures) and 9 diacritics. Most Arabic letters have a one-to-one mapping to an MSA phoneme; however, there are a small number of common exceptions (Habash et al., 2007; El-Imam, 2004) which we summarize next. 3.1 Optional Diacritics Arabic script commonly uses nine optional diacritics: (a) three short-vowel diacritics representing the vowels /a/, /u/ and /i/; (b) one long-vowel diacritic (Dagger Alif ‘) representing the long vowel /A/ in a 2We provide Arabic script orthographic transliteration in the Buckwalter transliteration scheme (Buckwalter, 2004). For Modern Standard Arabic phonological transcription, we use a variant of the Buckwalter transliteration with the following exceptions: glottal stops are represented as /G/ and long vowels as /A/, /U/ and /</context>
</contexts>
<marker>Habash, Soudi, Buckwalter, 2007</marker>
<rawString>N. Habash, A. Soudi, and T. Buckwalter. 2007. On Arabic Transliteration. In A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology: Knowledge-based and Empirical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>H Jin</author>
<author>T Buckwalter</author>
</authors>
<date>2003</date>
<booktitle>Arabic treebank: Part 1 v 2.0. Distributed by the Linguistic Data Consortium. LDC Catalog No.: LDC2003T06.</booktitle>
<contexts>
<context position="4340" citStr="Maamouri et al., 2003" startWordPosition="635" endWordPosition="638">r in word orthography and interact with orthographic variations. 397 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 397–405, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We conclude in Section 7 and identify directions for future research. 2 Related Work Most recent work on ASR for MSA uses a single pronunciation dictionary constructed by mapping every undiacritized word in the training corpus to all of the diacritized Buckwalter analyses and the diacritized versions of this word in the Arabic Treebank (Maamouri et al., 2003; Afify et al., 2005; Messaoudi et al., 2006; Soltau et al., 2007). In these papers, each diacritized word is converted to a single pronunciation with a one-to-one mapping using “very few” unspecified rules. None of these systems use morphological disambiguation to determine the most likely pronunciation of the word given its context. Vergyri et al. (2008) do use morphological information to predict word pronunciation. They select the top choice from the MADA (Morphological Analysis and Disambiguation for Arabic) system for each word to train their acoustic models. For the test lexicon they us</context>
</contexts>
<marker>Maamouri, Bies, Jin, Buckwalter, 2003</marker>
<rawString>M. Maamouri, A. Bies, H. Jin, and T. Buckwalter. 2003. Arabic treebank: Part 1 v 2.0. Distributed by the Linguistic Data Consortium. LDC Catalog No.: LDC2003T06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Messaoudi</author>
<author>J L Gauvain</author>
<author>L Lamel</author>
</authors>
<title>Arabic broadcast news transcription using a one million word vocalized vocabulary.</title>
<date>2006</date>
<booktitle>In Proceedings of ICASP 2006,</booktitle>
<volume>1</volume>
<pages>1093--1096</pages>
<contexts>
<context position="4384" citStr="Messaoudi et al., 2006" startWordPosition="643" endWordPosition="646">hographic variations. 397 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 397–405, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We conclude in Section 7 and identify directions for future research. 2 Related Work Most recent work on ASR for MSA uses a single pronunciation dictionary constructed by mapping every undiacritized word in the training corpus to all of the diacritized Buckwalter analyses and the diacritized versions of this word in the Arabic Treebank (Maamouri et al., 2003; Afify et al., 2005; Messaoudi et al., 2006; Soltau et al., 2007). In these papers, each diacritized word is converted to a single pronunciation with a one-to-one mapping using “very few” unspecified rules. None of these systems use morphological disambiguation to determine the most likely pronunciation of the word given its context. Vergyri et al. (2008) do use morphological information to predict word pronunciation. They select the top choice from the MADA (Morphological Analysis and Disambiguation for Arabic) system for each word to train their acoustic models. For the test lexicon they used the undiacritized orthography, as well as</context>
</contexts>
<marker>Messaoudi, Gauvain, Lamel, 2006</marker>
<rawString>A. Messaoudi, J. L. Gauvain, and L. Lamel. 2006. Arabic broadcast news transcription using a one million word vocalized vocabulary. In Proceedings of ICASP 2006, volume 1, pages 1093–1096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Soltau</author>
<author>G Saon</author>
<author>D Povey</author>
<author>L Mangu</author>
<author>B Kingsbury</author>
<author>J Kuo</author>
<author>M Omar</author>
<author>G Zweig</author>
</authors>
<date>2007</date>
<booktitle>The IBM 2006 GALE Arabic ASR System. In Proceedings ofICASP</booktitle>
<contexts>
<context position="4406" citStr="Soltau et al., 2007" startWordPosition="647" endWordPosition="650">7 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 397–405, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We conclude in Section 7 and identify directions for future research. 2 Related Work Most recent work on ASR for MSA uses a single pronunciation dictionary constructed by mapping every undiacritized word in the training corpus to all of the diacritized Buckwalter analyses and the diacritized versions of this word in the Arabic Treebank (Maamouri et al., 2003; Afify et al., 2005; Messaoudi et al., 2006; Soltau et al., 2007). In these papers, each diacritized word is converted to a single pronunciation with a one-to-one mapping using “very few” unspecified rules. None of these systems use morphological disambiguation to determine the most likely pronunciation of the word given its context. Vergyri et al. (2008) do use morphological information to predict word pronunciation. They select the top choice from the MADA (Morphological Analysis and Disambiguation for Arabic) system for each word to train their acoustic models. For the test lexicon they used the undiacritized orthography, as well as all diacritizations f</context>
</contexts>
<marker>Soltau, Saon, Povey, Mangu, Kingsbury, Kuo, Omar, Zweig, 2007</marker>
<rawString>H. Soltau, G. Saon, D. Povey, L. Mangu, B. Kingsbury, J. Kuo, M. Omar, and G. Zweig. 2007. The IBM 2006 GALE Arabic ASR System. In Proceedings ofICASP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Stolcke</author>
</authors>
<title>Tokenization, Morphological Analysis, and Part-of-Speech Tagging for Arabic in One Fell Swoop.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP 2002,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="29570" citStr="Stolcke, 2002" startWordPosition="4663" endWordPosition="4664"> application.11 We employ the baseline pronunciation rules to generate the baseline training and decoding pronunciation dictionaries. Using these dictionaries, we build the baseline ASR system (BASEWR). Using our extended pronunciation rules, we generate our dictionaries and train our ASR system (XWR). Both systems have the same model settings, as described in Section 6.1. They also share the same language model (LM), a trigram LM trained on the undiacritized transcripts of the training data and a subset of Arabic gigawords (approximately 281 million words, in total), using the SRILM toolkit (Stolcke, 2002). Table 2 presents the comparison of BASEWR with the XWR system. In Section 5.1, we noted that the top two choices from MADA may be included in the XWR pronunciation dictionary when the difference in MADA confidence scores for these two is less than a given threshold. So we analyze the impact of including this second MADA option in both the training and decoding dictionaries on ASR results. In all cases, whether the second MADA choice 11It should be noted that we have not attempted to build a state-of-the-art Arabic speech recognizer; our goal is purely to evaluate our approach to pronunciatio</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>S. Stolcke. 2002. Tokenization, Morphological Analysis, and Part-of-Speech Tagging for Arabic in One Fell Swoop. In Proceedings ofICSLP 2002, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vergyri</author>
<author>K Kirchhoff</author>
</authors>
<title>Automatic Diacritization of Arabic for Acoustic Modeling in Speech Recognition.</title>
<date>2004</date>
<booktitle>In Ali Farghaly and Karine Megerdoomian, editors, COLING 2004 Workshop on Computational Approaches to Arabic Script-based Languages,</booktitle>
<pages>66--73</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="8191" citStr="Vergyri and Kirchhoff, 2004" startWordPosition="1234" endWordPosition="1237">is makes these two letters ambiguous. Diacritics are largely restricted to religious texts and Arabic language school textbooks. In other texts, fewer than 1.5% of words contain a diacritic. Some diacritics are lexical (where word meaning varies) and others are inflectional (where nominal case or verbal mood varies). Inflectional diacritics are typically word final. Since nominal case, verbal mood and nunation have all disappeared in spoken dialectal Arabic, Arabic speakers do not always produce these inflections correctly or at all. Much work has been done on automatic Arabic diacritization (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005; Zitouni et al., 2006; Habash and Rambow, 2007). In this paper, we use the MADA (Morphological Analysis and Disambiguation for Arabic) system to diacritize Arabic (Habash and Rambow, 2005; Habash and Rambow, 2007). MADA, which uses the Buckwalter Arabic morphological Analyzer databases (Buckwalter, 2004), provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the special symbol “{” for Hamzat-Wasl. 398 3.2 Hamza Spelling The consonant Hamza (glottal stop /G/) has multiple fo</context>
</contexts>
<marker>Vergyri, Kirchhoff, 2004</marker>
<rawString>D. Vergyri and K. Kirchhoff. 2004. Automatic Diacritization of Arabic for Acoustic Modeling in Speech Recognition. In Ali Farghaly and Karine Megerdoomian, editors, COLING 2004 Workshop on Computational Approaches to Arabic Script-based Languages, pages 66–73, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vergyri</author>
<author>A Mandal</author>
<author>W Wang</author>
<author>A Stolcke</author>
<author>J Zheng</author>
<author>M Graciarena</author>
<author>D Rybach</author>
<author>C Gollan</author>
<author>R Schluter</author>
<author>K Kirchhoff</author>
<author>A Faria</author>
<author>N Morgan</author>
</authors>
<title>Development of the SRI/Nightingale Arabic ASR system.</title>
<date>2008</date>
<booktitle>In Proceedings ofInterspeech</booktitle>
<pages>1437--1440</pages>
<contexts>
<context position="4698" citStr="Vergyri et al. (2008)" startWordPosition="693" endWordPosition="696">work on ASR for MSA uses a single pronunciation dictionary constructed by mapping every undiacritized word in the training corpus to all of the diacritized Buckwalter analyses and the diacritized versions of this word in the Arabic Treebank (Maamouri et al., 2003; Afify et al., 2005; Messaoudi et al., 2006; Soltau et al., 2007). In these papers, each diacritized word is converted to a single pronunciation with a one-to-one mapping using “very few” unspecified rules. None of these systems use morphological disambiguation to determine the most likely pronunciation of the word given its context. Vergyri et al. (2008) do use morphological information to predict word pronunciation. They select the top choice from the MADA (Morphological Analysis and Disambiguation for Arabic) system for each word to train their acoustic models. For the test lexicon they used the undiacritized orthography, as well as all diacritizations found for each word in the training data as possible pronunciation variants. We use this system as our baseline for comparison. 3 Arabic Orthography and Pronunciation MSA is written in a morpho-phonemic orthographic representation using the Arabic script, an alphabet accented with optional di</context>
<context position="12645" citStr="Vergyri et al., 2008" startWordPosition="1993" endWordPosition="1996">he rule. I. Shared Pronunciation Rules 1. Dagger Alif: ‘ → /A/ (e.g., h‘*A → hA*A) (This rule affected 1.8% of all the words in our training data) 2. Madda: |→ /G A/ (e.g., Al|n → AlGAn) (affected 1.9%) 3. Nunation: AF → /a n/, F → /a n/, /K/ → /i n/, N → /u n/ (e.g., kutubAF → kutuban) (affected 9.7%) 4. Hamza: All Hamza forms: ′, }, &amp;, &lt;, &gt; → /G/ (e.g., &gt;kala → Gakala) (affected 21.3%) 3Our script that generates the pronunciation dictionaries from MADA output can be downloaded from www.cs.columbia.edu/speech/software.cgi. 4We have attempted to replicate the baseline pronunciation rules for (Vergyri et al., 2008) based on published work and personal communications with the authors. 399 5. Ta-Marbuta: p —* /t/ (e.g., madrasapa —* madrasata) (affected 15.3%) As a post-processing step in all systems, we remove the Sukun diacritic and convert every letter X to phoneme /X/. In XPR and XWR, we also remove short vowels that precede or succeed a long vowel. II. Modified Pronunciation Rules 1. Alif-Maqsura: Y —* /a/ (e.g., salomY —* saloma) (affected 4.2%) (Baseline: Y —* /A/) 2. Shadda: Shadda is always removed (e.g., ba$—ara —* ba$ara) (affected 23.8%) (Baseline: the consonant was doubled) 3. U and I: uwo —*</context>
</contexts>
<marker>Vergyri, Mandal, Wang, Stolcke, Zheng, Graciarena, Rybach, Gollan, Schluter, Kirchhoff, Faria, Morgan, 2008</marker>
<rawString>D. Vergyri, A. Mandal, W. Wang, A. Stolcke, J. Zheng, M. Graciarena, D. Rybach, C. Gollan, R. Schluter, K. Kirchhoff, A. Faria, and N. Morgan. 2008. Development of the SRI/Nightingale Arabic ASR system. In Proceedings ofInterspeech 2008, pages 1437–1440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>G Evermann</author>
<author>M Gales</author>
<author>D Kershaw</author>
<author>G Moore</author>
<author>J Odell</author>
<author>D Ollason</author>
<author>D Povey</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<title>The HTK Book, version 3.4: htk.eng.cam.ac.uk.</title>
<date>2006</date>
<institution>Cambridge University Engineering Department.</institution>
<contexts>
<context position="14930" citStr="Young et al., 2006" startWordPosition="2362" endWordPosition="2365">ation (e.g., yaktubu —* yaktub (affected 60.9%) 5 Building the Pronunciation Dictionaries As noted above, pronunciation dictionaries map words to one or more phonetically expressed pronunciation variants. These dictionaries are used for training and decoding in ASR systems. Typically, most data available to train large vocabulary ASR systems is orthographically (not phonetically) transcribed. There are two well-known alternatives for training acoustic models in ASR: (1) bootstrap training, when some phonetically annotated data is available, and (2) flat-start, when such data is not available (Young et al., 2006). In flat-start training, for example, the pronunciation dictionary is used to map the orthographic transcription of the training data to a sequence of phonetic labels to train the initial monophone models. Next, the dictionary is employed again to produce networks of possible pronunciations which can be used in forced alignment to obtain the most likely phone sequence that matches the acoustic data. Finally, the monophone acoustic models are re-estimated. In our work, we refer to this dictionary as the training pronunciation dictionary. The second usage of the pronunciation dictionary is to g</context>
<context position="21676" citStr="Young et al., 2006" startWordPosition="3413" endWordPosition="3416">aussians per state. We extract standard MFCC (Mel Frequency Cepstral Coefficients) features from 25 ms frames, with a frame shift of 10 ms. Each feature vector is 39D: 13 features (12 cepstral features plus energy), 13 deltas, and 13 double-deltas. The features are normalized using cepstral mean normalization. For our ASR experiments, tied context-dependent cross-word triphone HMMs are created with the same settings as monophones. The acoustic models are speaker- and gender-independent, trained using ML (maximum likelihood) with flat-start.8 We build our framework using the HMM Toolkit (HTK) (Young et al., 2006). 6.2 Phone Recognition Evaluation We hypothesize that improved pronunciation rules will have a profound impact on phone recognition accuracy. To compare our phone recognition (XPR) system with the baseline (BASEPR), we train two phone recognizers using HTK. The BASEPR recognizer uses the training-pronunciation dictionary generated using the baseline rules; the XPR system uses a pronunciation dictionary generated using these rules plus our modified and new rules (cf. Section 5). The two systems are identical except for their pronunciation dictionaries. We evaluate the two systems under two con</context>
</contexts>
<marker>Young, Evermann, Gales, Kershaw, Moore, Odell, Ollason, Povey, Valtchev, Woodland, 2006</marker>
<rawString>S. Young, G. Evermann, M. Gales, D. Kershaw, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland. 2006. The HTK Book, version 3.4: htk.eng.cam.ac.uk. Cambridge University Engineering Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>J S Sorensen</author>
<author>R Sarikaya</author>
</authors>
<title>Maximum Entropy Based Restoration of Arabic Diacritics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>577--584</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="8243" citStr="Zitouni et al., 2006" startWordPosition="1243" endWordPosition="1246">y restricted to religious texts and Arabic language school textbooks. In other texts, fewer than 1.5% of words contain a diacritic. Some diacritics are lexical (where word meaning varies) and others are inflectional (where nominal case or verbal mood varies). Inflectional diacritics are typically word final. Since nominal case, verbal mood and nunation have all disappeared in spoken dialectal Arabic, Arabic speakers do not always produce these inflections correctly or at all. Much work has been done on automatic Arabic diacritization (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005; Zitouni et al., 2006; Habash and Rambow, 2007). In this paper, we use the MADA (Morphological Analysis and Disambiguation for Arabic) system to diacritize Arabic (Habash and Rambow, 2005; Habash and Rambow, 2007). MADA, which uses the Buckwalter Arabic morphological Analyzer databases (Buckwalter, 2004), provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the special symbol “{” for Hamzat-Wasl. 398 3.2 Hamza Spelling The consonant Hamza (glottal stop /G/) has multiple forms in Arabic script:_′,1&gt;,1&lt;,_,&amp;,ò }, 1|. There are</context>
</contexts>
<marker>Zitouni, Sorensen, Sarikaya, 2006</marker>
<rawString>I. Zitouni, J. S. Sorensen, and R. Sarikaya. 2006. Maximum Entropy Based Restoration of Arabic Diacritics. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 577–584, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>