<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990459">
Translation as Weighted Deduction
</title>
<author confidence="0.996442">
Adam Lopez
</author>
<affiliation confidence="0.998933">
University of Edinburgh
</affiliation>
<address confidence="0.986236333333333">
10 Crichton Street
Edinburgh, EH8 9AB
United Kingdom
</address>
<email confidence="0.998523">
alopez@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994784" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954625">
We present a unified view of many trans-
lation algorithms that synthesizes work on
deductive parsing, semiring parsing, and
efficient approximate search algorithms.
This gives rise to clean analyses and com-
pact descriptions that can serve as the ba-
sis for modular implementations. We illus-
trate this with several examples, showing
how to build search spaces for several dis-
parate phrase-based search strategies, inte-
grate non-local features, and devise novel
models. Although the framework is drawn
from parsing and applied to translation, it
is applicable to many dynamic program-
ming problems arising in natural language
processing and other areas.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922676470589">
Implementing a large-scale translation system is
a major engineering effort requiring substantial
time and resources, and understanding the trade-
offs involved in model and algorithm design de-
cisions is important for success. As the space of
systems described in the literature becomes more
crowded, identifying their common elements and
isolating their differences becomes crucial to this
understanding. In this work, we present a com-
mon framework for model manipulation and anal-
ysis that accomplishes this, and use it to derive sur-
prising conclusions about phrase-based models.
Most translation algorithms do the same thing:
dynamic programming search over a space of
weighted rules (§2). Fortunately, we need
not search far for modular descriptions of dy-
namic programming algorithms. Deductive logic
(Pereira and Warren, 1983), extended with semir-
ings (Goodman, 1999), is an established formal-
ism used in parsing. It is occasionally used
to describe formally syntactic translation mod-
els, but these treatments tend to be brief (Chiang,
2007; Venugopal et al., 2007; Dyer et al., 2008;
Melamed, 2004). We apply weighted deduction
much more thoroughly, first extending it to phrase-
based models and showing that the set of search
strategies used by these models have surprisingly
different implications for model and search error
(§3, §4). We then show how it can be used to an-
alyze common translation problems such as non-
local parameterizations (§5), alignment, and novel
model design (§6). Finally, we show that it leads to
a simple analysis of cube pruning (Chiang, 2007),
an important approximate search algorithm (§7).
</bodyText>
<sectionHeader confidence="0.992589" genericHeader="introduction">
2 Translation Models
</sectionHeader>
<bodyText confidence="0.9996261">
A translation model consists of two distinct ele-
ments: an unweighted ruleset, and a parameteriza-
tion (Lopez, 2008). A ruleset licenses the steps by
which a source string f1...fI may be rewritten as
a target string e1...eJ, thereby defining the finite
set of all possible rewritings of a source string. A
parameterization defines a weight function over
every sequence of rule applications.
In a phrase-based model, the ruleset is simply
the unweighted phrase table, where each phrase
pair fi...fi,/ej...ej, states that phrase fi...fi, in the
source is rewritten as ej...ej, in the the target.
The model operates by iteratively applying
rewrites to the source sentence until each source
word has been consumed by exactly one rule. We
call a sequence of rule applications a derivation.
A target string e1...eJ yielded by a derivation D is
obtained by concatenating the target phrases of the
rules in the order in which they were applied. We
define Y (D) to be the target string yielded by D.
</bodyText>
<note confidence="0.923146">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 532–540,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.993149">
532
</page>
<bodyText confidence="0.976651">
Now consider the Viterbi approximation to a
noisy channel parameterization of this model,
P(f|D) · P(D).1 We define P(f|D) in the stan-
dard way.
</bodyText>
<equation confidence="0.983873666666667">
P(f|D) = H p(fi...fi/|ea...ea/)
fi...fi//ej...ej/∈D
(1)
</equation>
<bodyText confidence="0.995362">
Note that in the channel model, we can replace any
rule application with any other rule containing the
same source phrase without affecting the partial
score of the rest of the derivation. We call this a
local parameterization.
Now we define a standard n-gram model P(D).
</bodyText>
<equation confidence="0.999626">
P(D) = H p(ea|ea−n+1...ea−1) (2)
ej∈Y (D)
</equation>
<bodyText confidence="0.999903375">
This parameterization differs from the channel
model in an important way. If we replace a single
rule in the derivation, the partial score of the rest
of derivation is also affected, because the terms
ea−n+1...ea may come from more than one rule. In
other words, this parameterization encodes a de-
pendency between the steps in a derivation. We
call this a non-local parameterization.
</bodyText>
<sectionHeader confidence="0.976619" genericHeader="method">
3 Translation As Deduction
</sectionHeader>
<bodyText confidence="0.999969380952381">
For the first part of the discussion that follows, we
consider deductive logics purely over unweighted
rulesets. As a way to introduce deductive logic, we
consider the CKY algorithm for context-free pars-
ing, a common example that we will revisit in §6.2.
It is also relevant since it can form the basis of a
decoder for inversion transduction grammar (Wu,
1996). In the discussion that follows, we use A, B,
and C to denote arbitrary nonterminal symbols, S
to denote the start nonterminal symbol, and a to
denote a terminal symbol. CKY works on gram-
mars in Chomsky normal form: all rules are either
binary as in A → BC, or unary as in A → a.
The number of possible binary-branching
parses of a sentence is defined by the Catalan num-
ber, an exponential combinatoric function (Church
and Patil, 1982), so dynamic programming is cru-
cial for efficiency. CKY computes all parses in
cubic time by reusing subparses. To parse a sen-
tence a1...aK, we compute a set of items in the
form [A, k, k0], where A is a nonterminal category,
</bodyText>
<footnote confidence="0.961735333333333">
1The true noisy channel parameterization p(f|e) · p(e)
would require a marginalization over D, and is intractable
for most models.
</footnote>
<bodyText confidence="0.998922454545455">
k and k0 are both integers in the range [0, n]. This
item represents the fact that there is some parse of
span ak+1...ak/ rooted at A (span indices are on
the spaces between words). CKY works by creat-
ing items over successively longer spans. First it
creates items [A, k−1, k] for any rule A → a such
that a = ak. It then considers spans of increasing
length, creating items [A, k, k0] whenever it finds
two items [B, k, k00] and [C, k00, k0] for some gram-
mar rule A → BC and some midpoint k00. Its goal
is an item [S, 0, K], indicating that there is a parse
of a1...aK rooted at S.
A CKY logic describes its actions as inference
rules, equivalent to Horn clauses. The inference
rule is a list of antecedents, items and rules that
must all be true for the inference to occur; and a
single consequent that is inferred. To denote the
creation of item [A, k, k0] based on existence of
rule A → BC and items [B, k, k00] and [C, k00, k0],
we write an inference rule with antecedents on the
top line and consequent on the second line, follow-
ing Goodman (1999) and Shieber et al. (1995).
</bodyText>
<equation confidence="0.843440375">
R(A → BC) [B, k, k00] [C, k00, k0]
[A, k, k0]
We now give the complete Logic CKY.
item form: [A, k, k0] goal: [S, 0, K]
rules: { R(A → BC) [B, k, k00] [C, k00, k0]
R(A → ak)
[A,k − 1, k]
[A, k, k0]
</equation>
<bodyText confidence="0.991602333333333">
(Logic CKY)
A benefit of this declarative description is that
complexity can be determined by inspection
(McAllester, 1999). We elaborate on complexity
in §7, but for now it suffices to point out that the
number of possible items and possible deductions
depends on the product of the domains of the free
variables. For example, the number of possible
CKY items for a grammar with G nonterminals
is O(GK2), because k and k0 are both in range
[0, K]. Likewise, the number of possible inference
rules that can fire is O(G3K3).
</bodyText>
<subsectionHeader confidence="0.997561">
3.1 A Simple Deductive Decoder
</subsectionHeader>
<bodyText confidence="0.9996608">
For our first example of a translation logic we con-
sider a simple case: monotone decoding (Mari˜no
et al., 2006; Zens and Ney, 2004). Here, rewrite
rules are applied strictly from left to right on the
source sentence. Despite its simplicity, the search
</bodyText>
<page confidence="0.9976">
533
</page>
<bodyText confidence="0.999916692307692">
space can be very large—in the limit there could
be a translation for every possible segmentation
of the sentence, so there are exponentially many
possible derivations. Fortunately, we know that
monotone decoding can easily be cast as a dy-
namic programming problem. For any position i
in the source sentence f1...fI, we can freely com-
bine any partial derivation covering f1...fi on its
left with any partial derivation covering fi+1...fI
on its right to yield a complete derivation.
In our deductive program for monotone decod-
ing, an item simply encodes the index of the right-
most word that has been rewritten.
</bodyText>
<equation confidence="0.974356">
rule: [i] R(fi+1...fi0/ej...ej0)
[i0]
</equation>
<bodyText confidence="0.9635452">
(Logic MONOTONE)
This is the algorithm of Zens and Ney (2004).
With a maximum phrase length of m, i0 will range
over [i + 1, min(i + m, I)], giving a complexity of
O(Im). In the limit it is O(I2).
</bodyText>
<subsectionHeader confidence="0.999792">
3.2 More Complex Decoders
</subsectionHeader>
<bodyText confidence="0.955762428571429">
Now we consider phrase-based decoders with
more permissive reordering. In the limit we al-
low arbitrary reordering, so our item must contain
a coverage vector. Let V be a binary vector of
length I; that is, V E {0,1}I. Le 0m be a vec-
tor of m 0’s. For example, bit vector 00000 will
be abbreviated 05 and bit vector 000110 will be
abbreviated 031201. Finally, we will need bitwise
AND (n) and OR (V). Note that we impose an ad-
ditional requirement that is not an item in the de-
ductive system as a side condition (we elaborate
on the significance of this in §4).
item form: [{0,1}I] goal: [1I]
rule:
</bodyText>
<equation confidence="0.9923405">
[V ] R(fi+1...fi0/ej...ej0) V n 0i1i0−i0I−i0 = 0I
[V V 0i1i0−i0I−i0]
</equation>
<bodyText confidence="0.995211545454546">
(Logic PHRASE-BASED)
The runtime complexity is exponential, O(I22I).
Practical decoding strategies are more restrictive,
implementing what is frequently called a distor-
tion limit or reordering limit. We found that these
terms are inexact, used to describe a variety of
quite different strategies.2 Since we did not feel
that the relationship between these various strate-
gies was obvious or well-known, we give logics
2Costa-juss`a and Fonollosa (2006) refer to the reordering
limit and distortion limit as two distinct strategies.
for several of them and a brief analysis of the
implications. Each strategy uses a parameter d,
generically called the distortion limit or reorder-
ing limit.
The Maximum Distortion d strategy (MDd)
requires that the first word of a phrase chosen for
translation be within d words of the the last word
of the most recently translated phrase (Figure 1).3
The effect of this strategy is that, up to the last
word covered in a partial derivation, there must be
a covered word in every d words. Its complexity
is O(I32d).
MDd can produce partial derivations that cannot
be completed by any allowed sequence of jumps.
To prevent this, the Window Length d strategy
(WLd) enforces a tighter restriction that the last
word of a phrase chosen for translation cannot be
more than d words from the leftmost untranslated
word in the source (Figure 1).4 For this logic we
use a bitwise shift operator (&lt;), and a predicate
(α1) that counts the number of leading ones in a
bit array.5 Its runtime is exponential in parameter
d, but linear in sentence length, O(d22dI).
The First d Uncovered Words strategy
(FdUW) is described by Tillman and Ney (2003)
and Zens and Ney (2004), who call it the IBM
Constraint.6 It requires at least one of the leftmost
d uncovered words to be covered by a new phrase.
Items in this strategy contain the index i of the
rightmost covered word and a vector U E [1, I]d
of the d leftmost uncovered words (Figure 1). Its
complexity is O(dI (d+1) ), which is roughly ex-
ponential in d.
There are additional variants, such as the Maxi-
mum Jump d strategy (MJd), a polynomial-time
strategy described by Kumar and Byrne (2005),
and possibly others. We lack space to describe all
of them, but simply depicting the strategies as log-
ics permits us to make some simple analyses.
First, it should be clear that these reorder-
ing strategies define overlapping but not identical
search spaces: for most values of d it is impossi-
ble to find d0 such that any of the other strategies
would be identical (except for degenerate cases
</bodyText>
<footnote confidence="0.985499444444444">
3Moore and Quirk (2007) give a nice description ofMDd.
4We do not know if WLd is documented anywhere, but
from inspection it is used in Moses (Koehn et al., 2007). This
was confirmed by Philipp Koehn and Hieu Hoang (p.c.).
5When a phrase covers the first uncovered word in the
source sentence, the new first uncovered word may be further
along in the sentence (if the phrase completely filled a gap),
or just past the end of the phrase (otherwise).
6We could not identify this strategy in the IBM patents.
</footnote>
<construct confidence="0.1766195">
item form: [i]
goal: [I]
</construct>
<page confidence="0.769888">
534
</page>
<listItem confidence="0.813665">
(1) item form: [i, {0,1}I] rule: [i00,V ] R(fi+1...fi0/ej...ej0)
[i0, V ∨ 0i1i0−i0I−i0] V ∧ 0i1i0−i0I−i0 = 0I, |i − i00 |≤ d
goal: [i ∈ [I − d, I], 1I]
(3) item form: [i, [1, I + d]d] goal: [I, [I + 1, I + d]]
</listItem>
<equation confidence="0.937180875">
(2)
item form: [i, {0,1}d]
goal: [I, 0d]
rules:
[i, C] R(fi+1...fi0/ej...ej0)
[i00, C « i00 − i]
C ∧ 1i0−i0d−i0+i = 0d, i0 − i ≤ d,
α1(C ∨ 1i0−i0d−i0+i) = i00 − i
[i, C] R(fi0...fi00/ej ... ej0) C ∧ 0i0−i1i00−i00d−i00+i = 0d i00 − i ≤ d
[i C ∨ 0i0 −i 1i00−i00d−i00+i]
{
[i, U] R(fi0...fi00/ej...ej0) i0 &gt; i, fi+1 ∈ U
[i00, U − [i0, i00] ∨ [i00, i00 + d −  |U − [i0, i00] |]]
[i, U − [i0, i00] ∨ [max(U ∨ i) + 1, max(U ∨ i) + 1 + d − |U − [i0, i00]|]] i0 &lt; i, [fi0, fi00] ⊂ U
rules: {
[i, U] R(fi0...fi00/ej...ej0)
</equation>
<figureCaption confidence="0.9667395">
Figure 1: Logics (1) MDd, (2) WLd, and (3) FdUW. Note that the goal item of MDd (1) requires that the
last word of the last phrase translated be within d words of the end of the source sentence.
</figureCaption>
<bodyText confidence="0.99995625">
d = 0 and d = I). This has important ramifi-
cations for scientific studies: results reported for
one strategy may not hold for others, and in cases
where the strategy is not clearly described it may
be impossible to replicate results. Furthermore, it
should be clear that the strategy can have signifi-
cant impact on decoding speed and pruning strate-
gies (§7). For example, MDd is more complex
than WLd, and we expect implementations of the
former to require more pruning and suffer from
more search errors, while the latter would suffer
from more model errors since its space of possible
reorderings is smaller.
We emphasize that many other translation mod-
els can be described this way. Logics for the IBM
Models (Brown et al., 1993) would be similar to
our logics for phrase-based models. Syntax-based
translation logics are similar to parsing logics; a
few examples already appear in the literature (Chi-
ang, 2007; Venugopal et al., 2007; Dyer et al.,
2008; Melamed, 2004). For simplicity, we will
use the MONOTONE logic for the remainder of our
examples, but all of them generalize to more com-
plex logics.
</bodyText>
<sectionHeader confidence="0.9530185" genericHeader="method">
4 Adding Local Parameterizations via
Semiring-Weighted Deduction
</sectionHeader>
<bodyText confidence="0.9994805">
So far we have focused solely on unweighted log-
ics, which correspond to search using only rule-
</bodyText>
<sectionHeader confidence="0.468567" genericHeader="method">
C
</sectionHeader>
<bodyText confidence="0.99992375">
Now suppose we want to find the highest-scoring
derivation. Each antecedent item A` has a proba-
bility p(A`): if A` is a rule, then the probability is
given, otherwise its probability is computed recur-
sively in the same way that we now compute p(C).
Since C can be the consequent of multiple deduc-
tions, we take the max of its current value (initially
0) and the result of the new deduction.
</bodyText>
<equation confidence="0.997228">
p(C) = max(p(C), (p(A1) × ... × p(AL))) (3)
</equation>
<bodyText confidence="0.9998844">
If for every A` that is an item, we replace p(A`)
recursively with this expression, we end up with a
maximization over a product of rule probabilities.
Applying this to logic MONOTONE, the result will
be a maximization (over all possible derivations
D) of the algebraic expression in Equation 1.
We might also want to calculate the total prob-
ability of all possible derivations, which is useful
for parameter estimation (Blunsom et al., 2008).
We can do this using the following equation.
</bodyText>
<equation confidence="0.999256">
p(C) = p(C) + (p(A1) × ... × p(AL)) (4)
</equation>
<bodyText confidence="0.995601333333333">
sets. Now we turn our focus to parameterizations.
As a first step, we consider only local parame-
terizations, which make computing the score of a
derivation quite simple. We are given a set of in-
ferences in the following form (interpreting side
conditions B1...BM as boolean constraints).
</bodyText>
<figure confidence="0.504657">
A1...AL
B1...BM
</figure>
<page confidence="0.987762">
535
</page>
<bodyText confidence="0.999714333333333">
Equations 3 and 4 are quite similar. This suggests
a useful generalization: semiring-weighted deduc-
tion (Goodman, 1999).7 A semiring (A, ®, ®)
consists of a domain A, a multiplicative opera-
tor ® and an additive operator ®.8 In Equa-
tion 3 we use the Viterbi semiring ([0, 1], x, max),
while in Equation 4 we use the inside semiring
([0, 1], x,+). The general form of Equations 3
and 4 can be written for weights w E A.
</bodyText>
<equation confidence="0.997191">
w(C)e= w(A1) ® ... ® w(At) (5)
</equation>
<bodyText confidence="0.999950666666667">
Many quantities can be computed simply by us-
ing the appropriate semiring. Goodman (1999) de-
scribes semirings for the Viterbi derivation, k-best
Viterbi derivations, derivation forest, and num-
ber of paths.9 Eisner (2002) describes the expec-
tation semiring for parameter learning. Gimpel
and Smith (2009) describe approximation semir-
ings for approximate summing in (usually in-
tractable) models. In parsing, the boolean semir-
ing ({T, L}, f1, U) is used to determine grammati-
cality of an input string. In translation it is relevant
for alignment (§6.1).
</bodyText>
<sectionHeader confidence="0.69251" genericHeader="method">
5 Adding Non-Local Parameterizations
with the PRODUCT Transform
</sectionHeader>
<bodyText confidence="0.999963666666667">
A problem arises with the semiring-weighted de-
ductive formalism when we add non-local parame-
terizations such as an n-gram model (Equation 2).
Suppose we have a derivation D = (d1,..., dM),
where each dm is a rule application. We can view
the language model as a function on D.
</bodyText>
<equation confidence="0.997524">
P(D) = f(d1, ..., dm,..., dM) (6)
</equation>
<bodyText confidence="0.9967692">
The problem is that replacing dm with a lower-
scoring rule d0m may actually improve f due to
the language model dependency. This means that
f is nonmonotonic—it does not display the opti-
mal substructure property on partial derivations,
which is required for dynamic programming (Cor-
men et al., 2001). The logics still work for some
semirings (e.g. boolean), but not others. There-
fore, non-local parameterizations break semiring-
weighted deduction, because we can no longer use
</bodyText>
<footnote confidence="0.974289444444445">
7General weighted deduction subsumes semiring-
weighted deduction (Eisner et al., 2005; Eisner and Blatz,
2006; Nederhof, 2003), but semiring-weighted deduction
covers all translation models we are aware of, so it is a good
first step in applying weighted deduction to translation.
8See Goodman (1999) for additional conditions on semir-
ings used in this framework.
9Eisner and Blatz (2006) give an alternate strategy for the
best derivation.
</footnote>
<bodyText confidence="0.99968075">
the same logic under all semirings. We need new
logics; for this we will use a logic programming
transform called the PRODUCT transform (Cohen
et al., 2008).
We first define a logic for the non-local param-
eterization. The logic for an n-gram language
model generates sequence e1...eQ by generating
each new word given the past n − 1 words.10
</bodyText>
<equation confidence="0.732699">
item form: [eq, ..., eq+n−2] goal: [eQ−n+2, ..., eQ]
[eq+1, ..., eq+n−1]
</equation>
<bodyText confidence="0.992038777777778">
(Logic NGRAM)
Now we want to combine NGRAM and MONO-
TONE. To make things easier, we modify MONO-
TONE to encode the idea that once a source phrase
has been recognized, its target words are gener-
ated one at a time. We will use ue and ve to denote
(possibly empty) sequences in ej...e0j. Borrowing
the notation of Earley (1970), we encode progress
using a dotted phrase ue • ve.
</bodyText>
<equation confidence="0.7601775">
item form: [i, ue • ve] goal: [I, ue • ve]
rules:
[i, ue•] R(fi+1...fi&apos;/ejve) [i, ue • ejve]
[i0, ej • ve] [i, ueej • ve]
</equation>
<bodyText confidence="0.958852">
(Logic MONOTONE-GENERATE)
We combine NGRAM and MONOTONE-
GENERATE using the PRODUCT transform,
which takes two logics as input and essentially
does the following.11
</bodyText>
<listItem confidence="0.935384125">
1. Create a new item type from the cross-
product of item types in the input logics.
2. Create inference rules for the new item type
from the cross-product of all inference rules
in the input logics.
3. Constrain the new logic as needed. This is
done by hand, but quite simple, as we will
show by example.
</listItem>
<bodyText confidence="0.999183888888889">
The first two steps give us logic MONOTONE-
GENERATE o NGRAM (Figure 2). This is close to
what we want, but not quite done. The constraint
we want to apply is that each word written by logic
MONOTONE-GENERATE is equal to the word gen-
erated by logic NGRAM. We accomplish this by
unifying variables eq and en−i in the inference
rules, giving us logic MONOTONE-GENERATE +
NGRAM (Figure 2).
</bodyText>
<footnote confidence="0.79172525">
10We ignore start and stop probabilities for simplicity.
11The description of Cohen et al. (2008) is much more
complete and includes several examples.
rule: [eq, ..., eq+n−2]R(eq, ..., eq+n−1)
</footnote>
<page confidence="0.924366">
536
</page>
<table confidence="0.9582705">
rules:
item form: [i, ue • ve, eq, ..., eq+n−2] [i, ue•, eq, ..., eq+n−2] R(fi...fi0/ejue) R(eq, ..., eq+n−1)
goal: [I, ue•, eQ−n+2, ..., eQ] [i0, ej • ue, eq+1, ..., eq+n−1]
[i, ue • ejve, eq, ..., eq+n−2] R(eq, ..., eq+n−1)
[i, ueej • ve, eq+1, ..., eq+n−1]
item form: [i, ue • ve, ej, ..., ej+n−2] rules:
goal: [I, ue•, eJ−n+2, ..., eJ] [i, ue•, ej−n+1, ..., ej−1] R(fi...fi0/ejve) R(ej−n+2...ej)
[i0, ej • ve, ej−n+2, ..., ej]
[i, ue • ei+n−1ve, ei, ..., ei+n−2] R(ej−n+2...ej)
[i + 1, ueej • ve, ej−n+2, ..., ej]
</table>
<listItem confidence="0.640742">
(3) item form: [i, ue • ve, ei, ..., en−i−2] goal: [I, ue•, eI−n+2, ..., eI]
</listItem>
<equation confidence="0.8093295">
rule: [i, ej−n+1, ..., ej−1] R(fi...fi0/ej...ej0)R(ej−n+1, ..., ej)...R(ej0−n+1...ej0)
[i0, ej0−n+2...ej0]
</equation>
<figureCaption confidence="0.819951">
Figure 2: Logics (1) MONOTONE-GENERATE ◦ NGRAM, (2) MONOTONE-GENERATE + NGRAM and
(3) MONOTONE-GENERATE + NGRAM SINGLE-SHOT.
</figureCaption>
<bodyText confidence="0.999955891891891">
This logic restores the optimal subproblem
property and we can apply semiring-weighted de-
duction. Efficient algorithms are given in §7, but
a brief comment is in order about the new logic.
In most descriptions of phrase-based decoding,
the n-gram language model is applied all at once.
MONOTONE-GENERATE+NGRAM applies the n-
gram language model one word at a time. This
illuminates a space of search strategies that are to
our knowledge unexplored. If a four-word phrase
were proposed as an extension of a partial hypoth-
esis in a typical decoder implementation using a
five-word language model, all four n-grams will
be applied even though the first n-gram might have
a very low score. Viewing each n-gram applica-
tion as producing a new state may yield new strate-
gies for approximate search.
We can derive the more familiar logic by ap-
plying a different transform: unfolding (Eisner
and Blatz, 2006). The idea is to replace an item
with the sequence of antecedents used to pro-
duce it (similar to function inlining). This gives
us MONOTONE-GENERATE+NGRAM SINGLE-
SHOT (Figure 2).
We call the ruleset-based logic the minimal
logic and the logic enhanced with non-local pa-
rameterization the complete logic. Note that the
set of variables in the complete logic is a superset
of the set of variables in the minimal logic. We
can view the minimal logic as a projection of the
complete logic into a smaller dimensional space.
It is important to note that complete logic is sub-
stantially more complex than the minimal logic,
by a factor of O(|VE|n) for a target vocabulary of
VE. Thus, the complexity of non-local parameteri-
zations often makes search spaces large regardless
of the complexity of the minimal logic.
</bodyText>
<sectionHeader confidence="0.986851" genericHeader="method">
6 Other Uses of the PRODUCT Transform
</sectionHeader>
<bodyText confidence="0.9991285">
The PRODUCT transform can also implement
alignment and help derive new models.
</bodyText>
<subsectionHeader confidence="0.998546">
6.1 Alignment
</subsectionHeader>
<bodyText confidence="0.9999202">
In the alignment problem (sometimes called con-
strained decoding or forced decoding), we are
given a reference target sentence r1, ..., rJ, and
we require the translation model to generate only
derivations that produce that sentence. Alignment
is often used in training both generative and dis-
criminative models (Brown et al., 1993; Blunsom
et al., 2008; Liang et al., 2006). Our approach to
alignment is similar to the one for language mod-
eling. First, we implement a logic requiring an
</bodyText>
<page confidence="0.992052">
537
</page>
<bodyText confidence="0.8900105">
input to be identical to the reference.
item form: [j]
</bodyText>
<equation confidence="0.676184">
goal: [J] rule: [j]
[j + 1] ej+1 = rj+1
</equation>
<bodyText confidence="0.975429714285714">
(Logic RECOGNIZE)
The logic only reaches its goal if the input is iden-
tical to the reference. In fact, partial derivations
must produce a prefix of the reference. When we
combine this logic with MONOTONE-GENERATE,
we obtain a logic that only succeeds if the transla-
tion logic generates the reference.
</bodyText>
<equation confidence="0.6535115">
item form: [i, j, ue • ve] goal: [I, j, ue•]
[i, + 1, ueej • ve] ej+1 = rj+1
</equation>
<bodyText confidence="0.9823417">
(Logic MONOTONE-ALIGN)
Under the boolean semiring, this (minimal) logic
decides if a training example is reachable by the
model, which is required by some discriminative
training regimens (Liang et al., 2006; Blunsom et
al., 2008). We can also compute the Viterbi deriva-
tion or the sum over all derivations of a training
example, needed for some parameter estimation
methods. Cohen et al. (2008) derive an alignment
logic for ITG from the product of two CKY logics.
</bodyText>
<subsectionHeader confidence="0.989118">
6.2 Translation Model Design
</subsectionHeader>
<bodyText confidence="0.997598827586207">
A motivation for many syntax-based translation
models is to use target-side syntax as a language
model (Charniak et al., 2003). Och et al. (2004)
showed that simply parsing the N-best outputs
of a phrase-based model did not work; to ob-
tain the full power of a language model, we need
to integrate it into the search process. Most ap-
proaches to this problem focus on synchronous
grammars, but it is possible to integrate the target-
side language model with a phrase-based transla-
tion model. As an exercise, we integrate CKY
with the output of logic MONOTONE-GENERATE.
The constraint is that the indices of the CKY items
unify with the items of the translation logic, which
form a word lattice. Note that this logic retains the
rules in the basic MONOTONE logic, which are not
depicted (Figure 3).
The result is a lattice parser on the output of the
translation model. Lattice parsing is not new to
translation (Dyer et al., 2008), but to our knowl-
edge it has not been used in this way. Viewing
Figure 4: Example graphs corresponding to a sim-
ple minimal (1) and complete (2) logic, with cor-
responding nodes in the same color. The state-
splitting induced by non-local features produces in
a large number of arcs which must be evaluated,
which can be reduced by cube pruning.
translation as deduction is helpful for the design
and construction of novel models.
</bodyText>
<sectionHeader confidence="0.991399" genericHeader="method">
7 Algorithms
</sectionHeader>
<bodyText confidence="0.999490966666667">
Most translation logics are too expensive to ex-
haustively search. However, the logics conve-
niently specify the full search space, which forms
a hypergraph (Klein and Manning, 2001).12 The
equivalence is useful for complexity analysis:
items correspond to nodes and deductions corre-
spond to hyperarcs. These equivalences make it
easy to compute algorithmic bounds.
Cube pruning (Chiang, 2007) is an approxi-
mate search technique for syntax-based translation
models with integrated language models. It op-
erates on two objects: a −LM graph containing
no language model state, and a +LM hypergraph
containing state. The idea is to generate a fixed
number of nodes in the +LM for each node in
the −LM graph, using a clever enumeration strat-
egy. We can view cube pruning as arising from
the interaction between a minimal logic and the
state splits induced by non-local features. Figure 4
shows how the added state information can dra-
matically increase the number of deductions that
must be evaluated. Cube pruning works by con-
sidering the most promising states paired with the
most promising extensions. In this way, it easily
fits any search space constructed using the tech-
nique of §5. Note that the efficiency of cube prun-
ing is limited by the minimal logic.
Stack decoding is a search heuristic that simpli-
fies the complexity of searching a minimal logic.
Each item is associated with a stack whose signa-
</bodyText>
<footnote confidence="0.83744825">
12Specifically a B-hypergraph, equivalent to an and-or
graph (Gallo et al., 1993) or context-free grammar (Neder-
hof, 2003). In the degenerate case, this is simply a graph, as
is the case with most phrase-based models.
</footnote>
<equation confidence="0.968093">
rules: {
[i,j, ue•] R(fi...fi,/ej...ej,)
[i&amp;quot; j, •ej...ej,]
[i, j, ue • ejve]
</equation>
<page confidence="0.927741">
538
</page>
<figure confidence="0.320623">
item forms: [i, ue • ve], [A, i, ue • ve, i0, u0e • v0e] goal: [5, 0, •, I, ue•]
rules:
[i, ue•] R(fi+1...fi&apos;/ejve) R(A → ej)
[A, i, ue•, i0, ej • ve]
[B,i,ue • ve, i00, u00 e • v00 e ] [C, i00, u00 e • v00 e , i0, u0 e • v0 e] R(A → BC)
[A, i, ue • ve, i0, u0e • v0e]
</figure>
<figureCaption confidence="0.965956">
Figure 3: Logic MONOTONE-GENERATE + CKY
</figureCaption>
<bodyText confidence="0.95994275">
[i, ue • ejve] R(A → ej)
[A, i, ue • ejve, i, ueej • ve]
ture is a projection of the item signature (or a pred-
icate on the item signatures)—multiple items are
associated to the same stack. The strength of the
pruning (and resulting complexity improvements)
depending on how much the projection reduces the
search space. In many phrase-based implemen-
tations the stack signature is just the number of
words translated, but other strategies are possible
(Tillman and Ney, 2003).
It is worth noting that logic FdUW (§3.2), de-
pends on stack pruning for speed. Because the
number of stacks is linear in the length of the in-
put, so is the number of unpruned nodes in the
search graph. In contrast, the complexity of logic
WLd is naturally linear in input length. As men-
tioned in §3.2, this implies a wide divergence in
the model and search errors of these logics, which
to our knowledge has not been investigated.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.9856078">
We are not the first to observe that phrase-based
models can be represented as logic programs (Eis-
ner et al., 2005; Eisner and Blatz, 2006), but to
our knowledge we are the first to provide explicit
logics for them.13 We also showed that deductive
logic is a useful analytical tool to tackle a variety
of problems in translation algorithm design.
Our work is strongly influenced by Goodman
(1999) and Eisner et al. (2005). They describe
many issues not mentioned here, including con-
ditions on semirings, termination conditions, and
strategies for cyclic search graphs. However,
while their weighted deductive formalism is gen-
eral, they focus on concerns relevant to parsing,
such as boolean semirings and cyclicity. Our work
focuses on concerns common for translation, in-
cluding a general view of non-local parameteriza-
tions and cube pruning.
13Huang and Chiang (2007) give an informal example, but
do not elaborate on it.
</bodyText>
<sectionHeader confidence="0.98726" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999997884615385">
We have described a general framework that syn-
thesizes and extends deductive parsing and semir-
ing parsing, and adapts it to translation. Our goal
has been to show that logics make an attractive
shorthand for description, analysis, and construc-
tion of translation models. For instance, we have
shown that it is quite easy to mechanically con-
struct search spaces using non-local features, and
to create exotic models. We showed that differ-
ent flavors of phrase-based models should suffer
from quite different types of error, a problem that
to our knowledge was heretofore unknown. How-
ever, we have only scratched the surface, and we
believe it is possibly to unify a wide variety of
translation algorithms. For example, we believe
that cube pruning can be described as an agenda
discipline in chart parsing (Kay, 1986).
Although the work presented here is abstract,
our motivation is practical. Isolating the errors
in translation systems is a difficult task which can
be made easier by describing and analyzing mod-
els in a modular way (Auli et al., 2009). Fur-
thermore, building large-scale translation systems
from scratch should be unnecessary if existing sys-
tems were built using modular logics and algo-
rithms. We aim to build such systems.
</bodyText>
<sectionHeader confidence="0.997024" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9911309">
This work developed from discussions with Phil
Blunsom, Chris Callison-Burch, Chris Dyer,
Hieu Hoang, Martin Kay, Philipp Koehn, Josh
Schroeder, and Lane Schwartz. Many thanks go to
Chris Dyer, Josh Schroeder, the three anonymous
EACL reviewers, and one anonymous NAACL re-
viewer for very helpful comments on earlier drafts.
This research was supported by the Euromatrix
Project funded by the European Commission (6th
Framework Programme).
</bodyText>
<page confidence="0.997619">
539
</page>
<sectionHeader confidence="0.983694" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899726495727">
M. Auli, A. Lopez, P. Koehn, and H. Hoang. 2009.
A systematic analysis of translation model search
spaces. In Proc. of WMT, Mar.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. ofACL:HLT.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311, Jun.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for statistical ma-
chine translation. In Proceedings of MT Summit,
Sept.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
K. Church and R. Patil. 1982. Coping with syntactic
ambiguity or how to put the block in the box on the
table. Computational Linguistics, 8(3–4):139–149,
Jul.
S. B. Cohen, R. J. Simmons, and N. A. Smith. 2008.
Dynamic programming algorithms as products of
weighted logic programs. In Proc. ofICLP.
T. H. Cormen, C. D. Leiserson, R. L. Rivest, and
C. Stein. 2001. Introduction to Algorithms. MIT
Press, 2nd edition.
M. R. Costa-juss`a and J. A. R. Fonollosa. 2006. Statis-
tical machine reordering. In Proc. ofEMNLP, pages
70–76, Jul.
C. J. Dyer, S. Muresan, and P. Resnik. 2008. General-
izing word lattice translation. In Proc. ofACL:HLT,
pages 1012–1020.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94–102,
Feb.
J. Eisner and J. Blatz. 2006. Program transformations
for optimization of parsing algorithms and other
weighted logic programs. In Proc. ofFormal Gram-
mar, pages 45–85.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Compil-
ing comp ling: Weighted dynamic programming and
the Dyna language. In Proc. ofHLT-EMNLP, pages
281–290.
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proc. ofACL, pages 1–8,
Jul.
G. Gallo, G. Longo, and S. Pallottino. 1993. Di-
rected hypergraphs and applications. Discrete Ap-
plied Mathematics, 42(2), Apr.
K. Gimpel and N. A. Smith. 2009. Approximation
semirings: Dynamic programming with non-local
features. In Proc. of EACL, Mar.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25(4):573–605, Dec.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. ofACL, pages 144–151, Jun.
M. Kay. 1986. Algorithm schemata and data structures
in syntactic processing. In B. J. Grosz, K. S. Jones,
and B. L. Webber, editors, Readings in Natural Lan-
guage Processing, pages 35–70. Morgan Kaufmann.
D. Klein and C. Manning. 2001. Parsing and hyper-
graphs. In Proc. ofIWPT.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demo and Poster Sessions, pages 177–180, Jun.
S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proc. ofHLT-EMNLP, pages 161–168.
P. Liang, A. Bouchard-Cˆot´e, B. Taskar, and D. Klein.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL-COLING, pages
761–768, Jul.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3), Aug.
J. B. Mari˜no, R. E. Banchs, J. M. Crego, A. de Gispert,
P. Lambert, J. A. R. Fonollosa, and M. R. Costa-
juss`a. 2006. N-gram based statistical machine
translation. Computational Linguistics, 32(4):527–
549, Dec.
D. McAllester. 1999. On the complexity analysis of
static analyses. In Proc. of Static Analysis Sympo-
sium, volume 1694/1999 ofLNCS. Springer Verlag.
I. D. Melamed. 2004. Statistical machine translation
by parsing. In Proc. ofACL, pages 654–661, Jul.
R. C. Moore and C. Quirk. 2007. Faster beam-search
decoding for phrasal statistical machine translation.
In Proc. ofMT Summit.
M.-J. Nederhof. 2003. Weighted deductive parsing
and Knuth’s algorithm. Computational Linguistics,
29(1):135–143, Mar.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In Proc. ofHLT-NAACL, pages 161–168, May.
F. C. N. Pereira and D. H. D. Warren. 1983. Parsing as
deduction. In Proc. ofACL, pages 137–144.
S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995.
Principles and implementation of deductive parsing.
Journal ofLogic Programming, 24(1–2):3–36, Jul.
C. Tillman and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):98–133, Mar.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An efficient two-pass approach to synchronous-CFG
driven statistical MT. In Proc. ofHLT-NAACL.
D. Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proc. ofACL, pages
152–158, Jun.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. of
HLT-NAACL, pages 257–264, May.
</reference>
<page confidence="0.997008">
540
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867425">
<title confidence="0.999888">Translation as Weighted Deduction</title>
<author confidence="0.999647">Adam Lopez</author>
<affiliation confidence="0.999978">University of Edinburgh</affiliation>
<address confidence="0.964337333333333">10 Crichton Street Edinburgh, EH8 9AB United Kingdom</address>
<email confidence="0.997129">alopez@inf.ed.ac.uk</email>
<abstract confidence="0.997842352941176">We present a unified view of many translation algorithms that synthesizes work on deductive parsing, semiring parsing, and efficient approximate search algorithms. This gives rise to clean analyses and compact descriptions that can serve as the basis for modular implementations. We illustrate this with several examples, showing how to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Auli</author>
<author>A Lopez</author>
<author>P Koehn</author>
<author>H Hoang</author>
</authors>
<title>A systematic analysis of translation model search spaces.</title>
<date>2009</date>
<booktitle>In Proc. of WMT,</booktitle>
<contexts>
<context position="30559" citStr="Auli et al., 2009" startWordPosition="5264" endWordPosition="5267">erent flavors of phrase-based models should suffer from quite different types of error, a problem that to our knowledge was heretofore unknown. However, we have only scratched the surface, and we believe it is possibly to unify a wide variety of translation algorithms. For example, we believe that cube pruning can be described as an agenda discipline in chart parsing (Kay, 1986). Although the work presented here is abstract, our motivation is practical. Isolating the errors in translation systems is a difficult task which can be made easier by describing and analyzing models in a modular way (Auli et al., 2009). Furthermore, building large-scale translation systems from scratch should be unnecessary if existing systems were built using modular logics and algorithms. We aim to build such systems. Acknowledgments This work developed from discussions with Phil Blunsom, Chris Callison-Burch, Chris Dyer, Hieu Hoang, Martin Kay, Philipp Koehn, Josh Schroeder, and Lane Schwartz. Many thanks go to Chris Dyer, Josh Schroeder, the three anonymous EACL reviewers, and one anonymous NAACL reviewer for very helpful comments on earlier drafts. This research was supported by the Euromatrix Project funded by the Eur</context>
</contexts>
<marker>Auli, Lopez, Koehn, Hoang, 2009</marker>
<rawString>M. Auli, A. Lopez, P. Koehn, and H. Hoang. 2009. A systematic analysis of translation model search spaces. In Proc. of WMT, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. ofACL:HLT.</booktitle>
<contexts>
<context position="15490" citStr="Blunsom et al., 2008" startWordPosition="2689" endWordPosition="2692"> can be the consequent of multiple deductions, we take the max of its current value (initially 0) and the result of the new deduction. p(C) = max(p(C), (p(A1) × ... × p(AL))) (3) If for every A` that is an item, we replace p(A`) recursively with this expression, we end up with a maximization over a product of rule probabilities. Applying this to logic MONOTONE, the result will be a maximization (over all possible derivations D) of the algebraic expression in Equation 1. We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al., 2008). We can do this using the following equation. p(C) = p(C) + (p(A1) × ... × p(AL)) (4) sets. Now we turn our focus to parameterizations. As a first step, we consider only local parameterizations, which make computing the score of a derivation quite simple. We are given a set of inferences in the following form (interpreting side conditions B1...BM as boolean constraints). A1...AL B1...BM 535 Equations 3 and 4 are quite similar. This suggests a useful generalization: semiring-weighted deduction (Goodman, 1999).7 A semiring (A, ®, ®) consists of a domain A, a multiplicative operator ® and an add</context>
<context position="23147" citStr="Blunsom et al., 2008" startWordPosition="3981" endWordPosition="3984">. Thus, the complexity of non-local parameterizations often makes search spaces large regardless of the complexity of the minimal logic. 6 Other Uses of the PRODUCT Transform The PRODUCT transform can also implement alignment and help derive new models. 6.1 Alignment In the alignment problem (sometimes called constrained decoding or forced decoding), we are given a reference target sentence r1, ..., rJ, and we require the translation model to generate only derivations that produce that sentence. Alignment is often used in training both generative and discriminative models (Brown et al., 1993; Blunsom et al., 2008; Liang et al., 2006). Our approach to alignment is similar to the one for language modeling. First, we implement a logic requiring an 537 input to be identical to the reference. item form: [j] goal: [J] rule: [j] [j + 1] ej+1 = rj+1 (Logic RECOGNIZE) The logic only reaches its goal if the input is identical to the reference. In fact, partial derivations must produce a prefix of the reference. When we combine this logic with MONOTONE-GENERATE, we obtain a logic that only succeeds if the translation logic generates the reference. item form: [i, j, ue • ve] goal: [I, j, ue•] [i, + 1, ueej • ve] </context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>P. Blunsom, T. Cohn, and M. Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. ofACL:HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14077" citStr="Brown et al., 1993" startWordPosition="2448" endWordPosition="2451">not hold for others, and in cases where the strategy is not clearly described it may be impossible to replicate results. Furthermore, it should be clear that the strategy can have significant impact on decoding speed and pruning strategies (§7). For example, MDd is more complex than WLd, and we expect implementations of the former to require more pruning and suffer from more search errors, while the latter would suffer from more model errors since its space of possible reorderings is smaller. We emphasize that many other translation models can be described this way. Logics for the IBM Models (Brown et al., 1993) would be similar to our logics for phrase-based models. Syntax-based translation logics are similar to parsing logics; a few examples already appear in the literature (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). For simplicity, we will use the MONOTONE logic for the remainder of our examples, but all of them generalize to more complex logics. 4 Adding Local Parameterizations via Semiring-Weighted Deduction So far we have focused solely on unweighted logics, which correspond to search using only ruleC Now suppose we want to find the highest-scoring derivation. Each</context>
<context position="23125" citStr="Brown et al., 1993" startWordPosition="3977" endWordPosition="3980">get vocabulary of VE. Thus, the complexity of non-local parameterizations often makes search spaces large regardless of the complexity of the minimal logic. 6 Other Uses of the PRODUCT Transform The PRODUCT transform can also implement alignment and help derive new models. 6.1 Alignment In the alignment problem (sometimes called constrained decoding or forced decoding), we are given a reference target sentence r1, ..., rJ, and we require the translation model to generate only derivations that produce that sentence. Alignment is often used in training both generative and discriminative models (Brown et al., 1993; Blunsom et al., 2008; Liang et al., 2006). Our approach to alignment is similar to the one for language modeling. First, we implement a logic requiring an 537 input to be identical to the reference. item form: [j] goal: [J] rule: [j] [j + 1] ej+1 = rj+1 (Logic RECOGNIZE) The logic only reaches its goal if the input is identical to the reference. In fact, partial derivations must produce a prefix of the reference. When we combine this logic with MONOTONE-GENERATE, we obtain a logic that only succeeds if the translation logic generates the reference. item form: [i, j, ue • ve] goal: [I, j, ue•</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit,</booktitle>
<contexts>
<context position="24380" citStr="Charniak et al., 2003" startWordPosition="4196" endWordPosition="4199"> (Logic MONOTONE-ALIGN) Under the boolean semiring, this (minimal) logic decides if a training example is reachable by the model, which is required by some discriminative training regimens (Liang et al., 2006; Blunsom et al., 2008). We can also compute the Viterbi derivation or the sum over all derivations of a training example, needed for some parameter estimation methods. Cohen et al. (2008) derive an alignment logic for ITG from the product of two CKY logics. 6.2 Translation Model Design A motivation for many syntax-based translation models is to use target-side syntax as a language model (Charniak et al., 2003). Och et al. (2004) showed that simply parsing the N-best outputs of a phrase-based model did not work; to obtain the full power of a language model, we need to integrate it into the search process. Most approaches to this problem focus on synchronous grammars, but it is possible to integrate the targetside language model with a phrase-based translation model. As an exercise, we integrate CKY with the output of logic MONOTONE-GENERATE. The constraint is that the indices of the CKY items unify with the items of the translation logic, which form a word lattice. Note that this logic retains the r</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-based language models for statistical machine translation. In Proceedings of MT Summit, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1857" citStr="Chiang, 2007" startWordPosition="275" endWordPosition="276"> common framework for model manipulation and analysis that accomplishes this, and use it to derive surprising conclusions about phrase-based models. Most translation algorithms do the same thing: dynamic programming search over a space of weighted rules (§2). Fortunately, we need not search far for modular descriptions of dynamic programming algorithms. Deductive logic (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 Translation Model</context>
<context position="14258" citStr="Chiang, 2007" startWordPosition="2477" endWordPosition="2479">ant impact on decoding speed and pruning strategies (§7). For example, MDd is more complex than WLd, and we expect implementations of the former to require more pruning and suffer from more search errors, while the latter would suffer from more model errors since its space of possible reorderings is smaller. We emphasize that many other translation models can be described this way. Logics for the IBM Models (Brown et al., 1993) would be similar to our logics for phrase-based models. Syntax-based translation logics are similar to parsing logics; a few examples already appear in the literature (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). For simplicity, we will use the MONOTONE logic for the remainder of our examples, but all of them generalize to more complex logics. 4 Adding Local Parameterizations via Semiring-Weighted Deduction So far we have focused solely on unweighted logics, which correspond to search using only ruleC Now suppose we want to find the highest-scoring derivation. Each antecedent item A` has a probability p(A`): if A` is a rule, then the probability is given, otherwise its probability is computed recursively in the same way that we now compute p(</context>
<context position="26013" citStr="Chiang, 2007" startWordPosition="4474" endWordPosition="4475">ced by non-local features produces in a large number of arcs which must be evaluated, which can be reduced by cube pruning. translation as deduction is helpful for the design and construction of novel models. 7 Algorithms Most translation logics are too expensive to exhaustively search. However, the logics conveniently specify the full search space, which forms a hypergraph (Klein and Manning, 2001).12 The equivalence is useful for complexity analysis: items correspond to nodes and deductions correspond to hyperarcs. These equivalences make it easy to compute algorithmic bounds. Cube pruning (Chiang, 2007) is an approximate search technique for syntax-based translation models with integrated language models. It operates on two objects: a −LM graph containing no language model state, and a +LM hypergraph containing state. The idea is to generate a fixed number of nodes in the +LM for each node in the −LM graph, using a clever enumeration strategy. We can view cube pruning as arising from the interaction between a minimal logic and the state splits induced by non-local features. Figure 4 shows how the added state information can dramatically increase the number of deductions that must be evaluate</context>
<context position="29421" citStr="Chiang (2007)" startWordPosition="5077" endWordPosition="5078">s a useful analytical tool to tackle a variety of problems in translation algorithm design. Our work is strongly influenced by Goodman (1999) and Eisner et al. (2005). They describe many issues not mentioned here, including conditions on semirings, termination conditions, and strategies for cyclic search graphs. However, while their weighted deductive formalism is general, they focus on concerns relevant to parsing, such as boolean semirings and cyclicity. Our work focuses on concerns common for translation, including a general view of non-local parameterizations and cube pruning. 13Huang and Chiang (2007) give an informal example, but do not elaborate on it. 9 Conclusions and Future Work We have described a general framework that synthesizes and extends deductive parsing and semiring parsing, and adapts it to translation. Our goal has been to show that logics make an attractive shorthand for description, analysis, and construction of translation models. For instance, we have shown that it is quite easy to mechanically construct search spaces using non-local features, and to create exotic models. We showed that different flavors of phrase-based models should suffer from quite different types of</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>R Patil</author>
</authors>
<title>Coping with syntactic ambiguity or how to put the block in the box on the table.</title>
<date>1982</date>
<journal>Computational Linguistics,</journal>
<pages>8--3</pages>
<contexts>
<context position="5351" citStr="Church and Patil, 1982" startWordPosition="853" endWordPosition="856">orithm for context-free parsing, a common example that we will revisit in §6.2. It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996). In the discussion that follows, we use A, B, and C to denote arbitrary nonterminal symbols, S to denote the start nonterminal symbol, and a to denote a terminal symbol. CKY works on grammars in Chomsky normal form: all rules are either binary as in A → BC, or unary as in A → a. The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function (Church and Patil, 1982), so dynamic programming is crucial for efficiency. CKY computes all parses in cubic time by reusing subparses. To parse a sentence a1...aK, we compute a set of items in the form [A, k, k0], where A is a nonterminal category, 1The true noisy channel parameterization p(f|e) · p(e) would require a marginalization over D, and is intractable for most models. k and k0 are both integers in the range [0, n]. This item represents the fact that there is some parse of span ak+1...ak/ rooted at A (span indices are on the spaces between words). CKY works by creating items over successively longer spans. F</context>
</contexts>
<marker>Church, Patil, 1982</marker>
<rawString>K. Church and R. Patil. 1982. Coping with syntactic ambiguity or how to put the block in the box on the table. Computational Linguistics, 8(3–4):139–149, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>R J Simmons</author>
<author>N A Smith</author>
</authors>
<title>Dynamic programming algorithms as products of weighted logic programs.</title>
<date>2008</date>
<booktitle>In Proc. ofICLP.</booktitle>
<publisher>MIT Press,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="18330" citStr="Cohen et al., 2008" startWordPosition="3160" endWordPosition="3163">e we can no longer use 7General weighted deduction subsumes semiringweighted deduction (Eisner et al., 2005; Eisner and Blatz, 2006; Nederhof, 2003), but semiring-weighted deduction covers all translation models we are aware of, so it is a good first step in applying weighted deduction to translation. 8See Goodman (1999) for additional conditions on semirings used in this framework. 9Eisner and Blatz (2006) give an alternate strategy for the best derivation. the same logic under all semirings. We need new logics; for this we will use a logic programming transform called the PRODUCT transform (Cohen et al., 2008). We first define a logic for the non-local parameterization. The logic for an n-gram language model generates sequence e1...eQ by generating each new word given the past n − 1 words.10 item form: [eq, ..., eq+n−2] goal: [eQ−n+2, ..., eQ] [eq+1, ..., eq+n−1] (Logic NGRAM) Now we want to combine NGRAM and MONOTONE. To make things easier, we modify MONOTONE to encode the idea that once a source phrase has been recognized, its target words are generated one at a time. We will use ue and ve to denote (possibly empty) sequences in ej...e0j. Borrowing the notation of Earley (1970), we encode progres</context>
<context position="20034" citStr="Cohen et al. (2008)" startWordPosition="3466" endWordPosition="3469">nce rules in the input logics. 3. Constrain the new logic as needed. This is done by hand, but quite simple, as we will show by example. The first two steps give us logic MONOTONEGENERATE o NGRAM (Figure 2). This is close to what we want, but not quite done. The constraint we want to apply is that each word written by logic MONOTONE-GENERATE is equal to the word generated by logic NGRAM. We accomplish this by unifying variables eq and en−i in the inference rules, giving us logic MONOTONE-GENERATE + NGRAM (Figure 2). 10We ignore start and stop probabilities for simplicity. 11The description of Cohen et al. (2008) is much more complete and includes several examples. rule: [eq, ..., eq+n−2]R(eq, ..., eq+n−1) 536 rules: item form: [i, ue • ve, eq, ..., eq+n−2] [i, ue•, eq, ..., eq+n−2] R(fi...fi0/ejue) R(eq, ..., eq+n−1) goal: [I, ue•, eQ−n+2, ..., eQ] [i0, ej • ue, eq+1, ..., eq+n−1] [i, ue • ejve, eq, ..., eq+n−2] R(eq, ..., eq+n−1) [i, ueej • ve, eq+1, ..., eq+n−1] item form: [i, ue • ve, ej, ..., ej+n−2] rules: goal: [I, ue•, eJ−n+2, ..., eJ] [i, ue•, ej−n+1, ..., ej−1] R(fi...fi0/ejve) R(ej−n+2...ej) [i0, ej • ve, ej−n+2, ..., ej] [i, ue • ei+n−1ve, ei, ..., ei+n−2] R(ej−n+2...ej) [i + 1, ueej • ve,</context>
<context position="24154" citStr="Cohen et al. (2008)" startWordPosition="4159" endWordPosition="4162">ference. When we combine this logic with MONOTONE-GENERATE, we obtain a logic that only succeeds if the translation logic generates the reference. item form: [i, j, ue • ve] goal: [I, j, ue•] [i, + 1, ueej • ve] ej+1 = rj+1 (Logic MONOTONE-ALIGN) Under the boolean semiring, this (minimal) logic decides if a training example is reachable by the model, which is required by some discriminative training regimens (Liang et al., 2006; Blunsom et al., 2008). We can also compute the Viterbi derivation or the sum over all derivations of a training example, needed for some parameter estimation methods. Cohen et al. (2008) derive an alignment logic for ITG from the product of two CKY logics. 6.2 Translation Model Design A motivation for many syntax-based translation models is to use target-side syntax as a language model (Charniak et al., 2003). Och et al. (2004) showed that simply parsing the N-best outputs of a phrase-based model did not work; to obtain the full power of a language model, we need to integrate it into the search process. Most approaches to this problem focus on synchronous grammars, but it is possible to integrate the targetside language model with a phrase-based translation model. As an exerc</context>
</contexts>
<marker>Cohen, Simmons, Smith, 2008</marker>
<rawString>S. B. Cohen, R. J. Simmons, and N. A. Smith. 2008. Dynamic programming algorithms as products of weighted logic programs. In Proc. ofICLP. T. H. Cormen, C. D. Leiserson, R. L. Rivest, and C. Stein. 2001. Introduction to Algorithms. MIT Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-juss`a</author>
<author>J A R Fonollosa</author>
</authors>
<title>Statistical machine reordering.</title>
<date>2006</date>
<booktitle>In Proc. ofEMNLP,</booktitle>
<pages>70--76</pages>
<marker>Costa-juss`a, Fonollosa, 2006</marker>
<rawString>M. R. Costa-juss`a and J. A. R. Fonollosa. 2006. Statistical machine reordering. In Proc. ofEMNLP, pages 70–76, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Dyer</author>
<author>S Muresan</author>
<author>P Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proc. ofACL:HLT,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="1900" citStr="Dyer et al., 2008" startWordPosition="281" endWordPosition="284">on and analysis that accomplishes this, and use it to derive surprising conclusions about phrase-based models. Most translation algorithms do the same thing: dynamic programming search over a space of weighted rules (§2). Fortunately, we need not search far for modular descriptions of dynamic programming algorithms. Deductive logic (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 Translation Models A translation model consists of two disti</context>
<context position="14301" citStr="Dyer et al., 2008" startWordPosition="2484" endWordPosition="2487">ng strategies (§7). For example, MDd is more complex than WLd, and we expect implementations of the former to require more pruning and suffer from more search errors, while the latter would suffer from more model errors since its space of possible reorderings is smaller. We emphasize that many other translation models can be described this way. Logics for the IBM Models (Brown et al., 1993) would be similar to our logics for phrase-based models. Syntax-based translation logics are similar to parsing logics; a few examples already appear in the literature (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). For simplicity, we will use the MONOTONE logic for the remainder of our examples, but all of them generalize to more complex logics. 4 Adding Local Parameterizations via Semiring-Weighted Deduction So far we have focused solely on unweighted logics, which correspond to search using only ruleC Now suppose we want to find the highest-scoring derivation. Each antecedent item A` has a probability p(A`): if A` is a rule, then the probability is given, otherwise its probability is computed recursively in the same way that we now compute p(C). Since C can be the consequent of multip</context>
<context position="25181" citStr="Dyer et al., 2008" startWordPosition="4337" endWordPosition="4340">search process. Most approaches to this problem focus on synchronous grammars, but it is possible to integrate the targetside language model with a phrase-based translation model. As an exercise, we integrate CKY with the output of logic MONOTONE-GENERATE. The constraint is that the indices of the CKY items unify with the items of the translation logic, which form a word lattice. Note that this logic retains the rules in the basic MONOTONE logic, which are not depicted (Figure 3). The result is a lattice parser on the output of the translation model. Lattice parsing is not new to translation (Dyer et al., 2008), but to our knowledge it has not been used in this way. Viewing Figure 4: Example graphs corresponding to a simple minimal (1) and complete (2) logic, with corresponding nodes in the same color. The statesplitting induced by non-local features produces in a large number of arcs which must be evaluated, which can be reduced by cube pruning. translation as deduction is helpful for the design and construction of novel models. 7 Algorithms Most translation logics are too expensive to exhaustively search. However, the logics conveniently specify the full search space, which forms a hypergraph (Kle</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>C. J. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing word lattice translation. In Proc. ofACL:HLT, pages 1012–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="18911" citStr="Earley (1970)" startWordPosition="3266" endWordPosition="3267">T transform (Cohen et al., 2008). We first define a logic for the non-local parameterization. The logic for an n-gram language model generates sequence e1...eQ by generating each new word given the past n − 1 words.10 item form: [eq, ..., eq+n−2] goal: [eQ−n+2, ..., eQ] [eq+1, ..., eq+n−1] (Logic NGRAM) Now we want to combine NGRAM and MONOTONE. To make things easier, we modify MONOTONE to encode the idea that once a source phrase has been recognized, its target words are generated one at a time. We will use ue and ve to denote (possibly empty) sequences in ej...e0j. Borrowing the notation of Earley (1970), we encode progress using a dotted phrase ue • ve. item form: [i, ue • ve] goal: [I, ue • ve] rules: [i, ue•] R(fi+1...fi&apos;/ejve) [i, ue • ejve] [i0, ej • ve] [i, ueej • ve] (Logic MONOTONE-GENERATE) We combine NGRAM and MONOTONEGENERATE using the PRODUCT transform, which takes two logics as input and essentially does the following.11 1. Create a new item type from the crossproduct of item types in the input logics. 2. Create inference rules for the new item type from the cross-product of all inference rules in the input logics. 3. Constrain the new logic as needed. This is done by hand, but q</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94–102, Feb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>J Blatz</author>
</authors>
<title>Program transformations for optimization of parsing algorithms and other weighted logic programs.</title>
<date>2006</date>
<booktitle>In Proc. ofFormal Grammar,</booktitle>
<pages>45--85</pages>
<contexts>
<context position="17842" citStr="Eisner and Blatz, 2006" startWordPosition="3082" endWordPosition="3085">n D. P(D) = f(d1, ..., dm,..., dM) (6) The problem is that replacing dm with a lowerscoring rule d0m may actually improve f due to the language model dependency. This means that f is nonmonotonic—it does not display the optimal substructure property on partial derivations, which is required for dynamic programming (Cormen et al., 2001). The logics still work for some semirings (e.g. boolean), but not others. Therefore, non-local parameterizations break semiringweighted deduction, because we can no longer use 7General weighted deduction subsumes semiringweighted deduction (Eisner et al., 2005; Eisner and Blatz, 2006; Nederhof, 2003), but semiring-weighted deduction covers all translation models we are aware of, so it is a good first step in applying weighted deduction to translation. 8See Goodman (1999) for additional conditions on semirings used in this framework. 9Eisner and Blatz (2006) give an alternate strategy for the best derivation. the same logic under all semirings. We need new logics; for this we will use a logic programming transform called the PRODUCT transform (Cohen et al., 2008). We first define a logic for the non-local parameterization. The logic for an n-gram language model generates s</context>
<context position="21858" citStr="Eisner and Blatz, 2006" startWordPosition="3768" endWordPosition="3771">ll at once. MONOTONE-GENERATE+NGRAM applies the ngram language model one word at a time. This illuminates a space of search strategies that are to our knowledge unexplored. If a four-word phrase were proposed as an extension of a partial hypothesis in a typical decoder implementation using a five-word language model, all four n-grams will be applied even though the first n-gram might have a very low score. Viewing each n-gram application as producing a new state may yield new strategies for approximate search. We can derive the more familiar logic by applying a different transform: unfolding (Eisner and Blatz, 2006). The idea is to replace an item with the sequence of antecedents used to produce it (similar to function inlining). This gives us MONOTONE-GENERATE+NGRAM SINGLESHOT (Figure 2). We call the ruleset-based logic the minimal logic and the logic enhanced with non-local parameterization the complete logic. Note that the set of variables in the complete logic is a superset of the set of variables in the minimal logic. We can view the minimal logic as a projection of the complete logic into a smaller dimensional space. It is important to note that complete logic is substantially more complex than the</context>
<context position="28692" citStr="Eisner and Blatz, 2006" startWordPosition="4961" endWordPosition="4964">are possible (Tillman and Ney, 2003). It is worth noting that logic FdUW (§3.2), depends on stack pruning for speed. Because the number of stacks is linear in the length of the input, so is the number of unpruned nodes in the search graph. In contrast, the complexity of logic WLd is naturally linear in input length. As mentioned in §3.2, this implies a wide divergence in the model and search errors of these logics, which to our knowledge has not been investigated. 8 Related Work We are not the first to observe that phrase-based models can be represented as logic programs (Eisner et al., 2005; Eisner and Blatz, 2006), but to our knowledge we are the first to provide explicit logics for them.13 We also showed that deductive logic is a useful analytical tool to tackle a variety of problems in translation algorithm design. Our work is strongly influenced by Goodman (1999) and Eisner et al. (2005). They describe many issues not mentioned here, including conditions on semirings, termination conditions, and strategies for cyclic search graphs. However, while their weighted deductive formalism is general, they focus on concerns relevant to parsing, such as boolean semirings and cyclicity. Our work focuses on con</context>
</contexts>
<marker>Eisner, Blatz, 2006</marker>
<rawString>J. Eisner and J. Blatz. 2006. Program transformations for optimization of parsing algorithms and other weighted logic programs. In Proc. ofFormal Grammar, pages 45–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>E Goldlust</author>
<author>N A Smith</author>
</authors>
<title>Compiling comp ling: Weighted dynamic programming and the Dyna language. In</title>
<date>2005</date>
<booktitle>Proc. ofHLT-EMNLP,</booktitle>
<pages>281--290</pages>
<contexts>
<context position="17818" citStr="Eisner et al., 2005" startWordPosition="3078" endWordPosition="3081">model as a function on D. P(D) = f(d1, ..., dm,..., dM) (6) The problem is that replacing dm with a lowerscoring rule d0m may actually improve f due to the language model dependency. This means that f is nonmonotonic—it does not display the optimal substructure property on partial derivations, which is required for dynamic programming (Cormen et al., 2001). The logics still work for some semirings (e.g. boolean), but not others. Therefore, non-local parameterizations break semiringweighted deduction, because we can no longer use 7General weighted deduction subsumes semiringweighted deduction (Eisner et al., 2005; Eisner and Blatz, 2006; Nederhof, 2003), but semiring-weighted deduction covers all translation models we are aware of, so it is a good first step in applying weighted deduction to translation. 8See Goodman (1999) for additional conditions on semirings used in this framework. 9Eisner and Blatz (2006) give an alternate strategy for the best derivation. the same logic under all semirings. We need new logics; for this we will use a logic programming transform called the PRODUCT transform (Cohen et al., 2008). We first define a logic for the non-local parameterization. The logic for an n-gram la</context>
<context position="28667" citStr="Eisner et al., 2005" startWordPosition="4956" endWordPosition="4960">but other strategies are possible (Tillman and Ney, 2003). It is worth noting that logic FdUW (§3.2), depends on stack pruning for speed. Because the number of stacks is linear in the length of the input, so is the number of unpruned nodes in the search graph. In contrast, the complexity of logic WLd is naturally linear in input length. As mentioned in §3.2, this implies a wide divergence in the model and search errors of these logics, which to our knowledge has not been investigated. 8 Related Work We are not the first to observe that phrase-based models can be represented as logic programs (Eisner et al., 2005; Eisner and Blatz, 2006), but to our knowledge we are the first to provide explicit logics for them.13 We also showed that deductive logic is a useful analytical tool to tackle a variety of problems in translation algorithm design. Our work is strongly influenced by Goodman (1999) and Eisner et al. (2005). They describe many issues not mentioned here, including conditions on semirings, termination conditions, and strategies for cyclic search graphs. However, while their weighted deductive formalism is general, they focus on concerns relevant to parsing, such as boolean semirings and cyclicity</context>
</contexts>
<marker>Eisner, Goldlust, Smith, 2005</marker>
<rawString>J. Eisner, E. Goldlust, and N. A. Smith. 2005. Compiling comp ling: Weighted dynamic programming and the Dyna language. In Proc. ofHLT-EMNLP, pages 281–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="16553" citStr="Eisner (2002)" startWordPosition="2877" endWordPosition="2878">eful generalization: semiring-weighted deduction (Goodman, 1999).7 A semiring (A, ®, ®) consists of a domain A, a multiplicative operator ® and an additive operator ®.8 In Equation 3 we use the Viterbi semiring ([0, 1], x, max), while in Equation 4 we use the inside semiring ([0, 1], x,+). The general form of Equations 3 and 4 can be written for weights w E A. w(C)e= w(A1) ® ... ® w(At) (5) Many quantities can be computed simply by using the appropriate semiring. Goodman (1999) describes semirings for the Viterbi derivation, k-best Viterbi derivations, derivation forest, and number of paths.9 Eisner (2002) describes the expectation semiring for parameter learning. Gimpel and Smith (2009) describe approximation semirings for approximate summing in (usually intractable) models. In parsing, the boolean semiring ({T, L}, f1, U) is used to determine grammaticality of an input string. In translation it is relevant for alignment (§6.1). 5 Adding Non-Local Parameterizations with the PRODUCT Transform A problem arises with the semiring-weighted deductive formalism when we add non-local parameterizations such as an n-gram model (Equation 2). Suppose we have a derivation D = (d1,..., dM), where each dm is</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>J. Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Proc. ofACL, pages 1–8, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gallo</author>
<author>G Longo</author>
<author>S Pallottino</author>
</authors>
<title>Directed hypergraphs and applications.</title>
<date>1993</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>42</volume>
<issue>2</issue>
<contexts>
<context position="27104" citStr="Gallo et al., 1993" startWordPosition="4657" endWordPosition="4660">eatures. Figure 4 shows how the added state information can dramatically increase the number of deductions that must be evaluated. Cube pruning works by considering the most promising states paired with the most promising extensions. In this way, it easily fits any search space constructed using the technique of §5. Note that the efficiency of cube pruning is limited by the minimal logic. Stack decoding is a search heuristic that simplifies the complexity of searching a minimal logic. Each item is associated with a stack whose signa12Specifically a B-hypergraph, equivalent to an and-or graph (Gallo et al., 1993) or context-free grammar (Nederhof, 2003). In the degenerate case, this is simply a graph, as is the case with most phrase-based models. rules: { [i,j, ue•] R(fi...fi,/ej...ej,) [i&amp;quot; j, •ej...ej,] [i, j, ue • ejve] 538 item forms: [i, ue • ve], [A, i, ue • ve, i0, u0e • v0e] goal: [5, 0, •, I, ue•] rules: [i, ue•] R(fi+1...fi&apos;/ejve) R(A → ej) [A, i, ue•, i0, ej • ve] [B,i,ue • ve, i00, u00 e • v00 e ] [C, i00, u00 e • v00 e , i0, u0 e • v0 e] R(A → BC) [A, i, ue • ve, i0, u0e • v0e] Figure 3: Logic MONOTONE-GENERATE + CKY [i, ue • ejve] R(A → ej) [A, i, ue • ejve, i, ueej • ve] ture is a projec</context>
</contexts>
<marker>Gallo, Longo, Pallottino, 1993</marker>
<rawString>G. Gallo, G. Longo, and S. Pallottino. 1993. Directed hypergraphs and applications. Discrete Applied Mathematics, 42(2), Apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Approximation semirings: Dynamic programming with non-local features.</title>
<date>2009</date>
<booktitle>In Proc. of EACL,</booktitle>
<contexts>
<context position="16636" citStr="Gimpel and Smith (2009)" startWordPosition="2887" endWordPosition="2890">ring (A, ®, ®) consists of a domain A, a multiplicative operator ® and an additive operator ®.8 In Equation 3 we use the Viterbi semiring ([0, 1], x, max), while in Equation 4 we use the inside semiring ([0, 1], x,+). The general form of Equations 3 and 4 can be written for weights w E A. w(C)e= w(A1) ® ... ® w(At) (5) Many quantities can be computed simply by using the appropriate semiring. Goodman (1999) describes semirings for the Viterbi derivation, k-best Viterbi derivations, derivation forest, and number of paths.9 Eisner (2002) describes the expectation semiring for parameter learning. Gimpel and Smith (2009) describe approximation semirings for approximate summing in (usually intractable) models. In parsing, the boolean semiring ({T, L}, f1, U) is used to determine grammaticality of an input string. In translation it is relevant for alignment (§6.1). 5 Adding Non-Local Parameterizations with the PRODUCT Transform A problem arises with the semiring-weighted deductive formalism when we add non-local parameterizations such as an n-gram model (Equation 2). Suppose we have a derivation D = (d1,..., dM), where each dm is a rule application. We can view the language model as a function on D. P(D) = f(d1</context>
</contexts>
<marker>Gimpel, Smith, 2009</marker>
<rawString>K. Gimpel and N. A. Smith. 2009. Approximation semirings: Dynamic programming with non-local features. In Proc. of EACL, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1684" citStr="Goodman, 1999" startWordPosition="247" endWordPosition="248">bed in the literature becomes more crowded, identifying their common elements and isolating their differences becomes crucial to this understanding. In this work, we present a common framework for model manipulation and analysis that accomplishes this, and use it to derive surprising conclusions about phrase-based models. Most translation algorithms do the same thing: dynamic programming search over a space of weighted rules (§2). Fortunately, we need not search far for modular descriptions of dynamic programming algorithms. Deductive logic (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and nov</context>
<context position="6761" citStr="Goodman (1999)" startWordPosition="1123" endWordPosition="1124">k0] for some grammar rule A → BC and some midpoint k00. Its goal is an item [S, 0, K], indicating that there is a parse of a1...aK rooted at S. A CKY logic describes its actions as inference rules, equivalent to Horn clauses. The inference rule is a list of antecedents, items and rules that must all be true for the inference to occur; and a single consequent that is inferred. To denote the creation of item [A, k, k0] based on existence of rule A → BC and items [B, k, k00] and [C, k00, k0], we write an inference rule with antecedents on the top line and consequent on the second line, following Goodman (1999) and Shieber et al. (1995). R(A → BC) [B, k, k00] [C, k00, k0] [A, k, k0] We now give the complete Logic CKY. item form: [A, k, k0] goal: [S, 0, K] rules: { R(A → BC) [B, k, k00] [C, k00, k0] R(A → ak) [A,k − 1, k] [A, k, k0] (Logic CKY) A benefit of this declarative description is that complexity can be determined by inspection (McAllester, 1999). We elaborate on complexity in §7, but for now it suffices to point out that the number of possible items and possible deductions depends on the product of the domains of the free variables. For example, the number of possible CKY items for a grammar</context>
<context position="16004" citStr="Goodman, 1999" startWordPosition="2776" endWordPosition="2777">ability of all possible derivations, which is useful for parameter estimation (Blunsom et al., 2008). We can do this using the following equation. p(C) = p(C) + (p(A1) × ... × p(AL)) (4) sets. Now we turn our focus to parameterizations. As a first step, we consider only local parameterizations, which make computing the score of a derivation quite simple. We are given a set of inferences in the following form (interpreting side conditions B1...BM as boolean constraints). A1...AL B1...BM 535 Equations 3 and 4 are quite similar. This suggests a useful generalization: semiring-weighted deduction (Goodman, 1999).7 A semiring (A, ®, ®) consists of a domain A, a multiplicative operator ® and an additive operator ®.8 In Equation 3 we use the Viterbi semiring ([0, 1], x, max), while in Equation 4 we use the inside semiring ([0, 1], x,+). The general form of Equations 3 and 4 can be written for weights w E A. w(C)e= w(A1) ® ... ® w(At) (5) Many quantities can be computed simply by using the appropriate semiring. Goodman (1999) describes semirings for the Viterbi derivation, k-best Viterbi derivations, derivation forest, and number of paths.9 Eisner (2002) describes the expectation semiring for parameter l</context>
<context position="18033" citStr="Goodman (1999)" startWordPosition="3113" endWordPosition="3114">does not display the optimal substructure property on partial derivations, which is required for dynamic programming (Cormen et al., 2001). The logics still work for some semirings (e.g. boolean), but not others. Therefore, non-local parameterizations break semiringweighted deduction, because we can no longer use 7General weighted deduction subsumes semiringweighted deduction (Eisner et al., 2005; Eisner and Blatz, 2006; Nederhof, 2003), but semiring-weighted deduction covers all translation models we are aware of, so it is a good first step in applying weighted deduction to translation. 8See Goodman (1999) for additional conditions on semirings used in this framework. 9Eisner and Blatz (2006) give an alternate strategy for the best derivation. the same logic under all semirings. We need new logics; for this we will use a logic programming transform called the PRODUCT transform (Cohen et al., 2008). We first define a logic for the non-local parameterization. The logic for an n-gram language model generates sequence e1...eQ by generating each new word given the past n − 1 words.10 item form: [eq, ..., eq+n−2] goal: [eQ−n+2, ..., eQ] [eq+1, ..., eq+n−1] (Logic NGRAM) Now we want to combine NGRAM a</context>
<context position="28949" citStr="Goodman (1999)" startWordPosition="5006" endWordPosition="5007">y of logic WLd is naturally linear in input length. As mentioned in §3.2, this implies a wide divergence in the model and search errors of these logics, which to our knowledge has not been investigated. 8 Related Work We are not the first to observe that phrase-based models can be represented as logic programs (Eisner et al., 2005; Eisner and Blatz, 2006), but to our knowledge we are the first to provide explicit logics for them.13 We also showed that deductive logic is a useful analytical tool to tackle a variety of problems in translation algorithm design. Our work is strongly influenced by Goodman (1999) and Eisner et al. (2005). They describe many issues not mentioned here, including conditions on semirings, termination conditions, and strategies for cyclic search graphs. However, while their weighted deductive formalism is general, they focus on concerns relevant to parsing, such as boolean semirings and cyclicity. Our work focuses on concerns common for translation, including a general view of non-local parameterizations and cube pruning. 13Huang and Chiang (2007) give an informal example, but do not elaborate on it. 9 Conclusions and Future Work We have described a general framework that </context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>J. Goodman. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="29421" citStr="Huang and Chiang (2007)" startWordPosition="5075" endWordPosition="5078">ve logic is a useful analytical tool to tackle a variety of problems in translation algorithm design. Our work is strongly influenced by Goodman (1999) and Eisner et al. (2005). They describe many issues not mentioned here, including conditions on semirings, termination conditions, and strategies for cyclic search graphs. However, while their weighted deductive formalism is general, they focus on concerns relevant to parsing, such as boolean semirings and cyclicity. Our work focuses on concerns common for translation, including a general view of non-local parameterizations and cube pruning. 13Huang and Chiang (2007) give an informal example, but do not elaborate on it. 9 Conclusions and Future Work We have described a general framework that synthesizes and extends deductive parsing and semiring parsing, and adapts it to translation. Our goal has been to show that logics make an attractive shorthand for description, analysis, and construction of translation models. For instance, we have shown that it is quite easy to mechanically construct search spaces using non-local features, and to create exotic models. We showed that different flavors of phrase-based models should suffer from quite different types of</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. ofACL, pages 144–151, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm schemata and data structures in syntactic processing.</title>
<date>1986</date>
<booktitle>Readings in Natural Language Processing,</booktitle>
<pages>35--70</pages>
<editor>In B. J. Grosz, K. S. Jones, and B. L. Webber, editors,</editor>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="30322" citStr="Kay, 1986" startWordPosition="5226" endWordPosition="5227">r description, analysis, and construction of translation models. For instance, we have shown that it is quite easy to mechanically construct search spaces using non-local features, and to create exotic models. We showed that different flavors of phrase-based models should suffer from quite different types of error, a problem that to our knowledge was heretofore unknown. However, we have only scratched the surface, and we believe it is possibly to unify a wide variety of translation algorithms. For example, we believe that cube pruning can be described as an agenda discipline in chart parsing (Kay, 1986). Although the work presented here is abstract, our motivation is practical. Isolating the errors in translation systems is a difficult task which can be made easier by describing and analyzing models in a modular way (Auli et al., 2009). Furthermore, building large-scale translation systems from scratch should be unnecessary if existing systems were built using modular logics and algorithms. We aim to build such systems. Acknowledgments This work developed from discussions with Phil Blunsom, Chris Callison-Burch, Chris Dyer, Hieu Hoang, Martin Kay, Philipp Koehn, Josh Schroeder, and Lane Schw</context>
</contexts>
<marker>Kay, 1986</marker>
<rawString>M. Kay. 1986. Algorithm schemata and data structures in syntactic processing. In B. J. Grosz, K. S. Jones, and B. L. Webber, editors, Readings in Natural Language Processing, pages 35–70. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proc. ofIWPT.</booktitle>
<contexts>
<context position="25802" citStr="Klein and Manning, 2001" startWordPosition="4442" endWordPosition="4445">08), but to our knowledge it has not been used in this way. Viewing Figure 4: Example graphs corresponding to a simple minimal (1) and complete (2) logic, with corresponding nodes in the same color. The statesplitting induced by non-local features produces in a large number of arcs which must be evaluated, which can be reduced by cube pruning. translation as deduction is helpful for the design and construction of novel models. 7 Algorithms Most translation logics are too expensive to exhaustively search. However, the logics conveniently specify the full search space, which forms a hypergraph (Klein and Manning, 2001).12 The equivalence is useful for complexity analysis: items correspond to nodes and deductions correspond to hyperarcs. These equivalences make it easy to compute algorithmic bounds. Cube pruning (Chiang, 2007) is an approximate search technique for syntax-based translation models with integrated language models. It operates on two objects: a −LM graph containing no language model state, and a +LM hypergraph containing state. The idea is to generate a fixed number of nodes in the +LM for each node in the −LM graph, using a clever enumeration strategy. We can view cube pruning as arising from </context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>D. Klein and C. Manning. 2001. Parsing and hypergraphs. In Proc. ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="12054" citStr="Koehn et al., 2007" startWordPosition="2052" endWordPosition="2055">a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as logics permits us to make some simple analyses. First, it should be clear that these reordering strategies define overlapping but not identical search spaces: for most values of d it is impossible to find d0 such that any of the other strategies would be identical (except for degenerate cases 3Moore and Quirk (2007) give a nice description ofMDd. 4We do not know if WLd is documented anywhere, but from inspection it is used in Moses (Koehn et al., 2007). This was confirmed by Philipp Koehn and Hieu Hoang (p.c.). 5When a phrase covers the first uncovered word in the source sentence, the new first uncovered word may be further along in the sentence (if the phrase completely filled a gap), or just past the end of the phrase (otherwise). 6We could not identify this strategy in the IBM patents. item form: [i] goal: [I] 534 (1) item form: [i, {0,1}I] rule: [i00,V ] R(fi+1...fi0/ej...ej0) [i0, V ∨ 0i1i0−i0I−i0] V ∧ 0i1i0−i0I−i0 = 0I, |i − i00 |≤ d goal: [i ∈ [I − d, I], 1I] (3) item form: [i, [1, I + d]d] goal: [I, [I + 1, I + d]] (2) item form: [i</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL Demo and Poster Sessions, pages 177–180, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofHLT-EMNLP,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="11497" citStr="Kumar and Byrne (2005)" startWordPosition="1953" endWordPosition="1956">d, but linear in sentence length, O(d22dI). The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint.6 It requires at least one of the leftmost d uncovered words to be covered by a new phrase. Items in this strategy contain the index i of the rightmost covered word and a vector U E [1, I]d of the d leftmost uncovered words (Figure 1). Its complexity is O(dI (d+1) ), which is roughly exponential in d. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as logics permits us to make some simple analyses. First, it should be clear that these reordering strategies define overlapping but not identical search spaces: for most values of d it is impossible to find d0 such that any of the other strategies would be identical (except for degenerate cases 3Moore and Quirk (2007) give a nice description ofMDd. 4We do not know if WLd is documented anywhere, but from inspection it is used in Moses (Koehn et al., 2007). This was confirmed by Philipp Koehn and H</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>S. Kumar and W. Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proc. ofHLT-EMNLP, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cˆot´e</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL-COLING,</booktitle>
<pages>761--768</pages>
<marker>Liang, Bouchard-Cˆot´e, Taskar, Klein, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cˆot´e, B. Taskar, and D. Klein. 2006. An end-to-end discriminative approach to machine translation. In Proc. of ACL-COLING, pages 761–768, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Statistical machine translation.</title>
<date>2008</date>
<journal>ACM Computing Surveys,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="2573" citStr="Lopez, 2008" startWordPosition="390" endWordPosition="391">ughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 Translation Models A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization (Lopez, 2008). A ruleset licenses the steps by which a source string f1...fI may be rewritten as a target string e1...eJ, thereby defining the finite set of all possible rewritings of a source string. A parameterization defines a weight function over every sequence of rule applications. In a phrase-based model, the ruleset is simply the unweighted phrase table, where each phrase pair fi...fi,/ej...ej, states that phrase fi...fi, in the source is rewritten as ej...ej, in the the target. The model operates by iteratively applying rewrites to the source sentence until each source word has been consumed by exa</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>A. Lopez. 2008. Statistical machine translation. ACM Computing Surveys, 40(3), Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Mari˜no</author>
<author>R E Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A R Fonollosa</author>
<author>M R Costajuss`a</author>
</authors>
<title>N-gram based statistical machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>549</pages>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costajuss`a, 2006</marker>
<rawString>J. B. Mari˜no, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, J. A. R. Fonollosa, and M. R. Costajuss`a. 2006. N-gram based statistical machine translation. Computational Linguistics, 32(4):527– 549, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
</authors>
<title>On the complexity analysis of static analyses.</title>
<date>1999</date>
<booktitle>In Proc. of Static Analysis Symposium,</booktitle>
<volume>1694</volume>
<pages>ofLNCS.</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="7110" citStr="McAllester, 1999" startWordPosition="1195" endWordPosition="1196">gle consequent that is inferred. To denote the creation of item [A, k, k0] based on existence of rule A → BC and items [B, k, k00] and [C, k00, k0], we write an inference rule with antecedents on the top line and consequent on the second line, following Goodman (1999) and Shieber et al. (1995). R(A → BC) [B, k, k00] [C, k00, k0] [A, k, k0] We now give the complete Logic CKY. item form: [A, k, k0] goal: [S, 0, K] rules: { R(A → BC) [B, k, k00] [C, k00, k0] R(A → ak) [A,k − 1, k] [A, k, k0] (Logic CKY) A benefit of this declarative description is that complexity can be determined by inspection (McAllester, 1999). We elaborate on complexity in §7, but for now it suffices to point out that the number of possible items and possible deductions depends on the product of the domains of the free variables. For example, the number of possible CKY items for a grammar with G nonterminals is O(GK2), because k and k0 are both in range [0, K]. Likewise, the number of possible inference rules that can fire is O(G3K3). 3.1 A Simple Deductive Decoder For our first example of a translation logic we consider a simple case: monotone decoding (Mari˜no et al., 2006; Zens and Ney, 2004). Here, rewrite rules are applied st</context>
</contexts>
<marker>McAllester, 1999</marker>
<rawString>D. McAllester. 1999. On the complexity analysis of static analyses. In Proc. of Static Analysis Symposium, volume 1694/1999 ofLNCS. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>654--661</pages>
<contexts>
<context position="1916" citStr="Melamed, 2004" startWordPosition="285" endWordPosition="286">t accomplishes this, and use it to derive surprising conclusions about phrase-based models. Most translation algorithms do the same thing: dynamic programming search over a space of weighted rules (§2). Fortunately, we need not search far for modular descriptions of dynamic programming algorithms. Deductive logic (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 Translation Models A translation model consists of two distinct elements: an</context>
<context position="14317" citStr="Melamed, 2004" startWordPosition="2488" endWordPosition="2489"> For example, MDd is more complex than WLd, and we expect implementations of the former to require more pruning and suffer from more search errors, while the latter would suffer from more model errors since its space of possible reorderings is smaller. We emphasize that many other translation models can be described this way. Logics for the IBM Models (Brown et al., 1993) would be similar to our logics for phrase-based models. Syntax-based translation logics are similar to parsing logics; a few examples already appear in the literature (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). For simplicity, we will use the MONOTONE logic for the remainder of our examples, but all of them generalize to more complex logics. 4 Adding Local Parameterizations via Semiring-Weighted Deduction So far we have focused solely on unweighted logics, which correspond to search using only ruleC Now suppose we want to find the highest-scoring derivation. Each antecedent item A` has a probability p(A`): if A` is a rule, then the probability is given, otherwise its probability is computed recursively in the same way that we now compute p(C). Since C can be the consequent of multiple deductions, w</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. D. Melamed. 2004. Statistical machine translation by parsing. In Proc. ofACL, pages 654–661, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>C Quirk</author>
</authors>
<title>Faster beam-search decoding for phrasal statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ofMT Summit.</booktitle>
<contexts>
<context position="11915" citStr="Moore and Quirk (2007)" startWordPosition="2026" endWordPosition="2029"> Its complexity is O(dI (d+1) ), which is roughly exponential in d. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as logics permits us to make some simple analyses. First, it should be clear that these reordering strategies define overlapping but not identical search spaces: for most values of d it is impossible to find d0 such that any of the other strategies would be identical (except for degenerate cases 3Moore and Quirk (2007) give a nice description ofMDd. 4We do not know if WLd is documented anywhere, but from inspection it is used in Moses (Koehn et al., 2007). This was confirmed by Philipp Koehn and Hieu Hoang (p.c.). 5When a phrase covers the first uncovered word in the source sentence, the new first uncovered word may be further along in the sentence (if the phrase completely filled a gap), or just past the end of the phrase (otherwise). 6We could not identify this strategy in the IBM patents. item form: [i] goal: [I] 534 (1) item form: [i, {0,1}I] rule: [i00,V ] R(fi+1...fi0/ej...ej0) [i0, V ∨ 0i1i0−i0I−i0] </context>
</contexts>
<marker>Moore, Quirk, 2007</marker>
<rawString>R. C. Moore and C. Quirk. 2007. Faster beam-search decoding for phrasal statistical machine translation. In Proc. ofMT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17859" citStr="Nederhof, 2003" startWordPosition="3086" endWordPosition="3087">m,..., dM) (6) The problem is that replacing dm with a lowerscoring rule d0m may actually improve f due to the language model dependency. This means that f is nonmonotonic—it does not display the optimal substructure property on partial derivations, which is required for dynamic programming (Cormen et al., 2001). The logics still work for some semirings (e.g. boolean), but not others. Therefore, non-local parameterizations break semiringweighted deduction, because we can no longer use 7General weighted deduction subsumes semiringweighted deduction (Eisner et al., 2005; Eisner and Blatz, 2006; Nederhof, 2003), but semiring-weighted deduction covers all translation models we are aware of, so it is a good first step in applying weighted deduction to translation. 8See Goodman (1999) for additional conditions on semirings used in this framework. 9Eisner and Blatz (2006) give an alternate strategy for the best derivation. the same logic under all semirings. We need new logics; for this we will use a logic programming transform called the PRODUCT transform (Cohen et al., 2008). We first define a logic for the non-local parameterization. The logic for an n-gram language model generates sequence e1...eQ b</context>
<context position="27145" citStr="Nederhof, 2003" startWordPosition="4664" endWordPosition="4666">nformation can dramatically increase the number of deductions that must be evaluated. Cube pruning works by considering the most promising states paired with the most promising extensions. In this way, it easily fits any search space constructed using the technique of §5. Note that the efficiency of cube pruning is limited by the minimal logic. Stack decoding is a search heuristic that simplifies the complexity of searching a minimal logic. Each item is associated with a stack whose signa12Specifically a B-hypergraph, equivalent to an and-or graph (Gallo et al., 1993) or context-free grammar (Nederhof, 2003). In the degenerate case, this is simply a graph, as is the case with most phrase-based models. rules: { [i,j, ue•] R(fi...fi,/ej...ej,) [i&amp;quot; j, •ej...ej,] [i, j, ue • ejve] 538 item forms: [i, ue • ve], [A, i, ue • ve, i0, u0e • v0e] goal: [5, 0, •, I, ue•] rules: [i, ue•] R(fi+1...fi&apos;/ejve) R(A → ej) [A, i, ue•, i0, ej • ve] [B,i,ue • ve, i00, u00 e • v00 e ] [C, i00, u00 e • v00 e , i0, u0 e • v0 e] R(A → BC) [A, i, ue • ve, i0, u0e • v0e] Figure 3: Logic MONOTONE-GENERATE + CKY [i, ue • ejve] R(A → ej) [A, i, ue • ejve, i, ueej • ve] ture is a projection of the item signature (or a predicat</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>M.-J. Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Computational Linguistics, 29(1):135–143, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. ofHLT-NAACL,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="24399" citStr="Och et al. (2004)" startWordPosition="4200" endWordPosition="4203">Under the boolean semiring, this (minimal) logic decides if a training example is reachable by the model, which is required by some discriminative training regimens (Liang et al., 2006; Blunsom et al., 2008). We can also compute the Viterbi derivation or the sum over all derivations of a training example, needed for some parameter estimation methods. Cohen et al. (2008) derive an alignment logic for ITG from the product of two CKY logics. 6.2 Translation Model Design A motivation for many syntax-based translation models is to use target-side syntax as a language model (Charniak et al., 2003). Och et al. (2004) showed that simply parsing the N-best outputs of a phrase-based model did not work; to obtain the full power of a language model, we need to integrate it into the search process. Most approaches to this problem focus on synchronous grammars, but it is possible to integrate the targetside language model with a phrase-based translation model. As an exercise, we integrate CKY with the output of logic MONOTONE-GENERATE. The constraint is that the indices of the CKY items unify with the items of the translation logic, which form a word lattice. Note that this logic retains the rules in the basic M</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of features for statistical machine translation. In Proc. ofHLT-NAACL, pages 161–168, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Parsing as deduction.</title>
<date>1983</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="1643" citStr="Pereira and Warren, 1983" startWordPosition="239" endWordPosition="242">mportant for success. As the space of systems described in the literature becomes more crowded, identifying their common elements and isolating their differences becomes crucial to this understanding. In this work, we present a common framework for model manipulation and analysis that accomplishes this, and use it to derive surprising conclusions about phrase-based models. Most translation algorithms do the same thing: dynamic programming search over a space of weighted rules (§2). Fortunately, we need not search far for modular descriptions of dynamic programming algorithms. Deductive logic (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal p</context>
</contexts>
<marker>Pereira, Warren, 1983</marker>
<rawString>F. C. N. Pereira and D. H. D. Warren. 1983. Parsing as deduction. In Proc. ofACL, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
<author>Y Schabes</author>
<author>F C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<booktitle>Journal ofLogic Programming,</booktitle>
<pages>24--1</pages>
<contexts>
<context position="6787" citStr="Shieber et al. (1995)" startWordPosition="1126" endWordPosition="1129">r rule A → BC and some midpoint k00. Its goal is an item [S, 0, K], indicating that there is a parse of a1...aK rooted at S. A CKY logic describes its actions as inference rules, equivalent to Horn clauses. The inference rule is a list of antecedents, items and rules that must all be true for the inference to occur; and a single consequent that is inferred. To denote the creation of item [A, k, k0] based on existence of rule A → BC and items [B, k, k00] and [C, k00, k0], we write an inference rule with antecedents on the top line and consequent on the second line, following Goodman (1999) and Shieber et al. (1995). R(A → BC) [B, k, k00] [C, k00, k0] [A, k, k0] We now give the complete Logic CKY. item form: [A, k, k0] goal: [S, 0, K] rules: { R(A → BC) [B, k, k00] [C, k00, k0] R(A → ak) [A,k − 1, k] [A, k, k0] (Logic CKY) A benefit of this declarative description is that complexity can be determined by inspection (McAllester, 1999). We elaborate on complexity in §7, but for now it suffices to point out that the number of possible items and possible deductions depends on the product of the domains of the free variables. For example, the number of possible CKY items for a grammar with G nonterminals is O(</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal ofLogic Programming, 24(1–2):3–36, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillman</author>
<author>H Ney</author>
</authors>
<title>Word reordering and a dynamic programming beam search algorithm for statistical machine translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11001" citStr="Tillman and Ney (2003)" startWordPosition="1861" endWordPosition="1864">d). MDd can produce partial derivations that cannot be completed by any allowed sequence of jumps. To prevent this, the Window Length d strategy (WLd) enforces a tighter restriction that the last word of a phrase chosen for translation cannot be more than d words from the leftmost untranslated word in the source (Figure 1).4 For this logic we use a bitwise shift operator (&lt;), and a predicate (α1) that counts the number of leading ones in a bit array.5 Its runtime is exponential in parameter d, but linear in sentence length, O(d22dI). The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint.6 It requires at least one of the leftmost d uncovered words to be covered by a new phrase. Items in this strategy contain the index i of the rightmost covered word and a vector U E [1, I]d of the d leftmost uncovered words (Figure 1). Its complexity is O(dI (d+1) ), which is roughly exponential in d. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as log</context>
<context position="28105" citStr="Tillman and Ney, 2003" startWordPosition="4853" endWordPosition="4856">ve, i00, u00 e • v00 e ] [C, i00, u00 e • v00 e , i0, u0 e • v0 e] R(A → BC) [A, i, ue • ve, i0, u0e • v0e] Figure 3: Logic MONOTONE-GENERATE + CKY [i, ue • ejve] R(A → ej) [A, i, ue • ejve, i, ueej • ve] ture is a projection of the item signature (or a predicate on the item signatures)—multiple items are associated to the same stack. The strength of the pruning (and resulting complexity improvements) depending on how much the projection reduces the search space. In many phrase-based implementations the stack signature is just the number of words translated, but other strategies are possible (Tillman and Ney, 2003). It is worth noting that logic FdUW (§3.2), depends on stack pruning for speed. Because the number of stacks is linear in the length of the input, so is the number of unpruned nodes in the search graph. In contrast, the complexity of logic WLd is naturally linear in input length. As mentioned in §3.2, this implies a wide divergence in the model and search errors of these logics, which to our knowledge has not been investigated. 8 Related Work We are not the first to observe that phrase-based models can be represented as logic programs (Eisner et al., 2005; Eisner and Blatz, 2006), but to our </context>
</contexts>
<marker>Tillman, Ney, 2003</marker>
<rawString>C. Tillman and H. Ney. 2003. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational Linguistics, 29(1):98–133, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>A Zollmann</author>
<author>S Vogel</author>
</authors>
<title>An efficient two-pass approach to synchronous-CFG driven statistical MT.</title>
<date>2007</date>
<booktitle>In Proc. ofHLT-NAACL.</booktitle>
<contexts>
<context position="1881" citStr="Venugopal et al., 2007" startWordPosition="277" endWordPosition="280">ork for model manipulation and analysis that accomplishes this, and use it to derive surprising conclusions about phrase-based models. Most translation algorithms do the same thing: dynamic programming search over a space of weighted rules (§2). Fortunately, we need not search far for modular descriptions of dynamic programming algorithms. Deductive logic (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 Translation Models A translation model co</context>
<context position="14282" citStr="Venugopal et al., 2007" startWordPosition="2480" endWordPosition="2483">decoding speed and pruning strategies (§7). For example, MDd is more complex than WLd, and we expect implementations of the former to require more pruning and suffer from more search errors, while the latter would suffer from more model errors since its space of possible reorderings is smaller. We emphasize that many other translation models can be described this way. Logics for the IBM Models (Brown et al., 1993) would be similar to our logics for phrase-based models. Syntax-based translation logics are similar to parsing logics; a few examples already appear in the literature (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). For simplicity, we will use the MONOTONE logic for the remainder of our examples, but all of them generalize to more complex logics. 4 Adding Local Parameterizations via Semiring-Weighted Deduction So far we have focused solely on unweighted logics, which correspond to search using only ruleC Now suppose we want to find the highest-scoring derivation. Each antecedent item A` has a probability p(A`): if A` is a rule, then the probability is given, otherwise its probability is computed recursively in the same way that we now compute p(C). Since C can be the c</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>A. Venugopal, A. Zollmann, and S. Vogel. 2007. An efficient two-pass approach to synchronous-CFG driven statistical MT. In Proc. ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>152--158</pages>
<contexts>
<context position="4914" citStr="Wu, 1996" startWordPosition="775" endWordPosition="776">n is also affected, because the terms ea−n+1...ea may come from more than one rule. In other words, this parameterization encodes a dependency between the steps in a derivation. We call this a non-local parameterization. 3 Translation As Deduction For the first part of the discussion that follows, we consider deductive logics purely over unweighted rulesets. As a way to introduce deductive logic, we consider the CKY algorithm for context-free parsing, a common example that we will revisit in §6.2. It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996). In the discussion that follows, we use A, B, and C to denote arbitrary nonterminal symbols, S to denote the start nonterminal symbol, and a to denote a terminal symbol. CKY works on grammars in Chomsky normal form: all rules are either binary as in A → BC, or unary as in A → a. The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function (Church and Patil, 1982), so dynamic programming is crucial for efficiency. CKY computes all parses in cubic time by reusing subparses. To parse a sentence a1...aK, we compute a set of it</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>D. Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. ofACL, pages 152–158, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in phrasebased statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>257--264</pages>
<contexts>
<context position="7674" citStr="Zens and Ney, 2004" startWordPosition="1294" endWordPosition="1297">xity can be determined by inspection (McAllester, 1999). We elaborate on complexity in §7, but for now it suffices to point out that the number of possible items and possible deductions depends on the product of the domains of the free variables. For example, the number of possible CKY items for a grammar with G nonterminals is O(GK2), because k and k0 are both in range [0, K]. Likewise, the number of possible inference rules that can fire is O(G3K3). 3.1 A Simple Deductive Decoder For our first example of a translation logic we consider a simple case: monotone decoding (Mari˜no et al., 2006; Zens and Ney, 2004). Here, rewrite rules are applied strictly from left to right on the source sentence. Despite its simplicity, the search 533 space can be very large—in the limit there could be a translation for every possible segmentation of the sentence, so there are exponentially many possible derivations. Fortunately, we know that monotone decoding can easily be cast as a dynamic programming problem. For any position i in the source sentence f1...fI, we can freely combine any partial derivation covering f1...fi on its left with any partial derivation covering fi+1...fI on its right to yield a complete deri</context>
<context position="11025" citStr="Zens and Ney (2004)" startWordPosition="1866" endWordPosition="1869"> derivations that cannot be completed by any allowed sequence of jumps. To prevent this, the Window Length d strategy (WLd) enforces a tighter restriction that the last word of a phrase chosen for translation cannot be more than d words from the leftmost untranslated word in the source (Figure 1).4 For this logic we use a bitwise shift operator (&lt;), and a predicate (α1) that counts the number of leading ones in a bit array.5 Its runtime is exponential in parameter d, but linear in sentence length, O(d22dI). The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint.6 It requires at least one of the leftmost d uncovered words to be covered by a new phrase. Items in this strategy contain the index i of the rightmost covered word and a vector U E [1, I]d of the d leftmost uncovered words (Figure 1). Its complexity is O(dI (d+1) ), which is roughly exponential in d. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as logics permits us to make s</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney. 2004. Improvements in phrasebased statistical machine translation. In Proc. of HLT-NAACL, pages 257–264, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>