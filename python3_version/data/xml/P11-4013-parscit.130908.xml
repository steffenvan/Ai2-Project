<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017883">
<title confidence="0.9981565">
Wikulu: An Extensible Architecture for Integrating Natural Language
Processing Techniques with Wikis
</title>
<author confidence="0.980819">
Daniel B¨ar, Nicolai Erbs, Torsten Zesch, and Iryna Gurevych
</author>
<affiliation confidence="0.882368">
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universit¨at Darmstadt
</affiliation>
<address confidence="0.887">
Hochschulstrasse 10, D-64289 Darmstadt, Germany
</address>
<email confidence="0.938631">
www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.992021" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992302">
We present Wikulu1, a system focusing on
supporting wiki users with their everyday
tasks by means of an intelligent interface.
Wikulu is implemented as an extensible archi-
tecture which transparently integrates natural
language processing (NLP) techniques with
wikis. It is designed to be deployed with any
wiki platform, and the current prototype inte-
grates a wide range of NLP algorithms such
as keyphrase extraction, link discovery, text
segmentation, summarization, or text similar-
ity. Additionally, we show how Wikulu can
be applied for visually analyzing the results
of NLP algorithms, educational purposes, and
enabling semantic wikis.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968857142857">
Wikis are web-based, collaborative content author-
ing systems (Leuf and Cunningham, 2001). As they
offer fast and simple means for adding and editing
content, they are used for various purposes such as
creating encyclopedias (e.g. Wikipedia2), construct-
ing dictionaries (e.g. Wiktionary3), or hosting online
communities (e.g. ACLWiki4). However, as wikis do
not enforce their users to structure pages or add com-
plementary metadata, wikis often end up as a mass
of unmanageable pages with meaningless page titles
and no usable link structure (Buffa, 2006).
To solve this issue, we present the Wikulu sys-
tem which uses natural language processing to sup-
port wiki users with their typical tasks of adding,
</bodyText>
<footnote confidence="0.9800674">
1Portmanteau of the Hawaiian terms wiki (“fast”) and kukulu
(“to organize”)
2http://www.wikipedia.org
3http://www.wiktionary.org
4http://aclweb.org/aclwiki
</footnote>
<page confidence="0.995013">
74
</page>
<bodyText confidence="0.999951533333333">
organizing, and finding content. For example,
Wikulu supports users with reading longer texts by
highlighting keyphrases using keyphrase extraction
methods such as TextRank (Mihalcea and Tarau,
2004). Support integrated in Wikulu also includes
text segmentation to segment long pages, text simi-
larity for detecting potential duplicates, or text sum-
marization to facilitate reading of lengthy pages.
Generally, Wikulu allows to integrate any NLP com-
ponent which conforms to the standards of Apache
UIMA (Ferrucci and Lally, 2004).
Wikulu is designed to integrate seamlessly with
any wiki. Our system is implemented as an HTTP
proxy server which intercepts the communication
between the web browser and the underlying wiki
engine. No further modifications to the original wiki
installation are necessary. Currently, our system pro-
totype contains adaptors for two widely used wiki
engines: MediaWiki5 and TWiki6. Adaptors for other
wiki engines can be added with minimal effort. Gen-
erally, Wikulu could also be applied to any web-
based system other than wikis with only slight mod-
ifications to its architecture.
In Figure 1, we show the integration of Wikulu
with Wikipedia.7 The additional user interface com-
ponents are integrated into the default toolbar (high-
lighted by a red box in the screenshot). In this ex-
ample, the user has requested keyphrase highlight-
ing in order to quickly get an idea about the main
content of the wiki article. Wikulu then invokes the
</bodyText>
<footnote confidence="0.863118">
5http://mediawiki.org (e.g. used by Wikipedia)
6http://twiki.org (often used for corporate wikis)
7As screenshots only provide a limited overview of
Wikulu’s capabilities, we refer the reader to a screencast:
http://www.ukp.tu-darmstadt.de/research/
projects/wikulu
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 74–79,
Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics
</footnote>
<figureCaption confidence="0.999419333333333">
Figure 1: Integration of Wikulu with Wikipedia. The aug-
mented toolbar (red box) and the results of a keyphrase
extraction algorithm (yellow text spans) are highlighted.
Figure 2: Automatic discovery of links to other wiki ar-
ticles. Suitable text phrases to place a link on are high-
lighted in green.
</figureCaption>
<bodyText confidence="0.99956375">
corresponding NLP component, and highlights the
returned keyphrases in the article. In the next sec-
tion, we give a more detailed overview of the differ-
ent types of support provided by Wikulu.
</bodyText>
<sectionHeader confidence="0.595083" genericHeader="method">
2 Supporting Wiki Users by Means of NLP
</sectionHeader>
<bodyText confidence="0.999787814814815">
In this section, we present the different types of
NLP-enabled support provided by Wikulu.
Detecting Duplicates Whenever users add new
content to a wiki there is the danger of duplicating
already contained information. In order to avoid du-
plication, users would need comprehensive knowl-
edge of what content is already present in the wiki,
which is almost impossible for large wikis like
Wikipedia. Wikulu helps to detect potential du-
plicates by computing the text similarity between
newly added content and each existing wiki page.
If a potential duplicate is detected, the user is noti-
fied and may decide to augment the duplicate page
instead of adding a new one. Wikulu integrates text
similarity measures such as Explicit Semantic Anal-
ysis (Gabrilovich and Markovitch, 2007) and Latent
Semantic Analysis (Landauer et al., 1998).
Suggesting Links While many wiki users read-
ily add textual contents to wikis, they often re-
strain from also adding links to related pages. How-
ever, links in wikis are crucial as they allow users
to quickly navigate from one page to another, or
browse through the wiki. Therefore, it may be rea-
sonable to augment a page about the topic sentiment
analysis by a link to a page providing related in-
formation such as evaluation datasets. Wikulu sup-
ports users in this tedious task by automatically sug-
gesting links. Link suggestion thereby is a two-step
process: (a) first, suitable text phrases are extracted
which might be worth to place a link on (see Fig-
ure 2), and (b) for each phrase, related pages are
ranked by comparing their relevance to the current
page, and then presented to the user. The user may
thus decide whether she wants to use a detected
phrase as a link or not, and if so, which other wiki
page to link this phrase to. Wikulu currently inte-
grates link suggestion algorithms by Geva (2007)
and Itakura and Clarke (2007).
Semantic Searching The capabilities of a wiki’s
built-in search engine are typically rather limited
as it traditionally performs e.g. keyword-based re-
trieval. If that keyword is not found in the wiki, the
query returns an empty result set. However, a page
might exist which is semantically related to the key-
word, and should thus yield a match.
As the search engine is typically a core part of the
wiki system, it is rather difficult to modify its be-
havior. However, by leveraging Wikulu’s architec-
ture, we can replace the default search mechanisms
by algorithms which allow for semantic search to al-
leviate the vocabulary mismatch problem (Gurevych
et al., 2007).
Segmenting Long Pages Due to the open edit-
ing policy of wikis, pages tend to grow rather fast.
</bodyText>
<page confidence="0.99745">
75
</page>
<figureCaption confidence="0.999070666666667">
Figure 3: Analysis of a wiki article with respect to topical
coherence. Suggested segment breaks are highlighted by
yellow bars.
</figureCaption>
<bodyText confidence="0.952966866666667">
For users, it is thus a major challenge to keep an
overview of what content is present on a certain
page. Wikulu therefore supports users by analyzing
long pages through employing text segmentation al-
gorithms which detect topically coherent segments
of text. It then suggests segment boundaries which
the user may or may not accept for inserting a sub-
heading which makes pages easier to read and better
to navigate. As shown in Figure 3, users are also en-
couraged to set a title for each segment.$ When ac-
cepting one or more of these suggested boundaries,
Wikulu stores them persistently in the wiki. Wikulu
currently integrates text segmentation methods such
as TextTiling (Hearst, 1997) or C99 (Choi, 2000).
Summarizing Pages Similarly to segmenting
pages, Wikulu makes long wiki pages more acces-
sible by generating an extractive summary. While
generative summaries generate a summary in own
words, extractive summaries analyze the original
wiki text sentence-by-sentence, rank each sentence,
and return a list of the most important ones (see Fig-
ure 4). Wikulu integrates extractive text summariza-
tion methods such as LexRank (Erkan and Radev,
2004).
Highlighting Keyphrases Another approach to
assist users in better grasping the idea of a wiki page
at a glance is to highlight important keyphrases (see
Figure 1). As Tucker and Whittaker (2009) have
$In future work, we plan to suggest suitable titles for each
segment automatically.
</bodyText>
<figureCaption confidence="0.9755905">
Figure 4: Extractive summary of the original wiki page
shown in Figure 3
</figureCaption>
<bodyText confidence="0.99993">
shown, highlighting important phrases assists users
with reading longer texts and yields faster under-
standing. Wikulu thus improves readability by em-
ploying automatic keyphrase extraction algorithms.
Additionally, Wikulu allows to dynamically adjust
the number of keyphrases shown by presenting a
slider to the user. We integrated keyphrase extrac-
tion methods such as TextRank (Mihalcea and Tarau,
2004) and KEA (Witten et al., 1999).
</bodyText>
<sectionHeader confidence="0.993743" genericHeader="method">
3 Further Use Cases
</sectionHeader>
<bodyText confidence="0.954166">
Further use cases for supporting wiki users include
</bodyText>
<listItem confidence="0.90156">
(i) visually analyzing the results of NLP algorithms,
(ii) educational purposes, and (iii) enabling semantic
wikis.
</listItem>
<subsectionHeader confidence="0.52045">
Visually Analyzing the Results of NLP Algo-
</subsectionHeader>
<bodyText confidence="0.999805666666667">
rithms Wikulu facilitates analyzing the results of
NLP algorithms by using wiki pages as input doc-
uments and visualizing the results directly on that
page. Consider an NLP algorithm which performs
sentiment analysis. Typically, we were to put our
analysis sentences in a text file, launch the NLP ap-
plication, process the file, and would read the output
from either a built-in console or a separate output
file. This procedure suffers from two major draw-
backs: (a) it is inconvenient to copy existing data
into a custom input format which can be fed into the
NLP system, and (b) the textual output does not al-
low presenting the results in a visually rich manner.
Wikulu tackles both challenges by using wiki
pages as input/output documents. For instance,
</bodyText>
<page confidence="0.927033">
76
</page>
<bodyText confidence="0.999916964285714">
by running the sentiment analysis component right
from within the wiki, its output can be written back
to the originating wiki page, resulting in visually
rich, possibly interactive presentations.
Educational Purposes Wikulu is a handy tool for
educational purposes as it allows to (a) rapidly create
test data in a collaborative manner (see Section 2),
and (b) visualize the results of NLP algorithms, as
described above. Students can gather hands-on ex-
perience by experimenting with NLP components in
an easy-to-use wiki system. They can both collab-
oratively edit input documents, and explore possi-
ble results of e.g. different configurations of NLP
components. In our system prototype, we integrated
highlighting parts-of-speech which have been deter-
mined by a POS tagger.
Enabling Semantic Wikis Semantic wikis such
as the Semantic MediaWiki (Kr¨otzsch et al., 2006)
augment standard wikis with machine-readable se-
mantic annotations of pages and links. As those
annotations have to be entered manually, this step
is often skipped by users which severely limits the
usefulness of semantic wikis. Wikulu could support
users e.g. by automatically suggesting the type of a
link by means of relation detection or the type of a
page by means of text categorization. Thus, Wikulu
could constitute an important step towards the se-
mantification of the content contained in wikis.
</bodyText>
<sectionHeader confidence="0.99113" genericHeader="method">
4 System Architecture
</sectionHeader>
<bodyText confidence="0.99983675">
In this section, we detail our system architecture and
describe what is necessary to make NLP algorithms
available through our system. We also give a walk-
through of Wikulu’s information flow.
</bodyText>
<subsectionHeader confidence="0.995972">
4.1 Core Components
</subsectionHeader>
<bodyText confidence="0.999913588235294">
Wikulu builds upon a modular architecture, as de-
picted in Figure 5. It acts as an HTTP proxy server
which intercepts the communication between the
web browser and the target wiki engine, while it al-
lows to run any Apache UIMA-compliant NLP com-
ponent using an extensible plugin mechanism.
In the remainder of this section, we introduce each
module: (a) the proxy server which allows to add
Wikulu to any target wiki engine, (b) the JavaScript
injection that bridges the gap between the client- and
server-side code, (c) the plugin manager which gives
access to any Apache UIMA-based NLP component,
and (d) the wiki abstraction layer which offers a
high-level interface to typical wiki operations such
as reading and writing the wiki content.
Proxy Server Wikulu is designed to work with
any underlying wiki engine such as MediaWiki or
TWiki. Consequently, we implemented it as an
HTTP proxy server which allows it to be enabled at
any time by changing the proxy settings of a user’s
web browser.9 The proxy server intercepts all re-
quests between the user who interacts with her web
browser, and the underlying wiki engine. For ex-
ample, Wikulu passes certain requests to its lan-
guage processing components, or augments the de-
fault wiki toolbar by additional commands. We elab-
orate on the latter in the following paragraph.
JavaScript Injection Wikulu modifies the re-
quests between web browser and target wiki by in-
jecting custom client-side JavaScript code. Wikulu
is thus capable of altering the default behavior of
the wiki engine, e.g. replacing a keyword-based re-
trieval by enhanced search methods (cf. Section 2),
adding novel behavior such as additional toolbar
buttons or advanced input fields, or augmenting the
originating web page after a certain request has been
processed, e.g. an NLP algorithm has been run.
Plugin Manager Wikulu does not perform lan-
guage processing itself. It relies on Apache UIMA-
compliant NLP components which use wiki pages
(or parts thereof) as input texts. Wikulu offers a so-
phisticated plugin manager which takes care of dy-
namically loading those NLP components. The plu-
gin loader is designed to run plugins either every
time a wiki page loads, or manually by picking them
from the augmented wiki toolbar.
The NLP components are available as server-side
Java classes. Via direct web remoting10, those com-
ponents are made accessible through a JavaScript
proxy object. Wikulu offers a generic language pro-
cessing plugin which takes the current page contents
</bodyText>
<footnote confidence="0.9977348">
9The process of enabling a custom proxy server can be
simplified by using web browser extensions such as Mul-
tiproxy Switch (https://addons.mozilla.org/de/
firefox/addon/multiproxy-switch).
10http://directwebremoting.org
</footnote>
<page confidence="0.995857">
77
</page>
<figure confidence="0.998730172413793">
Browser Proxy JS Keyphr. Wiki Wiki
Server Injection Plugin Abstr. Lay.
highlight
keyphrases
inject
keyphrases
get content from wiki page get
page
extract
keyphrases
Wikulu Proxy
Wiki Abstraction
Layer
JavaScript
Injection
Plugin
Manager
User
Browser
Wiki
Apache UIMA-compliant
NLP components
Duplicate Detection
Link Suggestion
Semantic Search
Text Segmentation
Text Summarization
Keyphrase Highlighting
���
</figure>
<figureCaption confidence="0.96475675">
Figure 5: Wikulu acts as a proxy server which intercepts
the communication between the web browser and the un-
derlying wiki engine. Its plugin manager allows to inte-
grate any Apache UIMA-compliant NLP component.
</figureCaption>
<bodyText confidence="0.999959176470588">
as input text, runs an NLP component, and writes its
output back to the wiki. To run a custom Apache
UIMA-compliant NLP component with Wikulu, one
just needs to plug that particular NLP component
into the generic plugin. No further adaptations to
the generic plugin are necessary. However, more ad-
vanced users may create fully customized plugins.
Wiki Abstraction Layer Wikulu communicates
with the underlying wiki engine via an abstraction
layer. That layer provides a generic interface for
accessing and manipulating the underlying wiki en-
gine. Thereby, Wikulu can both be tightly coupled to
a certain wiki instance such as MediaWiki or TWiki,
while being flexible at the same time to adapt to a
changing environment. New adaptors for other tar-
get wiki engines such as Confluence11 can be added
with minimal effort.
</bodyText>
<subsectionHeader confidence="0.994953">
4.2 Walk-Through Example
</subsectionHeader>
<bodyText confidence="0.999885333333333">
Let’s assume that a user encounters a wiki page
which is rather lengthy. She realizes that Wikulu’s
keyphrase extraction component might help her to
better grasp the idea of this page at a glance, so
she activates Wikulu by setting her web browser to
pass all requests through the proxy server. After
</bodyText>
<footnote confidence="0.9750255">
11http://www.atlassian.com/software/
confluence
</footnote>
<figureCaption confidence="0.982803">
Figure 6: Illustration of Wikulu’s information flow when
a user has requested to highlight keyphrases on the cur-
rent page as described in Section 4.2
</figureCaption>
<bodyText confidence="0.999928">
applying the settings, the JavaScript injection mod-
ule adds additional links to the wiki’s toolbar on
the originating wiki page. Having decided to ap-
ply keyphrase extraction, she then invokes that NLP
component by clicking the corresponding link (see
Figure 6). Before the request is passed to that com-
ponent, Wikulu extracts the wiki page contents us-
ing the high-level wiki abstraction layer. Thereafter,
the request is passed via direct web remoting to the
NLP component which has been loaded by Wikulu’s
plugin mechanism. After processing the request, the
extracted keyphrases are returned to Wikulu’s cus-
tom JavaScript handlers and finally highlighted in
the originating wiki page.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999971538461538">
Supporting wiki users with NLP techniques has not
attracted a lot of research attention yet. A no-
table exception is the work by Witte and Gitzinger
(2007). They propose an architecture to connect
wikis to services providing NLP functionality which
are based on the General Architecture for Text En-
gineering (Cunningham et al., 2002). Contrary to
Wikulu, though, their system does not integrate
transparently with an underlying wiki engine, but
rather uses a separate application to apply NLP tech-
niques. Thereby, wiki users can leverage the power
of NLP algorithms, but need to interrupt their cur-
rent workflow to switch to a different application.
</bodyText>
<page confidence="0.993519">
78
</page>
<bodyText confidence="0.9999791">
Moreover, their system is only loosely coupled with
the underlying wiki engine. While it allows to read
and write existing pages, it does not allow further
modifications such as adding user interface controls.
A lot of work in the wiki community is done in the
context of Wikipedia. For example, the FastestFox12
plug-in for Wikipedia is able to suggest links to re-
lated articles. However, unlike Wikulu, FastestFox
is tailored towards Wikipedia and cannot be used
with any other wiki platform.
</bodyText>
<sectionHeader confidence="0.998826" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999971416666667">
We presented Wikulu, an extensible system which
integrates natural language processing techniques
with wikis. Wikulu addresses the major challenge of
supporting wiki users with their everyday tasks. Be-
sides that, we demonstrated how Wikulu serves as
a flexible environment for (a) visually analyzing the
results of NLP algorithms, (b) educational purposes,
and (c) enabling semantic wikis. By its modular and
flexible architecture, we envision that Wikulu can
support wiki users both in small focused environ-
ments as well as in large-scale communities such as
Wikipedia.
</bodyText>
<sectionHeader confidence="0.997511" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.975122857142857">
This work has been supported by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship Program
under grant No. I/82806, and by the Klaus Tschira Foun-
dation under project No. 00.133.2008. We would like to
thank Johannes Hoffart for designing and implementing
the foundations of this work, as well as Artem Vovk and
Carolin Deeg for their contributions.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998938283582089">
Michel Buffa. 2006. Intranet Wikis. In Proceedings
of the IntraWebs Workshop at the 15th International
Conference on World Wide Web.
Freddy Y. Y. Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proceedings of the
1st Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26–33.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In
Proc. of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 168–175.
12http://smarterfox.com
G¨unes¸ Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence Re-
search, 22:457–479.
David Ferrucci and Adam Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information Pro-
cessing in the Corporate Research Environment. Nat-
ural Language Engineering, pages 1–26.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606–1611.
Shlomo Geva. 2007. GPX: Ad-Hoc Queries and Auto-
mated Link Discovery in the Wikipedia. In Prepro-
ceedings of the INEX Workshop, pages 404–416.
Iryna Gurevych, Christof M¨uller, and Torsten Zesch.
2007. What to be?–Electronic Career Guidance Based
on Semantic Relatedness. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics, pages 1032–1039.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
Kelly Y. Itakura and Charles L. A. Clarke. 2007. Univer-
sity of Waterloo at INEX2007: Adhoc and Link-the-
Wiki Tracks. In INEX 2007 Workshop Preproceed-
ings, pages 417–425.
Markus Kr¨otzsch, Denny Vrandeˇci´c, and Max V¨olkel.
2006. Semantic MediaWiki. In Proc. of the 5th Inter-
national Semantic Web Conference, pages 935–942.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to Latent Semantic Analysis.
Discourse Processes, 25(2):259–284.
Bo Leuf and Ward Cunningham. 2001. The Wiki Way:
Collaboration and Sharing on the Internet. Addison-
Wesley Professional.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Texts. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 404–411.
Simon Tucker and Steve Whittaker. 2009. Have A Say
Over What You See: Evaluating Interactive Compres-
sion Techniques. In Proceedings of the Intl. Confer-
ence on Intelligent User Interfaces, pages 37–46.
Ren´e Witte and Thomas Gitzinger. 2007. Connecting
wikis and natural language processing systems. In
Proc. of the Intl. Symposium on Wikis, pages 165–176.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In Proceed-
ings of the 4th ACM Conference on Digital Libraries,
pages 254–255.
</reference>
<page confidence="0.99904">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.568069">
<title confidence="0.999347">Wikulu: An Extensible Architecture for Integrating Natural Processing Techniques with Wikis</title>
<author confidence="0.887739">Daniel B¨ar</author>
<author confidence="0.887739">Nicolai Erbs</author>
<author confidence="0.887739">Torsten Zesch</author>
<author confidence="0.887739">Iryna</author>
<affiliation confidence="0.834858">Ubiquitous Knowledge Processing Computer Science Department, Technische Universit¨at</affiliation>
<address confidence="0.777473">Hochschulstrasse 10, D-64289 Darmstadt,</address>
<email confidence="0.998755">www.ukp.tu-darmstadt.de</email>
<abstract confidence="0.9989384375">present a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michel Buffa</author>
</authors>
<title>Intranet Wikis.</title>
<date>2006</date>
<booktitle>In Proceedings of the IntraWebs Workshop at the 15th International Conference on World Wide Web.</booktitle>
<contexts>
<context position="1552" citStr="Buffa, 2006" startWordPosition="217" endWordPosition="218">onal purposes, and enabling semantic wikis. 1 Introduction Wikis are web-based, collaborative content authoring systems (Leuf and Cunningham, 2001). As they offer fast and simple means for adding and editing content, they are used for various purposes such as creating encyclopedias (e.g. Wikipedia2), constructing dictionaries (e.g. Wiktionary3), or hosting online communities (e.g. ACLWiki4). However, as wikis do not enforce their users to structure pages or add complementary metadata, wikis often end up as a mass of unmanageable pages with meaningless page titles and no usable link structure (Buffa, 2006). To solve this issue, we present the Wikulu system which uses natural language processing to support wiki users with their typical tasks of adding, 1Portmanteau of the Hawaiian terms wiki (“fast”) and kukulu (“to organize”) 2http://www.wikipedia.org 3http://www.wiktionary.org 4http://aclweb.org/aclwiki 74 organizing, and finding content. For example, Wikulu supports users with reading longer texts by highlighting keyphrases using keyphrase extraction methods such as TextRank (Mihalcea and Tarau, 2004). Support integrated in Wikulu also includes text segmentation to segment long pages, text si</context>
</contexts>
<marker>Buffa, 2006</marker>
<rawString>Michel Buffa. 2006. Intranet Wikis. In Proceedings of the IntraWebs Workshop at the 15th International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="7742" citStr="Choi, 2000" startWordPosition="1213" endWordPosition="1214">in page. Wikulu therefore supports users by analyzing long pages through employing text segmentation algorithms which detect topically coherent segments of text. It then suggests segment boundaries which the user may or may not accept for inserting a subheading which makes pages easier to read and better to navigate. As shown in Figure 3, users are also encouraged to set a title for each segment.$ When accepting one or more of these suggested boundaries, Wikulu stores them persistently in the wiki. Wikulu currently integrates text segmentation methods such as TextTiling (Hearst, 1997) or C99 (Choi, 2000). Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight important keyphrases (see F</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>168--175</pages>
<contexts>
<context position="17285" citStr="Cunningham et al., 2002" startWordPosition="2713" endWordPosition="2716"> request is passed via direct web remoting to the NLP component which has been loaded by Wikulu’s plugin mechanism. After processing the request, the extracted keyphrases are returned to Wikulu’s custom JavaScript handlers and finally highlighted in the originating wiki page. 5 Related Work Supporting wiki users with NLP techniques has not attracted a lot of research attention yet. A notable exception is the work by Witte and Gitzinger (2007). They propose an architecture to connect wikis to services providing NLP functionality which are based on the General Architecture for Text Engineering (Cunningham et al., 2002). Contrary to Wikulu, though, their system does not integrate transparently with an underlying wiki engine, but rather uses a separate application to apply NLP techniques. Thereby, wiki users can leverage the power of NLP algorithms, but need to interrupt their current workflow to switch to a different application. 78 Moreover, their system is only loosely coupled with the underlying wiki engine. While it allows to read and write existing pages, it does not allow further modifications such as adding user interface controls. A lot of work in the wiki community is done in the context of Wikipedi</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics, pages 168–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir Radev</author>
</authors>
<title>LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>22--457</pages>
<contexts>
<context position="8185" citStr="Erkan and Radev, 2004" startWordPosition="1277" endWordPosition="1280">hese suggested boundaries, Wikulu stores them persistently in the wiki. Wikulu currently integrates text segmentation methods such as TextTiling (Hearst, 1997) or C99 (Choi, 2000). Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight important keyphrases (see Figure 1). As Tucker and Whittaker (2009) have $In future work, we plan to suggest suitable titles for each segment automatically. Figure 4: Extractive summary of the original wiki page shown in Figure 3 shown, highlighting important phrases assists users with reading longer texts and yields faster understanding. Wikulu thus improves readability by employing automatic keyphrase extraction algorithms. Additionally, Wikulu allows to dynamical</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir Radev. 2004. LexRank: Graph-based Lexical Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research, 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment. Natural Language Engineering,</title>
<date>2004</date>
<pages>1--26</pages>
<contexts>
<context position="2388" citStr="Ferrucci and Lally, 2004" startWordPosition="333" endWordPosition="336">to organize”) 2http://www.wikipedia.org 3http://www.wiktionary.org 4http://aclweb.org/aclwiki 74 organizing, and finding content. For example, Wikulu supports users with reading longer texts by highlighting keyphrases using keyphrase extraction methods such as TextRank (Mihalcea and Tarau, 2004). Support integrated in Wikulu also includes text segmentation to segment long pages, text similarity for detecting potential duplicates, or text summarization to facilitate reading of lengthy pages. Generally, Wikulu allows to integrate any NLP component which conforms to the standards of Apache UIMA (Ferrucci and Lally, 2004). Wikulu is designed to integrate seamlessly with any wiki. Our system is implemented as an HTTP proxy server which intercepts the communication between the web browser and the underlying wiki engine. No further modifications to the original wiki installation are necessary. Currently, our system prototype contains adaptors for two widely used wiki engines: MediaWiki5 and TWiki6. Adaptors for other wiki engines can be added with minimal effort. Generally, Wikulu could also be applied to any webbased system other than wikis with only slight modifications to its architecture. In Figure 1, we show</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment. Natural Language Engineering, pages 1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="5050" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="750" endWordPosition="753">o a wiki there is the danger of duplicating already contained information. In order to avoid duplication, users would need comprehensive knowledge of what content is already present in the wiki, which is almost impossible for large wikis like Wikipedia. Wikulu helps to detect potential duplicates by computing the text similarity between newly added content and each existing wiki page. If a potential duplicate is detected, the user is notified and may decide to augment the duplicate page instead of adding a new one. Wikulu integrates text similarity measures such as Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and Latent Semantic Analysis (Landauer et al., 1998). Suggesting Links While many wiki users readily add textual contents to wikis, they often restrain from also adding links to related pages. However, links in wikis are crucial as they allow users to quickly navigate from one page to another, or browse through the wiki. Therefore, it may be reasonable to augment a page about the topic sentiment analysis by a link to a page providing related information such as evaluation datasets. Wikulu supports users in this tedious task by automatically suggesting links. Link suggestion thereby is a two-s</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Geva</author>
</authors>
<title>GPX: Ad-Hoc Queries and Automated Link Discovery in the Wikipedia.</title>
<date>2007</date>
<booktitle>In Preproceedings of the INEX Workshop,</booktitle>
<pages>404--416</pages>
<contexts>
<context position="6110" citStr="Geva (2007)" startWordPosition="940" endWordPosition="941">formation such as evaluation datasets. Wikulu supports users in this tedious task by automatically suggesting links. Link suggestion thereby is a two-step process: (a) first, suitable text phrases are extracted which might be worth to place a link on (see Figure 2), and (b) for each phrase, related pages are ranked by comparing their relevance to the current page, and then presented to the user. The user may thus decide whether she wants to use a detected phrase as a link or not, and if so, which other wiki page to link this phrase to. Wikulu currently integrates link suggestion algorithms by Geva (2007) and Itakura and Clarke (2007). Semantic Searching The capabilities of a wiki’s built-in search engine are typically rather limited as it traditionally performs e.g. keyword-based retrieval. If that keyword is not found in the wiki, the query returns an empty result set. However, a page might exist which is semantically related to the keyword, and should thus yield a match. As the search engine is typically a core part of the wiki system, it is rather difficult to modify its behavior. However, by leveraging Wikulu’s architecture, we can replace the default search mechanisms by algorithms which</context>
</contexts>
<marker>Geva, 2007</marker>
<rawString>Shlomo Geva. 2007. GPX: Ad-Hoc Queries and Automated Link Discovery in the Wikipedia. In Preproceedings of the INEX Workshop, pages 404–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Christof M¨uller</author>
<author>Torsten Zesch</author>
</authors>
<title>What to be?–Electronic Career Guidance Based on Semantic Relatedness.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1032--1039</pages>
<marker>Gurevych, M¨uller, Zesch, 2007</marker>
<rawString>Iryna Gurevych, Christof M¨uller, and Torsten Zesch. 2007. What to be?–Electronic Career Guidance Based on Semantic Relatedness. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 1032–1039.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="7722" citStr="Hearst, 1997" startWordPosition="1209" endWordPosition="1210"> is present on a certain page. Wikulu therefore supports users by analyzing long pages through employing text segmentation algorithms which detect topically coherent segments of text. It then suggests segment boundaries which the user may or may not accept for inserting a subheading which makes pages easier to read and better to navigate. As shown in Figure 3, users are also encouraged to set a title for each segment.$ When accepting one or more of these suggested boundaries, Wikulu stores them persistently in the wiki. Wikulu currently integrates text segmentation methods such as TextTiling (Hearst, 1997) or C99 (Choi, 2000). Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight importa</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kelly Y Itakura</author>
<author>Charles L A Clarke</author>
</authors>
<date>2007</date>
<booktitle>University of Waterloo at INEX2007: Adhoc and Link-theWiki Tracks. In INEX 2007 Workshop Preproceedings,</booktitle>
<pages>417--425</pages>
<contexts>
<context position="6140" citStr="Itakura and Clarke (2007)" startWordPosition="943" endWordPosition="946">s evaluation datasets. Wikulu supports users in this tedious task by automatically suggesting links. Link suggestion thereby is a two-step process: (a) first, suitable text phrases are extracted which might be worth to place a link on (see Figure 2), and (b) for each phrase, related pages are ranked by comparing their relevance to the current page, and then presented to the user. The user may thus decide whether she wants to use a detected phrase as a link or not, and if so, which other wiki page to link this phrase to. Wikulu currently integrates link suggestion algorithms by Geva (2007) and Itakura and Clarke (2007). Semantic Searching The capabilities of a wiki’s built-in search engine are typically rather limited as it traditionally performs e.g. keyword-based retrieval. If that keyword is not found in the wiki, the query returns an empty result set. However, a page might exist which is semantically related to the keyword, and should thus yield a match. As the search engine is typically a core part of the wiki system, it is rather difficult to modify its behavior. However, by leveraging Wikulu’s architecture, we can replace the default search mechanisms by algorithms which allow for semantic search to </context>
</contexts>
<marker>Itakura, Clarke, 2007</marker>
<rawString>Kelly Y. Itakura and Charles L. A. Clarke. 2007. University of Waterloo at INEX2007: Adhoc and Link-theWiki Tracks. In INEX 2007 Workshop Preproceedings, pages 417–425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Kr¨otzsch</author>
<author>Denny Vrandeˇci´c</author>
<author>Max V¨olkel</author>
</authors>
<title>Semantic MediaWiki.</title>
<date>2006</date>
<booktitle>In Proc. of the 5th International Semantic Web Conference,</booktitle>
<pages>935--942</pages>
<marker>Kr¨otzsch, Vrandeˇci´c, V¨olkel, 2006</marker>
<rawString>Markus Kr¨otzsch, Denny Vrandeˇci´c, and Max V¨olkel. 2006. Semantic MediaWiki. In Proc. of the 5th International Semantic Web Conference, pages 935–942.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="5103" citStr="Landauer et al., 1998" startWordPosition="758" endWordPosition="761">formation. In order to avoid duplication, users would need comprehensive knowledge of what content is already present in the wiki, which is almost impossible for large wikis like Wikipedia. Wikulu helps to detect potential duplicates by computing the text similarity between newly added content and each existing wiki page. If a potential duplicate is detected, the user is notified and may decide to augment the duplicate page instead of adding a new one. Wikulu integrates text similarity measures such as Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and Latent Semantic Analysis (Landauer et al., 1998). Suggesting Links While many wiki users readily add textual contents to wikis, they often restrain from also adding links to related pages. However, links in wikis are crucial as they allow users to quickly navigate from one page to another, or browse through the wiki. Therefore, it may be reasonable to augment a page about the topic sentiment analysis by a link to a page providing related information such as evaluation datasets. Wikulu supports users in this tedious task by automatically suggesting links. Link suggestion thereby is a two-step process: (a) first, suitable text phrases are ext</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to Latent Semantic Analysis. Discourse Processes, 25(2):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Leuf</author>
<author>Ward Cunningham</author>
</authors>
<title>The Wiki Way: Collaboration and Sharing on the Internet.</title>
<date>2001</date>
<publisher>AddisonWesley Professional.</publisher>
<contexts>
<context position="1087" citStr="Leuf and Cunningham, 2001" startWordPosition="143" endWordPosition="146">erface. Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis. 1 Introduction Wikis are web-based, collaborative content authoring systems (Leuf and Cunningham, 2001). As they offer fast and simple means for adding and editing content, they are used for various purposes such as creating encyclopedias (e.g. Wikipedia2), constructing dictionaries (e.g. Wiktionary3), or hosting online communities (e.g. ACLWiki4). However, as wikis do not enforce their users to structure pages or add complementary metadata, wikis often end up as a mass of unmanageable pages with meaningless page titles and no usable link structure (Buffa, 2006). To solve this issue, we present the Wikulu system which uses natural language processing to support wiki users with their typical tas</context>
</contexts>
<marker>Leuf, Cunningham, 2001</marker>
<rawString>Bo Leuf and Ward Cunningham. 2001. The Wiki Way: Collaboration and Sharing on the Internet. AddisonWesley Professional.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing Order into Texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="2059" citStr="Mihalcea and Tarau, 2004" startWordPosition="283" endWordPosition="286">s often end up as a mass of unmanageable pages with meaningless page titles and no usable link structure (Buffa, 2006). To solve this issue, we present the Wikulu system which uses natural language processing to support wiki users with their typical tasks of adding, 1Portmanteau of the Hawaiian terms wiki (“fast”) and kukulu (“to organize”) 2http://www.wikipedia.org 3http://www.wiktionary.org 4http://aclweb.org/aclwiki 74 organizing, and finding content. For example, Wikulu supports users with reading longer texts by highlighting keyphrases using keyphrase extraction methods such as TextRank (Mihalcea and Tarau, 2004). Support integrated in Wikulu also includes text segmentation to segment long pages, text similarity for detecting potential duplicates, or text summarization to facilitate reading of lengthy pages. Generally, Wikulu allows to integrate any NLP component which conforms to the standards of Apache UIMA (Ferrucci and Lally, 2004). Wikulu is designed to integrate seamlessly with any wiki. Our system is implemented as an HTTP proxy server which intercepts the communication between the web browser and the underlying wiki engine. No further modifications to the original wiki installation are necessa</context>
<context position="8948" citStr="Mihalcea and Tarau, 2004" startWordPosition="1393" endWordPosition="1396">t keyphrases (see Figure 1). As Tucker and Whittaker (2009) have $In future work, we plan to suggest suitable titles for each segment automatically. Figure 4: Extractive summary of the original wiki page shown in Figure 3 shown, highlighting important phrases assists users with reading longer texts and yields faster understanding. Wikulu thus improves readability by employing automatic keyphrase extraction algorithms. Additionally, Wikulu allows to dynamically adjust the number of keyphrases shown by presenting a slider to the user. We integrated keyphrase extraction methods such as TextRank (Mihalcea and Tarau, 2004) and KEA (Witten et al., 1999). 3 Further Use Cases Further use cases for supporting wiki users include (i) visually analyzing the results of NLP algorithms, (ii) educational purposes, and (iii) enabling semantic wikis. Visually Analyzing the Results of NLP Algorithms Wikulu facilitates analyzing the results of NLP algorithms by using wiki pages as input documents and visualizing the results directly on that page. Consider an NLP algorithm which performs sentiment analysis. Typically, we were to put our analysis sentences in a text file, launch the NLP application, process the file, and would </context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Texts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tucker</author>
<author>Steve Whittaker</author>
</authors>
<title>Have A Say Over What You See: Evaluating Interactive Compression Techniques.</title>
<date>2009</date>
<booktitle>In Proceedings of the Intl. Conference on Intelligent User Interfaces,</booktitle>
<pages>37--46</pages>
<contexts>
<context position="8382" citStr="Tucker and Whittaker (2009)" startWordPosition="1309" endWordPosition="1312"> Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight important keyphrases (see Figure 1). As Tucker and Whittaker (2009) have $In future work, we plan to suggest suitable titles for each segment automatically. Figure 4: Extractive summary of the original wiki page shown in Figure 3 shown, highlighting important phrases assists users with reading longer texts and yields faster understanding. Wikulu thus improves readability by employing automatic keyphrase extraction algorithms. Additionally, Wikulu allows to dynamically adjust the number of keyphrases shown by presenting a slider to the user. We integrated keyphrase extraction methods such as TextRank (Mihalcea and Tarau, 2004) and KEA (Witten et al., 1999). 3 </context>
</contexts>
<marker>Tucker, Whittaker, 2009</marker>
<rawString>Simon Tucker and Steve Whittaker. 2009. Have A Say Over What You See: Evaluating Interactive Compression Techniques. In Proceedings of the Intl. Conference on Intelligent User Interfaces, pages 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ren´e Witte</author>
<author>Thomas Gitzinger</author>
</authors>
<title>Connecting wikis and natural language processing systems.</title>
<date>2007</date>
<booktitle>In Proc. of the Intl. Symposium on Wikis,</booktitle>
<pages>165--176</pages>
<contexts>
<context position="17107" citStr="Witte and Gitzinger (2007)" startWordPosition="2686" endWordPosition="2689">rresponding link (see Figure 6). Before the request is passed to that component, Wikulu extracts the wiki page contents using the high-level wiki abstraction layer. Thereafter, the request is passed via direct web remoting to the NLP component which has been loaded by Wikulu’s plugin mechanism. After processing the request, the extracted keyphrases are returned to Wikulu’s custom JavaScript handlers and finally highlighted in the originating wiki page. 5 Related Work Supporting wiki users with NLP techniques has not attracted a lot of research attention yet. A notable exception is the work by Witte and Gitzinger (2007). They propose an architecture to connect wikis to services providing NLP functionality which are based on the General Architecture for Text Engineering (Cunningham et al., 2002). Contrary to Wikulu, though, their system does not integrate transparently with an underlying wiki engine, but rather uses a separate application to apply NLP techniques. Thereby, wiki users can leverage the power of NLP algorithms, but need to interrupt their current workflow to switch to a different application. 78 Moreover, their system is only loosely coupled with the underlying wiki engine. While it allows to rea</context>
</contexts>
<marker>Witte, Gitzinger, 2007</marker>
<rawString>Ren´e Witte and Thomas Gitzinger. 2007. Connecting wikis and natural language processing systems. In Proc. of the Intl. Symposium on Wikis, pages 165–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Gordon W Paynter</author>
<author>Eibe Frank</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>KEA: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4th ACM Conference on Digital Libraries,</booktitle>
<pages>254--255</pages>
<contexts>
<context position="8978" citStr="Witten et al., 1999" startWordPosition="1399" endWordPosition="1402">ker and Whittaker (2009) have $In future work, we plan to suggest suitable titles for each segment automatically. Figure 4: Extractive summary of the original wiki page shown in Figure 3 shown, highlighting important phrases assists users with reading longer texts and yields faster understanding. Wikulu thus improves readability by employing automatic keyphrase extraction algorithms. Additionally, Wikulu allows to dynamically adjust the number of keyphrases shown by presenting a slider to the user. We integrated keyphrase extraction methods such as TextRank (Mihalcea and Tarau, 2004) and KEA (Witten et al., 1999). 3 Further Use Cases Further use cases for supporting wiki users include (i) visually analyzing the results of NLP algorithms, (ii) educational purposes, and (iii) enabling semantic wikis. Visually Analyzing the Results of NLP Algorithms Wikulu facilitates analyzing the results of NLP algorithms by using wiki pages as input documents and visualizing the results directly on that page. Consider an NLP algorithm which performs sentiment analysis. Typically, we were to put our analysis sentences in a text file, launch the NLP application, process the file, and would read the output from either a </context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl Gutwin, and Craig G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. In Proceedings of the 4th ACM Conference on Digital Libraries, pages 254–255.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>