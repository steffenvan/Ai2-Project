<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000096">
<title confidence="0.99643">
Lessons Learned in Part-of-Speech Tagging of Conversational Speech
</title>
<author confidence="0.994445">
Vladimir Eidelman†, Zhongqiang Huang†, and Mary Harper†‡†Laboratory for Computational Linguistics and Information Processing
</author>
<affiliation confidence="0.9545265">
Institute for Advanced Computer Studies
University of Maryland, College Park, MD
‡Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD
</affiliation>
<email confidence="0.99871">
{vlad,zhuang,mharper}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841083333333">
This paper examines tagging models for spon-
taneous English speech transcripts. We ana-
lyze the performance of state-of-the-art tag-
ging models, either generative or discrimi-
native, left-to-right or bidirectional, with or
without latent annotations, together with the
use of ToBI break indexes and several meth-
ods for segmenting the speech transcripts (i.e.,
conversation side, speaker turn, or human-
annotated sentence). Based on these studies,
we observe that: (1) bidirectional models tend
to achieve better accuracy levels than left-to-
right models, (2) generative models seem to
perform somewhat better than discriminative
models on this task, and (3) prosody improves
tagging performance of models on conversa-
tion sides, but has much less impact on smaller
segments. We conclude that, although the use
of break indexes can indeed significantly im-
prove performance over baseline models with-
out them on conversation sides, tagging ac-
curacy improves more by using smaller seg-
ments, for which the impact of the break in-
dexes is marginal.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999361428571429">
Natural language processing technologies, such as
parsing and tagging, often require reconfiguration
when they are applied to challenging domains that
differ significantly from newswire, e.g., blogs, twit-
ter text (Foster, 2010), or speech. In contrast to
text, conversational speech represents a significant
challenge because the transcripts are not segmented
into sentences. Furthermore, the transcripts are of-
ten disfluent and lack punctuation and case informa-
tion. On the other hand, speech provides additional
information, beyond simply the sequence of words,
which could be exploited to more accurately assign
each word in the transcript a part-of-speech (POS)
tag. One potentially beneficial type of information
is prosody (Cutler et al., 1997).
Prosody provides cues for lexical disambigua-
tion, sentence segmentation and classification,
phrase structure and attachment, discourse struc-
ture, speaker affect, etc. Prosody has been found
to play an important role in speech synthesis sys-
tems (Batliner et al., 2001; Taylor and Black, 1998),
as well as in speech recognition (Gallwitz et al.,
2002; Hasegawa-Johnson et al., 2005; Ostendorf et
al., 2003). Additionally, prosodic features such as
pause length, duration of words and phones, pitch
contours, energy contours, and their normalized val-
ues have been used for speech processing tasks like
sentence boundary detection (Liu et al., 2005).
Linguistic encoding schemes like ToBI (Silver-
man et al., 1992) have also been used for sentence
boundary detection (Roark et al., 2006; Harper et al.,
2005), as well as for parsing (Dreyer and Shafran,
2007; Gregory et al., 2004; Kahn et al., 2005). In
the ToBI scheme, aspects of prosody such as tone,
prominence, and degree of juncture between words
are represented symbolically. For instance, Dreyer
and Shafran (2007) use three classes of automati-
cally detected ToBI break indexes, indicating major
intonational breaks with a 4, hesitation with a p, and
all other breaks with a 1.
Recently, Huang and Harper (2010) found that
they could effectively integrate prosodic informa-
</bodyText>
<page confidence="0.974211">
821
</page>
<note confidence="0.817503">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999666875">
tion in the form of this simplified three class ToBI
encoding when parsing spontaneous speech by us-
ing a prosodically enriched PCFG model with latent
annotations (PCFG-LA) (Matsuzaki et al., 2005;
Petrov and Klein, 2007) to rescore n-best parses
produced by a baseline PCFG-LA model without
prosodic enrichment. However, the prosodically en-
riched models by themselves did not perform sig-
nificantly better than the baseline PCFG-LA model
without enrichment, due to the negative effect that
misalignments between automatic prosodic breaks
and true phrase boundaries have on the model.
This paper investigates methods for using state-
of-the-art taggers on conversational speech tran-
scriptions and the effect that prosody has on tagging
accuracy. Improving POS tagging performance of
speech transcriptions has implications for improving
downstream applications that rely on accurate POS
tags, including sentence boundary detection (Liu
et al., 2005), automatic punctuation (Hillard et al.,
2006), information extraction from speech, parsing,
and syntactic language modeling (Heeman, 1999;
Filimonov and Harper, 2009). While there have
been several attempts to integrate prosodic informa-
tion to improve parse accuracy of speech transcripts,
to the best of our knowledge there has been little
work on using this type of information for POS tag-
ging. Furthermore, most of the parsing work has
involved generative models and rescoring/reranking
of hypotheses from the generative models. In this
work, we will analyze several factors related to ef-
fective POS tagging of conversational speech:
</bodyText>
<listItem confidence="0.9996976">
• discriminative versus generative POS tagging
models (Section 2)
• prosodic features in the form of simplified ToBI
break indexes (Section 4)
• type of speech segmentation (Section 5)
</listItem>
<sectionHeader confidence="0.769917" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.999980444444445">
In order to fully evaluate the difficulties inherent in
tagging conversational speech, as well as the possi-
ble benefits of prosodic information, we conducted
experiments with six different POS tagging mod-
els. The models can be broadly separated into two
classes: generative and discriminative. As the first
of our generative models, we used a Hidden Markov
Model (HMM) trigram tagger (Thede and Harper,
1999), which serves to establish a baseline and to
gauge the difficulty of the task at hand. Our sec-
ond model, HMM-LA, was the latent variable bi-
gram HMM tagger of Huang et al. (2009), which
achieved state-of-the-art tagging performance by in-
troducing latent tags to weaken the stringent Markov
independence assumptions that generally hinder tag-
ging performance in generative models.
For the third model, we implemented a bidirec-
tional variant of the HMM-LA (HMM-LA-Bidir)
that combines evidence from two HMM-LA tag-
gers, one trained left-to-right and the other right-to-
left. For decoding, we use a product model (Petrov,
2010). The intuition is that the context information
from the left and the right of the current position
is complementary for predicting the current tag and
thus, the combination should serve to improve per-
formance over the HMM-LA tagger.
Since prior work on parsing speech with prosody
has relied on generative models, it was necessary
to modify equations of the model in order to incor-
porate the prosodic information, and then perform
rescoring in order to achieve gains. However, it is
far simpler to directly integrate prosody as features
into the model by using a discriminative approach.
Hence, we also investigate several log-linear mod-
els, which allow us to easily include an arbitrary
number and varying kinds of possibly overlapping
and non-independent features.
First, we implemented a Conditional Random
Field (CRF) tagger, which is an attractive choice due
to its ability to learn the globally optimal labeling
for a sequence and proven excellent performance on
sequence labeling tasks (Lafferty et al., 2001). In
contrast to an HMM which optimizes the joint like-
lihood of the word sequence and tags, a CRF opti-
mizes the conditional likelihood, given by:
</bodyText>
<equation confidence="0.872302">
pλ(t|w) _ exp Ej AjFj(t, w) (1)
Et exp Ej AjFj(t, w)
</equation>
<bodyText confidence="0.9980845">
where the A’s are the parameters of the model to es-
timate and F indicates the feature functions used.
The denominator in (1) is Zλ(x), the normalization
factor, with:
</bodyText>
<equation confidence="0.953834">
Fj(t, w) _ � fj(t, w, i)
i
</equation>
<page confidence="0.99489">
822
</page>
<table confidence="0.999051">
Class Model Name Latent Variable Bidirectional N-best-Extraction Markov Order
Generative Trigram HMM √ √ √ 2nd
HMM-LA √ √ 1st
HMM-LA-Bidir 1st
Discriminative Stanford Bidir √ 2nd
Stanford Left5 2nd
CRF 2nd
</table>
<tableCaption confidence="0.999896">
Table 1: Description of tagging models
</tableCaption>
<bodyText confidence="0.845855">
The objective we need to maximize then becomes :
</bodyText>
<equation confidence="0.99358325">
X ⎡ ⎤
L = ⎣X AjFj(tni wn) − log Za(xn) − kA 2uk2
n 2Q2
j
</equation>
<bodyText confidence="0.999840481481482">
where we use a spherical Gaussian prior to pre-
vent overfitting of the model (Chen and Rosen-
feld, 1999) and the wide-spread quasi-Newtonian
L-BFGS method to optimize the model parame-
ters (Liu and Nocedal, 1989). Decoding is per-
formed with the Viterbi algorithm.
We also evaluate state-of-the-art Maximum En-
tropy taggers: the Stanford Left5 tagger (Toutanova
and Manning, 2000) and the Stanford bidirectional
tagger (Toutanova et al., 2003), with the former us-
ing only left context and the latter bidirectional de-
pendencies.
Table 1 summarizes the major differences be-
tween the models along several dimensions: (1) gen-
erative versus discriminative, (2) directionality of
decoding, (3) the presence or absence of latent anno-
tations, (4) the availability of n-best extraction, and
(5) the model order.
In order to assess the quality of our models, we
evaluate them on the section 23 test set of the stan-
dard newswire WSJ tagging task after training all
models on sections 0-22. Results appear in Ta-
ble 2. Clearly, all the models have high accuracy
on newswire data, but the Stanford bidirectional tag-
ger significantly outperforms the other models with
the exception of the HMM-LA-Bidir model on this
task.1
</bodyText>
<footnote confidence="0.7921005">
1Statistically significant improvements are calculated using
the sign test (p &lt; 0.05).
</footnote>
<table confidence="0.998069571428571">
Model Accuracy
Trigram HMM 96.58
HMM-LA 97.05
HMM-LA-Bidir 97.16
Stanford Bidir 97.28
Stanford Left5 97.07
CRF 96.81
</table>
<tableCaption confidence="0.99956">
Table 2: Tagging accuracy on WSJ
</tableCaption>
<sectionHeader confidence="0.996773" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.997503409090909">
In the rest of this paper, we evaluate the tag-
ging models described in Section 2 on conver-
sational speech. We chose to utilize the Penn
Switchboard (Godfrey et al., 1992) and Fisher tree-
banks (Harper et al., 2005; Bies et al., 2006) because
they provide gold standard tags for conversational
speech and we have access to corresponding auto-
matically generated ToBI break indexes provided by
(Dreyer and Shafran, 2007; Harper et al., 2005)2.
We utilized the Fisher dev1 and dev2 sets contain-
ing 16,519 sentences (112,717 words) as the primary
training data and the entire Penn Switchboard tree-
bank containing 110,504 sentences (837,863 words)
as an additional training source3. The treebanks
were preprocessed as follows: the tags of auxiliary
verbs were replaced with the AUX tag, empty nodes
2A small fraction of words in the Switchboard treebank do
not align with the break indexes because they were produced
based on a later refinement of the transcripts used to produce
the treebank. For these cases, we heuristically added break *1*
to words in the middle of a sentence and *4* to words that end
a sentence.
</bodyText>
<footnote confidence="0.858665">
3Preliminary experiments evaluating the effect of training
data size on performance indicated using the additional Switch-
board data leads to more accurate models, and so we use the
combined training set.
</footnote>
<page confidence="0.997756">
823
</page>
<bodyText confidence="0.999948055555556">
and function tags were removed, words were down-
cased, punctuation was deleted, and the words and
their tags were extracted. Because the Fisher tree-
bank was developed using the lessons learned when
developing Switchboard, we chose to use its eval
portion for development (the first 1,020 tagged sen-
tences containing 7,184 words) and evaluation (the
remaining 3,917 sentences with 29,173 words).
We utilize the development set differently for the
generative and discriminative models. Since the EM
algorithm used for estimating the parameters in the
latent variable models introduces a lot of variabil-
ity, we train five models with a different seed and
then choose the best one based on dev set perfor-
mance. For the discriminative models, we tuned
their respective regularization parameters on the dev
set. All results reported in the rest of this paper are
on the test set.
</bodyText>
<sectionHeader confidence="0.968252" genericHeader="method">
4 Integration of Prosodic Information
</sectionHeader>
<bodyText confidence="0.999819428571429">
In this work, we use three classes of automatically
generated ToBI break indexes to represent prosodic
information (Kahn et al., 2005; Dreyer and Shafran,
2007; Huang and Harper, 2010): 4, 1, and p.
Consider the following speech transcription exam-
ple, which is enriched with ToBI break indexes in
parentheses and tags: i(1)/PRP did(1)/VBD
</bodyText>
<equation confidence="0.7324745">
n’t(1)/RB you(1)/PRP know(4)/VBP
i(1)/PRP did(1)/AUX n’t(1)/RB...
</equation>
<bodyText confidence="0.9999926875">
The speaker begins an utterance, and then restarts
the utterance. The automatically predicted break 4
associated with know in the utterance compellingly
indicates an intonational phrase boundary and could
provide useful information for tagging if we can
model it appropriately.
To integrate prosody into our generative models,
we utilize the method from (Dreyer and Shafran,
2007) to add prosodic breaks. As Figure 1 shows,
ToBI breaks provide a secondary sequence of ob-
servations that is parallel to the sequence of words
that comprise the sentence. Each break bi in the sec-
ondary sequence is generated by the same tag ti as
that which generates the corresponding word wi, and
so it is conditionally independent of its correspond-
ing word given the tag:
</bodyText>
<equation confidence="0.900828">
P(w, b1t) = P(wjt)P(bjt)
</equation>
<figureCaption confidence="0.9657125">
Figure 1: Parallel generation of words and breaks for the
HMM models
</figureCaption>
<bodyText confidence="0.996172230769231">
The HMM-LA taggers are then able to split tags to
capture implicit higher order interactions among the
sequence of tags, words, and breaks.
The discriminative models are able to utilize
prosodic features directly, enabling the use of con-
textual interactions with other features to further im-
prove tagging accuracy. Specifically, in addition to
the standard set of features used in the tagging lit-
erature, we use the feature templates presented in
Table 3, where each feature associates the break bi,
word wi, or some combination of the two with the
current tag ti4.
Break and/or word values Tag value
</bodyText>
<equation confidence="0.999721875">
bi=B ti = T
bi=B &amp; bi_1=C ti = T
wi=W &amp; bi=B ti = T
wi+1=W &amp; bi=B ti = T
wi+2=W &amp; bi=B ti = T
wi_1=W &amp; bi=B ti = T
wi_2=W &amp; bi=B ti = T
wi=W &amp; bi=B &amp; bi_1=C ti = T
</equation>
<tableCaption confidence="0.992424">
Table 3: Prosodic feature templates
</tableCaption>
<sectionHeader confidence="0.999255" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999908">
5.1 Conversation side segmentation
</subsectionHeader>
<bodyText confidence="0.999964">
When working with raw speech transcripts, we ini-
tially have a long stream of unpunctuated words,
which is called a conversation side. As the average
length of conversation side segments in our data is
approximately 630 words, it poses quite a challeng-
ing tagging task. Thus, we hypothesize that it is on
these large segments that we should achieve the most
</bodyText>
<footnote confidence="0.6934765">
4We modified the Stanford taggers to handle these prosodic
features.
</footnote>
<figure confidence="0.976767277777778">
PRP
i
VBD
1 did
RB
1 n’t
VBP
1 know
4
824
94.5
94.2
93.9
93.6
93.3
93
FrMM-LA FrMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody OracleBreak OracleBreak+Sent OracleSent OracleBreak-Sent Rescoring
</figure>
<figureCaption confidence="0.999654">
Figure 2: Tagging accuracy on conversation sides
</figureCaption>
<bodyText confidence="0.999863615384615">
improvement from the addition of prosodic informa-
tion.
In fact, as the baseline results in Figure 2 show,
the accuracies achieved on this task are much lower
than those on the newswire task. The trigram HMM
tagger accuracy drops to 92.43%, while all the other
models fall to within the range of 93.3%-94.12%,
a significant departure from the 96-97.3% range on
newswire sentences. Note that the Stanford bidi-
rectional and HMM-LA tagger perform very simi-
larly, although the HMM-LA-Bidir tagger performs
significantly better than both. In contrast to the
newswire task on which the Stanford bidirectional
tagger performed the best, on this genre, it is slightly
worse than the HMM-LA tagger, albeit the differ-
ence is not statistically significant.
With the direct integration of prosody into the
generative models (see Figure 2), there is a slight but
statistically insignificant shift in performance. How-
ever, integrating prosody directly into the discrimi-
native models leads to significant improvements in
the CRF and Stanford Left5 taggers. The gain in
the Stanford bidirectional tagger is not statistically
significant, however, which suggests that the left-
to-right models benefit more from the addition of
prosody than bidirectional models.
</bodyText>
<subsectionHeader confidence="0.974207">
5.2 Human-annotated sentences
</subsectionHeader>
<bodyText confidence="0.999942055555556">
Given the lack-luster performance of the tagging
models on conversation side segments, even with the
direct addition of prosody, we chose to determine the
performance levels that could be achieved on this
task using human-annotated sentences, which we
will refer to as sentence segmentation. Figure 3 re-
ports the baseline tagging accuracy on sentence seg-
ments, and we see significant improvements across
all models. The HMM Trigram tagger performance
increases to 93.00%, while the increase in accuracy
for the other models ranges from around 0.2-0.3%.
The HMM-LA taggers once again achieve the best
performance, with the Stanford bidirectional close
behind. Although the addition of prosody has very
little impact on either the generative or discrimina-
tive models when applied to sentences, the base-
line tagging models (i.e., not prosodically enriched)
significantly outperform all of the prosodically en-
riched models operating on conversation sides.
At this point, it would be apt to suggest us-
ing automatic sentence boundary detection to cre-
ate shorter segments. Table 4 presents the results
of using baseline models without prosodic enrich-
ment trained on the human-annotated sentences to
tag automatically segmented speech5. As can be
seen, the results are quite similar to the conversation
side segmentation performances, and thus signifi-
cantly lower than when tagging human-annotated
sentences. A caveat to consider here is that we break
the standard assumption that the training and test set
be drawn from the same distribution, since the train-
ing data is human-annotated and the test is automat-
ically segmented. However, it can be quite challeng-
ing to create a corpus to train on that represents the
biases of the systems that perform automatic sen-
tence segmentation. Instead, we will examine an-
</bodyText>
<footnote confidence="0.948407">
5We used the Baseline Structural Metadata System de-
scribed in Harper et al. (2005) to predict sentence boundaries.
</footnote>
<page confidence="0.993903">
825
</page>
<figureCaption confidence="0.998573">
Figure 3: Tagging accuracy on human-annotated segments
</figureCaption>
<figure confidence="0.94591">
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody OracleBreak Rescoring
94.5
94.2
93.9
93.6
93.3
93
</figure>
<bodyText confidence="0.99719375">
other segmentation method to shorten the segments
automatically, i.e., by training and testing on speaker
turns, which preserves the train-test match, in Sec-
tion 5.5.
</bodyText>
<table confidence="0.998622666666667">
Model Accuracy
HMM-LA 93.95
HMM-LA-Bidir 94.07
Stanford Bidir 93.77
Stanford Left5 93.35
CRF 93.29
</table>
<tableCaption confidence="0.9892715">
Table 4: Baseline tagging accuracy on automatically de-
tected sentence boundaries
</tableCaption>
<subsectionHeader confidence="0.988678">
5.3 Oracle Break Insertion
</subsectionHeader>
<bodyText confidence="0.999989960784314">
As we believe one of the major roles that prosodic
cues serve for tagging conversation sides is as a
proxy for sentence boundaries, perhaps the efficacy
of the prosodic breaks can, at least partially, be at-
tributed to errors in the automatically induced break
indexes themselves, as they can misalign with syn-
tactic phrase boundaries, as discussed in Huang and
Harper (2010). This may degrade the performance
of our models more than the improvement achieved
from correctly placed breaks. Hence, we conduct
a series of experiments in which we systematically
eliminate noisy phrase and disfluency breaks and
show that under these improved conditions, prosodi-
cally enriched models can indeed be more effective.
To investigate to what extent noisy breaks are im-
peding the possible improvements from prosodically
enriched models, we replaced all 4 and p breaks in
the training and evaluation sets that did not align
to the correct phrase boundaries as indicated by the
treebank with break 1 for both the conversation sides
and human-annotated sentences. The results from
using Oracle Breaks on conversation sides can be
seen in Figure 2. All models except Stanford Left5
and HMM-LA-Bidir significantly improve in accu-
racy when trained and tested on the Oracle Break
modified data. On human-annotated sentences, Fig-
ure 3 shows improvements in accuracies across all
models, however, they are statistically insignificant.
To further analyze why prosodically enriched
models achieve more improvement on conversation
sides than on sentences, we conducted three more
Oracle experiments on conversation sides. For the
first, OracleBreak-Sent, we further modified the data
such that all breaks corresponding to a sentence
ending in the human-annotated segments were con-
verted to break 1, thus effectively only leaving in-
side sentence phrasal boundaries. This modification
results in a significant drop in performance, as can
be seen in Figure 2.
For the second, OracleSent, we converted all
the breaks corresponding to a sentence end in the
human-annotated segmentations to break 4, and all
the others to break 1, thus effectively only leaving
sentence boundary breaks. This performed largely
on par with OracleBreak, suggesting that the phrase-
aligned prosodic breaks seem to be a stand-in for
sentence boundaries.
Finally, in the last condition, OracleBreak+Sent,
we modified the OracleBreak data such that all
breaks corresponding to a sentence ending in the
human-annotated sentences were converted to break
</bodyText>
<page confidence="0.993165">
826
</page>
<figure confidence="0.993287375">
94.5
94.2
93.9
93.6
93.3
93
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody Rescoring
</figure>
<figureCaption confidence="0.999967">
Figure 4: Tagging accuracy on speaker turns
</figureCaption>
<bodyText confidence="0.998581">
4 (essentially combining OracleBreak and Oracle-
Sent). As Figure 2 indicates, this modification re-
sults in the best tagging accuracies for all the mod-
els. All models were able to match or even improve
upon the baseline accuracies achieved on the human
segmented data. This suggests that when we have
breaks that align with phrasal and sentence bound-
aries, prosodically enriched models are highly effec-
tive.
</bodyText>
<subsectionHeader confidence="0.982024">
5.4 N-best Rescoring
</subsectionHeader>
<bodyText confidence="0.997131130434783">
Based on the findings in the previous section and the
findings of (Huang and Harper, 2010), we next ap-
ply a rescoring strategy in which the search space
of the prosodically enriched generative models is re-
stricted to the n-best list generated from the base-
line model (without prosodic enrichment). In this
manner, the prosodically enriched model can avoid
poor tag sequences produced due to the misaligned
break indexes. As Figure 2 shows, using the base-
line conversation side model to produce an n-best
list for the prosodically enriched model to rescore
results in significant improvements in performance
for the HMM-LA model, similar to the parsing re-
sults of (Huang and Harper, 2010). The size of the
n-best list directly impacts performance, as reducing
to n = 1 is akin to tagging with the baseline model,
and increasing n —* oc amounts to tagging with the
prosodically enriched model. We experimented with
a number of different sizes for n and chose the best
one using the dev set. Figure 3 presents the results
for this method applied to human-annotated sen-
tences, where it produces only marginal improve-
ments6.
</bodyText>
<subsectionHeader confidence="0.964323">
5.5 Speaker turn segmentation
</subsectionHeader>
<bodyText confidence="0.962201451612903">
The results presented thus far indicate that if we
have access to close to perfect break indexes, we
can use them effectively, but this is not likely to be
true in practice. We have also observed that tagging
accuracy on shorter conversation sides is greater
than longer conversation sides, suggesting that post-
processing the conversation sides to produce shorter
segments would be desirable.
We thus devised a scheme by which we could
automatically extract shorter speaker turn segments
from conversation sides. For this study, speaker
turns, which effectively indicate speaker alterna-
tions, were obtained by using the metadata in the
treebank to split the sentences into chunks based on
speaker change. Every time a speaker begins talk-
ing after the other speaker was talking, we start a
new segment for that speaker. In practice, this would
need to be done based on audio cues and automatic
transcriptions, so these results represent an upper
bound.
Figure 4 presents tagging results on speaker turn
segments. For most models, the difference in accu-
racy achieved on these segments and that of human-
annotated sentences is statistically insignificant. The
only exception is the Stanford bidirectional tagger,
6Rescoring using the CRF model was also performed, but
led to a performance degradation. We believe this is due to
the fact that the prosodically enriched CRF model was able to
directly use the break index information, and so restricting it to
the baseline CRF model search space limits the performance to
that of the baseline model.
</bodyText>
<page confidence="0.981288">
827
</page>
<figure confidence="0.9635081">
400
Number of Errors
300
200
100
0
NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD IN
(a) Number of errors by part of speech category for the HMM-LA model with and without prosody
NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD IN
(b) Number of errors by part of speech category for the CRF model with and without prosody
</figure>
<figureCaption confidence="0.950886">
Figure 5: Error reduction for prosodically enriched HMM-LA (a) and CRF (b) models
</figureCaption>
<figure confidence="0.984937375">
Conv Baseline Conv Rescore Conv OracleBreak Sent Baseline
Conv Baseline Conv Prosody Conv OracleBreak Sent Baseline
0
400
Number of Errors
300
200
100
</figure>
<bodyText confidence="0.999075777777778">
which performs worse on these slightly longer seg-
ments. With the addition of break indexes, we see
marginal changes in most of the models; only the
CRF tagger receives a significant boost. Thus, mod-
els achieve performance gains from tagging shorter
segments, but at the cost of limited usefulness of the
prosodic breaks. Overall, speaker turn segmenta-
tion is an attractive compromise between the original
conversation sides and human-annotated sentences.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999976346153846">
Across the different models, we have found that tag-
gers applied to shorter segments, either sentences or
speaker turns, do not tend to benefit significantly
from prosodic enrichment, in contrast to conversa-
tion sides. To analyze this further we broke down
the results by part of speech for the two models
for which break indexes improved performance the
most: the CRF and HMM-LA rescoring models,
which achieved an overall error reduction of 2.8%
and 2.1%, respectively. We present those categories
that obtained the greatest benefit from prosody in
Figure 5 (a) and (b). For both models, the LTH cate-
gory had a dramatic improvement from the addition
of prosody, achieving up to a 10% reduction in error.
For the CRF model, other categories that saw im-
pressive error reductions were NN and VB, with
10% and 5%, respectively. Table 5 lists the prosodic
features that received the highest weight in the CRF
model. These are quite intuitive, as they seem to rep-
resent places where the prosody indicates sentence
or clausal boundaries. For the HMM-LA model,
the VB and DT tags had major reductions in error
of 13% and 10%, respectively. For almost all cat-
egories, the number of errors is reduced by the ad-
dition of breaks, and further reduced by using the
OracleBreak processing described above.
</bodyText>
<subsectionHeader confidence="0.811155">
Weight Feature
</subsectionHeader>
<bodyText confidence="0.89608">
2.2212 wz=um &amp; bz=4 &amp; t=LTH
1.9464 wz=uh &amp; bz=4 &amp; t=LTH
1.7965 wz=yes &amp; bz=4 &amp; t=LTH
1.7751 wz=and &amp; bz=4 &amp; t=CC
1.7554 wz=so &amp; bz=4 &amp; t=RB
1.7373 wz=but &amp; bz=4 &amp; t=CC
</bodyText>
<tableCaption confidence="0.963975">
Table 5: Top break 4 prosody features in CRF prosody
model
</tableCaption>
<bodyText confidence="0.999869125">
To determine more precisely the effect that the
segment size has on tagging accuracy, we extracted
the oracle tag sequences from the HMM-LA and
CRF baseline and prosodically enriched models
across conversation sides, sentences, and speaker
turn segments. As the plot in Figure 6 shows, as
we increase the n-best list size to 500, the ora-
cle accuracy of the models trained on sentences in-
</bodyText>
<page confidence="0.986521">
828
</page>
<figure confidence="0.79041">
2 5 10 20 50 100 200 500
N-Best size
</figure>
<figureCaption confidence="0.973">
Figure 6: Oracle comparison: solid lines for sentences,
dashed lines for speaker turns, and dotted lines for con-
versation sides
</figureCaption>
<bodyText confidence="0.995893666666667">
creases rapidly to 99%; whereas, the oracle accu-
racy of models on conversation sides grow slowly
to between 94% and 95%. The speaker turn trained
models, however, behave closely to those using sen-
tences, climbing rapidly to accuracies of around
98%. This difference is directly attributable to the
length of the segments. As can be seen in Table 6,
the speaker turn segments are more comparable in
length to sentences.
</bodyText>
<table confidence="0.998747">
Train Eval
Conv 627.87 ± 281.57 502.98 ± 151.22
Sent 7.52± 7.86 7.45 ± 8.29
Speaker 15.60± 29.66 15.27± 21.01
</table>
<tableCaption confidence="0.999465">
Table 6: Length statistics of different data segmentations
</tableCaption>
<bodyText confidence="0.999954428571429">
Next, we return to the large performance degrada-
tion when tagging speech rather than newswire text
to examine the major differences among the mod-
els. Using two of our best performing models, the
Stanford bidirectional and HMM-LA, in Figure 7
we present the categories for which performance
degradation was the greatest when comparing per-
formance of a tagger trained on WSJ to a tagger
trained on spoken sentences and conversation sides.
The performance decrease is quite similar across
both models, with the greatest degradation on the
NNP, RP, VBN, and RBS categories.
Unsurprisingly, both the discriminative and gen-
erative bidirectional models achieve the most im-
pressive results. However, the generative HMM-
LA and HMM-LA-Bidir models achieved the best
results across all three segmentations, and the best
overall result, of 94.35%, on prosodically enriched
sentence-segmented data. Since the Stanford bidi-
rectional model incorporates all of the features that
produced its state-of-the-art performance on WSJ,
we believe the fact that the HMM-LA outperforms
it, despite the discriminative model’s more expres-
sive feature set, is indicative of the HMM-LA’s abil-
ity to more effectively adapt to novel domains during
training. Another challenge for the discriminative
models is the need for regularization tuning, requir-
ing additional time and effort to train several mod-
els and select the most appropriate parameter each
time the domain changes. Whereas for the HMM-
LA models, although we also train several models,
they can be combined into a product model, such as
that described by Petrov (2010), in order to further
improve performance.
Since the prosodic breaks are noisier features than
the others incorporated in the discriminative models,
it may be useful to set their regularization param-
eter separately from the rest of the features, how-
ever, we have not explored this alternative. Our ex-
periments used human transcriptions of the conver-
sational speech; however, realistically our models
would be applied to speech recognition transcripts.
In such a case, word error will introduce noise in ad-
dition to the prosodic breaks. In future work, we will
evaluate the use of break indexes for tagging when
there is lexical error. We would also apply the n-
best rescoring method to exploit break indexes in the
HMM-LA bidirectional model, as this would likely
produce further improvements.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="references">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998078">
In this work, we have evaluated factors that are im-
portant for developing accurate tagging models for
speech. Given that prosodic breaks were effective
knowledge sources for parsing, an important goal
of this work was to evaluate their impact on vari-
ous tagging model configurations. Specifically, we
have examined the use of prosodic information for
tagging conversational speech with several different
discriminative and generative models across three
different speech transcript segmentations. Our find-
</bodyText>
<figure confidence="0.97178056">
Sentences
98
Speaker tuns
94
Conversation sides
100
96
Accuracy
92
829
Error Rate
40%
20%
50%
30%
10%
0%
- r -
( a o ) ( )
Conv (Stanford-Bidir) Conv (HMM-LA)
HMM
Sent (Stanford-Bidir) Sent
-LA)
NNP VBN WP CD RP EX WRB WDT JJR POS JJS RBS
d conversation sides
</figure>
<figureCaption confidence="0.999964">
Figure 7: Comparison of error rates between the Standford Bidir and HMM-LA models trained on WSJ, sentences,
</figureCaption>
<bodyText confidence="0.978509433333333">
an
ings suggest that generative models with latent an-
notations achieve the best performance in this chal-
lenging domain. In terms of transcript segmenta-
tion, if sentences are available, it is preferable to use
them. In the case that no such annotation is avail-
able, then using automatic sentence boundary detec-
tion does not serve as an appropriate replacement,
but if automatic speaker turn segments can be ob-
tained, then this is a good alternative, despite the fact
that prosodic enrichment is less effective.
Our investigation also shows that in the event that
conversation sides must be used, prosodic enrich-
ment of the discriminative and generative models
produces significant improvements in tagging accu-
racy (by direct integration of prosody features for
the former and by restricting the search space and
rescoring with the latter). For tagging, the most im-
portant role of the break indexes appears to be as a
stand in for sentence boundaries. The oracle break
experiments suggest that if the accuracy of the au-
tomatically induced break indexes can be improved,
then the prosodically enriched models will perform
as well, or even better, than their human-annotated
sentence counterpart
This research was supported in part by NSF IIS-
0703859 and the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-2-001. Any opinions, findings, and rec-
ommendations expressed in this paper are those of
</bodyText>
<figure confidence="0.64798325">
nces
Anton Batliner, Bernd
Gregor
Antje
</figure>
<reference confidence="0.972504139534884">
Schweitzer, and Elmar
Prosodic models,
automatic speech understanding, and speech synthesis:
toward the common ground. In Eurospeech.
Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki
Maeda, Seth Kulick, Yang Liu, Mary Harper, and
Matthew Lease. 2006. Linguistic resources for speech
parsing. In LREC.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical report, Technical Report CMU-CS-99-108,
Carnegie Mellon University.
Anne Cutler, Delphine
and Wilma v an Donselaar.
1997. Prosody in comprehension of spoken language:
A literature review. Language and Speech.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Inter-
speech.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Jennifer Foster. 2010.
to check the
Inves-
tigating parser performance on discussion forum posts.
In NAACL-HLT.
Florian Gallwitz, Heinrich Niemann, Elmar
and
Volker Warnke. 2002. Integrated recognition of words
and prosodic phrase boundaries. Speech Communica-
tion.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In ICASSP.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In NAACL.
Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. 2005 Johns Hopkins Summer Work-
shop Final Report on Parsing an
</reference>
<figure confidence="0.955084363636364">
Refere
M¨obius,
M¨ohler,
N¨oth.2001.
Dahan,
“cba
spelling”:
N¨oth,
d Spoken Structural
s.
8 Acknowledgments
</figure>
<bodyText confidence="0.844114">
the authors an
d do not necessarily reflect the views
of the funding agency or the institutions where the
work was completed.
</bodyText>
<page confidence="0.982027">
830
</page>
<reference confidence="0.999450470588235">
Event Detection. Technical report, Johns Hopkins
University.
Mark Hasegawa-Johnson, Ken Chen, Jennifer Cole,
Sarah Borys, Sung suk Kim, Aaron Cohen, Tong
Zhang, Jeung yoon Choi, Heejin Kim, Taejin Yoon,
and Ra Chavarria. 2005. Simultaneous recognition
of words and prosody in the boston university radio
speech corpus. speech communication. Speech Com-
munication.
Peter A. Heeman. 1999. POS tags and decision trees for
language modeling. In EMNLP.
Dustin Hillard, Zhongqiang Huang, Heng Ji, Ralph Gr-
ishman, Dilek Hakkani-Tur, Mary Harper, Mari Os-
tendorf, and Wen Wang. 2006. Impact of automatic
comma prediction on POS/name tagging of speech. In
ICASSP.
Zhongqiang Huang and Mary Harper. 2010. Appropri-
ately handled prosodic breaks help PCFG parsing. In
NAACL.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm part-
of-speech tagger by latent annotation and self-training.
In NAACL-HLT.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP-HLT.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
D. C. Liu and Jorge Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math-
ematical Programming.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Mari Ostendorf, Izhak Shafran, and Rebecca Bates.
2003. Prosody models for conversational speech
recognition. In Plenary Meeting and Symposium on
Prosody and Speech Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov. 2010. Products of random latent variable
grammars. In HLT-NAACL.
Brian Roark, Yang Liu, Mary Harper, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Reranking for sentence boundary detec-
tion in conversational speech. In ICASSP.
Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, and Julia Hirshberg. 1992. ToBI: A standard for
labeling English prosody. In ICSLP.
Paul Taylor and Alan W. Black. 1998. Assigning
phrase breaks from part-of-speech sequences. Com-
puter Speech and Language.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden markov model for part-of-speech tag-
ging. In ACL.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL.
</reference>
<page confidence="0.998384">
831
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.725237">
<title confidence="0.9380355">Lessons Learned in Part-of-Speech Tagging of Conversational Speech Zhongqiang and Mary for Computational Linguistics and Information</title>
<affiliation confidence="0.964643">Institute for Advanced Computer University of Maryland, College Park, Language Technology Center of Johns Hopkins University, Baltimore,</affiliation>
<abstract confidence="0.99823976">This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Schweitzer</author>
</authors>
<title>Elmar Prosodic models, automatic speech understanding, and speech synthesis: toward the common ground.</title>
<booktitle>In Eurospeech.</booktitle>
<marker>Schweitzer, </marker>
<rawString>Schweitzer, and Elmar Prosodic models, automatic speech understanding, and speech synthesis: toward the common ground. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Stephanie Strassel</author>
<author>Haejoong Lee</author>
<author>Kazuaki Maeda</author>
<author>Seth Kulick</author>
<author>Yang Liu</author>
<author>Mary Harper</author>
<author>Matthew Lease</author>
</authors>
<title>Linguistic resources for speech parsing.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="9998" citStr="Bies et al., 2006" startWordPosition="1546" endWordPosition="1549">ord bidirectional tagger significantly outperforms the other models with the exception of the HMM-LA-Bidir model on this task.1 1Statistically significant improvements are calculated using the sign test (p &lt; 0.05). Model Accuracy Trigram HMM 96.58 HMM-LA 97.05 HMM-LA-Bidir 97.16 Stanford Bidir 97.28 Stanford Left5 97.07 CRF 96.81 Table 2: Tagging accuracy on WSJ 3 Experimental Setup In the rest of this paper, we evaluate the tagging models described in Section 2 on conversational speech. We chose to utilize the Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006) because they provide gold standard tags for conversational speech and we have access to corresponding automatically generated ToBI break indexes provided by (Dreyer and Shafran, 2007; Harper et al., 2005)2. We utilized the Fisher dev1 and dev2 sets containing 16,519 sentences (112,717 words) as the primary training data and the entire Penn Switchboard treebank containing 110,504 sentences (837,863 words) as an additional training source3. The treebanks were preprocessed as follows: the tags of auxiliary verbs were replaced with the AUX tag, empty nodes 2A small fraction of words in the Switch</context>
</contexts>
<marker>Bies, Strassel, Lee, Maeda, Kulick, Liu, Harper, Lease, 2006</marker>
<rawString>Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki Maeda, Seth Kulick, Yang Liu, Mary Harper, and Matthew Lease. 2006. Linguistic resources for speech parsing. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report, Technical Report CMU-CS-99-108,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="8407" citStr="Chen and Rosenfeld, 1999" startWordPosition="1289" endWordPosition="1293">meters of the model to estimate and F indicates the feature functions used. The denominator in (1) is Zλ(x), the normalization factor, with: Fj(t, w) _ � fj(t, w, i) i 822 Class Model Name Latent Variable Bidirectional N-best-Extraction Markov Order Generative Trigram HMM √ √ √ 2nd HMM-LA √ √ 1st HMM-LA-Bidir 1st Discriminative Stanford Bidir √ 2nd Stanford Left5 2nd CRF 2nd Table 1: Description of tagging models The objective we need to maximize then becomes : X ⎡ ⎤ L = ⎣X AjFj(tni wn) − log Za(xn) − kA 2uk2 n 2Q2 j where we use a spherical Gaussian prior to prevent overfitting of the model (Chen and Rosenfeld, 1999) and the wide-spread quasi-Newtonian L-BFGS method to optimize the model parameters (Liu and Nocedal, 1989). Decoding is performed with the Viterbi algorithm. We also evaluate state-of-the-art Maximum Entropy taggers: the Stanford Left5 tagger (Toutanova and Manning, 2000) and the Stanford bidirectional tagger (Toutanova et al., 2003), with the former using only left context and the latter bidirectional dependencies. Table 1 summarizes the major differences between the models along several dimensions: (1) generative versus discriminative, (2) directionality of decoding, (3) the presence or abs</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical report, Technical Report CMU-CS-99-108, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Cutler</author>
</authors>
<title>Delphine and Wilma v an Donselaar.</title>
<date>1997</date>
<marker>Cutler, 1997</marker>
<rawString>Anne Cutler, Delphine and Wilma v an Donselaar. 1997. Prosody in comprehension of spoken language: A literature review. Language and Speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Izhak Shafran</author>
</authors>
<title>Exploiting prosody for PCFGs with latent annotations.</title>
<date>2007</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="3062" citStr="Dreyer and Shafran, 2007" startWordPosition="442" endWordPosition="445">thesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, MIT, Massachusetts, </context>
<context position="10181" citStr="Dreyer and Shafran, 2007" startWordPosition="1573" endWordPosition="1576">lated using the sign test (p &lt; 0.05). Model Accuracy Trigram HMM 96.58 HMM-LA 97.05 HMM-LA-Bidir 97.16 Stanford Bidir 97.28 Stanford Left5 97.07 CRF 96.81 Table 2: Tagging accuracy on WSJ 3 Experimental Setup In the rest of this paper, we evaluate the tagging models described in Section 2 on conversational speech. We chose to utilize the Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006) because they provide gold standard tags for conversational speech and we have access to corresponding automatically generated ToBI break indexes provided by (Dreyer and Shafran, 2007; Harper et al., 2005)2. We utilized the Fisher dev1 and dev2 sets containing 16,519 sentences (112,717 words) as the primary training data and the entire Penn Switchboard treebank containing 110,504 sentences (837,863 words) as an additional training source3. The treebanks were preprocessed as follows: the tags of auxiliary verbs were replaced with the AUX tag, empty nodes 2A small fraction of words in the Switchboard treebank do not align with the break indexes because they were produced based on a later refinement of the transcripts used to produce the treebank. For these cases, we heuristi</context>
<context position="12156" citStr="Dreyer and Shafran, 2007" startWordPosition="1892" endWordPosition="1895">enerative and discriminative models. Since the EM algorithm used for estimating the parameters in the latent variable models introduces a lot of variability, we train five models with a different seed and then choose the best one based on dev set performance. For the discriminative models, we tuned their respective regularization parameters on the dev set. All results reported in the rest of this paper are on the test set. 4 Integration of Prosodic Information In this work, we use three classes of automatically generated ToBI break indexes to represent prosodic information (Kahn et al., 2005; Dreyer and Shafran, 2007; Huang and Harper, 2010): 4, 1, and p. Consider the following speech transcription example, which is enriched with ToBI break indexes in parentheses and tags: i(1)/PRP did(1)/VBD n’t(1)/RB you(1)/PRP know(4)/VBP i(1)/PRP did(1)/AUX n’t(1)/RB... The speaker begins an utterance, and then restarts the utterance. The automatically predicted break 4 associated with know in the utterance compellingly indicates an intonational phrase boundary and could provide useful information for tagging if we can model it appropriately. To integrate prosody into our generative models, we utilize the method from </context>
</contexts>
<marker>Dreyer, Shafran, 2007</marker>
<rawString>Markus Dreyer and Izhak Shafran. 2007. Exploiting prosody for PCFGs with latent annotations. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>A joint language model with fine-grain syntactic tags.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4846" citStr="Filimonov and Harper, 2009" startWordPosition="702" endWordPosition="705">that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcriptions has implications for improving downstream applications that rely on accurate POS tags, including sentence boundary detection (Liu et al., 2005), automatic punctuation (Hillard et al., 2006), information extraction from speech, parsing, and syntactic language modeling (Heeman, 1999; Filimonov and Harper, 2009). While there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts, to the best of our knowledge there has been little work on using this type of information for POS tagging. Furthermore, most of the parsing work has involved generative models and rescoring/reranking of hypotheses from the generative models. In this work, we will analyze several factors related to effective POS tagging of conversational speech: • discriminative versus generative POS tagging models (Section 2) • prosodic features in the form of simplified ToBI break indexe</context>
</contexts>
<marker>Filimonov, Harper, 2009</marker>
<rawString>Denis Filimonov and Mary Harper. 2009. A joint language model with fine-grain syntactic tags. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>to check the Investigating parser performance on discussion forum posts.</title>
<date>2010</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="1684" citStr="Foster, 2010" startWordPosition="235" endWordPosition="236">oves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal. 1 Introduction Natural language processing technologies, such as parsing and tagging, often require reconfiguration when they are applied to challenging domains that differ significantly from newswire, e.g., blogs, twitter text (Foster, 2010), or speech. In contrast to text, conversational speech represents a significant challenge because the transcripts are not segmented into sentences. Furthermore, the transcripts are often disfluent and lack punctuation and case information. On the other hand, speech provides additional information, beyond simply the sequence of words, which could be exploited to more accurately assign each word in the transcript a part-of-speech (POS) tag. One potentially beneficial type of information is prosody (Cutler et al., 1997). Prosody provides cues for lexical disambiguation, sentence segmentation and</context>
</contexts>
<marker>Foster, 2010</marker>
<rawString>Jennifer Foster. 2010. to check the Investigating parser performance on discussion forum posts. In NAACL-HLT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Florian Gallwitz</author>
<author>Heinrich Niemann</author>
</authors>
<note>Elmar and</note>
<marker>Gallwitz, Niemann, </marker>
<rawString>Florian Gallwitz, Heinrich Niemann, Elmar and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volker Warnke</author>
</authors>
<title>Integrated recognition of words and prosodic phrase boundaries. Speech Communication.</title>
<date>2002</date>
<marker>Warnke, 2002</marker>
<rawString>Volker Warnke. 2002. Integrated recognition of words and prosodic phrase boundaries. Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="9936" citStr="Godfrey et al., 1992" startWordPosition="1534" endWordPosition="1537">all the models have high accuracy on newswire data, but the Stanford bidirectional tagger significantly outperforms the other models with the exception of the HMM-LA-Bidir model on this task.1 1Statistically significant improvements are calculated using the sign test (p &lt; 0.05). Model Accuracy Trigram HMM 96.58 HMM-LA 97.05 HMM-LA-Bidir 97.16 Stanford Bidir 97.28 Stanford Left5 97.07 CRF 96.81 Table 2: Tagging accuracy on WSJ 3 Experimental Setup In the rest of this paper, we evaluate the tagging models described in Section 2 on conversational speech. We chose to utilize the Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006) because they provide gold standard tags for conversational speech and we have access to corresponding automatically generated ToBI break indexes provided by (Dreyer and Shafran, 2007; Harper et al., 2005)2. We utilized the Fisher dev1 and dev2 sets containing 16,519 sentences (112,717 words) as the primary training data and the entire Penn Switchboard treebank containing 110,504 sentences (837,863 words) as an additional training source3. The treebanks were preprocessed as follows: the tags of auxiliary verbs were replaced with the</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle L Gregory</author>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>Sentence-internal prosody does not help parsing the way punctuation does.</title>
<date>2004</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3084" citStr="Gregory et al., 2004" startWordPosition="446" endWordPosition="449">t al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, MIT, Massachusetts, USA, 9-11 October 2010</context>
</contexts>
<marker>Gregory, Johnson, Charniak, 2004</marker>
<rawString>Michelle L. Gregory, Mark Johnson, and Eugene Charniak. 2004. Sentence-internal prosody does not help parsing the way punctuation does. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary P Harper</author>
<author>Bonnie J Dorr</author>
<author>John Hale</author>
<author>Brian Roark</author>
<author>Izhak Shafran</author>
<author>Matthew Lease</author>
</authors>
<title>Johns Hopkins Summer Workshop Final Report on Parsing an Event Detection.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Johns Hopkins University.</institution>
<location>Yang Liu, Matthew Snover, Lisa Yung, Anna</location>
<contexts>
<context position="3012" citStr="Harper et al., 2005" startWordPosition="433" endWordPosition="436"> found to play an important role in speech synthesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Langua</context>
<context position="9978" citStr="Harper et al., 2005" startWordPosition="1542" endWordPosition="1545">e data, but the Stanford bidirectional tagger significantly outperforms the other models with the exception of the HMM-LA-Bidir model on this task.1 1Statistically significant improvements are calculated using the sign test (p &lt; 0.05). Model Accuracy Trigram HMM 96.58 HMM-LA 97.05 HMM-LA-Bidir 97.16 Stanford Bidir 97.28 Stanford Left5 97.07 CRF 96.81 Table 2: Tagging accuracy on WSJ 3 Experimental Setup In the rest of this paper, we evaluate the tagging models described in Section 2 on conversational speech. We chose to utilize the Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006) because they provide gold standard tags for conversational speech and we have access to corresponding automatically generated ToBI break indexes provided by (Dreyer and Shafran, 2007; Harper et al., 2005)2. We utilized the Fisher dev1 and dev2 sets containing 16,519 sentences (112,717 words) as the primary training data and the entire Penn Switchboard treebank containing 110,504 sentences (837,863 words) as an additional training source3. The treebanks were preprocessed as follows: the tags of auxiliary verbs were replaced with the AUX tag, empty nodes 2A small fraction of</context>
<context position="17943" citStr="Harper et al. (2005)" startWordPosition="2824" endWordPosition="2827">e similar to the conversation side segmentation performances, and thus significantly lower than when tagging human-annotated sentences. A caveat to consider here is that we break the standard assumption that the training and test set be drawn from the same distribution, since the training data is human-annotated and the test is automatically segmented. However, it can be quite challenging to create a corpus to train on that represents the biases of the systems that perform automatic sentence segmentation. Instead, we will examine an5We used the Baseline Structural Metadata System described in Harper et al. (2005) to predict sentence boundaries. 825 Figure 3: Tagging accuracy on human-annotated segments HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF Baseline Prosody OracleBreak Rescoring 94.5 94.2 93.9 93.6 93.3 93 other segmentation method to shorten the segments automatically, i.e., by training and testing on speaker turns, which preserves the train-test match, in Section 5.5. Model Accuracy HMM-LA 93.95 HMM-LA-Bidir 94.07 Stanford Bidir 93.77 Stanford Left5 93.35 CRF 93.29 Table 4: Baseline tagging accuracy on automatically detected sentence boundaries 5.3 Oracle Break Insertion As we believe</context>
</contexts>
<marker>Harper, Dorr, Hale, Roark, Shafran, Lease, 2005</marker>
<rawString>Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark, Izhak Shafran, Matthew Lease, Yang Liu, Matthew Snover, Lisa Yung, Anna Krasnyanskaya, and Robin Stewart. 2005. 2005 Johns Hopkins Summer Workshop Final Report on Parsing an Event Detection. Technical report, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hasegawa-Johnson</author>
<author>Ken Chen</author>
<author>Jennifer Cole</author>
<author>Sarah Borys</author>
<author>Sung suk Kim</author>
<author>Aaron Cohen</author>
<author>Tong Zhang</author>
<author>Jeung yoon Choi</author>
<author>Heejin Kim</author>
<author>Taejin Yoon</author>
<author>Ra Chavarria</author>
</authors>
<title>Simultaneous recognition of words and prosody in the boston university radio speech corpus. speech communication. Speech Communication.</title>
<date>2005</date>
<contexts>
<context position="2588" citStr="Hasegawa-Johnson et al., 2005" startWordPosition="367" endWordPosition="370"> additional information, beyond simply the sequence of words, which could be exploited to more accurately assign each word in the transcript a part-of-speech (POS) tag. One potentially beneficial type of information is prosody (Cutler et al., 1997). Prosody provides cues for lexical disambiguation, sentence segmentation and classification, phrase structure and attachment, discourse structure, speaker affect, etc. Prosody has been found to play an important role in speech synthesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of jun</context>
</contexts>
<marker>Hasegawa-Johnson, Chen, Cole, Borys, Kim, Cohen, Zhang, Choi, Kim, Yoon, Chavarria, 2005</marker>
<rawString>Mark Hasegawa-Johnson, Ken Chen, Jennifer Cole, Sarah Borys, Sung suk Kim, Aaron Cohen, Tong Zhang, Jeung yoon Choi, Heejin Kim, Taejin Yoon, and Ra Chavarria. 2005. Simultaneous recognition of words and prosody in the boston university radio speech corpus. speech communication. Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
</authors>
<title>POS tags and decision trees for language modeling.</title>
<date>1999</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4817" citStr="Heeman, 1999" startWordPosition="700" endWordPosition="701">gative effect that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcriptions has implications for improving downstream applications that rely on accurate POS tags, including sentence boundary detection (Liu et al., 2005), automatic punctuation (Hillard et al., 2006), information extraction from speech, parsing, and syntactic language modeling (Heeman, 1999; Filimonov and Harper, 2009). While there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts, to the best of our knowledge there has been little work on using this type of information for POS tagging. Furthermore, most of the parsing work has involved generative models and rescoring/reranking of hypotheses from the generative models. In this work, we will analyze several factors related to effective POS tagging of conversational speech: • discriminative versus generative POS tagging models (Section 2) • prosodic features in the form of</context>
</contexts>
<marker>Heeman, 1999</marker>
<rawString>Peter A. Heeman. 1999. POS tags and decision trees for language modeling. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Hillard</author>
<author>Zhongqiang Huang</author>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Dilek Hakkani-Tur</author>
<author>Mary Harper</author>
<author>Mari Ostendorf</author>
<author>Wen Wang</author>
</authors>
<title>Impact of automatic comma prediction on POS/name tagging of speech.</title>
<date>2006</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="4725" citStr="Hillard et al., 2006" startWordPosition="687" endWordPosition="690">id not perform significantly better than the baseline PCFG-LA model without enrichment, due to the negative effect that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcriptions has implications for improving downstream applications that rely on accurate POS tags, including sentence boundary detection (Liu et al., 2005), automatic punctuation (Hillard et al., 2006), information extraction from speech, parsing, and syntactic language modeling (Heeman, 1999; Filimonov and Harper, 2009). While there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts, to the best of our knowledge there has been little work on using this type of information for POS tagging. Furthermore, most of the parsing work has involved generative models and rescoring/reranking of hypotheses from the generative models. In this work, we will analyze several factors related to effective POS tagging of conversational speech: • discri</context>
</contexts>
<marker>Hillard, Huang, Ji, Grishman, Hakkani-Tur, Harper, Ostendorf, Wang, 2006</marker>
<rawString>Dustin Hillard, Zhongqiang Huang, Heng Ji, Ralph Grishman, Dilek Hakkani-Tur, Mary Harper, Mari Ostendorf, and Wen Wang. 2006. Impact of automatic comma prediction on POS/name tagging of speech. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Appropriately handled prosodic breaks help PCFG parsing.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3474" citStr="Huang and Harper (2010)" startWordPosition="509" endWordPosition="512">05). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics tion in the form of this simplified three class ToBI encoding when parsing spontaneous speech by using a prosodically enriched PCFG model with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) to rescore n-best parses produced by a baseline PCFG-LA model without prosodic enrichment. However, the prosodically e</context>
<context position="12181" citStr="Huang and Harper, 2010" startWordPosition="1896" endWordPosition="1899">ve models. Since the EM algorithm used for estimating the parameters in the latent variable models introduces a lot of variability, we train five models with a different seed and then choose the best one based on dev set performance. For the discriminative models, we tuned their respective regularization parameters on the dev set. All results reported in the rest of this paper are on the test set. 4 Integration of Prosodic Information In this work, we use three classes of automatically generated ToBI break indexes to represent prosodic information (Kahn et al., 2005; Dreyer and Shafran, 2007; Huang and Harper, 2010): 4, 1, and p. Consider the following speech transcription example, which is enriched with ToBI break indexes in parentheses and tags: i(1)/PRP did(1)/VBD n’t(1)/RB you(1)/PRP know(4)/VBP i(1)/PRP did(1)/AUX n’t(1)/RB... The speaker begins an utterance, and then restarts the utterance. The automatically predicted break 4 associated with know in the utterance compellingly indicates an intonational phrase boundary and could provide useful information for tagging if we can model it appropriately. To integrate prosody into our generative models, we utilize the method from (Dreyer and Shafran, 2007</context>
<context position="18904" citStr="Huang and Harper (2010)" startWordPosition="2970" endWordPosition="2973">ves the train-test match, in Section 5.5. Model Accuracy HMM-LA 93.95 HMM-LA-Bidir 94.07 Stanford Bidir 93.77 Stanford Left5 93.35 CRF 93.29 Table 4: Baseline tagging accuracy on automatically detected sentence boundaries 5.3 Oracle Break Insertion As we believe one of the major roles that prosodic cues serve for tagging conversation sides is as a proxy for sentence boundaries, perhaps the efficacy of the prosodic breaks can, at least partially, be attributed to errors in the automatically induced break indexes themselves, as they can misalign with syntactic phrase boundaries, as discussed in Huang and Harper (2010). This may degrade the performance of our models more than the improvement achieved from correctly placed breaks. Hence, we conduct a series of experiments in which we systematically eliminate noisy phrase and disfluency breaks and show that under these improved conditions, prosodically enriched models can indeed be more effective. To investigate to what extent noisy breaks are impeding the possible improvements from prosodically enriched models, we replaced all 4 and p breaks in the training and evaluation sets that did not align to the correct phrase boundaries as indicated by the treebank w</context>
<context position="21702" citStr="Huang and Harper, 2010" startWordPosition="3403" endWordPosition="3406"> Stanford Bidir Stanford Left5 CRF Baseline Prosody Rescoring Figure 4: Tagging accuracy on speaker turns 4 (essentially combining OracleBreak and OracleSent). As Figure 2 indicates, this modification results in the best tagging accuracies for all the models. All models were able to match or even improve upon the baseline accuracies achieved on the human segmented data. This suggests that when we have breaks that align with phrasal and sentence boundaries, prosodically enriched models are highly effective. 5.4 N-best Rescoring Based on the findings in the previous section and the findings of (Huang and Harper, 2010), we next apply a rescoring strategy in which the search space of the prosodically enriched generative models is restricted to the n-best list generated from the baseline model (without prosodic enrichment). In this manner, the prosodically enriched model can avoid poor tag sequences produced due to the misaligned break indexes. As Figure 2 shows, using the baseline conversation side model to produce an n-best list for the prosodically enriched model to rescore results in significant improvements in performance for the HMM-LA model, similar to the parsing results of (Huang and Harper, 2010). T</context>
</contexts>
<marker>Huang, Harper, 2010</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2010. Appropriately handled prosodic breaks help PCFG parsing. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Vladimir Eidelman</author>
<author>Mary Harper</author>
</authors>
<title>Improving a simple bigram hmm partof-speech tagger by latent annotation and self-training.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="6097" citStr="Huang et al. (2009)" startWordPosition="905" endWordPosition="908">egmentation (Section 5) 2 Models In order to fully evaluate the difficulties inherent in tagging conversational speech, as well as the possible benefits of prosodic information, we conducted experiments with six different POS tagging models. The models can be broadly separated into two classes: generative and discriminative. As the first of our generative models, we used a Hidden Markov Model (HMM) trigram tagger (Thede and Harper, 1999), which serves to establish a baseline and to gauge the difficulty of the task at hand. Our second model, HMM-LA, was the latent variable bigram HMM tagger of Huang et al. (2009), which achieved state-of-the-art tagging performance by introducing latent tags to weaken the stringent Markov independence assumptions that generally hinder tagging performance in generative models. For the third model, we implemented a bidirectional variant of the HMM-LA (HMM-LA-Bidir) that combines evidence from two HMM-LA taggers, one trained left-to-right and the other right-toleft. For decoding, we use a product model (Petrov, 2010). The intuition is that the context information from the left and the right of the current position is complementary for predicting the current tag and thus,</context>
</contexts>
<marker>Huang, Eidelman, Harper, 2009</marker>
<rawString>Zhongqiang Huang, Vladimir Eidelman, and Mary Harper. 2009. Improving a simple bigram hmm partof-speech tagger by latent annotation and self-training. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy G Kahn</author>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Mari Ostendorf</author>
</authors>
<title>Effective use of prosody in parsing conversational speech.</title>
<date>2005</date>
<booktitle>In EMNLP-HLT.</booktitle>
<contexts>
<context position="3104" citStr="Kahn et al., 2005" startWordPosition="450" endWordPosition="453">d Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association</context>
<context position="12130" citStr="Kahn et al., 2005" startWordPosition="1888" endWordPosition="1891">fferently for the generative and discriminative models. Since the EM algorithm used for estimating the parameters in the latent variable models introduces a lot of variability, we train five models with a different seed and then choose the best one based on dev set performance. For the discriminative models, we tuned their respective regularization parameters on the dev set. All results reported in the rest of this paper are on the test set. 4 Integration of Prosodic Information In this work, we use three classes of automatically generated ToBI break indexes to represent prosodic information (Kahn et al., 2005; Dreyer and Shafran, 2007; Huang and Harper, 2010): 4, 1, and p. Consider the following speech transcription example, which is enriched with ToBI break indexes in parentheses and tags: i(1)/PRP did(1)/VBD n’t(1)/RB you(1)/PRP know(4)/VBP i(1)/PRP did(1)/AUX n’t(1)/RB... The speaker begins an utterance, and then restarts the utterance. The automatically predicted break 4 associated with know in the utterance compellingly indicates an intonational phrase boundary and could provide useful information for tagging if we can model it appropriately. To integrate prosody into our generative models, w</context>
</contexts>
<marker>Kahn, Lease, Charniak, Johnson, Ostendorf, 2005</marker>
<rawString>Jeremy G. Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf. 2005. Effective use of prosody in parsing conversational speech. In EMNLP-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7557" citStr="Lafferty et al., 2001" startWordPosition="1131" endWordPosition="1134">osodic information, and then perform rescoring in order to achieve gains. However, it is far simpler to directly integrate prosody as features into the model by using a discriminative approach. Hence, we also investigate several log-linear models, which allow us to easily include an arbitrary number and varying kinds of possibly overlapping and non-independent features. First, we implemented a Conditional Random Field (CRF) tagger, which is an attractive choice due to its ability to learn the globally optimal labeling for a sequence and proven excellent performance on sequence labeling tasks (Lafferty et al., 2001). In contrast to an HMM which optimizes the joint likelihood of the word sequence and tags, a CRF optimizes the conditional likelihood, given by: pλ(t|w) _ exp Ej AjFj(t, w) (1) Et exp Ej AjFj(t, w) where the A’s are the parameters of the model to estimate and F indicates the feature functions used. The denominator in (1) is Zλ(x), the normalization factor, with: Fj(t, w) _ � fj(t, w, i) i 822 Class Model Name Latent Variable Bidirectional N-best-Extraction Markov Order Generative Trigram HMM √ √ √ 2nd HMM-LA √ √ 1st HMM-LA-Bidir 1st Discriminative Stanford Bidir √ 2nd Stanford Left5 2nd CRF 2</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming.</booktitle>
<contexts>
<context position="8514" citStr="Liu and Nocedal, 1989" startWordPosition="1306" endWordPosition="1309">he normalization factor, with: Fj(t, w) _ � fj(t, w, i) i 822 Class Model Name Latent Variable Bidirectional N-best-Extraction Markov Order Generative Trigram HMM √ √ √ 2nd HMM-LA √ √ 1st HMM-LA-Bidir 1st Discriminative Stanford Bidir √ 2nd Stanford Left5 2nd CRF 2nd Table 1: Description of tagging models The objective we need to maximize then becomes : X ⎡ ⎤ L = ⎣X AjFj(tni wn) − log Za(xn) − kA 2uk2 n 2Q2 j where we use a spherical Gaussian prior to prevent overfitting of the model (Chen and Rosenfeld, 1999) and the wide-spread quasi-Newtonian L-BFGS method to optimize the model parameters (Liu and Nocedal, 1989). Decoding is performed with the Viterbi algorithm. We also evaluate state-of-the-art Maximum Entropy taggers: the Stanford Left5 tagger (Toutanova and Manning, 2000) and the Stanford bidirectional tagger (Toutanova et al., 2003), with the former using only left context and the latter bidirectional dependencies. Table 1 summarizes the major differences between the models along several dimensions: (1) generative versus discriminative, (2) directionality of decoding, (3) the presence or absence of latent annotations, (4) the availability of n-best extraction, and (5) the model order. In order to</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Mary Harper</author>
</authors>
<title>Using conditional random fields for sentence boundary detection in speech.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2854" citStr="Liu et al., 2005" startWordPosition="407" endWordPosition="410">xical disambiguation, sentence segmentation and classification, phrase structure and attachment, discourse structure, speaker affect, etc. Prosody has been found to play an important role in speech synthesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Hua</context>
<context position="4679" citStr="Liu et al., 2005" startWordPosition="681" endWordPosition="684">osodically enriched models by themselves did not perform significantly better than the baseline PCFG-LA model without enrichment, due to the negative effect that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcriptions has implications for improving downstream applications that rely on accurate POS tags, including sentence boundary detection (Liu et al., 2005), automatic punctuation (Hillard et al., 2006), information extraction from speech, parsing, and syntactic language modeling (Heeman, 1999; Filimonov and Harper, 2009). While there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts, to the best of our knowledge there has been little work on using this type of information for POS tagging. Furthermore, most of the parsing work has involved generative models and rescoring/reranking of hypotheses from the generative models. In this work, we will analyze several factors related to effective </context>
</contexts>
<marker>Liu, Stolcke, Shriberg, Harper, 2005</marker>
<rawString>Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary Harper. 2005. Using conditional random fields for sentence boundary detection in speech. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3930" citStr="Matsuzaki et al., 2005" startWordPosition="574" endWordPosition="577">ally detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics tion in the form of this simplified three class ToBI encoding when parsing spontaneous speech by using a prosodically enriched PCFG model with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) to rescore n-best parses produced by a baseline PCFG-LA model without prosodic enrichment. However, the prosodically enriched models by themselves did not perform significantly better than the baseline PCFG-LA model without enrichment, due to the negative effect that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcrip</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Izhak Shafran</author>
<author>Rebecca Bates</author>
</authors>
<title>Prosody models for conversational speech recognition.</title>
<date>2003</date>
<booktitle>In Plenary Meeting and Symposium on Prosody and Speech Processing.</booktitle>
<contexts>
<context position="2613" citStr="Ostendorf et al., 2003" startWordPosition="371" endWordPosition="374"> simply the sequence of words, which could be exploited to more accurately assign each word in the transcript a part-of-speech (POS) tag. One potentially beneficial type of information is prosody (Cutler et al., 1997). Prosody provides cues for lexical disambiguation, sentence segmentation and classification, phrase structure and attachment, discourse structure, speaker affect, etc. Prosody has been found to play an important role in speech synthesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are r</context>
</contexts>
<marker>Ostendorf, Shafran, Bates, 2003</marker>
<rawString>Mari Ostendorf, Izhak Shafran, and Rebecca Bates. 2003. Prosody models for conversational speech recognition. In Plenary Meeting and Symposium on Prosody and Speech Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="3955" citStr="Petrov and Klein, 2007" startWordPosition="578" endWordPosition="581"> indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics tion in the form of this simplified three class ToBI encoding when parsing spontaneous speech by using a prosodically enriched PCFG model with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) to rescore n-best parses produced by a baseline PCFG-LA model without prosodic enrichment. However, the prosodically enriched models by themselves did not perform significantly better than the baseline PCFG-LA model without enrichment, due to the negative effect that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcriptions has implications fo</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of random latent variable grammars.</title>
<date>2010</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="6540" citStr="Petrov, 2010" startWordPosition="972" endWordPosition="973">serves to establish a baseline and to gauge the difficulty of the task at hand. Our second model, HMM-LA, was the latent variable bigram HMM tagger of Huang et al. (2009), which achieved state-of-the-art tagging performance by introducing latent tags to weaken the stringent Markov independence assumptions that generally hinder tagging performance in generative models. For the third model, we implemented a bidirectional variant of the HMM-LA (HMM-LA-Bidir) that combines evidence from two HMM-LA taggers, one trained left-to-right and the other right-toleft. For decoding, we use a product model (Petrov, 2010). The intuition is that the context information from the left and the right of the current position is complementary for predicting the current tag and thus, the combination should serve to improve performance over the HMM-LA tagger. Since prior work on parsing speech with prosody has relied on generative models, it was necessary to modify equations of the model in order to incorporate the prosodic information, and then perform rescoring in order to achieve gains. However, it is far simpler to directly integrate prosody as features into the model by using a discriminative approach. Hence, we a</context>
<context position="29615" citStr="Petrov (2010)" startWordPosition="4723" endWordPosition="4724">the-art performance on WSJ, we believe the fact that the HMM-LA outperforms it, despite the discriminative model’s more expressive feature set, is indicative of the HMM-LA’s ability to more effectively adapt to novel domains during training. Another challenge for the discriminative models is the need for regularization tuning, requiring additional time and effort to train several models and select the most appropriate parameter each time the domain changes. Whereas for the HMMLA models, although we also train several models, they can be combined into a product model, such as that described by Petrov (2010), in order to further improve performance. Since the prosodic breaks are noisier features than the others incorporated in the discriminative models, it may be useful to set their regularization parameter separately from the rest of the features, however, we have not explored this alternative. Our experiments used human transcriptions of the conversational speech; however, realistically our models would be applied to speech recognition transcripts. In such a case, word error will introduce noise in addition to the prosodic breaks. In future work, we will evaluate the use of break indexes for ta</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of random latent variable grammars. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Yang Liu</author>
<author>Mary Harper</author>
<author>Robin Stewart</author>
<author>Matthew Lease</author>
<author>Matthew Snover</author>
<author>Izhak Shafran</author>
<author>Bonnie Dorr</author>
<author>John Hale</author>
<author>Anna Krasnyanskaya</author>
<author>Lisa Yung</author>
</authors>
<title>Reranking for sentence boundary detection in conversational speech.</title>
<date>2006</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="2990" citStr="Roark et al., 2006" startWordPosition="429" endWordPosition="432">tc. Prosody has been found to play an important role in speech synthesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Met</context>
</contexts>
<marker>Roark, Liu, Harper, Stewart, Lease, Snover, Shafran, Dorr, Hale, Krasnyanskaya, Yung, 2006</marker>
<rawString>Brian Roark, Yang Liu, Mary Harper, Robin Stewart, Matthew Lease, Matthew Snover, Izhak Shafran, Bonnie Dorr, John Hale, Anna Krasnyanskaya, and Lisa Yung. 2006. Reranking for sentence boundary detection in conversational speech. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Silverman</author>
<author>Mary Beckman</author>
<author>John Pitrelli</author>
<author>Mari Ostendorf</author>
<author>Colin Wightman</author>
<author>Patti Price</author>
<author>Janet Pierrehumbert</author>
<author>Julia Hirshberg</author>
</authors>
<title>ToBI: A standard for labeling English prosody.</title>
<date>1992</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="2918" citStr="Silverman et al., 1992" startWordPosition="416" endWordPosition="420">tion, phrase structure and attachment, discourse structure, speaker affect, etc. Prosody has been found to play an important role in speech synthesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate</context>
</contexts>
<marker>Silverman, Beckman, Pitrelli, Ostendorf, Wightman, Price, Pierrehumbert, Hirshberg, 1992</marker>
<rawString>Kim Silverman, Mary Beckman, John Pitrelli, Mari Ostendorf, Colin Wightman, Patti Price, Janet Pierrehumbert, and Julia Hirshberg. 1992. ToBI: A standard for labeling English prosody. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Taylor</author>
<author>Alan W Black</author>
</authors>
<title>Assigning phrase breaks from part-of-speech sequences. Computer Speech and Language.</title>
<date>1998</date>
<contexts>
<context position="2500" citStr="Taylor and Black, 1998" startWordPosition="353" endWordPosition="356">uent and lack punctuation and case information. On the other hand, speech provides additional information, beyond simply the sequence of words, which could be exploited to more accurately assign each word in the transcript a part-of-speech (POS) tag. One potentially beneficial type of information is prosody (Cutler et al., 1997). Prosody provides cues for lexical disambiguation, sentence segmentation and classification, phrase structure and attachment, discourse structure, speaker affect, etc. Prosody has been found to play an important role in speech synthesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2</context>
</contexts>
<marker>Taylor, Black, 1998</marker>
<rawString>Paul Taylor and Alan W. Black. 1998. Assigning phrase breaks from part-of-speech sequences. Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott M Thede</author>
<author>Mary P Harper</author>
</authors>
<title>A secondorder hidden markov model for part-of-speech tagging.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5919" citStr="Thede and Harper, 1999" startWordPosition="871" endWordPosition="874">nversational speech: • discriminative versus generative POS tagging models (Section 2) • prosodic features in the form of simplified ToBI break indexes (Section 4) • type of speech segmentation (Section 5) 2 Models In order to fully evaluate the difficulties inherent in tagging conversational speech, as well as the possible benefits of prosodic information, we conducted experiments with six different POS tagging models. The models can be broadly separated into two classes: generative and discriminative. As the first of our generative models, we used a Hidden Markov Model (HMM) trigram tagger (Thede and Harper, 1999), which serves to establish a baseline and to gauge the difficulty of the task at hand. Our second model, HMM-LA, was the latent variable bigram HMM tagger of Huang et al. (2009), which achieved state-of-the-art tagging performance by introducing latent tags to weaken the stringent Markov independence assumptions that generally hinder tagging performance in generative models. For the third model, we implemented a bidirectional variant of the HMM-LA (HMM-LA-Bidir) that combines evidence from two HMM-LA taggers, one trained left-to-right and the other right-toleft. For decoding, we use a product</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>Scott M. Thede and Mary P. Harper. 1999. A secondorder hidden markov model for part-of-speech tagging. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8680" citStr="Toutanova and Manning, 2000" startWordPosition="1330" endWordPosition="1333">MM √ √ √ 2nd HMM-LA √ √ 1st HMM-LA-Bidir 1st Discriminative Stanford Bidir √ 2nd Stanford Left5 2nd CRF 2nd Table 1: Description of tagging models The objective we need to maximize then becomes : X ⎡ ⎤ L = ⎣X AjFj(tni wn) − log Za(xn) − kA 2uk2 n 2Q2 j where we use a spherical Gaussian prior to prevent overfitting of the model (Chen and Rosenfeld, 1999) and the wide-spread quasi-Newtonian L-BFGS method to optimize the model parameters (Liu and Nocedal, 1989). Decoding is performed with the Viterbi algorithm. We also evaluate state-of-the-art Maximum Entropy taggers: the Stanford Left5 tagger (Toutanova and Manning, 2000) and the Stanford bidirectional tagger (Toutanova et al., 2003), with the former using only left context and the latter bidirectional dependencies. Table 1 summarizes the major differences between the models along several dimensions: (1) generative versus discriminative, (2) directionality of decoding, (3) the presence or absence of latent annotations, (4) the availability of n-best extraction, and (5) the model order. In order to assess the quality of our models, we evaluate them on the section 23 test set of the standard newswire WSJ tagging task after training all models on sections 0-22. R</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="8743" citStr="Toutanova et al., 2003" startWordPosition="1339" endWordPosition="1342"> Bidir √ 2nd Stanford Left5 2nd CRF 2nd Table 1: Description of tagging models The objective we need to maximize then becomes : X ⎡ ⎤ L = ⎣X AjFj(tni wn) − log Za(xn) − kA 2uk2 n 2Q2 j where we use a spherical Gaussian prior to prevent overfitting of the model (Chen and Rosenfeld, 1999) and the wide-spread quasi-Newtonian L-BFGS method to optimize the model parameters (Liu and Nocedal, 1989). Decoding is performed with the Viterbi algorithm. We also evaluate state-of-the-art Maximum Entropy taggers: the Stanford Left5 tagger (Toutanova and Manning, 2000) and the Stanford bidirectional tagger (Toutanova et al., 2003), with the former using only left context and the latter bidirectional dependencies. Table 1 summarizes the major differences between the models along several dimensions: (1) generative versus discriminative, (2) directionality of decoding, (3) the presence or absence of latent annotations, (4) the availability of n-best extraction, and (5) the model order. In order to assess the quality of our models, we evaluate them on the section 23 test set of the standard newswire WSJ tagging task after training all models on sections 0-22. Results appear in Table 2. Clearly, all the models have high acc</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>