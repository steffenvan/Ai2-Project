<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.980381">
Neural Responding Machine for Short-Text Conversation
</title>
<author confidence="0.5539165">
Lifeng Shang, Zhengdong Lu, Hang Li
Noah’s Ark Lab
Huawei Technologies Co. Ltd.
Sha Tin, Hong Kong
</author>
<email confidence="0.995695">
{shang.lifeng,lu.zhengdong,hangli.hl}@huawei.com
</email>
<sectionHeader confidence="0.997357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877277777778">
We propose Neural Responding Ma-
chine (NRM), a neural network-based re-
sponse generator for Short-Text Conver-
sation. NRM takes the general encoder-
decoder framework: it formalizes the gen-
eration of response as a decoding process
based on the latent representation of the in-
put text, while both encoding and decod-
ing are realized with recurrent neural net-
works (RNN). The NRM is trained with
a large amount of one-round conversation
data collected from a microblogging ser-
vice. Empirical study shows that NRM
can generate grammatically correct and
content-wise appropriate responses to over
75% of the input text, outperforming state-
of-the-arts in the same setting, including
retrieval-based and SMT-based models.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996803487804878">
Natural language conversation is one of the
most challenging artificial intelligence problems,
which involves language understanding, reason-
ing, and the utilization of common sense knowl-
edge. Previous works in this direction mainly fo-
cus on either rule-based or learning-based meth-
ods (Williams and Young, 2007; Schatzmann et
al., 2006; Misu et al., 2012; Litman et al., 2000).
These types of methods often rely on manual effort
in designing rules or automatic training of model
with a particular learning algorithm and a small
amount of data, which makes it difficult to develop
an extensible open domain conversation system.
Recently due to the explosive growth of mi-
croblogging services such as Twitter1 and Weibo2,
the amount of conversation data available on the
web has tremendously increased. This makes a
lhttps://twitter.com/.
zhttp://www.weibo.com/.
data-driven approach to attack the conversation
problem (Ji et al., 2014; Ritter et al., 2011) pos-
sible. Instead of multiple rounds of conversation,
the task at hand, referred to as Short-Text Conver-
sation (STC), only considers one round of conver-
sation, in which each round is formed by two short
texts, with the former being an input (referred to as
post) from a user and the latter a response given by
the computer. The research on STC may shed light
on understanding the complicated mechanism of
natural language conversation.
Previous methods for STC fall into two cat-
egories, 1) the retrieval-based method (Ji et al.,
2014), and 2) the statistical machine translation
(SMT) based method (Sordoni et al., 2015; Rit-
ter et al., 2011). The basic idea of retrieval-
based method is to pick a suitable response by
ranking the candidate responses with a linear or
non-linear combination of various matching fea-
tures (e.g. number of shared words). The main
drawbacks of the retrieval-based method are the
following
</bodyText>
<listItem confidence="0.7408155">
• the responses are pre-existing and hard to cus-
tomize for the particular text or requirement
from the task, e.g., style or attitude.
• the use of matching features alone is usu-
</listItem>
<bodyText confidence="0.9591402">
ally not sufficient for distinguishing posi-
tive responses from negative ones, even after
time consuming feature engineering. (e.g., a
penalty due to mismatched named entities is
difficult to incorporate into the model)
The SMT-based method, on the other hand, is
generative. Basically it treats the response genera-
tion as a translation problem, in which the model is
trained on a parallel corpus of post-response pairs.
Despite its generative nature, the method is intrin-
sically unsuitable for response generation, because
the responses are not semantically equivalent to
the posts as in translation. Actually one post can
receive responses with completely different con-
tent, as manifested through the example in the fol-
</bodyText>
<page confidence="0.944762">
1577
</page>
<note confidence="0.92003825">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1577–1586,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
lowing figure:
Post Having my fish sandwich right now
UserA For god’s sake, it is 11 in the morning
UserB Enhhhh... sounds yummy
UserC which restaurant exactly?
</note>
<bodyText confidence="0.999681444444444">
Empirical studies also showed that SMT-based
methods often yield responses with grammatical
errors and in rigid forms, due to the unnecessary
alignment between the “source” post and the “tar-
get” response (Ritter et al., 2011). This rigid-
ity is still a serious problem in the recent work
of (Sordoni et al., 2015), despite its use of neu-
ral network-based generative model as features in
decoding.
</bodyText>
<subsectionHeader confidence="0.962485">
1.1 Overview
</subsectionHeader>
<bodyText confidence="0.999608538461539">
In this paper, we take a probabilistic model to ad-
dress the response generation problem, and pro-
pose employing a neural encoder-decoder for this
task, named Neural Responding Machine (NRM).
The neural encoder-decoder model, as illustrated
in Figure 1, first summarizes the post as a vector
representation, then feeds this representation to a
decoder to generate responses. We further gener-
alize this scheme to allow the post representation
to dynamically change during the generation pro-
cess, following the idea in (Bahdanau et al., 2014)
originally proposed for neural-network-based ma-
chine translation with automatic alignment.
</bodyText>
<figureCaption confidence="0.788229">
Figure 1: The diagram of encoder-decoder frame-
</figureCaption>
<bodyText confidence="0.98940996">
work for automatic response generation.
NRM essentially estimates the likelihood of a
response given a post. Clearly the estimated prob-
ability should be complex enough to represent all
the suitable responses. Similar framework has
been used for machine translation with a remark-
able success (Kalchbrenner and Blunsom, 2013;
Auli et al., 2013; Sutskever et al., 2014; Bah-
danau et al., 2014). Note that in machine trans-
lation, the task is to estimate the probability of a
target language sentence conditioned on the source
language sentence with the same meaning, which
is much easier than the task of STC which we
are considering here. In this paper, we demon-
strate that NRM, when equipped with a reasonable
amount of data, can yield a satisfying estimator of
responses (hence response generator) for STC, de-
spite the difficulty of the task.
Our main contributions are two-folds: 1) we
propose to use an encoder-decoder-based neu-
ral network to generate a response in STC; 2)
we have empirically verified that the proposed
method, when trained with a reasonable amount of
data, can yield performance better than traditional
retrieval-based and translation-based methods.
</bodyText>
<subsectionHeader confidence="0.944863">
1.2 RoadMap
</subsectionHeader>
<bodyText confidence="0.999929333333333">
In the remainder of this paper, we start with in-
troducing the dataset for STC in Section 2. Then
we elaborate on the model of NRM in Section 3,
followed by the details on training in Section 4.
After that, we report the experimental results in
Section 5. In Section 6 we conclude the paper.
</bodyText>
<sectionHeader confidence="0.959715" genericHeader="method">
2 The Dataset for STC
</sectionHeader>
<bodyText confidence="0.9983075">
Our models are trained on a corpus of roughly 4.4
million pairs of conversations from Weibo 3.
</bodyText>
<subsectionHeader confidence="0.999748">
2.1 Conversations on Sina Weibo
</subsectionHeader>
<bodyText confidence="0.999987">
Weibo is a popular Twitter-like microblogging ser-
vice in China, on which a user can post short mes-
sages (referred to as post in the reminder of this
paper) visible to the public or a group of users fol-
lowing her/him. Other users make comment on a
published post, which will be referred to as a re-
sponse. Just like Twitter, Weibo also has the length
limit of 140 Chinese characters on both posts and
responses, making the post-response pair an ideal
surrogate for short-text conversation.
</bodyText>
<subsectionHeader confidence="0.998662">
2.2 Dataset Description
</subsectionHeader>
<bodyText confidence="0.991487375">
To construct this million scale dataset, we first
crawl hundreds of millions of post-response pairs,
and then clean the raw data in a similar way as
suggested in (Wang et al., 2013), including 1) re-
moving trivial responses like “wow”, 2) filtering
out potential advertisements, and 3) removing the
responses after first 30 ones for topic consistency.
Table 1 shows some statistics of the dataset used
</bodyText>
<footnote confidence="0.565289">
3http://www.noahlab.com.hk/topics/ShortTextConversation
</footnote>
<figure confidence="0.836376">
For god&apos;s sake, it is 11 in the morning
Having my fish sandwich right now
Enhhhh... sounds yummy which restaurant exactly?
Decoder
vector
Encoder
</figure>
<page confidence="0.918084">
1578
</page>
<table confidence="0.954073416666667">
Training #posts 219,905
#responses 4,308,211
#pairs 4,435,959
Test Data #test posts 110
Labeled Dataset #posts 225
(retrieval-based)
#responses 6,017
#labeled pairs 6,017
Fine Tuning #posts 2,925
(SMT-based)
#responses 3,000
#pairs 3,000
</table>
<tableCaption confidence="0.72371175">
Table 1: Some statistics of the dataset. Labeled
Dataset and Fine Tuning are used by retrieval-
based method for learning to rank and SMT-based
method for fine tuning, respectively.
</tableCaption>
<bodyText confidence="0.9986186">
in this work. It can be seen that each post have 20
different responses on average. In addition to the
semantic gap between post and its responses, this
is another key difference to a general parallel data
set used for traditional translation.
</bodyText>
<sectionHeader confidence="0.966051" genericHeader="method">
3 Neural Responding Machines for STC
</sectionHeader>
<bodyText confidence="0.999810964285714">
The basic idea of NRM is to build a hidden rep-
resentation of a post, and then generate the re-
sponse based on it, as shown in Figure 2. In
the particular illustration, the encoder converts
the input sequence x = (x1, · · · , xT) into a set
of high-dimensional hidden representations h =
(h1, · · · , hT), which, along with the attention sig-
nal at time t (denoted as αt), are fed to the context-
generator to build the context input to decoder at
time t (denoted as ct). Then ct is linearly trans-
formed by a matrix L (as part of the decoder) into
a stimulus of generating RNN to produce the t-th
word of response (denoted as yt).
In neural translation system, L converts the rep-
resentation in source language to that of target lan-
guage. In NRM, L plays a more difficult role: it
needs to transform the representation of post (or
some part of it) to the rich representation of many
plausible responses. It is a bit surprising that this
can be achieved to a reasonable level with a linear
transformation in the “space of representation”, as
validated in Section 5.3, where we show that one
post can actually invoke many different responses
from NRM.
The role of attention signal is to determine
which part of the hidden representation h should
be emphasized during the generation process. It
should be noted that αt could be fixed over time or
</bodyText>
<figureCaption confidence="0.8691215">
Figure 2: The general framework and dataflow of
the encoder-decoder-based NRM.
</figureCaption>
<bodyText confidence="0.938247357142857">
changes dynamically during the generation of re-
sponse sequence y. In the dynamic settings, αt
can be function of historically generated subse-
quence (y1, · · · , yt−1), input sequence x or their
latent representations, more details will be shown
later in Section 3.2.
We use Recurrent Neural Network (RNN) for
both encoder and decoder, for its natural ability
to summarize and generate word sequence of ar-
bitrary lengths (Mikolov et al., 2010; Sutskever et
al., 2014; Cho et al., 2014).
͙͘
͙͘
͙͘
</bodyText>
<figureCaption confidence="0.759004">
Figure 3: The graphical model of RNN decoder.
</figureCaption>
<bodyText confidence="0.947519666666667">
The dashed lines denote the variables related to the
function g(·), and the solid lines denote the vari-
ables related to the function f(·).
</bodyText>
<subsectionHeader confidence="0.999506">
3.1 The Computation in Decoder
</subsectionHeader>
<bodyText confidence="0.9997506">
Figure 3 gives the graphical model of the de-
coder, which is essentially a standard RNN lan-
guage model except conditioned on the context in-
put c. The generation probability of the t-th word
is calculated by
</bodyText>
<figure confidence="0.9342538">
Xyt|yt−1, ··· , y1, x) = g(yt−1, st, ct),
Encoder
Decoder
Attention Signal
Context Generator
</figure>
<page confidence="0.986184">
1579
</page>
<bodyText confidence="0.999982333333333">
where yt is a one-hot word representation, g(·) is
a softmax activation function, and st is the hidden
state of decoder at time t calculated by
</bodyText>
<equation confidence="0.801597">
st = f(yt−1, st−1, ct),
</equation>
<bodyText confidence="0.999906105263158">
and f(·) is a non-linear activation function and
the transformation L is often assigned as pa-
rameters of f(·). Here f(·) can be a logistic
function, the sophisticated long short-term mem-
ory (LSTM) unit (Hochreiter and Schmidhuber,
1997), or the recently proposed gated recurrent
unit (GRU) (Chung et al., 2014; Cho et al., 2014).
Compared to “ungated” logistic function, LSTM
and GRU are specially designed for its long term
memory: it can store information over extended
time steps without too much decay. We use GRU
in this work, since it performs comparably to
LSTM on squence modeling (Chung et al., 2014;
Greff et al., 2015), but has less parameters and eas-
ier to train.
We adopt the notation of GRU from (Bahdanau
et al., 2014), the hidden state st at time t is a linear
combination of its previous hidden state st−1 and
a new candidate state ˆst:
</bodyText>
<equation confidence="0.794325">
st = (1 − zt) ◦ st−1 + zt ◦ ˆst,
</equation>
<bodyText confidence="0.78263525">
where ◦ is point-wise multiplication, zt is the up-
date gate calculated by
zt = a (Wze(yt−1) + Uzst−1 + Lzct) , (1)
and ˆst is calculated by
</bodyText>
<equation confidence="0.956786">
ˆst=tanh (We(yt−1) + U(rt ◦ st−1) + Lct) , (2)
</equation>
<bodyText confidence="0.985595">
where the reset gate rt is calculated by
</bodyText>
<equation confidence="0.828597">
rt = a (Wre(yt−1) + Urst−1 + Lrct) . (3)
</equation>
<bodyText confidence="0.999913285714286">
In Equation (1)-(2), and (3), e(yt−1) is word em-
bedding of the word yt−1, L = {L, Lz, Lr} spec-
ifies the transformations to convert a hidden rep-
resentation from encoder to that of decoder. In
the STC task, L should have the ability to trans-
form one post (or its segments) to multiple differ-
ent words of appropriate responses.
</bodyText>
<subsectionHeader confidence="0.999749">
3.2 The Computation in Encoder
</subsectionHeader>
<bodyText confidence="0.999834190476191">
We consider three types of encoding schemes,
namely 1) the global scheme, 2) the local scheme,
and the hybrid scheme which combines 1) and 2).
Global Scheme: Figure 4 shows the graphical
model of the RNN-encoder and related context
generator for a global encoding scheme. The
hidden state at time t is calculated by ht =
f(xt, ht−1) (i.e. still GRU unit), and with a trivial
context generation operation, we essentially use
the final hidden state hT as the global represen-
tation of the sentence. The same strategy has been
taken in (Cho et al., 2014) and (Sutskever et al.,
2014) for building the intermediate representation
for machine translation. This scheme however has
its drawbacks: a vectorial summarization of the
entire post is often hard to obtain and may lose im-
portant details for response generation, especially
when the dimension of the hidden state is not big
enough4. In the reminder of this paper, a NRM
with this global encoding scheme is referred to as
NRM-glo.
</bodyText>
<subsectionHeader confidence="0.582802">
Context Generator
</subsectionHeader>
<bodyText confidence="0.937634176470588">
Figure 4: The graphical model of the encoder in
NRM-glo, where the last hidden state is used as
the context vector ct = hT.
Local Scheme: Recently, Bahdanau et al.
(2014) and Graves (2013) introduced an attention
mechanism that allows the decoder to dynamically
select and linearly combine different parts of the
input sequence ct = ET j�1 atjhj, where weight-
ing factors atj determine which part should be se-
lected to generate the new word yt, which in turn
is a function of hidden states atj = q(hj, st−1),
as pictorially shown in Figure 5. Basically, the at-
tention mechanism atj models the alignment be-
tween the inputs around position j and the output
at position t, so it can be viewed as a local match-
ing model. This local scheme is devised in (Bah-
danau et al., 2014) for automatic alignment be-
</bodyText>
<footnote confidence="0.771948">
4Sutskever et al. (2014) has to use 4, 000 dimension for
satisfying performance on machine translation, while (Cho et
al., 2014) with a smaller dimension perform poorly on trans-
lating an entire sentence.
</footnote>
<page confidence="0.6197695">
͙͘
1580
</page>
<bodyText confidence="0.999233">
tween the source sentence and the partial target
sentence in machine translation. This scheme en-
joys the advantage of adaptively focusing on some
important words of the input text according to the
generated words of response. A NRM with this
local encoding scheme is referred to as NRM-loc.
</bodyText>
<figureCaption confidence="0.545437333333333">
Figure 5: The graphical model of the encoder in
NRM-loc, where the weighted sum of hidden sates
is used as the context vector ct = E 1 αtjhj.
</figureCaption>
<subsectionHeader confidence="0.998077">
3.3 Extensions: Local and Global Model
</subsectionHeader>
<bodyText confidence="0.999952891891892">
In the task of STC, NRM-glo has the summariza-
tion of the entire post, while NRM-loc can adap-
tively select the important words in post for vari-
ous suitable responses. Since post-response pairs
in STC are not strictly parallel and a word in differ-
ent context can have different meanings, we con-
jecture that the global representation in NRM-glo
may provide useful context for extracting the local
context, therefore complementary to the scheme
in NRM-loc. It is therefore a natural extension
to combine the two models by concatenating their
encoded hidden states to form an extended hid-
den representation for each time stamp, as illus-
trated in Figure 6. We can see the summarization
h9� is incorporated into ct and αtj to provide a
global context for local matching. With this hy-
brid method, we hope both the local and global in-
formation can be introduced into the generation of
response. The model with this context generation
mechanism is denoted as NRM-hyb.
It should be noticed that the context generator
in NRM-hyb will evoke different encoding mecha-
nisms in the global encoder and the local encoder,
although they will be combined later in forming
a unified representation. More specifically, the
last hidden state of NRM-glo plays a role differ-
ent from that of the last state of NRM-loc, since
it has the responsibility to encode the entire input
sentence. This role of NRM-glo, however, tends
to be not adequately emphasized in training the
hybrid encoder when the parameters of the two
encoding RNNs are learned jointly from scratch.
For this we use the following trick: we first ini-
tialize NRM-hyb with the parameters of NRM-loc
and NRM-glo trained separately, then fine tune the
parameters in encoder along with training the pa-
rameters of decoder.
</bodyText>
<figure confidence="0.416869">
local encoder
</figure>
<figureCaption confidence="0.8383135">
Figure 6: The graphical model for the encoder
in NRM-hyb, while context generator function is
</figureCaption>
<equation confidence="0.372731">
ct = E 1 αtj[hlj; h9 � ], here [hlj; h9�] denotes the
concatenation of vectors hlj and h9�
</equation>
<bodyText confidence="0.999802333333333">
To learn the parameters of the model, we max-
imize the likelihood of observing the original re-
sponse conditioned on the post in the training set.
For a new post, NRMs generate their responses by
using a left-to-right beam search with beam size =
10.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999969">
We evaluate three different settings of NRM de-
scribed in Section 3, namely NRM-glo, NRM-
loc, and NRM-hyb, and compare them to retrieval-
based and SMT-based methods.
</bodyText>
<subsectionHeader confidence="0.992318">
4.1 Implementation Details
</subsectionHeader>
<bodyText confidence="0.999942333333333">
We use Stanford Chinese word segmenter 5 to split
the posts and responses into sequences of words.
Although both posts and responses are written in
the same language, the distributions on words for
the two are different: the number of unique words
in post text is 125,237, and that of response text is
679,958. We therefore construct two separate vo-
cabularies for posts and responses by using 40,000
most frequent words on each side, covering 97.8%
</bodyText>
<footnote confidence="0.63537">
5http://nlp.stanford.edu/software/segmenter.shtml
</footnote>
<figure confidence="0.950073">
Attention Signal
Context Generator
͙͘
global encoder
͙͘
͙͘
Attention Signal
Context Generator
</figure>
<page confidence="0.970374">
1581
</page>
<bodyText confidence="0.999912363636364">
usage of words for post and 96.2% for response
respectively. All the remaining words are replaced
by a special token “UNK”. The dimensions of the
hidden states of encoder and decoder are both
1,000. Model parameters are initialized by ran-
domly sampling from a uniform distribution be-
tween -0.1 and 0.1. All our models were trained on
a NVIDIA Tesla K40 GPU using stochastic gra-
dient descent (SGD) algorithm with mini-batch.
The training stage of each model took about two
weeks.
</bodyText>
<subsectionHeader confidence="0.947589">
4.2 Competitor Models
</subsectionHeader>
<bodyText confidence="0.999953083333334">
Retrieval-based: with retrieval-based models,
for any given post p*, the response r* is retrieved
from a big post-response pairs (p, r) repository.
Such models rely on three key components: a big
repository, sets of feature functions -bi(p*, (p, r)),
and a machine learning model to combine these
features. In this work, the whole 4.4 million
Weibo pairs are used as the repository, 14 fea-
tures, ranging from simple cosine similarity to
some deep matching models (Ji et al., 2014) are
used to determine the suitability of a post to a
given post p* through the following linear model
</bodyText>
<equation confidence="0.9947985">
�score(p*, (p, r)) _ wi-bi(p*, (p, r)). (4)
i
</equation>
<bodyText confidence="0.982825741935484">
Following the ranking strategy in (Ji et al., 2014),
we pick 225 posts and about 30 retrieved re-
sponses for each of them given by a baseline re-
triever6 from the 4.4M repository, and manually
label them to obtain labeled 6,017 post-response
pairs. We use ranking SVM model (Joachims,
2006) for the parameters wi based on the labeled
dataset. In comparison to NRM, only the top one
response is considered in the evaluation process.
SMT-based: In SMT-based models, the post-
response pairs are directly used as parallel data
for training a translation model. We use the most
widely used open-source phrase-based translation
model-Moses (Koehn et al., 2007). Another par-
allel data consisting of 3000 post-response pairs is
used to tune the system. In (Ritter et al., 2011),
the authors used a modified SMT model to obtain
the “Response” of Twitter “Stimulus”. The main
modification is in replacing the standard GIZA++
word alignment model (Och and Ney, 2003) with a
new phrase-pair selection method, in which all the
6we use the default similarity function of Lucene 7
possible phrase-pairs in the training data are con-
sidered and their associated probabilities are es-
timated by the Fisher’s Exact Test, which yields
performance slightly better than default setting8.
Compared to retrieval-based methods, the gener-
ated responses by SMT-based methods often have
fluency or even grammatical problems. In this
work, we choose the Moses with default settings
as our SMT model.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999741461538462">
Automatic evaluation of response generation is
still an open problem. The widely accepted evalu-
ation methods in translation (e.g. BLEU score (Pa-
pineni et al., 2002)) do not apply, since the range
of the suitable responses is so large that it is prac-
tically impossible to give reference with adequate
coverage. It is also not reasonable to evaluate with
Perplexity, a generally used measurement in statis-
tical language modeling, because the naturalness
of response and the relatedness to the post can not
be well evaluated. We therefore resort to human
judgement, similar to that taken in (Ritter et al.,
2011) but with an important difference.
</bodyText>
<subsectionHeader confidence="0.533908">
5.1 Evaluation Methods
</subsectionHeader>
<bodyText confidence="0.999882545454545">
We adopt human annotation to compare the per-
formance of different models. Five labelers with
at least three-year experience of Sina Weibo are in-
vited to do human evaluation. Responses obtained
from the five evaluated models are pooled and ran-
domly permuted for each labeler. The labelers are
instructed to imagine that they were the authors
of the original posts and judge whether a response
(generated or retrieved) is appropriate and natural
to a input post. Three levels are assigned to a re-
sponse with scores from 0 to 2:
</bodyText>
<listItem confidence="0.976846333333333">
• Suitable (+2): the response is evidently an ap-
propriate and natural response to the post;
• Neutral (+1): the response can be a suitable
response in a specific scenario;
• Unsuitable (0): it is hard or impossible to find
a scenario where response is suitable.
</listItem>
<bodyText confidence="0.929154833333333">
To make the annotation task operable, the suit-
ability of generated responses is judged from the
following five criteria:
(a) Grammar and Fluency: Responses should
be natural language and free of any fluency or
grammatical errors;
</bodyText>
<footnote confidence="0.9925895">
8Reported results showed that the new model outper-
formed the baseline SMT model 57.7% of the time.
</footnote>
<page confidence="0.962763">
1582
</page>
<table confidence="0.991077384615385">
Post ᝅཝ࡟⾷॰䠂㘷ᱥޡѠӰ...འ཮ᕖҼ Related Criterion Labels
There are always 8 players at the Italian restricted area. Unbelievable!
Response1ᡇᱥᝅཝ࡟䱕Ⲻ⨹䘭θㅿᖻ∊䎑 Logic Consistency Unsuitable
ᔶခȾ
I am a big fan of the Italy team, waiting for the football game to start
Response2 ᝅཝ࡟Ⲻ伕⢟འ㗄઩Ҽ Semantic Relevance Unsuitable
Italian food is absolutely delicious.
Response3 འ཮ᕖҼ੝! Generality Neutral
Unbelievable!
Response4 ૾૾૾ԃ❬ᱥ0φ0 Ⱦ䘎⋗ⵁࡦ䘑⨹Ⱦ Scenario Dependence Neutral
Ha!Ha!Ha! it is still 0:0, no goal so far.
Response5 䘏↙ᱥᝅཝ࡟ᕅ䱨ᆾ䏩⨹Ⱦ 22 Suitable
This is exactly the Italian defending style football game.
</table>
<figureCaption confidence="0.9071756">
Figure 7: An example post and its five candidate responses with human annotation. The content of the
post implies that the football match is already started, while the author of Response1 is still waiting for
the match to start. Response2 talks about the food of Italy. Response3 is a widely used response, but it
is suitable to this post. Response4 states that the current score is still 0:0, it is a suitable response only in
this specific scenario.
</figureCaption>
<table confidence="0.9994705">
Models Mean Score Suitable (+2) Neutral (+1) Unsuitable (0) Agreement
NRM-glo 0.969 34.0% 28.9% 37.1% 0.397
NRM-loc 1.065 36.0% 34.5% 29.5% 0.247
NRM-hyb 1.156 39.3% 37.1% 23.6% 0.309
Rtr.-based 0.984 29.8% 38.7% 31.5% 0.346
SMT-based 0.313 5.6 % 20.0% 74.4% 0.448
</table>
<tableCaption confidence="0.9517425">
Table 2: The results of evaluated methods. Mean score is the average value of annotated scores over all
annotations. (Rtr.-based means the retrieval-based method)
</tableCaption>
<table confidence="0.987953823529412">
ID Post NRM-glo NRM-loc NRM-hyb Rtr.-based
1 ∅ᒪݹᰜ䜳ᶛ儎✝Ⱦ ཐՇᚥθ໔ᕰރ⯡࣑έ ∅ᒪݹᰜ䜳ৱॱ䲘ⵁ ⾓ᰟᰛᓭགྷθ䓡։ڛ 儎✝...
High fever attacks me Rest more to boost ⯻ ᓭέ High fever...
every New Year&apos;s Day your immune system. Go to see a doctor Get well soon and stay
every New Year&apos;s Day healthy!
2 ৾㘷Ҽж኷θ᝕䉘ᡇԢ ⭕ᰛᘡ҆έ≮䘒᭥ᤷ ⭕ᰛᘡ҆έ≮䘒ᒪ䖱 㘷Ҽ㘷Ҽθ኷ᴾу侬 ᡇਠᱥ㘷Ҽж኷
㓺θᡇᖾ儎ު ֖έ ┸Ӥέ Ӱ஀έ I just gain only one
I gain one more year. Happy birthday! Will Happy birthday! Wish Getting old now. Time more year.
Grateful to my group, so stand by you forever! you always stay young has no mercy.
happy and pretty!
3 ਹ㍖ㅢжཟθඐᤷօθ ᗻ亱ᶛ᭥ᤷέࣖ⋯έ ඐᤷօθඐᤷсৱȾ ∅ཟ䜳ᱥ㍖伕ѱѿ㘻 ඐᤷօέ
ፊᵫ᣿Ⱦ I come to offer my Hold on, keep it up. I have been being a Hold on!
First day of being a support. Keep it up! vegetarian everyday
vegetarian. Hold on,
CuiDuoLa
4 䘏〃ཟ≊ⵕᓊ䈛ᑜѠᐻ 䘏ᱥଠݵ஀ϋⵕ㗄έ 䱩ݿ᱄აθᗹ᛻㡈⭻Ⱦ 䘏〃ཟ≊ⵕуᱥⴌⲺȾ ᮽኧ஀ ࠰ৱ䑅䶈ሱ
䬻࠰ৱ䑅䶈䎅㣧 Where is it? It is so Such a nice sunny day! It is indeed a very nice ᢴ⚫᝕ ૾૾
</table>
<bodyText confidence="0.60417975">
We should go out with beautiful! I am in a great mood. weather. WenShan, let&apos;s go out
some cute guys to enjoy to get some
a great outing in such a inspiration. Ha! Ha!
nice weather.
</bodyText>
<figureCaption confidence="0.9313165">
Figure 8: Some responses generated by different models (originally in Chinese with their literal English
translation), where the words in boldfaces are entity names.
</figureCaption>
<page confidence="0.94125">
1583
</page>
<listItem confidence="0.972360666666667">
(b) Logic Consistency: Responses should be log-
ically consistent with the test post;
(c) Semantic Relevance: Responses should be
semantically relevant to the test post;
(d) Scenario Dependence: Responses can de-
pend on a specific scenario but should not con-
tradict the first three criteria;
(e) Generality: Responses can be general but
should not contradict the first three criteria;
</listItem>
<bodyText confidence="0.999476818181818">
If any of the first three criteria (a), (b), and (c)
is contradicted, the generated response should be
labeled as “Unsuitable”. The responses that are
general or suitable to post in a specific scenario
should be labeled as “Neutral”. Figure 7 shows
an example of the labeling results of a post and its
responses. The first two responses are labeled as
“Unsuitable” because of the logic consistency and
semantic relevance errors. Response4 depends on
the scenario (i.e., the current score is 0:0), and is
therefore annotated as “Neutral”.
</bodyText>
<table confidence="0.998371545454545">
Model A Model B p value
Average
rankings
NRM-loc NRM-glo (1.463, 1.537) 2.01%
NRM-hyb NRM-glo (1.434, 1.566) 0.01%
NRM-hyb NRM-loc (1.465, 1.535) 3.09%
Rtr.-based NRM-glo (1.512, 1.488) 48.1%
Rtr.-based NRM-loc (1.533, 1.467) 6.20%
Rtr.-based NRM-hyb (1.552, 1.448) 0.32%
SMT NRM-hyb (1.785, 1.215) 0.00 %
SMT Rtr.-based (1.738, 1.262) 0.00 %
</table>
<tableCaption confidence="0.989501">
Table 3: p-values and average rankings of Fried-
</tableCaption>
<bodyText confidence="0.779462">
man test for pairwise model comparison. (Rtr.-
based means the retrieval-based method)
</bodyText>
<subsectionHeader confidence="0.925438">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9999009375">
Our test set consists of 110 posts that do not ap-
pear in the training set, with length between 6 to
22 Chinese words and 12.5 words on average. The
experimental results based on human annotation
are summarized in Table 2, consisting of the ra-
tio of three categories and the agreement among
the five labelers for each model. The agreement is
evaluated by Fleiss’ kappa (Fleiss, 1971), as a sta-
tistical measure of inter-rater consistency. Except
the SMT-based model, the value of agreement is
in a range from 0.2 to 0.4 for all the other mod-
els, which should be interpreted as “Fair agree-
ment”. The SMT-based model has a relatively
higher kappa value 0.448, which is larger than 0.4
and considered as “Moderate agreement”, since
the responses generated by the SMT often have the
fluency and grammatical errors, making it easy to
reach an agreement on such unsuitable cases.
From Table 2, we can see the SMT method per-
forms significantly worse than the retrieval-based
and NRM models and 74.4% of the generated re-
sponses were labeled as unsuitable mainly due to
fluency and relevance errors. This observation
confirms with our intuition that the STC dataset,
with one post potentially corresponding to many
responses, can not be simply taken as parallel cor-
pus in a SMT model. Surprisingly, more than 60%
of responses generated by all the three NRM are
labeled as “Suitable” or “Neutral”, which means
that most generated responses are fluent and se-
mantically relevant to post. Among all the NRM
variants
</bodyText>
<listItem confidence="0.9014611">
• NRM-loc outperforms NRM-glo, suggesting
that a dynamically generated context might
be more effective than a “static” fixed-length
vector for the entire post, which is consistent
with the observation made in (Bahdanau et al.,
2014) for machine translation;
• NRM-hyp outperforms NRM-loc and NRM-
glo, suggesting that a global representation of
post is complementary to dynamically gener-
ated local context.
</listItem>
<bodyText confidence="0.999994318181818">
The retrieval-based model has the similar mean
score as NRM-glo, and its ratio on neutral cases
outperforms all the other methods. This is be-
cause 1) the responses retrieved by retrieval-based
method are actually written by human, so they
do not suffer from grammatical and fluency prob-
lems, and 2) the combination of various feature
functions potentially makes sure the picked re-
sponses are semantically relevant to test posts.
However the picked responses are not customized
for new test posts, so the ratio of suitable cases is
lower than the three neural generation models.
To test statistical significance, we use the
Friedman test (Howell, 2010), which is a non-
parametric test on the differences of several re-
lated samples, based on ranking. Table 3 shows
the average rankings over all annotations and the
corresponding p-values for comparisons between
different pairs of methods. The comparison be-
tween retrieval-based and NRM-glo is not signif-
icant and their difference in ranking is tiny. This
indicates that the retrieval-based method is com-
</bodyText>
<page confidence="0.982574">
1584
</page>
<bodyText confidence="0.999874714285714">
parable to the NRM-glo method. The NRM-hyb
outperforms all the other methods, and the differ-
ence is statistically significant (p &lt; 0.05). The
difference between NRM-loc and retrieval-based
method is marginal (p = 0.062). SMT is signif-
icantly worse than retrieval-based and NRM-hyb
methods.
</bodyText>
<subsectionHeader confidence="0.999671">
5.3 Case Study
</subsectionHeader>
<bodyText confidence="0.999531238095238">
Figure 8 shows some example responses gener-
ated by our NRMs (only the one with biggest
likelihood is given) and the comparable retrieval-
based model. It is intriguing to notice that three
NRM variants give suitable but quite distinct re-
sponses, with different perspectives and choices
of words. This, as we conjecture, is caused by
both the architecture variations among models as
well as the variations from random effects like
the initialization of parameters. Another interest-
ing observation is on the fourth example, where
the retrieval-based method returns a response with
the mismatched entity name “WenShan”, which
is actually a quite common problem for retrieval-
based model, where the inconsistency details (e.g.,
dates, named entities), which often render the re-
sponse unsuitable, cannot be adequately consid-
ered in the matching function employed in retriev-
ing the responses. In contrast, we observe that
NRMs tend to make general response and barely
generate those details.
</bodyText>
<table confidence="0.793461066666667">
Post ਹ㍖ㅢжཟθඐᤷօθፊᵫ᣿Ⱦ
First day of being a vegetarian. Hold on, CuiDuoLa
R1 ඐᤷօθඐᤷ㍖伕έ
Hold on, Keep up your vegetarian diet.
R2 ᜩ㾷ڛᓭ੍ϋᜩ㾷䮵ስ੍ϋ䛙ቧީ⌞ᡇⲺᗤঐ੝Ⱦ
Would you like to be healthy? Would you like to live
long? If so, follow me on Weibo.
R3 䘏ቧᱥՖ䈪ѣⲺਹ㍖੍ϋ
This is the so-called vegetarian diet ?
R4 ਹ侧Ҽᢃᴿ࣑≊ࠅ㛛
Eat up so that you will have enough energy to go for
a diet!
R5 䓡։ᱥ䶟ળⲺᵢ䫧θу㾷⵶ᙛέ
Health is the essence for winning your future battles.
No rush!
</table>
<figureCaption confidence="0.8115795">
Figure 9: Multiple responses generated by the
NRM-hyb.
</figureCaption>
<bodyText confidence="0.999974541666667">
We also use the NRM-hyb as an example to in-
vestigate the ability of NRM to generate multi-
ple responses. Figure 9 lists 5 responses to the
same post, which are gotten with beam search with
beam size = 500, among which we keep only the
best one (biggest likelihood) for each first word.
It can be seen that the responses are fluent, rele-
vant to the post, and still vastly different from each
other, validating our initial conjecture that NRM,
when fueled with large and rich training corpus,
could work as a generator that can cover a lot of
modes in its density estimation.
It is worth mentioning that automatic evaluation
metrics, such as BLEU (Papineni et al., 2002) as
adopted by machine translation and recently SMT-
based responding models (Sordoni et al., 2015), do
not work very well on this task, especially when
the reference responses are few. Our results show
that the average BLEU values are less than 2 for
all models discussed in this paper, including SMT-
based ones, on instances with single reference.
Probably more importantly, the ranking given by
the BLEU value diverges greatly from the human
judgment of response quality.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999956384615385">
In this paper, we explored using encoder-decoder-
based neural network system, with coined name
Neural Responding Machine, to generate re-
sponses to a post. Empirical studies confirm that
the newly proposed NRMs, especially the hybrid
encoding scheme, can outperform state-of-the-art
retrieval-based and SMT-based methods. Our pre-
liminary study also shows that NRM can generate
multiple responses with great variety to a given
post. In future work, we would consider adding
the intention (or sentiment) of users as an external
signal of decoder to generate responses with spe-
cific goals.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999669">
The authors would like to thank Tao Cai for tech-
nical support. This work is supported in part by
China National 973 project 2014CB340301.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999342">
Michael Auli, Michel Galley, Chris Quirk, and Ge-
offrey Zweig. 2013. Joint language and transla-
tion modeling with recurrent neural networks. In
EMNLP, pages 1044–1054.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.
</reference>
<page confidence="0.803518">
1585
</page>
<reference confidence="0.999311144444444">
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
Alex Graves. 2013. Generating sequences with recur-
rent neural networks. preprint arXiv:1308.0850.
Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık,
Bas R. Steunebrink, and J¨urgen Schmidhuber.
2015. LSTM: A search space odyssey. CoRR,
abs/1503.04069.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
David C. Howell. 2010. Fundamental Statistics for the
Behavioral Sciences. PSY 200 (300) Quantitative
Methods in Psychology Series. Wadsworth Cengage
Learning.
Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An
information retrieval approach to short text conver-
sation. arXiv preprint arXiv:1408.6988.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In SIGKDD, pages 217–226. ACM.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In EMNLP, pages
1700–1709.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. ACL.
Diane Litman, Satinder Singh, Michael Kearns, and
Marilyn Walker. 2000. Njfun: a reinforcement
learning spoken dialogue system. In Proceedings
of the 2000 ANLP/NAACL Workshop on Conversa-
tional systems, pages 17–20. ACL.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, pages 1045–1048.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual mu-
seum guides. In SIGDIAL, pages 84–93. ACL.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
EMNLP, pages 583–593. Association for Computa-
tional Linguistics.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of di-
alogue management strategies. The knowledge en-
gineering review, 21(02):97–126.
Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun
Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural
network approach to context-sensitive generation of
conversational responses. Conference of the North
American Chapter of the Association for Computa-
tional Linguistics Human Language Technologies
(NAACL-HLT 2015), June.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS, pages 3104–3112.
Hao Wang, Zhengdong Lu, Hang Li, and Enhong
Chen. 2013. A dataset for research on short-text
conversations. In EMNLP, pages 935–945.
Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech &amp; Language,
21(2):393–422.
</reference>
<page confidence="0.992901">
1586
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.329618">
<title confidence="0.652445">Neural Responding Machine for Short-Text Conversation Lifeng Shang, Zhengdong Lu, Hang Noah’s Ark Huawei Technologies Co.</title>
<author confidence="0.736226">Sha Tin</author>
<author confidence="0.736226">Hong</author>
<abstract confidence="0.999005894736842">We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoderdecoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming stateof-the-arts in the same setting, including retrieval-based and SMT-based models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1044--1054</pages>
<contexts>
<context position="5559" citStr="Auli et al., 2013" startWordPosition="853" endWordPosition="856">scheme to allow the post representation to dynamically change during the generation process, following the idea in (Bahdanau et al., 2014) originally proposed for neural-network-based machine translation with automatic alignment. Figure 1: The diagram of encoder-decoder framework for automatic response generation. NRM essentially estimates the likelihood of a response given a post. Clearly the estimated probability should be complex enough to represent all the suitable responses. Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Note that in machine translation, the task is to estimate the probability of a target language sentence conditioned on the source language sentence with the same meaning, which is much easier than the task of STC which we are considering here. In this paper, we demonstrate that NRM, when equipped with a reasonable amount of data, can yield a satisfying estimator of responses (hence response generator) for STC, despite the difficulty of the task. Our main contributions are two-folds: 1) we propose to use an encoder-decoder-based neural network t</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In EMNLP, pages 1044–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</title>
<date>2014</date>
<contexts>
<context position="5080" citStr="Bahdanau et al., 2014" startWordPosition="784" endWordPosition="787"> use of neural network-based generative model as features in decoding. 1.1 Overview In this paper, we take a probabilistic model to address the response generation problem, and propose employing a neural encoder-decoder for this task, named Neural Responding Machine (NRM). The neural encoder-decoder model, as illustrated in Figure 1, first summarizes the post as a vector representation, then feeds this representation to a decoder to generate responses. We further generalize this scheme to allow the post representation to dynamically change during the generation process, following the idea in (Bahdanau et al., 2014) originally proposed for neural-network-based machine translation with automatic alignment. Figure 1: The diagram of encoder-decoder framework for automatic response generation. NRM essentially estimates the likelihood of a response given a post. Clearly the estimated probability should be complex enough to represent all the suitable responses. Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Note that in machine translation, the task is to estimate the probabili</context>
<context position="11991" citStr="Bahdanau et al., 2014" startWordPosition="1950" endWordPosition="1953">Here f(·) can be a logistic function, the sophisticated long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997), or the recently proposed gated recurrent unit (GRU) (Chung et al., 2014; Cho et al., 2014). Compared to “ungated” logistic function, LSTM and GRU are specially designed for its long term memory: it can store information over extended time steps without too much decay. We use GRU in this work, since it performs comparably to LSTM on squence modeling (Chung et al., 2014; Greff et al., 2015), but has less parameters and easier to train. We adopt the notation of GRU from (Bahdanau et al., 2014), the hidden state st at time t is a linear combination of its previous hidden state st−1 and a new candidate state ˆst: st = (1 − zt) ◦ st−1 + zt ◦ ˆst, where ◦ is point-wise multiplication, zt is the update gate calculated by zt = a (Wze(yt−1) + Uzst−1 + Lzct) , (1) and ˆst is calculated by ˆst=tanh (We(yt−1) + U(rt ◦ st−1) + Lct) , (2) where the reset gate rt is calculated by rt = a (Wre(yt−1) + Urst−1 + Lrct) . (3) In Equation (1)-(2), and (3), e(yt−1) is word embedding of the word yt−1, L = {L, Lz, Lr} specifies the transformations to convert a hidden representation from encoder to that o</context>
<context position="13939" citStr="Bahdanau et al. (2014)" startWordPosition="2304" endWordPosition="2307">14) and (Sutskever et al., 2014) for building the intermediate representation for machine translation. This scheme however has its drawbacks: a vectorial summarization of the entire post is often hard to obtain and may lose important details for response generation, especially when the dimension of the hidden state is not big enough4. In the reminder of this paper, a NRM with this global encoding scheme is referred to as NRM-glo. Context Generator Figure 4: The graphical model of the encoder in NRM-glo, where the last hidden state is used as the context vector ct = hT. Local Scheme: Recently, Bahdanau et al. (2014) and Graves (2013) introduced an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence ct = ET j�1 atjhj, where weighting factors atj determine which part should be selected to generate the new word yt, which in turn is a function of hidden states atj = q(hj, st−1), as pictorially shown in Figure 5. Basically, the attention mechanism atj models the alignment between the inputs around position j and the output at position t, so it can be viewed as a local matching model. This local scheme is devised in (Bahdanau et al., 2014</context>
<context position="28533" citStr="Bahdanau et al., 2014" startWordPosition="4719" endWordPosition="4722">rms with our intuition that the STC dataset, with one post potentially corresponding to many responses, can not be simply taken as parallel corpus in a SMT model. Surprisingly, more than 60% of responses generated by all the three NRM are labeled as “Suitable” or “Neutral”, which means that most generated responses are fluent and semantically relevant to post. Among all the NRM variants • NRM-loc outperforms NRM-glo, suggesting that a dynamically generated context might be more effective than a “static” fixed-length vector for the entire post, which is consistent with the observation made in (Bahdanau et al., 2014) for machine translation; • NRM-hyp outperforms NRM-loc and NRMglo, suggesting that a global representation of post is complementary to dynamically generated local context. The retrieval-based model has the similar mean score as NRM-glo, and its ratio on neutral cases outperforms all the other methods. This is because 1) the responses retrieved by retrieval-based method are actually written by human, so they do not suffer from grammatical and fluency problems, and 2) the combination of various feature functions potentially makes sure the picked responses are semantically relevant to test posts</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyoung Chung</author>
<author>Caglar Gulcehre</author>
<author>KyungHyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</title>
<date>2014</date>
<contexts>
<context position="11567" citStr="Chung et al., 2014" startWordPosition="1875" endWordPosition="1878">bability of the t-th word is calculated by Xyt|yt−1, ··· , y1, x) = g(yt−1, st, ct), Encoder Decoder Attention Signal Context Generator 1579 where yt is a one-hot word representation, g(·) is a softmax activation function, and st is the hidden state of decoder at time t calculated by st = f(yt−1, st−1, ct), and f(·) is a non-linear activation function and the transformation L is often assigned as parameters of f(·). Here f(·) can be a logistic function, the sophisticated long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997), or the recently proposed gated recurrent unit (GRU) (Chung et al., 2014; Cho et al., 2014). Compared to “ungated” logistic function, LSTM and GRU are specially designed for its long term memory: it can store information over extended time steps without too much decay. We use GRU in this work, since it performs comparably to LSTM on squence modeling (Chung et al., 2014; Greff et al., 2015), but has less parameters and easier to train. We adopt the notation of GRU from (Bahdanau et al., 2014), the hidden state st at time t is a linear combination of its previous hidden state st−1 and a new candidate state ˆst: st = (1 − zt) ◦ st−1 + zt ◦ ˆst, where ◦ is point-wise </context>
</contexts>
<marker>Chung, Gulcehre, Cho, Bengio, 2014</marker>
<rawString>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="27182" citStr="Fleiss, 1971" startWordPosition="4498" endWordPosition="4499"> 1.448) 0.32% SMT NRM-hyb (1.785, 1.215) 0.00 % SMT Rtr.-based (1.738, 1.262) 0.00 % Table 3: p-values and average rankings of Friedman test for pairwise model comparison. (Rtr.- based means the retrieval-based method) 5.2 Results Our test set consists of 110 posts that do not appear in the training set, with length between 6 to 22 Chinese words and 12.5 words on average. The experimental results based on human annotation are summarized in Table 2, consisting of the ratio of three categories and the agreement among the five labelers for each model. The agreement is evaluated by Fleiss’ kappa (Fleiss, 1971), as a statistical measure of inter-rater consistency. Except the SMT-based model, the value of agreement is in a range from 0.2 to 0.4 for all the other models, which should be interpreted as “Fair agreement”. The SMT-based model has a relatively higher kappa value 0.448, which is larger than 0.4 and considered as “Moderate agreement”, since the responses generated by the SMT often have the fluency and grammatical errors, making it easy to reach an agreement on such unsuitable cases. From Table 2, we can see the SMT method performs significantly worse than the retrieval-based and NRM models a</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Generating sequences with recurrent neural networks.</title>
<date>2013</date>
<tech>preprint arXiv:1308.0850.</tech>
<contexts>
<context position="13957" citStr="Graves (2013)" startWordPosition="2309" endWordPosition="2310">2014) for building the intermediate representation for machine translation. This scheme however has its drawbacks: a vectorial summarization of the entire post is often hard to obtain and may lose important details for response generation, especially when the dimension of the hidden state is not big enough4. In the reminder of this paper, a NRM with this global encoding scheme is referred to as NRM-glo. Context Generator Figure 4: The graphical model of the encoder in NRM-glo, where the last hidden state is used as the context vector ct = hT. Local Scheme: Recently, Bahdanau et al. (2014) and Graves (2013) introduced an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence ct = ET j�1 atjhj, where weighting factors atj determine which part should be selected to generate the new word yt, which in turn is a function of hidden states atj = q(hj, st−1), as pictorially shown in Figure 5. Basically, the attention mechanism atj models the alignment between the inputs around position j and the output at position t, so it can be viewed as a local matching model. This local scheme is devised in (Bahdanau et al., 2014) for automatic al</context>
</contexts>
<marker>Graves, 2013</marker>
<rawString>Alex Graves. 2013. Generating sequences with recurrent neural networks. preprint arXiv:1308.0850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Greff</author>
<author>Rupesh Kumar Srivastava</author>
<author>Jan Koutn´ık</author>
<author>Bas R Steunebrink</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>LSTM: A search space odyssey.</title>
<date>2015</date>
<location>CoRR, abs/1503.04069.</location>
<marker>Greff, Srivastava, Koutn´ık, Steunebrink, Schmidhuber, 2015</marker>
<rawString>Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık, Bas R. Steunebrink, and J¨urgen Schmidhuber. 2015. LSTM: A search space odyssey. CoRR, abs/1503.04069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="11494" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1863" endWordPosition="1866">tandard RNN language model except conditioned on the context input c. The generation probability of the t-th word is calculated by Xyt|yt−1, ··· , y1, x) = g(yt−1, st, ct), Encoder Decoder Attention Signal Context Generator 1579 where yt is a one-hot word representation, g(·) is a softmax activation function, and st is the hidden state of decoder at time t calculated by st = f(yt−1, st−1, ct), and f(·) is a non-linear activation function and the transformation L is often assigned as parameters of f(·). Here f(·) can be a logistic function, the sophisticated long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997), or the recently proposed gated recurrent unit (GRU) (Chung et al., 2014; Cho et al., 2014). Compared to “ungated” logistic function, LSTM and GRU are specially designed for its long term memory: it can store information over extended time steps without too much decay. We use GRU in this work, since it performs comparably to LSTM on squence modeling (Chung et al., 2014; Greff et al., 2015), but has less parameters and easier to train. We adopt the notation of GRU from (Bahdanau et al., 2014), the hidden state st at time t is a linear combination of its previous hidden state st−1 and a new can</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David C Howell</author>
</authors>
<title>Fundamental Statistics for the Behavioral Sciences.</title>
<date>2010</date>
<booktitle>PSY 200 (300) Quantitative Methods in Psychology Series. Wadsworth Cengage Learning.</booktitle>
<contexts>
<context position="29357" citStr="Howell, 2010" startWordPosition="4851" endWordPosition="4852">lar mean score as NRM-glo, and its ratio on neutral cases outperforms all the other methods. This is because 1) the responses retrieved by retrieval-based method are actually written by human, so they do not suffer from grammatical and fluency problems, and 2) the combination of various feature functions potentially makes sure the picked responses are semantically relevant to test posts. However the picked responses are not customized for new test posts, so the ratio of suitable cases is lower than the three neural generation models. To test statistical significance, we use the Friedman test (Howell, 2010), which is a nonparametric test on the differences of several related samples, based on ranking. Table 3 shows the average rankings over all annotations and the corresponding p-values for comparisons between different pairs of methods. The comparison between retrieval-based and NRM-glo is not significant and their difference in ranking is tiny. This indicates that the retrieval-based method is com1584 parable to the NRM-glo method. The NRM-hyb outperforms all the other methods, and the difference is statistically significant (p &lt; 0.05). The difference between NRM-loc and retrieval-based method</context>
</contexts>
<marker>Howell, 2010</marker>
<rawString>David C. Howell. 2010. Fundamental Statistics for the Behavioral Sciences. PSY 200 (300) Quantitative Methods in Psychology Series. Wadsworth Cengage Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zongcheng Ji</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
</authors>
<title>An information retrieval approach to short text conversation. arXiv preprint arXiv:1408.6988.</title>
<date>2014</date>
<contexts>
<context position="1868" citStr="Ji et al., 2014" startWordPosition="272" endWordPosition="275">zmann et al., 2006; Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system. Recently due to the explosive growth of microblogging services such as Twitter1 and Weibo2, the amount of conversation data available on the web has tremendously increased. This makes a lhttps://twitter.com/. zhttp://www.weibo.com/. data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible. Instead of multiple rounds of conversation, the task at hand, referred to as Short-Text Conversation (STC), only considers one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) from a user and the latter a response given by the computer. The research on STC may shed light on understanding the complicated mechanism of natural language conversation. Previous methods for STC fall into two categories, 1) the retrieval-based method (Ji et al., 2014), and 2) the statistical machine translatio</context>
<context position="19238" citStr="Ji et al., 2014" startWordPosition="3195" endWordPosition="3198">ochastic gradient descent (SGD) algorithm with mini-batch. The training stage of each model took about two weeks. 4.2 Competitor Models Retrieval-based: with retrieval-based models, for any given post p*, the response r* is retrieved from a big post-response pairs (p, r) repository. Such models rely on three key components: a big repository, sets of feature functions -bi(p*, (p, r)), and a machine learning model to combine these features. In this work, the whole 4.4 million Weibo pairs are used as the repository, 14 features, ranging from simple cosine similarity to some deep matching models (Ji et al., 2014) are used to determine the suitability of a post to a given post p* through the following linear model �score(p*, (p, r)) _ wi-bi(p*, (p, r)). (4) i Following the ranking strategy in (Ji et al., 2014), we pick 225 posts and about 30 retrieved responses for each of them given by a baseline retriever6 from the 4.4M repository, and manually label them to obtain labeled 6,017 post-response pairs. We use ranking SVM model (Joachims, 2006) for the parameters wi based on the labeled dataset. In comparison to NRM, only the top one response is considered in the evaluation process. SMT-based: In SMT-bas</context>
</contexts>
<marker>Ji, Lu, Li, 2014</marker>
<rawString>Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An information retrieval approach to short text conversation. arXiv preprint arXiv:1408.6988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In SIGKDD,</booktitle>
<pages>217--226</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="19675" citStr="Joachims, 2006" startWordPosition="3275" endWordPosition="3276">res. In this work, the whole 4.4 million Weibo pairs are used as the repository, 14 features, ranging from simple cosine similarity to some deep matching models (Ji et al., 2014) are used to determine the suitability of a post to a given post p* through the following linear model �score(p*, (p, r)) _ wi-bi(p*, (p, r)). (4) i Following the ranking strategy in (Ji et al., 2014), we pick 225 posts and about 30 retrieved responses for each of them given by a baseline retriever6 from the 4.4M repository, and manually label them to obtain labeled 6,017 post-response pairs. We use ranking SVM model (Joachims, 2006) for the parameters wi based on the labeled dataset. In comparison to NRM, only the top one response is considered in the evaluation process. SMT-based: In SMT-based models, the postresponse pairs are directly used as parallel data for training a translation model. We use the most widely used open-source phrase-based translation model-Moses (Koehn et al., 2007). Another parallel data consisting of 3000 post-response pairs is used to tune the system. In (Ritter et al., 2011), the authors used a modified SMT model to obtain the “Response” of Twitter “Stimulus”. The main modification is in replac</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear svms in linear time. In SIGKDD, pages 217–226. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="5540" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="849" endWordPosition="852">ses. We further generalize this scheme to allow the post representation to dynamically change during the generation process, following the idea in (Bahdanau et al., 2014) originally proposed for neural-network-based machine translation with automatic alignment. Figure 1: The diagram of encoder-decoder framework for automatic response generation. NRM essentially estimates the likelihood of a response given a post. Clearly the estimated probability should be complex enough to represent all the suitable responses. Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Note that in machine translation, the task is to estimate the probability of a target language sentence conditioned on the source language sentence with the same meaning, which is much easier than the task of STC which we are considering here. In this paper, we demonstrate that NRM, when equipped with a reasonable amount of data, can yield a satisfying estimator of responses (hence response generator) for STC, despite the difficulty of the task. Our main contributions are two-folds: 1) we propose to use an encoder-decoder-bas</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In EMNLP, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting of the ACL on</booktitle>
<pages>177--180</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="20038" citStr="Koehn et al., 2007" startWordPosition="3330" endWordPosition="3333">n (Ji et al., 2014), we pick 225 posts and about 30 retrieved responses for each of them given by a baseline retriever6 from the 4.4M repository, and manually label them to obtain labeled 6,017 post-response pairs. We use ranking SVM model (Joachims, 2006) for the parameters wi based on the labeled dataset. In comparison to NRM, only the top one response is considered in the evaluation process. SMT-based: In SMT-based models, the postresponse pairs are directly used as parallel data for training a translation model. We use the most widely used open-source phrase-based translation model-Moses (Koehn et al., 2007). Another parallel data consisting of 3000 post-response pairs is used to tune the system. In (Ritter et al., 2011), the authors used a modified SMT model to obtain the “Response” of Twitter “Stimulus”. The main modification is in replacing the standard GIZA++ word alignment model (Och and Ney, 2003) with a new phrase-pair selection method, in which all the 6we use the default similarity function of Lucene 7 possible phrase-pairs in the training data are considered and their associated probabilities are estimated by the Fisher’s Exact Test, which yields performance slightly better than default</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, pages 177–180. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Satinder Singh</author>
<author>Michael Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>Njfun: a reinforcement learning spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems,</booktitle>
<pages>17--20</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1312" citStr="Litman et al., 2000" startWordPosition="189" endWordPosition="192">al study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming stateof-the-arts in the same setting, including retrieval-based and SMT-based models. 1 Introduction Natural language conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system. Recently due to the explosive growth of microblogging services such as Twitter1 and Weibo2, the amount of conversation data available on the web has tremendously increased. This makes a lhttps://twitter.com/. zhttp://www.weibo.com/. data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible. Instead of </context>
</contexts>
<marker>Litman, Singh, Kearns, Walker, 2000</marker>
<rawString>Diane Litman, Satinder Singh, Michael Kearns, and Marilyn Walker. 2000. Njfun: a reinforcement learning spoken dialogue system. In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems, pages 17–20. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruhisa Misu</author>
<author>Kallirroi Georgila</author>
<author>Anton Leuski</author>
<author>David Traum</author>
</authors>
<title>Reinforcement learning of question-answering dialogue policies for virtual museum guides.</title>
<date>2012</date>
<booktitle>In SIGDIAL,</booktitle>
<pages>84--93</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1290" citStr="Misu et al., 2012" startWordPosition="185" endWordPosition="188">ng service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming stateof-the-arts in the same setting, including retrieval-based and SMT-based models. 1 Introduction Natural language conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system. Recently due to the explosive growth of microblogging services such as Twitter1 and Weibo2, the amount of conversation data available on the web has tremendously increased. This makes a lhttps://twitter.com/. zhttp://www.weibo.com/. data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011)</context>
</contexts>
<marker>Misu, Georgila, Leuski, Traum, 2012</marker>
<rawString>Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and David Traum. 2012. Reinforcement learning of question-answering dialogue policies for virtual museum guides. In SIGDIAL, pages 84–93. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="20339" citStr="Och and Ney, 2003" startWordPosition="3380" endWordPosition="3383">taset. In comparison to NRM, only the top one response is considered in the evaluation process. SMT-based: In SMT-based models, the postresponse pairs are directly used as parallel data for training a translation model. We use the most widely used open-source phrase-based translation model-Moses (Koehn et al., 2007). Another parallel data consisting of 3000 post-response pairs is used to tune the system. In (Ritter et al., 2011), the authors used a modified SMT model to obtain the “Response” of Twitter “Stimulus”. The main modification is in replacing the standard GIZA++ word alignment model (Och and Ney, 2003) with a new phrase-pair selection method, in which all the 6we use the default similarity function of Lucene 7 possible phrase-pairs in the training data are considered and their associated probabilities are estimated by the Fisher’s Exact Test, which yields performance slightly better than default setting8. Compared to retrieval-based methods, the generated responses by SMT-based methods often have fluency or even grammatical problems. In this work, we choose the Moses with default settings as our SMT model. 5 Results and Analysis Automatic evaluation of response generation is still an open p</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21041" citStr="Papineni et al., 2002" startWordPosition="3489" endWordPosition="3493">imilarity function of Lucene 7 possible phrase-pairs in the training data are considered and their associated probabilities are estimated by the Fisher’s Exact Test, which yields performance slightly better than default setting8. Compared to retrieval-based methods, the generated responses by SMT-based methods often have fluency or even grammatical problems. In this work, we choose the Moses with default settings as our SMT model. 5 Results and Analysis Automatic evaluation of response generation is still an open problem. The widely accepted evaluation methods in translation (e.g. BLEU score (Papineni et al., 2002)) do not apply, since the range of the suitable responses is so large that it is practically impossible to give reference with adequate coverage. It is also not reasonable to evaluate with Perplexity, a generally used measurement in statistical language modeling, because the naturalness of response and the relatedness to the post can not be well evaluated. We therefore resort to human judgement, similar to that taken in (Ritter et al., 2011) but with an important difference. 5.1 Evaluation Methods We adopt human annotation to compare the performance of different models. Five labelers with at l</context>
<context position="32218" citStr="Papineni et al., 2002" startWordPosition="5319" endWordPosition="5322">stigate the ability of NRM to generate multiple responses. Figure 9 lists 5 responses to the same post, which are gotten with beam search with beam size = 500, among which we keep only the best one (biggest likelihood) for each first word. It can be seen that the responses are fluent, relevant to the post, and still vastly different from each other, validating our initial conjecture that NRM, when fueled with large and rich training corpus, could work as a generator that can cover a lot of modes in its density estimation. It is worth mentioning that automatic evaluation metrics, such as BLEU (Papineni et al., 2002) as adopted by machine translation and recently SMTbased responding models (Sordoni et al., 2015), do not work very well on this task, especially when the reference responses are few. Our results show that the average BLEU values are less than 2 for all models discussed in this paper, including SMTbased ones, on instances with single reference. Probably more importantly, the ranking given by the BLEU value diverges greatly from the human judgment of response quality. 6 Conclusions and Future Work In this paper, we explored using encoder-decoderbased neural network system, with coined name Neur</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>William B Dolan</author>
</authors>
<title>Data-driven response generation in social media.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>583--593</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1890" citStr="Ritter et al., 2011" startWordPosition="276" endWordPosition="279">6; Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system. Recently due to the explosive growth of microblogging services such as Twitter1 and Weibo2, the amount of conversation data available on the web has tremendously increased. This makes a lhttps://twitter.com/. zhttp://www.weibo.com/. data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible. Instead of multiple rounds of conversation, the task at hand, referred to as Short-Text Conversation (STC), only considers one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) from a user and the latter a response given by the computer. The research on STC may shed light on understanding the complicated mechanism of natural language conversation. Previous methods for STC fall into two categories, 1) the retrieval-based method (Ji et al., 2014), and 2) the statistical machine translation (SMT) based method (</context>
<context position="4358" citStr="Ritter et al., 2011" startWordPosition="668" endWordPosition="671">l Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1577–1586, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics lowing figure: Post Having my fish sandwich right now UserA For god’s sake, it is 11 in the morning UserB Enhhhh... sounds yummy UserC which restaurant exactly? Empirical studies also showed that SMT-based methods often yield responses with grammatical errors and in rigid forms, due to the unnecessary alignment between the “source” post and the “target” response (Ritter et al., 2011). This rigidity is still a serious problem in the recent work of (Sordoni et al., 2015), despite its use of neural network-based generative model as features in decoding. 1.1 Overview In this paper, we take a probabilistic model to address the response generation problem, and propose employing a neural encoder-decoder for this task, named Neural Responding Machine (NRM). The neural encoder-decoder model, as illustrated in Figure 1, first summarizes the post as a vector representation, then feeds this representation to a decoder to generate responses. We further generalize this scheme to allow </context>
<context position="20153" citStr="Ritter et al., 2011" startWordPosition="3350" endWordPosition="3353">ever6 from the 4.4M repository, and manually label them to obtain labeled 6,017 post-response pairs. We use ranking SVM model (Joachims, 2006) for the parameters wi based on the labeled dataset. In comparison to NRM, only the top one response is considered in the evaluation process. SMT-based: In SMT-based models, the postresponse pairs are directly used as parallel data for training a translation model. We use the most widely used open-source phrase-based translation model-Moses (Koehn et al., 2007). Another parallel data consisting of 3000 post-response pairs is used to tune the system. In (Ritter et al., 2011), the authors used a modified SMT model to obtain the “Response” of Twitter “Stimulus”. The main modification is in replacing the standard GIZA++ word alignment model (Och and Ney, 2003) with a new phrase-pair selection method, in which all the 6we use the default similarity function of Lucene 7 possible phrase-pairs in the training data are considered and their associated probabilities are estimated by the Fisher’s Exact Test, which yields performance slightly better than default setting8. Compared to retrieval-based methods, the generated responses by SMT-based methods often have fluency or </context>
<context position="21486" citStr="Ritter et al., 2011" startWordPosition="3564" endWordPosition="3567">d Analysis Automatic evaluation of response generation is still an open problem. The widely accepted evaluation methods in translation (e.g. BLEU score (Papineni et al., 2002)) do not apply, since the range of the suitable responses is so large that it is practically impossible to give reference with adequate coverage. It is also not reasonable to evaluate with Perplexity, a generally used measurement in statistical language modeling, because the naturalness of response and the relatedness to the post can not be well evaluated. We therefore resort to human judgement, similar to that taken in (Ritter et al., 2011) but with an important difference. 5.1 Evaluation Methods We adopt human annotation to compare the performance of different models. Five labelers with at least three-year experience of Sina Weibo are invited to do human evaluation. Responses obtained from the five evaluated models are pooled and randomly permuted for each labeler. The labelers are instructed to imagine that they were the authors of the original posts and judge whether a response (generated or retrieved) is appropriate and natural to a input post. Three levels are assigned to a response with scores from 0 to 2: • Suitable (+2):</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2011</marker>
<rawString>Alan Ritter, Colin Cherry, and William B Dolan. 2011. Data-driven response generation in social media. In EMNLP, pages 583–593. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Karl Weilhammer</author>
<author>Matt Stuttle</author>
<author>Steve Young</author>
</authors>
<title>A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. The knowledge engineering review,</title>
<date>2006</date>
<pages>21--02</pages>
<contexts>
<context position="1271" citStr="Schatzmann et al., 2006" startWordPosition="181" endWordPosition="184">lected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming stateof-the-arts in the same setting, including retrieval-based and SMT-based models. 1 Introduction Natural language conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system. Recently due to the explosive growth of microblogging services such as Twitter1 and Weibo2, the amount of conversation data available on the web has tremendously increased. This makes a lhttps://twitter.com/. zhttp://www.weibo.com/. data-driven approach to attack the conversation problem (Ji et al., 2014; R</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. 2006. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. The knowledge engineering review, 21(02):97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Sordoni</author>
<author>Michel Galley</author>
<author>Michael Auli</author>
<author>Chris Brockett</author>
<author>Yangfeng Ji</author>
<author>Meg Mitchell</author>
<author>Jian-Yun Nie</author>
<author>Jianfeng Gao</author>
<author>Bill Dolan</author>
</authors>
<title>A neural network approach to context-sensitive generation of conversational responses.</title>
<date>2015</date>
<booktitle>Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT</booktitle>
<contexts>
<context position="2510" citStr="Sordoni et al., 2015" startWordPosition="380" endWordPosition="383"> possible. Instead of multiple rounds of conversation, the task at hand, referred to as Short-Text Conversation (STC), only considers one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) from a user and the latter a response given by the computer. The research on STC may shed light on understanding the complicated mechanism of natural language conversation. Previous methods for STC fall into two categories, 1) the retrieval-based method (Ji et al., 2014), and 2) the statistical machine translation (SMT) based method (Sordoni et al., 2015; Ritter et al., 2011). The basic idea of retrievalbased method is to pick a suitable response by ranking the candidate responses with a linear or non-linear combination of various matching features (e.g. number of shared words). The main drawbacks of the retrieval-based method are the following • the responses are pre-existing and hard to customize for the particular text or requirement from the task, e.g., style or attitude. • the use of matching features alone is usually not sufficient for distinguishing positive responses from negative ones, even after time consuming feature engineering. (</context>
<context position="4445" citStr="Sordoni et al., 2015" startWordPosition="685" endWordPosition="688">oint Conference on Natural Language Processing, pages 1577–1586, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics lowing figure: Post Having my fish sandwich right now UserA For god’s sake, it is 11 in the morning UserB Enhhhh... sounds yummy UserC which restaurant exactly? Empirical studies also showed that SMT-based methods often yield responses with grammatical errors and in rigid forms, due to the unnecessary alignment between the “source” post and the “target” response (Ritter et al., 2011). This rigidity is still a serious problem in the recent work of (Sordoni et al., 2015), despite its use of neural network-based generative model as features in decoding. 1.1 Overview In this paper, we take a probabilistic model to address the response generation problem, and propose employing a neural encoder-decoder for this task, named Neural Responding Machine (NRM). The neural encoder-decoder model, as illustrated in Figure 1, first summarizes the post as a vector representation, then feeds this representation to a decoder to generate responses. We further generalize this scheme to allow the post representation to dynamically change during the generation process, following </context>
<context position="32315" citStr="Sordoni et al., 2015" startWordPosition="5334" endWordPosition="5337">post, which are gotten with beam search with beam size = 500, among which we keep only the best one (biggest likelihood) for each first word. It can be seen that the responses are fluent, relevant to the post, and still vastly different from each other, validating our initial conjecture that NRM, when fueled with large and rich training corpus, could work as a generator that can cover a lot of modes in its density estimation. It is worth mentioning that automatic evaluation metrics, such as BLEU (Papineni et al., 2002) as adopted by machine translation and recently SMTbased responding models (Sordoni et al., 2015), do not work very well on this task, especially when the reference responses are few. Our results show that the average BLEU values are less than 2 for all models discussed in this paper, including SMTbased ones, on instances with single reference. Probably more importantly, the ranking given by the BLEU value diverges greatly from the human judgment of response quality. 6 Conclusions and Future Work In this paper, we explored using encoder-decoderbased neural network system, with coined name Neural Responding Machine, to generate responses to a post. Empirical studies confirm that the newly </context>
</contexts>
<marker>Sordoni, Galley, Auli, Brockett, Ji, Mitchell, Nie, Gao, Dolan, 2015</marker>
<rawString>Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural network approach to context-sensitive generation of conversational responses. Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT 2015), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="5583" citStr="Sutskever et al., 2014" startWordPosition="857" endWordPosition="860"> post representation to dynamically change during the generation process, following the idea in (Bahdanau et al., 2014) originally proposed for neural-network-based machine translation with automatic alignment. Figure 1: The diagram of encoder-decoder framework for automatic response generation. NRM essentially estimates the likelihood of a response given a post. Clearly the estimated probability should be complex enough to represent all the suitable responses. Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Note that in machine translation, the task is to estimate the probability of a target language sentence conditioned on the source language sentence with the same meaning, which is much easier than the task of STC which we are considering here. In this paper, we demonstrate that NRM, when equipped with a reasonable amount of data, can yield a satisfying estimator of responses (hence response generator) for STC, despite the difficulty of the task. Our main contributions are two-folds: 1) we propose to use an encoder-decoder-based neural network to generate a response in</context>
<context position="10540" citStr="Sutskever et al., 2014" startWordPosition="1697" endWordPosition="1700">he generation process. It should be noted that αt could be fixed over time or Figure 2: The general framework and dataflow of the encoder-decoder-based NRM. changes dynamically during the generation of response sequence y. In the dynamic settings, αt can be function of historically generated subsequence (y1, · · · , yt−1), input sequence x or their latent representations, more details will be shown later in Section 3.2. We use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of arbitrary lengths (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014). ͙͘ ͙͘ ͙͘ Figure 3: The graphical model of RNN decoder. The dashed lines denote the variables related to the function g(·), and the solid lines denote the variables related to the function f(·). 3.1 The Computation in Decoder Figure 3 gives the graphical model of the decoder, which is essentially a standard RNN language model except conditioned on the context input c. The generation probability of the t-th word is calculated by Xyt|yt−1, ··· , y1, x) = g(yt−1, st, ct), Encoder Decoder Attention Signal Context Generator 1579 where yt is a one-hot word representation, g(·) is</context>
<context position="13349" citStr="Sutskever et al., 2014" startWordPosition="2206" endWordPosition="2209">e responses. 3.2 The Computation in Encoder We consider three types of encoding schemes, namely 1) the global scheme, 2) the local scheme, and the hybrid scheme which combines 1) and 2). Global Scheme: Figure 4 shows the graphical model of the RNN-encoder and related context generator for a global encoding scheme. The hidden state at time t is calculated by ht = f(xt, ht−1) (i.e. still GRU unit), and with a trivial context generation operation, we essentially use the final hidden state hT as the global representation of the sentence. The same strategy has been taken in (Cho et al., 2014) and (Sutskever et al., 2014) for building the intermediate representation for machine translation. This scheme however has its drawbacks: a vectorial summarization of the entire post is often hard to obtain and may lose important details for response generation, especially when the dimension of the hidden state is not big enough4. In the reminder of this paper, a NRM with this global encoding scheme is referred to as NRM-glo. Context Generator Figure 4: The graphical model of the encoder in NRM-glo, where the last hidden state is used as the context vector ct = hT. Local Scheme: Recently, Bahdanau et al. (2014) and Grave</context>
<context position="14591" citStr="Sutskever et al. (2014)" startWordPosition="2420" endWordPosition="2424"> an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence ct = ET j�1 atjhj, where weighting factors atj determine which part should be selected to generate the new word yt, which in turn is a function of hidden states atj = q(hj, st−1), as pictorially shown in Figure 5. Basically, the attention mechanism atj models the alignment between the inputs around position j and the output at position t, so it can be viewed as a local matching model. This local scheme is devised in (Bahdanau et al., 2014) for automatic alignment be4Sutskever et al. (2014) has to use 4, 000 dimension for satisfying performance on machine translation, while (Cho et al., 2014) with a smaller dimension perform poorly on translating an entire sentence. ͙͘ 1580 tween the source sentence and the partial target sentence in machine translation. This scheme enjoys the advantage of adaptively focusing on some important words of the input text according to the generated words of response. A NRM with this local encoding scheme is referred to as NRM-loc. Figure 5: The graphical model of the encoder in NRM-loc, where the weighted sum of hidden sates is used as the context ve</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In NIPS, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Wang</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Enhong Chen</author>
</authors>
<title>A dataset for research on short-text conversations.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>935--945</pages>
<contexts>
<context position="7532" citStr="Wang et al., 2013" startWordPosition="1192" endWordPosition="1195">user can post short messages (referred to as post in the reminder of this paper) visible to the public or a group of users following her/him. Other users make comment on a published post, which will be referred to as a response. Just like Twitter, Weibo also has the length limit of 140 Chinese characters on both posts and responses, making the post-response pair an ideal surrogate for short-text conversation. 2.2 Dataset Description To construct this million scale dataset, we first crawl hundreds of millions of post-response pairs, and then clean the raw data in a similar way as suggested in (Wang et al., 2013), including 1) removing trivial responses like “wow”, 2) filtering out potential advertisements, and 3) removing the responses after first 30 ones for topic consistency. Table 1 shows some statistics of the dataset used 3http://www.noahlab.com.hk/topics/ShortTextConversation For god&apos;s sake, it is 11 in the morning Having my fish sandwich right now Enhhhh... sounds yummy which restaurant exactly? Decoder vector Encoder 1578 Training #posts 219,905 #responses 4,308,211 #pairs 4,435,959 Test Data #test posts 110 Labeled Dataset #posts 225 (retrieval-based) #responses 6,017 #labeled pairs 6,017 Fi</context>
</contexts>
<marker>Wang, Lu, Li, Chen, 2013</marker>
<rawString>Hao Wang, Zhengdong Lu, Hang Li, and Enhong Chen. 2013. A dataset for research on short-text conversations. In EMNLP, pages 935–945.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1246" citStr="Williams and Young, 2007" startWordPosition="177" endWordPosition="180">ound conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming stateof-the-arts in the same setting, including retrieval-based and SMT-based models. 1 Introduction Natural language conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. Previous works in this direction mainly focus on either rule-based or learning-based methods (Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system. Recently due to the explosive growth of microblogging services such as Twitter1 and Weibo2, the amount of conversation data available on the web has tremendously increased. This makes a lhttps://twitter.com/. zhttp://www.weibo.com/. data-driven approach to attack the conversation pr</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech &amp; Language, 21(2):393–422.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>