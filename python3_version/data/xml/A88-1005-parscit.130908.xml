<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002481">
<note confidence="0.6234815">
TWO SIMPLE PREDICTION ALGORITHMS
TO FACILITATE TEXT PRODUCTION
</note>
<title confidence="0.5538895">
Lois Boggess
P.O. Drawer CS
Mississippi State University
Mississippi State, MS 39762
</title>
<sectionHeader confidence="0.878104" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.987621181818182">
Several simple prediction schemes are
presented for systems intended to facili—
tate text production for handicapped
individuals. The schemes are based on
single—subject language models, where the
system is self—adapting to the past
language use of the subject. Sentence
position, the immediately preceding one
or two words, and initial letters of the
desired word are cues which may be used
by the systems.
</bodyText>
<sectionHeader confidence="0.997886" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.971342655172414">
For some years we have been investi—
gating the use of a sizeable sample of a
particular individual&apos;s language habits
in predicting future language use for
that individual. The research has taken
two directions.
One of these, the HWYE (Hear What
You Expect) system, builds a large lan—
guage model of the past language history
of the individual, with special emphasis
on the most frequent words of that
person, and the result is used in speech
recognition. In studying the language
model developed by the HWYE system,
several simple predictive schemes were
noted which are capable of anticipating,
during the generation of a sentence, a
small set of words from which the next
desired word can be selected. The two
schemes described here are used for text
generation (not speech recognition) in a
format that could be of use to a physi—
cally handicapped person; hence the
schemes have no right context available.
One of the schemes does use left context,
and the other uses only sentence position
as &amp;quot;context&amp;quot;. Both are implemented on
IBM—PC systems with minimal memory
requirements.
</bodyText>
<sectionHeader confidence="0.968732" genericHeader="method">
MOTIVATION
</sectionHeader>
<bodyText confidence="0.998923806451613">
One hundred English words account for
47 per cent of the Brown corpus (about
one million words of American English
text taken from a wide range of sources).
It seems reasonable to suppose that a
single individual might in fact require
fewer words to account for a large
proportion of generated text. From our
work on the HWYE system it was known
that 75 words accounted for half of all
the text of Vanity Fair, a 300,000 word
Victorian English novel by Thackeray
(which incorporated a fairly involved
syntax, much embedded quotation, and
passages in dialect and in French)
[English and Boggess, 1986]. We further
found that 50 words accounted for half of
all the verbiage in a 20,000 word set of
sentences provided by an individual who
collaborated with us. This latter
corpus, called the Sherri data, is a set
of texts provided by a speech—handicapped
individual who uses a typewriter to
communicate, even with her family; it is
conversational in nature, as can be seen
in Figure 1. Most of the work reported
in this paper gives special attention to
the set of words required to account for
half of all the verbiage of a given
individual. We refer to this set as the
set of high—frequency words.
</bodyText>
<page confidence="0.996484">
33
</page>
<figure confidence="0.973861272727273">
You said something about a magazine that &lt;namel&gt; had
about computers that I might like to borrow.
I would some time.
I think we have to pick up the children while &lt;name2&gt;
is in the hospital.
I want to visit her in the hospital.
But you have to lift me up to the window for me to see
the baby.
Well, it&apos;s May first now. Help!
I thought it would not be so busy but it looks like it
might be now.
</figure>
<figureCaption confidence="0.99992">
Figure 1. Sample set of contiguous sentences in Sherri data
</figureCaption>
<bodyText confidence="0.984152814285714">
It seems reasonable to suppose that
for conversational English, approximately
50 words may account for half of the
verbiage of most English users. From
the standpoint of human factors, an
argument could be made that one should
simply put the 50 words up on the screen
with the alphabet and thus be assured
that half of all the words desired by the
user were instantly available, in known
locations that the user would quickly
become accustomed to. Constantly
changing menus introduce an element of
user fatigue [Gibler and Childress,
1982]. That argument may especially make
sense as larger screens with more lines
per screen and more characters per line
become more common.
If we limit ourselves to the top 20
most frequent words as a constant menu,
only about 30 per cent of the user&apos;s
verbiage is accounted for. However, it
was observed, while working with the
HWYE system, that if one looked at the
top 20 words for any given sentence
position, one did not see the same set of
words occurring. Clearly the high
frequency words (the set that comprise
half of word use) are mildly sensitive to
&apos;context&amp;quot; even when &amp;quot;context&amp;quot; is so
broadly defined as sentence position.
Different subsets of the 50 member set of
high frequency words appear in the set of
20 most frequent words for a given
sentence position. Moreover, after
processing approximately 2000 sentences
from the user, it was still the case that
some of the top 20 words for a given
position were not members of the high
frequency set at all. For example, the
word they, a member of the menu for the
first sentence position (see Figure 2)
and hence one of the 20 most frequent
words to start a sentence, is not a
member of the global high frequency set.
A preliminary analysis by English
suggested that, whereas a constant
&amp;quot;prediction&amp;quot; of the top 20 most frequent
words would yield a success rate of 30
per cent, predicting the top 20 most
frequent words per position in sentence
would yield a success rate of 40 per
cent.
&amp;quot;CONTEXT&amp;quot; AS SENTENCE POSITION
The simplest scheme, which has been
built as a prototype on an IBM PC with
two floppy disk drives, presents the user
with the top 20 most frequent words that
the user has employed at whatever
position in a sentence is current. For
example, Figure 2 shows the screen
presented to the user at the beginning of
production of a sentence. On the left is
a list of the 20 words which that
particular user is known to have used
most often to begin sentences. On the
right is the alphabet, which is normally
available to the user; and in other
places on the screen are special
functions. (Selection of words, letters
</bodyText>
<page confidence="0.996053">
34
</page>
<figure confidence="0.969701523809524">
1 but SPELL
2 can CAPITAL
3 could PUNCTUATION
4 do
5 he
6 how
7 I HELP-MENU p 1. J Ix 1
8 1..4
9 if ENDING
10 it
11 it. NUMBER
12 Lois
13 she SPECIAL
14 that
15 the REVIEW
16 they
17 we HARD-COPY
18 what
19 when SAVE-SENT
20 you NEU ERASE QUIT
NEN SENTENCE:
</figure>
<figureCaption confidence="0.999927">
Figure 2. Initial Screen
</figureCaption>
<bodyText confidence="0.997094632653061">
and functions is made by mouse, though
the actual selection mechanism is
separated from the bulk of the code so
that replacement with another selection
mechanism should be relatively easy to
implement.) The sentence is built at the
bottom of the screen. If the user
selects a word from the menu at the left,
it is placed in first position in the
sentence, and a second menu, consisting
of the 20 most frequent words that the
user has used in second place in a
sentence, appears in the left portion of
the screen. After a second word has been
produced and added to the sentence, a
third menu, consisting of the 20 most
frequent words for that user in third
place in a sentence, is offered, and so
on.
At any time the user may reject the
lefthand menu by selecting a letter of the
alphabet. Figure 3 shows the screen after
the user has produced two words of a
sentence and has begun to spell a third
word by selecting the letter &amp;quot;a&amp;quot;. At this
point, the top 20 most frequently used
words beginning with &amp;quot;a&amp;quot; have been offered
at the left. If the desired word is not
in the list, the user continues by select—
ing the second letter of the desired word
(in this case, &amp;quot;n&amp;quot;). The left—hand menu
becomes the 20 most frequently used words
beginning with the pair of letters given
so far. As is shown in Figure 4, there
are times when fewer than 20 words of a
given two—letter starting combination have
been encountered from the user&apos;s past
history, in which case this algorithm
offers a shortened list.
In the case illustrated, the desired
word was on the list. If it were not, the
user would have had to spell out the en—
tire word, and it would have been entered
into the sentence. In either case, the
system subsequently returns to offering
the menu of most—frequently—used words for
the fourth position, and continues in
similar fashion to the end of the
sentence.
</bodyText>
<figure confidence="0.963336818181818">
1 a
2 aLle
3 about
4 after b a a • r
5 afternoon
6 again
7 all h .1 k
8 an
9 an
10 and N n o P ql P
11 anyway
12 apple
13 April t u V Ye x
14 aro
15
16 as 9 5
17 ask
18 asked
19 at
20 aunt
NEN SENTENCE:
I have
</figure>
<figureCaption confidence="0.982096">
Figure 3: User has selected &amp;quot;a&amp;quot;
</figureCaption>
<figure confidence="0.734892">
NEW SENTENCE:
I hap•
</figure>
<figureCaption confidence="0.996162">
Figure 4: User has selected &amp;quot;a—n&amp;quot;
</figureCaption>
<figure confidence="0.997741272727273">
1 animal
2 animals
3 Anita
4 anniversary
5 Army
6 another
7
8
9 any
le
LI anything
</figure>
<page confidence="0.994053">
35
</page>
<bodyText confidence="0.999679181818182">
The system keeps up with how often a
word has been used and with how many
times it has occurred in each position in
a sentence, so that from time to time a
word is promoted to one of the top 20
alphabetic or top 20 position—related sets
of words. For details on the file organi—
zation scheme that allows this to be done
in real time, see Wei [1987]. Details on
the mouse—based implementation for IBM
PC&apos;s are available in Chow [1986].
</bodyText>
<sectionHeader confidence="0.828182" genericHeader="method">
A SECOND ALGORITHM
</sectionHeader>
<bodyText confidence="0.999865577777778">
An alternative predictive algorithm
has been implemented which replaces the
sentence—position—based first menu. It
pays special attention to the 50 most
frequently used words in the individual&apos;s
vocabulary (the high—frequency words) and
to the words most likely to follow them.
By virtue of their frequency, these are
precisely the words about which the most
is known, with the greatest confidence,
after a relatively small body of input
such as a few thousand sentences.
For each of the 50 high—frequency
words, a list is kept of the top 20 most
frequent words to follow that word. Let
us call these the first order followers.
For each of the first order followers,
there is a list of second—order followers:
words known to have followed the two
word sequence consisting of the high—
frequency word and its first order
follower.
For example, the word &amp;quot;I&amp;quot; is a high—
frequency word. The first order followers
for V include the word &amp;quot;world&amp;quot;. The
second—order followers for &amp;quot;I would&amp;quot;
include the word &amp;quot;like&amp;quot;. (See Figure 5.)
The second—order followers for &amp;quot;I would&amp;quot;
also include many one—time—only followers,
as well, so the system maintains a
threshold for the number of occurrances
below which a word is not included in the
list of second—order followers. The
reasoning is that a word&apos;s having occurred
only once in an environment that by
definition occurs frequently may be taken
as counter—evidence that the word should
be predicted.
Rather than predict a word with low
reliability, one of two alternatives are
taken. If the first—order follower is
itself a high—frequency word, then low—
reliability second—order followers may be
replaced with the first—order follower&apos;s
own followers. (&amp;quot;Would&amp;quot; is a first—order
</bodyText>
<figure confidence="0.870844666666667">
._sorthink
would
don&apos;t ;
hope
was
wish
like
wI ll
have know &apos;
want really
wonder want
sot have
</figure>
<figureCaption confidence="0.970635">
Figure 5. First— and second— followers
for &amp;quot;I&amp;quot;
</figureCaption>
<bodyText confidence="0.999915175">
follower of &amp;quot;I&amp;quot; and is itself a high—
frequency word. There are relatively few
reliable second—order followers to &amp;quot;would&amp;quot;
in the left context of &amp;quot;I&amp;quot;, so the list is
augmented with first—order followers of
&amp;quot;would&amp;quot; to round out a list of 20 words.)
The other alternative, taken when the
first—order follower is not a high—
frequency word, is to fill out any short
list of second—order words with the high—
frequency words themselves.
This algorithm is related to, but
takes less memory and is less powerful
than a full—blown second order Markov
model. Each state in a second—order
(trigram) Markov model is uniquely
determined by the previous two inputs.
For an input vocabulary of 2000 words, the
number of mathematically possible states
in a trigram Markov model is 4,000,000,
with more than 8 billion arcs intercon—
necting the states. Fortunately, in the
real world most of these mathematically
possible states and arcs do not actually
occur, but a trigram model for the real
world possibilities is still quite large.
We experimented with abstracting the
input vocabulary by restricting it to the
50 highest—frequency words plus the
pseudo—input OTHER onto which all other
words were mapped. When we did so, the
number of states and arcs in the various
order Markov models was still fairly large
for the real world data [English and
Boggess, 1986]. As Figure 6 shows, for
example, the rate of growth for a fourth—
order abstract Markov model (just the 50
highest—frequency words plus OTHER plus
end—of—sentence) is in the neighborhood of
250 new states and 450 new arcs per 1000
</bodyText>
<figure confidence="0.976210333333333">
111
the
we
It
it..
of
Son
,so
Clike_
</figure>
<page confidence="0.967079">
36
</page>
<table confidence="0.9986645">
wor ds Sherri data Thackeray data
new states new arcs new states new arcs
1000 527 677 639 830
2000 469 620 624 818
3000 471 636 476 705
4000 399 562 467 716
5000 397 566 463 714
6000 391 579 437 668
7000 337 507 389 642
8000 311 476 370 628
9000 323 500 361 612
10000 285 486 384 629
11000 329 518 348 601
12000 278 448 331 588
13000 276 445 310 543
14000 240 408 291 530
15000 248 425 287 529
16000 244 420 290 533
17000 243 414 269 497
18000 259 446 234 468
</table>
<figureCaption confidence="0.918769">
Figure 6. Growth of abstracted fourth—order Markov models
</figureCaption>
<bodyText confidence="0.991848378787879">
PREDICTIVE CAPABILITIES
new words of text, after 17000 words of
input. This was true for both the Sherri
data (conversational English) and the more
formal Thackeray data. Moreover, the
fourth—order Markov model for the
abstracted Thackeray data continued to
grow. After 100,000 words of input, with
a model of approximately 22,000 states and
approximately 45,000 arcs, the rate of
growth was still more than 1,000 states
and 3,000 arcs per 10,000 words of input.
For this particular implementation,
however, neither a full—blown Markov
model using total vocabulary nor an
abstract model using the 50—word vocabu—
lary seemed appropriate. On the one hand,
models of the entire vocabulary confirmed
that many multiple word sequences did
occur regularly. Nevertheless, for any
but the simplest order Markov models
(orders zero and one), the vast bulk of
the networks were taken by word combina—
tions that occurred only once. On the
other hand, restricting the predictive
mechanism to only the high—frequency words
obviously left out some of the regularly
occurring word combinations. Our first—
and second—follower algorithm described on
the previous pages allows lower frequency
words to be predicted when they occur
regularly in combination with high—
frequency words.
The data used to test the predictive
capabilities of the system were type—
scripts provided by the user, who was
utilizing a manual typewriter; it follows
that the results were not biased by the
user&apos;s favoring sentence patterns that the
system itself provided. The system had
been given 1750 prior sentences produced
by the user and the data collected were
for the performance of the system over the
next 97 sentences. The 1750 sentences
were 14,669 words in length with a vocabu—
lary of 1512 words. Twelve sentences of
the 1750 were a single word in length
(e.g. &amp;quot;yeah&amp;quot;, no and &amp;quot;gesundheit&amp;quot;) and
51 were of length 20 or greater. Average
length of sentence for the initial body
was 8.4 words per sentence. The first 200
sentences included transcriptions of oral
sentences, which were much shorter on
average, since the user is speech handi—
capped. If the first 200 sentences are
omitted, the average sentence length is
8.6 for the following 1550 sentences.
Of the next 97 sentences generated,
the shortest sentence was &amp;quot;Thanks again.&amp;quot;
The longest was You said something about
a magazine that Jenni had about computers
that I might like to borrow.&amp;quot; The 97
sentences consisted of 884 words (six of
which were numbers in digital form), for
an average length of 9.1 words per
sentence.
</bodyText>
<page confidence="0.998676">
37
</page>
<bodyText confidence="0.999985846153846">
Of the 884 words, 350 were presented
on the first menu, 373 were presented on
the second menu (after one letter had been
spelled), 109 were presented on the third
menu (after two letters had been spelled),
2 were presented on the fourth menu (after
three letters had been spelled, 43 were
spelled out in their entirety, and 7 were
numbers in digital form, produced using
the number screen of the system.
From the above, it is obvious that
the device of predicting the 20 most
frequent words by sentence position is
successful 39.6 per cent of the time;
42.2 per cent of the time, the desired
word is among the 20 most frequent words
of a given initial letter but not in the
20 most frequent words by position;
combining these two facts, we see that
81.8 per cent of the time, this simple
prediction scheme presents the desired
word on a first or second selection. The
desired word is offered in the first,
second, or third menu 94.1 per cent of the
time, and most of the rest of the time
(5.7 per cent of total), the desired word
is unknown to the system and is &amp;quot;spelled
out, where &amp;quot;spelling&amp;quot; includes producing
numbers.
Although the fourth menu, consisting
of words with a three—letter initial
sequence, presently has a low success
rate, it is precisely this category that
we expect to see improve as more of the
user&apos;s words become known to the system
through spelling. That is, as time
passes, we expect the user to have to
resort to complete spelling less and less
because the known vocabulary will include
more and more of the actual vocabulary of
the user. Many of the new words will be
low frequency words that we would expect
to find on the menu for three—letter com—
binations after they are known.
The second algorithm, using first— and
second—followers of the high—frequency
words, was run on 100 sentences, the
shortest of which was &amp;quot;Help!&amp;quot; (94 of the
97 test sentences for the first algorithm
were represented in the test set for the
second.) There were 895 words in the
sample, of which 448 were presented on the
first menu, 280 were presented on the
second (after one letter had been spelled
out, 83 on the third (after two letters
were spelled), 1 on the fourth, and 83
were spelled out in their entirety (this
category included numbers).
Running the second test gave us a
very quick appreciation for the value of
adding new words to the system as they
are encountered, since this implementation
of the second algorithm did not. One
especially striking example was a word
beginning with &amp;quot;w—o&amp;quot; which had never been
used before, but which occurred five times
in the 100 test sentences and had to be
spelled out each time. This was especial—
ly irritating since the &amp;quot;w—o&amp;quot; menu (third
menu) had fewer than 20 entries and would
have accommodated the new word. A com—
parison of the two columns of Figure 7
suggcsts that for the text held in common
by the two tests, approximately 30 words
had to be spelled out by the second algo—
rithm, which were selected by menu in the
first algorithm because it added new words
to its data sets as they were encountered.
</bodyText>
<sectionHeader confidence="0.955783" genericHeader="method">
PROPOSED EXTENSIONS
</sectionHeader>
<bodyText confidence="0.997141">
We have several plans for the future,
most of them involving the second algo—
rithm. Our first task is to increase the
number of sentences in the Sherri data to
3000 and determine how much (if at all)
an enlarged base of experience improves
the ability of the algorithm to predict
Sentence position algorithm frequent word/left context algorithm
number sentences: 97 number sentences: 100
number of words : 884 number of words : 895
</bodyText>
<table confidence="0.990091142857143">
words % total words % total
first menu: 350 39.6% 39.6% first menu: 448 50% 50%
second menu: 373 42.2% 81.8% second menu: 280 31.3% 81.3%
third menu: 109 12.3% 94.1% third menu: 83 9.3% 90.0k
fourth menu: 2 0.2% 94.3% fourth menu: 1 0.1% 90.7%
spelled: 43 4.8% 99.2.k &amp;quot;spelled&amp;quot;: 83 9.3% 100%
numbers: 7 0.8% 100%
</table>
<figureCaption confidence="0.99592">
Figure 7. Comparison of the predictive capabilities.
</figureCaption>
<page confidence="0.997363">
38
</page>
<bodyText confidence="0.99995236">
the desired word on the first try.
In its present form, the system is
reliable in its predictions after several
hundred sentences by the user have been
processed. We intend to take something
like the Brown corpus for American
English and from it create a vanilla—
flavored predictor as a start—up version
for a new user, with facilities built in
to have the user&apos;s own language patterns
gradually outweigh the Brown corpus
initialization as they are input.
Eventually the Brown corpus would have
essentially no effect, or at least no
effect overriding the user&apos;s individual
use of language (it might serve as a
basic dictionary for text vocabulary not
yet seen from the user).
We intend to investigate what effect
generating sentences while using the
system has on our collaborator. To date,
she has obligingly been willing to
continue to use a typewriter to generate
text, but she does own a personal computer
and is able to use a mouse. Our own
experience in entering her sentences on
the system has made it clear that in many
instances she would have expressed the
same ideas more rapidly on the system with
a slight change in wording. Since the
proferred words and patterns are derived
by the system from her own language
history, they should feel normal and
natural to her and could influence her to
modify her intentions in generating a
sentence. On the other hand, a different
handicapped individual (a quadriplegic)
has informed us that ease of mechanical
production of a sentence has little or no
effect on his choice of words, and that
would appear to be the case for our
collaborator while she uses the
typewriter.
Finally, we wish to make use of the
much larger amounts of memory available
on personal computers by taking account of
the followers for many of the moderate—
frequency words. For example, in the
sentence &amp;quot;would you be able...&amp;quot; the word
&amp;quot;able&amp;quot; is not high frequency. Neverthe—
less, the system could easily deduce what
following word to expect, since every
known occurrence of &amp;quot;able&amp;quot; is followed by
to. As it happens, to is one of the
top 20 most frequent words and hence
fortuitously is on the default menu after
the non—high—frequency word &amp;quot;able&amp;quot;, but
there are many other examples where the
system is not so lucky. For instance,
&amp;quot;pick&amp;quot; is usually followed by &amp;quot;up&amp;quot; in the
Sherri data, but &amp;quot;pick&amp;quot; is low frequency
and &amp;quot;up&amp;quot; is not on the default first menu.
Similarly, &amp;quot;think&amp;quot; is a high—frequency
word and has a well developed set of
followers. &amp;quot;Thinks&amp;quot; and &amp;quot;thought&amp;quot; are not
high—frequency and hence are followed by
the default first menu. Yet virtually
every follower for &amp;quot;thinks&amp;quot; and &amp;quot;thought&amp;quot;
in the Sherri data happens to belong to
the set of followers for &amp;quot;think&amp;quot;. We
believe that by storing information on
moderate frequency words with strongly
associated followers and on clusters of
verb forms we may significantly improve
the success of the first menu.
</bodyText>
<sectionHeader confidence="0.890338" genericHeader="related work">
RELATED WORK
</sectionHeader>
<bodyText confidence="0.999771921052632">
That a small number of words account
for a large proportion of the total ver—
biage in conversation has been known for
some time [Kucera and Francis, 1967].
The idca of using the first several
letters typed by a handicapped individual
to anticipate the next desired word has
been used in numerous systems (e.g.,
[Gibler and Childress, 1982], [Pickering
et al., 1984]). The Gibler and Childress
system is typical in that it uses a few—
thousand—word vocabulary drawn from the
general public, plus a few hundred words
specific to the user of the system. The
user must type the first two letters
before the system provides a menu of
words beginning with the letter pair. If
the desired word was not on the menu, the
user had to spell the word out. It was
felt that one letter was not informative
enough to warrant a menu. Furthermore,
Gilbler and Childress showed that increas—
ing the system vocabulary degraded the
performance of their system and they
recommended limitation of the vocabulary
for human factors reasons.
By contrast, our system costs the
user no more effort in terms of selecting
the first two letters — if indeed they
have needed to go that far; 80 per cent
of the time, they haven&apos;t needed to pro—
vide two letters. Further, there is no
question that for our system, allowing the
vocabulary to grow is of benefit both to
system performance and to user satis—
faction.
Galliers [1987] describes a different
approach for physically handicapped
</bodyText>
<page confidence="0.997898">
39
</page>
<bodyText confidence="0.999565414634146">
persons conversant in the Bliss communi—
cations system. Communication with Bliss
involves a high degree of interpretation
by the &amp;quot;listener&amp;quot;, and Galliers reports an
impressive 75 per cent success rate in
automating such interpretation. The
Galliers system is single—subject, as ours
is, and it does use past history to
facilitate interpretation. It was, how—
ever, limited to a very small domain for
the experiment described.
One statistic cited by this last paper
was that the same text produced from the
Bliss communication, had it been produced
by typing into a word processing system,
would have required three times as many
key—press operations. Our own ratio of
key—press operations to characters
produced was 45 per cent for the sentence
position algorithm. That is, on average
it took 45 presses of a mouse button to
produce 100 characters. Part of the
reason for such a high ratio has to do
with punctuation, capitalization, and
special screens such as the number screen,
which requires not only the same number of
presses of the button as there are digits,
for example, but additional presses of the
button to summon the screen and quit the
menu. But primarily the ratio seems to
derive from the fact that many of the
words in any text are short —
the, of, &amp;quot;in&amp;quot;, and on being examples
from this very paragraph. If the first
menu does not contain a desired two—letter
word, one has to spell the first letter
and then make a selection from the second
menu — requiring two presses of a button.
By contrast, Bliss users commonly use
a telegraphic style of communication and
omit function words altogether.
</bodyText>
<sectionHeader confidence="0.995517" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999936966666667">
In summary, evidence exists that for
a system built around a single user&apos;s
language, a prediction scheme that simply
anticipated fifty or so words would on
average be correct about half the time.
Limiting such a system to only the top 20
most frequent words would give a success
rate of about 30 per cent. However, not
all of the high frequency words are dis—
tributed evenly by sentence position. A
system that offers the top 20 most fre—
quently occurring words for each position
of a sentence was successful about 40 per
cent of the time on the next 97 sentences.
Allowing a user to reject the first set of
words by giving the first letter of the
desired word and offering the 20 most
frequent words beginning with that letter
resulted in success for the combined first
and second menus 82 per cent of the time.
After a training body of 1750 sen—
tences (14,669 words), with a vocabulary
of 1512 words, it was still the case that
about six per cent of the desired words
were unknown to the system.
An alternative algorithm for the first
offering of 20 words, based primarily on
the right hand contexts of the high fre—
quency words, is successful on the first
guess 50 per cent of the time.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999554378378378">
Boggess, Lois and Thomas M. English, The
HWYE speech recognition system: a user—
specific model for expectation—based
recognition, in Proceedings of the 25th
Southeast Regional Conference of the
ACM, Birmingham, 1987.
Chow, C. L. A mouse—driven menu—based
text prosthesis for the speech
handicapped, M.C.S. project report,
Mississippi State University, 1986.
English, T. M. and Lois Boggess, A gram—
matical approach to reducing the statis—
tical sparsity of language models in nat—
ural domains, Proceedings of the Inter—
national Conference on Acoustics. Speech,
and Signal Processing, Tokyo, 1986.
Galliers, Julia, Al for special needs —
an &amp;quot;intelligent&amp;quot; communication aid for
Bliss users, Applied Artificial
Intelligence, 1(1):77-86, 1987.
Gibler, D. C. and D. S. Childress, Lan—
guage anticipation with a computer based
scanning aid, Proceedings of the IEEE
Computer Workshop on Computers
to Aid the Handicapped., 1982.
Kucera, H. and W. N. Francis, Computa—
tional analysis of present—day American
English. Brown University Press, 1967.
Pickering, J., J. L. Arnott, J. G. Wolff,
and A. L. Swif fin, Prediction and adap—
tation in a communication aid for the
disabled, Proceedings of the IFIP
Conference on Human—Computer
Interaction, London, 1984.
Wei, Jan—Soong, File organization of
Sherri System, M.C.S. project report,
Mississippi State University, 1987.
</reference>
<page confidence="0.998645">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.853119">
<title confidence="0.9692335">TWO SIMPLE PREDICTION ALGORITHMS TO FACILITATE TEXT PRODUCTION</title>
<author confidence="0.977748">Lois Boggess</author>
<affiliation confidence="0.95882">P.O. Drawer CS Mississippi State University</affiliation>
<address confidence="0.999436">Mississippi State, MS 39762</address>
<abstract confidence="0.999664">Several simple prediction schemes are presented for systems intended to facili— tate text production for handicapped individuals. The schemes are based on single—subject language models, where the system is self—adapting to the past language use of the subject. Sentence position, the immediately preceding one or two words, and initial letters of the desired word are cues which may be used by the systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lois Boggess</author>
<author>Thomas M English</author>
</authors>
<title>The HWYE speech recognition system: a user— specific model for expectation—based recognition,</title>
<date>1987</date>
<booktitle>in Proceedings of the 25th Southeast Regional Conference of the ACM,</booktitle>
<location>Birmingham,</location>
<marker>Boggess, English, 1987</marker>
<rawString>Boggess, Lois and Thomas M. English, The HWYE speech recognition system: a user— specific model for expectation—based recognition, in Proceedings of the 25th Southeast Regional Conference of the ACM, Birmingham, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Chow</author>
</authors>
<title>A mouse—driven menu—based text prosthesis for the speech handicapped, M.C.S. project report,</title>
<date>1986</date>
<institution>Mississippi State University,</institution>
<marker>Chow, 1986</marker>
<rawString>Chow, C. L. A mouse—driven menu—based text prosthesis for the speech handicapped, M.C.S. project report, Mississippi State University, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M English</author>
<author>Lois Boggess</author>
</authors>
<title>A gram— matical approach to reducing the statis— tical sparsity of language models in nat— ural domains,</title>
<date>1986</date>
<booktitle>Proceedings of the Inter— national Conference on Acoustics. Speech, and Signal Processing,</booktitle>
<location>Tokyo,</location>
<contexts>
<context position="2272" citStr="English and Boggess, 1986" startWordPosition="365" endWordPosition="368"> requirements. MOTIVATION One hundred English words account for 47 per cent of the Brown corpus (about one million words of American English text taken from a wide range of sources). It seems reasonable to suppose that a single individual might in fact require fewer words to account for a large proportion of generated text. From our work on the HWYE system it was known that 75 words accounted for half of all the text of Vanity Fair, a 300,000 word Victorian English novel by Thackeray (which incorporated a fairly involved syntax, much embedded quotation, and passages in dialect and in French) [English and Boggess, 1986]. We further found that 50 words accounted for half of all the verbiage in a 20,000 word set of sentences provided by an individual who collaborated with us. This latter corpus, called the Sherri data, is a set of texts provided by a speech—handicapped individual who uses a typewriter to communicate, even with her family; it is conversational in nature, as can be seen in Figure 1. Most of the work reported in this paper gives special attention to the set of words required to account for half of all the verbiage of a given individual. We refer to this set as the set of high—frequency words. 33</context>
<context position="12138" citStr="English and Boggess, 1986" startWordPosition="2149" endWordPosition="2152">ssible states in a trigram Markov model is 4,000,000, with more than 8 billion arcs intercon— necting the states. Fortunately, in the real world most of these mathematically possible states and arcs do not actually occur, but a trigram model for the real world possibilities is still quite large. We experimented with abstracting the input vocabulary by restricting it to the 50 highest—frequency words plus the pseudo—input OTHER onto which all other words were mapped. When we did so, the number of states and arcs in the various order Markov models was still fairly large for the real world data [English and Boggess, 1986]. As Figure 6 shows, for example, the rate of growth for a fourth— order abstract Markov model (just the 50 highest—frequency words plus OTHER plus end—of—sentence) is in the neighborhood of 250 new states and 450 new arcs per 1000 111 the we It it.. of Son ,so Clike_ 36 wor ds Sherri data Thackeray data new states new arcs new states new arcs 1000 527 677 639 830 2000 469 620 624 818 3000 471 636 476 705 4000 399 562 467 716 5000 397 566 463 714 6000 391 579 437 668 7000 337 507 389 642 8000 311 476 370 628 9000 323 500 361 612 10000 285 486 384 629 11000 329 518 348 601 12000 278 448 331 58</context>
</contexts>
<marker>English, Boggess, 1986</marker>
<rawString>English, T. M. and Lois Boggess, A gram— matical approach to reducing the statis— tical sparsity of language models in nat— ural domains, Proceedings of the Inter— national Conference on Acoustics. Speech, and Signal Processing, Tokyo, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Galliers</author>
</authors>
<title>Al for special needs — an &amp;quot;intelligent&amp;quot; communication aid for Bliss users,</title>
<date>1987</date>
<journal>Applied Artificial Intelligence,</journal>
<pages>1--1</pages>
<marker>Galliers, 1987</marker>
<rawString>Galliers, Julia, Al for special needs — an &amp;quot;intelligent&amp;quot; communication aid for Bliss users, Applied Artificial Intelligence, 1(1):77-86, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Gibler</author>
<author>D S Childress</author>
</authors>
<title>Lan— guage anticipation with a computer based scanning aid,</title>
<date>1982</date>
<booktitle>Proceedings of the IEEE Computer Workshop on Computers to Aid the Handicapped.,</booktitle>
<contexts>
<context position="3865" citStr="Gibler and Childress, 1982" startWordPosition="655" endWordPosition="658">looks like it might be now. Figure 1. Sample set of contiguous sentences in Sherri data It seems reasonable to suppose that for conversational English, approximately 50 words may account for half of the verbiage of most English users. From the standpoint of human factors, an argument could be made that one should simply put the 50 words up on the screen with the alphabet and thus be assured that half of all the words desired by the user were instantly available, in known locations that the user would quickly become accustomed to. Constantly changing menus introduce an element of user fatigue [Gibler and Childress, 1982]. That argument may especially make sense as larger screens with more lines per screen and more characters per line become more common. If we limit ourselves to the top 20 most frequent words as a constant menu, only about 30 per cent of the user&apos;s verbiage is accounted for. However, it was observed, while working with the HWYE system, that if one looked at the top 20 words for any given sentence position, one did not see the same set of words occurring. Clearly the high frequency words (the set that comprise half of word use) are mildly sensitive to &apos;context&amp;quot; even when &amp;quot;context&amp;quot; is so broadl</context>
<context position="22620" citStr="Gibler and Childress, 1982" startWordPosition="3972" endWordPosition="3975">hought&amp;quot; in the Sherri data happens to belong to the set of followers for &amp;quot;think&amp;quot;. We believe that by storing information on moderate frequency words with strongly associated followers and on clusters of verb forms we may significantly improve the success of the first menu. RELATED WORK That a small number of words account for a large proportion of the total ver— biage in conversation has been known for some time [Kucera and Francis, 1967]. The idca of using the first several letters typed by a handicapped individual to anticipate the next desired word has been used in numerous systems (e.g., [Gibler and Childress, 1982], [Pickering et al., 1984]). The Gibler and Childress system is typical in that it uses a few— thousand—word vocabulary drawn from the general public, plus a few hundred words specific to the user of the system. The user must type the first two letters before the system provides a menu of words beginning with the letter pair. If the desired word was not on the menu, the user had to spell the word out. It was felt that one letter was not informative enough to warrant a menu. Furthermore, Gilbler and Childress showed that increas— ing the system vocabulary degraded the performance of their syst</context>
</contexts>
<marker>Gibler, Childress, 1982</marker>
<rawString>Gibler, D. C. and D. S. Childress, Lan— guage anticipation with a computer based scanning aid, Proceedings of the IEEE Computer Workshop on Computers to Aid the Handicapped., 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kucera</author>
<author>W N Francis</author>
</authors>
<title>Computa— tional analysis of present—day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<contexts>
<context position="22435" citStr="Kucera and Francis, 1967" startWordPosition="3942" endWordPosition="3945">has a well developed set of followers. &amp;quot;Thinks&amp;quot; and &amp;quot;thought&amp;quot; are not high—frequency and hence are followed by the default first menu. Yet virtually every follower for &amp;quot;thinks&amp;quot; and &amp;quot;thought&amp;quot; in the Sherri data happens to belong to the set of followers for &amp;quot;think&amp;quot;. We believe that by storing information on moderate frequency words with strongly associated followers and on clusters of verb forms we may significantly improve the success of the first menu. RELATED WORK That a small number of words account for a large proportion of the total ver— biage in conversation has been known for some time [Kucera and Francis, 1967]. The idca of using the first several letters typed by a handicapped individual to anticipate the next desired word has been used in numerous systems (e.g., [Gibler and Childress, 1982], [Pickering et al., 1984]). The Gibler and Childress system is typical in that it uses a few— thousand—word vocabulary drawn from the general public, plus a few hundred words specific to the user of the system. The user must type the first two letters before the system provides a menu of words beginning with the letter pair. If the desired word was not on the menu, the user had to spell the word out. It was fe</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>Kucera, H. and W. N. Francis, Computa— tional analysis of present—day American English. Brown University Press, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pickering</author>
<author>J L Arnott</author>
<author>J G Wolff</author>
<author>A L</author>
</authors>
<title>Swif fin, Prediction and adap— tation in a communication aid for the disabled,</title>
<date>1984</date>
<booktitle>Proceedings of the IFIP Conference on Human—Computer Interaction,</booktitle>
<location>London,</location>
<contexts>
<context position="22646" citStr="Pickering et al., 1984" startWordPosition="3976" endWordPosition="3979">pens to belong to the set of followers for &amp;quot;think&amp;quot;. We believe that by storing information on moderate frequency words with strongly associated followers and on clusters of verb forms we may significantly improve the success of the first menu. RELATED WORK That a small number of words account for a large proportion of the total ver— biage in conversation has been known for some time [Kucera and Francis, 1967]. The idca of using the first several letters typed by a handicapped individual to anticipate the next desired word has been used in numerous systems (e.g., [Gibler and Childress, 1982], [Pickering et al., 1984]). The Gibler and Childress system is typical in that it uses a few— thousand—word vocabulary drawn from the general public, plus a few hundred words specific to the user of the system. The user must type the first two letters before the system provides a menu of words beginning with the letter pair. If the desired word was not on the menu, the user had to spell the word out. It was felt that one letter was not informative enough to warrant a menu. Furthermore, Gilbler and Childress showed that increas— ing the system vocabulary degraded the performance of their system and they recommended li</context>
</contexts>
<marker>Pickering, Arnott, Wolff, L, 1984</marker>
<rawString>Pickering, J., J. L. Arnott, J. G. Wolff, and A. L. Swif fin, Prediction and adap— tation in a communication aid for the disabled, Proceedings of the IFIP Conference on Human—Computer Interaction, London, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan—Soong Wei</author>
</authors>
<title>File organization of Sherri System,</title>
<date>1987</date>
<tech>M.C.S. project report,</tech>
<institution>Mississippi State University,</institution>
<marker>Wei, 1987</marker>
<rawString>Wei, Jan—Soong, File organization of Sherri System, M.C.S. project report, Mississippi State University, 1987.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>