<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019843">
<title confidence="0.997702">
An Active Learning Approach to Finding Related Terms
</title>
<author confidence="0.988231">
David Vickrey Oscar Kipersztok Daphne Koller
</author>
<affiliation confidence="0.983408">
Stanford University Boeing Research &amp; Technology Stanford Univeristy
</affiliation>
<email confidence="0.919134">
dvickrey@cs.stanford.edu oscar.kipersztok koller@cs.stanford.edu
@boeing.com
</email>
<sectionHeader confidence="0.991319" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967136363637">
We present a novel system that helps non-
experts find sets of similar words. The
user begins by specifying one or more seed
words. The system then iteratively sug-
gests a series of candidate words, which
the user can either accept or reject. Cur-
rent techniques for this task typically boot-
strap a classifier based on a fixed seed
set. In contrast, our system involves
the user throughout the labeling process,
using active learning to intelligently ex-
plore the space of similar words. In
particular, our system can take advan-
tage of negative examples provided by the
user. Our system combines multiple pre-
existing sources of similarity data (a stan-
dard thesaurus, WordNet, contextual sim-
ilarity), enabling it to capture many types
of similarity groups (“synonyms of crash,”
“types of car,” etc.). We evaluate on a
hand-labeled evaluation set; our system
improves over a strong baseline by 36%.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986555555556">
Set expansion is a well-studied NLP problem
where a machine-learning algorithm is given a
fixed set of seed words and asked to find additional
members of the implied set. For example, given
the seed set {“elephant,” “horse,” “bat”J, the al-
gorithm is expected to return other mammals. Past
work, e.g. (Roark &amp; Charniak, 1998; Ghahramani
&amp; Heller, 2005; Wang &amp; Cohen, 2007; Pantel
et al., 2009), generally focuses on semi-automatic
acquisition of the remaining members of the set by
mining large amounts of unlabeled data.
State-of-the-art set expansion systems work
well for well-defined sets of nouns, e.g. “US Pres-
idents,” particularly when given a large seed set.
Set expansions is more difficult with fewer seed
words and for other kinds of sets. The seed words
may have multiple senses and the user may have in
mind a variety of attributes that the answer must
match. For example, suppose the seed word is
“jaguar”. First, there is sense ambiguity; we could
be referring to either a “large cat” or a “car.” Be-
yond this, we might have in mind various more (or
less) specific groups: “Mexican animals,” “preda-
tors,” “luxury cars,” “British cars,” etc.
We propose a system which addresses sev-
eral shortcomings of many set expansion systems.
First, these systems can be difficult to use. As ex-
plored by Vyas et al. (2009), non-expert users
produce seed sets that lead to poor quality expan-
sions, for a variety of reasons including ambiguity
and lack of coverage. Even for expert users, con-
structing seed sets can be a laborious and time-
consuming process. Second, most set expansion
systems do not use negative examples, which can
be very useful for weeding out other bad answers.
Third, many set expansion systems concentrate on
noun classes such as “US Presidents” and are not
effective or do not apply to other kinds of sets.
Our system works as follows. The user initially
thinks of at least one seed word belonging to the
desired set. One at a time, the system presents can-
didate words to the user and asks whether the can-
didate fits the concept. The user’s answer is fed
back into the system, which takes into account this
new information and presents a new candidate to
the user. This continues until the user is satisfied
with the compiled list of “Yes” answers. Our sys-
tem uses both positive and negative examples to
guide the search, allowing it to recover from ini-
tially poor seed words. By using multiple sources
of similarity data, our system captures a variety of
kinds of similarity. Our system replaces the poten-
tially difficult problem of thinking of many seed
words with the easier task of answering yes/no
questions. The downside is a possibly increased
amount of user interaction (although standard set
expansion requires a non-trivial amount of user in-
teraction to build the seed set).
There are many practical uses for such a sys-
tem. Building a better, more comprehensive the-
saurus/gazetteer is one obvious application. An-
other application is in high-precision query expan-
sion, where a human manually builds a list of ex-
</bodyText>
<page confidence="0.983545">
371
</page>
<note confidence="0.5066755">
Proceedings of the ACL 2010 Conference Short Papers, pages 371–376,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999385">
pansion terms. Suppose we are looking for pages
discussing “public safety.” Then synonyms (or
near-synonyms) of “safety” would be useful (e.g.
“security”) but also non-synonyms such as “pre-
cautions” or “prevention” are also likely to return
good results. In this case, the concept we are inter-
ested in is “Words which imply that safety is being
discussed.” Another interesting direction not pur-
sued in this paper is using our system as part of
a more-traditional set expansion system to build
seed sets more quickly.
</bodyText>
<sectionHeader confidence="0.928704" genericHeader="method">
2 Set Expansion
</sectionHeader>
<bodyText confidence="0.997099857142857">
As input, we are provided with a small set of seed
words s. The desired output is a target set of
words G, consisting of all words that fit the de-
sired concept. A particular seed set s can belong
to many possible goal sets G, so additional infor-
mation may be required to do well.
Previous work tries to do as much as possible
using only s. Typically s is assumed to contain at
least 2 words and often many more. Pantel et al.
(2009) discusses the issue of seed set size in detail,
concluding that 5-20 seed words are often required
for good performance.
There are several problems with the fixed seed
set approach. It is not always easy to think of
even a single additional seed word (e.g., the user is
trying to find “German automakers” and can only
think of “Volkswagen”). Even if the user can think
of additional seed words, time and effort might be
saved by using active learning to find good sug-
gestions. Also, as Vyas et al. (2009) show, non-
expert users often produce poor-quality seed sets.
</bodyText>
<sectionHeader confidence="0.964493" genericHeader="method">
3 Active Learning System
</sectionHeader>
<bodyText confidence="0.998582254237288">
Any system for this task relies on information
about similarity between words. Our system takes
as input a rectangular matrix M. Each column
corresponds to a particular word. Each row cor-
responds to a unique dimension of similarity; the
jth entry in row i mij is a number between 0 and
1 indicating the degree to which wj belongs to the
ith
sions include “How similar is word wj to the verb
jump?” “Is wj a type of cat?” and “Are the words
which appear in the context of wj similar to those
that appear in the context of boat?” Each row ri
of M is labeled with a word li. This may follow
intuitively from the similarity axis (e.g., “jump,”
“cat,” and “boat”, respectively), or it can be gen-
erated automatically (e.g. the word wj with the
highest membership mij).
Let 0 be a vector of weights, one per row, which
correspond to how well each row aligns with the
goal set G. Thus, 0i should be large and positive if
row i has large entries for positive but not negative
examples; and it should be large and negative if
row i has large entries for negative but not positive
examples. Suppose that we have already chosen
an appropriate weight vector 0. We wish to rank
all possible words (i.e., the columns of M) so that
the most promising word gets the highest score.
A natural way to generate a score zj for column
j is to take the dot product of 0 with column j,
zj = Ei 0imij. This rewards word wj for having
high membership in rows with positive 0, and low
membership in rows with negative 0.
Our system uses a “batch” approach to active
learning. At iteration i, it chooses a new 0 based
on all data labeled so far (for the 1st iteration,
this data consists of the seed set s). It then
chooses the column (word) with the highest score
(among words not yet labeled) as the candidate
word wi. The user answers “Yes” or “No,” indicat-
ing whether or not wi belongs to G. wi is added
to the positive set p or the negative set n based
on the user’s answer. Thus, we have a labeled data
set that grows from iteration to iteration as the user
labels each candidate word. Unlike set expansion,
this procedure generates (and uses) both positive
and negative examples.
We explore two options for choosing 0. Recall
that each row i is associated with a label li. The
first method is to set 0i = 1 if li E p (that is, the
set of positively labeled words includes label li),
0i = −1 if li E n, and 0i = 0 otherwise. We
refer to this method as “Untrained”, although it is
still adaptive — it takes into account the labeled
examples the user has provided so far.
The second method uses a standard machine
learning algorithm, logistic regression. As be-
fore, the final ranking over words is based on the
score zj. However, zj is passed through the lo-
gistic function to produce a score between 0 and
</bodyText>
<equation confidence="0.751686">
1, z� = 1
</equation>
<bodyText confidence="0.883916666666667">
1+e��j . We can interpret this score
as the probability that wj is a positive example,
Pθ(Y |wj). This leads to the objective function
</bodyText>
<equation confidence="0.994287">
L(0) = log( � Pθ(Y |wj) � (1−Pθ(Y |wj)))-
</equation>
<bodyText confidence="0.932831375">
wjEp wjEn
This objective is convex and can be optimized us-
ing standard methods such as L-BFGS (Liu &amp; No-
cedal, 1989). Following standard practice we add
an L2 regularization term −θ� θ
2σ2 to the objective.
This method does not use the row labels li.
similarity group. Possible similarity dimen-
</bodyText>
<page confidence="0.971201">
372
</page>
<table confidence="0.99974">
Data Word Similar words
Moby arrive accomplish, achieve, achieve success, advance, appear, approach, arrive at, arrive in, attain,...
WordNet factory (plant,-1.9);(arsenal,-2.8);(mill,-2.9);(sweatshop,-4.1);(refinery,-4.2);(winery,-4.5);...
DistSim watch (jewerly,.137),(wristwatch,.115),(shoe,0.09),(appliance,0.09),(household appliance,0.089),...
</table>
<tableCaption confidence="0.999649">
Table 1: Examples of unprocessed similarity entries from each data source.
</tableCaption>
<sectionHeader confidence="0.97498" genericHeader="method">
4 Data Sources
</sectionHeader>
<bodyText confidence="0.999968093023256">
We consider three similarity data sources: the
Moby thesaurus1, WordNet (Fellbaum, 1998), and
distributional similarity based on a large corpus
of text (Lin, 1998). Table 1 shows similarity lists
from each. These sources capture different kinds
of similarity information, which increases the rep-
resentational power of our system. For all sources,
the similarity of a word with itself is set to 1.0.
It is worth noting that our system is not strictly
limited to choosing from pre-existing groups. For
example, if we have a list of luxury items, and an-
other list of cars, our system can learn weights so
that it prefers items in the intersection, luxury cars.
Moby thesaurus consists of a list of word-
based thesaurus entries. Each word wi has a list of
similar words simij. Moby has a total of about 2.5
million related word pairs. Unlike some other the-
sauri (such as WordNet and thesaurus.com), en-
tries are not broken down by word sense.
In the raw format, the similarity relation is not
symmetric; for example, there are many words
that occur only in similarity lists but do not have
their own entries. We augmented the thesaurus to
make it symmetric: if “dog” is in the similarity en-
try for “cat,” we add “cat” to the similarity entry
for “dog” (creating an entry for “dog” if it does not
exist yet). We then have a row i for every similar-
ity entry in the augmented thesaurus; mij is 1 if
wj appears in the similarity list of wi, and 0 other-
wise. The label li of row i is simply word wi. Un-
like some other thesauri (including WordNet and
thesaurus.com), the entries are not broken down
by word sense or part of speech. For polysemic
words, there will be a mix of the words similar to
each sense and part of speech.
WordNet is a well-known dictionary/thesaurus/
ontology often used in NLP applications. It con-
sists of a large number of synsets; a synset is a set
of one or more similar word senses. The synsets
are then connected with hypernym/hyponym links,
which represent IS-A relationships. We focused
on measuring similarity in WordNet using the hy-
pernym hierarchy.2. There are many methods for
</bodyText>
<footnote confidence="0.996981666666667">
1Available at icon.shef.ac.uk/Moby/.
2A useful similarity metric we did not explore in this pa-
per is similarity between WordNet dictionary definitions
</footnote>
<bodyText confidence="0.992488935483871">
converting this hierarchy into a similarity score;
we chose to use the Jiang-Conrath distance (Jiang
&amp; Conrath, 1997) because it tends to be more ro-
bust to the exact structure of WordNet. The num-
ber of types of similarity in WordNet tends to be
less than that captured by Moby, because synsets
in WordNet are (usually) only allowed to have a
single parent. For example, “murder” is classified
as a type of killing, but not as a type of crime.
The Jiang-Conrath distance gives scores for
pairs of word senses, not pairs of words. We han-
dle this by adding one row for every word sense
with the right part of speech (rather than for ev-
ery word); each row measures the similarity of ev-
ery word to a particular word sense. The label of
each row is the (undisambiguated) word; multiple
rows can have the same label. For the columns, we
do need to collapse the word senses into words;
for each word, we take a maximum across all of
its senses. For example, to determine how similar
(the only sense of) “factory” is to the word “plant,”
we compute the similarity of “factory” to the “in-
dustrial plant” sense of “plant” and to the “living
thing” sense of “plant” and take the higher of the
two (in this case, the former).
The Jiang-Conrath distance is a number be-
tween −oc and 0. By examination, we determined
that scores below −12.0 indicate virtually no sim-
ilarity. We cut off scores below this point and
linearly mapped each score x to the range 0 to
1, yielding a final similarity of min(0,x+12)
</bodyText>
<sectionHeader confidence="0.406606" genericHeader="method">
12 .This
</sectionHeader>
<bodyText confidence="0.990165733333333">
greatly sparsified the similarity matrix M.
Distributional similarity. We used Dekang
Lin’s dependency-based thesaurus, available at
www.cs.ualberta.ca/˜lindek/downloads.htm.
This resource groups words based on the words
they co-occur with in normal text. The words
most similar to “cat” are “dog,” “animal,” and
“monkey,” presumably because they all “eat,”
“walk,” etc. Like Moby, similarity entries are not
divided by word sense; usually, only the dominant
sense of each word is represented. This type of
similarity is considerably different from the other
two types, tending to focus less on minor details
and more on broad patterns.
Each similarity entry corresponds to a single
</bodyText>
<page confidence="0.998442">
373
</page>
<bodyText confidence="0.999921090909091">
word wz and is a list of scored similar words sim�.
The scores vary between 0 and 1, but usually the
highest-scored word in a similarity list gets a score
of no more than 0.3. To calibrate these scores
with the previous two types, we divided all scores
by the score of the highest-scored word in that
list. Since each row is normalized individually,
the similarity matrix M is not symmetric. Also,
there are separate similarity lists for each of nouns,
verbs, and modifiers; we only used the lists match-
ing the seed word’s part of speech.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999987246153847">
Given a seed set s and a complete target set G, it is
easy to evaluate our system; we say “Yes” to any-
thing in G, “No” to everything else, and see how
many of the candidate words are in G. However,
building a complete gold-standard G is in practice
prohibitively difficult; instead, we are only capa-
ble of saying whether or not a word belongs to G
when presented with that word.
To evaluate a particular active learning algo-
rithm, we can just run the algorithm manually, and
see how many candidate words we say “Yes” to
(note that this will not give us an accurate estimate
of the recall of our algorithm). Evaluating several
different algorithms for the same s and G is more
difficult. We could run each algorithm separately,
but there are several problems with this approach.
First, we might unconsciously (or consciously)
bias the results in favor of our preferred algo-
rithms. Second, it would be fairly difficult to be
consistent across multiple runs. Third, it would be
inefficient, since we would label the same words
multiple times for different algorithms.
We solved this problem by building a labeling
system which runs all algorithms that we wish to
test in parallel. At each step, we pick a random al-
gorithm and either present its current candidate to
the user or, if that candidate has already been la-
beled, we supply that algorithm with the given an-
swer. We do NOT ever give an algorithm a labeled
training example unless it actually asks for it – this
guarantees that the combined system is equivalent
to running each algorithm separately. This pro-
cedure has the property that the user cannot tell
which algorithms presented which words.
To evaluate the relative contribution of active
learning, we consider a version of our system
where active learning is disabled. Instead of re-
training the system every iteration, we train it once
on the seed set s and keep the weight vector 0 fixed
from iteration to iteration.
We evaluated our algorithms along three axes.
First, the method for choosing 0: Untrained and
Logistic (U and L). Second, the data sources used:
each source separately (M for Moby, W for Word-
Net, D for distributional similarity), and all three
in combination (MWD). Third, whether active
learning is used (+/-). Thus, logistic regression us-
ing Moby and no active learning is L(M,-). For lo-
gistic regression, we set the regularization penalty
Q2 to 1, based on qualitative analysis during devel-
opment (before seeing the test data).
We also compared the performance of our
algorithms to the popular online thesaurus
http://thesaurus.com. The entries in this
thesaurus are similar to Moby, except that each
word may have multiple sense-disambiguated en-
tries. For each seed word w, we downloaded the
page for w and extracted a set of synonyms en-
tries for that word. To compare fairly with our al-
gorithms, we propose a word-by-word method for
exploring the thesaurus, intended to model a user
scanning the thesaurus. This method checks the
first 3 words from each entry; if none of these are
labeled “Yes,” it moves on to the next entry. We
omit details for lack of space.
</bodyText>
<sectionHeader confidence="0.990306" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999973653846154">
We designed a test set containing different types
of similarity. Table 2 shows each category, with
examples of specific similarity queries. For each
type, we tested on five different queries. For each
query, the first author built the seed set by writ-
ing down the first three words that came to mind.
For most queries this was easy. However, for the
similarity type Hard Synonyms, coming up with
more than one seed word was considerably more
difficult. To build seed sets for these queries, we
ran our evaluation system using a single seed word
and took the first two positive candidates; this en-
sured that we were not biasing our seed set in favor
of a particular algorithm or data set.
For each query, we ran our evaluation system
until each algorithm had suggested 25 candidate
words, for a total of 625 labeled words per algo-
rithm. We measured performance using mean av-
erage precision (MAP), which corresponds to area
under the precision-recall curve. It gives an over-
all assessment across different stopping points.
Table 3 shows results for an informative sub-
set of the tested algorithms. There are many con-
clusions we can draw. Thesaurus.Com performs
poorly overall; our best system, L(MWD,+),
outscores it by 164%. The next group of al-
</bodyText>
<page confidence="0.999131">
374
</page>
<tableCaption confidence="0.993593">
Table 2: Categories and examples
</tableCaption>
<table confidence="0.9996855">
SG CG Syn HS M
Thesaurus.Com .041 .060 .275 .173 .060
L(D,+) .377 .344 .211 .329 .177
L(M,-) .102 .118 .393 .279 .119
U(W,+) .097 .136 .296 .277 .165
U(MWD,+) .194 .153 .438 .357 .213
L(MWD,-) .344 .207 .360 .345 .173
L(MWD,+) .366 .335 .379 .372 .158
</table>
<tableCaption confidence="0.99982">
Table 4: Results by category
</tableCaption>
<bodyText confidence="0.99994434375">
gorithms, U(*,-), add together the similarity en-
tries of the seed words for a particular similarity
source. The best of these uses distributional simi-
larity; L(MWD,+) outscores it by 53%. Combin-
ing all similarity types, U(MWD,-) improves by
10% over U(D,-). L(MWD,+) improves over the
best single-source, L(D,+), by a similar margin.
Using logistic regression instead of the un-
trained weights significantly improves perfor-
mance. For example, L(MWD,+) outscores
U(MWD,+) by 19%. Using active learning also
significantly improves performance: L(MWD,+)
outscores L(MWD,-) by 13%. This shows that
active learning is useful even when a reasonable
amount of initial information is available (three
seed words for each test case). The gains from
logistic regression and active learning are cumula-
tive; L(MWD,+) outscores U(MWD,-) by 38%.
Finally, our best system, L(MWD,+) improves
over L(D,-), the best system using a single data
source and no active learning, by 36%. We con-
sider L(D,-) to be a strong baseline; this compari-
son demonstrates the usefulness of the main con-
tributions of this paper, the use of multiple data
sources and active learning. L(D,-) is still fairly
sophisticated, since it combines information from
the similarity entries for different words.
Table 4 shows the breakdown of results by cat-
egory. For this chart, we chose the best set-
ting for each similarity type. Broadly speaking,
the thesauri work reasonably well for synonyms,
but poorly for groups. Meronyms were difficult
</bodyText>
<table confidence="0.999132727272727">
Algorithm MAP
Thesaurus.Com .122
U(M,-) .176
U(W,-) .182
U(D,-) .211
L(D,-) .236
L(D,+) .288
U(MWD,-) .233
U(MWD,+) .271
L(MWD,-) .286
L(MWD,+) .322
</table>
<tableCaption confidence="0.999945">
Table 3: Comparison of algorithms
</tableCaption>
<bodyText confidence="0.999906222222222">
across the board. Neither logistic regression nor
active learning always improved performance, but
L(MWD,+) performs near the top for every cate-
gory. The complex groups category is particularly
interesting, because achieving high performance
on this category required using both logistic re-
gression and active learning. This makes sense
since negative evidence is particularly important
for this category.
</bodyText>
<sectionHeader confidence="0.994536" genericHeader="discussions">
7 Discussion and Related Work
</sectionHeader>
<bodyText confidence="0.999896482758621">
The biggest difference between our system and
previous work is the use of active learning, espe-
cially in allowing the use of negative examples.
Most previous set expansion systems use boot-
strapping from a small set of positive examples.
Recently, the use of negative examples for set ex-
pansion was proposed by Vyas and Pantel (2009),
although in a different way. First, set expansion is
run as normal using a fixed seed set. Then, human
annotators label a small number of negative exam-
ples from the returned results, which are used to
weed out other bad answers. Our method incorpo-
rates negative examples at an earlier stage. Also,
we use a logistic regression model to robustly in-
corporate negative information, rather than deter-
ministically ruling out words and features.
Our system is limited by our data sources. Sup-
pose we want actors who appeared in Star Wars. If
we only know that Harrison Ford and Mark Hamill
are actors, we have little to go on. There has
been a large amount of work on other sources of
word-similarity. Hughes and Ramage (2007) use
random walks over WordNet, incorporating infor-
mation such as meronymy and dictionary glosses.
Snow et al. (2006) extract hypernyms from free
text. Wang and Cohen (2007) exploit web-page
structure, while Pasca and Durme (2008) exam-
ine query logs. We expect that adding these types
of data would significantly improve our system.
</bodyText>
<figure confidence="0.646392166666667">
Category Name Example Similarity Queries
Simple Groups (SG) car brands, countries, mammals, crimes
Complex Groups (CG) luxury car brands, sub-Saharan countries
Synonyms (Syn) syn of {scandal, helicopter, arrogant, slay}
Hard Synonyms (HS) syn of {(stock-market) crash, (legal) maneuver}
Meronym/Material (M) parts of a car, things made of wood
</figure>
<page confidence="0.995975">
375
</page>
<sectionHeader confidence="0.99559" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999477923076923">
Fellbaum, C. (Ed.). (1998). Wordnet: An elec-
tronic lexical database. MIT Press.
Ghahramani, Z., &amp; Heller, K. (2005). Bayesian
sets. Advances in Neural Information Process-
ing Systems (NIPS).
Hughes, T., &amp; Ramage, D. (2007). Lexical se-
mantic relatedness with random graph walks.
EMNLP-CoNLL.
Jiang, J., &amp; Conrath, D. (1997). Semantic similar-
ity based on corpus statistics and lexical taxon-
omy. Proceedings of International Conference
on Research in Computational Linguistics.
Lin, D. (1998). An information-theoretic defini-
tion of similarity. Proceedings of ICML.
Liu, D. C., &amp; Nocedal, J. (1989). On the lim-
ited memory method for large scale optimiza-
tion. Mathematical Programming B.
Pantel, P., Crestan, E., Borkovsky, A., Popescu,
A., &amp; Vyas, V. (2009). Web-scale distributional
similarity and entity set expansion. EMNLP.
Pasca, M., &amp; Durme, B. V. (2008). Weakly-
supervised acquisition of open-domain classes
and class attributes from web documents and
query logs. ACL.
Roark, B., &amp; Charniak, E. (1998). Noun-phrase
co-occurrence statistics for semiautomatic se-
mantic lexicon construction. ACL-COLING.
Snow, R., Jurafsky, D., &amp; Ng, A. (2006). Seman-
tic taxonomy induction from heterogenous evi-
dence. ACL.
Vyas, V., &amp; Pantel, P. (2009). Semi-automatic en-
tity set refinement. NAACL/HLT.
Vyas, V., Pantel, P., &amp; Crestan, E. (2009). Helping
editors choose better seed sets for entity expan-
sion. CIKM.
Wang, R., &amp; Cohen, W. (2007). Language-
independent set expansion of named entities us-
ing the web. Seventh IEEE International Con-
ference on Data Mining.
</reference>
<page confidence="0.999106">
376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.835820">
<title confidence="0.999979">An Active Learning Approach to Finding Related Terms</title>
<author confidence="0.999233">David Vickrey Oscar Kipersztok Daphne Koller</author>
<affiliation confidence="0.996873">Stanford University Boeing Research &amp; Technology Stanford Univeristy</affiliation>
<email confidence="0.9825605">dvickrey@cs.stanford.eduoscar.kipersztokkoller@cs.stanford.edu@boeing.com</email>
<abstract confidence="0.994288826086956">We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>Wordnet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9704" citStr="Fellbaum, 1998" startWordPosition="1653" endWordPosition="1654">s method does not use the row labels li. similarity group. Possible similarity dimen372 Data Word Similar words Moby arrive accomplish, achieve, achieve success, advance, appear, approach, arrive at, arrive in, attain,... WordNet factory (plant,-1.9);(arsenal,-2.8);(mill,-2.9);(sweatshop,-4.1);(refinery,-4.2);(winery,-4.5);... DistSim watch (jewerly,.137),(wristwatch,.115),(shoe,0.09),(appliance,0.09),(household appliance,0.089),... Table 1: Examples of unprocessed similarity entries from each data source. 4 Data Sources We consider three similarity data sources: the Moby thesaurus1, WordNet (Fellbaum, 1998), and distributional similarity based on a large corpus of text (Lin, 1998). Table 1 shows similarity lists from each. These sources capture different kinds of similarity information, which increases the representational power of our system. For all sources, the similarity of a word with itself is set to 1.0. It is worth noting that our system is not strictly limited to choosing from pre-existing groups. For example, if we have a list of luxury items, and another list of cars, our system can learn weights so that it prefers items in the intersection, luxury cars. Moby thesaurus consists of a l</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (Ed.). (1998). Wordnet: An electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Ghahramani</author>
<author>K Heller</author>
</authors>
<title>Bayesian sets.</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="1510" citStr="Ghahramani &amp; Heller, 2005" startWordPosition="231" endWordPosition="234">y data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%. 1 Introduction Set expansion is a well-studied NLP problem where a machine-learning algorithm is given a fixed set of seed words and asked to find additional members of the implied set. For example, given the seed set {“elephant,” “horse,” “bat”J, the algorithm is expected to return other mammals. Past work, e.g. (Roark &amp; Charniak, 1998; Ghahramani &amp; Heller, 2005; Wang &amp; Cohen, 2007; Pantel et al., 2009), generally focuses on semi-automatic acquisition of the remaining members of the set by mining large amounts of unlabeled data. State-of-the-art set expansion systems work well for well-defined sets of nouns, e.g. “US Presidents,” particularly when given a large seed set. Set expansions is more difficult with fewer seed words and for other kinds of sets. The seed words may have multiple senses and the user may have in mind a variety of attributes that the answer must match. For example, suppose the seed word is “jaguar”. First, there is sense ambiguit</context>
</contexts>
<marker>Ghahramani, Heller, 2005</marker>
<rawString>Ghahramani, Z., &amp; Heller, K. (2005). Bayesian sets. Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hughes</author>
<author>D Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<tech>EMNLP-CoNLL.</tech>
<contexts>
<context position="22465" citStr="Hughes and Ramage (2007)" startWordPosition="3816" endWordPosition="3819"> human annotators label a small number of negative examples from the returned results, which are used to weed out other bad answers. Our method incorporates negative examples at an earlier stage. Also, we use a logistic regression model to robustly incorporate negative information, rather than deterministically ruling out words and features. Our system is limited by our data sources. Suppose we want actors who appeared in Star Wars. If we only know that Harrison Ford and Mark Hamill are actors, we have little to go on. There has been a large amount of work on other sources of word-similarity. Hughes and Ramage (2007) use random walks over WordNet, incorporating information such as meronymy and dictionary glosses. Snow et al. (2006) extract hypernyms from free text. Wang and Cohen (2007) exploit web-page structure, while Pasca and Durme (2008) examine query logs. We expect that adding these types of data would significantly improve our system. Category Name Example Similarity Queries Simple Groups (SG) car brands, countries, mammals, crimes Complex Groups (CG) luxury car brands, sub-Saharan countries Synonyms (Syn) syn of {scandal, helicopter, arrogant, slay} Hard Synonyms (HS) syn of {(stock-market) crash</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Hughes, T., &amp; Ramage, D. (2007). Lexical semantic relatedness with random graph walks. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>Proceedings of International Conference on Research in Computational Linguistics.</booktitle>
<contexts>
<context position="11984" citStr="Jiang &amp; Conrath, 1997" startWordPosition="2048" endWordPosition="2051">nary/thesaurus/ ontology often used in NLP applications. It consists of a large number of synsets; a synset is a set of one or more similar word senses. The synsets are then connected with hypernym/hyponym links, which represent IS-A relationships. We focused on measuring similarity in WordNet using the hypernym hierarchy.2. There are many methods for 1Available at icon.shef.ac.uk/Moby/. 2A useful similarity metric we did not explore in this paper is similarity between WordNet dictionary definitions converting this hierarchy into a similarity score; we chose to use the Jiang-Conrath distance (Jiang &amp; Conrath, 1997) because it tends to be more robust to the exact structure of WordNet. The number of types of similarity in WordNet tends to be less than that captured by Moby, because synsets in WordNet are (usually) only allowed to have a single parent. For example, “murder” is classified as a type of killing, but not as a type of crime. The Jiang-Conrath distance gives scores for pairs of word senses, not pairs of words. We handle this by adding one row for every word sense with the right part of speech (rather than for every word); each row measures the similarity of every word to a particular word sense.</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jiang, J., &amp; Conrath, D. (1997). Semantic similarity based on corpus statistics and lexical taxonomy. Proceedings of International Conference on Research in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>Proceedings of ICML.</booktitle>
<contexts>
<context position="9779" citStr="Lin, 1998" startWordPosition="1665" endWordPosition="1666">imen372 Data Word Similar words Moby arrive accomplish, achieve, achieve success, advance, appear, approach, arrive at, arrive in, attain,... WordNet factory (plant,-1.9);(arsenal,-2.8);(mill,-2.9);(sweatshop,-4.1);(refinery,-4.2);(winery,-4.5);... DistSim watch (jewerly,.137),(wristwatch,.115),(shoe,0.09),(appliance,0.09),(household appliance,0.089),... Table 1: Examples of unprocessed similarity entries from each data source. 4 Data Sources We consider three similarity data sources: the Moby thesaurus1, WordNet (Fellbaum, 1998), and distributional similarity based on a large corpus of text (Lin, 1998). Table 1 shows similarity lists from each. These sources capture different kinds of similarity information, which increases the representational power of our system. For all sources, the similarity of a word with itself is set to 1.0. It is worth noting that our system is not strictly limited to choosing from pre-existing groups. For example, if we have a list of luxury items, and another list of cars, our system can learn weights so that it prefers items in the intersection, luxury cars. Moby thesaurus consists of a list of wordbased thesaurus entries. Each word wi has a list of similar word</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). An information-theoretic definition of similarity. Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B.</journal>
<contexts>
<context position="8995" citStr="Liu &amp; Nocedal, 1989" startWordPosition="1569" endWordPosition="1573">adaptive — it takes into account the labeled examples the user has provided so far. The second method uses a standard machine learning algorithm, logistic regression. As before, the final ranking over words is based on the score zj. However, zj is passed through the logistic function to produce a score between 0 and 1, z� = 1 1+e��j . We can interpret this score as the probability that wj is a positive example, Pθ(Y |wj). This leads to the objective function L(0) = log( � Pθ(Y |wj) � (1−Pθ(Y |wj)))- wjEp wjEn This objective is convex and can be optimized using standard methods such as L-BFGS (Liu &amp; Nocedal, 1989). Following standard practice we add an L2 regularization term −θ� θ 2σ2 to the objective. This method does not use the row labels li. similarity group. Possible similarity dimen372 Data Word Similar words Moby arrive accomplish, achieve, achieve success, advance, appear, approach, arrive at, arrive in, attain,... WordNet factory (plant,-1.9);(arsenal,-2.8);(mill,-2.9);(sweatshop,-4.1);(refinery,-4.2);(winery,-4.5);... DistSim watch (jewerly,.137),(wristwatch,.115),(shoe,0.09),(appliance,0.09),(household appliance,0.089),... Table 1: Examples of unprocessed similarity entries from each data so</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Liu, D. C., &amp; Nocedal, J. (1989). On the limited memory method for large scale optimization. Mathematical Programming B.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>E Crestan</author>
<author>A Borkovsky</author>
<author>A Popescu</author>
<author>V Vyas</author>
</authors>
<title>Web-scale distributional similarity and entity set expansion.</title>
<date>2009</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="1552" citStr="Pantel et al., 2009" startWordPosition="239" endWordPosition="242">al similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%. 1 Introduction Set expansion is a well-studied NLP problem where a machine-learning algorithm is given a fixed set of seed words and asked to find additional members of the implied set. For example, given the seed set {“elephant,” “horse,” “bat”J, the algorithm is expected to return other mammals. Past work, e.g. (Roark &amp; Charniak, 1998; Ghahramani &amp; Heller, 2005; Wang &amp; Cohen, 2007; Pantel et al., 2009), generally focuses on semi-automatic acquisition of the remaining members of the set by mining large amounts of unlabeled data. State-of-the-art set expansion systems work well for well-defined sets of nouns, e.g. “US Presidents,” particularly when given a large seed set. Set expansions is more difficult with fewer seed words and for other kinds of sets. The seed words may have multiple senses and the user may have in mind a variety of attributes that the answer must match. For example, suppose the seed word is “jaguar”. First, there is sense ambiguity; we could be referring to either a “larg</context>
<context position="5321" citStr="Pantel et al. (2009)" startWordPosition="883" endWordPosition="886">discussed.” Another interesting direction not pursued in this paper is using our system as part of a more-traditional set expansion system to build seed sets more quickly. 2 Set Expansion As input, we are provided with a small set of seed words s. The desired output is a target set of words G, consisting of all words that fit the desired concept. A particular seed set s can belong to many possible goal sets G, so additional information may be required to do well. Previous work tries to do as much as possible using only s. Typically s is assumed to contain at least 2 words and often many more. Pantel et al. (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. There are several problems with the fixed seed set approach. It is not always easy to think of even a single additional seed word (e.g., the user is trying to find “German automakers” and can only think of “Volkswagen”). Even if the user can think of additional seed words, time and effort might be saved by using active learning to find good suggestions. Also, as Vyas et al. (2009) show, nonexpert users often produce poor-quality seed sets. 3 Active Learning System Any syste</context>
</contexts>
<marker>Pantel, Crestan, Borkovsky, Popescu, Vyas, 2009</marker>
<rawString>Pantel, P., Crestan, E., Borkovsky, A., Popescu, A., &amp; Vyas, V. (2009). Web-scale distributional similarity and entity set expansion. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>B V Durme</author>
</authors>
<title>Weaklysupervised acquisition of open-domain classes and class attributes from web documents and query logs.</title>
<date>2008</date>
<publisher>ACL.</publisher>
<marker>Pasca, Durme, 2008</marker>
<rawString>Pasca, M., &amp; Durme, B. V. (2008). Weaklysupervised acquisition of open-domain classes and class attributes from web documents and query logs. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase co-occurrence statistics for semiautomatic semantic lexicon construction.</title>
<date>1998</date>
<publisher>ACL-COLING.</publisher>
<contexts>
<context position="1483" citStr="Roark &amp; Charniak, 1998" startWordPosition="227" endWordPosition="230">ing sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%. 1 Introduction Set expansion is a well-studied NLP problem where a machine-learning algorithm is given a fixed set of seed words and asked to find additional members of the implied set. For example, given the seed set {“elephant,” “horse,” “bat”J, the algorithm is expected to return other mammals. Past work, e.g. (Roark &amp; Charniak, 1998; Ghahramani &amp; Heller, 2005; Wang &amp; Cohen, 2007; Pantel et al., 2009), generally focuses on semi-automatic acquisition of the remaining members of the set by mining large amounts of unlabeled data. State-of-the-art set expansion systems work well for well-defined sets of nouns, e.g. “US Presidents,” particularly when given a large seed set. Set expansions is more difficult with fewer seed words and for other kinds of sets. The seed words may have multiple senses and the user may have in mind a variety of attributes that the answer must match. For example, suppose the seed word is “jaguar”. Fir</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>Roark, B., &amp; Charniak, E. (1998). Noun-phrase co-occurrence statistics for semiautomatic semantic lexicon construction. ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<publisher>ACL.</publisher>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Snow, R., Jurafsky, D., &amp; Ng, A. (2006). Semantic taxonomy induction from heterogenous evidence. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vyas</author>
<author>P Pantel</author>
</authors>
<title>Semi-automatic entity set refinement.</title>
<date>2009</date>
<publisher>NAACL/HLT.</publisher>
<contexts>
<context position="21743" citStr="Vyas and Pantel (2009)" startWordPosition="3690" endWordPosition="3693">ry. The complex groups category is particularly interesting, because achieving high performance on this category required using both logistic regression and active learning. This makes sense since negative evidence is particularly important for this category. 7 Discussion and Related Work The biggest difference between our system and previous work is the use of active learning, especially in allowing the use of negative examples. Most previous set expansion systems use bootstrapping from a small set of positive examples. Recently, the use of negative examples for set expansion was proposed by Vyas and Pantel (2009), although in a different way. First, set expansion is run as normal using a fixed seed set. Then, human annotators label a small number of negative examples from the returned results, which are used to weed out other bad answers. Our method incorporates negative examples at an earlier stage. Also, we use a logistic regression model to robustly incorporate negative information, rather than deterministically ruling out words and features. Our system is limited by our data sources. Suppose we want actors who appeared in Star Wars. If we only know that Harrison Ford and Mark Hamill are actors, we</context>
</contexts>
<marker>Vyas, Pantel, 2009</marker>
<rawString>Vyas, V., &amp; Pantel, P. (2009). Semi-automatic entity set refinement. NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vyas</author>
<author>P Pantel</author>
<author>E Crestan</author>
</authors>
<title>Helping editors choose better seed sets for entity expansion.</title>
<date>2009</date>
<publisher>CIKM.</publisher>
<contexts>
<context position="2481" citStr="Vyas et al. (2009)" startWordPosition="396" endWordPosition="399"> fewer seed words and for other kinds of sets. The seed words may have multiple senses and the user may have in mind a variety of attributes that the answer must match. For example, suppose the seed word is “jaguar”. First, there is sense ambiguity; we could be referring to either a “large cat” or a “car.” Beyond this, we might have in mind various more (or less) specific groups: “Mexican animals,” “predators,” “luxury cars,” “British cars,” etc. We propose a system which addresses several shortcomings of many set expansion systems. First, these systems can be difficult to use. As explored by Vyas et al. (2009), non-expert users produce seed sets that lead to poor quality expansions, for a variety of reasons including ambiguity and lack of coverage. Even for expert users, constructing seed sets can be a laborious and timeconsuming process. Second, most set expansion systems do not use negative examples, which can be very useful for weeding out other bad answers. Third, many set expansion systems concentrate on noun classes such as “US Presidents” and are not effective or do not apply to other kinds of sets. Our system works as follows. The user initially thinks of at least one seed word belonging to</context>
<context position="5826" citStr="Vyas et al. (2009)" startWordPosition="973" endWordPosition="976">ssible using only s. Typically s is assumed to contain at least 2 words and often many more. Pantel et al. (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. There are several problems with the fixed seed set approach. It is not always easy to think of even a single additional seed word (e.g., the user is trying to find “German automakers” and can only think of “Volkswagen”). Even if the user can think of additional seed words, time and effort might be saved by using active learning to find good suggestions. Also, as Vyas et al. (2009) show, nonexpert users often produce poor-quality seed sets. 3 Active Learning System Any system for this task relies on information about similarity between words. Our system takes as input a rectangular matrix M. Each column corresponds to a particular word. Each row corresponds to a unique dimension of similarity; the jth entry in row i mij is a number between 0 and 1 indicating the degree to which wj belongs to the ith sions include “How similar is word wj to the verb jump?” “Is wj a type of cat?” and “Are the words which appear in the context of wj similar to those that appear in the cont</context>
</contexts>
<marker>Vyas, Pantel, Crestan, 2009</marker>
<rawString>Vyas, V., Pantel, P., &amp; Crestan, E. (2009). Helping editors choose better seed sets for entity expansion. CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wang</author>
<author>W Cohen</author>
</authors>
<title>Languageindependent set expansion of named entities using the web.</title>
<date>2007</date>
<booktitle>Seventh IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="1530" citStr="Wang &amp; Cohen, 2007" startWordPosition="235" endWordPosition="238">s, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%. 1 Introduction Set expansion is a well-studied NLP problem where a machine-learning algorithm is given a fixed set of seed words and asked to find additional members of the implied set. For example, given the seed set {“elephant,” “horse,” “bat”J, the algorithm is expected to return other mammals. Past work, e.g. (Roark &amp; Charniak, 1998; Ghahramani &amp; Heller, 2005; Wang &amp; Cohen, 2007; Pantel et al., 2009), generally focuses on semi-automatic acquisition of the remaining members of the set by mining large amounts of unlabeled data. State-of-the-art set expansion systems work well for well-defined sets of nouns, e.g. “US Presidents,” particularly when given a large seed set. Set expansions is more difficult with fewer seed words and for other kinds of sets. The seed words may have multiple senses and the user may have in mind a variety of attributes that the answer must match. For example, suppose the seed word is “jaguar”. First, there is sense ambiguity; we could be refer</context>
</contexts>
<marker>Wang, Cohen, 2007</marker>
<rawString>Wang, R., &amp; Cohen, W. (2007). Languageindependent set expansion of named entities using the web. Seventh IEEE International Conference on Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>