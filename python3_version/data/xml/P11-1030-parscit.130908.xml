<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.922834">
Local Histograms of Character N-grams for Authorship Attribution
</title>
<author confidence="0.381995">
Hugo Jair Escalante Thamar Solorio
</author>
<affiliation confidence="0.149666">
Graduate Program in Systems Eng. Dept. of Computer and Information Sciences
</affiliation>
<address confidence="0.267566">
Universidad Aut´onoma de Nuevo Le´on, University of Alabama at Birmingham,
San Nicol´as de los Garza, NL, 66450, M´exico Birmingham, AL, 35294, USA
</address>
<email confidence="0.96593">
hugo.jair@gmail.com solorio@cis.uab.edu
</email>
<author confidence="0.995806">
Manuel Montes-y-G´omez
</author>
<affiliation confidence="0.98540625">
Computer Science Department, INAOE,
Tonantzintla, Puebla, 72840, M´exico
Department of Computer and Information Sciences,
University of Alabama at Birmingham,
</affiliation>
<address confidence="0.779637">
Birmingham, AL, 35294, USA
</address>
<email confidence="0.998183">
mmontesg@cis.uab.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999778083333334">
This paper proposes the use of local his-
tograms (LH) over character n-grams for au-
thorship attribution (AA). LHs are enriched
histogram representations that preserve se-
quential information in documents; they have
been successfully used for text categorization
and document visualization using word his-
tograms. In this work we explore the suitabil-
ity of LHs over n-grams at the character-level
for AA. We show that LHs are particularly
helpful for AA, because they provide useful
information for uncovering, to some extent,
the writing style of authors. We report experi-
mental results in AA data sets that confirm that
LHs over character n-grams are more help-
ful for AA than the usual global histograms,
yielding results far superior to state of the art
approaches. We found that LHs are even more
advantageous in challenging conditions, such
as having imbalanced and small training sets.
Our results motivate further research on the
use of LHs for modeling the writing style of
authors for related tasks, such as authorship
verification and plagiarism detection.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993884947368421">
Authorship attribution (AA) is the task of deciding
whom, from a set of candidates, is the author of a
given document (Houvardas and Stamatatos, 2006;
Luyckx and Daelemans, 2010; Stamatatos, 2009b).
There is a broad field of application for AA meth-
ods, including spam filtering (de Vel et al., 2001),
fraud detection, computer forensics (Lambers and
Veenman, 2009), cyber bullying (Pillay and Solorio,
2010) and plagiarism detection (Stamatatos, 2009a).
Therefore, the development of automated AA tech-
niques has received much attention recently (Sta-
matatos, 2009b). The AA problem can be natu-
rally posed as one of single-label multiclass clas-
sification, with as many classes as candidate au-
thors. However, unlike usual text categorization
tasks, where the core problem is modeling the the-
matic content of documents (Sebastiani, 2002), the
goal in AA is modeling authors’ writing style (Sta-
matatos, 2009b). Hence, document representations
that reveal information about writing style are re-
quired to achieve good accuracy in AA.
Word and character based representations have
been used in AA with some success so far (Houvar-
das and Stamatatos, 2006; Luyckx and Daelemans,
2010; Plakias and Stamatatos, 2008b). Such rep-
resentations can capture style information through
word or character usage, but they lack sequential in-
formation, which can reveal further stylistic infor-
mation. In this paper, we study the use of richer
document representations for the AA task. In partic-
ular, we consider local histograms over n-grams at
the character-level obtained via the locally-weighted
bag of words (LOWBOW) framework (Lebanon et
al., 2007).
Under LOWBOW, a document is represented by a
set of local histograms, computed across the whole
document but smoothed by kernels centered on dif-
ferent document locations. In this way, document
</bodyText>
<page confidence="0.961269">
288
</page>
<note confidence="0.9795765">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288–298,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999542043478261">
representations preserve both word/character usage
and sequential information (i.e., information about
the positions in which words or characters occur),
which can be more helpful for modeling the writ-
ing style of authors. We report experimental re-
sults in an AA data set used in previous studies un-
der several conditions (Houvardas and Stamatatos,
2006; Plakias and Stamatatos, 2008b; Plakias and
Stamatatos, 2008a). Results confirm that local his-
tograms of character n-grams are more helpful for
AA than the usual global histograms of words or
character n-grams (Luyckx and Daelemans, 2010);
our results are superior to those reported in re-
lated works. We also show that local histograms
over character n-grams are more helpful than lo-
cal histograms over words, as originally proposed
by (Lebanon et al., 2007). Further, we performed
experiments with imbalanced and small training
sets (i.e., under a realistic AA setting) using the
aforementioned representations. We found that the
LOWBOW-based representation resulted even more
advantageous in these challenging conditions. The
contributions of this work are as follows:
</bodyText>
<listItem confidence="0.974370833333333">
• We show that the LOWBOW framework can be
helpful for AA, giving evidence that sequential in-
formation encoded in local histograms is useful for
modeling the writing style of authors.
• We propose the use of local histograms over
character-level n-grams for AA. We show that
character-level representations, which have proved
to be very effective for AA (Luyckx and Daelemans,
2010), can be further improved by adopting a local
histogram formulation. Also, we empirically show
that local histograms at the character-level are more
helpful than local histograms at the word-level for
AA.
• We study several kernels for a support vector ma-
chine AA classifier under the local histograms for-
mulation. Our study confirms that the diffusion ker-
nel (Lafferty and Lebanon, 2005) is the most ef-
fective among those we tried, although competitive
performance can be obtained with simpler kernels.
• We report experimental results that are superior to
state of the art approaches (Plakias and Stamatatos,
2008b; Plakias and Stamatatos, 2008a), with im-
provements ranging from 2%−6% in balanced data
sets and from 14% − 30% in imbalanced data sets.
</listItem>
<sectionHeader confidence="0.999434" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999858568627451">
AA can be faced as a multiclass classifica-
tion task with as many classes as candidate au-
thors. Standard classification methods have been
applied to this problem, including support vec-
tor machine (SVM) classifiers (Houvardas and Sta-
matatos, 2006) and variants thereon (Plakias and
Stamatatos, 2008b; Plakias and Stamatatos, 2008a),
neural networks (Tearle et al., 2008), Bayesian clas-
sifiers (Coyotl-Morales et al., 2006), decision tree
methods (Koppel et al., 2009) and similarity based
techniques (Keselj et al., 2003; Lambers and Veen-
man, 2009; Stamatatos, 2009b; Koppel et al., 2009).
In this work, we chose an SVM classifier as it has
reported acceptable performance in AA and because
it will allow us to directly compare results with pre-
vious work that has used this same classifier.
A broad diversity of features has been used to rep-
resent documents in AA (Stamatatos, 2009b). How-
ever, as in text categorization (Sebastiani, 2002),
word-based and character-based features are among
the most widely used features (Stamatatos, 2009b;
Luyckx and Daelemans, 2010). With respect to
word-based features, word histograms (i.e., the bag-
of-words paradigm) are the most frequently used
representations in AA (Zhao and Zobel, 2005;
Argamon and Levitan, 2005; Stamatatos, 2009b).
Some researchers have gone a step further and
have attempted to capture sequential information
by using n-grams at the word-level (Peng et al.,
2004) or by discovering maximal frequent word se-
quences (Coyotl-Morales et al., 2006). Unfortu-
nately, because of computational limitations, the lat-
ter methods cannot discover enough sequential in-
formation from documents (e.g., word n-grams are
often restricted to n E 11, 2, 3}, while full se-
quential information would be obtained with n E
11 ... D} where D is the maximum number of
words in a document).
With respect to character-based features, n-grams
at the character level have been widely used in AA
as well (Plakias and Stamatatos, 2008b; Peng et
al., 2003; Luyckx and Daelemans, 2010). Peng et
al. (2003) propose the use of language models at the
n-gram character-level for AA, whereas Keselj et
al. (2003) build author profiles based on a selection
of frequent n-grams for each author. Stamatatos and
co-workers have studied the impact of feature se-
lection, with character n-grams, in AA (Houvardas
and Stamatatos, 2006; Stamatatos, 2006a), ensem-
ble learning with character n-grams (Stamatatos,
2006b) and novel classification techniques based
</bodyText>
<page confidence="0.967551">
289
</page>
<bodyText confidence="0.999867068181818">
on characters at the n-gram level (Plakias and Sta- with SVMs.
matatos, 2008a).
Acceptable performance in AA has been reported
with character n-gram representations. However,
as with word-based features, character n-grams are
unable to incorporate sequential information from
documents in their original form (in terms of the
positions in which the terms appear across a doc-
ument). We believe that sequential clues can be
helpful for AA because different authors are ex-
pected to use different character n-grams or words
in different parts of the document. Accordingly,
in this work we adopt the popular character-based
and word-based representations, but we enrich them
in a way that they incorporate sequential informa-
tion via the LOWBOW framework. Hence, the pro-
posed features preserve sequential information be-
sides capturing character and word usage informa-
tion. Our hypothesis is that the combination of se-
quential and frequency information can be particu-
larly helpful for AA.
The LOWBOW framework has been mainly used
for document visualization (Lebanon et al., 2007;
Mao et al., 2007), where researchers have used in-
formation derived from local histograms for dis-
playing a 2D representation of document’s con-
tent. More recently, Chasanis et al. (2009) used
the LOWBOW framework for segmenting movies
into chapters and scenes. LOWBOW representa-
tions have also been applied to discourse segmen-
tation (AMIDA, 2007) and have been suggested for
text summarization (Das and Martins, 2007). How-
ever, to the best of our knowledge the use of the
LOWBOW framework for AA has not been studied
elsewhere. Actually, the only two references using
this framework for text categorization are (Lebanon
et al., 2007; AMIDA, 2007). The latter can be due to
the fact that local histograms provide little gain over
usual global histograms for thematic classification
tasks. In this paper we show that LOWBOW rep-
resentations provide important improvements over
global histograms for AA; in particular, local his-
tograms at the character-level achieve the highest
performance in our experiments.
</bodyText>
<sectionHeader confidence="0.873869" genericHeader="method">
3 Background
</sectionHeader>
<subsectionHeader confidence="0.902358">
3.1 Bag of words representations
</subsectionHeader>
<bodyText confidence="0.999394619047619">
In the bag of words (BOW) representation, docu-
ments are represented by histograms over the vo-
cabulary1 that was used to generate a collection of
documents; that is, a document i is represented as:
di = [xi,1, ... , xi,jV j] (1)
where V is the vocabulary and |V  |is the number of
elements in V , di,j = xi,j is a weight that denotes
the contribution of term j to the representation of
document i; usually xi,j is related to the occurrence
(binary weighting) or the weighted frequency of oc-
currence (e.g., the tf-idf weighting scheme) of the
term j in document i.
3.2 Locally-weighted bag-of-words
representation
Instead of using the BOW framework directly, we
adopted the LOWBOW framework for document
representation (Lebanon et al., 2007). The underly-
ing idea in LOWBOW is to compute several local
histograms per document, where these histograms
are smoothed by a kernel function, see Figure 1.
The parameters of the kernel specify the position of
the kernel in the document (i.e., where the local his-
togram is centered) and its scale (i.e., to what extent
it is smoothed). In this way the sequential informa-
tion in the document is preserved together with term
usage statistics.
Let Wi = Jwi,1, ... , wi,Ni}, denote the terms
(in order of appearance) in document i where Ni
is the number of terms that appear in document i
and wi,j E V is the term appearing at position
j; let vi = Jvi,1, ... , vi,Ni} be the set of indexes
in the vocabulary V of the terms appearing in Wi,
such that vi,j is the index in V of the term wi,j;
let t = [t1, ... , tNi] be a set of (equally spaced)
scalars that determine intervals, with 0 &lt; tj &lt; 1 and
�Ni j=1 tj = 1, such that each tj can be associated to
a position in Wi. Given a kernel smoothing function
Ksµ,σ : [0, 1] —* R with location parameter p and
scale parameter Q, where Ekj=1 Ksµ,σ(tj) = 1 and
This section describes preliminary information on 1In the following we will refer to arbitrary vocabularies,
document representations and pattern classification which can be formed with terms from either words or character
290 n-grams.
</bodyText>
<figureCaption confidence="0.945531428571429">
Figure 1: Diagram of the process for obtaining local
histograms. Terms (wi) appearing in different posi-
tions (1, ... , N) of the document are weighted according
to the locations (µ1, ... , µk) of the smoothing function
Kµ a (x). Then, the term position weighting is combined
with term frequency weighting for obtaining local his-
tograms over the terms in the vocabulary (1, ... , |V |).
</figureCaption>
<bodyText confidence="0.996037333333333">
µ ∈ [0, 1]. The LOWBOW framework computes a
local histogram for each position µj ∈ {µ1, ... , µk}
as follows:
</bodyText>
<equation confidence="0.756246">
dlji,{vi,1,...,vi,Ni} = di,{vi,1,...,vi,Ni} × Ksµj,σ(t) (2)
</equation>
<bodyText confidence="0.969673153846154">
where dli,vj.vj6∈vi = const, a small constant value,
and di,j is defined as above. Hence, a set dl{1,...,k}
i
of k local histograms are computed for each doc-
ument i. Each histogram dlji carries information
about the distribution of terms at a certain position
µj of the document, where a determines how the
nearby terms to µj influence the local histogram
j. Thus, sequential information of the document is
considered throughout these local histograms. Note
that when a is small, most of the sequential informa-
tion is preserved, as local histograms are calculated
at very local scales; whereas when a ≥ 1, local his-
tograms resemble the traditional BOW representa-
tion.
Under LOWBOW documents can be represented
in two forms (Lebanon et al., 2007): as a single his-
togram dLi = const × �kj=1 dlji (hereafter LOW-
BOW histograms) or by the set of local histograms
itself dl{1,...,k} .We performed experiments with
i
both forms of representation and considered words
and n-grams at the character-level as terms (c.f. Sec-
tion 5). Regarding the smoothing function, we con-
sidered the re-normalized Gaussian pdf restricted to
[0, 1]:
</bodyText>
<equation confidence="0.9573095">
1N(x°µ,σ)µ if x ∈ [0, 1] (3)
0 otherwise
</equation>
<bodyText confidence="0.999605">
where O(x) is the cumulative distribution function
for a Gaussian with mean 0 and standard deviation 1,
evaluated at x, see (Lebanon et al., 2007) for further
details.
</bodyText>
<subsectionHeader confidence="0.998453">
3.3 Support vector machines
</subsectionHeader>
<bodyText confidence="0.9999835">
Support vector machines (SVMs) are pattern classi-
fication methods that aim to find an optimal sepa-
rating hyperplane between examples from two dif-
ferent classes (Shawe-Taylor and Cristianini, 2004).
Let {xi, yi}N be pairs of training patterns-outputs,
where xi ∈ Rd and y ∈ {−1, 1}, with d the di-
mensionality of the problem. SVMs aim at learn-
ing a mapping from training instances to outputs.
This is done by considering a linear function of the
form: f(x) = Wx + b, where parameters W and b
are learned from training data. The particular linear
function considered by SVMs is as follows:
</bodyText>
<equation confidence="0.9849075">
f(x) = � aiyiK(xi, x) − b (4)
i
</equation>
<bodyText confidence="0.948922">
that is, a linear function over (a subset of) training
examples, where ai is the weight associated with
training example i (those for which ai &gt; 0 are the so
called support vectors) and yi is the label associated
with training example i, K(xi, xj) is a kernel2 func-
tion that aims at mapping the input vectors, (xi, xj),
into the so called feature space, and b is a bias
term. Intuitively, K(xi,xj) evaluates how similar
instances xi and xj are, thus the particular choice of
kernel is problem dependent. The parameters in ex-
pression (4), namely a{1,...,N} and b, are learned by
using exact optimization techniques (Shawe-Taylor
and Cristianini, 2004).
2One should not confuse the kernel smoothing function,
Ksµ,σ(x), defined in Equation (3) with the Mercer kernel in
Equation (4), as the former acts as a smoothing function and
the latter acts as a similarity function.
</bodyText>
<equation confidence="0.995543">
Ksµ,σ(x) = {
</equation>
<page confidence="0.990094">
291
</page>
<sectionHeader confidence="0.9915815" genericHeader="method">
4 Authorship Attribution with LOWBOW
Representations
</sectionHeader>
<bodyText confidence="0.999862098360656">
For AA we represent the training documents of
each author using the framework described in Sec-
tion 3.2, thus each document of each candidate au-
thor is either a LOWBOW histogram or a bag of lo-
cal histograms (BOLH). Recall that LOWBOW his-
tograms are an un-weighted sum of local histograms
and hence can be considered a summary of term us-
age and sequential information; whereas the BOLH
can be seen as term occurrence frequencies across
different locations of the document.
For both types of representations we consider an
SVM classifier under the one-vs-all formulation for
facing the AA problem. We consider SVM as base
classifier because this method has proved to be very
effective in a large number of applications, including
AA (Houvardas and Stamatatos, 2006; Plakias and
Stamatatos, 2008b; Plakias and Stamatatos, 2008a);
further, since SVMs are kernel-based methods, they
allow us to use local histograms for AA by consid-
ering kernels that work over sets of histograms.
We build a multiclass SVM classifier by con-
sidering the pairs of patterns-outputs associated to
documents-authors. Where each pattern can be ei-
ther a LOWBOW histogram or the set of local his-
tograms associated with the corresponding docu-
ment, and the output associated to each pattern is
a categorical random variable (outputs) that asso-
ciates the representation of each document to its cor-
responding author y1�...�N E {1, ... , C}, with C
the number of candidate authors. For building the
multiclass classifier we adopted the one-vs-all for-
mulation, where C binary classifiers are built and
where each classifier fi discriminates among exam-
ples from class i (positive examples) and the rest
j : j E {1, ... , C}, j =� i; despite being one of the
simplest formulations, this approach has shown to
obtain comparable and even superior performance to
that obtained by more complex formulations (Rifkin
and Klautau, 2004).
For AA using LOWBOW histograms, we con-
sider a linear kernel since it has been success-
fully applied to a wide variety of problems (Shawe-
Taylor and Cristianini, 2004), including AA (Hou-
vardas and Stamatatos, 2006; Plakias and Sta-
matatos, 2008b). However, standard kernels can-
not work for input spaces where each instance is de-
scribed by a set of vectors. Therefore, usual kernels
are not applicable for AA using BOLH. Instead, we
rely on particular kernels defined for sets of vectors
rather than for a single vector. Specifically, we con-
sider kernels of the form (Rubner et al., 2001; Grau-
man, 2006):
where D(P, Q) is the sum of the distances between
the elements of the bag of local histograms asso-
ciated to author P and the elements of the bag of
histograms associated with author Q; -y is the scale
parameter of K. Let P = {p1, ... , pk} and Q =
{q1, ... , qk} be the elements of the bags of local
histograms for instances P and Q, respectively, Ta-
ble 1 presents the distance measures we consider for
AA using local histograms.
</bodyText>
<subsectionHeader confidence="0.60246">
Kernel Distance
</subsectionHeader>
<equation confidence="0.993269833333333">
Diffusion D(P, Q) = Pkl=1 arccos ¡(.,/pl � �ql�¢
EMD D(P, Q) = EMD(P, Q)
qPk
Eucidean D(P, Q) = l=1(pl − ql).2
x2 D(P) Q) = q Pl=1 (pl−ql)2
(pl+ql)
</equation>
<tableCaption confidence="0.9894025">
Table 1: Distance functions used to calculate the kernel
defined in Equation (5).
</tableCaption>
<bodyText confidence="0.999888846153846">
Diffusion, Euclidean, and k2 kernels compare lo-
cal histograms one to one, which means that the lo-
cal histograms calculated at the same locations are
compared to each other. We believe that for AA
this is advantageous as it is expected that an author
uses similar terms at similar locations of the docu-
ment. The Earth mover’s distance (EMD), on the
other hand, is an estimate of the optimal cost in tak-
ing local histograms from Q to local histograms in
P (Rubner et al., 2001); that is, this measure com-
putes the optimal matching distance between local
histograms from different authors that are not neces-
sarily computed at similar locations.
</bodyText>
<sectionHeader confidence="0.998084" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99814225">
For our experiments we considered the data set used
in (Plakias and Stamatatos, 2008b; Plakias and Sta-
matatos, 2008a). This corpus is a subset of the
RCV1 collection (Lewis et al., 2004) and comprises
</bodyText>
<equation confidence="0.803472">
K(P, Q) = exp ( − D(P, Q)2
�
� (5)
</equation>
<page confidence="0.986305">
292
</page>
<bodyText confidence="0.999996869565217">
documents authored by 10 authors. All of the docu-
ments belong to the same topic. Since this data set
has predefined training and testing partitions, our re-
sults are comparable to those obtained by other re-
searchers. There are 50 documents per author for
training and 50 documents per author for testing.
We performed experiments with LOWBOW3 rep-
resentations at word and character-level. For the ex-
periments with words, we took the top 2,500 most
common words used across the training documents
and obtained LOWBOW representations. We used
this setting in agreement with previous work on
AA (Houvardas and Stamatatos, 2006). For our
character n-gram experiments, we obtained LOW-
BOW representations for character 3-grams (only
n-grams of size n = 3 were used) considering
the 2,500 most common n-grams. Again, this set-
ting was adopted in agreement with previous work
on AA with character n-grams (Houvardas and
Stamatatos, 2006; Plakias and Stamatatos, 2008b;
Plakias and Stamatatos, 2008a; Luyckx and Daele-
mans, 2010). All our experiments use the SVM im-
plementation provided by Canu et al. (2005).
</bodyText>
<subsectionHeader confidence="0.98059">
5.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999845">
In order to compare our methods to related works
we adopted the following experimental setting. We
perform experiments using all of the training doc-
uments per author, that is, a balanced corpus (we
call this setting BC). Next we evaluate the perfor-
mance of classifiers over reduced training sets. We
tried balanced reduced data sets with: 1, 3, 5 and
10 documents per author (we call this configura-
tion RBC). Also, we experimented with reduced-
imbalanced data sets using the same imbalance rates
reported in (Plakias and Stamatatos, 2008b; Plakias
and Stamatatos, 2008a): we tried settings 2 − 10,
5 − 10, and 10 − 20, where, for example, setting 2-
10 means that we use at least 2 and at most 10 doc-
uments per author (we call this setting IRBC). BC
setting represents the AA problem under ideal con-
ditions, whereas settings RBC and IRBC aim at em-
ulating a more realistic scenario, where limited sam-
ple documents are available and the whole data set is
highly imbalanced (Plakias and Stamatatos, 2008b).
</bodyText>
<footnote confidence="0.9822565">
3We used LOWBOW code of G. Lebanon and Y. Mao avail-
able from http://www.cc.gatech.edu/∼ymao8/lowbow.htm
</footnote>
<subsectionHeader confidence="0.997229">
5.2 Experimental results in balanced data
</subsectionHeader>
<bodyText confidence="0.99942325">
We first compare the performance of the LOWBOW
histogram representation to that of the traditional
BOW representation. Table 2 shows the accuracy
(i.e., percentage of documents in the test set that
were associated to its correct author) for the BOW
and LOWBOW histogram representations when us-
ing words and character n-grams information. For
LOWBOW histograms, we report results with three
different configurations for p. As in (Lebanon et al.,
2007), we consider uniformly distributed locations
and we varied the number of locations that were in-
cluded in each setting. We denote with k the number
of local histograms. In preliminary experiments we
tried several other values for k, although we found
that representative results can be obtained with the
values we considered here.
</bodyText>
<table confidence="0.9984142">
Method Parameters Words Characters
BOW - 78.2% 75.0%
LOWBOW k = 2; v = 0.2 75.8% 72.0%
LOWBOW k = 5; v = 0.2 77.4% 75.2%
LOWBOW k = 20; v = 0.2 77.4% 75.0%
</table>
<tableCaption confidence="0.5654148">
Table 2: Authorship attribution accuracy for the BOW
representation and LOWBOW histograms. Column 2
shows the parameters we used for the LOWBOW his-
tograms; columns 3 and 4 show results using words and
character n-grams, respectively.
</tableCaption>
<bodyText confidence="0.99995905">
From Table 2 we can see that the BOW repre-
sentation is very effective, outperforming most of
the LOWBOW histogram configurations. Despite a
small difference in performance, BOW is advanta-
geous over LOWBOW histograms because it is sim-
pler to compute and it does not rely on parameter
selection. Recall that the LOWBOW histogram rep-
resentations are obtained by the combination of sev-
eral local histograms calculated at different locations
of the document, hence, it seems that the raw sum of
local histograms results in a loss of useful informa-
tion for representing documents. The worse perfor-
mance was obtained when k = 2 local histograms
are considered (see row 3 in Table 2). This re-
sult is somewhat expected since the larger the num-
ber of local histograms, the more LOWBOW his-
tograms approach the BOW formulation (Lebanon
et al., 2007).
We now describe the AA performance obtained
when using the BOLH formulation; these results
</bodyText>
<page confidence="0.996497">
293
</page>
<bodyText confidence="0.999841666666667">
are shown in Table 3. Most of the results from
this table are superior to those reported in Table 2,
showing that bags of local histograms are a better
way to exploit the LOWBOW framework for AA.
As expected, different kernels yield different results.
However, the diffusion kernel outperformed most of
the results obtained with other kernels; confirming
the results obtained by other researchers (Lebanon
et al., 2007; Lafferty and Lebanon, 2005).
</bodyText>
<table confidence="0.999585777777778">
Kernel Euc. Diffusion EMD x2
Words
Setting-1 78.6% 81.0% 75.0% 75.4%
Setting-2 77.6% 82.0% 76.8% 77.2%
Setting-3 79.2% 80.8% 77.0% 79.0%
Characters
Setting-1 83.4% 82.8% 84.4% 83.8%
Setting-2 83.4% 84.2% 82.2% 84.6%
Setting-3 83.6% 86.4% 81.0% 85.2%
</table>
<tableCaption confidence="0.998995">
Table 3: Authorship attribution accuracy when using bags
</tableCaption>
<bodyText confidence="0.975120069767442">
of local histograms and different kernels for word-based
and character-based representations. The BC data set is
used. Settings 1, 2 and 3 correspond to k = 2, 5 and 20,
respectively.
On average, the worse kernel was that based on
the earth mover’s distance (EMD), suggesting that
the comparison of local histograms at different loca-
tions is not a fruitful approach (recall that this is the
only kernel that compares local histograms at differ-
ent locations). This result evidences that authors use
similar word/character distributions at similar loca-
tions when writing different documents.
The best performance across settings and kernels
was obtained with the diffusion kernel (in bold, col-
umn 3, row 9) (86.4%); that result is 8% higher
than that obtained with the BOW representation and
9% better than the best configuration of LOWBOW
histograms, see Table 2. Furthermore, that result
is more than 5% higher than the best reported re-
sult in related work (80.8% as reported in (Plakias
and Stamatatos, 2008b)). Therefore, the consid-
ered local histogram representations over character
n-grams have proved to be very effective for AA.
One should note that, in general, better per-
formance was obtained when using character-level
rather than word-level information. This confirms
the results already reported by other researchers
that have used character-level and word-level infor-
mation for AA (Houvardas and Stamatatos, 2006;
Plakias and Stamatatos, 2008b; Plakias and Sta-
matatos, 2008a; Peng et al., 2003). We believe this
can be attributed to the fact that character n-grams
provide a representation for the document at a finer
granularity, which can be better exploited with local
histogram representations. Note that by considering
3-grams, words of length up to three are incorpo-
rated, and usually these words are function words
(e.g., the, it, as, etc.), which are known to be in-
dicative of writing style. Also, n-gram information
is more dense in documents than word-level infor-
mation. Hence, the local histograms are less sparse
when using character-level information, which re-
sults in better AA performance.
</bodyText>
<table confidence="0.45276">
True author
AC AS BL DL JM JG MM MD RS TN
88 2 0 0 0 0 0 0 0 0
10 98 0 0 0 0 0 0 0 0
0 0 68 0 40 0 0 0 0 0
0 0 0 80 0 0 0 0 0 4
0 0 12 2 42 0 0 2 0 0
0 0 0 0 0 100 0 0 0 2
2 0 2 0 0 0 100 0 0 0
0 0 18 0 18 0 0 98 0 0
0 0 0 2 0 0 0 0 100 4
0 0 0 16 0 0 0 0 0 90
</table>
<tableCaption confidence="0.745903">
Table 4: Confusion matrix (in terms of percentages) for
</tableCaption>
<bodyText confidence="0.991223772727273">
the best result in the BC corpus (i.e., last row, column 3
in Table 3). Columns show the true author for test docu-
ments and rows show the authors predicted by the SVM.
Table 4 shows the confusion matrix for the setting
that reached the best results (i.e., column 3, last row
in Table 3). From this table we can see that 8 out
of the 10 authors were recognized with an accuracy
higher or equal to 80%. For these authors sequential
information seems to be particularly helpful. How-
ever, low recognition performance was obtained for
authors BL (B. K. Lim) and JM (J. MacArtney).
The SVM with BOW representation of character n-
grams achieved recognition rates of 40% and 50%
for BL and JM respectively. Thus, we can state that
sequential information was indeed helpful for mod-
eling BL writing style (improvement of 28%), al-
though it is an author that resulted very difficult to
model. On the other hand, local histograms were not
very useful for identifying documents written by JM
(made it worse by −8%). The largest improvement
(38%) of local histograms over the BOW formula-
tion was obtained for author TN (T. Nissen). This
</bodyText>
<page confidence="0.994642">
294
</page>
<bodyText confidence="0.999837375">
result gives evidence that TN uses a similar distri-
bution of words in similar locations across the doc-
uments he writes. These results are interesting, al-
though we would like to perform a careful analysis
of results in order to determine for what type of au-
thors it would be beneficial to use local histograms,
and what type of authors are better modeled with a
standard BOW approach.
</bodyText>
<subsectionHeader confidence="0.999296">
5.3 Experimental results in imbalanced data
</subsectionHeader>
<bodyText confidence="0.999977162790698">
In this section we report results with RBC and
IRBC data sets, which aim to evaluate the perfor-
mance of our methods in a realistic setting. For
these experiments we compare the performance of
the BOW, LOWBOW histogram and BOLH repre-
sentations; for the latter, we considered the best set-
ting as reported in Table 3 (i.e., an SVM with dif-
fusion kernel and k = 20). Tables 5 and 6 show
the AA performances when using word and charac-
ter information, respectively.
We first analyze the results in the RBC data set
(recall that for this data set we consider 1, 3, 5, 10,
and 50, randomly selected documents per author).
From Tables 5 and 6 we can see that BOW and
LOWBOW histogram representations obtained sim-
ilar performance to each other across the different
training set sizes, which agree with results in Table 2
for the BC data sets. The best performance across
the different configurations of the RBC data set was
obtained with the BOLH formulation (row 6 in Ta-
bles 5 and 6). The improvements of local histograms
over the BOW formulation vary across different set-
tings and when using information at word-level and
character-level. When using words (columns 2-6
in Table 5) the differences in performance are of
15.6%, 6.2%, 6.8%, 2.9%, 3.8% when using 1, 3, 5,
10 and 50 documents per author, respectively. Thus,
it is evident that local histograms are more beneficial
when less documents are considered. Here, the lack
of information is compensated by the availability of
several histograms per author.
When using character n-grams (columns 2-6 in
Table 6) the corresponding differences in perfor-
mance are of 5.4%, 6.4%, 6.4%, 6% and 11.4%,
when using 1, 3, 5, 10, and 50 documents per au-
thor, respectively. In this case, the larger improve-
ment was obtained when 50 documents per author
are available; nevertheless, one should note that re-
sults using character-level information are, in gen-
eral, significantly better than those obtained with
word-level information; hence, improvements are
expected to be smaller.
When we compare the results of the BOLH for-
mulation with the best reported results elsewhere
(c.f. last row 6 in Tables 5 and 6) (Plakias and Sta-
matatos, 2008b), we found that the improvements
range from 14% to 30.2% when using character n-
grams and from 1.2% to 26% when using words.
The differences in performance are larger when less
information is used (e.g., when 5 documents are
used for training) and we believe the differences
would be even larger if results for 1 and 3 documents
were available. These are very positive results; for
example, we can obtain almost 71% of accuracy, us-
ing local histograms of character n-grams when a
single document is available per author (recall that
we have used all of the test samples for evaluating
the performance of our methods).
We now analyze the performance of the different
methods when using the IRBC data set (columns 7-
9 in Tables 5 and 6). The same pattern as before can
be observed in experimental results for these data
sets as well: BOW and LOWBOW histograms ob-
tained comparable performance to each other and
the BOLH formulation performed the best. The
BOLH formulation outperforms state of the art ap-
proaches by a considerable margin that ranges from
10% to 27%. Again, better results were obtained
when using character n-grams for the local his-
tograms. With respect to RBC data sets, the BOLH
at the character-level resulted very robust to the re-
duction of training set size and the highly imbal-
anced data.
Summarizing, the results obtained in RBC and
IRBC data sets show that the use of local histograms
is advantageous under challenging conditions. An
SVM under the BOLH representation is less sen-
sitive to the number of training examples available
and to the imbalance of data than an SVM using
the BOW representation. Our hypothesis for this
behavior is that local histograms can be thought of
as expanding training instances, because for each
training instance in the BOW formulation we have
k−training instances under BOLH. The benefits of
such expansion become more notorious as the num-
ber of available documents per author decreases.
</bodyText>
<page confidence="0.995662">
295
</page>
<table confidence="0.957936714285714">
WORDS
Data set Balanced Imbalanced
Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20
BOW 36.8% 57.1% 62.4% 69.9% 78.2% 62.3% 67.2% 71.2%
LOWBOW 37.9% 55.6% 60.5% 69.3% 77.4% 61.1% 67.4% 71.5%
Diffusion kernel 52.4% 63.3% 69.2% 72.8% 82.0% 66.6% 70.7% 74.1%
Reference - - 53.4% 67.8% 80.8% 49.2% 59.8% 63.0%
</table>
<tableCaption confidence="0.995476666666667">
Table 5: AA accuracy in RBC (columns 2-6) and IRBC (columns 7-9) data sets when using words as terms. We report
results for the BOW, LOWBOW histogram and BOLH representations. For reference (last row), we also include the
best result reported in (Plakias and Stamatatos, 2008b), when available, for each configuration.
</tableCaption>
<table confidence="0.943006714285714">
CHARACTER N-GRAMS
Data set Balanced Imbalanced
Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20
BOW 65.3% 71.9% 74.2% 76.2% 75.0% 70.1% 73.4% 73.1%
LOWBOW 61.9% 71.6% 74.5% 73.8% 75.0% 70.8% 72.8% 72.1%
Diffusion kernel 70.7% 78.3% 80.6% 82.2% 86.4% 77.8% 80.5% 82.2%
Reference - - 50.4% 67.8% 76.6% 49.2% 59.8% 63.0%
</table>
<tableCaption confidence="0.99779">
Table 6: AA accuracy in the RBC and IRBC data sets when using character n-grams as terms.
</tableCaption>
<sectionHeader confidence="0.99898" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999990481481482">
We have described the use of local histograms (LH)
over character n-grams for AA. LHs are enriched
histogram representations that preserve sequential
information in documents (in terms of the positions
of terms in documents); we explored the suitabil-
ity of LHs over n-grams at the character-level for
AA. We showed evidence supporting our hypothe-
sis that LHs are very helpful for AA; we believe that
this is due to the fact that LOWBOW representations
can uncover, to some extent, the writing preferences
of authors. Our experimental results showed that
LHs outperform traditional bag-of-words formula-
tions and state of the art techniques in balanced,
imbalanced, and reduced data sets. The improve-
ments were larger in reduced and imbalanced data
sets, which is a very positive result as in real AA
applications one often faces highly imbalanced and
small sample issues. Our results are promising and
motivate further research on the use and extension
of the LOWBOW framework for related tasks (e.g.
authorship verification and plagiarism detection).
As future work we would like to explore the use
of LOWBOW representations for profile-based AA
and related tasks. Also, we would like to develop
model selection strategies for learning what combi-
nation of hyperparameters works better for modeling
each author.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999964285714286">
We thank E. Stamatatos for making his data set
available. Also, we are grateful for the thought-
ful comments of L. A. Barr´on and those of the
anonymous reviewers. This work was partially sup-
ported by CONACYT under project grants 61335,
and CB-2009-134186, and by UAB faculty develop-
ment grant 3110841.
</bodyText>
<sectionHeader confidence="0.998492" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997106181818182">
AMIDA. 2007. Augmented multi-party interaction
with distance access. Available from http://www.
amidaproject.org/, AMIDA Report.
S. Argamon and S. Levitan. 2005. Measuring the useful-
ness of function words for authorship attribution. In
Proceedings of the Joint Conference of the Association
for Computers and the Humanities and the Association
for Literary and Linguistic Computing, Victoria, BC,
Canada.
S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotoma-
monjy. 2005. SVM and kernel methods Matlab tool-
box. Perception Systmes et Information, INSA de
Rouen, Rouen, France.
V. Chasanis, A. Kalogeratos, and A. Likas. 2009. Movie
segmentation into scenes and chapters using locally
weighted bag of visual words. In Proceedings of the
ACM International Conference on Image and Video
Retrieval, pages 35:1–35:7, Santorini, Fira, Greece.
ACM Press.
R. M. Coyotl-Morales, L. Villase˜nor-Pineda, M. Montes-
y-G´omez, and P. Rosso. 2006. Authorship attribu-
tion using word sequences. In Proceedings of 11th
</reference>
<page confidence="0.990362">
296
</page>
<reference confidence="0.999790351851852">
Iberoamerican Congress on Pattern Recognition, vol-
ume 4225 of LNCS, pages 844–852, Cancun, Mexico.
Springer.
D. Das and A. Martins. 2007. A survey on au-
tomatic text summarization. Available from:
http://www.cs.cmu.edu/˜nasmith/LS2/
das-martins.07.pdf, Literature Survey for the
Language and Statistics II course at Carnegie Mellon
University.
O. de Vel, A. Anderson, M. Corney, and G. Mohay. 2001.
Multitopic email authorship attribution forensics. In
Proceedings of the ACM Conference on Computer Se-
curity - Workshop on Data Mining for Security Appli-
cations, Philadelphia, PA, USA.
K. Grauman. 2006. Matching Sets of Features for Ef-
ficient Retrieval and Recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
J. Houvardas and E. Stamatatos. 2006. N-gram fea-
ture selection for author identification. In Proceedings
of the 12th International Conference on Artificial In-
telligence: Methodology, Systems, and Applications,
volume 4183 of LNCS, pages 77–86, Varna, Bulgaria.
Springer.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-
gram-based author profiles for authorship attribution.
In Proceedings of the Pacific Association for Compu-
tational Linguistics, pages 255–264, Halifax, Canada.
M. Koppel, J. Schler, and S. Argamon. 2009. Computa-
tional methods in authorship attribution. Journal of the
American Society for Information Science and Tech-
nology, 60:9–26.
J. Lafferty and G. Lebanon. 2005. Diffusion kernels
on statistical manifolds. Journal of Machine Learning
Research, 6:129–163.
M. Lambers and C. J. Veenman. 2009. Forensic author-
ship attribution using compression distances to pro-
totypes. In Computational Forensics, Lecture Notes
in Computer Science, Volume 5718. ISBN 978-3-642-
03520-3. Springer Berlin Heidelberg, 2009, p. 13, vol-
ume 5718 of LNCS, pages 13–24. Springer.
G. Lebanon, Y. Mao, and J. Dillon. 2007. The locally
weighted bag of words framework for document rep-
resentation. Journal of Machine Learning Research,
8:2405–2441.
D. Lewis, T. Yang, and F. Rose. 2004. RCV1: A new
benchmark collection for text categorization research.
Journal of Machine Learning Research, 5:361–397.
K. Luyckx and W. Daelemans. 2010. The effect of au-
thor set size and data size in authorship attribution.
Literary and Linguistic Computing, pages 1–21, Au-
gust.
Y. Mao, J. Dillon, and G. Lebanon. 2007. Sequential
document visualization. IEEE Transactions on Visu-
alization and Computer Graphics, 13(6):1208–1215.
F. Peng, D. Shuurmans, V. Keselj, and S. Wang. 2003.
Language independent authorship attribution using
character level language models. In Proceedings of the
10th conference of the European chapter of the Associ-
ation for Computational Linguistics, volume 1, pages
267–274, Budapest, Hungary.
F. Peng, D. Shuurmans, and S. Wang. 2004. Augmenting
naive Bayes classifiers with statistical language mod-
els. Information Retrieval Journal, 7(1):317–345.
S. R. Pillay and T. Solorio. 2010. Authorship attribution
of web forum posts. In Proceedings of the eCrime Re-
searchers Summit (eCrime), 2010, pages 1–7, Dallas,
TX, USA. IEEE.
S. Plakias and E. Stamatatos. 2008a. Author identifi-
cation using a tensor space representation. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence, volume 178, pages 833–834, Patras,
Greece. IOS Press.
S. Plakias and E. Stamatatos. 2008b. Tensor space mod-
els for authorship attribution. In Proceedings of the 5th
Hellenic Conference on Artificial Intelligence: Theo-
ries, Models and Applications, volume 5138 of LNCS,
pages 239–249, Syros, Greece. Springer.
R. Rifkin and A. Klautau. 2004. In defense of one-vs-all
classification. Journal of Machine Learning Research,
5:101–141.
Y. Rubner, C. Tomasi, J. Leonidas, and J. Guibas. 2001.
The earth mover’s distance as a metric for image re-
trieval. International Journal of Computer Vision,
40(2):99–121.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1–47.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
E. Stamatatos. 2006a. Authorship attribution based on
feature set subspacing ensembles. International Jour-
nal on Artificial Intelligence Tools, 15(5):823–838.
E. Stamatatos. 2006b. Ensemble-based author identifi-
cation using character n-grams. In Proceedings of the
3rd International Workshop on Text-based Information
Retrieval, pages 41–46, Riva del Garda, Italy.
E. Stamatatos. 2009a. Intrinsic plagiarism detec-
tion using character n-gram profiles. In Proceed-
ings of the 3rd International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse,
PAN’09, pages 38–46, Donostia-San Sebastian, Spain.
E. Stamatatos. 2009b. A survey of modern authorship
attribution methods. Journal of the American Society
for Information Science and Technology, 60(3):538–
556.
M. Tearle, K. Taylor, and H. Demuth. 2008. An
algorithm for automated authorship attribution using
neural networks. Literary and Linguist Computing,
23(4):425–442.
</reference>
<page confidence="0.96131">
297
</page>
<reference confidence="0.997806">
Y. Zhao and J. Zobel. 2005. Effective and scalable au-
thorship attribution using function words. In Proceed-
ings of 2nd Asian Information Retrieval Symposium,
volume 3689 of LNCS, pages 174–189, Jeju Island,
Korea. Springer.
</reference>
<page confidence="0.997114">
298
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.895902">
<title confidence="0.999621">Histograms of Character for Authorship Attribution</title>
<author confidence="0.998917">Hugo Jair Escalante Thamar Solorio</author>
<affiliation confidence="0.999576">Graduate Program in Systems Eng. Dept. of Computer and Information Sciences Universidad Aut´onoma de Nuevo Le´on, University of Alabama at Birmingham,</affiliation>
<address confidence="0.999892">San Nicol´as de los Garza, NL, 66450, M´exico Birmingham, AL, 35294, USA</address>
<email confidence="0.999002">hugo.jair@gmail.comsolorio@cis.uab.edu</email>
<author confidence="0.978253">Manuel</author>
<affiliation confidence="0.999979">Computer Science Department,</affiliation>
<address confidence="0.997542">Tonantzintla, Puebla, 72840,</address>
<affiliation confidence="0.9981005">Department of Computer and Information University of Alabama at</affiliation>
<address confidence="0.959479">Birmingham, AL, 35294,</address>
<email confidence="0.999782">mmontesg@cis.uab.edu</email>
<abstract confidence="0.99848908">This paper proposes the use of local his- (LH) over character for authorship attribution (AA). LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization document visualization using histograms. In this work we explore the suitabilof LHs over at the for AA. We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors. We report experimental results in AA data sets that confirm that over character are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AMIDA</author>
</authors>
<title>Augmented multi-party interaction with distance access. Available from http://www. amidaproject.org/,</title>
<date>2007</date>
<journal>AMIDA Report.</journal>
<contexts>
<context position="9866" citStr="AMIDA, 2007" startWordPosition="1512" endWordPosition="1513">tion besides capturing character and word usage information. Our hypothesis is that the combination of sequential and frequency information can be particularly helpful for AA. The LOWBOW framework has been mainly used for document visualization (Lebanon et al., 2007; Mao et al., 2007), where researchers have used information derived from local histograms for displaying a 2D representation of document’s content. More recently, Chasanis et al. (2009) used the LOWBOW framework for segmenting movies into chapters and scenes. LOWBOW representations have also been applied to discourse segmentation (AMIDA, 2007) and have been suggested for text summarization (Das and Martins, 2007). However, to the best of our knowledge the use of the LOWBOW framework for AA has not been studied elsewhere. Actually, the only two references using this framework for text categorization are (Lebanon et al., 2007; AMIDA, 2007). The latter can be due to the fact that local histograms provide little gain over usual global histograms for thematic classification tasks. In this paper we show that LOWBOW representations provide important improvements over global histograms for AA; in particular, local histograms at the charact</context>
</contexts>
<marker>AMIDA, 2007</marker>
<rawString>AMIDA. 2007. Augmented multi-party interaction with distance access. Available from http://www. amidaproject.org/, AMIDA Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>S Levitan</author>
</authors>
<title>Measuring the usefulness of function words for authorship attribution.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computers and the Humanities and the Association for Literary and Linguistic Computing,</booktitle>
<location>Victoria, BC,</location>
<contexts>
<context position="7225" citStr="Argamon and Levitan, 2005" startWordPosition="1098" endWordPosition="1101">r as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histograms (i.e., the bagof-words paradigm) are the most frequently used representations in AA (Zhao and Zobel, 2005; Argamon and Levitan, 2005; Stamatatos, 2009b). Some researchers have gone a step further and have attempted to capture sequential information by using n-grams at the word-level (Peng et al., 2004) or by discovering maximal frequent word sequences (Coyotl-Morales et al., 2006). Unfortunately, because of computational limitations, the latter methods cannot discover enough sequential information from documents (e.g., word n-grams are often restricted to n E 11, 2, 3}, while full sequential information would be obtained with n E 11 ... D} where D is the maximum number of words in a document). With respect to character-bas</context>
</contexts>
<marker>Argamon, Levitan, 2005</marker>
<rawString>S. Argamon and S. Levitan. 2005. Measuring the usefulness of function words for authorship attribution. In Proceedings of the Joint Conference of the Association for Computers and the Humanities and the Association for Literary and Linguistic Computing, Victoria, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Canu</author>
<author>Y Grandvalet</author>
<author>V Guigue</author>
<author>A Rakotomamonjy</author>
</authors>
<date>2005</date>
<booktitle>SVM and kernel methods Matlab toolbox. Perception Systmes et Information, INSA de</booktitle>
<location>Rouen, Rouen, France.</location>
<contexts>
<context position="21294" citStr="Canu et al. (2005)" startWordPosition="3470" endWordPosition="3473">aining documents and obtained LOWBOW representations. We used this setting in agreement with previous work on AA (Houvardas and Stamatatos, 2006). For our character n-gram experiments, we obtained LOWBOW representations for character 3-grams (only n-grams of size n = 3 were used) considering the 2,500 most common n-grams. Again, this setting was adopted in agreement with previous work on AA with character n-grams (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Luyckx and Daelemans, 2010). All our experiments use the SVM implementation provided by Canu et al. (2005). 5.1 Experimental settings In order to compare our methods to related works we adopted the following experimental setting. We perform experiments using all of the training documents per author, that is, a balanced corpus (we call this setting BC). Next we evaluate the performance of classifiers over reduced training sets. We tried balanced reduced data sets with: 1, 3, 5 and 10 documents per author (we call this configuration RBC). Also, we experimented with reducedimbalanced data sets using the same imbalance rates reported in (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a): w</context>
</contexts>
<marker>Canu, Grandvalet, Guigue, Rakotomamonjy, 2005</marker>
<rawString>S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotomamonjy. 2005. SVM and kernel methods Matlab toolbox. Perception Systmes et Information, INSA de Rouen, Rouen, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Chasanis</author>
<author>A Kalogeratos</author>
<author>A Likas</author>
</authors>
<title>Movie segmentation into scenes and chapters using locally weighted bag of visual words.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACM International Conference on Image and Video Retrieval,</booktitle>
<pages>35--1</pages>
<publisher>ACM Press.</publisher>
<location>Santorini, Fira, Greece.</location>
<contexts>
<context position="9706" citStr="Chasanis et al. (2009)" startWordPosition="1486" endWordPosition="1489">presentations, but we enrich them in a way that they incorporate sequential information via the LOWBOW framework. Hence, the proposed features preserve sequential information besides capturing character and word usage information. Our hypothesis is that the combination of sequential and frequency information can be particularly helpful for AA. The LOWBOW framework has been mainly used for document visualization (Lebanon et al., 2007; Mao et al., 2007), where researchers have used information derived from local histograms for displaying a 2D representation of document’s content. More recently, Chasanis et al. (2009) used the LOWBOW framework for segmenting movies into chapters and scenes. LOWBOW representations have also been applied to discourse segmentation (AMIDA, 2007) and have been suggested for text summarization (Das and Martins, 2007). However, to the best of our knowledge the use of the LOWBOW framework for AA has not been studied elsewhere. Actually, the only two references using this framework for text categorization are (Lebanon et al., 2007; AMIDA, 2007). The latter can be due to the fact that local histograms provide little gain over usual global histograms for thematic classification tasks</context>
</contexts>
<marker>Chasanis, Kalogeratos, Likas, 2009</marker>
<rawString>V. Chasanis, A. Kalogeratos, and A. Likas. 2009. Movie segmentation into scenes and chapters using locally weighted bag of visual words. In Proceedings of the ACM International Conference on Image and Video Retrieval, pages 35:1–35:7, Santorini, Fira, Greece. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Coyotl-Morales</author>
<author>L Villase˜nor-Pineda</author>
<author>M Montesy-G´omez</author>
<author>P Rosso</author>
</authors>
<title>Authorship attribution using word sequences.</title>
<date>2006</date>
<booktitle>In Proceedings of 11th</booktitle>
<marker>Coyotl-Morales, Villase˜nor-Pineda, Montesy-G´omez, Rosso, 2006</marker>
<rawString>R. M. Coyotl-Morales, L. Villase˜nor-Pineda, M. Montesy-G´omez, and P. Rosso. 2006. Authorship attribution using word sequences. In Proceedings of 11th</rawString>
</citation>
<citation valid="false">
<title>Iberoamerican Congress on Pattern Recognition,</title>
<volume>4225</volume>
<pages>844--852</pages>
<publisher>Springer.</publisher>
<location>Cancun, Mexico.</location>
<marker></marker>
<rawString>Iberoamerican Congress on Pattern Recognition, volume 4225 of LNCS, pages 844–852, Cancun, Mexico. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>A Martins</author>
</authors>
<title>A survey on automatic text summarization. Available from: http://www.cs.cmu.edu/˜nasmith/LS2/ das-martins.07.pdf, Literature Survey for the Language and Statistics II course at</title>
<date>2007</date>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="9937" citStr="Das and Martins, 2007" startWordPosition="1521" endWordPosition="1524">Our hypothesis is that the combination of sequential and frequency information can be particularly helpful for AA. The LOWBOW framework has been mainly used for document visualization (Lebanon et al., 2007; Mao et al., 2007), where researchers have used information derived from local histograms for displaying a 2D representation of document’s content. More recently, Chasanis et al. (2009) used the LOWBOW framework for segmenting movies into chapters and scenes. LOWBOW representations have also been applied to discourse segmentation (AMIDA, 2007) and have been suggested for text summarization (Das and Martins, 2007). However, to the best of our knowledge the use of the LOWBOW framework for AA has not been studied elsewhere. Actually, the only two references using this framework for text categorization are (Lebanon et al., 2007; AMIDA, 2007). The latter can be due to the fact that local histograms provide little gain over usual global histograms for thematic classification tasks. In this paper we show that LOWBOW representations provide important improvements over global histograms for AA; in particular, local histograms at the character-level achieve the highest performance in our experiments. 3 Backgrou</context>
</contexts>
<marker>Das, Martins, 2007</marker>
<rawString>D. Das and A. Martins. 2007. A survey on automatic text summarization. Available from: http://www.cs.cmu.edu/˜nasmith/LS2/ das-martins.07.pdf, Literature Survey for the Language and Statistics II course at Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O de Vel</author>
<author>A Anderson</author>
<author>M Corney</author>
<author>G Mohay</author>
</authors>
<title>Multitopic email authorship attribution forensics.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM Conference on Computer Security - Workshop on Data Mining for Security Applications,</booktitle>
<location>Philadelphia, PA, USA.</location>
<marker>de Vel, Anderson, Corney, Mohay, 2001</marker>
<rawString>O. de Vel, A. Anderson, M. Corney, and G. Mohay. 2001. Multitopic email authorship attribution forensics. In Proceedings of the ACM Conference on Computer Security - Workshop on Data Mining for Security Applications, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Grauman</author>
</authors>
<title>Matching Sets of Features for Efficient Retrieval and Recognition.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="18613" citStr="Grauman, 2006" startWordPosition="3003" endWordPosition="3005">Klautau, 2004). For AA using LOWBOW histograms, we consider a linear kernel since it has been successfully applied to a wide variety of problems (ShaweTaylor and Cristianini, 2004), including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b). However, standard kernels cannot work for input spaces where each instance is described by a set of vectors. Therefore, usual kernels are not applicable for AA using BOLH. Instead, we rely on particular kernels defined for sets of vectors rather than for a single vector. Specifically, we consider kernels of the form (Rubner et al., 2001; Grauman, 2006): where D(P, Q) is the sum of the distances between the elements of the bag of local histograms associated to author P and the elements of the bag of histograms associated with author Q; -y is the scale parameter of K. Let P = {p1, ... , pk} and Q = {q1, ... , qk} be the elements of the bags of local histograms for instances P and Q, respectively, Table 1 presents the distance measures we consider for AA using local histograms. Kernel Distance Diffusion D(P, Q) = Pkl=1 arccos ¡(.,/pl � �ql�¢ EMD D(P, Q) = EMD(P, Q) qPk Eucidean D(P, Q) = l=1(pl − ql).2 x2 D(P) Q) = q Pl=1 (pl−ql)2 (pl+ql) Tabl</context>
</contexts>
<marker>Grauman, 2006</marker>
<rawString>K. Grauman. 2006. Matching Sets of Features for Efficient Retrieval and Recognition. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Houvardas</author>
<author>E Stamatatos</author>
</authors>
<title>N-gram feature selection for author identification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th International Conference on Artificial Intelligence: Methodology, Systems, and Applications,</booktitle>
<volume>4183</volume>
<pages>77--86</pages>
<publisher>Springer.</publisher>
<location>Varna, Bulgaria.</location>
<contexts>
<context position="1830" citStr="Houvardas and Stamatatos, 2006" startWordPosition="267" endWordPosition="270">ts that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). There is a broad field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, whe</context>
<context position="4051" citStr="Houvardas and Stamatatos, 2006" startWordPosition="603" endWordPosition="606">smoothed by kernels centered on different document locations. In this way, document 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288–298, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics representations preserve both word/character usage and sequential information (i.e., information about the positions in which words or characters occur), which can be more helpful for modeling the writing style of authors. We report experimental results in an AA data set used in previous studies under several conditions (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a). Results confirm that local histograms of character n-grams are more helpful for AA than the usual global histograms of words or character n-grams (Luyckx and Daelemans, 2010); our results are superior to those reported in related works. We also show that local histograms over character n-grams are more helpful than local histograms over words, as originally proposed by (Lebanon et al., 2007). Further, we performed experiments with imbalanced and small training sets (i.e., under a realistic AA setting) using the aforementioned rep</context>
<context position="6218" citStr="Houvardas and Stamatatos, 2006" startWordPosition="942" endWordPosition="946"> 2005) is the most effective among those we tried, although competitive performance can be obtained with simpler kernels. • We report experimental results that are superior to state of the art approaches (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), with improvements ranging from 2%−6% in balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent</context>
<context position="8319" citStr="Houvardas and Stamatatos, 2006" startWordPosition="1275" endWordPosition="1278">al information would be obtained with n E 11 ... D} where D is the maximum number of words in a document). With respect to character-based features, n-grams at the character level have been widely used in AA as well (Plakias and Stamatatos, 2008b; Peng et al., 2003; Luyckx and Daelemans, 2010). Peng et al. (2003) propose the use of language models at the n-gram character-level for AA, whereas Keselj et al. (2003) build author profiles based on a selection of frequent n-grams for each author. Stamatatos and co-workers have studied the impact of feature selection, with character n-grams, in AA (Houvardas and Stamatatos, 2006; Stamatatos, 2006a), ensemble learning with character n-grams (Stamatatos, 2006b) and novel classification techniques based 289 on characters at the n-gram level (Plakias and Sta- with SVMs. matatos, 2008a). Acceptable performance in AA has been reported with character n-gram representations. However, as with word-based features, character n-grams are unable to incorporate sequential information from documents in their original form (in terms of the positions in which the terms appear across a document). We believe that sequential clues can be helpful for AA because different authors are expe</context>
<context position="16883" citStr="Houvardas and Stamatatos, 2006" startWordPosition="2714" endWordPosition="2717">candidate author is either a LOWBOW histogram or a bag of local histograms (BOLH). Recall that LOWBOW histograms are an un-weighted sum of local histograms and hence can be considered a summary of term usage and sequential information; whereas the BOLH can be seen as term occurrence frequencies across different locations of the document. For both types of representations we consider an SVM classifier under the one-vs-all formulation for facing the AA problem. We consider SVM as base classifier because this method has proved to be very effective in a large number of applications, including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a); further, since SVMs are kernel-based methods, they allow us to use local histograms for AA by considering kernels that work over sets of histograms. We build a multiclass SVM classifier by considering the pairs of patterns-outputs associated to documents-authors. Where each pattern can be either a LOWBOW histogram or the set of local histograms associated with the corresponding document, and the output associated to each pattern is a categorical random variable (outputs) that associates the representation of each document to its </context>
<context position="18225" citStr="Houvardas and Stamatatos, 2006" startWordPosition="2934" endWordPosition="2938">s classifier we adopted the one-vs-all formulation, where C binary classifiers are built and where each classifier fi discriminates among examples from class i (positive examples) and the rest j : j E {1, ... , C}, j =� i; despite being one of the simplest formulations, this approach has shown to obtain comparable and even superior performance to that obtained by more complex formulations (Rifkin and Klautau, 2004). For AA using LOWBOW histograms, we consider a linear kernel since it has been successfully applied to a wide variety of problems (ShaweTaylor and Cristianini, 2004), including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b). However, standard kernels cannot work for input spaces where each instance is described by a set of vectors. Therefore, usual kernels are not applicable for AA using BOLH. Instead, we rely on particular kernels defined for sets of vectors rather than for a single vector. Specifically, we consider kernels of the form (Rubner et al., 2001; Grauman, 2006): where D(P, Q) is the sum of the distances between the elements of the bag of local histograms associated to author P and the elements of the bag of histograms associated with author Q; -y is the scale parameter</context>
<context position="20821" citStr="Houvardas and Stamatatos, 2006" startWordPosition="3395" endWordPosition="3398"> (5) 292 documents authored by 10 authors. All of the documents belong to the same topic. Since this data set has predefined training and testing partitions, our results are comparable to those obtained by other researchers. There are 50 documents per author for training and 50 documents per author for testing. We performed experiments with LOWBOW3 representations at word and character-level. For the experiments with words, we took the top 2,500 most common words used across the training documents and obtained LOWBOW representations. We used this setting in agreement with previous work on AA (Houvardas and Stamatatos, 2006). For our character n-gram experiments, we obtained LOWBOW representations for character 3-grams (only n-grams of size n = 3 were used) considering the 2,500 most common n-grams. Again, this setting was adopted in agreement with previous work on AA with character n-grams (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Luyckx and Daelemans, 2010). All our experiments use the SVM implementation provided by Canu et al. (2005). 5.1 Experimental settings In order to compare our methods to related works we adopted the following experimental setting. We </context>
<context position="26756" citStr="Houvardas and Stamatatos, 2006" startWordPosition="4358" endWordPosition="4361">etter than the best configuration of LOWBOW histograms, see Table 2. Furthermore, that result is more than 5% higher than the best reported result in related work (80.8% as reported in (Plakias and Stamatatos, 2008b)). Therefore, the considered local histogram representations over character n-grams have proved to be very effective for AA. One should note that, in general, better performance was obtained when using character-level rather than word-level information. This confirms the results already reported by other researchers that have used character-level and word-level information for AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Peng et al., 2003). We believe this can be attributed to the fact that character n-grams provide a representation for the document at a finer granularity, which can be better exploited with local histogram representations. Note that by considering 3-grams, words of length up to three are incorporated, and usually these words are function words (e.g., the, it, as, etc.), which are known to be indicative of writing style. Also, n-gram information is more dense in documents than word-level information. Hence, the local histograms are</context>
</contexts>
<marker>Houvardas, Stamatatos, 2006</marker>
<rawString>J. Houvardas and E. Stamatatos. 2006. N-gram feature selection for author identification. In Proceedings of the 12th International Conference on Artificial Intelligence: Methodology, Systems, and Applications, volume 4183 of LNCS, pages 77–86, Varna, Bulgaria. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Keselj</author>
<author>F Peng</author>
<author>N Cercone</author>
<author>C Thomas</author>
</authors>
<title>Ngram-based author profiles for authorship attribution.</title>
<date>2003</date>
<booktitle>In Proceedings of the Pacific Association for Computational Linguistics,</booktitle>
<pages>255--264</pages>
<location>Halifax, Canada.</location>
<contexts>
<context position="6491" citStr="Keselj et al., 2003" startWordPosition="982" endWordPosition="985"> ranging from 2%−6% in balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histogram</context>
<context position="8105" citStr="Keselj et al. (2003)" startWordPosition="1242" endWordPosition="1245">nately, because of computational limitations, the latter methods cannot discover enough sequential information from documents (e.g., word n-grams are often restricted to n E 11, 2, 3}, while full sequential information would be obtained with n E 11 ... D} where D is the maximum number of words in a document). With respect to character-based features, n-grams at the character level have been widely used in AA as well (Plakias and Stamatatos, 2008b; Peng et al., 2003; Luyckx and Daelemans, 2010). Peng et al. (2003) propose the use of language models at the n-gram character-level for AA, whereas Keselj et al. (2003) build author profiles based on a selection of frequent n-grams for each author. Stamatatos and co-workers have studied the impact of feature selection, with character n-grams, in AA (Houvardas and Stamatatos, 2006; Stamatatos, 2006a), ensemble learning with character n-grams (Stamatatos, 2006b) and novel classification techniques based 289 on characters at the n-gram level (Plakias and Sta- with SVMs. matatos, 2008a). Acceptable performance in AA has been reported with character n-gram representations. However, as with word-based features, character n-grams are unable to incorporate sequentia</context>
</contexts>
<marker>Keselj, Peng, Cercone, Thomas, 2003</marker>
<rawString>V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. Ngram-based author profiles for authorship attribution. In Proceedings of the Pacific Association for Computational Linguistics, pages 255–264, Halifax, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>J Schler</author>
<author>S Argamon</author>
</authors>
<title>Computational methods in authorship attribution.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<pages>60--9</pages>
<contexts>
<context position="6438" citStr="Koppel et al., 2009" startWordPosition="974" endWordPosition="977">08b; Plakias and Stamatatos, 2008a), with improvements ranging from 2%−6% in balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010)</context>
</contexts>
<marker>Koppel, Schler, Argamon, 2009</marker>
<rawString>M. Koppel, J. Schler, and S. Argamon. 2009. Computational methods in authorship attribution. Journal of the American Society for Information Science and Technology, 60:9–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>G Lebanon</author>
</authors>
<title>Diffusion kernels on statistical manifolds.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--129</pages>
<contexts>
<context position="5593" citStr="Lafferty and Lebanon, 2005" startWordPosition="842" endWordPosition="845">r modeling the writing style of authors. • We propose the use of local histograms over character-level n-grams for AA. We show that character-level representations, which have proved to be very effective for AA (Luyckx and Daelemans, 2010), can be further improved by adopting a local histogram formulation. Also, we empirically show that local histograms at the character-level are more helpful than local histograms at the word-level for AA. • We study several kernels for a support vector machine AA classifier under the local histograms formulation. Our study confirms that the diffusion kernel (Lafferty and Lebanon, 2005) is the most effective among those we tried, although competitive performance can be obtained with simpler kernels. • We report experimental results that are superior to state of the art approaches (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), with improvements ranging from 2%−6% in balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvar</context>
<context position="25023" citStr="Lafferty and Lebanon, 2005" startWordPosition="4090" endWordPosition="4093">ams, the more LOWBOW histograms approach the BOW formulation (Lebanon et al., 2007). We now describe the AA performance obtained when using the BOLH formulation; these results 293 are shown in Table 3. Most of the results from this table are superior to those reported in Table 2, showing that bags of local histograms are a better way to exploit the LOWBOW framework for AA. As expected, different kernels yield different results. However, the diffusion kernel outperformed most of the results obtained with other kernels; confirming the results obtained by other researchers (Lebanon et al., 2007; Lafferty and Lebanon, 2005). Kernel Euc. Diffusion EMD x2 Words Setting-1 78.6% 81.0% 75.0% 75.4% Setting-2 77.6% 82.0% 76.8% 77.2% Setting-3 79.2% 80.8% 77.0% 79.0% Characters Setting-1 83.4% 82.8% 84.4% 83.8% Setting-2 83.4% 84.2% 82.2% 84.6% Setting-3 83.6% 86.4% 81.0% 85.2% Table 3: Authorship attribution accuracy when using bags of local histograms and different kernels for word-based and character-based representations. The BC data set is used. Settings 1, 2 and 3 correspond to k = 2, 5 and 20, respectively. On average, the worse kernel was that based on the earth mover’s distance (EMD), suggesting that the compar</context>
</contexts>
<marker>Lafferty, Lebanon, 2005</marker>
<rawString>J. Lafferty and G. Lebanon. 2005. Diffusion kernels on statistical manifolds. Journal of Machine Learning Research, 6:129–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lambers</author>
<author>C J Veenman</author>
</authors>
<title>Forensic authorship attribution using compression distances to prototypes.</title>
<date>2009</date>
<booktitle>In Computational Forensics, Lecture Notes in Computer Science, Volume 5718. ISBN</booktitle>
<volume>13</volume>
<pages>978--3</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="2045" citStr="Lambers and Veenman, 2009" startWordPosition="300" endWordPosition="303">llenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). There is a broad field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, where the core problem is modeling the thematic content of documents (Sebastiani, 2002), the goal in AA is modeling authors’ writing style (Stamatatos, 2009b). Hence, document representations that reveal information ab</context>
<context position="6518" citStr="Lambers and Veenman, 2009" startWordPosition="986" endWordPosition="990">n balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histograms (i.e., the bagof-words pa</context>
</contexts>
<marker>Lambers, Veenman, 2009</marker>
<rawString>M. Lambers and C. J. Veenman. 2009. Forensic authorship attribution using compression distances to prototypes. In Computational Forensics, Lecture Notes in Computer Science, Volume 5718. ISBN 978-3-642-03520-3. Springer Berlin Heidelberg, 2009, p. 13, volume 5718 of LNCS, pages 13–24. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lebanon</author>
<author>Y Mao</author>
<author>J Dillon</author>
</authors>
<title>The locally weighted bag of words framework for document representation.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>8--2405</pages>
<contexts>
<context position="3310" citStr="Lebanon et al., 2007" startWordPosition="493" endWordPosition="496">od accuracy in AA. Word and character based representations have been used in AA with some success so far (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Plakias and Stamatatos, 2008b). Such representations can capture style information through word or character usage, but they lack sequential information, which can reveal further stylistic information. In this paper, we study the use of richer document representations for the AA task. In particular, we consider local histograms over n-grams at the character-level obtained via the locally-weighted bag of words (LOWBOW) framework (Lebanon et al., 2007). Under LOWBOW, a document is represented by a set of local histograms, computed across the whole document but smoothed by kernels centered on different document locations. In this way, document 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288–298, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics representations preserve both word/character usage and sequential information (i.e., information about the positions in which words or characters occur), which can be more helpful for modeling the writing style o</context>
<context position="9520" citStr="Lebanon et al., 2007" startWordPosition="1456" endWordPosition="1459">nt authors are expected to use different character n-grams or words in different parts of the document. Accordingly, in this work we adopt the popular character-based and word-based representations, but we enrich them in a way that they incorporate sequential information via the LOWBOW framework. Hence, the proposed features preserve sequential information besides capturing character and word usage information. Our hypothesis is that the combination of sequential and frequency information can be particularly helpful for AA. The LOWBOW framework has been mainly used for document visualization (Lebanon et al., 2007; Mao et al., 2007), where researchers have used information derived from local histograms for displaying a 2D representation of document’s content. More recently, Chasanis et al. (2009) used the LOWBOW framework for segmenting movies into chapters and scenes. LOWBOW representations have also been applied to discourse segmentation (AMIDA, 2007) and have been suggested for text summarization (Das and Martins, 2007). However, to the best of our knowledge the use of the LOWBOW framework for AA has not been studied elsewhere. Actually, the only two references using this framework for text categori</context>
<context position="11311" citStr="Lebanon et al., 2007" startWordPosition="1748" endWordPosition="1751">to generate a collection of documents; that is, a document i is represented as: di = [xi,1, ... , xi,jV j] (1) where V is the vocabulary and |V |is the number of elements in V , di,j = xi,j is a weight that denotes the contribution of term j to the representation of document i; usually xi,j is related to the occurrence (binary weighting) or the weighted frequency of occurrence (e.g., the tf-idf weighting scheme) of the term j in document i. 3.2 Locally-weighted bag-of-words representation Instead of using the BOW framework directly, we adopted the LOWBOW framework for document representation (Lebanon et al., 2007). The underlying idea in LOWBOW is to compute several local histograms per document, where these histograms are smoothed by a kernel function, see Figure 1. The parameters of the kernel specify the position of the kernel in the document (i.e., where the local histogram is centered) and its scale (i.e., to what extent it is smoothed). In this way the sequential information in the document is preserved together with term usage statistics. Let Wi = Jwi,1, ... , wi,Ni}, denote the terms (in order of appearance) in document i where Ni is the number of terms that appear in document i and wi,j E V is</context>
<context position="13948" citStr="Lebanon et al., 2007" startWordPosition="2216" endWordPosition="2219">tograms are computed for each document i. Each histogram dlji carries information about the distribution of terms at a certain position µj of the document, where a determines how the nearby terms to µj influence the local histogram j. Thus, sequential information of the document is considered throughout these local histograms. Note that when a is small, most of the sequential information is preserved, as local histograms are calculated at very local scales; whereas when a ≥ 1, local histograms resemble the traditional BOW representation. Under LOWBOW documents can be represented in two forms (Lebanon et al., 2007): as a single histogram dLi = const × �kj=1 dlji (hereafter LOWBOW histograms) or by the set of local histograms itself dl{1,...,k} .We performed experiments with i both forms of representation and considered words and n-grams at the character-level as terms (c.f. Section 5). Regarding the smoothing function, we considered the re-normalized Gaussian pdf restricted to [0, 1]: 1N(x°µ,σ)µ if x ∈ [0, 1] (3) 0 otherwise where O(x) is the cumulative distribution function for a Gaussian with mean 0 and standard deviation 1, evaluated at x, see (Lebanon et al., 2007) for further details. 3.3 Support v</context>
<context position="22921" citStr="Lebanon et al., 2007" startWordPosition="3737" endWordPosition="3740"> and Stamatatos, 2008b). 3We used LOWBOW code of G. Lebanon and Y. Mao available from http://www.cc.gatech.edu/∼ymao8/lowbow.htm 5.2 Experimental results in balanced data We first compare the performance of the LOWBOW histogram representation to that of the traditional BOW representation. Table 2 shows the accuracy (i.e., percentage of documents in the test set that were associated to its correct author) for the BOW and LOWBOW histogram representations when using words and character n-grams information. For LOWBOW histograms, we report results with three different configurations for p. As in (Lebanon et al., 2007), we consider uniformly distributed locations and we varied the number of locations that were included in each setting. We denote with k the number of local histograms. In preliminary experiments we tried several other values for k, although we found that representative results can be obtained with the values we considered here. Method Parameters Words Characters BOW - 78.2% 75.0% LOWBOW k = 2; v = 0.2 75.8% 72.0% LOWBOW k = 5; v = 0.2 77.4% 75.2% LOWBOW k = 20; v = 0.2 77.4% 75.0% Table 2: Authorship attribution accuracy for the BOW representation and LOWBOW histograms. Column 2 shows the par</context>
<context position="24479" citStr="Lebanon et al., 2007" startWordPosition="4003" endWordPosition="4006">because it is simpler to compute and it does not rely on parameter selection. Recall that the LOWBOW histogram representations are obtained by the combination of several local histograms calculated at different locations of the document, hence, it seems that the raw sum of local histograms results in a loss of useful information for representing documents. The worse performance was obtained when k = 2 local histograms are considered (see row 3 in Table 2). This result is somewhat expected since the larger the number of local histograms, the more LOWBOW histograms approach the BOW formulation (Lebanon et al., 2007). We now describe the AA performance obtained when using the BOLH formulation; these results 293 are shown in Table 3. Most of the results from this table are superior to those reported in Table 2, showing that bags of local histograms are a better way to exploit the LOWBOW framework for AA. As expected, different kernels yield different results. However, the diffusion kernel outperformed most of the results obtained with other kernels; confirming the results obtained by other researchers (Lebanon et al., 2007; Lafferty and Lebanon, 2005). Kernel Euc. Diffusion EMD x2 Words Setting-1 78.6% 81.</context>
</contexts>
<marker>Lebanon, Mao, Dillon, 2007</marker>
<rawString>G. Lebanon, Y. Mao, and J. Dillon. 2007. The locally weighted bag of words framework for document representation. Journal of Machine Learning Research, 8:2405–2441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>T Yang</author>
<author>F Rose</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="20145" citStr="Lewis et al., 2004" startWordPosition="3280" endWordPosition="3283">that an author uses similar terms at similar locations of the document. The Earth mover’s distance (EMD), on the other hand, is an estimate of the optimal cost in taking local histograms from Q to local histograms in P (Rubner et al., 2001); that is, this measure computes the optimal matching distance between local histograms from different authors that are not necessarily computed at similar locations. 5 Experiments and Results For our experiments we considered the data set used in (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a). This corpus is a subset of the RCV1 collection (Lewis et al., 2004) and comprises K(P, Q) = exp ( − D(P, Q)2 � � (5) 292 documents authored by 10 authors. All of the documents belong to the same topic. Since this data set has predefined training and testing partitions, our results are comparable to those obtained by other researchers. There are 50 documents per author for training and 50 documents per author for testing. We performed experiments with LOWBOW3 representations at word and character-level. For the experiments with words, we took the top 2,500 most common words used across the training documents and obtained LOWBOW representations. We used this se</context>
</contexts>
<marker>Lewis, Yang, Rose, 2004</marker>
<rawString>D. Lewis, T. Yang, and F. Rose. 2004. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Luyckx</author>
<author>W Daelemans</author>
</authors>
<title>The effect of author set size and data size in authorship attribution.</title>
<date>2010</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>1--21</pages>
<contexts>
<context position="1858" citStr="Luyckx and Daelemans, 2010" startWordPosition="271" endWordPosition="274">aracter n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). There is a broad field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, where the core problem is model</context>
<context position="4290" citStr="Luyckx and Daelemans, 2010" startWordPosition="639" endWordPosition="642">tion for Computational Linguistics representations preserve both word/character usage and sequential information (i.e., information about the positions in which words or characters occur), which can be more helpful for modeling the writing style of authors. We report experimental results in an AA data set used in previous studies under several conditions (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a). Results confirm that local histograms of character n-grams are more helpful for AA than the usual global histograms of words or character n-grams (Luyckx and Daelemans, 2010); our results are superior to those reported in related works. We also show that local histograms over character n-grams are more helpful than local histograms over words, as originally proposed by (Lebanon et al., 2007). Further, we performed experiments with imbalanced and small training sets (i.e., under a realistic AA setting) using the aforementioned representations. We found that the LOWBOW-based representation resulted even more advantageous in these challenging conditions. The contributions of this work are as follows: • We show that the LOWBOW framework can be helpful for AA, giving e</context>
<context position="7038" citStr="Luyckx and Daelemans, 2010" startWordPosition="1070" endWordPosition="1073">thods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histograms (i.e., the bagof-words paradigm) are the most frequently used representations in AA (Zhao and Zobel, 2005; Argamon and Levitan, 2005; Stamatatos, 2009b). Some researchers have gone a step further and have attempted to capture sequential information by using n-grams at the word-level (Peng et al., 2004) or by discovering maximal frequent word sequences (Coyotl-Morales et al., 2006). Unfortunately, because of computational limitations, the latter methods cannot discover enough sequential information from documents (e.g., word n-grams are oft</context>
<context position="21215" citStr="Luyckx and Daelemans, 2010" startWordPosition="3455" endWordPosition="3459">r the experiments with words, we took the top 2,500 most common words used across the training documents and obtained LOWBOW representations. We used this setting in agreement with previous work on AA (Houvardas and Stamatatos, 2006). For our character n-gram experiments, we obtained LOWBOW representations for character 3-grams (only n-grams of size n = 3 were used) considering the 2,500 most common n-grams. Again, this setting was adopted in agreement with previous work on AA with character n-grams (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Luyckx and Daelemans, 2010). All our experiments use the SVM implementation provided by Canu et al. (2005). 5.1 Experimental settings In order to compare our methods to related works we adopted the following experimental setting. We perform experiments using all of the training documents per author, that is, a balanced corpus (we call this setting BC). Next we evaluate the performance of classifiers over reduced training sets. We tried balanced reduced data sets with: 1, 3, 5 and 10 documents per author (we call this configuration RBC). Also, we experimented with reducedimbalanced data sets using the same imbalance rate</context>
</contexts>
<marker>Luyckx, Daelemans, 2010</marker>
<rawString>K. Luyckx and W. Daelemans. 2010. The effect of author set size and data size in authorship attribution. Literary and Linguistic Computing, pages 1–21, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mao</author>
<author>J Dillon</author>
<author>G Lebanon</author>
</authors>
<title>Sequential document visualization.</title>
<date>2007</date>
<journal>IEEE Transactions on Visualization and Computer Graphics,</journal>
<volume>13</volume>
<issue>6</issue>
<contexts>
<context position="9539" citStr="Mao et al., 2007" startWordPosition="1460" endWordPosition="1463">d to use different character n-grams or words in different parts of the document. Accordingly, in this work we adopt the popular character-based and word-based representations, but we enrich them in a way that they incorporate sequential information via the LOWBOW framework. Hence, the proposed features preserve sequential information besides capturing character and word usage information. Our hypothesis is that the combination of sequential and frequency information can be particularly helpful for AA. The LOWBOW framework has been mainly used for document visualization (Lebanon et al., 2007; Mao et al., 2007), where researchers have used information derived from local histograms for displaying a 2D representation of document’s content. More recently, Chasanis et al. (2009) used the LOWBOW framework for segmenting movies into chapters and scenes. LOWBOW representations have also been applied to discourse segmentation (AMIDA, 2007) and have been suggested for text summarization (Das and Martins, 2007). However, to the best of our knowledge the use of the LOWBOW framework for AA has not been studied elsewhere. Actually, the only two references using this framework for text categorization are (Lebanon</context>
</contexts>
<marker>Mao, Dillon, Lebanon, 2007</marker>
<rawString>Y. Mao, J. Dillon, and G. Lebanon. 2007. Sequential document visualization. IEEE Transactions on Visualization and Computer Graphics, 13(6):1208–1215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>D Shuurmans</author>
<author>V Keselj</author>
<author>S Wang</author>
</authors>
<title>Language independent authorship attribution using character level language models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th conference of the European chapter of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>267--274</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="7954" citStr="Peng et al., 2003" startWordPosition="1217" endWordPosition="1220">tion by using n-grams at the word-level (Peng et al., 2004) or by discovering maximal frequent word sequences (Coyotl-Morales et al., 2006). Unfortunately, because of computational limitations, the latter methods cannot discover enough sequential information from documents (e.g., word n-grams are often restricted to n E 11, 2, 3}, while full sequential information would be obtained with n E 11 ... D} where D is the maximum number of words in a document). With respect to character-based features, n-grams at the character level have been widely used in AA as well (Plakias and Stamatatos, 2008b; Peng et al., 2003; Luyckx and Daelemans, 2010). Peng et al. (2003) propose the use of language models at the n-gram character-level for AA, whereas Keselj et al. (2003) build author profiles based on a selection of frequent n-grams for each author. Stamatatos and co-workers have studied the impact of feature selection, with character n-grams, in AA (Houvardas and Stamatatos, 2006; Stamatatos, 2006a), ensemble learning with character n-grams (Stamatatos, 2006b) and novel classification techniques based 289 on characters at the n-gram level (Plakias and Sta- with SVMs. matatos, 2008a). Acceptable performance in </context>
<context position="26838" citStr="Peng et al., 2003" startWordPosition="4371" endWordPosition="4374">is more than 5% higher than the best reported result in related work (80.8% as reported in (Plakias and Stamatatos, 2008b)). Therefore, the considered local histogram representations over character n-grams have proved to be very effective for AA. One should note that, in general, better performance was obtained when using character-level rather than word-level information. This confirms the results already reported by other researchers that have used character-level and word-level information for AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Peng et al., 2003). We believe this can be attributed to the fact that character n-grams provide a representation for the document at a finer granularity, which can be better exploited with local histogram representations. Note that by considering 3-grams, words of length up to three are incorporated, and usually these words are function words (e.g., the, it, as, etc.), which are known to be indicative of writing style. Also, n-gram information is more dense in documents than word-level information. Hence, the local histograms are less sparse when using character-level information, which results in better AA pe</context>
</contexts>
<marker>Peng, Shuurmans, Keselj, Wang, 2003</marker>
<rawString>F. Peng, D. Shuurmans, V. Keselj, and S. Wang. 2003. Language independent authorship attribution using character level language models. In Proceedings of the 10th conference of the European chapter of the Association for Computational Linguistics, volume 1, pages 267–274, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>D Shuurmans</author>
<author>S Wang</author>
</authors>
<title>Augmenting naive Bayes classifiers with statistical language models.</title>
<date>2004</date>
<journal>Information Retrieval Journal,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="7396" citStr="Peng et al., 2004" startWordPosition="1124" endWordPosition="1127">y of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histograms (i.e., the bagof-words paradigm) are the most frequently used representations in AA (Zhao and Zobel, 2005; Argamon and Levitan, 2005; Stamatatos, 2009b). Some researchers have gone a step further and have attempted to capture sequential information by using n-grams at the word-level (Peng et al., 2004) or by discovering maximal frequent word sequences (Coyotl-Morales et al., 2006). Unfortunately, because of computational limitations, the latter methods cannot discover enough sequential information from documents (e.g., word n-grams are often restricted to n E 11, 2, 3}, while full sequential information would be obtained with n E 11 ... D} where D is the maximum number of words in a document). With respect to character-based features, n-grams at the character level have been widely used in AA as well (Plakias and Stamatatos, 2008b; Peng et al., 2003; Luyckx and Daelemans, 2010). Peng et al.</context>
</contexts>
<marker>Peng, Shuurmans, Wang, 2004</marker>
<rawString>F. Peng, D. Shuurmans, and S. Wang. 2004. Augmenting naive Bayes classifiers with statistical language models. Information Retrieval Journal, 7(1):317–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Pillay</author>
<author>T Solorio</author>
</authors>
<title>Authorship attribution of web forum posts.</title>
<date>2010</date>
<booktitle>In Proceedings of the eCrime Researchers Summit (eCrime),</booktitle>
<pages>1--7</pages>
<publisher>IEEE.</publisher>
<location>Dallas, TX, USA.</location>
<contexts>
<context position="2088" citStr="Pillay and Solorio, 2010" startWordPosition="306" endWordPosition="309">ed and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). There is a broad field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, where the core problem is modeling the thematic content of documents (Sebastiani, 2002), the goal in AA is modeling authors’ writing style (Stamatatos, 2009b). Hence, document representations that reveal information about writing style are required to achieve g</context>
</contexts>
<marker>Pillay, Solorio, 2010</marker>
<rawString>S. R. Pillay and T. Solorio. 2010. Authorship attribution of web forum posts. In Proceedings of the eCrime Researchers Summit (eCrime), 2010, pages 1–7, Dallas, TX, USA. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Plakias</author>
<author>E Stamatatos</author>
</authors>
<title>Author identification using a tensor space representation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th European Conference on Artificial Intelligence,</booktitle>
<volume>178</volume>
<pages>833--834</pages>
<publisher>IOS Press.</publisher>
<location>Patras, Greece.</location>
<contexts>
<context position="2884" citStr="Plakias and Stamatatos, 2008" startWordPosition="428" endWordPosition="431">oblem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, where the core problem is modeling the thematic content of documents (Sebastiani, 2002), the goal in AA is modeling authors’ writing style (Stamatatos, 2009b). Hence, document representations that reveal information about writing style are required to achieve good accuracy in AA. Word and character based representations have been used in AA with some success so far (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Plakias and Stamatatos, 2008b). Such representations can capture style information through word or character usage, but they lack sequential information, which can reveal further stylistic information. In this paper, we study the use of richer document representations for the AA task. In particular, we consider local histograms over n-grams at the character-level obtained via the locally-weighted bag of words (LOWBOW) framework (Lebanon et al., 2007). Under LOWBOW, a document is represented by a set of local histograms, computed across the whole document but smoothed by kernels centered on different document locations. I</context>
<context position="4112" citStr="Plakias and Stamatatos, 2008" startWordPosition="611" endWordPosition="614">n this way, document 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288–298, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics representations preserve both word/character usage and sequential information (i.e., information about the positions in which words or characters occur), which can be more helpful for modeling the writing style of authors. We report experimental results in an AA data set used in previous studies under several conditions (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a). Results confirm that local histograms of character n-grams are more helpful for AA than the usual global histograms of words or character n-grams (Luyckx and Daelemans, 2010); our results are superior to those reported in related works. We also show that local histograms over character n-grams are more helpful than local histograms over words, as originally proposed by (Lebanon et al., 2007). Further, we performed experiments with imbalanced and small training sets (i.e., under a realistic AA setting) using the aforementioned representations. We found that the LOWBOW-based representation r</context>
<context position="5820" citStr="Plakias and Stamatatos, 2008" startWordPosition="878" endWordPosition="881">lemans, 2010), can be further improved by adopting a local histogram formulation. Also, we empirically show that local histograms at the character-level are more helpful than local histograms at the word-level for AA. • We study several kernels for a support vector machine AA classifier under the local histograms formulation. Our study confirms that the diffusion kernel (Lafferty and Lebanon, 2005) is the most effective among those we tried, although competitive performance can be obtained with simpler kernels. • We report experimental results that are superior to state of the art approaches (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), with improvements ranging from 2%−6% in balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Ko</context>
<context position="7934" citStr="Plakias and Stamatatos, 2008" startWordPosition="1213" endWordPosition="1216">d to capture sequential information by using n-grams at the word-level (Peng et al., 2004) or by discovering maximal frequent word sequences (Coyotl-Morales et al., 2006). Unfortunately, because of computational limitations, the latter methods cannot discover enough sequential information from documents (e.g., word n-grams are often restricted to n E 11, 2, 3}, while full sequential information would be obtained with n E 11 ... D} where D is the maximum number of words in a document). With respect to character-based features, n-grams at the character level have been widely used in AA as well (Plakias and Stamatatos, 2008b; Peng et al., 2003; Luyckx and Daelemans, 2010). Peng et al. (2003) propose the use of language models at the n-gram character-level for AA, whereas Keselj et al. (2003) build author profiles based on a selection of frequent n-grams for each author. Stamatatos and co-workers have studied the impact of feature selection, with character n-grams, in AA (Houvardas and Stamatatos, 2006; Stamatatos, 2006a), ensemble learning with character n-grams (Stamatatos, 2006b) and novel classification techniques based 289 on characters at the n-gram level (Plakias and Sta- with SVMs. matatos, 2008a). Accept</context>
<context position="16913" citStr="Plakias and Stamatatos, 2008" startWordPosition="2718" endWordPosition="2721">BOW histogram or a bag of local histograms (BOLH). Recall that LOWBOW histograms are an un-weighted sum of local histograms and hence can be considered a summary of term usage and sequential information; whereas the BOLH can be seen as term occurrence frequencies across different locations of the document. For both types of representations we consider an SVM classifier under the one-vs-all formulation for facing the AA problem. We consider SVM as base classifier because this method has proved to be very effective in a large number of applications, including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a); further, since SVMs are kernel-based methods, they allow us to use local histograms for AA by considering kernels that work over sets of histograms. We build a multiclass SVM classifier by considering the pairs of patterns-outputs associated to documents-authors. Where each pattern can be either a LOWBOW histogram or the set of local histograms associated with the corresponding document, and the output associated to each pattern is a categorical random variable (outputs) that associates the representation of each document to its corresponding author y1�...�N </context>
<context position="18255" citStr="Plakias and Stamatatos, 2008" startWordPosition="2939" endWordPosition="2943">vs-all formulation, where C binary classifiers are built and where each classifier fi discriminates among examples from class i (positive examples) and the rest j : j E {1, ... , C}, j =� i; despite being one of the simplest formulations, this approach has shown to obtain comparable and even superior performance to that obtained by more complex formulations (Rifkin and Klautau, 2004). For AA using LOWBOW histograms, we consider a linear kernel since it has been successfully applied to a wide variety of problems (ShaweTaylor and Cristianini, 2004), including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b). However, standard kernels cannot work for input spaces where each instance is described by a set of vectors. Therefore, usual kernels are not applicable for AA using BOLH. Instead, we rely on particular kernels defined for sets of vectors rather than for a single vector. Specifically, we consider kernels of the form (Rubner et al., 2001; Grauman, 2006): where D(P, Q) is the sum of the distances between the elements of the bag of local histograms associated to author P and the elements of the bag of histograms associated with author Q; -y is the scale parameter of K. Let P = {p1, ... , pk} </context>
<context position="20043" citStr="Plakias and Stamatatos, 2008" startWordPosition="3262" endWordPosition="3265">t the same locations are compared to each other. We believe that for AA this is advantageous as it is expected that an author uses similar terms at similar locations of the document. The Earth mover’s distance (EMD), on the other hand, is an estimate of the optimal cost in taking local histograms from Q to local histograms in P (Rubner et al., 2001); that is, this measure computes the optimal matching distance between local histograms from different authors that are not necessarily computed at similar locations. 5 Experiments and Results For our experiments we considered the data set used in (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a). This corpus is a subset of the RCV1 collection (Lewis et al., 2004) and comprises K(P, Q) = exp ( − D(P, Q)2 � � (5) 292 documents authored by 10 authors. All of the documents belong to the same topic. Since this data set has predefined training and testing partitions, our results are comparable to those obtained by other researchers. There are 50 documents per author for training and 50 documents per author for testing. We performed experiments with LOWBOW3 representations at word and character-level. For the experiments with words, we took the top 2,500 mos</context>
<context position="21858" citStr="Plakias and Stamatatos, 2008" startWordPosition="3562" endWordPosition="3565">iments use the SVM implementation provided by Canu et al. (2005). 5.1 Experimental settings In order to compare our methods to related works we adopted the following experimental setting. We perform experiments using all of the training documents per author, that is, a balanced corpus (we call this setting BC). Next we evaluate the performance of classifiers over reduced training sets. We tried balanced reduced data sets with: 1, 3, 5 and 10 documents per author (we call this configuration RBC). Also, we experimented with reducedimbalanced data sets using the same imbalance rates reported in (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a): we tried settings 2 − 10, 5 − 10, and 10 − 20, where, for example, setting 2- 10 means that we use at least 2 and at most 10 documents per author (we call this setting IRBC). BC setting represents the AA problem under ideal conditions, whereas settings RBC and IRBC aim at emulating a more realistic scenario, where limited sample documents are available and the whole data set is highly imbalanced (Plakias and Stamatatos, 2008b). 3We used LOWBOW code of G. Lebanon and Y. Mao available from http://www.cc.gatech.edu/∼ymao8/lowbow.htm 5.2 Experimental results in b</context>
<context position="26340" citStr="Plakias and Stamatatos, 2008" startWordPosition="4299" endWordPosition="4302">his is the only kernel that compares local histograms at different locations). This result evidences that authors use similar word/character distributions at similar locations when writing different documents. The best performance across settings and kernels was obtained with the diffusion kernel (in bold, column 3, row 9) (86.4%); that result is 8% higher than that obtained with the BOW representation and 9% better than the best configuration of LOWBOW histograms, see Table 2. Furthermore, that result is more than 5% higher than the best reported result in related work (80.8% as reported in (Plakias and Stamatatos, 2008b)). Therefore, the considered local histogram representations over character n-grams have proved to be very effective for AA. One should note that, in general, better performance was obtained when using character-level rather than word-level information. This confirms the results already reported by other researchers that have used character-level and word-level information for AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Peng et al., 2003). We believe this can be attributed to the fact that character n-grams provide a representation for th</context>
<context position="31492" citStr="Plakias and Stamatatos, 2008" startWordPosition="5232" endWordPosition="5236"> in Table 6) the corresponding differences in performance are of 5.4%, 6.4%, 6.4%, 6% and 11.4%, when using 1, 3, 5, 10, and 50 documents per author, respectively. In this case, the larger improvement was obtained when 50 documents per author are available; nevertheless, one should note that results using character-level information are, in general, significantly better than those obtained with word-level information; hence, improvements are expected to be smaller. When we compare the results of the BOLH formulation with the best reported results elsewhere (c.f. last row 6 in Tables 5 and 6) (Plakias and Stamatatos, 2008b), we found that the improvements range from 14% to 30.2% when using character ngrams and from 1.2% to 26% when using words. The differences in performance are larger when less information is used (e.g., when 5 documents are used for training) and we believe the differences would be even larger if results for 1 and 3 documents were available. These are very positive results; for example, we can obtain almost 71% of accuracy, using local histograms of character n-grams when a single document is available per author (recall that we have used all of the test samples for evaluating the performanc</context>
</contexts>
<marker>Plakias, Stamatatos, 2008</marker>
<rawString>S. Plakias and E. Stamatatos. 2008a. Author identification using a tensor space representation. In Proceedings of the 18th European Conference on Artificial Intelligence, volume 178, pages 833–834, Patras, Greece. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Plakias</author>
<author>E Stamatatos</author>
</authors>
<title>Tensor space models for authorship attribution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th Hellenic Conference on Artificial Intelligence: Theories, Models and Applications,</booktitle>
<volume>5138</volume>
<pages>239--249</pages>
<publisher>Springer.</publisher>
<location>Syros, Greece.</location>
<contexts>
<context position="34034" citStr="Plakias and Stamatatos, 2008" startWordPosition="5659" endWordPosition="5662">documents per author decreases. 295 WORDS Data set Balanced Imbalanced Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20 BOW 36.8% 57.1% 62.4% 69.9% 78.2% 62.3% 67.2% 71.2% LOWBOW 37.9% 55.6% 60.5% 69.3% 77.4% 61.1% 67.4% 71.5% Diffusion kernel 52.4% 63.3% 69.2% 72.8% 82.0% 66.6% 70.7% 74.1% Reference - - 53.4% 67.8% 80.8% 49.2% 59.8% 63.0% Table 5: AA accuracy in RBC (columns 2-6) and IRBC (columns 7-9) data sets when using words as terms. We report results for the BOW, LOWBOW histogram and BOLH representations. For reference (last row), we also include the best result reported in (Plakias and Stamatatos, 2008b), when available, for each configuration. CHARACTER N-GRAMS Data set Balanced Imbalanced Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20 BOW 65.3% 71.9% 74.2% 76.2% 75.0% 70.1% 73.4% 73.1% LOWBOW 61.9% 71.6% 74.5% 73.8% 75.0% 70.8% 72.8% 72.1% Diffusion kernel 70.7% 78.3% 80.6% 82.2% 86.4% 77.8% 80.5% 82.2% Reference - - 50.4% 67.8% 76.6% 49.2% 59.8% 63.0% Table 6: AA accuracy in the RBC and IRBC data sets when using character n-grams as terms. 6 Conclusions We have described the use of local histograms (LH) over character n-grams for AA. LHs are enriched histogram representation</context>
</contexts>
<marker>Plakias, Stamatatos, 2008</marker>
<rawString>S. Plakias and E. Stamatatos. 2008b. Tensor space models for authorship attribution. In Proceedings of the 5th Hellenic Conference on Artificial Intelligence: Theories, Models and Applications, volume 5138 of LNCS, pages 239–249, Syros, Greece. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rifkin</author>
<author>A Klautau</author>
</authors>
<title>In defense of one-vs-all classification.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--101</pages>
<contexts>
<context position="18013" citStr="Rifkin and Klautau, 2004" startWordPosition="2899" endWordPosition="2902">tegorical random variable (outputs) that associates the representation of each document to its corresponding author y1�...�N E {1, ... , C}, with C the number of candidate authors. For building the multiclass classifier we adopted the one-vs-all formulation, where C binary classifiers are built and where each classifier fi discriminates among examples from class i (positive examples) and the rest j : j E {1, ... , C}, j =� i; despite being one of the simplest formulations, this approach has shown to obtain comparable and even superior performance to that obtained by more complex formulations (Rifkin and Klautau, 2004). For AA using LOWBOW histograms, we consider a linear kernel since it has been successfully applied to a wide variety of problems (ShaweTaylor and Cristianini, 2004), including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b). However, standard kernels cannot work for input spaces where each instance is described by a set of vectors. Therefore, usual kernels are not applicable for AA using BOLH. Instead, we rely on particular kernels defined for sets of vectors rather than for a single vector. Specifically, we consider kernels of the form (Rubner et al., 2001; Grauman, 2006)</context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>R. Rifkin and A. Klautau. 2004. In defense of one-vs-all classification. Journal of Machine Learning Research, 5:101–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Rubner</author>
<author>C Tomasi</author>
<author>J Leonidas</author>
<author>J Guibas</author>
</authors>
<title>The earth mover’s distance as a metric for image retrieval.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="18597" citStr="Rubner et al., 2001" startWordPosition="2999" endWordPosition="3002">ulations (Rifkin and Klautau, 2004). For AA using LOWBOW histograms, we consider a linear kernel since it has been successfully applied to a wide variety of problems (ShaweTaylor and Cristianini, 2004), including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b). However, standard kernels cannot work for input spaces where each instance is described by a set of vectors. Therefore, usual kernels are not applicable for AA using BOLH. Instead, we rely on particular kernels defined for sets of vectors rather than for a single vector. Specifically, we consider kernels of the form (Rubner et al., 2001; Grauman, 2006): where D(P, Q) is the sum of the distances between the elements of the bag of local histograms associated to author P and the elements of the bag of histograms associated with author Q; -y is the scale parameter of K. Let P = {p1, ... , pk} and Q = {q1, ... , qk} be the elements of the bags of local histograms for instances P and Q, respectively, Table 1 presents the distance measures we consider for AA using local histograms. Kernel Distance Diffusion D(P, Q) = Pkl=1 arccos ¡(.,/pl � �ql�¢ EMD D(P, Q) = EMD(P, Q) qPk Eucidean D(P, Q) = l=1(pl − ql).2 x2 D(P) Q) = q Pl=1 (pl−q</context>
</contexts>
<marker>Rubner, Tomasi, Leonidas, Guibas, 2001</marker>
<rawString>Y. Rubner, C. Tomasi, J. Leonidas, and J. Guibas. 2001. The earth mover’s distance as a metric for image retrieval. International Journal of Computer Vision, 40(2):99–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2514" citStr="Sebastiani, 2002" startWordPosition="372" endWordPosition="373">field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, where the core problem is modeling the thematic content of documents (Sebastiani, 2002), the goal in AA is modeling authors’ writing style (Stamatatos, 2009b). Hence, document representations that reveal information about writing style are required to achieve good accuracy in AA. Word and character based representations have been used in AA with some success so far (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Plakias and Stamatatos, 2008b). Such representations can capture style information through word or character usage, but they lack sequential information, which can reveal further stylistic information. In this paper, we study the use of richer document repre</context>
<context position="6909" citStr="Sebastiani, 2002" startWordPosition="1055" endWordPosition="1056">os, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histograms (i.e., the bagof-words paradigm) are the most frequently used representations in AA (Zhao and Zobel, 2005; Argamon and Levitan, 2005; Stamatatos, 2009b). Some researchers have gone a step further and have attempted to capture sequential information by using n-grams at the word-level (Peng et al., 2004) or by discovering maximal frequent word sequences (Coyotl-Morales et al., 2006). Unfortunately, because of compu</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="14759" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2348" endWordPosition="2351">of representation and considered words and n-grams at the character-level as terms (c.f. Section 5). Regarding the smoothing function, we considered the re-normalized Gaussian pdf restricted to [0, 1]: 1N(x°µ,σ)µ if x ∈ [0, 1] (3) 0 otherwise where O(x) is the cumulative distribution function for a Gaussian with mean 0 and standard deviation 1, evaluated at x, see (Lebanon et al., 2007) for further details. 3.3 Support vector machines Support vector machines (SVMs) are pattern classification methods that aim to find an optimal separating hyperplane between examples from two different classes (Shawe-Taylor and Cristianini, 2004). Let {xi, yi}N be pairs of training patterns-outputs, where xi ∈ Rd and y ∈ {−1, 1}, with d the dimensionality of the problem. SVMs aim at learning a mapping from training instances to outputs. This is done by considering a linear function of the form: f(x) = Wx + b, where parameters W and b are learned from training data. The particular linear function considered by SVMs is as follows: f(x) = � aiyiK(xi, x) − b (4) i that is, a linear function over (a subset of) training examples, where ai is the weight associated with training example i (those for which ai &gt; 0 are the so called support vect</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>Authorship attribution based on feature set subspacing ensembles.</title>
<date>2006</date>
<journal>International Journal on Artificial Intelligence Tools,</journal>
<volume>15</volume>
<issue>5</issue>
<contexts>
<context position="1830" citStr="Stamatatos, 2006" startWordPosition="269" endWordPosition="270">m that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). There is a broad field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, whe</context>
<context position="4051" citStr="Stamatatos, 2006" startWordPosition="605" endWordPosition="606">rnels centered on different document locations. In this way, document 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288–298, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics representations preserve both word/character usage and sequential information (i.e., information about the positions in which words or characters occur), which can be more helpful for modeling the writing style of authors. We report experimental results in an AA data set used in previous studies under several conditions (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a). Results confirm that local histograms of character n-grams are more helpful for AA than the usual global histograms of words or character n-grams (Luyckx and Daelemans, 2010); our results are superior to those reported in related works. We also show that local histograms over character n-grams are more helpful than local histograms over words, as originally proposed by (Lebanon et al., 2007). Further, we performed experiments with imbalanced and small training sets (i.e., under a realistic AA setting) using the aforementioned rep</context>
<context position="6218" citStr="Stamatatos, 2006" startWordPosition="944" endWordPosition="946">most effective among those we tried, although competitive performance can be obtained with simpler kernels. • We report experimental results that are superior to state of the art approaches (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), with improvements ranging from 2%−6% in balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent</context>
<context position="8319" citStr="Stamatatos, 2006" startWordPosition="1277" endWordPosition="1278"> would be obtained with n E 11 ... D} where D is the maximum number of words in a document). With respect to character-based features, n-grams at the character level have been widely used in AA as well (Plakias and Stamatatos, 2008b; Peng et al., 2003; Luyckx and Daelemans, 2010). Peng et al. (2003) propose the use of language models at the n-gram character-level for AA, whereas Keselj et al. (2003) build author profiles based on a selection of frequent n-grams for each author. Stamatatos and co-workers have studied the impact of feature selection, with character n-grams, in AA (Houvardas and Stamatatos, 2006; Stamatatos, 2006a), ensemble learning with character n-grams (Stamatatos, 2006b) and novel classification techniques based 289 on characters at the n-gram level (Plakias and Sta- with SVMs. matatos, 2008a). Acceptable performance in AA has been reported with character n-gram representations. However, as with word-based features, character n-grams are unable to incorporate sequential information from documents in their original form (in terms of the positions in which the terms appear across a document). We believe that sequential clues can be helpful for AA because different authors are expe</context>
<context position="16883" citStr="Stamatatos, 2006" startWordPosition="2716" endWordPosition="2717">or is either a LOWBOW histogram or a bag of local histograms (BOLH). Recall that LOWBOW histograms are an un-weighted sum of local histograms and hence can be considered a summary of term usage and sequential information; whereas the BOLH can be seen as term occurrence frequencies across different locations of the document. For both types of representations we consider an SVM classifier under the one-vs-all formulation for facing the AA problem. We consider SVM as base classifier because this method has proved to be very effective in a large number of applications, including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a); further, since SVMs are kernel-based methods, they allow us to use local histograms for AA by considering kernels that work over sets of histograms. We build a multiclass SVM classifier by considering the pairs of patterns-outputs associated to documents-authors. Where each pattern can be either a LOWBOW histogram or the set of local histograms associated with the corresponding document, and the output associated to each pattern is a categorical random variable (outputs) that associates the representation of each document to its </context>
<context position="18225" citStr="Stamatatos, 2006" startWordPosition="2937" endWordPosition="2938">e adopted the one-vs-all formulation, where C binary classifiers are built and where each classifier fi discriminates among examples from class i (positive examples) and the rest j : j E {1, ... , C}, j =� i; despite being one of the simplest formulations, this approach has shown to obtain comparable and even superior performance to that obtained by more complex formulations (Rifkin and Klautau, 2004). For AA using LOWBOW histograms, we consider a linear kernel since it has been successfully applied to a wide variety of problems (ShaweTaylor and Cristianini, 2004), including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b). However, standard kernels cannot work for input spaces where each instance is described by a set of vectors. Therefore, usual kernels are not applicable for AA using BOLH. Instead, we rely on particular kernels defined for sets of vectors rather than for a single vector. Specifically, we consider kernels of the form (Rubner et al., 2001; Grauman, 2006): where D(P, Q) is the sum of the distances between the elements of the bag of local histograms associated to author P and the elements of the bag of histograms associated with author Q; -y is the scale parameter</context>
<context position="20821" citStr="Stamatatos, 2006" startWordPosition="3397" endWordPosition="3398">ents authored by 10 authors. All of the documents belong to the same topic. Since this data set has predefined training and testing partitions, our results are comparable to those obtained by other researchers. There are 50 documents per author for training and 50 documents per author for testing. We performed experiments with LOWBOW3 representations at word and character-level. For the experiments with words, we took the top 2,500 most common words used across the training documents and obtained LOWBOW representations. We used this setting in agreement with previous work on AA (Houvardas and Stamatatos, 2006). For our character n-gram experiments, we obtained LOWBOW representations for character 3-grams (only n-grams of size n = 3 were used) considering the 2,500 most common n-grams. Again, this setting was adopted in agreement with previous work on AA with character n-grams (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Luyckx and Daelemans, 2010). All our experiments use the SVM implementation provided by Canu et al. (2005). 5.1 Experimental settings In order to compare our methods to related works we adopted the following experimental setting. We </context>
<context position="26756" citStr="Stamatatos, 2006" startWordPosition="4360" endWordPosition="4361"> best configuration of LOWBOW histograms, see Table 2. Furthermore, that result is more than 5% higher than the best reported result in related work (80.8% as reported in (Plakias and Stamatatos, 2008b)). Therefore, the considered local histogram representations over character n-grams have proved to be very effective for AA. One should note that, in general, better performance was obtained when using character-level rather than word-level information. This confirms the results already reported by other researchers that have used character-level and word-level information for AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Peng et al., 2003). We believe this can be attributed to the fact that character n-grams provide a representation for the document at a finer granularity, which can be better exploited with local histogram representations. Note that by considering 3-grams, words of length up to three are incorporated, and usually these words are function words (e.g., the, it, as, etc.), which are known to be indicative of writing style. Also, n-gram information is more dense in documents than word-level information. Hence, the local histograms are</context>
</contexts>
<marker>Stamatatos, 2006</marker>
<rawString>E. Stamatatos. 2006a. Authorship attribution based on feature set subspacing ensembles. International Journal on Artificial Intelligence Tools, 15(5):823–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>Ensemble-based author identification using character n-grams.</title>
<date>2006</date>
<booktitle>In Proceedings of the 3rd International Workshop on Text-based Information Retrieval,</booktitle>
<pages>41--46</pages>
<location>Riva del Garda, Italy.</location>
<contexts>
<context position="1830" citStr="Stamatatos, 2006" startWordPosition="269" endWordPosition="270">m that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). There is a broad field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, whe</context>
<context position="4051" citStr="Stamatatos, 2006" startWordPosition="605" endWordPosition="606">rnels centered on different document locations. In this way, document 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288–298, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics representations preserve both word/character usage and sequential information (i.e., information about the positions in which words or characters occur), which can be more helpful for modeling the writing style of authors. We report experimental results in an AA data set used in previous studies under several conditions (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a). Results confirm that local histograms of character n-grams are more helpful for AA than the usual global histograms of words or character n-grams (Luyckx and Daelemans, 2010); our results are superior to those reported in related works. We also show that local histograms over character n-grams are more helpful than local histograms over words, as originally proposed by (Lebanon et al., 2007). Further, we performed experiments with imbalanced and small training sets (i.e., under a realistic AA setting) using the aforementioned rep</context>
<context position="6218" citStr="Stamatatos, 2006" startWordPosition="944" endWordPosition="946">most effective among those we tried, although competitive performance can be obtained with simpler kernels. • We report experimental results that are superior to state of the art approaches (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), with improvements ranging from 2%−6% in balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent</context>
<context position="8319" citStr="Stamatatos, 2006" startWordPosition="1277" endWordPosition="1278"> would be obtained with n E 11 ... D} where D is the maximum number of words in a document). With respect to character-based features, n-grams at the character level have been widely used in AA as well (Plakias and Stamatatos, 2008b; Peng et al., 2003; Luyckx and Daelemans, 2010). Peng et al. (2003) propose the use of language models at the n-gram character-level for AA, whereas Keselj et al. (2003) build author profiles based on a selection of frequent n-grams for each author. Stamatatos and co-workers have studied the impact of feature selection, with character n-grams, in AA (Houvardas and Stamatatos, 2006; Stamatatos, 2006a), ensemble learning with character n-grams (Stamatatos, 2006b) and novel classification techniques based 289 on characters at the n-gram level (Plakias and Sta- with SVMs. matatos, 2008a). Acceptable performance in AA has been reported with character n-gram representations. However, as with word-based features, character n-grams are unable to incorporate sequential information from documents in their original form (in terms of the positions in which the terms appear across a document). We believe that sequential clues can be helpful for AA because different authors are expe</context>
<context position="16883" citStr="Stamatatos, 2006" startWordPosition="2716" endWordPosition="2717">or is either a LOWBOW histogram or a bag of local histograms (BOLH). Recall that LOWBOW histograms are an un-weighted sum of local histograms and hence can be considered a summary of term usage and sequential information; whereas the BOLH can be seen as term occurrence frequencies across different locations of the document. For both types of representations we consider an SVM classifier under the one-vs-all formulation for facing the AA problem. We consider SVM as base classifier because this method has proved to be very effective in a large number of applications, including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a); further, since SVMs are kernel-based methods, they allow us to use local histograms for AA by considering kernels that work over sets of histograms. We build a multiclass SVM classifier by considering the pairs of patterns-outputs associated to documents-authors. Where each pattern can be either a LOWBOW histogram or the set of local histograms associated with the corresponding document, and the output associated to each pattern is a categorical random variable (outputs) that associates the representation of each document to its </context>
<context position="18225" citStr="Stamatatos, 2006" startWordPosition="2937" endWordPosition="2938">e adopted the one-vs-all formulation, where C binary classifiers are built and where each classifier fi discriminates among examples from class i (positive examples) and the rest j : j E {1, ... , C}, j =� i; despite being one of the simplest formulations, this approach has shown to obtain comparable and even superior performance to that obtained by more complex formulations (Rifkin and Klautau, 2004). For AA using LOWBOW histograms, we consider a linear kernel since it has been successfully applied to a wide variety of problems (ShaweTaylor and Cristianini, 2004), including AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b). However, standard kernels cannot work for input spaces where each instance is described by a set of vectors. Therefore, usual kernels are not applicable for AA using BOLH. Instead, we rely on particular kernels defined for sets of vectors rather than for a single vector. Specifically, we consider kernels of the form (Rubner et al., 2001; Grauman, 2006): where D(P, Q) is the sum of the distances between the elements of the bag of local histograms associated to author P and the elements of the bag of histograms associated with author Q; -y is the scale parameter</context>
<context position="20821" citStr="Stamatatos, 2006" startWordPosition="3397" endWordPosition="3398">ents authored by 10 authors. All of the documents belong to the same topic. Since this data set has predefined training and testing partitions, our results are comparable to those obtained by other researchers. There are 50 documents per author for training and 50 documents per author for testing. We performed experiments with LOWBOW3 representations at word and character-level. For the experiments with words, we took the top 2,500 most common words used across the training documents and obtained LOWBOW representations. We used this setting in agreement with previous work on AA (Houvardas and Stamatatos, 2006). For our character n-gram experiments, we obtained LOWBOW representations for character 3-grams (only n-grams of size n = 3 were used) considering the 2,500 most common n-grams. Again, this setting was adopted in agreement with previous work on AA with character n-grams (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Luyckx and Daelemans, 2010). All our experiments use the SVM implementation provided by Canu et al. (2005). 5.1 Experimental settings In order to compare our methods to related works we adopted the following experimental setting. We </context>
<context position="26756" citStr="Stamatatos, 2006" startWordPosition="4360" endWordPosition="4361"> best configuration of LOWBOW histograms, see Table 2. Furthermore, that result is more than 5% higher than the best reported result in related work (80.8% as reported in (Plakias and Stamatatos, 2008b)). Therefore, the considered local histogram representations over character n-grams have proved to be very effective for AA. One should note that, in general, better performance was obtained when using character-level rather than word-level information. This confirms the results already reported by other researchers that have used character-level and word-level information for AA (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Peng et al., 2003). We believe this can be attributed to the fact that character n-grams provide a representation for the document at a finer granularity, which can be better exploited with local histogram representations. Note that by considering 3-grams, words of length up to three are incorporated, and usually these words are function words (e.g., the, it, as, etc.), which are known to be indicative of writing style. Also, n-gram information is more dense in documents than word-level information. Hence, the local histograms are</context>
</contexts>
<marker>Stamatatos, 2006</marker>
<rawString>E. Stamatatos. 2006b. Ensemble-based author identification using character n-grams. In Proceedings of the 3rd International Workshop on Text-based Information Retrieval, pages 41–46, Riva del Garda, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>Intrinsic plagiarism detection using character n-gram profiles.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd International Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse, PAN’09,</booktitle>
<pages>38--46</pages>
<location>Donostia-San Sebastian,</location>
<contexts>
<context position="1876" citStr="Stamatatos, 2009" startWordPosition="275" endWordPosition="276">pful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). There is a broad field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, where the core problem is modeling the thematic c</context>
<context position="6536" citStr="Stamatatos, 2009" startWordPosition="991" endWordPosition="992">om 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histograms (i.e., the bagof-words paradigm) are the mo</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>E. Stamatatos. 2009a. Intrinsic plagiarism detection using character n-gram profiles. In Proceedings of the 3rd International Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse, PAN’09, pages 38–46, Donostia-San Sebastian, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>A survey of modern authorship attribution methods.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>3</issue>
<pages>556</pages>
<contexts>
<context position="1876" citStr="Stamatatos, 2009" startWordPosition="275" endWordPosition="276">pful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). There is a broad field of application for AA methods, including spam filtering (de Vel et al., 2001), fraud detection, computer forensics (Lambers and Veenman, 2009), cyber bullying (Pillay and Solorio, 2010) and plagiarism detection (Stamatatos, 2009a). Therefore, the development of automated AA techniques has received much attention recently (Stamatatos, 2009b). The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors. However, unlike usual text categorization tasks, where the core problem is modeling the thematic c</context>
<context position="6536" citStr="Stamatatos, 2009" startWordPosition="991" endWordPosition="992">om 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histograms (i.e., the bagof-words paradigm) are the mo</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>E. Stamatatos. 2009b. A survey of modern authorship attribution methods. Journal of the American Society for Information Science and Technology, 60(3):538– 556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tearle</author>
<author>K Taylor</author>
<author>H Demuth</author>
</authors>
<title>An algorithm for automated authorship attribution using neural networks.</title>
<date>2008</date>
<booktitle>Literary and Linguist Computing,</booktitle>
<pages>23--4</pages>
<contexts>
<context position="6341" citStr="Tearle et al., 2008" startWordPosition="960" endWordPosition="963">experimental results that are superior to state of the art approaches (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), with improvements ranging from 2%−6% in balanced data sets and from 14% − 30% in imbalanced data sets. 2 Related Work AA can be faced as a multiclass classification task with as many classes as candidate authors. Standard classification methods have been applied to this problem, including support vector machine (SVM) classifiers (Houvardas and Stamatatos, 2006) and variants thereon (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a), neural networks (Tearle et al., 2008), Bayesian classifiers (Coyotl-Morales et al., 2006), decision tree methods (Koppel et al., 2009) and similarity based techniques (Keselj et al., 2003; Lambers and Veenman, 2009; Stamatatos, 2009b; Koppel et al., 2009). In this work, we chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based</context>
</contexts>
<marker>Tearle, Taylor, Demuth, 2008</marker>
<rawString>M. Tearle, K. Taylor, and H. Demuth. 2008. An algorithm for automated authorship attribution using neural networks. Literary and Linguist Computing, 23(4):425–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>J Zobel</author>
</authors>
<title>Effective and scalable authorship attribution using function words.</title>
<date>2005</date>
<booktitle>In Proceedings of 2nd Asian Information Retrieval Symposium,</booktitle>
<volume>3689</volume>
<pages>174--189</pages>
<publisher>Springer.</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="7198" citStr="Zhao and Zobel, 2005" startWordPosition="1094" endWordPosition="1097">chose an SVM classifier as it has reported acceptable performance in AA and because it will allow us to directly compare results with previous work that has used this same classifier. A broad diversity of features has been used to represent documents in AA (Stamatatos, 2009b). However, as in text categorization (Sebastiani, 2002), word-based and character-based features are among the most widely used features (Stamatatos, 2009b; Luyckx and Daelemans, 2010). With respect to word-based features, word histograms (i.e., the bagof-words paradigm) are the most frequently used representations in AA (Zhao and Zobel, 2005; Argamon and Levitan, 2005; Stamatatos, 2009b). Some researchers have gone a step further and have attempted to capture sequential information by using n-grams at the word-level (Peng et al., 2004) or by discovering maximal frequent word sequences (Coyotl-Morales et al., 2006). Unfortunately, because of computational limitations, the latter methods cannot discover enough sequential information from documents (e.g., word n-grams are often restricted to n E 11, 2, 3}, while full sequential information would be obtained with n E 11 ... D} where D is the maximum number of words in a document). Wi</context>
</contexts>
<marker>Zhao, Zobel, 2005</marker>
<rawString>Y. Zhao and J. Zobel. 2005. Effective and scalable authorship attribution using function words. In Proceedings of 2nd Asian Information Retrieval Symposium, volume 3689 of LNCS, pages 174–189, Jeju Island, Korea. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>