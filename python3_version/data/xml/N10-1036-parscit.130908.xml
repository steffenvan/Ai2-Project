<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005822">
<title confidence="0.815961">
Utility Evaluation of Cross-document Information Extraction
</title>
<note confidence="0.6807554">
Heng Jia aaaba
, Zheng Chen, Jonathan Feldman, Antonio Gonzalez, Ralph Grishman, Vivek Upadhyay
a
Computer Science Department, Queens College and the Graduate Center, City University of New York
New York, NY 11367, USA
</note>
<figure confidence="0.752838">
b
</figure>
<affiliation confidence="0.92886">
Computer Science Department, New York University, New York, NY 10003, USA
</affiliation>
<email confidence="0.876373">
hengji@cs.qc.cuny.edu, zchen1@gc.cuny.edu, agonzalez117@qc.cuny.edu, grishman@cs.nyu.edu,
vivekqc@gmail.com
</email>
<sectionHeader confidence="0.998547" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912125">
We describe a utility evaluation to determine
whether cross-document information extrac-
tion (IE) techniques measurably improve user
performance in news summary writing. Two
groups of subjects were asked to perform the
same time-restricted summary writing tasks,
reading news under different conditions: with
no IE results at all, with traditional single-
document IE results, and with cross-document
IE results. Our results show that, in compari-
son to using source documents only, the qual-
ity of summary reports assembled using IE
results, especially from cross-document IE,
was significantly better and user satisfaction
was higher. We also compare the impact of
different user groups on the results.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992175">
Information Extraction (IE) is a task of identifying
‘facts’ (entities, relations and events) within un-
structured documents, and converting them into
structured representations (e.g., databases). IE
techniques have been effectively applied to differ-
ent domains (e.g. daily news, Wikipedia, biomedi-
cal reports, financial analysis and legal
documentations) and different languages. Recently
we described a new cross-document IE task (Ji et
al., 2009) to extract events across-documents and
track them on a time line. Compared to traditional
single-document IE, this new task can extract more
salient, accurate and concise event information.
However, a significant question remains: will
the events extracted by IE, especially this new
cross-document IE task, actually help end-users to
make better use of the large volumes of news? In
order to investigate whether we have reached this
goal, we performed an extrinsic utility (i.e., use-
fulness) and usability evaluation on IE results.
Two groups of subjects were asked to perform the
same time-restricted summary writing tasks, read-
ing news under different conditions: with no IE
results at all, with traditional single-document IE
results, and with cross-document IE results. Our
results show that, in comparison to using source
documents only, the quality of summary reports
assembled using IE techniques, especially from
cross-document IE, was significantly better. Also,
as extraction quality increases from no IE at all to
single-document IE and then to cross-document IE,
user satisfaction increases. We also compare the
impact of different user groups on the results. To
the best of our knowledge, this is the first system-
atic evaluation of cross-document IE from a us-
ability perspective.
</bodyText>
<sectionHeader confidence="0.820509" genericHeader="method">
2 Overview of IE Systems
</sectionHeader>
<bodyText confidence="0.999613538461538">
We applied the English single-document IE system
(Ji and Grishman, 2008) and cross-document IE
system presented in (Ji et al., 2009). Both systems
were developed for the ACE program1.
The single-document IE system can extract
events from individual documents. The core stages
include entity extraction, time expression extrac-
tion and normalization, relation extraction and
event extraction. Events include the 33 distinct
types defined in ACE05. The extraction results are
presented in tabular form.
The cross-document IE system can identify im-
portant person entities which are frequently in-
</bodyText>
<footnote confidence="0.949376">
1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/
</footnote>
<page confidence="0.926752">
285
</page>
<subsubsectionHeader confidence="0.552519">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 285–288,
</subsubsectionHeader>
<subsectionHeader confidence="0.266687">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.998388571428571">
volved in events as ‘centroid entities&apos;; and then for
each centroid entity, link and order the events cen-
tered around it on a time line and associate them to
a geographical map. The event chains are pre-
sented in a user-friendly graphical interface (Ji and
Chen, 2009). Both systems link the events back to
their context documents.
</bodyText>
<sectionHeader confidence="0.999732" genericHeader="method">
3 Evaluation Methods
</sectionHeader>
<subsectionHeader confidence="0.999045">
3.1 Study Execution
</subsectionHeader>
<bodyText confidence="0.9999831">
Our measurement challenge is to assess how IE
techniques affect users&apos; abilities to perform real-
world tasks. We followed the summary writing
task described in the Integrated Feasibility Ex-
periment of the DARPA TIDES program (Colbath
and Kubala, 2003) and the daily task conducted by
intelligence analysts (Bodnar, 2003). Each task in
our evaluation is based on writing a summary of
ACE-type events involving a specific centroid en-
tity, using one of three levels of support:
</bodyText>
<listItem confidence="0.999413">
• Level (I): Read the news articles, with assistance
of keyword based sentence search;
• Level (II): (I) + with assistance from single-
document IE results;
• Level (III): (I) + with assistance from cross-
document IE results.
</listItem>
<bodyText confidence="0.999845285714286">
The summary writing task for each entity using
any level should be finished in 10 minutes. The
users can choose to trust the IE results to create
new sentences or select relevant sentences from
the source documents. The IE systems were ap-
plied to a corpus of 106 articles from ACE 2005
training data.
</bodyText>
<subsectionHeader confidence="0.999845">
3.2 Summary Scoring
</subsectionHeader>
<bodyText confidence="0.999053">
We measure user responses in three aspects:
</bodyText>
<listItem confidence="0.999524142857143">
• Observer-based Quantity -- How many sen-
tences are extracted in each summary? How
many of them are uniquely correct?
• Observer-based Quality-- How fluent and coher-
ent are the sentences in each summary?
• User-based Usability -- How does the user feel
about the system?
</listItem>
<subsectionHeader confidence="0.998726">
3.3 User Group Selection
</subsectionHeader>
<bodyText confidence="0.9993765">
We selected user groups based on the principles
that we should run as many tests as we can afford
(Nielsen, 1994), and at least 5 to insure that we
detect any major usability problems (Faulkner,
2003). Two different groups of users were asked to
conduct the evaluation:
</bodyText>
<listItem confidence="0.851508">
(1) Hallway Evaluation
</listItem>
<bodyText confidence="0.999404272727273">
We chose the first group of users with a &amp;quot;Hallway
Testing&amp;quot; user-study method described in (Nielsen,
1994). We randomly asked 11 PhD students in the
field of natural language processing to conduct the
evaluation. In order to evaluate these three levels
independently, each student was asked to write at
most one summary, using one of the three levels,
for any single centroid entity. To avoid the impact
of diverse text comprehension abilities, each stu-
dent was involved in all of these three levels for
different centroid entities.
</bodyText>
<listItem confidence="0.623528">
(2) Remote Evaluation
</listItem>
<bodyText confidence="0.999954642857143">
An effective utility evaluation will require users
with a diversity of prior knowledge and computer
experience. Therefore we asked the second group
of 11 users in a remote usability testing mode
(Hammontree et al., 1994). We sent out the request
to university-wide undergraduate student mailing
lists and found 11 users to work on the evaluation.
The evaluation procedure follows the Hallway
Testing method, except that the tests are carried
out in the user&apos;s own environment (rather than labs)
helping further simulate real-life scenario testing.
Also the users didn&apos;t meet with the observers and
thus they were not aware of any expectations for
results.
</bodyText>
<sectionHeader confidence="0.997018" genericHeader="evaluation">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.993633666666667">
In this section we will focus on reporting the re-
sults from Hallway Evaluation, while providing
comparisons with Remote Evaluation.
</bodyText>
<subsectionHeader confidence="0.969108">
4.1 Observer-based Quantity
</subsectionHeader>
<bodyText confidence="0.9998687">
The summaries were judged by two annotators and
the judgements reconciled. A summary sentence is
judged as uniquely correct if it: (1) includes rele-
vant events involving the centroid entity; and (2)
the same information was not included in previous
sentences in the current summary. This metric can
be considered as an approximate com bination of
the &amp;quot;content responsiveness&amp;quot;, &amp;quot;non-
redundancy&amp;quot;and &amp;quot;focus&amp;quot; criteria in the NIST TAC
summarization track2. Table 1 presents the
</bodyText>
<footnote confidence="0.996746">
2http://www.nist.gov/tac/2009/Summarization/update.su
mm.09.guidelines.html
</footnote>
<page confidence="0.970555">
286
</page>
<table confidence="0.999505333333333">
Cen- (I) (II) (III) Cen- (I) (II) (III) Cen- (I) (II) (III)
troid troid troid
Bush 3/1/0 5/1/2 6/0/0 Al-douri 4/3/3 4/2/0 6/0/1 Ba’asyir 3/1/0 3/0/0 5/0/0
Ibrahim 4/0/1 5/0/0 8/0/0 Giuliani 2/0/0 3/2/0 5/0/0 Erdogan 1/0/1 4/0/0 4/0/0
Toefting 0/0/0 7/1/0 4/0/0 Blair 2/0/1 3/0/0 5/0/0 Diller 3/0/0 4/1/0 3/0/0
Putin 2/1/0 4/3/2 7/1/1 Pasko 3/0/0 3/0/0 2/0/0 Overall 27/6/6 45/10/5 55/1/2
</table>
<tableCaption confidence="0.9070975">
Table 1. # (uniquely correct sentences)/ #(redundant correct sentences)/
#(spurious sentences) in a summary in Hallway Evaluation
</tableCaption>
<bodyText confidence="0.999757148148148">
quantified Hallway Testing results for each cen-
troid separately and the overall score. It shows that
overall Level (II) contained 18 more correct sen-
tences than the baseline (I), while (III) achieved 11
further correct sentences. (I) obtained significantly
fewer sentences without assistance from IE tools.
We conducted the Wilcoxon Matched-Pairs
Signed-Ranks Test on a query entity basis for ac-
curacy - number of (uniquely correct sen-
tences)/number of (total extracted sentences in a
summary). The results show that (III) is signifi-
cantly better than (I) at a 99.2% confidence level,
and better than (II) at a 96.9% confidence level. (II)
is not significantly better than (I).
We can also see that for some centroid entities
such as &amp;quot;Putin&amp;quot;, &amp;quot;Al-douri&amp;quot; and &amp;quot;Giuliani&amp;quot;, (II)
generated more sentences but also introduced more
redundant information. The user feedback has in-
dicated that they did not have enough time to re-
move redundancy. In contrast, (III) yielded much
less redundant information. In fact, the average
time the users spent using (III) was only about 7.2
minutes. Therefore we can conclude that cross-
document IE can produce more informative sum-
maries in a more efficient way.
Error analysis showed that the major error types
propagated from IE to summaries are as follows.
</bodyText>
<listItem confidence="0.996376071428571">
1. Event time errors. For example, the summary
sentence &amp;quot;Toefting was convicted in September
2001 of assaulting a pair of restaurant workers in
the capital&amp;quot; was judged as incorrect because the
time argument should be &amp;quot;October 2002&amp;quot;.
2. Pronoun resolution errors. When a pronoun is
mistakenly linked to an entity, incorrect event ar-
guments will be included in the summaries.
3. Event type errors. When an event is mis-
classified, the users tend to use incorrect templates
and thus generate wrong summaries.
4. Negative events. Sometimes the event attrib-
ute classifier makes mistakes and the users include
negative events in the summaries.
</listItem>
<subsectionHeader confidence="0.999738">
4.2 Impact of User Groups
</subsectionHeader>
<bodyText confidence="0.99997775">
In the Remote Testing, the accuracy results from
the three levels are as follows: 21/37, 28/37 and
31/36. Thus both user groups benefited from using
IE techniques, but the enhancements vary a lot. In
the Hallway Testing, the users were better trained
and more familiar with IE tools (including the
graphical interface of cross-document IE); and thus
they can benefit more from the IE techniques. In
contrast, in the Remote Evaluation, the users had
quite diverse knowledge backgrounds. For exam-
ple, one remote user was only able to find 1-2 sen-
tences using any of the three levels; while another,
more skilled remote user found more than 5 sen-
tences with any level. However the Remote
Evaluation is important to gather the feedback of
the more subjective usability evaluation in section
4.4. Because the users in Hallway Testing may be
aware of the observations that the observer is hop-
ing to achieve, they may provide potentially biased
feedback.
</bodyText>
<subsectionHeader confidence="0.998831">
4.3 Observer-based Quality
</subsectionHeader>
<bodyText confidence="0.999733">
The evaluation also showed that (III) produced
summaries with better quality. We asked the ob-
servers to give a score between [1, 10] to each
summary according to the following TAC summa-
rization quality criteria: Readability/Fluency, Ref-
erential Clarity and Structure/Coherence. Table 2
shows the evaluation results for the three different
methods.
</bodyText>
<table confidence="0.9980875">
Criteria (I) (II) (III)
Readability/Fluency 9.4 8.5 8.2
Referential Clarity 6.1 8.3 8.7
Structure/Coherence 7.1 7.6 8.5
</table>
<tableCaption confidence="0.999532">
Table 2. Observer-based Average Quality
</tableCaption>
<bodyText confidence="0.8149825">
In their detailed feedback, the users indicated
that (III) has the following advantages: (1) Better
</bodyText>
<page confidence="0.991475">
287
</page>
<bodyText confidence="0.9999223">
pronoun resolution; (2) More complete and accu-
rate temporal order because (III) Can recover un-
known time arguments using cross-document
inference. (3) Can generate abstractive summaries.
For the biographical events (e.g. employment),
some users were able to use specific templates
such as &amp;quot;PER was hired by ORG at TIME&amp;quot; to write
summaries. For example, a sentence &amp;quot;Bush and
Blair met at Camp David and the UK three times in
March 2003&amp;quot; was derived from three different
&amp;quot;Contact-Meeting&amp;quot; events in the event chains. (4)
Can connect related events into more concise
summaries. For example, several events were con-
nected to generate the following sentences &amp;quot;Pasko
was appealed for treason crime on April 16, 2003
and then released on June 15, 2003&amp;quot;. The readabil-
ity scores in Table 2 also indicate that a more ef-
fective template generation method should be
developed to produce more fluent summaries based
on IE results.
</bodyText>
<subsectionHeader confidence="0.991235">
4.4 User-based Usability
</subsectionHeader>
<bodyText confidence="0.999934352941177">
The user feedback from both evaluations also
showed that (II) and (III) results were trusted al-
most equally, and (III) was claimed to provide the
most useful functions. The positive comments
about (III) include &amp;quot;Temporal Linking allows logi-
cal reasoning and generalization&amp;quot;, &amp;quot;Centroid search
helps to focus immediately&amp;quot;, &amp;quot;Spatial Linking al-
lows to browse all the places which a person has
visited&amp;quot;, &amp;quot;Name disambiguation helps to filter ir-
relevant information&amp;quot;, &amp;quot;Can find key information
from event chains&amp;quot;, &amp;quot;Timeline helps correlate
events&amp;quot;; and the negative comments include
&amp;quot;Sometimes IE errors mislead locating the sen-
tences&amp;quot;, &amp;quot;No support of name pair search for meet-
ing events&amp;quot;, &amp;quot;No color emphasis of events on the
original documents&amp;quot; and &amp;quot;No suggestions of tem-
plates to compose summary sentences&amp;quot;.
</bodyText>
<sectionHeader confidence="0.993533" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999976705882353">
Through a utility evaluation on summary writing
we have proved that IE techniques, especially
cross-document IE, can aid news browsing, search
and analysis. In particular, temporal event tracking
across documents helps users perform better at
fact-gathering than they do without IE. Users also
produced more informative summaries with cross-
document IE than with traditional single-document
IE. We also compared and analyzed the differences
between two user groups. Such measures of the
benefits to the eventual end users also provided
feedback on what works well and identified addi-
tional research problems, such as to expand the
centroid to a pair of entities and to provide confi-
dence metrics in the interface. In the future we aim
to set up an online news article analysis system and
perform larger and regular utility evaluations.
</bodyText>
<sectionHeader confidence="0.975122" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.961757571428572">
This work was supported by the U.S. NSF
CAREER Award under Grant IIS-0953149, the
U.S. Army Research Laboratory under Coopera-
tive Agreement Number W911NF-09-2-0053,
Google, Inc., CUNY Research Enhancement Pro-
gram, Faculty Publication Program and GRTI Pro-
gram. The views and conclusions contained in this
document are those of the authors and should not
be interpreted as representing the official policies,
either expressed or implied, of the Army Research
Laboratory or the U.S. Government. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Government purposes notwith-
standing any copyright notation here on.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999795782608696">
John W. Bodnar. 2003. Warning Analysis for the In-
formation Age: Rethinking the Intelligence Process.
Center for Strategic Intelligence Research, Joint
Military Intelligence College, Washington, D.C.
Sean Colbath and Francis Kubala. 2003. TAP-XL: An
Automated Analyst’s Assistant. Proc. HLT-NAACL
2003 (demonstrations).
Laura Faulkner. 2003. Beyond the five-user assumption:
Benefits of increased sample sizes in usability testing.
Behavior Research Methods Instruments and Com-
puters 35(3), 379-383.
Monty Hammontree, Paul Weiler and Nandini Nayak.
1994. Remote Usability Testing. Interactions. Vol-
ume 1, Issue 3. Pages: 21-25.
Heng Ji and Zheng Chen. 2009. Cross-document Tem-
poral and Spatial Person Tracking System Demon-
stration. Proc. HLT-NAACL 2009.
Heng Ji, Ralph Grishman, Zheng Chen and Prashant
Gupta. 2009. Cross-document Event Extraction,
Ranking and Tracking. Proc. Recent Advances in
Natural Language Processing 2009.
Jakob Nielsen. 1994. Usability Engineering. Morgan
Kaufmann Publishers.
</reference>
<page confidence="0.997125">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.143304">
<title confidence="0.996153">Utility Evaluation of Cross-document Information Extraction</title>
<author confidence="0.6230895">Zheng Chen</author>
<author confidence="0.6230895">Jonathan Feldman</author>
<author confidence="0.6230895">Antonio Gonzalez</author>
<author confidence="0.6230895">Ralph Grishman</author>
<author confidence="0.6230895">Vivek Upadhyay</author>
<affiliation confidence="0.8186965">a Computer Science Department, Queens College and the Graduate Center, City University of New</affiliation>
<address confidence="0.998968">New York, NY 11367, USA</address>
<email confidence="0.876206">b</email>
<address confidence="0.761512">Computer Science Department, New York University, New York, NY 10003, USA</address>
<email confidence="0.979972">hengji@cs.qc.cuny.edu,zchen1@gc.cuny.edu,agonzalez117@qc.cuny.edu,grishman@cs.nyu.edu,vivekqc@gmail.com</email>
<abstract confidence="0.999753352941176">We describe a utility evaluation to determine whether cross-document information extraction (IE) techniques measurably improve user performance in news summary writing. Two groups of subjects were asked to perform the same time-restricted summary writing tasks, reading news under different conditions: with no IE results at all, with traditional singledocument IE results, and with cross-document IE results. Our results show that, in comparison to using source documents only, the quality of summary reports assembled using IE results, especially from cross-document IE, was significantly better and user satisfaction was higher. We also compare the impact of different user groups on the results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John W Bodnar</author>
</authors>
<title>Warning Analysis for the Information Age: Rethinking the Intelligence Process.</title>
<date>2003</date>
<institution>Center for Strategic Intelligence Research, Joint Military Intelligence College,</institution>
<location>Washington, D.C.</location>
<contexts>
<context position="4492" citStr="Bodnar, 2003" startWordPosition="657" endWordPosition="658">nk and order the events centered around it on a time line and associate them to a geographical map. The event chains are presented in a user-friendly graphical interface (Ji and Chen, 2009). Both systems link the events back to their context documents. 3 Evaluation Methods 3.1 Study Execution Our measurement challenge is to assess how IE techniques affect users&apos; abilities to perform realworld tasks. We followed the summary writing task described in the Integrated Feasibility Experiment of the DARPA TIDES program (Colbath and Kubala, 2003) and the daily task conducted by intelligence analysts (Bodnar, 2003). Each task in our evaluation is based on writing a summary of ACE-type events involving a specific centroid entity, using one of three levels of support: • Level (I): Read the news articles, with assistance of keyword based sentence search; • Level (II): (I) + with assistance from singledocument IE results; • Level (III): (I) + with assistance from crossdocument IE results. The summary writing task for each entity using any level should be finished in 10 minutes. The users can choose to trust the IE results to create new sentences or select relevant sentences from the source documents. The IE</context>
</contexts>
<marker>Bodnar, 2003</marker>
<rawString>John W. Bodnar. 2003. Warning Analysis for the Information Age: Rethinking the Intelligence Process. Center for Strategic Intelligence Research, Joint Military Intelligence College, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Colbath</author>
<author>Francis Kubala</author>
</authors>
<title>TAP-XL: An Automated Analyst’s Assistant.</title>
<date>2003</date>
<booktitle>Proc. HLT-NAACL</booktitle>
<contexts>
<context position="4423" citStr="Colbath and Kubala, 2003" startWordPosition="645" endWordPosition="648">s volved in events as ‘centroid entities&apos;; and then for each centroid entity, link and order the events centered around it on a time line and associate them to a geographical map. The event chains are presented in a user-friendly graphical interface (Ji and Chen, 2009). Both systems link the events back to their context documents. 3 Evaluation Methods 3.1 Study Execution Our measurement challenge is to assess how IE techniques affect users&apos; abilities to perform realworld tasks. We followed the summary writing task described in the Integrated Feasibility Experiment of the DARPA TIDES program (Colbath and Kubala, 2003) and the daily task conducted by intelligence analysts (Bodnar, 2003). Each task in our evaluation is based on writing a summary of ACE-type events involving a specific centroid entity, using one of three levels of support: • Level (I): Read the news articles, with assistance of keyword based sentence search; • Level (II): (I) + with assistance from singledocument IE results; • Level (III): (I) + with assistance from crossdocument IE results. The summary writing task for each entity using any level should be finished in 10 minutes. The users can choose to trust the IE results to create new sen</context>
</contexts>
<marker>Colbath, Kubala, 2003</marker>
<rawString>Sean Colbath and Francis Kubala. 2003. TAP-XL: An Automated Analyst’s Assistant. Proc. HLT-NAACL 2003 (demonstrations).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Faulkner</author>
</authors>
<title>Beyond the five-user assumption: Benefits of increased sample sizes in usability testing.</title>
<date>2003</date>
<journal>Behavior Research Methods Instruments and Computers</journal>
<volume>35</volume>
<issue>3</issue>
<pages>379--383</pages>
<contexts>
<context position="5731" citStr="Faulkner, 2003" startWordPosition="872" endWordPosition="873"> a corpus of 106 articles from ACE 2005 training data. 3.2 Summary Scoring We measure user responses in three aspects: • Observer-based Quantity -- How many sentences are extracted in each summary? How many of them are uniquely correct? • Observer-based Quality-- How fluent and coherent are the sentences in each summary? • User-based Usability -- How does the user feel about the system? 3.3 User Group Selection We selected user groups based on the principles that we should run as many tests as we can afford (Nielsen, 1994), and at least 5 to insure that we detect any major usability problems (Faulkner, 2003). Two different groups of users were asked to conduct the evaluation: (1) Hallway Evaluation We chose the first group of users with a &amp;quot;Hallway Testing&amp;quot; user-study method described in (Nielsen, 1994). We randomly asked 11 PhD students in the field of natural language processing to conduct the evaluation. In order to evaluate these three levels independently, each student was asked to write at most one summary, using one of the three levels, for any single centroid entity. To avoid the impact of diverse text comprehension abilities, each student was involved in all of these three levels for diff</context>
</contexts>
<marker>Faulkner, 2003</marker>
<rawString>Laura Faulkner. 2003. Beyond the five-user assumption: Benefits of increased sample sizes in usability testing. Behavior Research Methods Instruments and Computers 35(3), 379-383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monty Hammontree</author>
<author>Paul Weiler</author>
<author>Nandini Nayak</author>
</authors>
<title>Remote Usability Testing.</title>
<date>1994</date>
<journal>Interactions.</journal>
<volume>1</volume>
<pages>21--25</pages>
<contexts>
<context position="6598" citStr="Hammontree et al., 1994" startWordPosition="1009" endWordPosition="1012">eld of natural language processing to conduct the evaluation. In order to evaluate these three levels independently, each student was asked to write at most one summary, using one of the three levels, for any single centroid entity. To avoid the impact of diverse text comprehension abilities, each student was involved in all of these three levels for different centroid entities. (2) Remote Evaluation An effective utility evaluation will require users with a diversity of prior knowledge and computer experience. Therefore we asked the second group of 11 users in a remote usability testing mode (Hammontree et al., 1994). We sent out the request to university-wide undergraduate student mailing lists and found 11 users to work on the evaluation. The evaluation procedure follows the Hallway Testing method, except that the tests are carried out in the user&apos;s own environment (rather than labs) helping further simulate real-life scenario testing. Also the users didn&apos;t meet with the observers and thus they were not aware of any expectations for results. 4 Evaluation Results In this section we will focus on reporting the results from Hallway Evaluation, while providing comparisons with Remote Evaluation. 4.1 Observe</context>
</contexts>
<marker>Hammontree, Weiler, Nayak, 1994</marker>
<rawString>Monty Hammontree, Paul Weiler and Nandini Nayak. 1994. Remote Usability Testing. Interactions. Volume 1, Issue 3. Pages: 21-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Zheng Chen</author>
</authors>
<title>Cross-document Temporal and Spatial Person Tracking System Demonstration.</title>
<date>2009</date>
<booktitle>Proc. HLT-NAACL</booktitle>
<contexts>
<context position="4068" citStr="Ji and Chen, 2009" startWordPosition="590" endWordPosition="593"> form. The cross-document IE system can identify important person entities which are frequently in1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 285 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 285–288, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics volved in events as ‘centroid entities&apos;; and then for each centroid entity, link and order the events centered around it on a time line and associate them to a geographical map. The event chains are presented in a user-friendly graphical interface (Ji and Chen, 2009). Both systems link the events back to their context documents. 3 Evaluation Methods 3.1 Study Execution Our measurement challenge is to assess how IE techniques affect users&apos; abilities to perform realworld tasks. We followed the summary writing task described in the Integrated Feasibility Experiment of the DARPA TIDES program (Colbath and Kubala, 2003) and the daily task conducted by intelligence analysts (Bodnar, 2003). Each task in our evaluation is based on writing a summary of ACE-type events involving a specific centroid entity, using one of three levels of support: • Level (I): Read the</context>
</contexts>
<marker>Ji, Chen, 2009</marker>
<rawString>Heng Ji and Zheng Chen. 2009. Cross-document Temporal and Spatial Person Tracking System Demonstration. Proc. HLT-NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Zheng Chen</author>
<author>Prashant Gupta</author>
</authors>
<title>Cross-document Event Extraction, Ranking and Tracking.</title>
<date>2009</date>
<booktitle>Proc. Recent Advances in Natural Language Processing</booktitle>
<contexts>
<context position="1637" citStr="Ji et al., 2009" startWordPosition="222" endWordPosition="225">from cross-document IE, was significantly better and user satisfaction was higher. We also compare the impact of different user groups on the results. 1 Introduction Information Extraction (IE) is a task of identifying ‘facts’ (entities, relations and events) within unstructured documents, and converting them into structured representations (e.g., databases). IE techniques have been effectively applied to different domains (e.g. daily news, Wikipedia, biomedical reports, financial analysis and legal documentations) and different languages. Recently we described a new cross-document IE task (Ji et al., 2009) to extract events across-documents and track them on a time line. Compared to traditional single-document IE, this new task can extract more salient, accurate and concise event information. However, a significant question remains: will the events extracted by IE, especially this new cross-document IE task, actually help end-users to make better use of the large volumes of news? In order to investigate whether we have reached this goal, we performed an extrinsic utility (i.e., usefulness) and usability evaluation on IE results. Two groups of subjects were asked to perform the same time-restric</context>
<context position="3089" citStr="Ji et al., 2009" startWordPosition="447" endWordPosition="450">ts only, the quality of summary reports assembled using IE techniques, especially from cross-document IE, was significantly better. Also, as extraction quality increases from no IE at all to single-document IE and then to cross-document IE, user satisfaction increases. We also compare the impact of different user groups on the results. To the best of our knowledge, this is the first systematic evaluation of cross-document IE from a usability perspective. 2 Overview of IE Systems We applied the English single-document IE system (Ji and Grishman, 2008) and cross-document IE system presented in (Ji et al., 2009). Both systems were developed for the ACE program1. The single-document IE system can extract events from individual documents. The core stages include entity extraction, time expression extraction and normalization, relation extraction and event extraction. Events include the 33 distinct types defined in ACE05. The extraction results are presented in tabular form. The cross-document IE system can identify important person entities which are frequently in1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 285 Human Language Technologies: The 2010 Annual Conference of the North American Chapter o</context>
</contexts>
<marker>Ji, Grishman, Chen, Gupta, 2009</marker>
<rawString>Heng Ji, Ralph Grishman, Zheng Chen and Prashant Gupta. 2009. Cross-document Event Extraction, Ranking and Tracking. Proc. Recent Advances in Natural Language Processing 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Nielsen</author>
</authors>
<title>Usability Engineering.</title>
<date>1994</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="5644" citStr="Nielsen, 1994" startWordPosition="857" endWordPosition="858">or select relevant sentences from the source documents. The IE systems were applied to a corpus of 106 articles from ACE 2005 training data. 3.2 Summary Scoring We measure user responses in three aspects: • Observer-based Quantity -- How many sentences are extracted in each summary? How many of them are uniquely correct? • Observer-based Quality-- How fluent and coherent are the sentences in each summary? • User-based Usability -- How does the user feel about the system? 3.3 User Group Selection We selected user groups based on the principles that we should run as many tests as we can afford (Nielsen, 1994), and at least 5 to insure that we detect any major usability problems (Faulkner, 2003). Two different groups of users were asked to conduct the evaluation: (1) Hallway Evaluation We chose the first group of users with a &amp;quot;Hallway Testing&amp;quot; user-study method described in (Nielsen, 1994). We randomly asked 11 PhD students in the field of natural language processing to conduct the evaluation. In order to evaluate these three levels independently, each student was asked to write at most one summary, using one of the three levels, for any single centroid entity. To avoid the impact of diverse text c</context>
</contexts>
<marker>Nielsen, 1994</marker>
<rawString>Jakob Nielsen. 1994. Usability Engineering. Morgan Kaufmann Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>