<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.982995">
Unsupervised Learning of Dependency Structure for Language Modeling
</title>
<author confidence="0.992752">
Jianfeng Gao
</author>
<affiliation confidence="0.987389">
Microsoft Research, Asia
</affiliation>
<address confidence="0.971655">
49 Zhichun Road, Haidian District
Beijing 100080 China
</address>
<email confidence="0.995945">
jfgao@microsoft.com
</email>
<author confidence="0.968012">
Hisami Suzuki
</author>
<affiliation confidence="0.946108">
Microsoft Research
</affiliation>
<address confidence="0.926616">
One Microsoft Way
Redmond WA 98052 USA
</address>
<email confidence="0.995555">
hisamis@microsoft.com
</email>
<sectionHeader confidence="0.993795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981842105263">
This paper presents a dependency language
model (DLM) that captures linguistic con-
straints via a dependency structure, i.e., a set
of probabilistic dependencies that express
the relations between headwords of each
phrase in a sentence by an acyclic, planar,
undirected graph. Our contributions are
three-fold. First, we incorporate the de-
pendency structure into an n-gram language
model to capture long distance word de-
pendency. Second, we present an unsuper-
vised learning method that discovers the
dependency structure of a sentence using a
bootstrapping procedure. Finally, we
evaluate the proposed models on a realistic
application (Japanese Kana-Kanji conver-
sion). Experiments show that the best DLM
achieves an 11.3% error rate reduction over
the word trigram model.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999992153846154">
In recent years, many efforts have been made to
utilize linguistic structure in language modeling,
which for practical reasons is still dominated by
trigram-based language models. There are two
major obstacles to successfully incorporating lin-
guistic structure into a language model: (1) captur-
ing longer distance word dependencies leads to
higher-order n-gram models, where the number of
parameters is usually too large to estimate; (2)
capturing deeper linguistic relations in a language
model requires a large annotated training corpus
and a decoder that assigns linguistic structure,
which are not always available.
This paper presents a new dependency language
model (DLM) that captures long distance linguistic
constraints between words via a dependency
structure, i.e., a set of probabilistic dependencies
that capture linguistic relations between headwords
of each phrase in a sentence. To deal with the first
obstacle mentioned above, we approximate
long-distance linguistic dependency by a model that
is similar to a skipping bigram model in which the
prediction of a word is conditioned on exactly one
other linguistically related word that lies arbitrarily
far in the past. This dependency model is then in-
terpolated with a headword bigram model and a
word trigram model, keeping the number of pa-
rameters of the combined model manageable. To
overcome the second obstacle, we used an unsu-
pervised learning method that discovers the de-
pendency structure of a given sentence using an
Expectation-Maximization (EM)-like procedure. In
this method, no manual syntactic annotation is
required, thereby opening up the possibility for
building a language model that performs well on a
wide variety of data and languages. The proposed
model is evaluated using Japanese Kana-Kanji
conversion, achieving significant error rate reduc-
tion over the word trigram model.
</bodyText>
<sectionHeader confidence="0.991206" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.9999535">
A trigram language model predicts the next word
based only on two preceding words, blindly dis-
carding any other relevant word that may lie three
or more positions to the left. Such a model is likely
to be linguistically implausible: consider the Eng-
lish sentence in Figure 1(a), where a trigram model
would predict cried from next seat, which does not
agree with our intuition. In this paper, we define a
dependency structure of a sentence as a set of
probabilistic dependencies that express linguistic
relations between words in a sentence by an acyclic,
planar graph, where two related words are con-
nected by an undirected graph edge (i.e., we do not
differentiate the modifier and the head in a de-
pendency). The dependency structure for the sen-
tence in Figure 1(a) is as shown; a model that uses
this dependency structure would predict cried from
baby, in agreement with our intuition.
</bodyText>
<figure confidence="0.9992895">
(a) [A baby] [in the next seat] cried [throughout the flight]
(b) [ /] [/] [ /] [/] [] [/]
</figure>
<figureCaption confidence="0.956235166666667">
Figure 1. Examples of dependency structure. (a) A
dependency structure of an English sentence. Square
brackets indicate base NPs; underlined words are the
headwords. (b) A Japanese equivalent of (a). Slashes
demarcate morpheme boundaries; square brackets
indicate phrases (bunsetsu).
</figureCaption>
<bodyText confidence="0.999915230769231">
A Japanese sentence is typically divided into
non-overlapping phrases called bunsetsu. As shown
in Figure 1(b), each bunsetsu consists of one con-
tent word, referred to here as the headword H, and
several function words F. Words (more precisely,
morphemes) within a bunsetsu are tightly bound
with each other, which can be adequately captured
by a word trigram model. However, headwords
across bunsetsu boundaries also have dependency
relations with each other, as the diagrams in Figure
1 show. Such long distance dependency relations
are expected to provide useful and complementary
information with the word trigram model in the task
of next word prediction.
In constructing language models for realistic
applications such as speech recognition and Asian
language input, we are faced with two constraints
that we would like to satisfy: First, the model must
operate in a left-to-right manner, because (1) the
search procedures for predicting words that corre-
spond to the input acoustic signal or phonetic string
work left to right, and (2) it can be easily combined
with a word trigram model in decoding. Second, the
model should be computationally feasible both in
training and decoding. In the next section, we offer
a DLM that satisfies both of these constraints.
</bodyText>
<sectionHeader confidence="0.997632" genericHeader="method">
3 Dependency Language Model
</sectionHeader>
<bodyText confidence="0.9893344">
The DLM attempts to generate the dependency
structure incrementally while traversing the sen-
tence left to right. It will assign a probability to
every word sequence W and its dependency struc-
ture D. The probability assignment is based on an
encoding of the (W, D) pair described below.
Let W be a sentence of length n words to which
we have prepended &lt;s&gt; and appended &lt;/s&gt; so that
w0 = &lt;s&gt;, and wn+1 = &lt;/s&gt;. In principle, a language
model recovers the probability of a sentence P(W)
over all possible D given W by estimating the joint
probability P(W, D): P(W) = ∑D P(W, D). In prac-
tice, we used the so-called maximum approximation
where the sum is approximated by a single term
P(W, D*):
</bodyText>
<equation confidence="0.998749">
P(W) =∑ P(W,D) ≈P(W,D). (1)
D
</equation>
<bodyText confidence="0.633438">
Here, D* is the most probable dependency structure
of the sentence, which is generally discovered by
maximizing P(W, D):
</bodyText>
<equation confidence="0.998454">
D = arg max P(W, D)
∗ . (2)
D
</equation>
<bodyText confidence="0.999968727272727">
Below we restrict the discussion to the most prob-
able dependency structure of a given sentence, and
simply use D to represent D*. In the remainder of
this section, we first present a statistical dependency
parser, which estimates the parsing probability at
the word level, and generates D incrementally while
traversing W left to right. Next, we describe the
elements of the DLM that assign probability to each
possible W and its most probable D, P(W, D). Fi-
nally, we present an EM-like iterative method for
unsupervised learning of dependency structure.
</bodyText>
<subsectionHeader confidence="0.999451">
3.1 Dependency parsing
</subsectionHeader>
<bodyText confidence="0.9998862">
The aim of dependency parsing is to find the most
probable D of a given W by maximizing the prob-
ability P(D|W). Let D be a set of probabilistic de-
pendencies d, i.e. d ∈ D. Assuming that the de-
pendencies are independent of each other, we have
</bodyText>
<equation confidence="0.984105">
P(D  |W) = ∏ P(d  |W) (3)
d∈D
</equation>
<bodyText confidence="0.861555416666667">
where P(d|W) is the dependency probability condi-
tioned by a particular sentence.1 It is impossible to
estimate P(d|W) directly because the same sentence
is very unlikely to appear in both training and test
data. We thus approximated P(d|W) by P(d), and
estimated the dependency probability from the
training corpus. Let dij = (wi, wj) be the dependency
1 The model in Equation (3) is not strictly probabilistic
because it drops the probabilities of illegal dependencies
(e.g., crossing dependencies).
between wi and wj. The maximum likelihood esti-
mation (MLE) of P(dij) is given by
</bodyText>
<equation confidence="0.996688333333333">
C(wi,w R) (4)
P(dij) =j
C(wi, wj
</equation>
<bodyText confidence="0.99989">
where C(wi, wj, R) is the number of times wi and wj
have a dependency relation in a sentence in training
data, and C(wi, wj) is the number of times wi and wj
are seen in the same sentence. To deal with the data
sparseness problem of MLE, we used the backoff
estimation strategy similar to the one proposed in
Collins (1996), which backs off to estimates that
use less conditioning context. More specifically, we
used the following three estimates:
</bodyText>
<equation confidence="0.9273625">
E1 =η1 E23 =η2 +η3 E4 =η4 , (5) δ δ δ + δ 1 2 3 4
,
</equation>
<bodyText confidence="0.975915">
in which * indicates a wild-card matching any
word. The final estimate E is given by linearly
interpolating these estimates:
</bodyText>
<equation confidence="0.975192">
E
(6)
</equation>
<bodyText confidence="0.991520066666667">
where
and
are smoothing parameters.
Given the above parsing model, we used an ap-
proximation parsing algorithm that is
Tradi-
tional techniques use an optimal Viterbi-style algo-
rithm (e.g., bottom-up chart parser) that is
Although the approximation algorithm is not
guaranteed to find the most probable D, we opted
for it because it works in a left-to-right manner, and
is very efficient and simple to implement. In our
experiments, we found that the algorithm performs
reasonably well on average, and its speed and sim-
plicity make it a bett
</bodyText>
<equation confidence="0.978109833333333">
η1=C(wi,wj,R),δ1= C(wi,wj)
η 2 = C ( w i ,*, R ) , δ 2 = C ( wi ,*) ,
η 3 = C (*, w j , R ) , δ 3 = C (*, wj ) ,
η 4 = C (*,*, R ) , δ 4 = C (*,*) .
E
=λ + −λ λ + −λ
1 1 (1 1)( 2 23 (1 2) 4)
E
λ1
λ2
O(n2).
O(n5).2
</equation>
<bodyText confidence="0.992533">
er choice in DLM training
where we need to parse a large amount of training
data iteratively, as described in Section 3.3.
The parsing algorithm is a slightly modified
version of that proposed in Yuret (1998). It reads a
sentence left to right; after reading each new word
For parsers that use bigram lexical dependencies, Eis-
ner and Satta (1999) presents parsing algorithms that are
or
</bodyText>
<page confidence="0.739061">
2
</page>
<bodyText confidence="0.792041545454546">
O(n4)
O(n3). We thank Joshua Goodman for pointing
this out.
it tries to link
to each of its previous words
and push the generated dependency
into a stack.
When a dependency crossing or a cycle is detected
in the stack, the dependency with the lowest de-
pendency probability in conflict is eliminated. The
algorithm is outlined in Figures 2 an
</bodyText>
<figure confidence="0.99194204">
1 for j
1 to
2 for i
downto 1
3 PUSH
into the stack Dj
4 if a dependency cycle
is detected in
(see Figure 3(a))
5 REMOVE d, where d
Å
LENGTH(W)
Å j-1
dij=(wi,wj)
(CY)
Dj
arg min P ( d )
=
d∈CY
6 while a dependency crossing (CR) is detected
in Dj (see Figure 3(b)) do
7 REMOVE d, where d arg min P ( d )
=
d∈CR
8 OUTPUT(D)
</figure>
<figureCaption confidence="0.995911">
Figure 2. Approximation algorithm of dependency
</figureCaption>
<figure confidence="0.804364333333333">
parsing
w1 w2 w3 w1 w2 w3 w4
(a) (b)
</figure>
<construct confidence="0.639407166666667">
that P(d23) is smaller than
and P(d13),
is
removed (represented as dotted line). (b) An example of
a dependency crossing: given that P(d13) is smaller than
P(d24), d13 is removed.
</construct>
<bodyText confidence="0.999983285714286">
Let the dependency probability be the measure of
the strength of a dependency, i.e., higher probabili-
ties mean stronger dependencies. Note that when a
strong new dependency crosses multiple weak
dependencies, the weak dependencies are removed
even if the new dependency is weaker than the sum
of the old
</bodyText>
<equation confidence="0.982023">
P(d12)
d23
</equation>
<bodyText confidence="0.999721">
dependencies. 3 Although this action
results in lower total probability, it was imple-
mented because multiple weak dependencies con-
nected to the beginning of the sentence often pre-
</bodyText>
<figure confidence="0.864182">
Where
wj,
wj
wi,
dij
d 3.
DEPENDENCY-PARSING(W)
</figure>
<figureCaption confidence="0.987176">
Figure 3. (a) An example of a dependency cycle: given
</figureCaption>
<footnote confidence="0.6526264">
3 This operation leaves some headwords disconnected; in
such a case, we assumed that each disconnected head-
word has a dependency relation with its pre
ceding
headword.
</footnote>
<bodyText confidence="0.999361">
vented a strong meaningful dependency from being
created. In this manner, the directional bias of the
approximation algorithm was partially compen-
sated for.4
</bodyText>
<subsectionHeader confidence="0.999495">
3.2 Language modeling
</subsectionHeader>
<bodyText confidence="0.998193555555555">
The DLM together with the dependency parser
provides an encoding of the (W, D) pair into a se-
quence of elementary model actions. Each action
conceptually consists of two stages. The first stage
assigns a probability to the next word given the left
context. The second stage updates the dependency
structure given the new word using the parsing
algorithm in Figure 2. The probability P(W, D) is
calculated as:
</bodyText>
<equation confidence="0.997543">
P(W,D) = (7)
n
∏ P(wj  |Φ(Wj−1,Dj−1))P(Di−1  |Φ(Wj−1,Dj−1),wj)
= 1
P(Dj−1  |Φ(Wj−1,Dj−1),wj) = (8)
P(pj  |Wjm,Dja,pi,...,pj i − .
i
=1
</equation>
<bodyText confidence="0.997422909090909">
Here (Wj-1, Dj-1) is the word-parse (j-1)-prefix that
Dj-1 is a dependency structure containing only those
dependencies whose two related words are included
in the word (j-1)-prefix, Wj-1. wj is the word to be
predicted. Dj-1j is the incremental dependency
structure that generates Dj = Dj-1  ||Dj-1j ( ||stands for
concatenation) when attached to Dj-1; it is the de-
pendency structure built on top of Dj-1 and the
newly predicted word wj (see the for-loop of line 2
in Figure 2). pi j denotes the ith action of the parser at
position j in the word string: to generate a new
dependency dij, and eliminate dependencies with
the lowest dependency probability in conflict (see
lines 4 – 7 in Figure 2). is a function that maps the
history (Wj-1, Dj-1) onto equivalence classes.
The model in Equation (8) is unfortunately in-
feasible because it is extremely difficult to estimate
the probability of pi j due to the large number of
parameters in the conditional part. According to the
parsing algorithm in Figure 2, the probability of
4 Theoretically, we should arrive at the same dependency
structure no matter whether we parse the sentence left to
right or right to left. However, this is not the case with the
approximation algorithm. This problem is called direc-
tional bias.
each action pij depends on the entire history (e.g.
for detecting a dependency crossing or cycle), so
any mapping that limits the equivalence classi-
fication to less context suitable for model estima-
tion would be very likely to drop critical conditional
information for predicting pij. In practice, we ap-
proximated P(Dj-1j |(Wj-1, Dj-1), wj) by P(Dj|Wj) of
Equation (3), yielding P(Wj, Dj) P(Wj |(Wj-1,
Dj-1)) P(Dj|Wj). This approximation is probabilisti-
cally deficient, but our goal is to apply the DLM to a
decoder in a realistic application, and the perform-
ance gain achieved by this approximation justifies
the modeling decision.
Now, we describe the way P(wj|(Wj-1,Dj-1)) is
estimated. As described in Section 2, headwords
and function words play different syntactic and
semantic roles capturing different types of de-
pendency relations, so the prediction of them can
better be done separately. Assuming that each word
token can be uniquely classified as a headword or a
function word in Japanese, the DLM can be con-
ceived of as a cluster-based language model with
two clusters, headword H and function word F. We
can then define the conditional probability of wj
based on its history as the product of two factors:
the probability of the category given its history, and
the probability of wj given its category. Let hj or fj be
the actual headword or function word in a sentence,
and let Hj or Fj be the category of the word wj.
P(wj|(Wj-1,Dj-1)) can then be formulated as:
</bodyText>
<equation confidence="0.999490666666667">
P(wj  |Φ(Wj−1,Dj−1)) = (9)
P(Hj  |Φ(Wj−1,Dj[I)) ×P(wj  |Φ(Wj1DDj−1),Hj )
+P(Fj  |Φ(WjU,Djm)) ×P(wj  |Φ(Wj−DDj−1),Fj).
</equation>
<bodyText confidence="0.9999582">
We first describe the estimation of headword
probability P(wj  |(Wj-1, Dj-1), Hj). Let HWj-1 be the
headwords in (j-1)-prefix, i.e., containing only
those headwords that are included in Wj-1. Because
HWj-1 is determined by Wj-1, the headword prob-
ability can be rewritten as P(wj  |(Wj-1, HWj-1, Dj-1),
Hj). The problem is to determine the mapping so
as to identify the related words in the left context
that we would like to condition on. Based on the
discussion in Section 2, we chose a mapping func-
tion that retains (1) two preceding words wj-1 and
wj-2 in Wj-1, (2) one preceding headword hj-1 in
HWj-1, and (3) one linguistically related word wi
according to Dj-1. wi is determined in two stages:
First, the parser updates the dependency structure
</bodyText>
<equation confidence="0.756041">
j
j
∏
</equation>
<bodyText confidence="0.99802675">
Dj-1 incrementally to Dj assuming that the next word
is wj. Second, when there are multiple words that
have dependency relations with wj in Dj, wi is se-
lected using the following decision rule:
</bodyText>
<equation confidence="0.99167075">
wi = arg max P(wj  |wi
w w w D
i : ( , )
i j ∈ j
</equation>
<bodyText confidence="0.994870666666667">
where the probability P(wj  |wi, R) of the word wj
given its linguistic related word wi is computed
using MLE by Equation (11):
We thus have the mapping function (Wj-1, HWj-1,
Dj-1) = (wj-2, wj-1, hj-1, wi). The estimate of headword
probability is an interpolation of three probabilities:
</bodyText>
<equation confidence="0.999952">
P(wj  |Φ(Wj−1,Dj−1),Hj) = (12)
λ λ P wj hj− Hj
1 ( 2 (  |1, )
+ (1 − λ2)P(wj  |wi,R))
+ −λ P wj wj− wj − Hj .
(1 1 ) (  |2, 1, )
</equation>
<bodyText confidence="0.996687823529412">
Here P(wj|wj-2, wj-1, Hj) is the word trigram prob-
ability given that wj is a headword, P(wj|hj-1, Hj) is
the headword bigram probability, and A1, A2 ∈[0,1]
are the interpolation weights optimized on held-out
data.
We now come back to the estimate of the other
three probabilities in Equation (9). Following the
work in Gao et al. (2002b), we used the unigram
estimate for word category probabilities, (i.e.,
P(Hj|(Wj-1, Dj-1)) z P(Hj) and P(Fj  |(Wj-1, Dj-1)) z
P(Fj)), and the standard trigram estimate for func-
tion word probability (i.e., P(wj |(Wj-1,Dj-1),Fj) z
P(wj  |wj-2, wj-1, Fj)). Let Cj be the category of wj; we
approximated P(Cj)× P(wj|wj-2, wj-1, Cj) by P(wj  |wj-2,
wj-1). By separating the estimates for the probabili-
ties of headwords and function words, the final
estimate is given below:
</bodyText>
<equation confidence="0.813805">
P(wj  |(Wj-1, Dj-1))= (13)
P(wj  |wj−2, wj−1 )
</equation>
<bodyText confidence="0.9764252">
wj: function word
All conditional probabilities in Equation (13) are
obtained using MLE on training data. In order to
deal with the data sparseness problem, we used a
backoff scheme (Katz, 1987) for parameter estima-
tion. This backoff scheme recursively estimates the
probability of an unseen n-gram by utilizing
(n–1)-gram estimates. In particular, the probability
of Equation (11) backs off to the estimate of
P(wj|R), which is computed as:
</bodyText>
<equation confidence="0.999407">
P(wj |R) =C(wj,R), 14
N ( )
</equation>
<bodyText confidence="0.999938125">
where N is the total number of dependencies in
training data, and C(wj, R) is the number of de-
pendencies that contains wj. To keep the model size
manageable, we removed all n-grams of count less
than 2 from the headword bigram model and the
word trigram model, but kept all long-distance
dependency bigrams that occurred in the training
data.
</bodyText>
<subsectionHeader confidence="0.999012">
3.3 Training data creation
</subsectionHeader>
<bodyText confidence="0.999944826086957">
This section describes two methods that were used
to tag raw text corpus for DLM training: (1) a
method for headword detection, and (2) an unsu-
pervised learning method for dependency structure
acquisition.
In order to classify a word uniquely as H or F,
we used a mapping table created in the following
way. We first assumed that the mapping from
part-of-speech (POS) to word category is unique
and fixed;5 we then used a POS-tagger to generate a
POS-tagged corpus, which are then turned into a
category-tagged corpus.6 Based on this corpus, we
created a mapping table which maps each word to a
unique category: when a word can be mapped to
either H or F, we chose the more frequent category
in the corpus. This method achieved a 98.5% ac-
curacy of headword detection on the test data we
used.
Given a headword-tagged corpus, we then used
an EM-like iterative method for joint optimization
of the parsing model and the dependency structure
of training data. This method uses the maximum
likelihood principle, which is consistent with lan-
</bodyText>
<footnote confidence="0.9610066">
5 The tag set we used included 1,187 POS tags, of which
102 counted as headwords in our experiments.
6 Since the POS-tagger does not identify phrases (bun-
setsu), our implementation identifies multiple headwords
in phrases headed by compounds.
</footnote>
<equation confidence="0.9979086">
, R), (10)
wj
C(wi, wj, R) (11) .
∑ C(wi, wj, R )
P(wj  |wi, R) =
</equation>
<bodyText confidence="0.839555142857143">







</bodyText>
<equation confidence="0.8720776">
λ P Hj λ P wj hj−
1 ( ( )( 2 (  |1 )
+(1 − λ2)P(wj  |wi,R))
+(1 −λ1)P(wj  |wj−2,wj−1 )
wj: headword
</equation>
<bodyText confidence="0.9994105">
guage model training. There are three steps in the
algorithm: (1) initialize, (2) (re-)parse the training
corpus, and (3) re-estimate the parameters of the
parsing model. Steps (2) and (3) are iterated until
the improvement in the probability of training data
is less than a threshold.
Initialize: We set a window of size N and assumed
that each headword pair within a headword N-gram
constitutes an initial dependency. The optimal value
of N is 3 in our experiments. That is, given a
headword trigram (h1, h2, h3), there are 3 initial
dependencies: d12, d13, and d23. From the initial
dependencies, we computed an initial dependency
parsing model by Equation (4).
(Re-)parse the corpus: Given the parsing model,
we used the parsing algorithm in Figure 2 to select
the most probable dependency structure for each
sentence in the training data. This provides an up-
dated set of dependencies.
Re-estimate the parameters of parsing model:
We then re-estimated the parsing model parameters
based on the updated dependency set.
</bodyText>
<sectionHeader confidence="0.990656" genericHeader="method">
4 Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.999851973684211">
In this study, we evaluated language models on the
application of Japanese Kana-Kanji conversion,
which is the standard method of inputting Japanese
text by converting the text of a syllabary-based
Kana string into the appropriate combination of
Kanji and Kana. This is a similar problem to speech
recognition, except that it does not include acoustic
ambiguity. Performance on this task is measured in
terms of the character error rate (CER), given by the
number of characters wrongly converted from the
phonetic string divided by the number of characters
in the correct transcript.
For our experiments, we used two newspaper
corpora, Nikkei and Yomiuri Newspapers, both of
which have been pre-word-segmented. We built
language models from a 36-million-word subset of
the Nikkei Newspaper corpus, performed parameter
optimization on a 100,000-word subset of the Yo-
miuri Newspaper (held-out data), and tested our
models on another 100,000-word subset of the
Yomiuri Newspaper corpus. The lexicon we used
contains 167,107 entries.
Our evaluation was done within a framework of
so-called “N-best rescoring” method, in which a list
of hypotheses is generated by the baseline language
model (a word trigram model in this study), which
is then rescored using a more sophisticated lan-
guage model. We use the N-best list of N=100,
whose “oracle” CER (i.e., the CER of the hy-
potheses with the minimum number of errors) is
presented in Table 1, indicating the upper bound on
performance. We also note in Table 1 that the per-
formance of the conversion using the baseline tri-
gram model is much better than the state-of-the-art
performance currently available in the marketplace,
presumably due to the large amount of training data
we used, and to the similarity between the training
and the test data.
</bodyText>
<tableCaption confidence="0.724442333333333">
Baseline Trigram Oracle of 100-best
3.73% 1.51%
Table 1. CER results of baseline and 100-best list
</tableCaption>
<sectionHeader confidence="0.9962" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9874945">
The results of applying our models to the task of
Japanese Kana-Kanji conversion are shown in
Table 2. The baseline result was obtained by using a
conventional word trigram model (WTM).7 HBM
stands for headword bigram model, which does not
use any dependency structure (i.e. λ2 = 1 in Equation
(13)). DLM_1 is the DLM that does not use head-
word bigram (i.e. λ 2 = 0 in Equation (13)). DLM_2
is the model where the headword probability is
estimated by interpolating the word trigram prob-
ability, the headword bigram probability, and the
probability given one previous linguistically related
word in the dependency structure.
Although Equation (7) suggests that the word
probability P(wj|(Wj-1,Dj-1)) and the parsing model
probability can be combined through simple multi-
plication, some weighting is desirable in practice,
especially when our parsing model is estimated
using an approximation by the parsing score
P(D|W). We therefore introduced a parsing model
weight PW: both DLM_1 and DLM_2 models were
built with and without PW. In Table 2, the PW-
prefix refers to the DLMs with PW = 0.5, and the
DLMs without PW- prefix refers to DLMs with PW
= 0. For both DLM_1 and DLM_2, models with the
parsing weight achieve better performance; we
</bodyText>
<note confidence="0.525411">
7 For a detailed description of the baseline trigram model,
see Gao et al. (2002a).
</note>
<bodyText confidence="0.7789695">
therefore discuss only DLMs with the parsing
weight for the rest of this section.
</bodyText>
<table confidence="0.999833285714286">
Model λ1 λ2 CER CER reduction
WTM ---- ---- 3.73% ----
HBM 0.2 1 3.40% 8.8%
DLM_1 0.1 0 3.48% 6.7%
PW-DLM_1 0.1 0 3.44% 7.8%
DLM_2 0.3 0.7 3.33% 10.7%
PW-DLM_2 0.3 0.7 3.31% 11.3%
</table>
<tableCaption confidence="0.999954">
Table 2. Comparison of CER results
</tableCaption>
<bodyText confidence="0.999941785714286">
By comparing both HBM and PW-LDM_1 models
with the baseline model, we can see that the use of
headword dependency contributes greatly to the
CER reduction: HBM outperformed the baseline
model by 8.8% in CER reduction, and PW-LDM_1
by 7.8%. By combining headword bigram and
dependency structure, we obtained the best model
PW-DLM_2 that achieves 11.3% CER reduction
over the baseline. The improvement achieved by
PW-DLM_2 over the HBM is statistically signifi-
cant according to the t test (P&lt;0.01). These results
demonstrate the effectiveness of our parsing tech-
nique and the use of dependency structure for lan-
guage modeling.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="evaluation">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99987028125">
In this section, we relate our model to previous
research and discuss several factors that we believe
to have the most significant impact on the per-
formance of DLM. The discussion includes: (1) the
use of DLM as a parser, (2) the definition of the
mapping function , and (3) the method of unsu-
pervised dependency structure acquisition.
One basic approach to using linguistic structure
for language modeling is to extend the conventional
language model P(W) to P(W, T), where T is a parse
tree of W. The extended model can then be used as a
parser to select the most likely parse by T* = arg-
maxT P(W, T). Many recent studies (e.g., Chelba
and Jelinek, 2000; Charniak, 2001; Roark, 2001)
adopt this approach. Similarly, dependency-based
models (e.g., Collins, 1996; Chelba et al., 1997) use
a dependency structure D of W instead of a parse
tree T, where D is extracted from syntactic trees.
Both of these models can be called grammar-based
models, in that they capture the syntactic structure
of a sentence, and the model parameters are esti-
mated from syntactically annotated corpora such as
the Penn Treebank. DLM, on the other hand, is a
non-grammar-based model, because it is not based
on any syntactic annotation: the dependency struc-
ture used in language modeling was learned directly
from data in an unsupervised manner, subject to two
weak syntactic constraints (i.e., dependency struc-
ture is acyclic and planar).8 This resulted in cap-
turing the dependency relations that are not pre-
cisely syntactic in nature within our model. For
example, in the conversion of the string below, the
word ban &apos;evening&apos; was correctly predicted in
DLM by using the long-distance bigram ~
asa~ban &apos;morning~evening&apos;, even though these two
words are not in any direct syntactic dependency
relationship:
&apos;asks for instructions in the morning and submits
daily reports in the evening&apos;
Though there is no doubt that syntactic dependency
relations provide useful information for language
modeling, the most linguistically related word in the
previous context may come in various linguistic
relations with the word being predicted, not limited
to syntactic dependency. This opens up new possi-
bilities for exploring the combination of different
knowledge sources in language modeling.
Regarding the function that maps the left
context onto equivalence classes, we used a simple
approximation that takes into account only one
linguistically related word in left context. An al-
ternative is to use the maximum entropy (ME)
approach (Rosenfeld, 1994; Chelba et al., 1997).
Although ME models provide a nice framework for
incorporating arbitrary knowledge sources that can
be encoded as a large set of constraints, training and
using ME models is extremely computationally
expensive. Our working hypothesis is that the in-
formation for predicting the new word is dominated
by a very limited set of words which can be selected
heuristically: in this paper, is defined as a heu-
ristic function that maps D to one word in D that has
the strongest linguistic relation with the word being
predicted, as in (8). This hypothesis is borne out by
</bodyText>
<footnote confidence="0.94505075">
8 In this sense, our model is an extension of a depend-
ency-based model proposed in Yuret (1998). However,
this work has not been evaluated as a language model
with error rate reduction.
</footnote>
<bodyText confidence="0.999986034482759">
an additional experiment we conducted, where we
used two words from D that had the strongest rela-
tion with the word being predicted; this resulted in a
very limited gain in CER reduction of 0.62%, which
is not statistically significant (P&gt;0.05 according to
the t test).
The EM-like method for learning dependency
relations described in Section 3.3 has also been
applied to other tasks such as hidden Markov model
training (Rabiner, 1989), syntactic relation learning
(Yuret, 1998), and Chinese word segmentation
(Gao et al., 2002a). In applying this method, two
factors need to be considered: (1) how to initialize
the model (i.e. the value of the window size N), and
(2) the number of iterations. We investigated the
impact of these two factors empirically on the CER
of Japanese Kana-Kanji conversion. We built a
series of DLMs using different window size N and
different number of iterations. Some sample results
are shown in Table 3: the improvement in CER
begins to saturate at the second iteration. We also
find that a larger N results in a better initial model
but makes the following iterations less effective.
The possible reason is that a larger N generates
more initial dependencies and would lead to a better
initial model, but it also introduces noise that pre-
vents the initial model from being improved. All
DLMs in Table 2 are initialized with N = 3 and are
run for two iterations.
</bodyText>
<table confidence="0.8493164">
Iteration N = 2 N = 3 N = 5 N = 7 N = 10
Init. 3.552% 3.523% 3.540% 3.514 % 3.511%
1 3.531% 3.503% 3.493% 3.509% 3.489%
2 3.527% 3.481% 3.483% 3.492% 3.488%
3 3.526% 3.481% 3.485% 3.490% 3.488%
</table>
<tableCaption confidence="0.989212">
Table 3. CER of DLM_1 models initialized with dif-
ferent window size N, for 0-3 iterations
</tableCaption>
<sectionHeader confidence="0.998239" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999987222222222">
We have presented a dependency language model
that captures linguistic constraints via a dependency
structure – a set of probabilistic dependencies that
express the relations between headwords of each
phrase in a sentence by an acyclic, planar, undi-
rected graph. Promising results of our experiments
suggest that long-distance dependency relations can
indeed be successfully exploited for the purpose of
language modeling.
There are many possibilities for future im-
provements. In particular, as discussed in Section 6,
syntactic dependency structure is believed to cap-
ture useful information for informed language
modeling, yet further improvements may be possi-
ble by incorporating non-syntax-based dependen-
cies. Correlating the accuracy of the dependency
parser as a parser vs. its utility in CER reduction
may suggest a useful direction for further research.
</bodyText>
<sectionHeader confidence="0.992835" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999944375">
Charniak, Eugine. 2001. Immediate-head parsing for
language models. In ACL/EACL 2001, pp.124-131.
Chelba, Ciprian and Frederick Jelinek. 2000. Structured
Language Modeling. Computer Speech and Language,
Vol. 14, No. 4. pp 283-332.
Chelba, C, D. Engle, F. Jelinek, V. Jimenez, S. Khu-
danpur, L. Mangu, H. Printz, E. S. Ristad, R.
Rosenfeld, A. Stolcke and D. Wu. 1997. Structure and
performance of a dependency language model. In
Processing of Eurospeech, Vol. 5, pp 2775-2778.
Collins, Michael John. 1996. A new statistical parser
based on bigram lexical dependencies. In ACL
34:184-191.
Eisner, Jason and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and head
automaton grammars. In ACL 37: 457-464.
Gao, Jianfeng, Joshua Goodman, Mingjing Li and
Kai-Fu Lee. 2002a. Toward a unified approach to sta-
tistical language modeling for Chinese. ACM Trans-
actions on Asian Language Information Processing,
1-1: 3-33.
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002b.
Exploiting headword dependency and predictive
clustering for language modeling. In EMNLP 2002:
248-256.
Katz, S. M. 1987. Estimation of probabilities from sparse
data for other language component of a speech recog-
nizer. IEEE transactions on Acoustics, Speech and
Signal Processing, 35(3): 400-401.
Rabiner, Lawrence R. 1989. A tutorial on hidden Markov
models and selected applications in speech recognition.
Proceedings of IEEE 77:257-286.
Roark, Brian. 2001. Probabilistic top-down parsing and
language modeling. Computational Linguistics, 17-2:
1-28.
Rosenfeld, Ronald. 1994. Adaptive statistical language
modeling: a maximum entropy approach. Ph.D. thesis,
Carnegie Mellon University.
Yuret, Deniz. 1998. Discovery of linguistic relations
using lexical attraction. Ph.D. thesis, MIT.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.889619">
<title confidence="0.998749">Unsupervised Learning of Dependency Structure for Language Modeling</title>
<author confidence="0.966789">Jianfeng Gao</author>
<affiliation confidence="0.984994">Microsoft Research, Asia</affiliation>
<address confidence="0.9812085">49 Zhichun Road, Haidian District Beijing 100080 China</address>
<email confidence="0.999875">jfgao@microsoft.com</email>
<author confidence="0.998081">Hisami Suzuki</author>
<affiliation confidence="0.999944">Microsoft Research</affiliation>
<address confidence="0.9899425">One Microsoft Way Redmond WA 98052 USA</address>
<email confidence="0.999699">hisamis@microsoft.com</email>
<abstract confidence="0.99925055">This paper presents a dependency language model (DLM) that captures linguistic constraints via a dependency structure, i.e., a set of probabilistic dependencies that express the relations between headwords of each phrase in a sentence by an acyclic, planar, undirected graph. Our contributions are three-fold. First, we incorporate the destructure into an language model to capture long distance word dependency. Second, we present an unsupervised learning method that discovers the dependency structure of a sentence using a bootstrapping procedure. Finally, we evaluate the proposed models on a realistic application (Japanese Kana-Kanji conversion). Experiments show that the best DLM achieves an 11.3% error rate reduction over the word trigram model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugine Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In ACL/EACL</booktitle>
<pages>124--131</pages>
<contexts>
<context position="25321" citStr="Charniak, 2001" startWordPosition="4298" endWordPosition="4299"> and discuss several factors that we believe to have the most significant impact on the performance of DLM. The discussion includes: (1) the use of DLM as a parser, (2) the definition of the mapping function , and (3) the method of unsupervised dependency structure acquisition. One basic approach to using linguistic structure for language modeling is to extend the conventional language model P(W) to P(W, T), where T is a parse tree of W. The extended model can then be used as a parser to select the most likely parse by T* = argmaxT P(W, T). Many recent studies (e.g., Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001) adopt this approach. Similarly, dependency-based models (e.g., Collins, 1996; Chelba et al., 1997) use a dependency structure D of W instead of a parse tree T, where D is extracted from syntactic trees. Both of these models can be called grammar-based models, in that they capture the syntactic structure of a sentence, and the model parameters are estimated from syntactically annotated corpora such as the Penn Treebank. DLM, on the other hand, is a non-grammar-based model, because it is not based on any syntactic annotation: the dependency structure used in language modeling was </context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Charniak, Eugine. 2001. Immediate-head parsing for language models. In ACL/EACL 2001, pp.124-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured Language Modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<pages>283--332</pages>
<contexts>
<context position="25305" citStr="Chelba and Jelinek, 2000" startWordPosition="4294" endWordPosition="4297">model to previous research and discuss several factors that we believe to have the most significant impact on the performance of DLM. The discussion includes: (1) the use of DLM as a parser, (2) the definition of the mapping function , and (3) the method of unsupervised dependency structure acquisition. One basic approach to using linguistic structure for language modeling is to extend the conventional language model P(W) to P(W, T), where T is a parse tree of W. The extended model can then be used as a parser to select the most likely parse by T* = argmaxT P(W, T). Many recent studies (e.g., Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001) adopt this approach. Similarly, dependency-based models (e.g., Collins, 1996; Chelba et al., 1997) use a dependency structure D of W instead of a parse tree T, where D is extracted from syntactic trees. Both of these models can be called grammar-based models, in that they capture the syntactic structure of a sentence, and the model parameters are estimated from syntactically annotated corpora such as the Penn Treebank. DLM, on the other hand, is a non-grammar-based model, because it is not based on any syntactic annotation: the dependency structure used in langua</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Chelba, Ciprian and Frederick Jelinek. 2000. Structured Language Modeling. Computer Speech and Language, Vol. 14, No. 4. pp 283-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>D Engle</author>
<author>F Jelinek</author>
<author>V Jimenez</author>
<author>S Khudanpur</author>
<author>L Mangu</author>
<author>H Printz</author>
<author>E S Ristad</author>
<author>R Rosenfeld</author>
<author>A Stolcke</author>
<author>D Wu</author>
</authors>
<title>Structure and performance of a dependency language model.</title>
<date>1997</date>
<booktitle>In Processing of Eurospeech,</booktitle>
<volume>5</volume>
<pages>2775--2778</pages>
<contexts>
<context position="25434" citStr="Chelba et al., 1997" startWordPosition="4311" endWordPosition="4314"> The discussion includes: (1) the use of DLM as a parser, (2) the definition of the mapping function , and (3) the method of unsupervised dependency structure acquisition. One basic approach to using linguistic structure for language modeling is to extend the conventional language model P(W) to P(W, T), where T is a parse tree of W. The extended model can then be used as a parser to select the most likely parse by T* = argmaxT P(W, T). Many recent studies (e.g., Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001) adopt this approach. Similarly, dependency-based models (e.g., Collins, 1996; Chelba et al., 1997) use a dependency structure D of W instead of a parse tree T, where D is extracted from syntactic trees. Both of these models can be called grammar-based models, in that they capture the syntactic structure of a sentence, and the model parameters are estimated from syntactically annotated corpora such as the Penn Treebank. DLM, on the other hand, is a non-grammar-based model, because it is not based on any syntactic annotation: the dependency structure used in language modeling was learned directly from data in an unsupervised manner, subject to two weak syntactic constraints (i.e., dependency</context>
<context position="27195" citStr="Chelba et al., 1997" startWordPosition="4588" endWordPosition="4591">ns provide useful information for language modeling, the most linguistically related word in the previous context may come in various linguistic relations with the word being predicted, not limited to syntactic dependency. This opens up new possibilities for exploring the combination of different knowledge sources in language modeling. Regarding the function that maps the left context onto equivalence classes, we used a simple approximation that takes into account only one linguistically related word in left context. An alternative is to use the maximum entropy (ME) approach (Rosenfeld, 1994; Chelba et al., 1997). Although ME models provide a nice framework for incorporating arbitrary knowledge sources that can be encoded as a large set of constraints, training and using ME models is extremely computationally expensive. Our working hypothesis is that the information for predicting the new word is dominated by a very limited set of words which can be selected heuristically: in this paper, is defined as a heuristic function that maps D to one word in D that has the strongest linguistic relation with the word being predicted, as in (8). This hypothesis is borne out by 8 In this sense, our model is an ext</context>
</contexts>
<marker>Chelba, Engle, Jelinek, Jimenez, Khudanpur, Mangu, Printz, Ristad, Rosenfeld, Stolcke, Wu, 1997</marker>
<rawString>Chelba, C, D. Engle, F. Jelinek, V. Jimenez, S. Khudanpur, L. Mangu, H. Printz, E. S. Ristad, R. Rosenfeld, A. Stolcke and D. Wu. 1997. Structure and performance of a dependency language model. In Processing of Eurospeech, Vol. 5, pp 2775-2778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In ACL</booktitle>
<pages>34--184</pages>
<contexts>
<context position="8167" citStr="Collins (1996)" startWordPosition="1323" endWordPosition="1324">= (wi, wj) be the dependency 1 The model in Equation (3) is not strictly probabilistic because it drops the probabilities of illegal dependencies (e.g., crossing dependencies). between wi and wj. The maximum likelihood estimation (MLE) of P(dij) is given by C(wi,w R) (4) P(dij) =j C(wi, wj where C(wi, wj, R) is the number of times wi and wj have a dependency relation in a sentence in training data, and C(wi, wj) is the number of times wi and wj are seen in the same sentence. To deal with the data sparseness problem of MLE, we used the backoff estimation strategy similar to the one proposed in Collins (1996), which backs off to estimates that use less conditioning context. More specifically, we used the following three estimates: E1 =η1 E23 =η2 +η3 E4 =η4 , (5) δ δ δ + δ 1 2 3 4 , in which * indicates a wild-card matching any word. The final estimate E is given by linearly interpolating these estimates: E (6) where and are smoothing parameters. Given the above parsing model, we used an approximation parsing algorithm that is Traditional techniques use an optimal Viterbi-style algorithm (e.g., bottom-up chart parser) that is Although the approximation algorithm is not guaranteed to find the most p</context>
<context position="25412" citStr="Collins, 1996" startWordPosition="4309" endWordPosition="4310">ormance of DLM. The discussion includes: (1) the use of DLM as a parser, (2) the definition of the mapping function , and (3) the method of unsupervised dependency structure acquisition. One basic approach to using linguistic structure for language modeling is to extend the conventional language model P(W) to P(W, T), where T is a parse tree of W. The extended model can then be used as a parser to select the most likely parse by T* = argmaxT P(W, T). Many recent studies (e.g., Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001) adopt this approach. Similarly, dependency-based models (e.g., Collins, 1996; Chelba et al., 1997) use a dependency structure D of W instead of a parse tree T, where D is extracted from syntactic trees. Both of these models can be called grammar-based models, in that they capture the syntactic structure of a sentence, and the model parameters are estimated from syntactically annotated corpora such as the Penn Treebank. DLM, on the other hand, is a non-grammar-based model, because it is not based on any syntactic annotation: the dependency structure used in language modeling was learned directly from data in an unsupervised manner, subject to two weak syntactic constra</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Collins, Michael John. 1996. A new statistical parser based on bigram lexical dependencies. In ACL 34:184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<journal>In ACL</journal>
<volume>37</volume>
<pages>457--464</pages>
<contexts>
<context position="9574" citStr="Eisner and Satta (1999)" startWordPosition="1603" endWordPosition="1607">y well on average, and its speed and simplicity make it a bett η1=C(wi,wj,R),δ1= C(wi,wj) η 2 = C ( w i ,*, R ) , δ 2 = C ( wi ,*) , η 3 = C (*, w j , R ) , δ 3 = C (*, wj ) , η 4 = C (*,*, R ) , δ 4 = C (*,*) . E =λ + −λ λ + −λ 1 1 (1 1)( 2 23 (1 2) 4) E λ1 λ2 O(n2). O(n5).2 er choice in DLM training where we need to parse a large amount of training data iteratively, as described in Section 3.3. The parsing algorithm is a slightly modified version of that proposed in Yuret (1998). It reads a sentence left to right; after reading each new word For parsers that use bigram lexical dependencies, Eisner and Satta (1999) presents parsing algorithms that are or 2 O(n4) O(n3). We thank Joshua Goodman for pointing this out. it tries to link to each of its previous words and push the generated dependency into a stack. When a dependency crossing or a cycle is detected in the stack, the dependency with the lowest dependency probability in conflict is eliminated. The algorithm is outlined in Figures 2 an 1 for j 1 to 2 for i downto 1 3 PUSH into the stack Dj 4 if a dependency cycle is detected in (see Figure 3(a)) 5 REMOVE d, where d Å LENGTH(W) Å j-1 dij=(wi,wj) (CY) Dj arg min P ( d ) = d∈CY 6 while a dependency c</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Eisner, Jason and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In ACL 37: 457-464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Joshua Goodman</author>
<author>Mingjing Li</author>
<author>Kai-Fu Lee</author>
</authors>
<title>Toward a unified approach to statistical language modeling for Chinese.</title>
<date>2002</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>1</volume>
<pages>3--33</pages>
<contexts>
<context position="16638" citStr="Gao et al. (2002" startWordPosition="2835" endWordPosition="2838">us have the mapping function (Wj-1, HWj-1, Dj-1) = (wj-2, wj-1, hj-1, wi). The estimate of headword probability is an interpolation of three probabilities: P(wj |Φ(Wj−1,Dj−1),Hj) = (12) λ λ P wj hj− Hj 1 ( 2 ( |1, ) + (1 − λ2)P(wj |wi,R)) + −λ P wj wj− wj − Hj . (1 1 ) ( |2, 1, ) Here P(wj|wj-2, wj-1, Hj) is the word trigram probability given that wj is a headword, P(wj|hj-1, Hj) is the headword bigram probability, and A1, A2 ∈[0,1] are the interpolation weights optimized on held-out data. We now come back to the estimate of the other three probabilities in Equation (9). Following the work in Gao et al. (2002b), we used the unigram estimate for word category probabilities, (i.e., P(Hj|(Wj-1, Dj-1)) z P(Hj) and P(Fj |(Wj-1, Dj-1)) z P(Fj)), and the standard trigram estimate for function word probability (i.e., P(wj |(Wj-1,Dj-1),Fj) z P(wj |wj-2, wj-1, Fj)). Let Cj be the category of wj; we approximated P(Cj)× P(wj|wj-2, wj-1, Cj) by P(wj |wj-2, wj-1). By separating the estimates for the probabilities of headwords and function words, the final estimate is given below: P(wj |(Wj-1, Dj-1))= (13) P(wj |wj−2, wj−1 ) wj: function word All conditional probabilities in Equation (13) are obtained using MLE </context>
<context position="23711" citStr="Gao et al. (2002" startWordPosition="4019" endWordPosition="4022"> the parsing model probability can be combined through simple multiplication, some weighting is desirable in practice, especially when our parsing model is estimated using an approximation by the parsing score P(D|W). We therefore introduced a parsing model weight PW: both DLM_1 and DLM_2 models were built with and without PW. In Table 2, the PWprefix refers to the DLMs with PW = 0.5, and the DLMs without PW- prefix refers to DLMs with PW = 0. For both DLM_1 and DLM_2, models with the parsing weight achieve better performance; we 7 For a detailed description of the baseline trigram model, see Gao et al. (2002a). therefore discuss only DLMs with the parsing weight for the rest of this section. Model λ1 λ2 CER CER reduction WTM ---- ---- 3.73% ---- HBM 0.2 1 3.40% 8.8% DLM_1 0.1 0 3.48% 6.7% PW-DLM_1 0.1 0 3.44% 7.8% DLM_2 0.3 0.7 3.33% 10.7% PW-DLM_2 0.3 0.7 3.31% 11.3% Table 2. Comparison of CER results By comparing both HBM and PW-LDM_1 models with the baseline model, we can see that the use of headword dependency contributes greatly to the CER reduction: HBM outperformed the baseline model by 8.8% in CER reduction, and PW-LDM_1 by 7.8%. By combining headword bigram and dependency structure, we o</context>
<context position="28474" citStr="Gao et al., 2002" startWordPosition="4802" endWordPosition="4805">wever, this work has not been evaluated as a language model with error rate reduction. an additional experiment we conducted, where we used two words from D that had the strongest relation with the word being predicted; this resulted in a very limited gain in CER reduction of 0.62%, which is not statistically significant (P&gt;0.05 according to the t test). The EM-like method for learning dependency relations described in Section 3.3 has also been applied to other tasks such as hidden Markov model training (Rabiner, 1989), syntactic relation learning (Yuret, 1998), and Chinese word segmentation (Gao et al., 2002a). In applying this method, two factors need to be considered: (1) how to initialize the model (i.e. the value of the window size N), and (2) the number of iterations. We investigated the impact of these two factors empirically on the CER of Japanese Kana-Kanji conversion. We built a series of DLMs using different window size N and different number of iterations. Some sample results are shown in Table 3: the improvement in CER begins to saturate at the second iteration. We also find that a larger N results in a better initial model but makes the following iterations less effective. The possib</context>
</contexts>
<marker>Gao, Goodman, Li, Lee, 2002</marker>
<rawString>Gao, Jianfeng, Joshua Goodman, Mingjing Li and Kai-Fu Lee. 2002a. Toward a unified approach to statistical language modeling for Chinese. ACM Transactions on Asian Language Information Processing, 1-1: 3-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Hisami Suzuki</author>
<author>Yang Wen</author>
</authors>
<title>Exploiting headword dependency and predictive clustering for language modeling.</title>
<date>2002</date>
<booktitle>In EMNLP 2002:</booktitle>
<pages>248--256</pages>
<contexts>
<context position="16638" citStr="Gao et al. (2002" startWordPosition="2835" endWordPosition="2838">us have the mapping function (Wj-1, HWj-1, Dj-1) = (wj-2, wj-1, hj-1, wi). The estimate of headword probability is an interpolation of three probabilities: P(wj |Φ(Wj−1,Dj−1),Hj) = (12) λ λ P wj hj− Hj 1 ( 2 ( |1, ) + (1 − λ2)P(wj |wi,R)) + −λ P wj wj− wj − Hj . (1 1 ) ( |2, 1, ) Here P(wj|wj-2, wj-1, Hj) is the word trigram probability given that wj is a headword, P(wj|hj-1, Hj) is the headword bigram probability, and A1, A2 ∈[0,1] are the interpolation weights optimized on held-out data. We now come back to the estimate of the other three probabilities in Equation (9). Following the work in Gao et al. (2002b), we used the unigram estimate for word category probabilities, (i.e., P(Hj|(Wj-1, Dj-1)) z P(Hj) and P(Fj |(Wj-1, Dj-1)) z P(Fj)), and the standard trigram estimate for function word probability (i.e., P(wj |(Wj-1,Dj-1),Fj) z P(wj |wj-2, wj-1, Fj)). Let Cj be the category of wj; we approximated P(Cj)× P(wj|wj-2, wj-1, Cj) by P(wj |wj-2, wj-1). By separating the estimates for the probabilities of headwords and function words, the final estimate is given below: P(wj |(Wj-1, Dj-1))= (13) P(wj |wj−2, wj−1 ) wj: function word All conditional probabilities in Equation (13) are obtained using MLE </context>
<context position="23711" citStr="Gao et al. (2002" startWordPosition="4019" endWordPosition="4022"> the parsing model probability can be combined through simple multiplication, some weighting is desirable in practice, especially when our parsing model is estimated using an approximation by the parsing score P(D|W). We therefore introduced a parsing model weight PW: both DLM_1 and DLM_2 models were built with and without PW. In Table 2, the PWprefix refers to the DLMs with PW = 0.5, and the DLMs without PW- prefix refers to DLMs with PW = 0. For both DLM_1 and DLM_2, models with the parsing weight achieve better performance; we 7 For a detailed description of the baseline trigram model, see Gao et al. (2002a). therefore discuss only DLMs with the parsing weight for the rest of this section. Model λ1 λ2 CER CER reduction WTM ---- ---- 3.73% ---- HBM 0.2 1 3.40% 8.8% DLM_1 0.1 0 3.48% 6.7% PW-DLM_1 0.1 0 3.44% 7.8% DLM_2 0.3 0.7 3.33% 10.7% PW-DLM_2 0.3 0.7 3.31% 11.3% Table 2. Comparison of CER results By comparing both HBM and PW-LDM_1 models with the baseline model, we can see that the use of headword dependency contributes greatly to the CER reduction: HBM outperformed the baseline model by 8.8% in CER reduction, and PW-LDM_1 by 7.8%. By combining headword bigram and dependency structure, we o</context>
<context position="28474" citStr="Gao et al., 2002" startWordPosition="4802" endWordPosition="4805">wever, this work has not been evaluated as a language model with error rate reduction. an additional experiment we conducted, where we used two words from D that had the strongest relation with the word being predicted; this resulted in a very limited gain in CER reduction of 0.62%, which is not statistically significant (P&gt;0.05 according to the t test). The EM-like method for learning dependency relations described in Section 3.3 has also been applied to other tasks such as hidden Markov model training (Rabiner, 1989), syntactic relation learning (Yuret, 1998), and Chinese word segmentation (Gao et al., 2002a). In applying this method, two factors need to be considered: (1) how to initialize the model (i.e. the value of the window size N), and (2) the number of iterations. We investigated the impact of these two factors empirically on the CER of Japanese Kana-Kanji conversion. We built a series of DLMs using different window size N and different number of iterations. Some sample results are shown in Table 3: the improvement in CER begins to saturate at the second iteration. We also find that a larger N results in a better initial model but makes the following iterations less effective. The possib</context>
</contexts>
<marker>Gao, Suzuki, Wen, 2002</marker>
<rawString>Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002b. Exploiting headword dependency and predictive clustering for language modeling. In EMNLP 2002: 248-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for other language component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>400--401</pages>
<contexts>
<context position="17344" citStr="Katz, 1987" startWordPosition="2951" endWordPosition="2952">(Hj) and P(Fj |(Wj-1, Dj-1)) z P(Fj)), and the standard trigram estimate for function word probability (i.e., P(wj |(Wj-1,Dj-1),Fj) z P(wj |wj-2, wj-1, Fj)). Let Cj be the category of wj; we approximated P(Cj)× P(wj|wj-2, wj-1, Cj) by P(wj |wj-2, wj-1). By separating the estimates for the probabilities of headwords and function words, the final estimate is given below: P(wj |(Wj-1, Dj-1))= (13) P(wj |wj−2, wj−1 ) wj: function word All conditional probabilities in Equation (13) are obtained using MLE on training data. In order to deal with the data sparseness problem, we used a backoff scheme (Katz, 1987) for parameter estimation. This backoff scheme recursively estimates the probability of an unseen n-gram by utilizing (n–1)-gram estimates. In particular, the probability of Equation (11) backs off to the estimate of P(wj|R), which is computed as: P(wj |R) =C(wj,R), 14 N ( ) where N is the total number of dependencies in training data, and C(wj, R) is the number of dependencies that contains wj. To keep the model size manageable, we removed all n-grams of count less than 2 from the headword bigram model and the word trigram model, but kept all long-distance dependency bigrams that occurred in </context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S. M. 1987. Estimation of probabilities from sparse data for other language component of a speech recognizer. IEEE transactions on Acoustics, Speech and Signal Processing, 35(3): 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of IEEE</booktitle>
<pages>77--257</pages>
<contexts>
<context position="28382" citStr="Rabiner, 1989" startWordPosition="4791" endWordPosition="4792"> sense, our model is an extension of a dependency-based model proposed in Yuret (1998). However, this work has not been evaluated as a language model with error rate reduction. an additional experiment we conducted, where we used two words from D that had the strongest relation with the word being predicted; this resulted in a very limited gain in CER reduction of 0.62%, which is not statistically significant (P&gt;0.05 according to the t test). The EM-like method for learning dependency relations described in Section 3.3 has also been applied to other tasks such as hidden Markov model training (Rabiner, 1989), syntactic relation learning (Yuret, 1998), and Chinese word segmentation (Gao et al., 2002a). In applying this method, two factors need to be considered: (1) how to initialize the model (i.e. the value of the window size N), and (2) the number of iterations. We investigated the impact of these two factors empirically on the CER of Japanese Kana-Kanji conversion. We built a series of DLMs using different window size N and different number of iterations. Some sample results are shown in Table 3: the improvement in CER begins to saturate at the second iteration. We also find that a larger N res</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, Lawrence R. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of IEEE 77:257-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<pages>1--28</pages>
<contexts>
<context position="25335" citStr="Roark, 2001" startWordPosition="4300" endWordPosition="4301">eral factors that we believe to have the most significant impact on the performance of DLM. The discussion includes: (1) the use of DLM as a parser, (2) the definition of the mapping function , and (3) the method of unsupervised dependency structure acquisition. One basic approach to using linguistic structure for language modeling is to extend the conventional language model P(W) to P(W, T), where T is a parse tree of W. The extended model can then be used as a parser to select the most likely parse by T* = argmaxT P(W, T). Many recent studies (e.g., Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001) adopt this approach. Similarly, dependency-based models (e.g., Collins, 1996; Chelba et al., 1997) use a dependency structure D of W instead of a parse tree T, where D is extracted from syntactic trees. Both of these models can be called grammar-based models, in that they capture the syntactic structure of a sentence, and the model parameters are estimated from syntactically annotated corpora such as the Penn Treebank. DLM, on the other hand, is a non-grammar-based model, because it is not based on any syntactic annotation: the dependency structure used in language modeling was learned direct</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Roark, Brian. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 17-2: 1-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Adaptive statistical language modeling: a maximum entropy approach.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="27173" citStr="Rosenfeld, 1994" startWordPosition="4586" endWordPosition="4587">ependency relations provide useful information for language modeling, the most linguistically related word in the previous context may come in various linguistic relations with the word being predicted, not limited to syntactic dependency. This opens up new possibilities for exploring the combination of different knowledge sources in language modeling. Regarding the function that maps the left context onto equivalence classes, we used a simple approximation that takes into account only one linguistically related word in left context. An alternative is to use the maximum entropy (ME) approach (Rosenfeld, 1994; Chelba et al., 1997). Although ME models provide a nice framework for incorporating arbitrary knowledge sources that can be encoded as a large set of constraints, training and using ME models is extremely computationally expensive. Our working hypothesis is that the information for predicting the new word is dominated by a very limited set of words which can be selected heuristically: in this paper, is defined as a heuristic function that maps D to one word in D that has the strongest linguistic relation with the word being predicted, as in (8). This hypothesis is borne out by 8 In this sens</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>Rosenfeld, Ronald. 1994. Adaptive statistical language modeling: a maximum entropy approach. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Discovery of linguistic relations using lexical attraction.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="9436" citStr="Yuret (1998)" startWordPosition="1582" endWordPosition="1583">right manner, and is very efficient and simple to implement. In our experiments, we found that the algorithm performs reasonably well on average, and its speed and simplicity make it a bett η1=C(wi,wj,R),δ1= C(wi,wj) η 2 = C ( w i ,*, R ) , δ 2 = C ( wi ,*) , η 3 = C (*, w j , R ) , δ 3 = C (*, wj ) , η 4 = C (*,*, R ) , δ 4 = C (*,*) . E =λ + −λ λ + −λ 1 1 (1 1)( 2 23 (1 2) 4) E λ1 λ2 O(n2). O(n5).2 er choice in DLM training where we need to parse a large amount of training data iteratively, as described in Section 3.3. The parsing algorithm is a slightly modified version of that proposed in Yuret (1998). It reads a sentence left to right; after reading each new word For parsers that use bigram lexical dependencies, Eisner and Satta (1999) presents parsing algorithms that are or 2 O(n4) O(n3). We thank Joshua Goodman for pointing this out. it tries to link to each of its previous words and push the generated dependency into a stack. When a dependency crossing or a cycle is detected in the stack, the dependency with the lowest dependency probability in conflict is eliminated. The algorithm is outlined in Figures 2 an 1 for j 1 to 2 for i downto 1 3 PUSH into the stack Dj 4 if a dependency cycl</context>
<context position="27854" citStr="Yuret (1998)" startWordPosition="4704" endWordPosition="4705">or incorporating arbitrary knowledge sources that can be encoded as a large set of constraints, training and using ME models is extremely computationally expensive. Our working hypothesis is that the information for predicting the new word is dominated by a very limited set of words which can be selected heuristically: in this paper, is defined as a heuristic function that maps D to one word in D that has the strongest linguistic relation with the word being predicted, as in (8). This hypothesis is borne out by 8 In this sense, our model is an extension of a dependency-based model proposed in Yuret (1998). However, this work has not been evaluated as a language model with error rate reduction. an additional experiment we conducted, where we used two words from D that had the strongest relation with the word being predicted; this resulted in a very limited gain in CER reduction of 0.62%, which is not statistically significant (P&gt;0.05 according to the t test). The EM-like method for learning dependency relations described in Section 3.3 has also been applied to other tasks such as hidden Markov model training (Rabiner, 1989), syntactic relation learning (Yuret, 1998), and Chinese word segmentati</context>
</contexts>
<marker>Yuret, 1998</marker>
<rawString>Yuret, Deniz. 1998. Discovery of linguistic relations using lexical attraction. Ph.D. thesis, MIT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>