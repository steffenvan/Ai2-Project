<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004836">
<title confidence="0.841472">
MSR SPLAT, a language analysis toolkit
</title>
<author confidence="0.73525225">
Chris Quirk, Pallavi Choudhury, Jianfeng Colin Cherry
Gao, Hisami Suzuki, Kristina Toutanova,
Michael Gamon, Wen-tau Yih, Lucy
Vanderwende
</author>
<affiliation confidence="0.631088">
Microsoft Research National Research Council Canada
</affiliation>
<address confidence="0.52974">
Redmond, WA 98052 USA 1200 Montreal Road
Ottawa, Ontario K1A 0R6
</address>
<keyword confidence="0.760795333333333">
{chrisq, pallavic, jfgao,
hisamis, kristout,
mgamon,scottyih,
</keyword>
<email confidence="0.500602">
lucyv@microsoft.com}
</email>
<sectionHeader confidence="0.990839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894357142857">
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the
linguistic analysis tools produced by the NLP
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools
such as part-of-speech taggers, constituency
and dependency parsers, and more recent de-
velopments such as sentiment detection and
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used
from a broad set of programming languages.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999545">
The availability of annotated data sets that have
become community standards, such as the Penn
TreeBank (Marcus et al., 1993) and PropBank
(Palmer et al., 2005), has enabled many research
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many
differences in how these components are built, re-
sulting in slight but noticeable variation in the
component output. In experimental settings, it has
proved sometimes difficult to distinguish between
improvements contributed by a specific component
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty,
and shared task organizers are now providing ac-
</bodyText>
<page confidence="0.986754">
21
</page>
<bodyText confidence="0.9801865">
colin.cherry@nrccnrc.gc.ca
companying parses and other analyses of the
shared task data. For instance, the BioNLP shared
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools.
On the other hand, many community members
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.
Our toolkit is offered in this same spirit. We
have created well-tested, efficient linguistic tools
in the course of our research, using commonly
available resources such as the PTB and PropBank.
We also have created some tools that are less
commonly available in the community, for exam-
ple linguistically valid base forms and semantic
role analyzers. These components are on par with
other state of the art systems.
We hope that sharing these tools will enable
some researchers to carry out their projects without
having to re-create or download commonly used
NLP components, or potentially allow researchers
to compare our results with those of their own
tools. The further advantage of designing MSR
SPLAT as a web service is that we can share new
components on an on-going basis.
</bodyText>
<sectionHeader confidence="0.599119" genericHeader="method">
2 Parsing Functionality
</sectionHeader>
<subsectionHeader confidence="0.984178">
2.1 Constituency Parsing
</subsectionHeader>
<footnote confidence="0.996136">
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask for
the description of other resources made available in addition to
the shared task data.
2 See, for example, http://nlp.stanford.edu/software;
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp;
http://incubator.apache.org/opennlp
</footnote>
<note confidence="0.7422015">
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21–24,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998795">
The syntactic parser in MSR SPLAT attempts to
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al., 1993). This rep-
resentation captures the notion of labeled syntactic
constituents using a parenthesized representation.
For instance, the sentence “Colorless green ideas
sleep furiously.” could be assigned the following
parse tree, written in the form of an S expression:
</bodyText>
<equation confidence="0.971398">
(TOP (S
(NP (JJ Colorless) (JJ green) (NNS ideas))
(VP (VB sleep) (ADVP (RB furiously)))
(. .)))
</equation>
<bodyText confidence="0.999925625">
For instance, this parse tree indicates that “Color-
less green ideas” is a noun phrase (NP), and “sleep
furiously” is a verb phrase (VP).
Using the Wall Street Journal portion of the
Penn TreeBank, we estimate a coarse grammar
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge
method of Petrov et al. (2006). Each input symbol
is split into two new symbols, both with a new
unique symbol label, and the grammar is updated
to include a copy of each original rule for each
such refinement, with a small amount of random
noise added to the probability of each production
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their
estimated contribution to the likelihood of the data)
are merged back into their original form and the
parameters are again re-estimated using AEM. We
found six split-merge iterations produced optimal
accuracy on the standard development set.
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al.
2006). Coarse-to-fine parsing with pruning at each
level helps increase speed; pruning thresholds are
picked for each level to have minimal impact on
development set accuracy. However, the initial
coarse pass still has runtime cubic in the length of
the sentence. Thus, we limit the search space of the
coarse parse by closing selected chart cells before
the parse begins (Roark and Hollingshead, 2008).
We train a classifier to determine if constituents
may start or end at each position in the sentence.
For instance, constituents seldom end at the word
“the” or begin at a comma. Closing a number of
chart cells can substantially improve runtime with
minimal impact on accuracy.
</bodyText>
<subsectionHeader confidence="0.995016">
2.2 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999992545454545">
The dependency parses produced by MSR SPLAT
are unlabeled, directed arcs indicating the syntactic
governor of each word.
These dependency trees are computed from the
output of the constituency parser. First, the head of
each non-terminal is computed according to a set
of rules (Collins, 1999). Then, the tree is flattened
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child
word c if the non-terminal headed by p is a parent
of the non-terminal headed by c.
</bodyText>
<subsectionHeader confidence="0.996007">
2.3 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.999960481481482">
The Semantic Role Labeling component of MSR
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al.,
2005). The semantic roles represent a level of
broad-coverage shallow semantic analysis which
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.
For example, in the two sentences “John broke
the window” and “The window broke”, the phrase
the window will be marked with a THEME label.
Note that the syntactic role of the phrase in the two
sentences is different but the semantic role is the
same. The actual labeling scheme makes use of
numbered argument labels, like ARG0, ARG1, ...,
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is
verb-specific, with ARG0 typically representing an
agent-like role, and ARG1 a patient-like role.
This implementation of an SRL system follows
the approach described in (Xue and Palmer, 04),
and includes two log-linear models for argument
identification and classification. A single syntax
tree generated by the MSR SPLAT split-merge
parser is used as input. Non-overlapping arguments
are derived using the dynamic programming algo-
rithm by Toutanova et al. (2008).
</bodyText>
<sectionHeader confidence="0.991423" genericHeader="method">
3 Other Language Analysis Functionality
</sectionHeader>
<subsectionHeader confidence="0.924195">
3.1 Sentence Boundary / Tokenization
</subsectionHeader>
<page confidence="0.944012">
22
</page>
<bodyText confidence="0.999952363636364">
This analyzer identifies sentence boundaries and
breaks the input into tokens. Both are represented
as offsets of character ranges. Each token has both
a raw form from the string and a normalized form
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects
sentence boundaries with high accuracy, and a set
of regular expressions tokenize the input.
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Stemming / Lemmatization
</subsectionHeader>
<bodyText confidence="0.999792333333333">
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational
morphology.
</bodyText>
<subsectionHeader confidence="0.568175">
3.2.1 Stems
</subsectionHeader>
<bodyText confidence="0.9999765">
The stemmer analyzer indicates a stem form for
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form “dai”
to “daily” and “day”, but as these forms are not
citation forms of these words, presentation to end
users is known to be problematic.
</bodyText>
<subsectionHeader confidence="0.721899">
3.2.2 Lemmas
</subsectionHeader>
<bodyText confidence="0.999720142857143">
The lemma analyzer uses inflectional morphology
to indicate the dictionary lookup form of the word.
For example, the lemma of “daily” will be “daily”,
while the lemma of “children” will be “child”. We
have mined the lemma form of input tokens using
a broad-coverage grammar NLPwin (Heidorn,
2000) over very large corpora.
</bodyText>
<subsectionHeader confidence="0.877833">
3.2.3 Bases
</subsectionHeader>
<bodyText confidence="0.999630727272727">
The base analyzer uses derivational morphology to
indicate the dictionary lookup form of the word; as
there can be more than one derivation for a given
word, the base type returns a list of forms. For ex-
ample, the base form of “daily” will be “day”,
while the base form of “additional” will be “addi-
tion” and “add”. We have generated a static list of
base forms of tokens using a broad-coverage
grammar NLPwin (Heidorn, 2000) over very large
corpora. If the token form has not been observed in
those corpora, we will not return a base form.
</bodyText>
<subsectionHeader confidence="0.998815">
3.3 POS tagging
</subsectionHeader>
<bodyText confidence="0.9999702">
We train a maximum entropy Markov Model on
part-of-speech tags from the Penn TreeBank. This
optimized implementation has very high accuracy
(over 96% on the test set) and yet can tag tens of
thousands of words per second.
</bodyText>
<subsectionHeader confidence="0.995132">
3.4 Chunking
</subsectionHeader>
<bodyText confidence="0.999569166666667">
The chunker (Gao et al., 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn
TreeBank. With state-of-the-art chunking accuracy
as evaluated on the benchmark dataset, the chunker
is also robust and efficient, and has been used to
process very large corpora of web documents.
</bodyText>
<sectionHeader confidence="0.946322" genericHeader="method">
4 The Flexibility of a Web Service
</sectionHeader>
<bodyText confidence="0.9999063125">
By making the MSR SPLAT toolkit available as a
web service, we can provide access to new tools,
e.g. sentiment analysis. We are in the process of
building out the tools to provide language analysis
for languages other than English. One step in this
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for
both directions.
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb,
and the head of the object, whenever such a triple
is encountered. We found this functionality to be
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task.
</bodyText>
<sectionHeader confidence="0.93743" genericHeader="method">
5 Programmatic Access
</sectionHeader>
<subsectionHeader confidence="0.98447">
5.1 Web service reference
</subsectionHeader>
<bodyText confidence="0.999951444444445">
We have designed a web service that accepts a
batch of text and applies a series of analysis tools
to that text, returning a bag of analyses. This main
web service call, named “Analyze”, requires four
parameters: the language of the text (such as “en”
for English), the raw text to be analyzed, the set of
analyzers to apply, and an access key to monitor
and, if necessary, constrain usage. It returns a list
of analyses, one from each requested analyzer, in a
</bodyText>
<page confidence="0.998537">
23
</page>
<figureCaption confidence="0.90872025">
Figure 1. Screenshot of the MSR SPLAT interactive UI
showing selected functionalities which can be toggled
on and off. This is the interface that we propose to
demo at NAACL.
</figureCaption>
<bodyText confidence="0.996668166666667">
simple JSON (JavaScript Object Notation) format
easy to parse in many programming languages.
In addition, there is a web service call “Lan-
guages” that enumerates the list of available lan-
guages, and “Analyzers” to discover the set of
analyzers available in a given language.
</bodyText>
<subsectionHeader confidence="0.997366">
5.2 Data Formats
</subsectionHeader>
<bodyText confidence="0.999995833333333">
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned
as S expressions, part-of-speech tags are returned
as lists, dependency trees are returned as lists of
parent indices, and so on. The website contains an
authoritative description of each analysis format.
</bodyText>
<subsectionHeader confidence="0.977141">
5.3 Speed
</subsectionHeader>
<bodyText confidence="0.999938444444445">
Speed of analysis is heavily dependent on the
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging
process thousands of sentences per second; our
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a
paragraph at a time) to minimize the impact of
network latency.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999476">
We hope that others will find the tools that we
have made available as useful as we have. We en-
courage people to send us their feedback so that we
can improve our tools and increase collaboration in
the community.
</bodyText>
<sectionHeader confidence="0.973541" genericHeader="conclusions">
7 Script Outline
</sectionHeader>
<bodyText confidence="0.994040142857143">
The interactive UI (Figure 1) allows an arbitrary
sentence to be entered and the desired levels of
analysis to be selected as output. As there exist
other such toolkits, the demonstration is primarily
aimed at allowing participants to assess the quality,
utility and speed of the MSR SPLAT tools.
http://research.microsoft.com/en-us/projects/msrsplat/
</bodyText>
<sectionHeader confidence="0.997971" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998737822222222">
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of
EMNLP.
Michael Collins. 1999. Head-driven statistical models for
natural language parsing. PhD Dissertation, University of
Pennsylvania.
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming
Zhou and Chang-Ning Huang. 2001. Improving query
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR.
George Heidorn. 2000. Intelligent writing assistance. In R.
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for
the Processing of Text. New York: Marcel Dekker.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguistics
19(2): 313-330.
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles.
Computational Linguistics, 31(1): 71-105
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of ACL.
Brian Roark and Kristy Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference. In
Proceedings of COLING.
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of
ICML.
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling,
Computational Linguistics, 34(2): 161-191.
Nianwen Xue and Martha Palmer. 2004. Calibrating Features
for Semantic Role Labeling. In Proceedings of EMNLP.
Munmun de Choudhury, Scott Counts, Michael Gamon. Not
All Moods are Created Equal! Exploring Human Emotional
States in Social Media. Accepted for presentation in
ICWSM 2012
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective
States in Social Media. Accepted for presentation (short
paper) in ICWSM 2012
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.230677">
<title confidence="0.992271">MSR SPLAT, a language analysis toolkit</title>
<author confidence="0.984217333333333">Chris Quirk</author>
<author confidence="0.984217333333333">Pallavi Choudhury</author>
<author confidence="0.984217333333333">Jianfeng Colin Cherry Gao</author>
<author confidence="0.984217333333333">Hisami Suzuki</author>
<author confidence="0.984217333333333">Kristina Toutanova</author>
<author confidence="0.984217333333333">Michael Gamon</author>
<author confidence="0.984217333333333">Wen-tau Yih</author>
<author confidence="0.984217333333333">Lucy</author>
<email confidence="0.360817">Vanderwende</email>
<affiliation confidence="0.993217">Microsoft Research National Research Council Canada</affiliation>
<address confidence="0.9971865">Redmond, WA 98052 USA 1200 Montreal Road Ottawa, Ontario K1A 0R6</address>
<email confidence="0.860116">{chrisq,pallavic,hisamis,lucyv@microsoft.com}</email>
<abstract confidence="0.999722">We describe MSR SPLAT, a toolkit for language analysis that allows easy access to the linguistic analysis tools produced by the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Hisami Suzuki</author>
</authors>
<title>Discriminative substring decoding for transliteration.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="10680" citStr="Cherry and Suzuki (2009)" startWordPosition="1687" endWordPosition="1690">odel, and is trained on the Penn TreeBank. With state-of-the-art chunking accuracy as evaluated on the benchmark dataset, the chunker is also robust and efficient, and has been used to process very large corpora of web documents. 4 The Flexibility of a Web Service By making the MSR SPLAT toolkit available as a web service, we can provide access to new tools, e.g. sentiment analysis. We are in the process of building out the tools to provide language analysis for languages other than English. One step in this direction is a tool for transliterating between English and Katakana words. Following Cherry and Suzuki (2009), the toolkit currently outputs the 10- best transliteration candidates with probabilities for both directions. Another included service is the Triples analyzer, which returns the head of the subject, the verb, and the head of the object, whenever such a triple is encountered. We found this functionality to be useful as we were exploring features for our system submitted to the BioNLP shared task. 5 Programmatic Access 5.1 Web service reference We have designed a web service that accepts a batch of text and applies a series of analysis tools to that text, returning a bag of analyses. This main</context>
</contexts>
<marker>Cherry, Suzuki, 2009</marker>
<rawString>Colin Cherry and Hisami Suzuki. 2009. Discriminative substring decoding for transliteration. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<institution>PhD Dissertation, University of Pennsylvania.</institution>
<contexts>
<context position="6218" citStr="Collins, 1999" startWordPosition="951" endWordPosition="952">ollingshead, 2008). We train a classifier to determine if constituents may start or end at each position in the sentence. For instance, constituents seldom end at the word “the” or begin at a comma. Closing a number of chart cells can substantially improve runtime with minimal impact on accuracy. 2.2 Dependency Parsing The dependency parses produced by MSR SPLAT are unlabeled, directed arcs indicating the syntactic governor of each word. These dependency trees are computed from the output of the constituency parser. First, the head of each non-terminal is computed according to a set of rules (Collins, 1999). Then, the tree is flattened into maximal projections of heads. Finally, we introduce an arc from a parent word p to a child word c if the non-terminal headed by p is a parent of the non-terminal headed by c. 2.3 Semantic Role Labeling The Semantic Role Labeling component of MSR SPLAT labels the semantic roles of verbs according to the PropBank specification (Palmer et al., 2005). The semantic roles represent a level of broad-coverage shallow semantic analysis which goes beyond syntax, but does not handle phenomena like co-reference and quantification. For example, in the two sentences “John </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. PhD Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
<author>Jian Zhang</author>
</authors>
<title>Endong Xun, Ming Zhou and Chang-Ning Huang.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="10024" citStr="Gao et al., 2001" startWordPosition="1575" endWordPosition="1578">ple, the base form of “daily” will be “day”, while the base form of “additional” will be “addition” and “add”. We have generated a static list of base forms of tokens using a broad-coverage grammar NLPwin (Heidorn, 2000) over very large corpora. If the token form has not been observed in those corpora, we will not return a base form. 3.3 POS tagging We train a maximum entropy Markov Model on part-of-speech tags from the Penn TreeBank. This optimized implementation has very high accuracy (over 96% on the test set) and yet can tag tens of thousands of words per second. 3.4 Chunking The chunker (Gao et al., 2001) is based on a Cascaded Markov Model, and is trained on the Penn TreeBank. With state-of-the-art chunking accuracy as evaluated on the benchmark dataset, the chunker is also robust and efficient, and has been used to process very large corpora of web documents. 4 The Flexibility of a Web Service By making the MSR SPLAT toolkit available as a web service, we can provide access to new tools, e.g. sentiment analysis. We are in the process of building out the tools to provide language analysis for languages other than English. One step in this direction is a tool for transliterating between Englis</context>
</contexts>
<marker>Gao, Nie, Zhang, 2001</marker>
<rawString>Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming Zhou and Chang-Ning Huang. 2001. Improving query translation for CLIR using statistical Models. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Heidorn</author>
</authors>
<title>Intelligent writing assistance. In</title>
<date>2000</date>
<booktitle>A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Text.</booktitle>
<editor>R. Dale, H. Moisl and H. Somers (eds.),</editor>
<publisher>Marcel Dekker.</publisher>
<location>New York:</location>
<contexts>
<context position="9164" citStr="Heidorn, 2000" startWordPosition="1424" endWordPosition="1425">the standard Porter stemming algorithm (Porter, 1980). These forms are known to be useful in applications such as clustering, as the algorithm assigns the same form “dai” to “daily” and “day”, but as these forms are not citation forms of these words, presentation to end users is known to be problematic. 3.2.2 Lemmas The lemma analyzer uses inflectional morphology to indicate the dictionary lookup form of the word. For example, the lemma of “daily” will be “daily”, while the lemma of “children” will be “child”. We have mined the lemma form of input tokens using a broad-coverage grammar NLPwin (Heidorn, 2000) over very large corpora. 3.2.3 Bases The base analyzer uses derivational morphology to indicate the dictionary lookup form of the word; as there can be more than one derivation for a given word, the base type returns a list of forms. For example, the base form of “daily” will be “day”, while the base form of “additional” will be “addition” and “add”. We have generated a static list of base forms of tokens using a broad-coverage grammar NLPwin (Heidorn, 2000) over very large corpora. If the token form has not been observed in those corpora, we will not return a base form. 3.3 POS tagging We tr</context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>George Heidorn. 2000. Intelligent writing assistance. In R. Dale, H. Moisl and H. Somers (eds.), A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Text. New York: Marcel Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="1131" citStr="Marcus et al., 1993" startWordPosition="168" endWordPosition="171">the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty,</context>
<context position="3608" citStr="Marcus et al., 1993" startWordPosition="523" endWordPosition="526">tionality 2.1 Constituency Parsing 1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask for the description of other resources made available in addition to the shared task data. 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21–24, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics The syntactic parser in MSR SPLAT attempts to reconstruct a parse tree according the Penn TreeBank specification (Marcus et al., 1993). This representation captures the notion of labeled syntactic constituents using a parenthesized representation. For instance, the sentence “Colorless green ideas sleep furiously.” could be assigned the following parse tree, written in the form of an S expression: (TOP (S (NP (JJ Colorless) (JJ green) (NNS ideas)) (VP (VB sleep) (ADVP (RB furiously))) (. .))) For instance, this parse tree indicates that “Colorless green ideas” is a noun phrase (NP), and “sleep furiously” is a verb phrase (VP). Using the Wall Street Journal portion of the Penn TreeBank, we estimate a coarse grammar over the gi</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics 19(2): 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>71--105</pages>
<contexts>
<context position="1166" citStr="Palmer et al., 2005" startWordPosition="174" endWordPosition="177">. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty, and shared task organizers are now</context>
<context position="6601" citStr="Palmer et al., 2005" startWordPosition="1018" endWordPosition="1021">d, directed arcs indicating the syntactic governor of each word. These dependency trees are computed from the output of the constituency parser. First, the head of each non-terminal is computed according to a set of rules (Collins, 1999). Then, the tree is flattened into maximal projections of heads. Finally, we introduce an arc from a parent word p to a child word c if the non-terminal headed by p is a parent of the non-terminal headed by c. 2.3 Semantic Role Labeling The Semantic Role Labeling component of MSR SPLAT labels the semantic roles of verbs according to the PropBank specification (Palmer et al., 2005). The semantic roles represent a level of broad-coverage shallow semantic analysis which goes beyond syntax, but does not handle phenomena like co-reference and quantification. For example, in the two sentences “John broke the window” and “The window broke”, the phrase the window will be marked with a THEME label. Note that the syntactic role of the phrase in the two sentences is different but the semantic role is the same. The actual labeling scheme makes use of numbered argument labels, like ARG0, ARG1, ..., ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arg</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1): 71-105</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>130--137</pages>
<contexts>
<context position="8603" citStr="Porter, 1980" startWordPosition="1329" endWordPosition="1330"> form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions tokenize the input. 3.2 Stemming / Lemmatization We provide three types of stemming: Porter stemming, inflectional morphology and derivational morphology. 3.2.1 Stems The stemmer analyzer indicates a stem form for each input token, using the standard Porter stemming algorithm (Porter, 1980). These forms are known to be useful in applications such as clustering, as the algorithm assigns the same form “dai” to “daily” and “day”, but as these forms are not citation forms of these words, presentation to end users is known to be problematic. 3.2.2 Lemmas The lemma analyzer uses inflectional morphology to indicate the dictionary lookup form of the word. For example, the lemma of “daily” will be “daily”, while the lemma of “children” will be “child”. We have mined the lemma form of input tokens using a broad-coverage grammar NLPwin (Heidorn, 2000) over very large corpora. 3.2.3 Bases T</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3): 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4427" citStr="Petrov et al. (2006)" startWordPosition="654" endWordPosition="657"> the following parse tree, written in the form of an S expression: (TOP (S (NP (JJ Colorless) (JJ green) (NNS ideas)) (VP (VB sleep) (ADVP (RB furiously))) (. .))) For instance, this parse tree indicates that “Colorless green ideas” is a noun phrase (NP), and “sleep furiously” is a verb phrase (VP). Using the Wall Street Journal portion of the Penn TreeBank, we estimate a coarse grammar over the given grammar symbols. Next, we perform a series of refinements to automatically learn fine-grained categories that better capture the implicit correlations in the tree using the split-merge method of Petrov et al. (2006). Each input symbol is split into two new symbols, both with a new unique symbol label, and the grammar is updated to include a copy of each original rule for each such refinement, with a small amount of random noise added to the probability of each production to break ties. We estimate new grammar parameters using an accelerated form of the EM algorithm (Salakhutdinov and Roweis, 2003). Then the lowest 50% of the split symbols (according to their estimated contribution to the likelihood of the data) are merged back into their original form and the parameters are again re-estimated using AEM. </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity context-free inference.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5622" citStr="Roark and Hollingshead, 2008" startWordPosition="853" endWordPosition="856">are again re-estimated using AEM. We found six split-merge iterations produced optimal accuracy on the standard development set. The best tree for a given input is selected according to the max-rule approach (cf. Petrov et al. 2006). Coarse-to-fine parsing with pruning at each level helps increase speed; pruning thresholds are picked for each level to have minimal impact on development set accuracy. However, the initial coarse pass still has runtime cubic in the length of the sentence. Thus, we limit the search space of the coarse parse by closing selected chart cells before the parse begins (Roark and Hollingshead, 2008). We train a classifier to determine if constituents may start or end at each position in the sentence. For instance, constituents seldom end at the word “the” or begin at a comma. Closing a number of chart cells can substantially improve runtime with minimal impact on accuracy. 2.2 Dependency Parsing The dependency parses produced by MSR SPLAT are unlabeled, directed arcs indicating the syntactic governor of each word. These dependency trees are computed from the output of the constituency parser. First, the head of each non-terminal is computed according to a set of rules (Collins, 1999). Th</context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2008. Classifying chart cells for quadratic complexity context-free inference. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Sam Roweis</author>
</authors>
<title>Adaptive Overrelaxed Bound Optimization Methods.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="4816" citStr="Salakhutdinov and Roweis, 2003" startWordPosition="722" endWordPosition="725">rse grammar over the given grammar symbols. Next, we perform a series of refinements to automatically learn fine-grained categories that better capture the implicit correlations in the tree using the split-merge method of Petrov et al. (2006). Each input symbol is split into two new symbols, both with a new unique symbol label, and the grammar is updated to include a copy of each original rule for each such refinement, with a small amount of random noise added to the probability of each production to break ties. We estimate new grammar parameters using an accelerated form of the EM algorithm (Salakhutdinov and Roweis, 2003). Then the lowest 50% of the split symbols (according to their estimated contribution to the likelihood of the data) are merged back into their original form and the parameters are again re-estimated using AEM. We found six split-merge iterations produced optimal accuracy on the standard development set. The best tree for a given input is selected according to the max-rule approach (cf. Petrov et al. 2006). Coarse-to-fine parsing with pruning at each level helps increase speed; pruning thresholds are picked for each level to have minimal impact on development set accuracy. However, the initial</context>
</contexts>
<marker>Salakhutdinov, Roweis, 2003</marker>
<rawString>Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Overrelaxed Bound Optimization Methods. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A global joint model for semantic role labeling,</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>161--191</pages>
<contexts>
<context position="7713" citStr="Toutanova et al. (2008)" startWordPosition="1192" endWordPosition="1195">els, like ARG0, ARG1, ..., ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arguments. The meaning of the numbered arguments is verb-specific, with ARG0 typically representing an agent-like role, and ARG1 a patient-like role. This implementation of an SRL system follows the approach described in (Xue and Palmer, 04), and includes two log-linear models for argument identification and classification. A single syntax tree generated by the MSR SPLAT split-merge parser is used as input. Non-overlapping arguments are derived using the dynamic programming algorithm by Toutanova et al. (2008). 3 Other Language Analysis Functionality 3.1 Sentence Boundary / Tokenization 22 This analyzer identifies sentence boundaries and breaks the input into tokens. Both are represented as offsets of character ranges. Each token has both a raw form from the string and a normalized form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions t</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling, Computational Linguistics, 34(2): 161-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating Features for Semantic Role Labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating Features for Semantic Role Labeling. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun de Choudhury</author>
<author>Scott Counts</author>
<author>Michael Gamon</author>
</authors>
<title>Not All Moods are Created Equal! Exploring Human Emotional States in Social Media. Accepted for presentation in ICWSM</title>
<date>2012</date>
<marker>de Choudhury, Counts, Gamon, 2012</marker>
<rawString>Munmun de Choudhury, Scott Counts, Michael Gamon. Not All Moods are Created Equal! Exploring Human Emotional States in Social Media. Accepted for presentation in ICWSM 2012</rawString>
</citation>
<citation valid="true">
<authors>
<author>Happy</author>
</authors>
<title>Nervous, Surprised? Classification of Human Affective States in Social Media. Accepted for presentation (short paper) in ICWSM</title>
<date>2012</date>
<marker>Happy, 2012</marker>
<rawString>Munmun de Choudhury, Scott Counts, Michael Gamon. Happy, Nervous, Surprised? Classification of Human Affective States in Social Media. Accepted for presentation (short paper) in ICWSM 2012</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>