<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.054565">
<title confidence="0.9980345">
Atypical Prosodic Structure as an Indicator of Reading Level
and Text Difficulty
</title>
<author confidence="0.978995">
Julie Medero and Mari Ostendorf
</author>
<affiliation confidence="0.980083">
Electrical Engineering Department
University of Washington
</affiliation>
<address confidence="0.861909">
Seattle, WA 98195 USA
</address>
<email confidence="0.999782">
{jmedero,ostendor}@uw.edu
</email>
<sectionHeader confidence="0.998607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999344733333334">
Automatic assessment of reading ability
builds on applying speech recognition tools
to oral reading, measuring words correct per
minute. This work looks at more fine-grained
analysis that accounts for effects of prosodic
context using a large corpus of read speech
from a literacy study. Experiments show that
lower-level readers tend to produce relatively
more lengthening on words that are not likely
to be final in a prosodic phrase, i.e. in less
appropriate locations. The results have impli-
cations for automatic assessment of text dif-
ficulty in that locations of atypical prosodic
lengthening are indicative of difficult lexical
items and syntactic constructions.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986614">
Fluent reading is known to be a good indicator of
reading comprehension, especially for early readers
(Rasinski, 2006), so oral reading is often used to
evaluate a student’s reading level. One method that
can be automated with speech recognition technol-
ogy is the number of words that a student can read
correctly of a normed passage, or Words Correct
Per Minute (WCPM) (Downey et al., 2011). Since
WCPM depends on speaking rate as well as liter-
acy, we are interested in identifying new measures
that can be automatically computed for use in com-
bination with WCPM to provide a better assessment
of reading level. In particular, we investigate fine-
grained measures that, if useful in identifying points
of difficulty for readers, can lead to new approaches
for assessing text difficulty.
The WCPM is reduced when a person repeats or
incorrectly reads a word, but also when they in-
troduce pauses and articulate words more slowly.
Pauses and lengthened articulation can be an indi-
cator of uncertainty for a low-level reader, but these
phenomena are also used by skilled readers to mark
prosodic phrase structure, facilitating comprehen-
sion in listeners. Since prosodic phrase boundaries
tend to occur in locations that coincide with certain
syntactic constituent boundaries, it is possible to au-
tomatically predict prosodic phrase boundary loca-
tions from part-of-speech labels and syntactic struc-
ture with fairly high reliability for read news stories
(Ananthakrishnan and Narayanan, 2008). Thus, we
hypothesize that we can more effectively leverage
word-level articulation and pause information by fo-
cusing on words that are less likely to be associ-
ated with prosodic phrase boundaries. By compar-
ing average statistics of articulation rate and paus-
ing for words at boundary vs. non-boundary loca-
tions, we hope to obtain a measure that could aug-
ment reading rate for evaluating reading ability. We
also hypothesize that the specific locations of hesita-
tion phenomena (word lengthening and pausing) ob-
served for multiple readers will be indicative of par-
ticular points of difficulty in a text, either because a
word is difficult or because a syntactic construction
is difficult. Detecting these regions and analyzing
the associated lexical and syntactic correlates is po-
tentially useful for automatically characterizing text
difficulty.
Our study of hesitation phenomena involves em-
pirical analysis of the oral reading data from the Flu-
ency Addition to the National Assessment of Adult
</bodyText>
<page confidence="0.977373">
715
</page>
<subsectionHeader confidence="0.295184">
Proceedings of NAACL-HLT 2013, pages 715–720,
</subsectionHeader>
<bodyText confidence="0.9643375">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
Literacy (FAN), which collected oral readings from
roughly 12,000 adults, reading short (150-200 word)
fourth- and eighth grade passages (Baer et al., 2009).
The participants in that study were chosen to re-
flect the demographics of adults in the United States;
thus, speakers of varying reading levels and non-
native speakers were included. For our study, we
had access to time alignments of automatic tran-
scriptions, but not the original audio files.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955868421052">
For low-level readers, reading rate and fluency are
good indicators of reading comprehension (Miller
and Schwanenflugel, 2006; Spear-Swerling, 2006).
Zhang and colleagues found that features of chil-
dren’s oral readings, along with their interactions
with an automated tutor, could predict a single stu-
dent’s comprehension question performance over
the course of a document (2007). Using oral read-
ings is appealing because it avoids the difficulty of
separating question difficulty from passage difficulty
(Ozuru et al., 2008) and of questions that can be an-
swered through world knowledge (Keenan and Bet-
jemann, 2006).
WCPM is generally used as a tool for assessing
reading level by averaging across one or more pas-
sages. It is more noisy when comparing the read-
ability of different texts, especially when the reading
level is measured at a fine-grained (e.g. word) level.
If longer words take longer to read orally, it may
be merely a consequence of having more phonemes,
and not of additional reading difficulty. Further,
for communication reasons, pauses and slow aver-
age articulation rates tend to coincide with major
phrase boundaries. In our work, we would like to ac-
count for prosodic context in using articulation rate
to identify difficult words and constructions.
Much of the previous work on using automatic
speech recognition (ASR) output for reading level
or readability analysis has focused on assessing the
reading level of children (Downey et al., 2011;
Duchateau et al., 2007). Similar success has been
seen in predicting fluency scores in oral reading
tests for L2 learners of English (Balogh et al., 2012;
Bernstein et al., 2011). Project LISTEN has a read-
ing tutor for children that gives real-time feedback,
and has used orthographic and phonemic features
of individual words to predict the likelihood of real
word subsitutions (Mostow et al., 2002).
</bodyText>
<sectionHeader confidence="0.970356" genericHeader="method">
3 FAN Literacy Scores
</sectionHeader>
<bodyText confidence="0.999979604651163">
To examine the utility of word-level pause and ar-
ticulation rate features for predicting reading level
when controlled for prosodic context, we use the Ba-
sic Reading Skills (BRS) score available for each
reader in the FAN data. The BRS score measures
an individual’s average reading rate in WCPM. Each
participant read three word lists, three pseudo-word
lists, one easy text passage, and one harder text pas-
sage, and the BRS is the average WCPM over the
eight different readings. Specifically, the WCPM
for each case is computed automatically using Ordi-
nate’s VersaReader system to transcribe the speech
given the target text (Balogh et al., 2005). The sys-
tem output is then automatically aligned to the tar-
get texts using the track-the-reader method of Ras-
mussen et al. (2011), which defines weights for re-
gressions and skipped words and then identifies a
least-cost alignment between the ASR output and a
text. Automatic calculation of WCPM has high cor-
relation (.96-1.0) with human judgment of WCPM
(Balogh et al., 2012), so it has the advantage of be-
ing easy to automate.
Word Error Rate (WER) for the the ASR compo-
nent in Ordinate’s prototype reading tracker (Balogh
et al., 2012) may be estimated to be between 6% and
10%. In a sample of 960 passage readings, where
various sets of two passages were read by each of
480 adults (160 native Spanish speakers, 160 native
English-speaking African Americans, and 160 other
native English speakers), the Ordinate ASR system
exhibited a 6.9% WER on the 595 passages that con-
tained no spoken material that was unintelligible to
human transcribers. On the complete set of 960 pas-
sages, the system exhibited a 9.9% WER, with each
unintelligible length of speech contributing one or
more errors to the word error count.
The greatest problem with speech recognition er-
rors is for very low-level readers (Balogh et al.,
2012). In order to have more reliable time align-
ments and BRS scores, approximately 15% of the
FAN participants were excluded from the current
analysis. This 15% were those participants whose
BRS score was labeled ”Below Basic” in the NAAL
</bodyText>
<page confidence="0.990433">
716
</page>
<bodyText confidence="0.99916025">
reading scale. Additional participants were elimi-
nated because of missing or incomplete (less than a
few seconds) recordings. With these exclusions, the
number of speakers in our study was 7587.
</bodyText>
<sectionHeader confidence="0.958171" genericHeader="method">
4 Prosodic Boundary Prediction
</sectionHeader>
<bodyText confidence="0.994401760869565">
We trained a regression tree1 on hand-annotated
data from the Boston University Radio News Cor-
pus (Ostendorf et al., 1995) to predict the locations
where we expect to see prosodic boundaries. Each
word in the Radio News Corpus is labeled with a
prosodic boundary score from 0 (clitic, no bound-
ary) to 6 (sentence boundary). For each word, we
use features based on parse depth and structure and
POS bigrams to predict the prosodic boundary value.
For evaluation, the break labels are grouped into:
0-2 (no intonational boundary marker), 3 (intermedi-
ate phrase), and 4-6 (intonational phrase boundary).
Words with 0-2 breaks are considered non-boundary
words; 4-6 are boundary words. We expect that, for
fluent readers, lengthening and possibly pausing will
be observed after boundary words but not after non-
boundary words. Since the intermediate boundaries
are the most difficult to classify, and may be can-
didates for both boundaries and non-boundaries for
fluent readers, we omit them in our analyses. Our
model achieves 87% accuracy in predicting f in-
tonational phrase boundaries and 83% accuracy in
predicting f no intonational boundary, treating in-
termediate phrase boundaries as negative instances
in both cases.
Note that our 3-way prosodic boundary predic-
tion is aimed at identifying locations where fluent
readers are likely to place boundaries (or not), i.e.,
reliable locations for feature extraction, vs. accept-
able locations for text-to-speech synthesis. Because
of this goal and because work on prosodic bound-
ary prediction labels varies in its treatment of inter-
mediate phrase boundaries, our results are not di-
rectly comparable to prior studies. However, per-
formance is in the range reported in recent studies
predicting prosodic breaks from text features only.
Treating intermediate phrase boundaries as positive
examples, Ananthakrishnan and Narayanan (2008)
1Our approach differs slightly from previous work in the use
of a regression (vs. classification) model; this gave a small per-
formance gain.
achieve 88% accuracy. Treating them as negative
examples, Margolis and Ostendorf (2010) achieve
similar results. Both report results on a single held-
out test set, while our results are based on 10-fold
cross validation.
</bodyText>
<sectionHeader confidence="0.978695" genericHeader="method">
5 Experiments with Prosodic Context
</sectionHeader>
<subsectionHeader confidence="0.996353">
5.1 Word-level Rate Features
</subsectionHeader>
<bodyText confidence="0.999863074074074">
We looked at two acoustic cues related to hesitation
or uncertainty: pause duration and word lengthen-
ing. While pause duration is straightforward to ex-
tract (and not typically normalized), various meth-
ods have been used for word lengthening. We ex-
plore two measures of word lengthening: i) the
longest normalized vowel, and ii) the average nor-
malized length of word-final phones (the last vowel
and all following consonants). Word-final length-
ening is known to be a correlate of fluent prosodic
phrase boundaries (Wightman et al., 1992), and
we hypothesized that the longest normalized vowel
might be useful for hesitations though it can also in-
dicate prosodic prominence.
For word-level measures of lengthening, it is stan-
dard to normalize to account for inherent phoneme
durations. We use a z-score: measured duration mi-
nus phone mean divided by phone standard devia-
tion. In addition, Wightman et al. (1992) found it
useful to account for speaking rate in normalizing
phone duration. We adopt the same model, which
assumes that phone durations can be characterized
by a Gamma distribution and that speaker variabil-
ity is characterized by a linear scaling of the phone-
dependent mean parameters, where the scaling term
is shared by all phones. The linear scale factor α for
a speaker is estimated as:
</bodyText>
<equation confidence="0.967512">
α = 1 N di (1)
i=1
N Ap(i)
</equation>
<bodyText confidence="0.999696666666667">
where di is the duration of the i-th phone which has
label p(i) and where Ap is the speaker-independent
mean of phone p. Here, we use a speaker-
independent phone mean computed from the TIMIT
Corpus,2 which has hand-marked phonetic labels
and times. We make use of the speaking rate model
</bodyText>
<footnote confidence="0.879726">
2Available from the Linguistic Data Consortium.
</footnote>
<page confidence="0.996032">
717
</page>
<bodyText confidence="0.99991275">
to adjust the speaker-independent TIMIT phone du-
rations to the speakers in the FAN corpus by cal-
culating the linear scale factor α for each speaker.
Thus, the phone mean and standard deviation used
in the z-score normalization is αppi and αupi, re-
spectively.
From the many readings of the eight passages, we
identified roughly 777K spoken word instances at
predicted phrase boundaries and 2.0M spoken words
at predicted non-boundaries. For each uttered word,
we calculated three features: the length of the fol-
lowing pause, the length of the longest normalized
vowel, and the averaged normed length of all phones
from the last vowel to the end of the word, as de-
scribed above. The word-level features can be av-
eraged across instances from a speaker for assessing
reading level or across instances of a particular word
in a text uttered by many speakers to assess local text
difficulty.
The phone and pause durations are based on rec-
ognizer output, so they will be somewhat noisy.
The fact that the recognizer is biased towards the
intended word sequence and the omission of the
lowest-level readers from this study together con-
tribute to reducing the error rate (&lt; 10%) and in-
creasing the reliability of the features. In addition,
noise is reduced by averaging over multiple words
or multiple speakers.
</bodyText>
<subsectionHeader confidence="0.999334">
5.2 Reading Level Analysis
</subsectionHeader>
<bodyText confidence="0.999402333333333">
To assess the potential for prosodic context to im-
prove the utility of word-level features for assessing
reading difficulty, we looked at duration lengthening
and pauses at boundary and non-boundary locations,
where the boundary labels are predicted using the
text-based algorithm and 3-class grouping described
in section 4.
First, for each speaker, we averaged each fea-
ture across all boundary words read by that person
and across all non-boundary words read by that per-
son. We hypothesized that skilled readers would
have shorter averages for all three features at non-
boundary words compared to at boundary words,
while the differences for lower-level readers would
be smaller because of lengthening due to uncertainty
at non-boundary words. The difference between the
boundary and non-boudnary word averages for nor-
malized duration of end-of-word phones is plotted in
</bodyText>
<figureCaption confidence="0.99009">
Figure 1: Mean end-of-word normalized phone duration
(+/- standard deviation) as a function of BRS score
</figureCaption>
<bodyText confidence="0.996758375">
Figure 1 as a function of reading level. As expected,
the difference increases with reading skill, as mea-
sured by BRS. A similar trend is observed for the
longest normalized vowel in the word.
We also looked at pause duration, finding that the
average pause duration decreases as reading skill in-
creases for both boundary and non-boundary words.
Since pauses are not always present at intonational
phrase boundaries, but are more likely at sentence
boundaries, we investigated dividing the cases by
punctuation rather than prosodic context. Table 1
shows that for both the top 20% of readers and the
bottom 20% of readers, sentence boundaries had
much longer pauses on average, followed by comma
boundaries, and unpunctuated word boundaries. The
drop in both pause frequency and average pause du-
ration is much greater for the more skilled readers.
Looking at all speakers, the unpunctuated words
had an average pause duration that scaled with the
speaking rate estimate for that passage, with high
correlation (0.94). The correlation was much lower
for sentence boundaries (0.44). Thus, we conclude
that the length of pauses at non-boundary locations
is related to the speaker’s reading ability.
</bodyText>
<subsectionHeader confidence="0.996728">
5.3 Identifying Difficult Texts
</subsectionHeader>
<bodyText confidence="0.9998704">
Instead of averaging over multiple words in a pas-
sage, we can average over multiple readings of a
particular word. We identified difficult regions in
texts by sorting all tokens by the average normalized
length of their end-of-word phones for the lowest
</bodyText>
<page confidence="0.99256">
718
</page>
<table confidence="0.9987136">
Top 20% Bottom 20%
Pause Rate Avg. Pause Duration Pause Rate Avg. Pause Duration
Sentence-final 81.0% 177 ms 84.7% 283 ms
Comma 26.1% 94 ms 47.0% 168 ms
No punctuation 4.6% 77 ms 16.6% 139 ms
</table>
<tableCaption confidence="0.919473">
Table 1: Frequency of occurrence and average duration of pauses at sentence boundaries, comma boundaries, and
unpunctuated word boundaries for the top and bottom 20% of all readers, as sorted by BRS score
</tableCaption>
<listItem confidence="0.934977">
20% of readers. The examples suggest that length-
ening may coincide with reading difficulty caused
by syntactic ambiguity. Two sentences, with the
lengthened word in bold, illustrate representative
ambiguities:
• She was there for me the whole time my
grandfather was in the hospital.
• Since dogs are gentler when raised by a fam-
ily the dogs are given to children when the dogs
are about fourteen months old.
</listItem>
<bodyText confidence="0.9999612">
In the first example, “me” could be the end of the
sentence, while in the second example, readers may
expect “gentler” to be the end of the subordinate
clause started by “since”. The lengthening on these
words is much smaller for the top 20% of readers,
suggesting that the extra lengthening is associated
with points of difficulty for the less skilled readers.
Similarly, we identified sentences with non-
boundary locations where readers commonly
paused, with the word after the pause in bold:
</bodyText>
<listItem confidence="0.891096">
• We have always been able to share our es-
capades and humor with our friends.
• Check with your doctor first if you are a man
over forty or a woman over fifty and you plan
to do vigorous activity instead of moderate ac-
tivity.
</listItem>
<bodyText confidence="0.9999022">
We observe a wider variety of potential difficulties
here. Some are associated with difficult words, as in
the first example, while others involve syntactic am-
biguities similar to the ones seen in the lengthening
cases.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.99999">
We have shown that duration lengthening and pause
cues align with expected prosodic structure (pre-
dicted from syntactic features) more for skilled read-
ers than for low-level readers, which we hope may
lead to a richer assessment of individual reading dif-
ficulties. In addition, we have proposed a method
of characterizing text difficulty at a fine grain based
on these features using multiple oral readings. In or-
der to better understand the information provided by
the different features, we are conducting eye track-
ing experiments on these passages, and future work
will include an analysis of readers’ gaze during read-
ing of these constructions that have been categorized
in terms of their likely prosodic context.
In this work, where the original recordings were
not available, the study was restricted to duration
features. However, other work has suggested that
other prosodic cues, particularly pitch and energy
features, are useful for detecting speaker uncertainty
(Litman et al., 2009; Litman et al., 2012; Pon-Barry
and Shieber, 2011). Incorporating these cues may
increase the reliability of detecting points of read-
ing difficulty and/or offer complementary informa-
tion for characterizing text difficulties.
</bodyText>
<sectionHeader confidence="0.997296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998865">
We are grateful to the anonymous reviewers for their
feedback, and to our colleagues at Pearson Knowl-
edge Technologies for their insights and data pro-
cessing assistance. This material is based upon
work supported by the National Science Founda-
tion Graduate Research Fellowship under Grant No.
DGE-0718124 and by the National Science Founda-
tion under Grant No. IIS-0916951. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.96703475">
S. Ananthakrishnan and S.S. Narayanan. 2008. Auto-
matic Prosodic Event Detection Using Acoustic, Lex-
ical, and Syntactic Evidence. IEEE Trans. Audio,
Speech, and Language Processing, 16(1):216–228.
</reference>
<page confidence="0.992881">
719
</page>
<reference confidence="0.999868236111112">
J. Baer, M. Kutner, J. Sabatini, and S. White. 2009. Basic
Reading Skills and the Literacy of Americas Least Lit-
erate Adults: Results from the 2003 National Assess-
ment of Adult Literacy (NAAL) Supplemental Stud-
ies. Technical report, NCES.
J. Balogh, J. Bernstein, J. Cheng, and B. Townshend.
2005. Final Report Ordinates Scoring of FAN NAAL
Phase III: Accuracy Analysis. Technical report, Ordi-
nate.
J. Balogh, J. Bernstein, J. Cheng, A. Van Moere,
B. Townshend, and M. Suzuki. 2012. Validation of
Automated Scoring of Oral Reading. Educational and
Psychological Measurement, 72:435–452.
J. Bernstein, J. Cheng, and M. Suzuki. 2011. Fluency
Changes with General Progress in L2 Proficiency. In
Proc. Interspeech, number August, pages 877–880.
R. Downey, D. Rubin, J. Cheng, and J. Bernstein.
2011. Performance of Automated Scoring for Chil-
dren’s Oral Reading. Proc. Workshop on Innovative
Use of NLP for Building Educational Applications,
(June):46–55, June.
J. Duchateau, L. Cleuren, H. Van, and P. Ghesqui. 2007.
Automatic Assessment of Childrens Reading Level.
Proc. Interspeech, pages 1210–1213.
J.M. Keenan and R. Betjemann. 2006. Comprehending
the Gray Oral Reading Test Without Reading It: Why
Comprehension Tests Should Not Include Passage-
Independent Items. Scientific Studies of Reading,
10(4):363–380.
D. Litman, M. Rotaru, and G. Nicholas. 2009. Classi-
fying turn-level uncertainty using word-level prosody.
In Proc. Interspeech.
D. Litman, H. Friedberg, and K. Forbes-Riley. 2012.
Prosodic cues to disengagement and uncertainty in
physics tutorial dialogues. In Proc. Interspeech.
A. Margolis, M. Ostendorf, and K. Livescu. 2010. Cross-
genre training for automatic prosody classification. In
Proc. Speech Prosody Conference.
J. Miller and P.J. Schwanenflugel. 2006. Prosody of Syn-
tactically Complex Sentences in the Oral Reading of
Young Children. Journal of Educational Psychology,
98(4):839–843.
J. Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin.
2002. Predicting Oral Reading Miscues. In Proc. IC-
SLP.
M. Ostendorf, P.J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University Radio News Corpus.
Technical report, Boston University, March.
Y. Ozuru, M. Rowe, T. O’Reilly, and D.S. McNamara.
2008. Where’s the difficulty in standardized reading
tests: the passage or the question? Behavior Research
Methods, 40(4):1001–1015.
H. Pon-Barry and S.M. Shieber. 2011. Recognizing un-
certainty in speech. CoRR, abs/1103.1898.
T. Rasinski. 2006. Reading fluency instruction: Mov-
ing beyond accuracy, automaticity, and prosody. The
Reading Teacher, 59(7):704–706, April.
M.H. Rasmussen, J. Mostow, Z. Tan, B. Lindberg, and
Y. Li. 2011. Evaluating Tracking Accuracy of an Au-
tomatic Reading Tutor. In Proc. Speech and Language
Technology in Education Workshop.
L. Spear-Swerling. 2006. Childrens Reading Com-
prehension and Oral Reading Fluency in Easy Text.
Reading and Writing, 19(2):199–220.
C.W. Wightman, S. Shattuck-Hufnagel, M. Ostendorf,
and P.J. Price. 1992. Segmental durations in the vicin-
ity of prosodic phrase boundaries. The Journal of the
Acoustical Society ofAmerica, 91(3):1707—-1717.
X.N. Zhang, J. Mostow, and J.E. Beck. 2007. Can a
Computer Listen for Fluctuations in Reading Com-
prehension? Artificial Intelligence in Education,
158:495–502.
</reference>
<page confidence="0.996846">
720
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936021">
<title confidence="0.9770625">Atypical Prosodic Structure as an Indicator of Reading Level and Text Difficulty</title>
<author confidence="0.993426">Julie Medero</author>
<author confidence="0.993426">Mari</author>
<affiliation confidence="0.995381">Electrical Engineering University of</affiliation>
<address confidence="0.99988">Seattle, WA 98195</address>
<abstract confidence="0.99947475">Automatic assessment of reading ability builds on applying speech recognition tools to oral reading, measuring words correct per minute. This work looks at more fine-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study. Experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase, i.e. in less appropriate locations. The results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Ananthakrishnan</author>
<author>S S Narayanan</author>
</authors>
<title>Automatic Prosodic Event Detection Using Acoustic, Lexical, and Syntactic Evidence.</title>
<date>2008</date>
<journal>IEEE Trans. Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2399" citStr="Ananthakrishnan and Narayanan, 2008" startWordPosition="364" endWordPosition="367">a word, but also when they introduce pauses and articulate words more slowly. Pauses and lengthened articulation can be an indicator of uncertainty for a low-level reader, but these phenomena are also used by skilled readers to mark prosodic phrase structure, facilitating comprehension in listeners. Since prosodic phrase boundaries tend to occur in locations that coincide with certain syntactic constituent boundaries, it is possible to automatically predict prosodic phrase boundary locations from part-of-speech labels and syntactic structure with fairly high reliability for read news stories (Ananthakrishnan and Narayanan, 2008). Thus, we hypothesize that we can more effectively leverage word-level articulation and pause information by focusing on words that are less likely to be associated with prosodic phrase boundaries. By comparing average statistics of articulation rate and pausing for words at boundary vs. non-boundary locations, we hope to obtain a measure that could augment reading rate for evaluating reading ability. We also hypothesize that the specific locations of hesitation phenomena (word lengthening and pausing) observed for multiple readers will be indicative of particular points of difficulty in a te</context>
<context position="10096" citStr="Ananthakrishnan and Narayanan (2008)" startWordPosition="1590" endWordPosition="1593">rosodic boundary prediction is aimed at identifying locations where fluent readers are likely to place boundaries (or not), i.e., reliable locations for feature extraction, vs. acceptable locations for text-to-speech synthesis. Because of this goal and because work on prosodic boundary prediction labels varies in its treatment of intermediate phrase boundaries, our results are not directly comparable to prior studies. However, performance is in the range reported in recent studies predicting prosodic breaks from text features only. Treating intermediate phrase boundaries as positive examples, Ananthakrishnan and Narayanan (2008) 1Our approach differs slightly from previous work in the use of a regression (vs. classification) model; this gave a small performance gain. achieve 88% accuracy. Treating them as negative examples, Margolis and Ostendorf (2010) achieve similar results. Both report results on a single heldout test set, while our results are based on 10-fold cross validation. 5 Experiments with Prosodic Context 5.1 Word-level Rate Features We looked at two acoustic cues related to hesitation or uncertainty: pause duration and word lengthening. While pause duration is straightforward to extract (and not typical</context>
</contexts>
<marker>Ananthakrishnan, Narayanan, 2008</marker>
<rawString>S. Ananthakrishnan and S.S. Narayanan. 2008. Automatic Prosodic Event Detection Using Acoustic, Lexical, and Syntactic Evidence. IEEE Trans. Audio, Speech, and Language Processing, 16(1):216–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baer</author>
<author>M Kutner</author>
<author>J Sabatini</author>
<author>S White</author>
</authors>
<title>Basic Reading Skills and the Literacy of Americas Least Literate Adults: Results from the 2003 National Assessment of Adult Literacy (NAAL) Supplemental Studies.</title>
<date>2009</date>
<tech>Technical report, NCES.</tech>
<contexts>
<context position="3684" citStr="Baer et al., 2009" startWordPosition="560" endWordPosition="563">ruction is difficult. Detecting these regions and analyzing the associated lexical and syntactic correlates is potentially useful for automatically characterizing text difficulty. Our study of hesitation phenomena involves empirical analysis of the oral reading data from the Fluency Addition to the National Assessment of Adult 715 Proceedings of NAACL-HLT 2013, pages 715–720, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Literacy (FAN), which collected oral readings from roughly 12,000 adults, reading short (150-200 word) fourth- and eighth grade passages (Baer et al., 2009). The participants in that study were chosen to reflect the demographics of adults in the United States; thus, speakers of varying reading levels and nonnative speakers were included. For our study, we had access to time alignments of automatic transcriptions, but not the original audio files. 2 Related Work For low-level readers, reading rate and fluency are good indicators of reading comprehension (Miller and Schwanenflugel, 2006; Spear-Swerling, 2006). Zhang and colleagues found that features of children’s oral readings, along with their interactions with an automated tutor, could predict a</context>
</contexts>
<marker>Baer, Kutner, Sabatini, White, 2009</marker>
<rawString>J. Baer, M. Kutner, J. Sabatini, and S. White. 2009. Basic Reading Skills and the Literacy of Americas Least Literate Adults: Results from the 2003 National Assessment of Adult Literacy (NAAL) Supplemental Studies. Technical report, NCES.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Balogh</author>
<author>J Bernstein</author>
<author>J Cheng</author>
<author>B Townshend</author>
</authors>
<title>Final Report Ordinates Scoring of FAN NAAL Phase III: Accuracy Analysis.</title>
<date>2005</date>
<tech>Technical report, Ordinate.</tech>
<contexts>
<context position="6535" citStr="Balogh et al., 2005" startWordPosition="1018" endWordPosition="1021">rd-level pause and articulation rate features for predicting reading level when controlled for prosodic context, we use the Basic Reading Skills (BRS) score available for each reader in the FAN data. The BRS score measures an individual’s average reading rate in WCPM. Each participant read three word lists, three pseudo-word lists, one easy text passage, and one harder text passage, and the BRS is the average WCPM over the eight different readings. Specifically, the WCPM for each case is computed automatically using Ordinate’s VersaReader system to transcribe the speech given the target text (Balogh et al., 2005). The system output is then automatically aligned to the target texts using the track-the-reader method of Rasmussen et al. (2011), which defines weights for regressions and skipped words and then identifies a least-cost alignment between the ASR output and a text. Automatic calculation of WCPM has high correlation (.96-1.0) with human judgment of WCPM (Balogh et al., 2012), so it has the advantage of being easy to automate. Word Error Rate (WER) for the the ASR component in Ordinate’s prototype reading tracker (Balogh et al., 2012) may be estimated to be between 6% and 10%. In a sample of 960</context>
</contexts>
<marker>Balogh, Bernstein, Cheng, Townshend, 2005</marker>
<rawString>J. Balogh, J. Bernstein, J. Cheng, and B. Townshend. 2005. Final Report Ordinates Scoring of FAN NAAL Phase III: Accuracy Analysis. Technical report, Ordinate.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Balogh</author>
<author>J Bernstein</author>
<author>J Cheng</author>
<author>A Van Moere</author>
<author>B Townshend</author>
<author>M Suzuki</author>
</authors>
<date>2012</date>
<booktitle>Validation of Automated Scoring of Oral Reading. Educational and Psychological Measurement,</booktitle>
<pages>72--435</pages>
<marker>Balogh, Bernstein, Cheng, Van Moere, Townshend, Suzuki, 2012</marker>
<rawString>J. Balogh, J. Bernstein, J. Cheng, A. Van Moere, B. Townshend, and M. Suzuki. 2012. Validation of Automated Scoring of Oral Reading. Educational and Psychological Measurement, 72:435–452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
<author>J Cheng</author>
<author>M Suzuki</author>
</authors>
<date>2011</date>
<booktitle>Fluency Changes with General Progress in L2 Proficiency. In Proc. Interspeech, number August,</booktitle>
<pages>877--880</pages>
<contexts>
<context position="5641" citStr="Bernstein et al., 2011" startWordPosition="873" endWordPosition="876">or communication reasons, pauses and slow average articulation rates tend to coincide with major phrase boundaries. In our work, we would like to account for prosodic context in using articulation rate to identify difficult words and constructions. Much of the previous work on using automatic speech recognition (ASR) output for reading level or readability analysis has focused on assessing the reading level of children (Downey et al., 2011; Duchateau et al., 2007). Similar success has been seen in predicting fluency scores in oral reading tests for L2 learners of English (Balogh et al., 2012; Bernstein et al., 2011). Project LISTEN has a reading tutor for children that gives real-time feedback, and has used orthographic and phonemic features of individual words to predict the likelihood of real word subsitutions (Mostow et al., 2002). 3 FAN Literacy Scores To examine the utility of word-level pause and articulation rate features for predicting reading level when controlled for prosodic context, we use the Basic Reading Skills (BRS) score available for each reader in the FAN data. The BRS score measures an individual’s average reading rate in WCPM. Each participant read three word lists, three pseudo-word</context>
</contexts>
<marker>Bernstein, Cheng, Suzuki, 2011</marker>
<rawString>J. Bernstein, J. Cheng, and M. Suzuki. 2011. Fluency Changes with General Progress in L2 Proficiency. In Proc. Interspeech, number August, pages 877–880.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Downey</author>
<author>D Rubin</author>
<author>J Cheng</author>
<author>J Bernstein</author>
</authors>
<title>Performance of Automated Scoring for Children’s Oral Reading.</title>
<date>2011</date>
<booktitle>Proc. Workshop on Innovative Use of NLP for Building Educational Applications, (June):46–55,</booktitle>
<contexts>
<context position="1304" citStr="Downey et al., 2011" startWordPosition="194" endWordPosition="197">priate locations. The results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions. 1 Introduction Fluent reading is known to be a good indicator of reading comprehension, especially for early readers (Rasinski, 2006), so oral reading is often used to evaluate a student’s reading level. One method that can be automated with speech recognition technology is the number of words that a student can read correctly of a normed passage, or Words Correct Per Minute (WCPM) (Downey et al., 2011). Since WCPM depends on speaking rate as well as literacy, we are interested in identifying new measures that can be automatically computed for use in combination with WCPM to provide a better assessment of reading level. In particular, we investigate finegrained measures that, if useful in identifying points of difficulty for readers, can lead to new approaches for assessing text difficulty. The WCPM is reduced when a person repeats or incorrectly reads a word, but also when they introduce pauses and articulate words more slowly. Pauses and lengthened articulation can be an indicator of uncer</context>
<context position="5461" citStr="Downey et al., 2011" startWordPosition="843" endWordPosition="846">ined (e.g. word) level. If longer words take longer to read orally, it may be merely a consequence of having more phonemes, and not of additional reading difficulty. Further, for communication reasons, pauses and slow average articulation rates tend to coincide with major phrase boundaries. In our work, we would like to account for prosodic context in using articulation rate to identify difficult words and constructions. Much of the previous work on using automatic speech recognition (ASR) output for reading level or readability analysis has focused on assessing the reading level of children (Downey et al., 2011; Duchateau et al., 2007). Similar success has been seen in predicting fluency scores in oral reading tests for L2 learners of English (Balogh et al., 2012; Bernstein et al., 2011). Project LISTEN has a reading tutor for children that gives real-time feedback, and has used orthographic and phonemic features of individual words to predict the likelihood of real word subsitutions (Mostow et al., 2002). 3 FAN Literacy Scores To examine the utility of word-level pause and articulation rate features for predicting reading level when controlled for prosodic context, we use the Basic Reading Skills (</context>
</contexts>
<marker>Downey, Rubin, Cheng, Bernstein, 2011</marker>
<rawString>R. Downey, D. Rubin, J. Cheng, and J. Bernstein. 2011. Performance of Automated Scoring for Children’s Oral Reading. Proc. Workshop on Innovative Use of NLP for Building Educational Applications, (June):46–55, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchateau</author>
<author>L Cleuren</author>
<author>H Van</author>
<author>P Ghesqui</author>
</authors>
<title>Automatic Assessment of Childrens Reading Level.</title>
<date>2007</date>
<booktitle>Proc. Interspeech,</booktitle>
<pages>1210--1213</pages>
<contexts>
<context position="5486" citStr="Duchateau et al., 2007" startWordPosition="847" endWordPosition="850">l. If longer words take longer to read orally, it may be merely a consequence of having more phonemes, and not of additional reading difficulty. Further, for communication reasons, pauses and slow average articulation rates tend to coincide with major phrase boundaries. In our work, we would like to account for prosodic context in using articulation rate to identify difficult words and constructions. Much of the previous work on using automatic speech recognition (ASR) output for reading level or readability analysis has focused on assessing the reading level of children (Downey et al., 2011; Duchateau et al., 2007). Similar success has been seen in predicting fluency scores in oral reading tests for L2 learners of English (Balogh et al., 2012; Bernstein et al., 2011). Project LISTEN has a reading tutor for children that gives real-time feedback, and has used orthographic and phonemic features of individual words to predict the likelihood of real word subsitutions (Mostow et al., 2002). 3 FAN Literacy Scores To examine the utility of word-level pause and articulation rate features for predicting reading level when controlled for prosodic context, we use the Basic Reading Skills (BRS) score available for </context>
</contexts>
<marker>Duchateau, Cleuren, Van, Ghesqui, 2007</marker>
<rawString>J. Duchateau, L. Cleuren, H. Van, and P. Ghesqui. 2007. Automatic Assessment of Childrens Reading Level. Proc. Interspeech, pages 1210–1213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Keenan</author>
<author>R Betjemann</author>
</authors>
<title>Comprehending the Gray Oral Reading Test Without Reading It: Why Comprehension Tests Should Not Include PassageIndependent Items. Scientific Studies of Reading,</title>
<date>2006</date>
<pages>10--4</pages>
<contexts>
<context position="4610" citStr="Keenan and Betjemann, 2006" startWordPosition="703" endWordPosition="707">lated Work For low-level readers, reading rate and fluency are good indicators of reading comprehension (Miller and Schwanenflugel, 2006; Spear-Swerling, 2006). Zhang and colleagues found that features of children’s oral readings, along with their interactions with an automated tutor, could predict a single student’s comprehension question performance over the course of a document (2007). Using oral readings is appealing because it avoids the difficulty of separating question difficulty from passage difficulty (Ozuru et al., 2008) and of questions that can be answered through world knowledge (Keenan and Betjemann, 2006). WCPM is generally used as a tool for assessing reading level by averaging across one or more passages. It is more noisy when comparing the readability of different texts, especially when the reading level is measured at a fine-grained (e.g. word) level. If longer words take longer to read orally, it may be merely a consequence of having more phonemes, and not of additional reading difficulty. Further, for communication reasons, pauses and slow average articulation rates tend to coincide with major phrase boundaries. In our work, we would like to account for prosodic context in using articula</context>
</contexts>
<marker>Keenan, Betjemann, 2006</marker>
<rawString>J.M. Keenan and R. Betjemann. 2006. Comprehending the Gray Oral Reading Test Without Reading It: Why Comprehension Tests Should Not Include PassageIndependent Items. Scientific Studies of Reading, 10(4):363–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>M Rotaru</author>
<author>G Nicholas</author>
</authors>
<title>Classifying turn-level uncertainty using word-level prosody.</title>
<date>2009</date>
<booktitle>In Proc. Interspeech.</booktitle>
<contexts>
<context position="18733" citStr="Litman et al., 2009" startWordPosition="3005" endWordPosition="3008">es using multiple oral readings. In order to better understand the information provided by the different features, we are conducting eye tracking experiments on these passages, and future work will include an analysis of readers’ gaze during reading of these constructions that have been categorized in terms of their likely prosodic context. In this work, where the original recordings were not available, the study was restricted to duration features. However, other work has suggested that other prosodic cues, particularly pitch and energy features, are useful for detecting speaker uncertainty (Litman et al., 2009; Litman et al., 2012; Pon-Barry and Shieber, 2011). Incorporating these cues may increase the reliability of detecting points of reading difficulty and/or offer complementary information for characterizing text difficulties. Acknowledgments We are grateful to the anonymous reviewers for their feedback, and to our colleagues at Pearson Knowledge Technologies for their insights and data processing assistance. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-0718124 and by the National Science Foundation under Grant No</context>
</contexts>
<marker>Litman, Rotaru, Nicholas, 2009</marker>
<rawString>D. Litman, M. Rotaru, and G. Nicholas. 2009. Classifying turn-level uncertainty using word-level prosody. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>H Friedberg</author>
<author>K Forbes-Riley</author>
</authors>
<title>Prosodic cues to disengagement and uncertainty in physics tutorial dialogues.</title>
<date>2012</date>
<booktitle>In Proc. Interspeech.</booktitle>
<contexts>
<context position="18754" citStr="Litman et al., 2012" startWordPosition="3009" endWordPosition="3012">l readings. In order to better understand the information provided by the different features, we are conducting eye tracking experiments on these passages, and future work will include an analysis of readers’ gaze during reading of these constructions that have been categorized in terms of their likely prosodic context. In this work, where the original recordings were not available, the study was restricted to duration features. However, other work has suggested that other prosodic cues, particularly pitch and energy features, are useful for detecting speaker uncertainty (Litman et al., 2009; Litman et al., 2012; Pon-Barry and Shieber, 2011). Incorporating these cues may increase the reliability of detecting points of reading difficulty and/or offer complementary information for characterizing text difficulties. Acknowledgments We are grateful to the anonymous reviewers for their feedback, and to our colleagues at Pearson Knowledge Technologies for their insights and data processing assistance. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-0718124 and by the National Science Foundation under Grant No. IIS-0916951. Any op</context>
</contexts>
<marker>Litman, Friedberg, Forbes-Riley, 2012</marker>
<rawString>D. Litman, H. Friedberg, and K. Forbes-Riley. 2012. Prosodic cues to disengagement and uncertainty in physics tutorial dialogues. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Margolis</author>
<author>M Ostendorf</author>
<author>K Livescu</author>
</authors>
<title>Crossgenre training for automatic prosody classification.</title>
<date>2010</date>
<booktitle>In Proc. Speech Prosody Conference.</booktitle>
<marker>Margolis, Ostendorf, Livescu, 2010</marker>
<rawString>A. Margolis, M. Ostendorf, and K. Livescu. 2010. Crossgenre training for automatic prosody classification. In Proc. Speech Prosody Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Miller</author>
<author>P J Schwanenflugel</author>
</authors>
<title>Prosody of Syntactically Complex Sentences in the Oral Reading of Young Children.</title>
<date>2006</date>
<journal>Journal of Educational Psychology,</journal>
<volume>98</volume>
<issue>4</issue>
<contexts>
<context position="4119" citStr="Miller and Schwanenflugel, 2006" startWordPosition="629" endWordPosition="632">sociation for Computational Linguistics Literacy (FAN), which collected oral readings from roughly 12,000 adults, reading short (150-200 word) fourth- and eighth grade passages (Baer et al., 2009). The participants in that study were chosen to reflect the demographics of adults in the United States; thus, speakers of varying reading levels and nonnative speakers were included. For our study, we had access to time alignments of automatic transcriptions, but not the original audio files. 2 Related Work For low-level readers, reading rate and fluency are good indicators of reading comprehension (Miller and Schwanenflugel, 2006; Spear-Swerling, 2006). Zhang and colleagues found that features of children’s oral readings, along with their interactions with an automated tutor, could predict a single student’s comprehension question performance over the course of a document (2007). Using oral readings is appealing because it avoids the difficulty of separating question difficulty from passage difficulty (Ozuru et al., 2008) and of questions that can be answered through world knowledge (Keenan and Betjemann, 2006). WCPM is generally used as a tool for assessing reading level by averaging across one or more passages. It i</context>
</contexts>
<marker>Miller, Schwanenflugel, 2006</marker>
<rawString>J. Miller and P.J. Schwanenflugel. 2006. Prosody of Syntactically Complex Sentences in the Oral Reading of Young Children. Journal of Educational Psychology, 98(4):839–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mostow</author>
<author>J Beck</author>
<author>S Winter</author>
<author>S Wang</author>
<author>B Tobin</author>
</authors>
<title>Predicting Oral Reading Miscues. In</title>
<date>2002</date>
<booktitle>Proc. ICSLP.</booktitle>
<contexts>
<context position="5863" citStr="Mostow et al., 2002" startWordPosition="908" endWordPosition="911">ords and constructions. Much of the previous work on using automatic speech recognition (ASR) output for reading level or readability analysis has focused on assessing the reading level of children (Downey et al., 2011; Duchateau et al., 2007). Similar success has been seen in predicting fluency scores in oral reading tests for L2 learners of English (Balogh et al., 2012; Bernstein et al., 2011). Project LISTEN has a reading tutor for children that gives real-time feedback, and has used orthographic and phonemic features of individual words to predict the likelihood of real word subsitutions (Mostow et al., 2002). 3 FAN Literacy Scores To examine the utility of word-level pause and articulation rate features for predicting reading level when controlled for prosodic context, we use the Basic Reading Skills (BRS) score available for each reader in the FAN data. The BRS score measures an individual’s average reading rate in WCPM. Each participant read three word lists, three pseudo-word lists, one easy text passage, and one harder text passage, and the BRS is the average WCPM over the eight different readings. Specifically, the WCPM for each case is computed automatically using Ordinate’s VersaReader sys</context>
</contexts>
<marker>Mostow, Beck, Winter, Wang, Tobin, 2002</marker>
<rawString>J. Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin. 2002. Predicting Oral Reading Miscues. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>P J Price</author>
<author>S Shattuck-Hufnagel</author>
</authors>
<date>1995</date>
<tech>Technical report,</tech>
<institution>The Boston University Radio News Corpus.</institution>
<contexts>
<context position="8343" citStr="Ostendorf et al., 1995" startWordPosition="1320" endWordPosition="1323">-level readers (Balogh et al., 2012). In order to have more reliable time alignments and BRS scores, approximately 15% of the FAN participants were excluded from the current analysis. This 15% were those participants whose BRS score was labeled ”Below Basic” in the NAAL 716 reading scale. Additional participants were eliminated because of missing or incomplete (less than a few seconds) recordings. With these exclusions, the number of speakers in our study was 7587. 4 Prosodic Boundary Prediction We trained a regression tree1 on hand-annotated data from the Boston University Radio News Corpus (Ostendorf et al., 1995) to predict the locations where we expect to see prosodic boundaries. Each word in the Radio News Corpus is labeled with a prosodic boundary score from 0 (clitic, no boundary) to 6 (sentence boundary). For each word, we use features based on parse depth and structure and POS bigrams to predict the prosodic boundary value. For evaluation, the break labels are grouped into: 0-2 (no intonational boundary marker), 3 (intermediate phrase), and 4-6 (intonational phrase boundary). Words with 0-2 breaks are considered non-boundary words; 4-6 are boundary words. We expect that, for fluent readers, leng</context>
</contexts>
<marker>Ostendorf, Price, Shattuck-Hufnagel, 1995</marker>
<rawString>M. Ostendorf, P.J. Price, and S. Shattuck-Hufnagel. 1995. The Boston University Radio News Corpus. Technical report, Boston University, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ozuru</author>
<author>M Rowe</author>
<author>T O’Reilly</author>
<author>D S McNamara</author>
</authors>
<title>Where’s the difficulty in standardized reading tests: the passage or the question?</title>
<date>2008</date>
<journal>Behavior Research Methods,</journal>
<volume>40</volume>
<issue>4</issue>
<marker>Ozuru, Rowe, O’Reilly, McNamara, 2008</marker>
<rawString>Y. Ozuru, M. Rowe, T. O’Reilly, and D.S. McNamara. 2008. Where’s the difficulty in standardized reading tests: the passage or the question? Behavior Research Methods, 40(4):1001–1015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Pon-Barry</author>
<author>S M Shieber</author>
</authors>
<date>2011</date>
<note>Recognizing uncertainty in speech. CoRR, abs/1103.1898.</note>
<contexts>
<context position="18784" citStr="Pon-Barry and Shieber, 2011" startWordPosition="3013" endWordPosition="3016">to better understand the information provided by the different features, we are conducting eye tracking experiments on these passages, and future work will include an analysis of readers’ gaze during reading of these constructions that have been categorized in terms of their likely prosodic context. In this work, where the original recordings were not available, the study was restricted to duration features. However, other work has suggested that other prosodic cues, particularly pitch and energy features, are useful for detecting speaker uncertainty (Litman et al., 2009; Litman et al., 2012; Pon-Barry and Shieber, 2011). Incorporating these cues may increase the reliability of detecting points of reading difficulty and/or offer complementary information for characterizing text difficulties. Acknowledgments We are grateful to the anonymous reviewers for their feedback, and to our colleagues at Pearson Knowledge Technologies for their insights and data processing assistance. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-0718124 and by the National Science Foundation under Grant No. IIS-0916951. Any opinions, findings, and conclusi</context>
</contexts>
<marker>Pon-Barry, Shieber, 2011</marker>
<rawString>H. Pon-Barry and S.M. Shieber. 2011. Recognizing uncertainty in speech. CoRR, abs/1103.1898.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rasinski</author>
</authors>
<title>Reading fluency instruction: Moving beyond accuracy, automaticity, and prosody. The Reading Teacher,</title>
<date>2006</date>
<volume>59</volume>
<issue>7</issue>
<contexts>
<context position="1031" citStr="Rasinski, 2006" startWordPosition="148" endWordPosition="149"> accounts for effects of prosodic context using a large corpus of read speech from a literacy study. Experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase, i.e. in less appropriate locations. The results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions. 1 Introduction Fluent reading is known to be a good indicator of reading comprehension, especially for early readers (Rasinski, 2006), so oral reading is often used to evaluate a student’s reading level. One method that can be automated with speech recognition technology is the number of words that a student can read correctly of a normed passage, or Words Correct Per Minute (WCPM) (Downey et al., 2011). Since WCPM depends on speaking rate as well as literacy, we are interested in identifying new measures that can be automatically computed for use in combination with WCPM to provide a better assessment of reading level. In particular, we investigate finegrained measures that, if useful in identifying points of difficulty fo</context>
</contexts>
<marker>Rasinski, 2006</marker>
<rawString>T. Rasinski. 2006. Reading fluency instruction: Moving beyond accuracy, automaticity, and prosody. The Reading Teacher, 59(7):704–706, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Rasmussen</author>
<author>J Mostow</author>
<author>Z Tan</author>
<author>B Lindberg</author>
<author>Y Li</author>
</authors>
<title>Evaluating Tracking Accuracy of an Automatic Reading Tutor.</title>
<date>2011</date>
<booktitle>In Proc. Speech and Language Technology in Education Workshop.</booktitle>
<contexts>
<context position="6665" citStr="Rasmussen et al. (2011)" startWordPosition="1040" endWordPosition="1044">ic Reading Skills (BRS) score available for each reader in the FAN data. The BRS score measures an individual’s average reading rate in WCPM. Each participant read three word lists, three pseudo-word lists, one easy text passage, and one harder text passage, and the BRS is the average WCPM over the eight different readings. Specifically, the WCPM for each case is computed automatically using Ordinate’s VersaReader system to transcribe the speech given the target text (Balogh et al., 2005). The system output is then automatically aligned to the target texts using the track-the-reader method of Rasmussen et al. (2011), which defines weights for regressions and skipped words and then identifies a least-cost alignment between the ASR output and a text. Automatic calculation of WCPM has high correlation (.96-1.0) with human judgment of WCPM (Balogh et al., 2012), so it has the advantage of being easy to automate. Word Error Rate (WER) for the the ASR component in Ordinate’s prototype reading tracker (Balogh et al., 2012) may be estimated to be between 6% and 10%. In a sample of 960 passage readings, where various sets of two passages were read by each of 480 adults (160 native Spanish speakers, 160 native Eng</context>
</contexts>
<marker>Rasmussen, Mostow, Tan, Lindberg, Li, 2011</marker>
<rawString>M.H. Rasmussen, J. Mostow, Z. Tan, B. Lindberg, and Y. Li. 2011. Evaluating Tracking Accuracy of an Automatic Reading Tutor. In Proc. Speech and Language Technology in Education Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Spear-Swerling</author>
</authors>
<title>Childrens Reading Comprehension and Oral Reading Fluency in Easy Text. Reading and Writing,</title>
<date>2006</date>
<contexts>
<context position="4142" citStr="Spear-Swerling, 2006" startWordPosition="633" endWordPosition="634">istics Literacy (FAN), which collected oral readings from roughly 12,000 adults, reading short (150-200 word) fourth- and eighth grade passages (Baer et al., 2009). The participants in that study were chosen to reflect the demographics of adults in the United States; thus, speakers of varying reading levels and nonnative speakers were included. For our study, we had access to time alignments of automatic transcriptions, but not the original audio files. 2 Related Work For low-level readers, reading rate and fluency are good indicators of reading comprehension (Miller and Schwanenflugel, 2006; Spear-Swerling, 2006). Zhang and colleagues found that features of children’s oral readings, along with their interactions with an automated tutor, could predict a single student’s comprehension question performance over the course of a document (2007). Using oral readings is appealing because it avoids the difficulty of separating question difficulty from passage difficulty (Ozuru et al., 2008) and of questions that can be answered through world knowledge (Keenan and Betjemann, 2006). WCPM is generally used as a tool for assessing reading level by averaging across one or more passages. It is more noisy when compa</context>
</contexts>
<marker>Spear-Swerling, 2006</marker>
<rawString>L. Spear-Swerling. 2006. Childrens Reading Comprehension and Oral Reading Fluency in Easy Text. Reading and Writing, 19(2):199–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Wightman</author>
<author>S Shattuck-Hufnagel</author>
<author>M Ostendorf</author>
<author>P J Price</author>
</authors>
<title>Segmental durations in the vicinity of prosodic phrase boundaries.</title>
<date>1992</date>
<journal>The Journal of the Acoustical Society ofAmerica,</journal>
<pages>91--3</pages>
<contexts>
<context position="11059" citStr="Wightman et al., 1992" startWordPosition="1742" endWordPosition="1745">ld cross validation. 5 Experiments with Prosodic Context 5.1 Word-level Rate Features We looked at two acoustic cues related to hesitation or uncertainty: pause duration and word lengthening. While pause duration is straightforward to extract (and not typically normalized), various methods have been used for word lengthening. We explore two measures of word lengthening: i) the longest normalized vowel, and ii) the average normalized length of word-final phones (the last vowel and all following consonants). Word-final lengthening is known to be a correlate of fluent prosodic phrase boundaries (Wightman et al., 1992), and we hypothesized that the longest normalized vowel might be useful for hesitations though it can also indicate prosodic prominence. For word-level measures of lengthening, it is standard to normalize to account for inherent phoneme durations. We use a z-score: measured duration minus phone mean divided by phone standard deviation. In addition, Wightman et al. (1992) found it useful to account for speaking rate in normalizing phone duration. We adopt the same model, which assumes that phone durations can be characterized by a Gamma distribution and that speaker variability is characterized</context>
</contexts>
<marker>Wightman, Shattuck-Hufnagel, Ostendorf, Price, 1992</marker>
<rawString>C.W. Wightman, S. Shattuck-Hufnagel, M. Ostendorf, and P.J. Price. 1992. Segmental durations in the vicinity of prosodic phrase boundaries. The Journal of the Acoustical Society ofAmerica, 91(3):1707—-1717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X N Zhang</author>
<author>J Mostow</author>
<author>J E Beck</author>
</authors>
<title>Can a Computer Listen for Fluctuations in Reading Comprehension?</title>
<date>2007</date>
<booktitle>Artificial Intelligence in Education,</booktitle>
<pages>158--495</pages>
<marker>Zhang, Mostow, Beck, 2007</marker>
<rawString>X.N. Zhang, J. Mostow, and J.E. Beck. 2007. Can a Computer Listen for Fluctuations in Reading Comprehension? Artificial Intelligence in Education, 158:495–502.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>