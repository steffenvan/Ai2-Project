<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986998">
Multi-document summarization using A* search and discriminative training
</title>
<author confidence="0.994996">
Ahmet Aker Trevor Cohn Robert Gaizauskas
</author>
<affiliation confidence="0.9995225">
Department of Computer Science
University of Sheffield, Sheffield, S1 4DP, UK
</affiliation>
<email confidence="0.995069">
{a.aker, t.cohn, r.gaizauskas}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.997521" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995369933333333">
In this paper we address two key challenges
for extractive multi-document summarization:
the search problem of finding the best scoring
summary and the training problem of learn-
ing the best model parameters. We propose an
A* search algorithm to find the best extractive
summary up to a given length, which is both
optimal and efficient to run. Further, we pro-
pose a discriminative training algorithm which
directly maximises the quality of the best sum-
mary, rather than assuming a sentence-level
decomposition as in earlier work. Our ap-
proach leads to significantly better results than
earlier techniques across a number of evalua-
tion metrics.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931254901961">
Multi-document summarization aims to present
multiple documents in form of a short summary.
This short summary can be used as a replacement
for the original documents to reduce, for instance,
the time a reader would spend if she were to read
the original documents. Following dominant trends
in summarization research (Mani, 2001), we focus
solely on extractive summarization which simplifies
the summarization task to the problem of identify-
ing a subset of units from the document collection
(here sentences) which are concatenated to form the
summary.
Most multi-document summarization systems de-
fine a model which assigns a score to a candidate
summary based on the features of the sentences in-
cluded in the summary. The research challenges are
then twofold: 1) the search problem of finding the
best scoring summary for a given document set, and
2) the training problem of learning the model pa-
rameters to best describe a training set consisting of
pairs of document sets with model or reference sum-
maries – typically human authored extractive or ab-
stractive summaries.
Search is typically performed by a greedy al-
gorithm which selects each sentence in decreasing
order of model score until the desired summary
length is reached (see, e.g., Saggion (2005)) or us-
ing heuristic strategies based on position in docu-
ment or lexical clues (Edmundson, 1969; Brandow
et al., 1995; Hearst, 1997; Ouyang et al., 2010).1
We show in this paper that the search problem can
be solved optimally and efficiently using A* search
(Russell et al., 1995). Assuming the model only uses
features local to each sentence in the summary, our
algorithm finds the best scoring extractive summary
up to a given length in words.
Framing summarization as search suggests that
many of the popular training techniques are max-
imising the wrong objective. These approaches train
a classifier, regression or ranking model to distin-
guish between good and bad sentences under an
evaluation metric, e.g., ROUGE (Lin, 2004). The
model is then used during search to find a summary
composed of high scoring (‘good’) sentences (see
for a review Ouyang et al. (2010)). However, there
is a disconnect between the model used for training
and the model used for prediction. In this paper we
present a solution to this disconnect in the form of
a training algorithm that optimises the full predic-
tion model directly with the search algorithm intact.
The training algorithm learns parameters such that
</bodyText>
<footnote confidence="0.79443975">
1Genetic algorithms have also been devised for solving the
search problem (see, e.g., Riedhammer et al. (2008)), however
these approaches do not guarantee optimality, nor are they effi-
cient enough to be practicable for large datasets.
</footnote>
<page confidence="0.92997">
482
</page>
<note confidence="0.818553">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 482–491,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999390571428571">
the best scoring whole summary under the model
has a high score under the evaluation metric. We
demonstrate that this leads to significantly better test
performance than a competitive baseline, to the tune
of 3% absolute increase for ROUGE-1, -2 and -SU4.
The paper is structured as follows. Section 2
presents the summarization model. Next in sec-
tion 3 we present an A* search algorithm for finding
the best scoring (argmax) summary under the model
with a constraint on the maximum summary length.
We show that this algorithm performs search effi-
ciently, even for very large document sets composed
of many sentences. The second contribution of the
paper is a new training method which directly opti-
mises the summarization system, and is presented in
section 4. This uses the minimum error-rate training
(MERT) technique from machine translation (Och,
2003) to optimise the summariser’s output to an ar-
bitrary evaluation metric. Section 5 describes our
experimental setup and section 6 the results. Finally
we conclude in section 7.
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="method">
2 Summarization Model
</sectionHeader>
<bodyText confidence="0.999595571428571">
Extractive multi-document summarization aims to
find the most important sentences from a set of doc-
uments, which are then collated and presented to
the user in form of a short summary. Following
the predominant approach to data-driven summari-
sation, we define a linear model which scores sum-
maries as the weighted sum of their features,
</bodyText>
<equation confidence="0.99603">
s(y|x) = -b(x, y) · A , (1)
</equation>
<bodyText confidence="0.999810666666667">
where x is the document set, composed of k sen-
tences, y ⊆ {1... k} are the set of selected sen-
tence indices, 4b(·, ·) is a feature function which re-
turns a vector of features for the candidate summary
and A are the model parameters. We further assume
that the features decompose with the sentences in
the summary, 4b(x, y) = Eicy O(xi), and there-
fore the scoring function also decomposes along the
same lines,
</bodyText>
<equation confidence="0.924869">
s(y|x) = 1: O(xi) · A . (2)
iEy
</equation>
<bodyText confidence="0.9999504">
While this assumption greatly simplifies inference, it
does constrain the representative power of the model
by disallowing global features, e.g., those which
measure duplication in the summary.2 Under this
model, the search problem is to solve
</bodyText>
<equation confidence="0.992245">
y� = arg max s(y|x) , (3)
y
</equation>
<bodyText confidence="0.999985428571429">
for which we develop a best-first algorithm using A*
search, as described in section 3. The training chal-
lenge is to find the parameters, A, to best model the
training set. This is achieved by finding A such that
y� is similar to the gold standard summary accord-
ing to an automatic evaluation metric, as described
in section 4.
</bodyText>
<sectionHeader confidence="0.994351" genericHeader="method">
3 A* Search
</sectionHeader>
<bodyText confidence="0.970625514285715">
The prediction problem is to find the best scoring
extractive summary (see Equation 3) up to a given
length, L. At first glance, this appears to be a sim-
ple problem that might be solved efficiently with a
greedy algorithm, say by taking the sentences in or-
der of decreasing score and stopping just before the
summary exceeds the length threshold. However,
the greedy algorithm cannot be guaranteed to find
the best summary; to do so requires arbitrary back-
tracking to revise previous incorrect decisions.
The problem of constructing the summary can be
considered a search problem in which we start with
an empty summary and incrementally enlarge the
summary by concatenating a sentence from our doc-
ument set. The search graph starts with an empty
summary (the starting state) and each outgoing edge
adds a sentence to produce a subsequent state, and
is assigned a score under the model. A goal state is
any state with no more words than the given thresh-
old. The summarisation problem is then equivalent
to finding the best scoring path (summed over the
edge scores) between the start state and a goal state.
The novel insight in our work is to use A* search
(Russell et al., 1995) to solve the prediction prob-
lem. A* is a best-first search algorithm which can
efficiently find the best scoring path or the n-best
paths (unlike the greedy algorithm which is not op-
timal, and the backtracking variant which is not ef-
ficient). The search procedure requires a scoring
function for each state, here s(y|x) from (2), and
2Our approach could be adapted to support global features,
which would require changes to the heuristic for A* search to
bound the score obtainable from the global features. This may
incur an additional computational cost over a purely local fea-
ture model and perhaps also necessitate using beam search.
</bodyText>
<page confidence="0.998928">
483
</page>
<bodyText confidence="0.90619808">
a heuristic function which estimates the additional
score to get from a given state to a goal state. For
the search to be optimal – guaranteed to find the best
scoring path as the first solution – the heuristic must
be admissible, meaning that it bounds from above
the score for reaching a goal state. We present three
different admissible heuristics later in this section,
which bound the score with differing tightness and
consequently different search cost.
Algorithm 1 presents A* search for our extractive
summarisation model. Given a set of sentences to
summary, a scoring and a heuristic function, it finds
the best scoring summary. This is achieved by build-
ing the search graph incrementally, and storing each
frontier state in a priority queue (line 1) which is
sorted by the sum of the state’s score and its heuris-
tic. These states are popped off the queue (line 3)
and expanded by adding a sentence, which is then
added to the schedule (lines 8–14). We designate
special finishing states using a boolean variable (the
last entry in the tuple in lines 1, 7 and 12). Fin-
ishing states (with value T) denote ceasing to ex-
pand the summary, and consequently their scores
do not include the heuristic component. When-
ever one of these states is popped in line 2, we
know that it outscores all competing hypotheses and
therefore represents the optimal summary (because
the heuristic is guaranteed to never underestimate
the cost to a goal state from an unfinished state).3
Note that in algorithm 1 we create the summary
by building a list of sentence indices in sorted or-
der to avoid spurious ambiguity which would un-
necessarily expand the search space. The function
length(y, x) = En∈y length(xn) returns the length
of sentences specified.
We now return to the problem of defining the
heuristic function, h(y; x, l) which provides an up-
per bound on the additional score achievable in
reaching a goal state from state y. We present three
different variants of increasing fidelity, that is, that
bound the cost to a goal state more tightly. Algo-
rithm 2 is the simplest, which simply finds the max-
imum score per word from the set of unused sen-
3To improve the efficiency of Algorithm 1 we make a small
modification to avoid expanding every possible edge in step 8,
of which there are O(k) options. Instead we expand a small
number (here, 3) at a time and defer the remaining items until
later by inserting a special node into the schedule. These special
nodes are represented using a third ‘to-be-continued’ state into
the done flag.
</bodyText>
<figure confidence="0.246967142857143">
Algorithm 1 A* search for extractive summarization.
Require: set of sentences, x = xl, ... , xk
Require: scoring function s(·)
Require: heuristic function h(·)
Require: summary length limit L
1: schedule = [(0, ∅, F)] {priority queue of triples}
{(A* score, sentence indices, done flag)}
</figure>
<listItem confidence="0.8246364">
2: while schedule =6 [] do
3: v, y, f ← pop(schedule)
4: iff=Tthen
5: return y {success}
6: else
7: push(schedule, (s(y|x), y, T))
8: for y ← [max(y) + 1, k] do
9: y&apos; ← y ∪ y
10: if length(y&apos;, x) ≤ L then
11: v&apos; ← s(y&apos;|x) + h(y&apos;; x, l)
12: push(schedule, (v&apos;, y&apos;, F))
13: end if
14: end for
15: end if
16: end while
</listItem>
<bodyText confidence="0.92261">
tences and then extrapolates this out over the re-
maining words available to the length threshold. In
the algorithm, we use the shorthand sn = O(xn) · A
for sentence n’s score, ln = length(xn) for its length
and ly = En∈y ln for the total length of the current
state (unfinished summary).
Algorithm 2 Uniform heuristic, h1(y; x, L)
</bodyText>
<listItem confidence="0.62829875">
Require: x sorted in order of score/length
1: n ← max(y) + 1
2: return (L − ly) max ( Sn , 0)
n
</listItem>
<bodyText confidence="0.99938995">
The h1 heuristic is overly simple in that it assumes
we can ‘reuse’ a high scoring short sentence many
times despite this being disallowed by the model.
For this reason we develop an improved bound, h2,
in Algorithm 3. This incrementally adds each sen-
tence in order of its score-per-word until the length
limit is reached. If the limit is to be exceeded,
the heuristic scales down the final sentence’s score
based on the fraction of words than can be used to
reach the limit.
The fractional usage of the final sentence in h2
could be considered overly optimistic, especially
when the state has length just shy of the limit L. If
the next best ranked sentence is a long one, then it
will be used in the heuristic to over-estimate of the
state. This is complicated to correct, and doing so
exactly would require full backtracking which is in-
tractable and would obviate the entire point of using
A* search. Instead we use a subtle modification in
h3 (Alg. 4) which is equivalent to h2 except in the
</bodyText>
<page confidence="0.995653">
484
</page>
<figure confidence="0.977947882352941">
Algorithm 3 Aggregated heuristic, h2(y; x, L)
Require: x sorted in order of score/length
1: v i— 0
2: l0 i— ly
3: for n E [max(y) + 1, k] do
4: if sn &lt; 0 then
5: return v
6: end if
7: if l0 + ln &lt; L then
8: l0 i— l0 + ln
9: v i— v + sn
10: else
11: return v + l�
L−l, sn
12: end if
13: end for
14: return v
</figure>
<bodyText confidence="0.9991805">
instance where the next best score/word sentence is
too long, where it skips over these sentences until
it finds the best scoring sentence that does fit. This
helps to address the overestimate of h2 and should
therefore lead to a smaller search graph and faster
runtime due to its early elimination of dead-ends.
</bodyText>
<figure confidence="0.326885125">
Algorithm 4 Agg.+final heuristic, h3(y; x, L)
Require: x sorted in order of score/length
1: n i— max(y) + 1
2: if n &lt; k n sn &gt; 0 then
3: if ly + ln &lt; L then
4: return h2(y&apos;, x, L)
5: else
6: form E [n + 1, k] do
7: if ly + lm &lt; L then
L−ly
8: return sm lm
9: end if
10: end for
11: end if
12: end if
13: return 0
</figure>
<bodyText confidence="0.9998455625">
The search process is illustrated in figure 1. When
a node is visited in the search, if it satisfied the
length constraint then the all its child nodes are
added to the schedule. These nodes are scored with
the score for the summary thus far plus a heuristic
term. For example, the value of 4+1.5=5.5 for the
{1} node arises from a score of 4 plus a heuristic of
(7−5)· 43 = 1.5, reflecting the additional score that
would arise if it were to use half of the next sentence
to finish the summary. Note that in finding the best
two summaries the search process did not need to
instantiate the full search graph.
To test the efficacy of A* search with each of the
different heuristic functions, we now present empir-
ical runtime results. We used the training data as
described in Section 5.2 and for each document set
</bodyText>
<figureCaption confidence="0.983383">
Figure 1: Example of the A* search graph created to find
</figureCaption>
<bodyText confidence="0.947987136363636">
the two top scoring summaries of length &lt; 7 when sum-
marising four sentences with scores of 4, 3, 2 and 1 re-
spectively and lengths of 5, 4, 3 and 1 respectively. The
hl heuristic was used and the score and heuristic scores
are shown separately for clarity. Bold nodes were visited
while dashed nodes were visited but found to exceed the
length constraint.
generated the 100-best summaries with word limit
L = 200. Figure 2 shows the number of nodes
and edges visited by A* search, reflecting the space
and time cost of the algorithm, as a function of the
number of sentences in the document set being sum-
marised. All three heuristics shown an empirical
increase in complexity that is roughly linear in the
document size, although there are some notable out-
liers, particularly for the uniform heuristic. Surpris-
ingly the aggregated heuristic, h2, is not consider-
ably more efficient than the uniform heuristic h1,
despite bounding the cost more precisely. However,
the aggregated+final heuristic, h3, consistently out-
performs the other two methods. For this reason we
have used h3 in all subsequent experimentation.
</bodyText>
<sectionHeader confidence="0.993453" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.930829272727273">
We frame the training problem as one of finding
model parameters, A, such that the predicted out-
put, y� closely matches the gold standard, r.4 The
quality of the match is measured using an automatic
evaluation metric. We adopt the standard machine
learning terminology of loss functions, which mea-
sure the degree of error in the prediction, A(y, r).
In our case the accuracy is measured by the ROUGE
4The gold standard is typically an abstractive summary, and
as such it is usually impossible for an extractive summarizer to
match it exactly.
</bodyText>
<figure confidence="0.982835344827586">
(7+0,{1,2},F)
+2
+3 (6+0,{1,3},F)
(4+1.5,{1},F)
+4
(5+0,{1,4},F)
finish
(5,{1,4},T)
+1
(3+2,{2},F)
+3
(5+0,{2,3},F)
+4
(6+0,{2,3,4},F)
+2
+4
finish
start
+3
+4
(2+2,{3},F)
(4+0,{2,4},F)
(5,{2,3},T)
fini
(1+0,{4},F)
(0,{},T)
485
5 10 20 50 100 200 500 1000 2000
sentences in document set
</figure>
<figureCaption confidence="0.855451444444445">
Figure 2: Efficiency of A* search search is roughly linear
in the number of sentences in the document set. The y
axis measures the search graph size in terms of the num-
ber of edges in the schedule and the number of nodes
visited. Measured with the final parameters after training
to optimise ROUGE-2 with the three different heuristics
and expanding five nodes in each step.
score, R, and the loss is simply 1 - R. The training
problem is to solve
</figureCaption>
<figure confidence="0.5584235">
A = arg min A(�y, r) , (4)
a
</figure>
<bodyText confidence="0.9996272">
where with a slight abuse of notation, y� and r are
taken to range over the corpus of many document-
sets and summaries.
To optimise the weights we use the minimum er-
ror rate training (MERT) technique (Och, 2003), as
used for training statistical machine translation sys-
tems. This approach is a first order optimization
method using Powell search to find the parameters
which minimise the loss on the training data. MERT
requires n-best lists which it uses to approximate
the full space of possible outcomes. We use the
A* search algorithm to construct these n-best lists,5
and use MERT to optimise the ROUGE score on the
training set for the R-1, R-2 and R-SU4 variants of
the metric.
</bodyText>
<footnote confidence="0.653241">
5We used n = 100 in our experiments.
</footnote>
<sectionHeader confidence="0.994972" genericHeader="method">
5 Experimental settings
</sectionHeader>
<bodyText confidence="0.999880333333333">
In this section we describe the features for which we
learn weights. We also describe the input data used
in training and testing.
</bodyText>
<subsectionHeader confidence="0.998294">
5.1 Summarization system
</subsectionHeader>
<bodyText confidence="0.989408">
The summarizer we use is an extractive, query-based
multi-document summarization system. It is given
two inputs: a query (place name) associated with an
image and a set of documents. The summarizer uses
the following features, as reported in previous work
(Edmundson, 1969; Brandow et al., 1995; Radev et
al., 2001; Conroy et al., 2005; Aker and Gaizauskas,
2009; Aker and Gaizauskas, 2010a):
</bodyText>
<listItem confidence="0.971742333333333">
• querySimilarity: Sentence similarity to the
query (cosine similarity over the vector repre-
sentation of the sentence and the query).
• centroidSimilarity: Sentence similarity to the
centroid. The centroid is composed of the 100
most frequently occurring non stop words in
the document collection (cosine similarity over
the vector representation of the sentence and
the centroid). For each word/term in the vec-
tor we store a value which is the product of
the term frequency in the document and the in-
verse document frequency, a measurement of
the term’s distribution over the set of docu-
ments (Salton and Buckley, 1988).
• sentencePosition: Position of the sentence
within its document. The first sentence in the
document gets the score 1 and the last one gets
1 where n is the number of sentences in the
</listItem>
<figure confidence="0.998471220779221">
●
●
●
●
●
●
●
●
● ● ● ●
●
●
●
● ● ● ●
● ●
● ●
●
●
● ● ●
● ● ● ●
●
●
● ●
●
● ●● ●
● ● ● ● ● ●
●
● ● ● ●
●
● ● ● ● ●
● ●
● ● ● ● ●
● ● ● ●
● ● ●
●●
● ●●
● ● ● ●
● ● ● ●
●
● ●
●
●
●
●
●
●
● ●
●
●
● ● ● ●
●
●
● ●
● ● ●
●
● ● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● uniform
aggregated
aggregated+final
●
●
1e+02 1e+03 1e+04 1e+05 1e+06
total edges and nodes
n
document.
</figure>
<listItem confidence="0.9539425">
• inFirst5: Binary feature indicating whether the
sentence occurs is one of the first 5 sentences
of the document.
• isStarter: A sentence gets a binary score if it
</listItem>
<bodyText confidence="0.966200818181818">
starts with the query term (e.g. Westminster
Abbey, The Westminster Abbey, The Westmin-
ster or The Abbey) or with the object type, e.g.
The church. We also allow gaps (up to four
words) between the and the query/object type
to capture cases such as The most magnificent
abbey, etc.
• LMProb: The probability of the sentence un-
der a unigram language model. We trained
a separate language model on Wikipedia arti-
cles about locations for each object type, e.g.,
</bodyText>
<page confidence="0.998177">
486
</page>
<bodyText confidence="0.9991388">
church, bridge, etc. When we generate a sum-
mary about a location of type church, for in-
stance, then we apply the church language
model on the related input documents related
to the location.6
</bodyText>
<listItem confidence="0.959231">
• sentenceCount: Each sentence gets assigned a
value of 1. This feature is used to learn whether
summaries with many sentences are better than
summaries with few sentences or vice versa.
• wordCount: Number of words in the summary,
to decide whether the model should favor long
summaries or short ones.
</listItem>
<subsectionHeader confidence="0.991859">
5.2 Data
</subsectionHeader>
<bodyText confidence="0.999920896551724">
For training and testing we use the freely avail-
able image description corpus described in Aker and
Gaizauskas (2010b). The corpus is based around
289 images of static located objects (e.g Eiffel
Tower, Mont Blanc) each with a manually assigned
place name and object type category (e.g. church,
mountain). For each place name there are up to
four model summaries that were created manually
after reading existing image descriptions taken from
the VirtualTourist travel community web-site. Each
summary contains a minimum of 190 and a maxi-
mum of 210 words. We divide this set of 289 place
names into training and testing sets. Both sets are
described in the following subsections.
Training We use 184 place names from the 289
set for training feature weights. For each train-
ing place name we gather all descriptions associ-
ated with it from VirtualTourist. We compute for
each sentence in each description a ROUGE score
by comparing the sentence to those included in the
model summaries for that particular place name and
retaining the highest score. Table 1 gives some de-
tails about this training data.
We use ROUGE as a metric to maximize be-
cause it is also used in DUC7 and TAC.8 How-
ever, it should be noted that any automatic metric
could be used instead of ROUGE. In particular we
use ROUGE 1 (R-1), ROUGE 2 (R-2) and ROUGE
SU4 (R-SU4). R-1 and R-2 compute the number
</bodyText>
<footnote confidence="0.9910802">
6For our training and testing sets we manually assigned each
location to its corresponding object type (Aker and Gaizauskas,
2009).
7http://duc.nist.gov/
8http://www.nist.gov/tac/
</footnote>
<table confidence="0.991459333333333">
Max Min Avg
Sentences/place 1724 3 260
Words/sentence 37 3 17
</table>
<tableCaption confidence="0.68055075">
Table 1: The training input data contains 184 place
names with 42333 sentences in total. The numbers in
the columns give detail about the number of sentences
for each place and the lengths of the sentences.
</tableCaption>
<table confidence="0.9996872">
Max Min Avg
Documents/place 20 5 12
Sentences/place 1716 15 132
Sentences/document 275 1 10
Words/sentence 211 1 20
</table>
<tableCaption confidence="0.994901">
Table 2: In domain test data. The numbers in the columns
</tableCaption>
<bodyText confidence="0.984613454545454">
give detail about the number of documents (descriptions)
for each place, number of sentences for each place and
document (description) and the lengths of the sentences.
of uni-gram and bi-gram overlaps, respectively, be-
tween the automatic and model summaries. R-SU4
allows bi-grams to be composed of non-contiguous
words, with a maximum of four words between the
bi-grams.
Testing For testing purposes we use the rest of
the place names (105) from the 289 place name
set. For each place name we use a set of input
documents, generate a summary from these docu-
ments using our summarizer and compare the results
against model summaries of that place name using
ROUGE. We experimented with two different input
document types: out of domain and in domain.
The in domain documents are the VirtualTourist
original image descriptions from which the model
summaries were derived. As with the training set
we take all place name descriptions for a particular
place and use them as input documents to our sum-
marizer. Table 2 summarizes these input documents.
The out of domain documents are retrieved from
the web. Compared to the in domain documents
these documents should more challenging to sum-
marize because they will contain different kinds
of documents to those seen in training. For each
place name we retrieved the top ten related web-
documents using the Yahoo! search engine with the
place name as a query. The text from these docu-
ments is extracted using an HTML parser and passed
to the summarizer. Table 3 gives an overview of this
data.
</bodyText>
<page confidence="0.995138">
487
</page>
<table confidence="0.99710075">
Max Min Avg
Sentences/place 1773 55 328
Sentence/document 874 1 32
Words/sentence 236 1 21
</table>
<tableCaption confidence="0.986037333333333">
Table 3: Out of domain test data. The numbers in the
columns give detail about the number of sentences for
each place and document and the lengths of the sentences.
</tableCaption>
<sectionHeader confidence="0.999358" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999906">
To evaluate our approach we used two different as-
sessment methods: ROUGE (Lin, 2004) and manual
readability. In the following we present the results
of each assessment.
</bodyText>
<subsectionHeader confidence="0.997949">
6.1 Automatic Evaluation using ROUGE
</subsectionHeader>
<bodyText confidence="0.999998129032258">
We report results for training and testing. In
both training and testing we distinguish between
three different summaries: wordLimit, sentence-
Limit and regression. WordLimit and sentenceLimit
summaries are the ones generated using the model
trained by MERT. As described in section 4 we
trained the summariser using the A* search decoder
to maximise the ROUGE score of the best scoring
summaries. We used the heuristic function h3 in
A* search because it is the best performing heuris-
tic, and 100-best lists. To experiment with differ-
ent summary length conditions we differentiate be-
tween summaries with a word limit (wordLimit, set
to 200 words) and summaries containing N number
of sentences (sentenceLimit) as stop condition in A*
search. We set N so that in both wordLimit and sen-
tenceLimit summaries we obtain more or less the
same number of words (because our training data
contains on average 17 words for each word we set
N to 12, 12*17=194). However, this is only the case
in the training. In the testing for both wordLimit and
sentenceLimit we generate summaries with the same
word limit constraint which allows us to have a fair
comparison between the ROUGE recall scores.
The regression summaries are our baseline. In
these summaries the sentences are ranked based on
the weighted features produced by Support Vec-
tor Regression (SVR).9 Ouyang et al. (2010) use
multi-document summarization and linear regres-
sion methods to rank sentences in the documents.
As regression model they used SVR and showed
</bodyText>
<footnote confidence="0.91187">
9We use the term regression to refer to SVR.
</footnote>
<table confidence="0.999882">
Type metric R-1 R-2 R-SU4
R-1 0.5792 0.3176 0.3580
wordLimit R-2 0.5656 0.3208 0.3510
R-SU4 0.5688 0.3197 0.3585
R-1 0.5915 0.3507 0.3881
sentenceLimit R-2 0.5783 0.3601 0.3890
R-SU4 0.5870 0.3546 0.3929
R-1 0.4993 0.1946 0.2448
regression R-2 0.4833 0.1949 0.2413
R-SU4 0.5009 0.2031 0.2562
</table>
<tableCaption confidence="0.999707">
Table 4: ROUGE scores obtained on the training data.
</tableCaption>
<bodyText confidence="0.999363763157895">
that it out-performed classification and Learning To
Rank methods on the DUC 2005 to 2007 data. For
comparison purpose we use SVR as a baseline sys-
tem for learning feature weights. It should be noted
that these weights are learned based on single sen-
tences. However, to have a fair comparison between
all our summary types we use these weights to gen-
erate summaries using the A* search with the word
limit as constraint. We do this for reporting both for
training and testing results.
The results for training are shown in Table 4. The
table shows ROUGE recall numbers obtained by
comparing model summaries against automatically
generated summaries on the training data. Because
in training we used three different metrics (R-1, R-2,
R-SU4) to train weights we report results for each of
these three different ROUGE metrics.
In Table 4 we can see that the scores for wordLimit
and sentenceLimit type summaries are always at
maximum on the metric they were trained on (this
can be observed by following the main diagonal of
the result matrix). This confirms that MERT is max-
imizing the metric for which it was trained. How-
ever, this is not the case for regression results. The
scores obtained with R-SU4 metric trained weights
achieve higher scores on R-1 and R-2 compared to
the scores obtained using weights trained on those
metrics. This is most likely due to SVR being
trained on sentences rather than over entire sum-
maries, and thereby not adequately optimising the
metric used for evaluation.
The results for testing are shown in Tables 5 and
6. As with the training setting we report ROUGE re-
call scores. We use the testing data described in sec-
tion 5.2 for this setting. However, because we have
two different input document sets we report sepa-
rate results for each of these (Table 5 shows result
for in domain data and Table 6 shows result for out
</bodyText>
<page confidence="0.995934">
488
</page>
<table confidence="0.9998408">
Type metric R-1 R-2 R-SU4
R-1 0.3733 0.0842 0.1399
wordLimit R-2 0.3731 0.0842 0.1402
R-SU4 0.3627 0.0794 0.1340
R-1 0.3664 0.0774 0.1321
sentenceLimit R-2 0.3559 0.0717 0.1251
R-SU4 0.3629 0.0778 0.1312
R-1 0.3431 0.0669 0.1229
regression R-2 0.2934 0.0560 0.1043
R-SU4 0.3417 0.0668 0.1226
</table>
<tableCaption confidence="0.995138">
Table 5: ROUGE scores obtained on the testing data. The
automated summaries are generated using the in domain
input documents.
</tableCaption>
<table confidence="0.999717">
Type metric R-1 R-2 R-SU4
R-1 0.3758 0.0882 0.1421
wordLimit R-2 0.3755 0.0895 0.1423
R-SU4 0.369 0.0812 0.137
R-1 0.3541 0.0693 0.1226
sentenceLimit R-2 0.3426 0.0638 0.1157
R-SU4 0.3573 0.073 0.1251
R-1 0.3392 0.0611 0.1179
regression R-2 0.3422 0.0606 0.1164
R-SU4 0.3413 0.0606 0.1176
</table>
<tableCaption confidence="0.995491">
Table 6: ROUGE scores obtained on the testing data. The
automated summaries are generated using the out of do-
main input documents.
</tableCaption>
<bodyText confidence="0.988692659574468">
of domain data). Again as with the training setting
we report results for the different metrics (R-1, R-2,
R-SU4) separately.
From Table 5 we can see that the wordLimit sum-
maries score highest compared to the other two types
of summaries. This is different from the train-
ing results where sentenceLimit summary type sum-
maries are the top scoring ones. As mentioned ear-
lier the sentenceLimit summaries contain exactly 12
sentences, where on average each sentence in the
training data has 17 words. We picked 12 sen-
tences to achieve roughly the same word limit con-
straint (12 x 17 = 204) so they can be compared
to the wordLimit and regression type summaries.
However, these sentenceLimit summaries have an
average of 221 words, which explains the higher
ROUGE recall scores seen in training compared to
testing (where a 200 word limit was imposed).
The wordLimit summaries are significantly better
than the scores from the other summary types ir-
respective of the evaluation metric.10 It should be
10Significance is reported at level P &lt; 0.001. We used
Wilcoxson signed ranked test to perform significance.
noted that these summaries are the only ones where
the training and testing had the same condition in
A* search concerning the summary word limit con-
straint. The scores in sentenceLimit type summaries
are significantly lower than wordLimit summaries,
despite using MERT to learn the weights. This
shows that training the true model is critical for
getting good accuracy. The regression type sum-
maries achieved the worst ROUGE metric scores.
The weights used to generate these summaries were
trained on single sentences using SVR. These results
indicate that if the goal is to generate high scoring
summaries under a length limit in testing, then the
same constraint should also be used in training.
From Table 5 and 6 we can see that the summaries
obtained from VirtualTourist captions (in domain
data) score roughly the same as the summaries gen-
erated using web-documents (out of domain data) as
input. A possible explanation is that in many cases
the VirtualTourist original captions contain text from
Wikipedia articles, which are also returned as results
from the web search. Therefore the web-document
sets included similar content to the VirtualTourist
captions.
</bodyText>
<subsectionHeader confidence="0.998969">
6.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.999958086956522">
We also evaluated our summaries using a readabil-
ity assessment as in DUC and TAC. DUC and TAC
manually assess the quality of automatically gener-
ated summaries by asking human subjects to score
each summary using five criteria – grammaticality,
redundancy, clarity, focus and coherence criteria.
Each criterion is scored on a five point scale with
high scores indicating a better result (Dang, 2005).
For this evaluation we used the best scoring sum-
maries from the wordLimit summary type (R-1, R-2
and R-SU4) generated using web-documents (out of
domain documents) as input. We also evaluate the
regression summary types generated using the same
input documents to investigate the correlation be-
tween high and low ROUGE metric scores to man-
ual evaluation ones. From the regression summary
type we only use summaries under the R2 and RSU4
trained models.
In total we evaluated five different summary types
(three from wordLimit and two from regression).
For each type we randomly selected 30 place names
and asked three people to assess the summaries for
these place names. Each person was shown all 150
</bodyText>
<page confidence="0.99666">
489
</page>
<table confidence="0.999822">
Criterion wordLimit regression
R1 R2 RSU4 R2 RSU4
clarity 4.03 3.92 3.99 3.00 2.92
coherence 3.31 3.06 2.99 2.12 1.88
focus 3.79 3.56 3.54 2.44 2.29
grammaticality 4.21 4.13 4.13 3.93 3.87
redundancy 4.19 4.33 4.41 4.47 4.44
</table>
<tableCaption confidence="0.85032">
Table 7: Manual evaluation results for the wordLimit (R1,
R2, RSU4) and regression (R2, RSU4) summary types.
The numbers in the columns are the average scores.
</tableCaption>
<bodyText confidence="0.999407222222222">
summaries (30 from each summary type) in a ran-
dom way and was asked to assess them according to
the DUC and TAC manual assessment scheme. The
results are shown in Table 7.11
From Table 7 we can see that overall the
wordLimit type summaries perform better than the
regression ones. For each metric in regression sum-
mary types (R-2 and R-SU4) we compute the sig-
nificance of the difference with the same metrics
in wordLimit summary types.12 The results for the
clarity, coherence and focus criteria in wordLimit
summaries are significantly better than in regression
ones (p&lt;0.001) irrespective of the training metric.
These results concur with the automatic evaluation
results as described in section 6.1. However, this
is not the case for the grammaticality and redun-
dancy criteria. Although in regression type sum-
maries the scores for the grammaticality criterion
are lower than those in wordLimit summaries the
difference is not significant. Furthermore, we can
see that the redundancy scores for regression sum-
maries are slightly higher than those for wordLimit
summaries.
One reason for these differences might be the
way we trained feature weights for wordLimit and
regression summaries. As mentioned above, fea-
ture weights for wordLimit summaries are trained
using summaries with a specific word limit con-
straint, whereas the weights for the regression sum-
maries are learned using single sentences. Maxi-
mizing the ROUGE metrics using “final or output
11We computed the agreement between the users using intra
class correlation with Cronbach’s Alpha where the correlation
coefficient ranges between 0 and 1. Numbers close to 1 indicate
high correlation and numbers close to 0 indicate low correlation.
For the clarity criterion the assessors’ correlation coefficient is
0.547, for coherence 0.687, for focus 0.688, for grammaticality
0.232 and for redundancy 0.453.
12We compute significance test for the manual evaluation re-
sults using X square.
like summaries” will lead to a higher content agree-
ment between the training and the model summaries
whereas this is not guaranteed with single sentences.
With single sentences we have only a guarantee for
high content overlap between single training and
model sentences. However, when these sentences
are combined into summaries it is not guaranteed
that these summaries will also have high content
overlap with the entire model ones. Therefore we
believe if there is a high content agreement between
the training and model summaries this could lead to
more readable summaries. However, as we can see
from Table 7 this hypothesis does not hold for all
criteria. In case of the redundancy criterion we have
compared to wordLimit summary type high scores
in regression summaries although wordLimit sum-
maries are significantly better than regression ones
when it concerns the ROUGE scores. Thus it is
likely that by aggressively optimising the ROUGE
metric the model learns to game the metric, which
does not penalise redundancy in the summaries.
As such it may no longer possible to extrapolate
trends from earlier correlation studies against human
judgements (Lin, 2004).
To minimize redundancy in summaries it is nec-
essary to also take into consideration global features
addressing the linguistic aspects of the summaries.
Furthermore, instead of ROUGE recall scores which
do not take the repetition of information into consid-
eration, ROUGE precision scores could be used as a
metric in order to minimize the redundant content in
the summaries.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9995208">
In this paper we have proposed an A* search ap-
proach for generating a summary from a ranked list
of sentences and learning feature weights for a fea-
ture based extractive multi-document summariza-
tion system. We developed an algorithm to learn
optimize an arbitrary metric and showed that our
approach significantly outperforms state of the art
techniques. Furthermore, we highlighted the impor-
tance of uniformity in training and testing and ar-
gued that if the goal is to generate high scoring sum-
maries under a length limit in testing, then the same
constraint should also be used in training.
In this paper we experimented with sentence-local
features. In the future we plan to expand this fea-
ture set with global features, especially ones mea-
</bodyText>
<page confidence="0.990859">
490
</page>
<bodyText confidence="0.999864090909091">
suring lexical diversity in the summaries to reduce
the redundancy in them. We will investigate vari-
ous ways of incorporating these global features into
our A* search. However this will incur an additional
computational cost over a purely local feature model
and therefore may necessitate using an approximate
beam search. We also plan to investigate using other
metrics in training in order to reduce redundant in-
formation in the summaries. Finally, we have made
our summarizer publicly available as open-source
software.13
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998270779661017">
A. Aker and R. Gaizauskas. 2009. Summary Gener-
ation for Toponym-Referenced Images using Object
Type Language Models. International Conference
on Recent Advances in Natural Language Processing
(RANLP) September 14-16, 2009, Borovets, Bulgaria.
A. Aker and R. Gaizauskas. 2010a. Generating im-
age descriptions using dependency relational patterns.
Proc. of the ACL 2010, Upsala, Sweden.
A. Aker and R. Gaizauskas. 2010b. Model Summaries
for Location-related Images. In Proc. of the LREC-
2010 Conference.
R. Brandow, K. Mitze, and L.F. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection* 1. Information Processing &amp; Management,
31(5):675–685.
J.M. Conroy, J.D. Schlesinger, and J.G. Stewart. 2005.
CLASSY query-based multi-document summariza-
tion. Proc. of the 2005 Document Understanding
Workshop, Boston.
H.T. Dang. 2005. Overview of DUC 2005. DUC 05
Workshop at HLT/EMNLP.
H. Edmundson, P. 1969. New Methods in Automatic
Extracting. Journal of the Association for Computing
Machinery, 16:264–285.
M.A. Hearst. 1997. TextTiling: segmenting text into
multi-paragraph subtopic passages. Computational
linguistics, 23(1):33–64.
C-Y. Lin. 2004. Rouge: A package for automatic evalua-
tion of summaries. Text Summarization Branches Out:
Proc. of the ACL-04 Workshop, pages 74–81.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Publishing Company.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. Proc. of the 41st Annual Meeting
on Association for Computational Linguistics-Volume
1, page 167.
13Available from http://www.dcs.shef.ac.uk/
˜tcohn/a-star
Y. Ouyang, W. Li, S. Li, and Q. Lu. 2010. Applying
regression models to query-focused multi-document
summarization. Information Processing &amp; Manage-
ment.
D.R. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001.
Experiments in single and multi-document summa-
rization using MEAD. Document Understanding Con-
ference.
K. Riedhammer, D. Gillick, B. Favre, and D. Hakkani-T
”ur. 2008. Packing the meeting summarization knap-
sack. Proc. Interspeech, Brisbane, Australia.
S.J. Russell, P. Norvig, J.F. Canny, J. Malik, and D.D.
Edwards. 1995. Artificial intelligence: a modern ap-
proach. Prentice hall Englewood Cliffs, NJ.
H. Saggion. 2005. Topic-based Summarization at
DUC 2005. Document Understanding Conference
(DUC05).
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management: an International Journal,
24(5):513–523.
</reference>
<page confidence="0.998752">
491
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.505388">
<title confidence="0.999738">Multi-document summarization using A* search and discriminative training</title>
<author confidence="0.999251">Ahmet Aker Trevor Cohn Robert Gaizauskas</author>
<affiliation confidence="0.999963">Department of Computer Science</affiliation>
<address confidence="0.538622">University of Sheffield, Sheffield, S1 4DP, UK</address>
<email confidence="0.951597">t.cohn,</email>
<abstract confidence="0.9974655">In this paper we address two key challenges for extractive multi-document summarization: the search problem of finding the best scoring summary and the training problem of learning the best model parameters. We propose an A* search algorithm to find the best extractive summary up to a given length, which is both optimal and efficient to run. Further, we propose a discriminative training algorithm which directly maximises the quality of the best summary, rather than assuming a sentence-level decomposition as in earlier work. Our approach leads to significantly better results than earlier techniques across a number of evaluation metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aker</author>
<author>R Gaizauskas</author>
</authors>
<title>Summary Generation for Toponym-Referenced Images using Object Type Language Models.</title>
<date>2009</date>
<booktitle>International Conference on Recent Advances in Natural Language Processing (RANLP)</booktitle>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="18159" citStr="Aker and Gaizauskas, 2009" startWordPosition="3152" endWordPosition="3155">he R-1, R-2 and R-SU4 variants of the metric. 5We used n = 100 in our experiments. 5 Experimental settings In this section we describe the features for which we learn weights. We also describe the input data used in training and testing. 5.1 Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): • querySimilarity: Sentence similarity to the query (cosine similarity over the vector representation of the sentence and the query). • centroidSimilarity: Sentence similarity to the centroid. The centroid is composed of the 100 most frequently occurring non stop words in the document collection (cosine similarity over the vector representation of the sentence and the centroid). For each word/term in the vector we store a value which is the product of the term frequency in the document and the inverse document frequency, a measurement of the term’s distribution o</context>
<context position="21984" citStr="Aker and Gaizauskas, 2009" startWordPosition="3883" endWordPosition="3886">sentence in each description a ROUGE score by comparing the sentence to those included in the model summaries for that particular place name and retaining the highest score. Table 1 gives some details about this training data. We use ROUGE as a metric to maximize because it is also used in DUC7 and TAC.8 However, it should be noted that any automatic metric could be used instead of ROUGE. In particular we use ROUGE 1 (R-1), ROUGE 2 (R-2) and ROUGE SU4 (R-SU4). R-1 and R-2 compute the number 6For our training and testing sets we manually assigned each location to its corresponding object type (Aker and Gaizauskas, 2009). 7http://duc.nist.gov/ 8http://www.nist.gov/tac/ Max Min Avg Sentences/place 1724 3 260 Words/sentence 37 3 17 Table 1: The training input data contains 184 place names with 42333 sentences in total. The numbers in the columns give detail about the number of sentences for each place and the lengths of the sentences. Max Min Avg Documents/place 20 5 12 Sentences/place 1716 15 132 Sentences/document 275 1 10 Words/sentence 211 1 20 Table 2: In domain test data. The numbers in the columns give detail about the number of documents (descriptions) for each place, number of sentences for each place </context>
</contexts>
<marker>Aker, Gaizauskas, 2009</marker>
<rawString>A. Aker and R. Gaizauskas. 2009. Summary Generation for Toponym-Referenced Images using Object Type Language Models. International Conference on Recent Advances in Natural Language Processing (RANLP) September 14-16, 2009, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aker</author>
<author>R Gaizauskas</author>
</authors>
<title>Generating image descriptions using dependency relational patterns.</title>
<date>2010</date>
<booktitle>Proc. of the ACL</booktitle>
<location>Upsala,</location>
<contexts>
<context position="18186" citStr="Aker and Gaizauskas, 2010" startWordPosition="3156" endWordPosition="3159">nts of the metric. 5We used n = 100 in our experiments. 5 Experimental settings In this section we describe the features for which we learn weights. We also describe the input data used in training and testing. 5.1 Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): • querySimilarity: Sentence similarity to the query (cosine similarity over the vector representation of the sentence and the query). • centroidSimilarity: Sentence similarity to the centroid. The centroid is composed of the 100 most frequently occurring non stop words in the document collection (cosine similarity over the vector representation of the sentence and the centroid). For each word/term in the vector we store a value which is the product of the term frequency in the document and the inverse document frequency, a measurement of the term’s distribution over the set of documents (S</context>
<context position="20599" citStr="Aker and Gaizauskas (2010" startWordPosition="3645" endWordPosition="3648"> 486 church, bridge, etc. When we generate a summary about a location of type church, for instance, then we apply the church language model on the related input documents related to the location.6 • sentenceCount: Each sentence gets assigned a value of 1. This feature is used to learn whether summaries with many sentences are better than summaries with few sentences or vice versa. • wordCount: Number of words in the summary, to decide whether the model should favor long summaries or short ones. 5.2 Data For training and testing we use the freely available image description corpus described in Aker and Gaizauskas (2010b). The corpus is based around 289 images of static located objects (e.g Eiffel Tower, Mont Blanc) each with a manually assigned place name and object type category (e.g. church, mountain). For each place name there are up to four model summaries that were created manually after reading existing image descriptions taken from the VirtualTourist travel community web-site. Each summary contains a minimum of 190 and a maximum of 210 words. We divide this set of 289 place names into training and testing sets. Both sets are described in the following subsections. Training We use 184 place names from</context>
</contexts>
<marker>Aker, Gaizauskas, 2010</marker>
<rawString>A. Aker and R. Gaizauskas. 2010a. Generating image descriptions using dependency relational patterns. Proc. of the ACL 2010, Upsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aker</author>
<author>R Gaizauskas</author>
</authors>
<title>Model Summaries for Location-related Images.</title>
<date>2010</date>
<booktitle>In Proc. of the LREC2010 Conference.</booktitle>
<contexts>
<context position="18186" citStr="Aker and Gaizauskas, 2010" startWordPosition="3156" endWordPosition="3159">nts of the metric. 5We used n = 100 in our experiments. 5 Experimental settings In this section we describe the features for which we learn weights. We also describe the input data used in training and testing. 5.1 Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): • querySimilarity: Sentence similarity to the query (cosine similarity over the vector representation of the sentence and the query). • centroidSimilarity: Sentence similarity to the centroid. The centroid is composed of the 100 most frequently occurring non stop words in the document collection (cosine similarity over the vector representation of the sentence and the centroid). For each word/term in the vector we store a value which is the product of the term frequency in the document and the inverse document frequency, a measurement of the term’s distribution over the set of documents (S</context>
<context position="20599" citStr="Aker and Gaizauskas (2010" startWordPosition="3645" endWordPosition="3648"> 486 church, bridge, etc. When we generate a summary about a location of type church, for instance, then we apply the church language model on the related input documents related to the location.6 • sentenceCount: Each sentence gets assigned a value of 1. This feature is used to learn whether summaries with many sentences are better than summaries with few sentences or vice versa. • wordCount: Number of words in the summary, to decide whether the model should favor long summaries or short ones. 5.2 Data For training and testing we use the freely available image description corpus described in Aker and Gaizauskas (2010b). The corpus is based around 289 images of static located objects (e.g Eiffel Tower, Mont Blanc) each with a manually assigned place name and object type category (e.g. church, mountain). For each place name there are up to four model summaries that were created manually after reading existing image descriptions taken from the VirtualTourist travel community web-site. Each summary contains a minimum of 190 and a maximum of 210 words. We divide this set of 289 place names into training and testing sets. Both sets are described in the following subsections. Training We use 184 place names from</context>
</contexts>
<marker>Aker, Gaizauskas, 2010</marker>
<rawString>A. Aker and R. Gaizauskas. 2010b. Model Summaries for Location-related Images. In Proc. of the LREC2010 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brandow</author>
<author>K Mitze</author>
<author>L F Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection* 1.</title>
<date>1995</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<volume>31</volume>
<issue>5</issue>
<contexts>
<context position="2282" citStr="Brandow et al., 1995" startWordPosition="353" endWordPosition="356">fold: 1) the search problem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a training set consisting of pairs of document sets with model or reference summaries – typically human authored extractive or abstractive summaries. Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g., Saggion (2005)) or using heuristic strategies based on position in document or lexical clues (Edmundson, 1969; Brandow et al., 1995; Hearst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUG</context>
<context position="18091" citStr="Brandow et al., 1995" startWordPosition="3140" endWordPosition="3143"> use MERT to optimise the ROUGE score on the training set for the R-1, R-2 and R-SU4 variants of the metric. 5We used n = 100 in our experiments. 5 Experimental settings In this section we describe the features for which we learn weights. We also describe the input data used in training and testing. 5.1 Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): • querySimilarity: Sentence similarity to the query (cosine similarity over the vector representation of the sentence and the query). • centroidSimilarity: Sentence similarity to the centroid. The centroid is composed of the 100 most frequently occurring non stop words in the document collection (cosine similarity over the vector representation of the sentence and the centroid). For each word/term in the vector we store a value which is the product of the term frequency in the document and the in</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>R. Brandow, K. Mitze, and L.F. Rau. 1995. Automatic condensation of electronic publications by sentence selection* 1. Information Processing &amp; Management, 31(5):675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Conroy</author>
<author>J D Schlesinger</author>
<author>J G Stewart</author>
</authors>
<title>CLASSY query-based multi-document summarization.</title>
<date>2005</date>
<booktitle>Proc. of the 2005 Document Understanding Workshop,</booktitle>
<location>Boston.</location>
<contexts>
<context position="18132" citStr="Conroy et al., 2005" startWordPosition="3148" endWordPosition="3151">he training set for the R-1, R-2 and R-SU4 variants of the metric. 5We used n = 100 in our experiments. 5 Experimental settings In this section we describe the features for which we learn weights. We also describe the input data used in training and testing. 5.1 Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): • querySimilarity: Sentence similarity to the query (cosine similarity over the vector representation of the sentence and the query). • centroidSimilarity: Sentence similarity to the centroid. The centroid is composed of the 100 most frequently occurring non stop words in the document collection (cosine similarity over the vector representation of the sentence and the centroid). For each word/term in the vector we store a value which is the product of the term frequency in the document and the inverse document frequency, a measurement o</context>
</contexts>
<marker>Conroy, Schlesinger, Stewart, 2005</marker>
<rawString>J.M. Conroy, J.D. Schlesinger, and J.G. Stewart. 2005. CLASSY query-based multi-document summarization. Proc. of the 2005 Document Understanding Workshop, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<date>2005</date>
<booktitle>Overview of DUC 2005. DUC 05 Workshop at HLT/EMNLP.</booktitle>
<contexts>
<context position="31787" citStr="Dang, 2005" startWordPosition="5501" endWordPosition="5502">aptions contain text from Wikipedia articles, which are also returned as results from the web search. Therefore the web-document sets included similar content to the VirtualTourist captions. 6.2 Manual Evaluation We also evaluated our summaries using a readability assessment as in DUC and TAC. DUC and TAC manually assess the quality of automatically generated summaries by asking human subjects to score each summary using five criteria – grammaticality, redundancy, clarity, focus and coherence criteria. Each criterion is scored on a five point scale with high scores indicating a better result (Dang, 2005). For this evaluation we used the best scoring summaries from the wordLimit summary type (R-1, R-2 and R-SU4) generated using web-documents (out of domain documents) as input. We also evaluate the regression summary types generated using the same input documents to investigate the correlation between high and low ROUGE metric scores to manual evaluation ones. From the regression summary type we only use summaries under the R2 and RSU4 trained models. In total we evaluated five different summary types (three from wordLimit and two from regression). For each type we randomly selected 30 place na</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>H.T. Dang. 2005. Overview of DUC 2005. DUC 05 Workshop at HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Edmundson</author>
<author>P</author>
</authors>
<title>New Methods in Automatic Extracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>16--264</pages>
<marker>Edmundson, P, 1969</marker>
<rawString>H. Edmundson, P. 1969. New Methods in Automatic Extracting. Journal of the Association for Computing Machinery, 16:264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling: segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<booktitle>Computational linguistics,</booktitle>
<pages>23--1</pages>
<contexts>
<context position="2296" citStr="Hearst, 1997" startWordPosition="357" endWordPosition="358">oblem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a training set consisting of pairs of document sets with model or reference summaries – typically human authored extractive or abstractive summaries. Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g., Saggion (2005)) or using heuristic strategies based on position in document or lexical clues (Edmundson, 1969; Brandow et al., 1995; Hearst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUGE (Lin, 2004).</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M.A. Hearst. 1997. TextTiling: segmenting text into multi-paragraph subtopic passages. Computational linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out:</title>
<date>2004</date>
<booktitle>Proc. of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="2895" citStr="Lin, 2004" startWordPosition="454" endWordPosition="455">earst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUGE (Lin, 2004). The model is then used during search to find a summary composed of high scoring (‘good’) sentences (see for a review Ouyang et al. (2010)). However, there is a disconnect between the model used for training and the model used for prediction. In this paper we present a solution to this disconnect in the form of a training algorithm that optimises the full prediction model directly with the search algorithm intact. The training algorithm learns parameters such that 1Genetic algorithms have also been devised for solving the search problem (see, e.g., Riedhammer et al. (2008)), however these app</context>
<context position="24369" citStr="Lin, 2004" startWordPosition="4284" endWordPosition="4285">h place name we retrieved the top ten related webdocuments using the Yahoo! search engine with the place name as a query. The text from these documents is extracted using an HTML parser and passed to the summarizer. Table 3 gives an overview of this data. 487 Max Min Avg Sentences/place 1773 55 328 Sentence/document 874 1 32 Words/sentence 236 1 21 Table 3: Out of domain test data. The numbers in the columns give detail about the number of sentences for each place and document and the lengths of the sentences. 6 Results To evaluate our approach we used two different assessment methods: ROUGE (Lin, 2004) and manual readability. In the following we present the results of each assessment. 6.1 Automatic Evaluation using ROUGE We report results for training and testing. In both training and testing we distinguish between three different summaries: wordLimit, sentenceLimit and regression. WordLimit and sentenceLimit summaries are the ones generated using the model trained by MERT. As described in section 4 we trained the summariser using the A* search decoder to maximise the ROUGE score of the best scoring summaries. We used the heuristic function h3 in A* search because it is the best performing </context>
<context position="36001" citStr="Lin, 2004" startWordPosition="6176" endWordPosition="6177">eadable summaries. However, as we can see from Table 7 this hypothesis does not hold for all criteria. In case of the redundancy criterion we have compared to wordLimit summary type high scores in regression summaries although wordLimit summaries are significantly better than regression ones when it concerns the ROUGE scores. Thus it is likely that by aggressively optimising the ROUGE metric the model learns to game the metric, which does not penalise redundancy in the summaries. As such it may no longer possible to extrapolate trends from earlier correlation studies against human judgements (Lin, 2004). To minimize redundancy in summaries it is necessary to also take into consideration global features addressing the linguistic aspects of the summaries. Furthermore, instead of ROUGE recall scores which do not take the repetition of information into consideration, ROUGE precision scores could be used as a metric in order to minimize the redundant content in the summaries. 7 Conclusion In this paper we have proposed an A* search approach for generating a summary from a ranked list of sentences and learning feature weights for a feature based extractive multi-document summarization system. We d</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C-Y. Lin. 2004. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out: Proc. of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="1236" citStr="Mani, 2001" startWordPosition="184" endWordPosition="185">ve training algorithm which directly maximises the quality of the best summary, rather than assuming a sentence-level decomposition as in earlier work. Our approach leads to significantly better results than earlier techniques across a number of evaluation metrics. 1 Introduction Multi-document summarization aims to present multiple documents in form of a short summary. This short summary can be used as a replacement for the original documents to reduce, for instance, the time a reader would spend if she were to read the original documents. Following dominant trends in summarization research (Mani, 2001), we focus solely on extractive summarization which simplifies the summarization task to the problem of identifying a subset of units from the document collection (here sentences) which are concatenated to form the summary. Most multi-document summarization systems define a model which assigns a score to a candidate summary based on the features of the sentences included in the summary. The research challenges are then twofold: 1) the search problem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a trai</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>I. Mani. 2001. Automatic Summarization. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>Proc. of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>167</pages>
<contexts>
<context position="4657" citStr="Och, 2003" startWordPosition="734" endWordPosition="735">nd -SU4. The paper is structured as follows. Section 2 presents the summarization model. Next in section 3 we present an A* search algorithm for finding the best scoring (argmax) summary under the model with a constraint on the maximum summary length. We show that this algorithm performs search efficiently, even for very large document sets composed of many sentences. The second contribution of the paper is a new training method which directly optimises the summarization system, and is presented in section 4. This uses the minimum error-rate training (MERT) technique from machine translation (Och, 2003) to optimise the summariser’s output to an arbitrary evaluation metric. Section 5 describes our experimental setup and section 6 the results. Finally we conclude in section 7. 2 Summarization Model Extractive multi-document summarization aims to find the most important sentences from a set of documents, which are then collated and presented to the user in form of a short summary. Following the predominant approach to data-driven summarisation, we define a linear model which scores summaries as the weighted sum of their features, s(y|x) = -b(x, y) · A , (1) where x is the document set, composed</context>
<context position="17106" citStr="Och, 2003" startWordPosition="2979" endWordPosition="2980">entences in the document set. The y axis measures the search graph size in terms of the number of edges in the schedule and the number of nodes visited. Measured with the final parameters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve A = arg min A(�y, r) , (4) a where with a slight abuse of notation, y� and r are taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-best lists,5 and use MERT to optimise the ROUGE score on the training set for the R-1, R-2 and R-SU4 variants of the metric. 5We used n = 100 in our experiments. 5 Experimental settings In this section we describe the features for which we learn weigh</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F.J. Och. 2003. Minimum error rate training in statistical machine translation. Proc. of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, page 167.</rawString>
</citation>
<citation valid="false">
<note>13Available from http://www.dcs.shef.ac.uk/ ˜tcohn/a-star</note>
<marker></marker>
<rawString>13Available from http://www.dcs.shef.ac.uk/ ˜tcohn/a-star</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ouyang</author>
<author>W Li</author>
<author>S Li</author>
<author>Q Lu</author>
</authors>
<title>Applying regression models to query-focused multi-document summarization.</title>
<date>2010</date>
<journal>Information Processing &amp; Management.</journal>
<contexts>
<context position="2318" citStr="Ouyang et al., 2010" startWordPosition="359" endWordPosition="362">ng the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a training set consisting of pairs of document sets with model or reference summaries – typically human authored extractive or abstractive summaries. Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g., Saggion (2005)) or using heuristic strategies based on position in document or lexical clues (Edmundson, 1969; Brandow et al., 1995; Hearst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUGE (Lin, 2004). The model is then use</context>
<context position="25859" citStr="Ouyang et al. (2010)" startWordPosition="4525" endWordPosition="4528"> N so that in both wordLimit and sentenceLimit summaries we obtain more or less the same number of words (because our training data contains on average 17 words for each word we set N to 12, 12*17=194). However, this is only the case in the training. In the testing for both wordLimit and sentenceLimit we generate summaries with the same word limit constraint which allows us to have a fair comparison between the ROUGE recall scores. The regression summaries are our baseline. In these summaries the sentences are ranked based on the weighted features produced by Support Vector Regression (SVR).9 Ouyang et al. (2010) use multi-document summarization and linear regression methods to rank sentences in the documents. As regression model they used SVR and showed 9We use the term regression to refer to SVR. Type metric R-1 R-2 R-SU4 R-1 0.5792 0.3176 0.3580 wordLimit R-2 0.5656 0.3208 0.3510 R-SU4 0.5688 0.3197 0.3585 R-1 0.5915 0.3507 0.3881 sentenceLimit R-2 0.5783 0.3601 0.3890 R-SU4 0.5870 0.3546 0.3929 R-1 0.4993 0.1946 0.2448 regression R-2 0.4833 0.1949 0.2413 R-SU4 0.5009 0.2031 0.2562 Table 4: ROUGE scores obtained on the training data. that it out-performed classification and Learning To Rank methods</context>
</contexts>
<marker>Ouyang, Li, Li, Lu, 2010</marker>
<rawString>Y. Ouyang, W. Li, S. Li, and Q. Lu. 2010. Applying regression models to query-focused multi-document summarization. Information Processing &amp; Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>S Blair-Goldensohn</author>
<author>Z Zhang</author>
</authors>
<title>Experiments in single and multi-document summarization using MEAD. Document Understanding Conference.</title>
<date>2001</date>
<contexts>
<context position="18111" citStr="Radev et al., 2001" startWordPosition="3144" endWordPosition="3147">the ROUGE score on the training set for the R-1, R-2 and R-SU4 variants of the metric. 5We used n = 100 in our experiments. 5 Experimental settings In this section we describe the features for which we learn weights. We also describe the input data used in training and testing. 5.1 Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): • querySimilarity: Sentence similarity to the query (cosine similarity over the vector representation of the sentence and the query). • centroidSimilarity: Sentence similarity to the centroid. The centroid is composed of the 100 most frequently occurring non stop words in the document collection (cosine similarity over the vector representation of the sentence and the centroid). For each word/term in the vector we store a value which is the product of the term frequency in the document and the inverse document frequ</context>
</contexts>
<marker>Radev, Blair-Goldensohn, Zhang, 2001</marker>
<rawString>D.R. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Experiments in single and multi-document summarization using MEAD. Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Riedhammer</author>
<author>D Gillick</author>
<author>B Favre</author>
<author>D Hakkani-T ”ur</author>
</authors>
<title>Packing the meeting summarization knapsack.</title>
<date>2008</date>
<booktitle>Proc. Interspeech,</booktitle>
<location>Brisbane, Australia.</location>
<marker>Riedhammer, Gillick, Favre, ”ur, 2008</marker>
<rawString>K. Riedhammer, D. Gillick, B. Favre, and D. Hakkani-T ”ur. 2008. Packing the meeting summarization knapsack. Proc. Interspeech, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Russell</author>
<author>P Norvig</author>
<author>J F Canny</author>
<author>J Malik</author>
<author>D D Edwards</author>
</authors>
<title>Artificial intelligence: a modern approach. Prentice hall Englewood Cliffs,</title>
<date>1995</date>
<location>NJ.</location>
<contexts>
<context position="2445" citStr="Russell et al., 1995" startWordPosition="381" endWordPosition="384">cribe a training set consisting of pairs of document sets with model or reference summaries – typically human authored extractive or abstractive summaries. Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g., Saggion (2005)) or using heuristic strategies based on position in document or lexical clues (Edmundson, 1969; Brandow et al., 1995; Hearst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUGE (Lin, 2004). The model is then used during search to find a summary composed of high scoring (‘good’) sentences (see for a review Ouyang et al. (2010)). However,</context>
<context position="7454" citStr="Russell et al., 1995" startWordPosition="1222" endWordPosition="1225">red a search problem in which we start with an empty summary and incrementally enlarge the summary by concatenating a sentence from our document set. The search graph starts with an empty summary (the starting state) and each outgoing edge adds a sentence to produce a subsequent state, and is assigned a score under the model. A goal state is any state with no more words than the given threshold. The summarisation problem is then equivalent to finding the best scoring path (summed over the edge scores) between the start state and a goal state. The novel insight in our work is to use A* search (Russell et al., 1995) to solve the prediction problem. A* is a best-first search algorithm which can efficiently find the best scoring path or the n-best paths (unlike the greedy algorithm which is not optimal, and the backtracking variant which is not efficient). The search procedure requires a scoring function for each state, here s(y|x) from (2), and 2Our approach could be adapted to support global features, which would require changes to the heuristic for A* search to bound the score obtainable from the global features. This may incur an additional computational cost over a purely local feature model and perha</context>
</contexts>
<marker>Russell, Norvig, Canny, Malik, Edwards, 1995</marker>
<rawString>S.J. Russell, P. Norvig, J.F. Canny, J. Malik, and D.D. Edwards. 1995. Artificial intelligence: a modern approach. Prentice hall Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
</authors>
<title>Topic-based Summarization at DUC</title>
<date>2005</date>
<booktitle>Document Understanding Conference (DUC05).</booktitle>
<contexts>
<context position="2165" citStr="Saggion (2005)" startWordPosition="335" endWordPosition="336">te summary based on the features of the sentences included in the summary. The research challenges are then twofold: 1) the search problem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a training set consisting of pairs of document sets with model or reference summaries – typically human authored extractive or abstractive summaries. Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g., Saggion (2005)) or using heuristic strategies based on position in document or lexical clues (Edmundson, 1969; Brandow et al., 1995; Hearst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classif</context>
</contexts>
<marker>Saggion, 2005</marker>
<rawString>H. Saggion. 2005. Topic-based Summarization at DUC 2005. Document Understanding Conference (DUC05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management: an International Journal,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="18810" citStr="Salton and Buckley, 1988" startWordPosition="3256" endWordPosition="3259">0a): • querySimilarity: Sentence similarity to the query (cosine similarity over the vector representation of the sentence and the query). • centroidSimilarity: Sentence similarity to the centroid. The centroid is composed of the 100 most frequently occurring non stop words in the document collection (cosine similarity over the vector representation of the sentence and the centroid). For each word/term in the vector we store a value which is the product of the term frequency in the document and the inverse document frequency, a measurement of the term’s distribution over the set of documents (Salton and Buckley, 1988). • sentencePosition: Position of the sentence within its document. The first sentence in the document gets the score 1 and the last one gets 1 where n is the number of sentences in the ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● uniform aggregated aggregated+final ● ● 1e+02 1e+03 1e+04 1e+05 1e+06 total edges and nodes n document. • inFirst5: Binary feature indicating whether the sentence</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing and Management: an International Journal, 24(5):513–523.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>