<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.861349">
Experiments on the France Telecom 3000 Voice Agency corpus: academic
research on an industrial spoken dialog system*
G´eraldine Damnati
</title>
<author confidence="0.675717">
France T´el´ecom R&amp;D
</author>
<affiliation confidence="0.60142">
TECH/SSTP/RVA
</affiliation>
<address confidence="0.970323">
2 av. Pierre Marzin
22307 Lannion Cedex 07, France
</address>
<email confidence="0.946441">
geraldine.damnati@orange-ftgroup.com
</email>
<sectionHeader confidence="0.998253" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991888">
The recent advances in speech recognition
technologies, and the experience acquired
in the development of WEB or Interac-
tive Voice Response interfaces, have facil-
itated the integration of speech modules
in robust Spoken Dialog Systems (SDS),
leading to the deployment on a large scale
of speech-enabled services. With these
services it is possible to obtain very large
corpora of human-machine interactions by
collecting system logs. This new kinds of
systems and dialogue corpora offer new
opportunities for academic research while
raising two issues: How can academic re-
search take profit of the system logs of
deployed SDS in order to build the next
generation of SDS, although the dialogues
collected have a dialogue flow constrained
by the previous SDS generation? On the
other side, what immediate benefits can
academic research offer for the improve-
ment of deployed system? This paper ad-
dresses these aspects in the framework of
the deployed France Telecom 3000 Voice
Agency service.
</bodyText>
<footnote confidence="0.839189666666667">
This work is supported by the 6th Framework Research
Programme of the European Union (EU), Project LUNA,
IST contract no 33549. The authors would like to thank
the EU for the financial support. For more information
about the LUNA project, please visit the project home-page,
www.ist-luna.eu.
</footnote>
<note confidence="0.963243">
Fr´ed´eric B´echet Renato De Mori
LIA
</note>
<affiliation confidence="0.361462">
University of Avignon
</affiliation>
<address confidence="0.5032765">
AGROPARC, 339 ch. des Meinajaries
84911 Avignon Cedex 09, France
</address>
<email confidence="0.61187">
frederic.bechet,renato.demori
@univ-avignon.fr
</email>
<sectionHeader confidence="0.998877" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999881615384615">
Since the deployment on a very large scale of the
AT&amp;T How May I Help You? (HMIHY) (Gorin et
al., 1997) service in 2000, Spoken Dialogue Sys-
tems (SDS) handling a very large number of calls are
now developed from an industrial point of view. Al-
though a lot of the remaining problems (robustness,
coverage, etc.) are still spoken language process-
ing research problems, the conception and the de-
ployment of such state-of-the-art systems mainly re-
quires knowledge in user interfaces.
The recent advances in speech recognition tech-
nologies, and the experience acquired in the devel-
opment of WEB or Interactive Voice Response inter-
faces have facilitated the integration of speech mod-
ules in robust SDS.
These new SDS can be deployed on a very large
scale, like the France Telecom 3000 Voice Agency
service considered in this study. With these services
it is possible to obtain very large corpora of human-
machine interactions by collecting system logs. The
main differences between these corpora and those
collected in the framework of evaluation programs
like the DARPA ATIS (Hemphill et al., 1990) or the
French Technolangue MEDIA (Bonneau-Maynard
et al., 2005) programs can be expressed through the
following dimensions:
</bodyText>
<listItem confidence="0.993184333333333">
• Size. There are virtually no limits in the
amount of speakers available or the time
needed for collecting the dialogues as thou-
sands of dialogues are automatically processed
every day and the system logs are stored.
Therefore Dialog processing becomes similar
</listItem>
<page confidence="0.994545">
48
</page>
<note confidence="0.7692485">
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 48–55,
NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.962883666666667">
to Broadcast News processing: the limit is not
in the amount of data available, but rather in the
amount of data that can be manually annotated.
</bodyText>
<listItem confidence="0.974839166666667">
• Speakers. Data are from real users. The speak-
ers are not professional ones or have no reward
for calling the system. Therefore their behav-
iors are not biased by the acquisition protocols.
Spontaneous speech and speech affects can be
observed.
• Complexity. The complexity of the services
widely deployed is necessarily limited in order
to guarantee robustness with a high automation
rate. Therefore the dialogues collected are of-
ten short dialogues.
• Semantic model. The semantic model of such
deployed system is task-oriented. The inter-
pretation of an utterance mostly consists in the
detection of application-specific entities. In an
application like the France Telecom 3000 Voice
Agency service this detection is performed by
hand-crafted specific knowledge.
</listItem>
<bodyText confidence="0.999284">
The AT&amp;T HMIHY corpus was the first large dia-
logue corpus, obtained from a deployed system, that
has the above mentioned characteristics. A service
like the France Telecom 3000 Voice Agency service
has been developed by a user interface development
lab. This new kind of systems and dialogue corpora
offer new opportunities for academic research that
can be summarized as follows:
</bodyText>
<listItem confidence="0.879036777777778">
• How can academic research take profit of the
system logs of deployed SDS in order to build
the next generation of SDS, although the di-
alogues collected have a dialogue flow con-
strained by the previous SDS generation?
• On the other side, what immediate benefits can
academic research offer for the improvement
of deployed system, while waiting for the next
SDS generation?
</listItem>
<bodyText confidence="0.999896357142857">
This paper addresses these aspects in the frame-
work of the deployed FT 3000 Voice Agency ser-
vice. Section 3 presents how the ASR process can
be modified in order to detect and reject Out-Of-
Domain utterances, leading to an improvement in
the understanding performance without modifying
the system. Section 4 shows how the FT 3000 cor-
pus can be used in order to build stochastic models
that are the basis of a new Spoken Language Un-
derstanding strategy, even if the current SLU system
used in the FT 3000 service is not stochastic. Sec-
tion 5 presents experimental results obtained on this
corpus justifying the need of a tighter integration be-
tween the ASR and the SLU models.
</bodyText>
<subsectionHeader confidence="0.55309">
2 Description of the France Telecom 3000
Voice Agency corpus
</subsectionHeader>
<bodyText confidence="0.9999666">
The France Telecom 3000 (FT3000) Voice Agency
service, the first deployed vocal service at France
Telecom exploiting natural language technologies,
has been made available to the general public in Oc-
tober 2005. FT3000 service enables customers to
obtain information and purchase almost 30 differ-
ent services and access the management of their ser-
vices. The continuous speech recognition system re-
lies on a bigram language model. The interpretation
is achieved through the Verbateam two-steps seman-
tic analyzer. Verbateam includes a set of rules to
convert the sequence of words hypothesized by the
speech recognition engine into a sequence of con-
cepts and an inference process that outputs an inter-
pretation label from a sequence of concepts.
</bodyText>
<subsectionHeader confidence="0.998424">
2.1 Specificities of interactions
</subsectionHeader>
<bodyText confidence="0.999975611111111">
Given the main functionalities of the application,
two types of dialogues can be distinguished. Some
users call FT 3000 to activate some services they
have already purchased. For such demands, users
are rerouted toward specific vocal services that are
dedicated to those particular tasks. In that case, the
FT3000 service can be seen as a unique automatic
frontal desk that efficiently redirects users. For such
dialogues the collected corpora only contain the in-
teraction prior to rerouting. It can be observed in that
case that users are rather familiar to the system and
are most of the time regular users. Hence, they are
more likely to use short utterances, sometimes just
keywords and the interaction is fast (between one or
two dialogue turns in order to be redirected to the
demanded specific service).
Such dialogues will be referred as transit dia-
logues and represent 80% of the calls to the FT3000
</bodyText>
<page confidence="0.996741">
49
</page>
<bodyText confidence="0.999537625">
service. As for the 20% other dialogues, referred to
as other, the whole interaction is proceeded within
the FT3000 application. They concern users that are
more generally asking for information about a given
service or users that are willing to purchase a new
service. For these dialogues, the average utterance
length is higher, as well as the average number of
dialogue turns.
</bodyText>
<table confidence="0.9928975">
other transit
# dialogues 350 467
# utterances 1288 717
# words 4141 1454
av. dialogue length 3.7 1.5
av. utterance length 3.2 2.0
OOV rate (%) 3.6 1.9
disfluency rate (%) 2.8 2.1
</table>
<tableCaption confidence="0.99992">
Table 1: Statistics on the transit and other dialogues
</tableCaption>
<bodyText confidence="0.999983583333333">
As can be observed in table 1 the fact that users
are less familiar with the application in the other dia-
logues implies higher OOV rate and disfluency rate1.
An important issue when designing ASR and SLU
models for such applications that are dedicated to
the general public is to be able to handle both naive
users and familiar users. Models have to be robust
enough for new users to accept the service and in
the meantime they have to be efficient enough for
familiar users to keep on using it. This is the reason
why experimental results will be detailed on the two
corpora described in this section.
</bodyText>
<subsectionHeader confidence="0.999644">
2.2 User behavior and OOD utterances
</subsectionHeader>
<bodyText confidence="0.999845846153846">
When dealing with real users corpora, one has to
take into account the occurrence of Out-Of-Domain
(OOD) utterances. Users that are familiar with a ser-
vice are likely to be efficient and to strictly answer
the system’s prompts. New users can have more di-
verse reactions and typically make more comments
about the system. By comments we refer to such
cases when a user can either be surprised what am
I supposed to say now?, irritated I’ve already said
that or even insulting the system. A critical aspect
for other dialogues is the higher rate of comments
uttered by users. For the transit dialogues this phe-
nomenon is much less frequent because users are fa-
</bodyText>
<footnote confidence="0.523502">
1by disfluency we consider here false starts and filled pauses
</footnote>
<bodyText confidence="0.9994164">
miliar to the system and they know how to be effi-
cient and how to reach their goal. As shown in ta-
ble 2, 14.3% of the other dialogues contain at least
one OOD comment, representing an overall 10.6%
of utterances in these dialogues.
</bodyText>
<table confidence="0.9952885">
other transit
# dialogues 350 467
# utterances 1288 717
# OOD comments 137 24
OOD rate (%) 10.6 3.3
dialogues with OOD (%) 14.3 3.6
</table>
<tableCaption confidence="0.988755">
Table 2: Occurrence of Out-Of-Domain comments
</tableCaption>
<bodyText confidence="0.937620333333333">
on the transit and other dialogues
Some utterances are just comments and some con-
tain both useful information and comments. In the
next section, we propose to detect these OOD se-
quences and to take this phenomenon into account
in the global SLU strategy.
</bodyText>
<sectionHeader confidence="0.850143" genericHeader="method">
3 Handling Out-Of-Domain utterances
</sectionHeader>
<bodyText confidence="0.999966904761905">
The general purpose of the proposed strategy is to
detect OOD utterances in a first step, before entering
the Spoken Language Understanding (SLU) mod-
ule. Indeed standard Language Models (LMs) ap-
plied to OOD utterances are likely to generate erro-
neous speech recognition outputs and more gener-
ally highly noisy word lattices from which it might
not be relevant and probably harmful to apply SLU
modules.
Furthermore, when designing a general interac-
tion model which aims at predicting dialogue states
as proposed in this paper, OOD utterances are as
harmful for state prediction as can be an out-of-
vocabulary word for the prediction of the next word
with an n-gram LM.
This is why we propose a new composite LM that
integrates two sub-LMs: one LM for transcribing in-
domain phrases, and one LM for detecting and delet-
ing OOD phrases. Finally the different SLU strate-
gies proposed in this paper are applied only to the
portions of signal labeled as in-domain utterances.
</bodyText>
<page confidence="0.97686">
50
</page>
<subsectionHeader confidence="0.936451">
3.1 Composite Language Model for decoding
spontaneous speech
</subsectionHeader>
<bodyText confidence="0.999803176470588">
As a starting point, the comments have been manu-
ally annotated in the training data in order to easily
separate OOD comment segments from in-domain
ones. A specific bigram language model is trained
for these comment segments. The comment LM was
designed from a 765 words lexicon and trained on
1712 comment sequences.
This comment LM, called LMOOD has been in-
tegrated in the general bigram LMG. Comment
sequences have been parsed in the training corpus
and replaced by a OOD tag. This tag is added to
the general LM vocabulary and bigram probabilities
P( OOD w) and P(wl OOD ) are trained along
with other bigram probabilities (following the prin-
ciple of a priori word classes). During the decoding
process, the general bigram LM probabilities and the
LMOOD bigram probabilities are combined.
</bodyText>
<subsectionHeader confidence="0.999284">
3.2 Decision strategy
</subsectionHeader>
<bodyText confidence="0.999362">
Given this composite LM, a decision strategy is ap-
plied to select those utterances for which the word
lattice will be processed by the SLU component.
This decision is made upon the one-best speech
recognition hypotheses and can be described as fol-
lows:
</bodyText>
<listItem confidence="0.948900111111111">
2. Else, if the one-best ASR output contains an
OOD tag along with other words, those words
are processed directly by the SLU component,
following the argument that the word lattice for
this utterance is likely to contain noisy infor-
mation.
3. Else (i.e. no OOD tag in the one-best ASR
output), the word-lattice is transmitted to fur-
ther SLU components.
</listItem>
<bodyText confidence="0.999923666666667">
It will be shown in the experimental section that
this pre-filtering step, in order to decide whether a
word lattice is worth being processed by the higher-
level SLU components, is an efficient way of pre-
venting concepts and interpretation hypothesis to be
decoded from an uninformative utterance.
</bodyText>
<subsectionHeader confidence="0.99913">
3.3 Experimental setup and evaluation
</subsectionHeader>
<bodyText confidence="0.999492866666667">
The models presented are trained on a corpus col-
lected thanks to the FT3000 service. It contains real
dialogues from the deployed service. The results
presented are obtained on the test corpus described
in section 2.
The results were evaluated according to 3 crite-
ria: the Word Error Rate (WER), the Concept Error
Rate (CER) and the Interpretation Error Rate (IER).
The CER is related to the correct translation of an
utterance into a string of basic concepts. The IER is
related to the global interpretation of an utterance
in the context of the dialogue service considered.
Therefore this last measure is the most significant
one as it is directly linked to the performance of the
dialogue system.
</bodyText>
<table confidence="0.99939075">
IER all other transit
size 2005 717 1288
LMS 16.5 22.3 13.0
LMS + OOD 15.0 18.6 12.8
</table>
<tableCaption confidence="0.967778">
Table 3: Interpretation error rate according to the
Language Model
</tableCaption>
<bodyText confidence="0.980488444444444">
Table 3 presents the IER results obtained with the
strategy strat1 with 2 different LMs for obtaining
W�: LMG which is the general word bigram model;
and LMG + OOD which is the LM with the OOD com-
ment model. As one can see, a very significant im-
provement, 3.7% absolute, is achieved on the other
dialogues, which are the ones containing most of
the comments. For the transit dialogues a small im-
provement (0.2%) is also obtained.
</bodyText>
<sectionHeader confidence="0.924598" genericHeader="method">
4 Building stochastic SLU strategies
</sectionHeader>
<subsectionHeader confidence="0.98675">
4.1 The FT3000 SLU module
</subsectionHeader>
<bodyText confidence="0.99762">
The SLU component of the FT3000 service consid-
ered in this study contains two stages:
</bodyText>
<listItem confidence="0.984298777777778">
1. the first one translates a string of words W =
wl, ... , w,,, into a string of elementary con-
cepts C = cl, ... , cl by means of hand-written
regular grammars;
2. the second stage is made of a set of about 1600
inference rules that take as input a string of con-
cepts C and output a global interpretation -y of
1. If the one-best ASR output is a single OOD
tag, the utterance is simply rejected.
</listItem>
<page confidence="0.993952">
51
</page>
<bodyText confidence="0.999821">
a message. These rules are ordered and the
first match obtained by processing the concept
string is kept as the output interpretation.
These message interpretations are expressed by an
attribute/value pair representing a function in the vo-
cal service.
The models used in these two stages are manually
defined by the service designers and are not stochas-
tic. We are going now to present how we can use a
corpus obtained with such models in order to define
an SLU strategy based on stochastic processes.
</bodyText>
<subsectionHeader confidence="0.995669">
4.2 Semantic knowledge representation
</subsectionHeader>
<bodyText confidence="0.999592044444445">
The actual FT3000 system includes semantic knowl-
edge represented by hand-written rules. These rules
can also be expressed in a logic form. For this rea-
son, some basic concepts are now described with the
purpose of showing how logic knowledge has been
integrated in a first probabilistic model and how it
can be used in a future version in which optimal poli-
cies can be applied.
The semantic knowledge of an application is a
knowledge base (KB) containing a set of logic for-
mulas. Formulas return truth and are constructed
using constants which represent objects and may be
typed, variables, functions which are mappings from
tuples of objects to objects and predicates which
represent relations among objects. An interpretation
specifies which objects, functions and relations in
the domain are represented by which symbol. Basic
inference problem is to determine whether KB |= F
which means that KB entails a formula F.
In SLU, interpretations are carried on by binding
variables and instantiating objects based on ASR re-
sults and inferences performed in the KB. Hypothe-
ses about functions and instantiated objects are writ-
ten into a Short Term Memory (STM).
A user goal is represented by a conjunction of
predicates. As dialogue progresses, some predi-
cates are grounded by the detection of predicate tags,
property tags and values. Such a detection is made
by the interpretation component. Other predicates
are grounded as a result of inference. A user goal G
is asserted when all the atoms of its conjunction are
grounded and asserted true.
Grouping the predicates whose conjunction is the
premise for asserting a goal Gi is a process that goes
through a sequence of states: S1(Gi), S2(Gi), .. .
Let Γik be the content of the STM used for as-
serting the predicates grounded at the k-th turn of a
dialogue. These predicates are part of the premise
for asserting the i-th goal.
Let Gi be an instance of the i-th goal asserted after
grounding all the predicates in the premise.
Γi k can be represented by a composition from a
partial hypothesis Γik − 1 available at turn k − 1, the
machine action ak−1 performed at turn k − 1 and
the semantic interpretation ryik i.e.:
</bodyText>
<equation confidence="0.8249065">
)
Γi k = � (�i k, ak−1, Γi k−1
</equation>
<bodyText confidence="0.9931265">
Sk(Gi) is an information state that can lead to a
user’s goal Gi and Γi k is part of the premise for as-
serting Gi at turn k.
State probability can be written as follows:
</bodyText>
<equation confidence="0.941287">
P (Sk(Gi)|Yk) = P (Gi|Γi )P (Γi k|Yk) (1)
k
</equation>
<bodyText confidence="0.996902619047619">
where P (Gi|Γi) is the probability that Gi is the
k
type of goal that corresponds to the user interac-
tion given the grounding predicates in Γi k. Yk is the
acoustic features of the user’s utterance at turn k.
Probabilities of states can be used to define a be-
lief of the dialogue system.
A first model allowing multiple dialog state se-
quence hypothesis is proposed in (Damnati et al.,
2007). In this model each dialog state correspond
to a system state in the dialog automaton. In order
to deal with flexible dialog strategies and following
previous work (Williams and Young, 2007), a new
model based on a Partially Observable Markov De-
cision Process (POMDP) is currently studied.
If no dialog history is taken into account,
P (Γ&amp;quot;JY ) comes down to P (-yik|Y ), -yik being a
semantic attribute/value pair produced by the Ver-
bateam interpretation rules.
The integration of this semantic decoding process
in the ASR process is presented in the next section.
</bodyText>
<sectionHeader confidence="0.927341" genericHeader="method">
5 Optimizing the ASR and SLU processes
</sectionHeader>
<bodyText confidence="0.999933833333333">
With the stochastic models proposed in section 4,
different strategies can be built and optimized. We
are interested here in the integration of the ASR and
SLU processes. As already shown by previous stud-
ies (Wang et al., 2005), the traditional sequential ap-
proach that first looks for the best sequence of words
</bodyText>
<page confidence="0.996292">
52
</page>
<bodyText confidence="0.999758769230769">
W before looking for the best interpretation γ� of an
utterance is sub-optimal. Performing SLU on a word
lattice output by the ASR module is an efficient way
of integrating the search for the best sequence of
words and the best interpretation. However there are
real-time issues in processing word lattices in SDS,
and therefore they are mainly used in research sys-
tems rather than deployed systems.
In section 3 a strategy is proposed for selecting
the utterances for which a word lattice is going to be
produced. We are going now to evaluate the gain in
performance that can be obtained thanks to an inte-
grated approach on these selected utterances.
</bodyText>
<subsectionHeader confidence="0.99473">
5.1 Sequential vs. integrated strategies
</subsectionHeader>
<bodyText confidence="0.886552333333333">
Two strategies are going to be evaluated. The first
one (strat1) is fully sequential: the best sequence of
word W� is first obtained with
</bodyText>
<equation confidence="0.870801833333333">
W� = argmaxP(W JY )
W
Then the best sequence of concepts C is obtained
with
C = argmaxP(Cl
C
</equation>
<bodyText confidence="0.9985798">
Finally the interpretation rules are applied to C in
order to obtain the best interpretation ry.
The second strategy (strat2) is fully integrated: γ�
is obtained by searching at the same time for W� and
C� and ry. In this case we have:
</bodyText>
<equation confidence="0.8406875">
γ� = argmaxP(γJC)P(CJW)P(WJY )
W,C,-y
</equation>
<bodyText confidence="0.9998958">
The stochastic models proposed are implemented
with a Finite State Machine (FSM) paradigm thanks
to the AT&amp;T FSM toolkit (Mohri et al., 2002).
Following the approach described in (Raymond
et al., 2006), the SLU first stage is implemented by
means of a word-to-concept transducer that trans-
lates a word lattice into a concept lattice. This con-
cept lattice is rescored with a Language Model on
the concepts (also encoded as FSMs with the AT&amp;T
GRM toolkit (Allauzen et al., 2003)).
The rule database of the SLU second stage is en-
coded as a transducer that takes as input concepts
and output semantic interpretations γ. By applying
this transducer to an FSM representing a concept lat-
tice, we directly obtain a lattice of interpretations.
The SLU process is therefore made of the com-
position of the ASR word lattice, two transducers
(word-to-concepts and concept-to-interpretations)
and an FSM representing a Language Model on the
concepts. The concept LM is trained on the FT3000
corpus.
This strategy push forward the approach devel-
opped at AT&amp;T in the How May I Help You? (Gorin
et al., 1997) project by using richer semantic mod-
els than call-types and named-entities models. More
precisely, the 1600 Verbateam interpretation rules
used in this study constitute a rich knowledge base.
By integrating them into the search, thanks to the
FSM paradigm, we can jointly optimize the search
for the best sequence of words, basic concepts, and
full semantic interpretations.
For the strategy strat1 only the best path is kept in
the FSM corresponding to the word lattice, simulat-
ing a sequential approach. For strat2 the best inter-
pretation γ� is obtained on the whole concept lattice.
</bodyText>
<table confidence="0.958907">
error WER CER IER
strat1 40.1 24.4 15.0
strat2 38.2 22.5 14.5
</table>
<tableCaption confidence="0.941768">
Table 4: Word Error Rate (WER), Concept Error
</tableCaption>
<bodyText confidence="0.978421555555556">
Rate (CER) and Interpretation Error Rate (IER) ac-
cording to the SLU strategy
The comparison among the two strategies is given
in table 4. As we can see a small improvement is ob-
tained for the interpretation error rate (IER) with the
integrated strategy (strat2). This gain is small; how-
ever it is interesting to look at the Oracle IER that
can be obtained on an n-best list of interpretations
produced by each strategy (the Oracle IER being the
lowest IER that can be obtained on an n-best list of
hypotheses with a perfect Oracle decision process).
This comparison is given in Figure 1. As one can
see a much lower Oracle IER can be achieved with
strat2. For example, with an n-best list of 5 interpre-
tations, the lowest IER is 7.4 for strat1 and only 4.8
for strat2. This is very interesting for dialogue sys-
tems as the Dialog Manager can use dialogue con-
text information in order to filter such n-best lists.
</bodyText>
<figure confidence="0.71285275">
W�)
53
1 2 3 4 5 6 7 8 9 10
size of the n-best list of interpretations
</figure>
<figureCaption confidence="0.997857">
Figure 1: Oracle IER according to an n-best list of interpretations for strategies strat1 and strat2
</figureCaption>
<figure confidence="0.99705">
10
4
7
9
6
5
8
sequential search (strat1)
integrated search (strat2)
Oracle IER
</figure>
<subsectionHeader confidence="0.986587">
5.2 Optimizing WER, CER and IER
</subsectionHeader>
<bodyText confidence="0.999725785714286">
Table 4 also indicates that the improvements ob-
tained on the WER and CER dimensions don’t al-
ways lead to similar improvements in IER. This is
due to the fact that the improvements in WER and
CER are mostly due to a significant reduction in the
insertion rates of words and concepts. Because the
same weight is usually given to all kinds of errors
(insertions, substitutions and deletions), a decrease
in the overall error rate can be misleading as inter-
pretation strategies can deal more easily with inser-
tions than deletions or substitutions. Therefore the
reduction of the overall WER and CER measures is
not a reliable indicator of an increase of performance
of the whole SLU module.
</bodyText>
<table confidence="0.97726825">
level 1-best Oracle hyp.
WER 33.7 20.0
CER 21.2 9.7
IER 13.0 4.4
</table>
<tableCaption confidence="0.5382054">
Table 5: Error rates on words, concepts and interpre-
tations for the 1-best hypothesis and for the Oracle
hypothesis of each level
These results have already been shown for WER
by previous studies like (Riccardi and Gorin, 1998)
</tableCaption>
<table confidence="0.8523535">
IER
from word Oracle 9.8
from concept Oracle 7.5
interpretation Oracle 4.4
</table>
<tableCaption confidence="0.842786">
Table 6: IER obtained on Oracle hypotheses com-
puted at different levels.
</tableCaption>
<bodyText confidence="0.999766666666667">
or more recently (Wang et al., 2003). They are il-
lustrated by Table 5 and Table 6. The figures shown
in these tables were computed on the subset of utter-
ances that were passed to the SLU component. Ut-
terances for which an OOD has been detected are
discarded. In Table 5 are displayed the error rates
obtained on words, concepts and interpretations both
on the 1-best hypothesis and on the Oracle hypothe-
sis (the one with the lowest error rate in the lattice).
These Oracle error rates were obtained by looking
for the best hypothesis in the lattice obtained at the
corresponding level (e.g. looking for the best se-
quence of concepts in the concept lattice). As for Ta-
ble 6, the mentioned IER are the one obtained when
applying SLU to the Oracles hypotheses computed
for each level. As one can see the lowest IER (4.4)
is not obtained on the hypotheses with the lowest
WER (9.8) or CER (7.5).
</bodyText>
<page confidence="0.997566">
54
</page>
<sectionHeader confidence="0.999574" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999985555555556">
This paper presents a study on the FT3000 corpus
collected from real users on a deployed general pub-
lic application. Two problematics are addressed:
How can such a corpus be helpful to carry on re-
search on advanced SLU methods eventhough it has
been collected from a more simple rule-based dia-
logue system? How can academic research trans-
late into short-term improvements for deployed ser-
vices? This paper proposes a strategy for integrating
advanced SLU components in deployed services.
This strategy consists in selecting the utterances for
which the advanced SLU components are going to
be applied. Section 3 presents such a strategy that
consists in filtering Out-Of-Domain utterances dur-
ing the ASR first pass, leading to significant im-
provement in the understanding performance.
For the SLU process applied to in-domain utter-
ances, an integrated approach is proposed that looks
simultaneously for the best sequence of words, con-
cepts and interpretations from the ASR word lat-
tices. Experiments presented in section 5 on real
data show the advantage of the integrated approach
towards the sequential approach. Finally, section 4
proposes a unified framework that enables to define
a dialogue state prediction model that can be applied
and trained on a corpus collected through an already
deployed service.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999804918367347">
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL’03), Sap-
poro, Japan.
Helene Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Se-
mantic annotation of the french media dialog corpus.
In Proceedings of the European Conference on Speech
Communication and Technology (Eurospeech), Lis-
boa, Portugal.
Geraldine Damnati, Frederic Bechet, and Renato
De Mori. 2007. Spoken Language Understanding
strategies on the France Telecom 3000 voice agency
corpus. In Proceedings of the International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), Honolulu, USA.
A. L. Gorin, G. Riccardi, and J.H. Wright. 1997. How
May I Help You ? In Speech Communication, vol-
ume 23, pages 113–127.
Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
tems pilot corpus. In Proceedings of the workshop on
Speech and Natural Language, pages 96–101, Hidden
Valley, Pennsylvania.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69–88.
Christian Raymond, Frederic Bechet, Renato De Mori,
and Geraldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48,3-4:288–304.
Giuseppe Riccardi and Allen L. Gorin. 1998. Language
models for speech recognition and understanding. In
Proceedings of the International Conference on Spo-
ken Langage Processing (ICSLP), Sidney, Australia.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is word
error rate a good indicator for spoken language under-
standing accuracy? In Automatic Speech Recognition
and Understanding workshop - ASRU’03, St. Thomas,
US-Virgin Islands.
Ye-Yi Wang, Li Deng, and Alex Acero. 2005. Spoken
language understanding. In Signal Processing Maga-
zine, IEEE, volume 22, pages 16–31.
Jason D. Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dialog
systems. Computer, Speech and Language, 21:393–
422.
</reference>
<page confidence="0.999059">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.143620">
<title confidence="0.651919">Experiments on the France Telecom 3000 Voice Agency corpus: academic on an industrial spoken dialog</title>
<author confidence="0.753924">G´eraldine</author>
<affiliation confidence="0.627175">France T´el´ecom</affiliation>
<address confidence="0.90721">2 av. Pierre 22307 Lannion Cedex 07, France</address>
<email confidence="0.999944">geraldine.damnati@orange-ftgroup.com</email>
<abstract confidence="0.997363870967742">The recent advances in speech recognition technologies, and the experience acquired in the development of WEB or Interactive Voice Response interfaces, have facilitated the integration of speech modules in robust Spoken Dialog Systems (SDS), leading to the deployment on a large scale of speech-enabled services. With these services it is possible to obtain very large corpora of human-machine interactions by collecting system logs. This new kinds of systems and dialogue corpora offer new opportunities for academic research while raising two issues: How can academic research take profit of the system logs of SDS in order to build the SDS, although the dialogues collected have a dialogue flow constrained the SDS On the other side, what immediate benefits can academic research offer for the improvement of deployed system? This paper addresses these aspects in the framework of the deployed France Telecom 3000 Voice Agency service. work is supported by the 6th Framework Research Programme of the European Union (EU), Project LUNA, IST contract no 33549. The authors would like to thank the EU for the financial support. For more information about the LUNA project, please visit the project home-page,</abstract>
<author confidence="0.991997">Fr´ed´eric B´echet Renato De</author>
<affiliation confidence="0.996889">University of</affiliation>
<address confidence="0.7817145">AGROPARC, 339 ch. des 84911 Avignon Cedex 09, France</address>
<email confidence="0.852981">@univ-avignon.fr</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics (ACL’03),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="20742" citStr="Allauzen et al., 2003" startWordPosition="3484" endWordPosition="3487"> fully integrated: γ� is obtained by searching at the same time for W� and C� and ry. In this case we have: γ� = argmaxP(γJC)P(CJW)P(WJY ) W,C,-y The stochastic models proposed are implemented with a Finite State Machine (FSM) paradigm thanks to the AT&amp;T FSM toolkit (Mohri et al., 2002). Following the approach described in (Raymond et al., 2006), the SLU first stage is implemented by means of a word-to-concept transducer that translates a word lattice into a concept lattice. This concept lattice is rescored with a Language Model on the concepts (also encoded as FSMs with the AT&amp;T GRM toolkit (Allauzen et al., 2003)). The rule database of the SLU second stage is encoded as a transducer that takes as input concepts and output semantic interpretations γ. By applying this transducer to an FSM representing a concept lattice, we directly obtain a lattice of interpretations. The SLU process is therefore made of the composition of the ASR word lattice, two transducers (word-to-concepts and concept-to-interpretations) and an FSM representing a Language Model on the concepts. The concept LM is trained on the FT3000 corpus. This strategy push forward the approach developped at AT&amp;T in the How May I Help You? (Gori</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In 41st Annual Meeting of the Association for Computational Linguistics (ACL’03), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helene Bonneau-Maynard</author>
<author>Sophie Rosset</author>
<author>Christelle Ayache</author>
<author>Anne Kuhn</author>
<author>Djamel Mostefa</author>
</authors>
<title>Semantic annotation of the french media dialog corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the European Conference on Speech Communication and Technology (Eurospeech),</booktitle>
<location>Lisboa, Portugal.</location>
<contexts>
<context position="2894" citStr="Bonneau-Maynard et al., 2005" startWordPosition="445" endWordPosition="448">ologies, and the experience acquired in the development of WEB or Interactive Voice Response interfaces have facilitated the integration of speech modules in robust SDS. These new SDS can be deployed on a very large scale, like the France Telecom 3000 Voice Agency service considered in this study. With these services it is possible to obtain very large corpora of humanmachine interactions by collecting system logs. The main differences between these corpora and those collected in the framework of evaluation programs like the DARPA ATIS (Hemphill et al., 1990) or the French Technolangue MEDIA (Bonneau-Maynard et al., 2005) programs can be expressed through the following dimensions: • Size. There are virtually no limits in the amount of speakers available or the time needed for collecting the dialogues as thousands of dialogues are automatically processed every day and the system logs are stored. Therefore Dialog processing becomes similar 48 Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 48–55, NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics to Broadcast News processing: the limit is not in the amount of data available</context>
</contexts>
<marker>Bonneau-Maynard, Rosset, Ayache, Kuhn, Mostefa, 2005</marker>
<rawString>Helene Bonneau-Maynard, Sophie Rosset, Christelle Ayache, Anne Kuhn, and Djamel Mostefa. 2005. Semantic annotation of the french media dialog corpus. In Proceedings of the European Conference on Speech Communication and Technology (Eurospeech), Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geraldine Damnati</author>
<author>Frederic Bechet</author>
<author>Renato De Mori</author>
</authors>
<title>Spoken Language Understanding strategies on the France Telecom 3000 voice agency corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<location>Honolulu, USA.</location>
<marker>Damnati, Bechet, De Mori, 2007</marker>
<rawString>Geraldine Damnati, Frederic Bechet, and Renato De Mori. 2007. Spoken Language Understanding strategies on the France Telecom 3000 voice agency corpus. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), Honolulu, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>G Riccardi</author>
<author>J H Wright</author>
</authors>
<title>How May I Help You ?</title>
<date>1997</date>
<journal>In Speech Communication,</journal>
<volume>23</volume>
<pages>113--127</pages>
<contexts>
<context position="1841" citStr="Gorin et al., 1997" startWordPosition="276" endWordPosition="279">e Telecom 3000 Voice Agency service. This work is supported by the 6th Framework Research Programme of the European Union (EU), Project LUNA, IST contract no 33549. The authors would like to thank the EU for the financial support. For more information about the LUNA project, please visit the project home-page, www.ist-luna.eu. Fr´ed´eric B´echet Renato De Mori LIA University of Avignon AGROPARC, 339 ch. des Meinajaries 84911 Avignon Cedex 09, France frederic.bechet,renato.demori @univ-avignon.fr 1 Introduction Since the deployment on a very large scale of the AT&amp;T How May I Help You? (HMIHY) (Gorin et al., 1997) service in 2000, Spoken Dialogue Systems (SDS) handling a very large number of calls are now developed from an industrial point of view. Although a lot of the remaining problems (robustness, coverage, etc.) are still spoken language processing research problems, the conception and the deployment of such state-of-the-art systems mainly requires knowledge in user interfaces. The recent advances in speech recognition technologies, and the experience acquired in the development of WEB or Interactive Voice Response interfaces have facilitated the integration of speech modules in robust SDS. These </context>
<context position="21357" citStr="Gorin et al., 1997" startWordPosition="3587" endWordPosition="3590">003)). The rule database of the SLU second stage is encoded as a transducer that takes as input concepts and output semantic interpretations γ. By applying this transducer to an FSM representing a concept lattice, we directly obtain a lattice of interpretations. The SLU process is therefore made of the composition of the ASR word lattice, two transducers (word-to-concepts and concept-to-interpretations) and an FSM representing a Language Model on the concepts. The concept LM is trained on the FT3000 corpus. This strategy push forward the approach developped at AT&amp;T in the How May I Help You? (Gorin et al., 1997) project by using richer semantic models than call-types and named-entities models. More precisely, the 1600 Verbateam interpretation rules used in this study constitute a rich knowledge base. By integrating them into the search, thanks to the FSM paradigm, we can jointly optimize the search for the best sequence of words, basic concepts, and full semantic interpretations. For the strategy strat1 only the best path is kept in the FSM corresponding to the word lattice, simulating a sequential approach. For strat2 the best interpretation γ� is obtained on the whole concept lattice. error WER CER</context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A. L. Gorin, G. Riccardi, and J.H. Wright. 1997. How May I Help You ? In Speech Communication, volume 23, pages 113–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles T Hemphill</author>
<author>John J Godfrey</author>
<author>George R Doddington</author>
</authors>
<title>The ATIS spoken language systems pilot corpus.</title>
<date>1990</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>96--101</pages>
<location>Hidden Valley, Pennsylvania.</location>
<contexts>
<context position="2830" citStr="Hemphill et al., 1990" startWordPosition="436" endWordPosition="439">terfaces. The recent advances in speech recognition technologies, and the experience acquired in the development of WEB or Interactive Voice Response interfaces have facilitated the integration of speech modules in robust SDS. These new SDS can be deployed on a very large scale, like the France Telecom 3000 Voice Agency service considered in this study. With these services it is possible to obtain very large corpora of humanmachine interactions by collecting system logs. The main differences between these corpora and those collected in the framework of evaluation programs like the DARPA ATIS (Hemphill et al., 1990) or the French Technolangue MEDIA (Bonneau-Maynard et al., 2005) programs can be expressed through the following dimensions: • Size. There are virtually no limits in the amount of speakers available or the time needed for collecting the dialogues as thousands of dialogues are automatically processed every day and the system logs are stored. Therefore Dialog processing becomes similar 48 Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 48–55, NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics to Broadcast N</context>
</contexts>
<marker>Hemphill, Godfrey, Doddington, 1990</marker>
<rawString>Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS spoken language systems pilot corpus. In Proceedings of the workshop on Speech and Natural Language, pages 96–101, Hidden Valley, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer, Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="20407" citStr="Mohri et al., 2002" startWordPosition="3427" endWordPosition="3430"> be evaluated. The first one (strat1) is fully sequential: the best sequence of word W� is first obtained with W� = argmaxP(W JY ) W Then the best sequence of concepts C is obtained with C = argmaxP(Cl C Finally the interpretation rules are applied to C in order to obtain the best interpretation ry. The second strategy (strat2) is fully integrated: γ� is obtained by searching at the same time for W� and C� and ry. In this case we have: γ� = argmaxP(γJC)P(CJW)P(WJY ) W,C,-y The stochastic models proposed are implemented with a Finite State Machine (FSM) paradigm thanks to the AT&amp;T FSM toolkit (Mohri et al., 2002). Following the approach described in (Raymond et al., 2006), the SLU first stage is implemented by means of a word-to-concept transducer that translates a word lattice into a concept lattice. This concept lattice is rescored with a Language Model on the concepts (also encoded as FSMs with the AT&amp;T GRM toolkit (Allauzen et al., 2003)). The rule database of the SLU second stage is encoded as a transducer that takes as input concepts and output semantic interpretations γ. By applying this transducer to an FSM representing a concept lattice, we directly obtain a lattice of interpretations. The SL</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer, Speech and Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Raymond</author>
<author>Frederic Bechet</author>
<author>Renato De Mori</author>
<author>Geraldine Damnati</author>
</authors>
<title>On the use of finite state transducers for semantic interpretation.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<pages>48--3</pages>
<marker>Raymond, Bechet, De Mori, Damnati, 2006</marker>
<rawString>Christian Raymond, Frederic Bechet, Renato De Mori, and Geraldine Damnati. 2006. On the use of finite state transducers for semantic interpretation. Speech Communication, 48,3-4:288–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Riccardi</author>
<author>Allen L Gorin</author>
</authors>
<title>Language models for speech recognition and understanding.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Spoken Langage Processing (ICSLP),</booktitle>
<location>Sidney, Australia.</location>
<contexts>
<context position="24227" citStr="Riccardi and Gorin, 1998" startWordPosition="4091" endWordPosition="4094">ions, substitutions and deletions), a decrease in the overall error rate can be misleading as interpretation strategies can deal more easily with insertions than deletions or substitutions. Therefore the reduction of the overall WER and CER measures is not a reliable indicator of an increase of performance of the whole SLU module. level 1-best Oracle hyp. WER 33.7 20.0 CER 21.2 9.7 IER 13.0 4.4 Table 5: Error rates on words, concepts and interpretations for the 1-best hypothesis and for the Oracle hypothesis of each level These results have already been shown for WER by previous studies like (Riccardi and Gorin, 1998) IER from word Oracle 9.8 from concept Oracle 7.5 interpretation Oracle 4.4 Table 6: IER obtained on Oracle hypotheses computed at different levels. or more recently (Wang et al., 2003). They are illustrated by Table 5 and Table 6. The figures shown in these tables were computed on the subset of utterances that were passed to the SLU component. Utterances for which an OOD has been detected are discarded. In Table 5 are displayed the error rates obtained on words, concepts and interpretations both on the 1-best hypothesis and on the Oracle hypothesis (the one with the lowest error rate in the l</context>
</contexts>
<marker>Riccardi, Gorin, 1998</marker>
<rawString>Giuseppe Riccardi and Allen L. Gorin. 1998. Language models for speech recognition and understanding. In Proceedings of the International Conference on Spoken Langage Processing (ICSLP), Sidney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>A Acero</author>
<author>C Chelba</author>
</authors>
<title>Is word error rate a good indicator for spoken language understanding accuracy?</title>
<date>2003</date>
<booktitle>In Automatic Speech Recognition and Understanding workshop - ASRU’03, St. Thomas, US-Virgin Islands.</booktitle>
<contexts>
<context position="24412" citStr="Wang et al., 2003" startWordPosition="4122" endWordPosition="4125">erefore the reduction of the overall WER and CER measures is not a reliable indicator of an increase of performance of the whole SLU module. level 1-best Oracle hyp. WER 33.7 20.0 CER 21.2 9.7 IER 13.0 4.4 Table 5: Error rates on words, concepts and interpretations for the 1-best hypothesis and for the Oracle hypothesis of each level These results have already been shown for WER by previous studies like (Riccardi and Gorin, 1998) IER from word Oracle 9.8 from concept Oracle 7.5 interpretation Oracle 4.4 Table 6: IER obtained on Oracle hypotheses computed at different levels. or more recently (Wang et al., 2003). They are illustrated by Table 5 and Table 6. The figures shown in these tables were computed on the subset of utterances that were passed to the SLU component. Utterances for which an OOD has been detected are discarded. In Table 5 are displayed the error rates obtained on words, concepts and interpretations both on the 1-best hypothesis and on the Oracle hypothesis (the one with the lowest error rate in the lattice). These Oracle error rates were obtained by looking for the best hypothesis in the lattice obtained at the corresponding level (e.g. looking for the best sequence of concepts in </context>
</contexts>
<marker>Wang, Acero, Chelba, 2003</marker>
<rawString>Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is word error rate a good indicator for spoken language understanding accuracy? In Automatic Speech Recognition and Understanding workshop - ASRU’03, St. Thomas, US-Virgin Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Li Deng</author>
<author>Alex Acero</author>
</authors>
<title>Spoken language understanding.</title>
<date>2005</date>
<booktitle>In Signal Processing Magazine, IEEE,</booktitle>
<volume>22</volume>
<pages>16--31</pages>
<contexts>
<context position="18979" citStr="Wang et al., 2005" startWordPosition="3180" endWordPosition="3183">a Partially Observable Markov Decision Process (POMDP) is currently studied. If no dialog history is taken into account, P (Γ&amp;quot;JY ) comes down to P (-yik|Y ), -yik being a semantic attribute/value pair produced by the Verbateam interpretation rules. The integration of this semantic decoding process in the ASR process is presented in the next section. 5 Optimizing the ASR and SLU processes With the stochastic models proposed in section 4, different strategies can be built and optimized. We are interested here in the integration of the ASR and SLU processes. As already shown by previous studies (Wang et al., 2005), the traditional sequential approach that first looks for the best sequence of words 52 W before looking for the best interpretation γ� of an utterance is sub-optimal. Performing SLU on a word lattice output by the ASR module is an efficient way of integrating the search for the best sequence of words and the best interpretation. However there are real-time issues in processing word lattices in SDS, and therefore they are mainly used in research systems rather than deployed systems. In section 3 a strategy is proposed for selecting the utterances for which a word lattice is going to be produc</context>
</contexts>
<marker>Wang, Deng, Acero, 2005</marker>
<rawString>Ye-Yi Wang, Li Deng, and Alex Acero. 2005. Spoken language understanding. In Signal Processing Magazine, IEEE, volume 22, pages 16–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer, Speech and Language,</journal>
<volume>21</volume>
<pages>422</pages>
<contexts>
<context position="18338" citStr="Williams and Young, 2007" startWordPosition="3071" endWordPosition="3074">P (Sk(Gi)|Yk) = P (Gi|Γi )P (Γi k|Yk) (1) k where P (Gi|Γi) is the probability that Gi is the k type of goal that corresponds to the user interaction given the grounding predicates in Γi k. Yk is the acoustic features of the user’s utterance at turn k. Probabilities of states can be used to define a belief of the dialogue system. A first model allowing multiple dialog state sequence hypothesis is proposed in (Damnati et al., 2007). In this model each dialog state correspond to a system state in the dialog automaton. In order to deal with flexible dialog strategies and following previous work (Williams and Young, 2007), a new model based on a Partially Observable Markov Decision Process (POMDP) is currently studied. If no dialog history is taken into account, P (Γ&amp;quot;JY ) comes down to P (-yik|Y ), -yik being a semantic attribute/value pair produced by the Verbateam interpretation rules. The integration of this semantic decoding process in the ASR process is presented in the next section. 5 Optimizing the ASR and SLU processes With the stochastic models proposed in section 4, different strategies can be built and optimized. We are interested here in the integration of the ASR and SLU processes. As already show</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D. Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer, Speech and Language, 21:393– 422.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>