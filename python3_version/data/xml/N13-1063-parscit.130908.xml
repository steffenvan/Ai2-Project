<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.075222">
<title confidence="0.994436">
Compound Embedding Features for Semi-supervised Learning
</title>
<author confidence="0.998618">
Mo Yu1, Tiejun Zhao1, Daxiang Dong2, Hao Tian2 and Dianhai Yu2
</author>
<affiliation confidence="0.8799455">
Harbin Institute of Technology, Harbin, China
Baidu Inc., Beijing, China
</affiliation>
<email confidence="0.920152">
{yumo,tjzhao}@mtlab.hit.edu.cn
{dongdaxiang,tianhao,yudianhai}@baidu.com
</email>
<sectionHeader confidence="0.995327" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999760705882353">
To solve data sparsity problem, recently there
has been a trend in discriminative methods of
NLP to use representations of lexical items
learned from unlabeled data as features. In
this paper, we investigated the usage of word
representations learned by neural language
models, i.e. word embeddings. The direct us-
age has disadvantages such as large amount of
computation, inadequacy with dealing word
ambiguity and rare-words, and the problem of
linear non-separability. To overcome these
problems, we instead built compound features
from continuous word embeddings based on
clustering. Experiments showed that the com-
pound features not only improved the perfor-
mances on several NLP tasks, but also ran
faster, suggesting the potential of embeddings.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998942057692308">
Supervised learning methods have achieved great
successes in the field of Natural Language Pro-
cessing (NLP). However, in practice most methods
are usually limited by the problem of data sparsity,
since it is impossible to obtain sufficient labeled
data for all NLP tasks. In these situations semi-
supervised learning can help to make use of both
labeled data and easy-to-obtain unlabeled data.
The semi-supervised framework that is widely
applied to NLP is to first learn word representa-
tions, which are feature vectors of lexical items,
from unlabeled data and then plug them into a su-
pervised system. These methods are very effective
in utilizing large-scale unlabeled data and have
successfully improved performances of state-of-
the-art supervised systems on a variety of tasks
(Koo et al., 2008; Huang and Yates, 2009; TŠck-
stršm et al., 2012).
With the development of neural language mod-
els (NLM) (Bengio et al., 2003; Mnih and Hinton,
2009), recently researchers become interested in
word representations (also called word embed-
dings) learned by these models. Word embeddings
are dense low dimensional real-valued vectors.
They are composed of some latent features, which
are expected to capture useful syntactic and seman-
tic properties. Word embeddings are usually served
as the first layer in deep learning systems for NLP
(Collobert and Weston, 2008; Socher et al., 2011a,
2011b) and help these systems perform compara-
bly with the state-of-the-art models based on hand-
crafted features. They also have been directly
added as features to the state-of-the-art models of
chunking and NER, and have achieved significant
improvements (Turian et al. 2010).
Although the direct usage of continuous embed-
dings has been proved to be an effective method
for enhancing the state-of-the-art supervised mod-
els, it has some disadvantages, which made them
be out-performed by simpler Brown cluster fea-
tures (Turian et al, 2010) and made them computa-
tionally complicated. Firstly, embeddings of rare
words are insufficiently trained since they are only
updated few times and are close to their random
initial values. As shown in (Turian et al, 2010), this
is the main reason that models with embedding
features made more errors than those with Brown
cluster features. Secondly, in NLMs, each word
has its unique representation, so it is difficult to
represent different senses for ambiguous words.
Thirdly, word embeddings are unsuitable for linear
models in some tasks as will be proved in Section
</bodyText>
<page confidence="0.995267">
563
</page>
<subsectionHeader confidence="0.28902">
Proceedings of NAACL-HLT 2013, pages 563–568,
</subsectionHeader>
<bodyText confidence="0.992815836363637">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
4.2. This is possibly because in these tasks, either
the target labels are correlated with combinations
of different dimensions of word embeddings, or
discriminative information may be coded in differ-
ent intervals in the same dimension. So treating
embeddings directly as inputs to a linear model
could not fully utilize them. Moreover, since em-
beddings are dense vectors, it will introduce large
amount of computations when they are directly
used as inputs, making the method impractical.
In this paper, we first introduced the idea of
clustering embeddings to overcome the last two
disadvantages discussed above. The high-
dimensional cluster features make samples from
different classes better separated by linear models.
And models with these features can still run fast
because the clusters are sparse and discrete.
Second, we proposed the compound features
based on clustering. Compound features, which are
conjunctive features of neighboring words, have
been widely used in NLP models for improving the
performances because they are more discriminative.
Compound features of embeddings can also help a
model to better predict labels associated with rare-
words and ambiguous words, because compound
features composed of embeddings of nearby words
can help to better describe the property of these
words. Compound features are difficult to build on
dense embeddings. However they are easy to in-
duce from the sparse embedding clusters proposed
in this paper.
Experiments on chunking and NER showed that
based on the same embeddings, the compound fea-
tures managed to achieve better performances.
Moreover, we proposed analyses to reveal the rea-
sons for the improvements of embedding-clusters
and compound features. They suggest that these
features can better deal with rare-words and word
ambiguity, and are more suitable for linear models.
In addition, although Brown clustering was con-
sidered better in (Turian et al 2010), our experi-
ment results and comparisons showed that our
compound features from embedding clustering is at
least comparable with those from Brown clustering.
Since embeddings can greatly benefit from the im-
provement and developing of deep learning in the
future, we believe that our proposed method has a
large space of performance growth and will benefit
more applications in NLP.
In the rest of the paper, Section 2 introduces
how compound embedding features were obtained.
Section 3 gives experimental results. In Section 4,
we give analysis about the advantages of com-
pound features. Section 5 gives the conclusions.
</bodyText>
<subsectionHeader confidence="0.833417">
2 Clustering of Word Embeddings
2.1 Learning Word Embeddings
</subsectionHeader>
<bodyText confidence="0.999971">
Word embeddings in this paper were trained by
NLMs (Bengio et al., 2003). The model predicts
the scores of probabilities of words given their
context information in the sentences. It first con-
verts the current word and its context words (e.g.
n-1 words before it as in n-gram models) into em-
beddings. Then these embeddings are put together
and propagate forward on the network to compute
the score of current word. After minimizing the
loss on training data, embeddings are learned and
can be further used as smoothing representations
for words.
</bodyText>
<subsectionHeader confidence="0.99994">
2.2 Clustering of embeddings
</subsectionHeader>
<bodyText confidence="0.999978166666667">
In order to get compound features of embeddings,
we first induce discrete clusters from the embed-
dings. Concretely, the k-means clustering algo-
rithm is used. Each word is treated as a single
sample. A cluster is represented as the mean of the
embeddings of words assigned to it. Similarities
between words and clusters are measured by Eu-
clidean distance. As discussed and experimented
later, different numbers of ks contain information
of different granularity. So we combine clustering
results achieved by different ks as features to better
utilize the embeddings.
</bodyText>
<subsectionHeader confidence="0.998549">
2.3 Compound features
</subsectionHeader>
<bodyText confidence="0.999985461538462">
Based on embedding clusters, more powerful com-
pound features can be built. Compound features
are conjunctions between basic features of words
and their contexts, which are widely used in NLP.
Koo et al. (2008) also observed that compound
features of Brown clusters achieved more im-
provements on parsing.
It is also necessary to build compound embed-
ding features since they can better deal with rare-
words and ambiguous words. For example, alt-
hough embedding of a rare-word is not fully
trained and hence inaccurate, embeddings of its
context words can still be accurate as long as they
</bodyText>
<page confidence="0.988735">
564
</page>
<bodyText confidence="0.999798466666667">
are not rare and are fully trained. So we could uti-
lize the combination of embeddings before and
after the word to predict its tag correctly. We con-
ducted analysis to verify our theory in Section4.
We combined the compound features together
with other state-of-the-art human-craft features in
supervised models. Examples of the resulted fea-
ture templates in chunking and NER are shown in
Table 1 &amp; 2. The feature y−, yo c−, c, in the last
row is an example of compound feature made up
of the embedding clusters of words before and af-
ter current word. Compound feature extraction can
similarly be applied to form compound features of
Brown clusters. For example, Brown clusters can
replace embedding clusters in 3th row of Table 1.
</bodyText>
<tableCaption confidence="0.9806782">
Table 1: Chunking features. Cluster features are suitable
for both Brown clusters and embedding clusters. Sym-
bol yi is the tag predicted on word wi.
Table 2: NER features. Hyp indicates if word contains
hyphen and Cap indicates if first letter is capitalized.
</tableCaption>
<sectionHeader confidence="0.999387" genericHeader="introduction">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999324">
3.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999962425925926">
We tested our compound features on the same
chunking (CoNLL2000) and NER (CoNLL2003)
tasks in (Turian et al., 2010). The Brown cluster
features were used for comparison, which shared
the same feature template used by clusters of em-
beddings. To compare with the work of (Turian et
al, 2010), which aimed to solve the same problem
but using embedding directly, we used the same
word embeddings (CW 50) and Brown clusters
(1000 clusters) they provided. The embeddings in
(Turian et al, 2010) are trained on RCV corpus,
while the CoNLL2000 data is a part of the WSJ
corpus. Since we believe that word representations
trained on similar domain may better help to im-
prove the results, we also used embeddings and
Brown clusters trained on unlabeled WSJ data
from (Nivre et al, 2007) for comparison.
Moreover, we wish to find out whether our
method extends well to languages other than Eng-
lish. So we conducted experiments on Chinese
NER, where large amount of training data exists,
which makes improving accuracies more difficult.
We used data from People’s Daily (Jan.-Jun. 1998)
and converted them following the style of Penn
CTB (Xue et al, 2005). Data from April was cho-
sen as test set (1,309,616 words in 55,177 sentenc-
es), others for training (6,119,063 words in
255,951 sentences). The Chinese word representa-
tions were trained on Chinese Wikipedia until
March 2011. The features used in Chinese NER
are similar to those in English, except for the or-
thography, pre/suffixes, and chunking features.
We did little pre-processing work for the train-
ing of word representations on WSJ data. The da-
tasets were tokenized and capital words were kept.
For training of Chinese Wikipedia, we retained the
bodies of all articles and replaced words with fre-
quencies lower than 10 as an “UK_WORD” token.
On each dataset, we induced embeddings with 64
dimensions based on 7-gram models and 1000
Brown clusters. The method in (Schwenk, 2007)
was used to accelerate the training processes of
NLMs. All the NLMs were trained for 5 epochs.
For clustering of embeddings we choose k=500
and 2500 since such combination performed best
on development set as shown in the next section.
We chose the Sofia-ml toolkit (Sculley 2010) for
clustering of embeddings in order to save time.
In the experiments CRF models were used and
were optimized by ASGD (implemented by L6on
Bottou). For comparison we re-implemented the
direct usage of embeddings in (Turian et al, 2010)
with CRFsuite (Okazaki, 2007) since their features
contain continuous values.
</bodyText>
<subsectionHeader confidence="0.988455">
3.2 Performances
</subsectionHeader>
<bodyText confidence="0.999512">
Table 3 shows the chunking results. The results
reported in (Turian et al. 2010) were denoted as
“direct”. Based on the same word representations,
our compound features got better performances in
all cases. The embedding features trained on unla-
beled WSJ data yield further improvements, show-
</bodyText>
<equation confidence="0.989850571428572">
Words wi,i∈{−2:2}, wi − 1 wi , i∈{0 , 1 }
POS Aq−2:2 },A−1 A,F{−1, 2 }
Cluster ci, i f 2:2 ) , ci 1 ci , i f0 j) , c 1 c1
∈ − − ∈ −
Transition y−1 y0 {w0,p0,c0,c−1 c1}
Words wi,i∈{−2:2 },wi − 1 wi,i∈{ 0 ,1 }
Pre/suffix wo X2:4} , wo;�1:4}
Orthography Hyp(w0 ), Cap(wo )
POS A,�∈{−2 :2 },A−1 A,�∈{−1 , 2}
Chunking bi,i∈{−2 :2 ),bi−1 bi,i∈{−1 , 2 )
Cluster
ci ° i { 2:2 } ° ci 1 ci ° i {0 °1 } ° c 1 c1
∈ − − ∈ −
Transition y−1 y0 {w0,p0,c0,c−1 c1}
</equation>
<page confidence="0.992569">
565
</page>
<bodyText confidence="0.9904385">
ing that word representations from similar domains
can better help the supervised tasks.
</bodyText>
<table confidence="0.999220625">
System Direct Compound
Baseline 93.75
+Embedding (RCV) 94.10 94.19
+Brown (RCV) 94.11 94.24
+Brown&amp;Emb (RCV) 94.35 94.42
+Embedding (WSJ) 94.20 94.37
+Brown (WSJ) 94.25 94.36
+Brown&amp;Emb (WSJ) 94.43 94.58
</table>
<tableCaption confidence="0.999706">
Table 3: F1-scores of chunking
</tableCaption>
<bodyText confidence="0.999879833333333">
In the experiments of NER, first we evaluated
how the numbers of clusters k will affect the per-
formances on development set (Figure 1). The re-
sults showed that both the cluster features
(excluding all compound embedding features) and
compound features could achieve better results
than direct usage of the same embeddings. It also
showed that the performances did not vary much
when k was between 500 and 3000. When k=2500,
the result was a little higher than others. We finally
chose combination of k=500 and 2500, which
achieved best results on development set.
</bodyText>
<figureCaption confidence="0.999442">
Figure 1: Relation between numbers of clusters k and
performances on development set.
</figureCaption>
<bodyText confidence="0.999608666666667">
The performances of NER on test set are shown
in Table 4. Our baseline is slightly lower than that
in (Turian et al, 2010), because the first-order CRF
cannot utilize context information of NE tags.
Despite of this, same conclusions with chunking
held.
</bodyText>
<table confidence="0.9947508">
System Direct Compound
Baseline 83.78
+Embedding 87.38 88.46
+Brown 88.14 88.23
+Brown&amp;Embedding 88.85 89.06
</table>
<tableCaption confidence="0.9986626">
Table 4: F1-scores of English NER on test data
Performances on Chinese NER are shown in
Table 5. Similar results were observed as in Eng-
lish NER, showing that our method extends to oth-
er languages as well.
</tableCaption>
<table confidence="0.9997234">
System Direct Compound
Baseline 88.24
+Embedding 89.98 90.37
+Brown 90.24 90.55
+Brown&amp;Embedding 90.66 90.96
</table>
<tableCaption confidence="0.999622">
Table 5: F1-scores of Chinese NER on test data
</tableCaption>
<bodyText confidence="0.999713384615385">
Above results gave evidences that although clus-
tering embeddings may lose some information, the
derived compound features did have better perfor-
mances. The compound features can also improve
the performances of Brown clusters, but not as
much as they did on embeddings. And the combi-
nation of embedding-clusters and Brown-clusters
could further improve the performances, since they
made use of different type of context information.
The compound features also reduced the time
cost of using embedding features. For example, the
time for tagging one sentence in English NER was
reduced from 5.6 ms to 1.6 ms, shown in Table 6.
</bodyText>
<table confidence="0.9982565">
Embedding Time (ms)
Baseline 1.2
Embeddings (direct) 5.6
Embeddings (compound) 1.6
</table>
<tableCaption confidence="0.998515">
Table 6: Running time of different features
</tableCaption>
<sectionHeader confidence="0.969722" genericHeader="method">
4 Analysis
</sectionHeader>
<bodyText confidence="0.999995">
Our compound embedding features greatly out-
performed the direct usage of same embeddings on
English NER. In this section we conducted anal-
yses to show the reasons for the improvements.
</bodyText>
<subsectionHeader confidence="0.996908">
4.1 Rare-words and ambiguous words
</subsectionHeader>
<bodyText confidence="0.997429588235294">
To show the compound features have stronger abil-
ities to handle rare words, we counted the numbers
of errors made on words with different frequencies
on unlabeled data. Here the word frequencies are
from the results of Brown clustering provided by
(Turian et al. 2010). We compared our compound
embedding features with direct usage of embed-
dings as well as Brown clusters, which is believed
to work better on rare words. Figure 2(a) shows
that the compound features indeed resulted in few-
er errors than the two baseline methods in most
cases. Errors of embeddings occurred on words
with frequencies lower than 2K and those in the
range of 16 to 256 were reduced by 10.55% and
24.44%, respectively.
Our compound features also reduced the errors
caused by ambiguous words, as shown in Figure
</bodyText>
<page confidence="0.995917">
566
</page>
<bodyText confidence="0.999947">
2(b), where the numbers of senses for a word are
measured by the numbers of different POS tags it
has in Penn Treebank. 12.1% of the errors on am-
biguous words were reduced, comparing to 8.4%
of the errors on unambiguous ones.
</bodyText>
<figureCaption confidence="0.995083">
Figure 2: Errors incurred on words with different fre-
quencies (a) and ambiguous words (b) in NER.
</figureCaption>
<subsectionHeader confidence="0.990546">
4.2 Linear separability of embeddings
</subsectionHeader>
<bodyText confidence="0.999931">
Another reason for the good performances of com-
pound features on NER is that they made linear
models better separate named entities (NEs) and
non-NEs, which are more difficult to be linearly
separated when embeddings are directly used as
features. Here we designed an experiment to prove
this. Based on training data of CoNLL2003, a clas-
sification task was built to tell whether a word be-
longs to NE or not. Linear SVM and a non-linear
model Multilayer Perceptron (MLP) were used to
build the classifiers. As shown in Table 7, when
embeddings were directly used as features, MLP
performed much better than linear SVM. And the
linear model was under-fitting on this task since it
had similar accuracies on both training set and de-
velopment set. Above observations showed that
linear models could not separate NEs and non-NEs
well in the space of embeddings.
When clusters of embeddings were used as fea-
tures, the accuracies of linear models increased
even when there were only one or two non-zero
features for each sample. At the same time the per-
formances of MLP decreased because of the loss of
information during clustering. The gaps between
accuracies of linear models and non-linear ones
decreased in the spaces of clusters, showing that
cluster features are more suitable for linear models.
At last, the compound features made the linear
model out-perform all non-linear ones, since extra
context information could be utilized.
</bodyText>
<table confidence="0.99698725">
Embeddings Models Accuracy
direct linear 94.38
direct MLP 96.87
cluster 1000 linear 95.31
cluster 1000 MLP 95.32
cluster 500+2500 linear 96.10
cluster 500+2500 MLP 96.02
compound linear 97.30
</table>
<tableCaption confidence="0.9950625">
Table 7: Performances of linear and non-linear models
on development set with different embedding features.
</tableCaption>
<sectionHeader confidence="0.969555" genericHeader="conclusions">
5 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.999968789473685">
In this paper, we first introduced the idea of clus-
tering embeddings and then proposed the com-
pound features based on clustering, in order to
overcome the disadvantages of the direct usage of
continuous embeddings. Experiments showed that
the compound features built on the same original
word representation features (either embeddings or
Brown clusters) achieve better performances on the
same tasks. Further analyses showed that the com-
pound features reduced errors on rare-words and
ambiguous words and could be better utilized by
linear models.
The usage of word embeddings also has some
limitations, e.g. they are weak in capturing struc-
tural information of languages, which is necessary
in NLP. In the future, we will research on task-
specific representations for sub-structures, such as
phrases and sub-trees based on word embeddings
and documents representations (Xu et al., 2012).
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999144375">
We would like to thank Dr. Hua Wu, Haifeng
Wang, Jie Zhou and Rui Zhang for many discus-
sions and thank the anonymous reviewers for their
valuable suggestions. This work was supported by
National Natural Science Foundation of China
(61173073), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2011AA01A207).
</bodyText>
<page confidence="0.995128">
567
</page>
<sectionHeader confidence="0.990191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999636415584416">
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C.
(2003). A neural probabilistic language models. The
Journal of Machine Learning Research, 3:1137–
1155.
Collobert, R. and Weston, J. (2008). A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In
Proceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Finkel, J., Grenager, T., and Manning, C. (2005).
Incorporating non-local information into information
extraction systems by gibbs sampling. In
Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages
363–370. Association for Computational Linguistics.
Huang, F. and Yates, A. (2009). Distributional
representations for handling sparsity in supervised
sequence labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on
Natural Language Processing of the AFNLP:
Volume 1-Volume 1, pages 495–503. Association for
Computational Linguistics.
Koo, T., Carreras, X., and Collins, M. (2008). Simple
semi-supervised dependency parsing. In Proceed-
ings of Association for Computational Linguistics,
pages 595–603. Association for Computational
Linguistics.
Mnih, A. and Hinton, G. E. (2009). A scalable
hierarchical distributed language model. Advances in
neural information processing systems, 21:1081–
1088.
Nivre, J., Hall, J., Kübler, S., McDonald, R., Nilsson, J.,
Riedel, S., and Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task Session of EMNLP-
CoNLL, pages 915–932.
Okazaki, N. (2007). Crfsuite: a fast implementation of
conditional random fields (crfs). URL
http://www.chokkan.org/software/crfsuite.
Schwenk, H. (2007). Continuous space language models.
Computer Speech &amp; Language, 21(3):492–518.
Sculley, D. (2010). Web-scale k-means clustering. In
Proceedings of the 19th international conference on
World Wide Web, pages 1177–1178. ACM.
Socher, R., Huang, E., Pennington, J., Ng, A., and
Manning, C. (2011a). Dynamic pooling and
unfolding recursive auto-encoders for paraphrase
detection. Advances in Neural Information
Processing Systems, 24:801–809.
Socher, R., Pennington, J., Huang, E., Ng, A., and
Manning, C. (2011b). Semi-supervised recursive
auto-encoders for predicting sentiment distributions.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
151–161. Association for Computational Linguistics.
TŠckstršm, O., McDonald, R., and Uszkoreit, J. (2012).
Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, pages 477–487, Montréal, Canada,
June 3-8, 2012.
Turian, J., Ratinov, L., and Bengio, Y. (2010). Word
representations: a simple and general method for
semi-supervised learning. In Annual Meeting-
Association For Computational Linguistics. Urbana,
51:61801.
Xu, Z., Chen, M., Weinberger, K., and Sha, F. An
alternative text representation to TF-IDF and Bag-of-
Words. In Proceedings of 21st ACM Conf. of
Information and Knowledge Management (CIKM),
Hawaii, 2012.
Xue, N., Xia, F., Chiou, F., and Palmer, M. (2005). The
penn chinese treebank: Phrase structure annotation of
a large corpus. Natural Language Engineering,
11(2):207.
</reference>
<page confidence="0.996728">
568
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.641500">
<title confidence="0.992018">Compound Embedding Features for Semi-supervised Learning</title>
<author confidence="0.78061">Tiejun Daxiang Hao</author>
<author confidence="0.78061">Dianhai</author>
<affiliation confidence="0.9140105">Harbin Institute of Technology, Harbin, Baidu Inc., Beijing,</affiliation>
<email confidence="0.999753">dongdaxiang@baidu.com</email>
<email confidence="0.999753">tianhao@baidu.com</email>
<email confidence="0.999753">yudianhai@baidu.com</email>
<abstract confidence="0.999495222222222">To solve data sparsity problem, recently there has been a trend in discriminative methods of NLP to use representations of lexical items learned from unlabeled data as features. In this paper, we investigated the usage of word representations learned by neural language models, i.e. word embeddings. The direct usage has disadvantages such as large amount of computation, inadequacy with dealing word ambiguity and rare-words, and the problem of linear non-separability. To overcome these problems, we instead built compound features from continuous word embeddings based on clustering. Experiments showed that the compound features not only improved the performances on several NLP tasks, but also ran faster, suggesting the potential of embeddings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language models.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1155</pages>
<contexts>
<context position="1960" citStr="Bengio et al., 2003" startWordPosition="289" endWordPosition="292">rvised learning can help to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representations, which are feature vectors of lexical items, from unlabeled data and then plug them into a supervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; TŠckstršm et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features. They also have been directl</context>
<context position="6340" citStr="Bengio et al., 2003" startWordPosition="967" endWordPosition="970"> from Brown clustering. Since embeddings can greatly benefit from the improvement and developing of deep learning in the future, we believe that our proposed method has a large space of performance growth and will benefit more applications in NLP. In the rest of the paper, Section 2 introduces how compound embedding features were obtained. Section 3 gives experimental results. In Section 4, we give analysis about the advantages of compound features. Section 5 gives the conclusions. 2 Clustering of Word Embeddings 2.1 Learning Word Embeddings Word embeddings in this paper were trained by NLMs (Bengio et al., 2003). The model predicts the scores of probabilities of words given their context information in the sentences. It first converts the current word and its context words (e.g. n-1 words before it as in n-gram models) into embeddings. Then these embeddings are put together and propagate forward on the network to compute the score of current word. After minimizing the loss on training data, embeddings are learned and can be further used as smoothing representations for words. 2.2 Clustering of embeddings In order to get compound features of embeddings, we first induce discrete clusters from the embed</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic language models. The Journal of Machine Learning Research, 3:1137– 1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2396" citStr="Collobert and Weston, 2008" startWordPosition="354" endWordPosition="357">fthe-art supervised systems on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; TŠckstršm et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features. They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements (Turian et al. 2010). Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features (Turian et al, 2010) and made them computationally complic</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel, J., Grenager, T., and Manning, C. (2005). Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>A Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>495--503</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1860" citStr="Huang and Yates, 2009" startWordPosition="271" endWordPosition="274">nce it is impossible to obtain sufficient labeled data for all NLP tasks. In these situations semisupervised learning can help to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representations, which are feature vectors of lexical items, from unlabeled data and then plug them into a supervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; TŠckstršm et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform co</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Huang, F. and Yates, A. (2009). Distributional representations for handling sparsity in supervised sequence labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 495–503. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of Association for Computational Linguistics,</booktitle>
<pages>595--603</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1837" citStr="Koo et al., 2008" startWordPosition="267" endWordPosition="270"> data sparsity, since it is impossible to obtain sufficient labeled data for all NLP tasks. In these situations semisupervised learning can help to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representations, which are feature vectors of lexical items, from unlabeled data and then plug them into a supervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; TŠckstršm et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help t</context>
<context position="7640" citStr="Koo et al. (2008)" startWordPosition="1175" endWordPosition="1178">s a single sample. A cluster is represented as the mean of the embeddings of words assigned to it. Similarities between words and clusters are measured by Euclidean distance. As discussed and experimented later, different numbers of ks contain information of different granularity. So we combine clustering results achieved by different ks as features to better utilize the embeddings. 2.3 Compound features Based on embedding clusters, more powerful compound features can be built. Compound features are conjunctions between basic features of words and their contexts, which are widely used in NLP. Koo et al. (2008) also observed that compound features of Brown clusters achieved more improvements on parsing. It is also necessary to build compound embedding features since they can better deal with rarewords and ambiguous words. For example, although embedding of a rare-word is not fully trained and hence inaccurate, embeddings of its context words can still be accurate as long as they 564 are not rare and are fully trained. So we could utilize the combination of embeddings before and after the word to predict its tag correctly. We conducted analysis to verify our theory in Section4. We combined the compou</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Koo, T., Carreras, X., and Collins, M. (2008). Simple semi-supervised dependency parsing. In Proceedings of Association for Computational Linguistics, pages 595–603. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model. Advances in neural information processing systems,</title>
<date>2009</date>
<pages>21--1081</pages>
<contexts>
<context position="1984" citStr="Mnih and Hinton, 2009" startWordPosition="293" endWordPosition="296">elp to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representations, which are feature vectors of lexical items, from unlabeled data and then plug them into a supervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; TŠckstršm et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features. They also have been directly added as features to t</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Mnih, A. and Hinton, G. E. (2009). A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081– 1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kübler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLPCoNLL,</booktitle>
<pages>915--932</pages>
<contexts>
<context position="9827" citStr="Nivre et al, 2007" startWordPosition="1546" endWordPosition="1549">comparison, which shared the same feature template used by clusters of embeddings. To compare with the work of (Turian et al, 2010), which aimed to solve the same problem but using embedding directly, we used the same word embeddings (CW 50) and Brown clusters (1000 clusters) they provided. The embeddings in (Turian et al, 2010) are trained on RCV corpus, while the CoNLL2000 data is a part of the WSJ corpus. Since we believe that word representations trained on similar domain may better help to improve the results, we also used embeddings and Brown clusters trained on unlabeled WSJ data from (Nivre et al, 2007) for comparison. Moreover, we wish to find out whether our method extends well to languages other than English. So we conducted experiments on Chinese NER, where large amount of training data exists, which makes improving accuracies more difficult. We used data from People’s Daily (Jan.-Jun. 1998) and converted them following the style of Penn CTB (Xue et al, 2005). Data from April was chosen as test set (1,309,616 words in 55,177 sentences), others for training (6,119,063 words in 255,951 sentences). The Chinese word representations were trained on Chinese Wikipedia until March 2011. The feat</context>
</contexts>
<marker>Nivre, Hall, Kübler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Nivre, J., Hall, J., Kübler, S., McDonald, R., Nilsson, J., Riedel, S., and Yuret, D. (2007). The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLPCoNLL, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
</authors>
<title>Crfsuite: a fast implementation of conditional random fields (crfs).</title>
<date>2007</date>
<note>URL http://www.chokkan.org/software/crfsuite.</note>
<contexts>
<context position="11525" citStr="Okazaki, 2007" startWordPosition="1827" endWordPosition="1828">gram models and 1000 Brown clusters. The method in (Schwenk, 2007) was used to accelerate the training processes of NLMs. All the NLMs were trained for 5 epochs. For clustering of embeddings we choose k=500 and 2500 since such combination performed best on development set as shown in the next section. We chose the Sofia-ml toolkit (Sculley 2010) for clustering of embeddings in order to save time. In the experiments CRF models were used and were optimized by ASGD (implemented by L6on Bottou). For comparison we re-implemented the direct usage of embeddings in (Turian et al, 2010) with CRFsuite (Okazaki, 2007) since their features contain continuous values. 3.2 Performances Table 3 shows the chunking results. The results reported in (Turian et al. 2010) were denoted as “direct”. Based on the same word representations, our compound features got better performances in all cases. The embedding features trained on unlabeled WSJ data yield further improvements, showWords wi,i∈{−2:2}, wi − 1 wi , i∈{0 , 1 } POS Aq−2:2 },A−1 A,F{−1, 2 } Cluster ci, i f 2:2 ) , ci 1 ci , i f0 j) , c 1 c1 ∈ − − ∈ − Transition y−1 y0 {w0,p0,c0,c−1 c1} Words wi,i∈{−2:2 },wi − 1 wi,i∈{ 0 ,1 } Pre/suffix wo X2:4} , wo;�1:4} Ort</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Okazaki, N. (2007). Crfsuite: a fast implementation of conditional random fields (crfs). URL http://www.chokkan.org/software/crfsuite.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="10977" citStr="Schwenk, 2007" startWordPosition="1737" endWordPosition="1738">ons were trained on Chinese Wikipedia until March 2011. The features used in Chinese NER are similar to those in English, except for the orthography, pre/suffixes, and chunking features. We did little pre-processing work for the training of word representations on WSJ data. The datasets were tokenized and capital words were kept. For training of Chinese Wikipedia, we retained the bodies of all articles and replaced words with frequencies lower than 10 as an “UK_WORD” token. On each dataset, we induced embeddings with 64 dimensions based on 7-gram models and 1000 Brown clusters. The method in (Schwenk, 2007) was used to accelerate the training processes of NLMs. All the NLMs were trained for 5 epochs. For clustering of embeddings we choose k=500 and 2500 since such combination performed best on development set as shown in the next section. We chose the Sofia-ml toolkit (Sculley 2010) for clustering of embeddings in order to save time. In the experiments CRF models were used and were optimized by ASGD (implemented by L6on Bottou). For comparison we re-implemented the direct usage of embeddings in (Turian et al, 2010) with CRFsuite (Okazaki, 2007) since their features contain continuous values. 3.2</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Schwenk, H. (2007). Continuous space language models. Computer Speech &amp; Language, 21(3):492–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sculley</author>
</authors>
<title>Web-scale k-means clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World Wide Web,</booktitle>
<pages>1177--1178</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11258" citStr="Sculley 2010" startWordPosition="1784" endWordPosition="1785">asets were tokenized and capital words were kept. For training of Chinese Wikipedia, we retained the bodies of all articles and replaced words with frequencies lower than 10 as an “UK_WORD” token. On each dataset, we induced embeddings with 64 dimensions based on 7-gram models and 1000 Brown clusters. The method in (Schwenk, 2007) was used to accelerate the training processes of NLMs. All the NLMs were trained for 5 epochs. For clustering of embeddings we choose k=500 and 2500 since such combination performed best on development set as shown in the next section. We chose the Sofia-ml toolkit (Sculley 2010) for clustering of embeddings in order to save time. In the experiments CRF models were used and were optimized by ASGD (implemented by L6on Bottou). For comparison we re-implemented the direct usage of embeddings in (Turian et al, 2010) with CRFsuite (Okazaki, 2007) since their features contain continuous values. 3.2 Performances Table 3 shows the chunking results. The results reported in (Turian et al. 2010) were denoted as “direct”. Based on the same word representations, our compound features got better performances in all cases. The embedding features trained on unlabeled WSJ data yield f</context>
</contexts>
<marker>Sculley, 2010</marker>
<rawString>Sculley, D. (2010). Web-scale k-means clustering. In Proceedings of the 19th international conference on World Wide Web, pages 1177–1178. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E Huang</author>
<author>J Pennington</author>
<author>A Ng</author>
<author>C Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive auto-encoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>24--801</pages>
<contexts>
<context position="2417" citStr="Socher et al., 2011" startWordPosition="358" endWordPosition="361">on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; TŠckstršm et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features. They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements (Turian et al. 2010). Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features (Turian et al, 2010) and made them computationally complicated. Firstly, embedd</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Socher, R., Huang, E., Pennington, J., Ng, A., and Manning, C. (2011a). Dynamic pooling and unfolding recursive auto-encoders for paraphrase detection. Advances in Neural Information Processing Systems, 24:801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E Huang</author>
<author>A Ng</author>
<author>C Manning</author>
</authors>
<title>Semi-supervised recursive auto-encoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2417" citStr="Socher et al., 2011" startWordPosition="358" endWordPosition="361">on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; TŠckstršm et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features. They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements (Turian et al. 2010). Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features (Turian et al, 2010) and made them computationally complicated. Firstly, embedd</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Socher, R., Pennington, J., Huang, E., Ng, A., and Manning, C. (2011b). Semi-supervised recursive auto-encoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O TŠckstršm</author>
<author>R McDonald</author>
<author>J Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>477--487</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="1885" citStr="TŠckstršm et al., 2012" startWordPosition="275" endWordPosition="279"> obtain sufficient labeled data for all NLP tasks. In these situations semisupervised learning can help to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representations, which are feature vectors of lexical items, from unlabeled data and then plug them into a supervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; TŠckstršm et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform comparably with the state-o</context>
</contexts>
<marker>TŠckstršm, McDonald, Uszkoreit, 2012</marker>
<rawString>TŠckstršm, O., McDonald, R., and Uszkoreit, J. (2012). Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477–487, Montréal, Canada, June 3-8, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Annual MeetingAssociation For Computational Linguistics. Urbana,</booktitle>
<pages>51--61801</pages>
<contexts>
<context position="2695" citStr="Turian et al. 2010" startWordPosition="400" endWordPosition="403">s) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features. They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements (Turian et al. 2010). Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features (Turian et al, 2010) and made them computationally complicated. Firstly, embeddings of rare words are insufficiently trained since they are only updated few times and are close to their random initial values. As shown in (Turian et al, 2010), this is the main reason that models with embedding features made more errors than those with Brown cluster feature</context>
<context position="5586" citStr="Turian et al 2010" startWordPosition="847" endWordPosition="850"> Compound features are difficult to build on dense embeddings. However they are easy to induce from the sparse embedding clusters proposed in this paper. Experiments on chunking and NER showed that based on the same embeddings, the compound features managed to achieve better performances. Moreover, we proposed analyses to reveal the reasons for the improvements of embedding-clusters and compound features. They suggest that these features can better deal with rare-words and word ambiguity, and are more suitable for linear models. In addition, although Brown clustering was considered better in (Turian et al 2010), our experiment results and comparisons showed that our compound features from embedding clustering is at least comparable with those from Brown clustering. Since embeddings can greatly benefit from the improvement and developing of deep learning in the future, we believe that our proposed method has a large space of performance growth and will benefit more applications in NLP. In the rest of the paper, Section 2 introduces how compound embedding features were obtained. Section 3 gives experimental results. In Section 4, we give analysis about the advantages of compound features. Section 5 gi</context>
<context position="9166" citStr="Turian et al., 2010" startWordPosition="1432" endWordPosition="1435"> and after current word. Compound feature extraction can similarly be applied to form compound features of Brown clusters. For example, Brown clusters can replace embedding clusters in 3th row of Table 1. Table 1: Chunking features. Cluster features are suitable for both Brown clusters and embedding clusters. Symbol yi is the tag predicted on word wi. Table 2: NER features. Hyp indicates if word contains hyphen and Cap indicates if first letter is capitalized. 3 Experiments 3.1 Experimental settings We tested our compound features on the same chunking (CoNLL2000) and NER (CoNLL2003) tasks in (Turian et al., 2010). The Brown cluster features were used for comparison, which shared the same feature template used by clusters of embeddings. To compare with the work of (Turian et al, 2010), which aimed to solve the same problem but using embedding directly, we used the same word embeddings (CW 50) and Brown clusters (1000 clusters) they provided. The embeddings in (Turian et al, 2010) are trained on RCV corpus, while the CoNLL2000 data is a part of the WSJ corpus. Since we believe that word representations trained on similar domain may better help to improve the results, we also used embeddings and Brown cl</context>
<context position="11495" citStr="Turian et al, 2010" startWordPosition="1821" endWordPosition="1824">ings with 64 dimensions based on 7-gram models and 1000 Brown clusters. The method in (Schwenk, 2007) was used to accelerate the training processes of NLMs. All the NLMs were trained for 5 epochs. For clustering of embeddings we choose k=500 and 2500 since such combination performed best on development set as shown in the next section. We chose the Sofia-ml toolkit (Sculley 2010) for clustering of embeddings in order to save time. In the experiments CRF models were used and were optimized by ASGD (implemented by L6on Bottou). For comparison we re-implemented the direct usage of embeddings in (Turian et al, 2010) with CRFsuite (Okazaki, 2007) since their features contain continuous values. 3.2 Performances Table 3 shows the chunking results. The results reported in (Turian et al. 2010) were denoted as “direct”. Based on the same word representations, our compound features got better performances in all cases. The embedding features trained on unlabeled WSJ data yield further improvements, showWords wi,i∈{−2:2}, wi − 1 wi , i∈{0 , 1 } POS Aq−2:2 },A−1 A,F{−1, 2 } Cluster ci, i f 2:2 ) , ci 1 ci , i f0 j) , c 1 c1 ∈ − − ∈ − Transition y−1 y0 {w0,p0,c0,c−1 c1} Words wi,i∈{−2:2 },wi − 1 wi,i∈{ 0 ,1 } Pre/</context>
<context position="13430" citStr="Turian et al, 2010" startWordPosition="2167" endWordPosition="2170">d that both the cluster features (excluding all compound embedding features) and compound features could achieve better results than direct usage of the same embeddings. It also showed that the performances did not vary much when k was between 500 and 3000. When k=2500, the result was a little higher than others. We finally chose combination of k=500 and 2500, which achieved best results on development set. Figure 1: Relation between numbers of clusters k and performances on development set. The performances of NER on test set are shown in Table 4. Our baseline is slightly lower than that in (Turian et al, 2010), because the first-order CRF cannot utilize context information of NE tags. Despite of this, same conclusions with chunking held. System Direct Compound Baseline 83.78 +Embedding 87.38 88.46 +Brown 88.14 88.23 +Brown&amp;Embedding 88.85 89.06 Table 4: F1-scores of English NER on test data Performances on Chinese NER are shown in Table 5. Similar results were observed as in English NER, showing that our method extends to other languages as well. System Direct Compound Baseline 88.24 +Embedding 89.98 90.37 +Brown 90.24 90.55 +Brown&amp;Embedding 90.66 90.96 Table 5: F1-scores of Chinese NER on test dat</context>
<context position="15284" citStr="Turian et al. 2010" startWordPosition="2461" endWordPosition="2464">ng Time (ms) Baseline 1.2 Embeddings (direct) 5.6 Embeddings (compound) 1.6 Table 6: Running time of different features 4 Analysis Our compound embedding features greatly outperformed the direct usage of same embeddings on English NER. In this section we conducted analyses to show the reasons for the improvements. 4.1 Rare-words and ambiguous words To show the compound features have stronger abilities to handle rare words, we counted the numbers of errors made on words with different frequencies on unlabeled data. Here the word frequencies are from the results of Brown clustering provided by (Turian et al. 2010). We compared our compound embedding features with direct usage of embeddings as well as Brown clusters, which is believed to work better on rare words. Figure 2(a) shows that the compound features indeed resulted in fewer errors than the two baseline methods in most cases. Errors of embeddings occurred on words with frequencies lower than 2K and those in the range of 16 to 256 were reduced by 10.55% and 24.44%, respectively. Our compound features also reduced the errors caused by ambiguous words, as shown in Figure 566 2(b), where the numbers of senses for a word are measured by the numbers o</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Turian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: a simple and general method for semi-supervised learning. In Annual MeetingAssociation For Computational Linguistics. Urbana, 51:61801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Xu</author>
<author>M Chen</author>
<author>K Weinberger</author>
<author>F Sha</author>
</authors>
<title>An alternative text representation to TF-IDF and Bag-ofWords.</title>
<date>2012</date>
<booktitle>In Proceedings of 21st ACM Conf. of Information and Knowledge Management (CIKM),</booktitle>
<location>Hawaii,</location>
<marker>Xu, Chen, Weinberger, Sha, 2012</marker>
<rawString>Xu, Z., Chen, M., Weinberger, K., and Sha, F. An alternative text representation to TF-IDF and Bag-ofWords. In Proceedings of 21st ACM Conf. of Information and Knowledge Management (CIKM), Hawaii, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F Chiou</author>
<author>M Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="10194" citStr="Xue et al, 2005" startWordPosition="1606" endWordPosition="1609">oNLL2000 data is a part of the WSJ corpus. Since we believe that word representations trained on similar domain may better help to improve the results, we also used embeddings and Brown clusters trained on unlabeled WSJ data from (Nivre et al, 2007) for comparison. Moreover, we wish to find out whether our method extends well to languages other than English. So we conducted experiments on Chinese NER, where large amount of training data exists, which makes improving accuracies more difficult. We used data from People’s Daily (Jan.-Jun. 1998) and converted them following the style of Penn CTB (Xue et al, 2005). Data from April was chosen as test set (1,309,616 words in 55,177 sentences), others for training (6,119,063 words in 255,951 sentences). The Chinese word representations were trained on Chinese Wikipedia until March 2011. The features used in Chinese NER are similar to those in English, except for the orthography, pre/suffixes, and chunking features. We did little pre-processing work for the training of word representations on WSJ data. The datasets were tokenized and capital words were kept. For training of Chinese Wikipedia, we retained the bodies of all articles and replaced words with f</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Xue, N., Xia, F., Chiou, F., and Palmer, M. (2005). The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>