<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000094">
<title confidence="0.992206">
Incorporating Information Status into Generation Ranking
</title>
<author confidence="0.91755">
Aoife Cahill and Arndt Riester
</author>
<affiliation confidence="0.9509715">
Institut f¨ur Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
</affiliation>
<address confidence="0.548124">
70174 Stuttgart, Germany
</address>
<email confidence="0.998674">
{aoife.cahill,arndt.riester}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999546">
We investigate the influence of informa-
tion status (IS) on constituent order in Ger-
man, and integrate our findings into a log-
linear surface realisation ranking model.
We show that the distribution of pairs of IS
categories is strongly asymmetric. More-
over, each category is correlated with mor-
phosyntactic features, which can be au-
tomatically detected. We build a log-
linear model that incorporates these asym-
metries for ranking German string reali-
sations from input LFG F-structures. We
show that it achieves a statistically signif-
icantly higher BLEU score than the base-
line system without these features.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98808666">
There are many factors that influence word order,
e.g. humanness, definiteness, linear order of gram-
matical functions, givenness, focus, constituent
weight. In some cases, it can be relatively straight-
forward to automatically detect these features (i.e.
in the case of definiteness, this is a syntactic prop-
erty). The more complex the feature, the more dif-
ficult it is to automatically detect. It is common
knowledge that information status1 (henceforth,
IS) has a strong influence on syntax and word or-
der; for instance, in inversions, where the subject
follows some preposed element, Birner (1994) re-
ports that the preposed element must not be newer
in the discourse than the subject. We would like
to be able to use information related to IS in the
automatic generation of German text. Ideally, we
would automatically annotate text with IS labels
and learn from this data. Unfortunately, however,
to date, there has been little success in automati-
cally annotating text with IS.
1We take information status to be a subarea of information
structure; the one dealing with varieties of givenness but not
with contrast and focus in the strictest sense.
We believe, however, that despite this shortcom-
ing, we can still take advantage of some of the in-
sights gained from looking at the influence of IS
on word order. Specifically, we look at the prob-
lem from a more general perspective by comput-
ing an asymmetry ratio for each pair of IS cate-
gories. Results show that there are a large num-
ber of pairs exhibiting clear ordering preferences
when co-occurring in the same clause. The ques-
tion then becomes, without being able to auto-
matically detect these IS category pairs, can we,
nevertheless, take advantage of these strong asym-
metric patterns in generation. We investigate the
(automatically detectable) morphosyntactic char-
acteristics of each asymmetric IS pair and inte-
grate these syntactic asymmetric properties into
the generation process.
The paper is structured as follows: Section 2
outlines the underlying realisation ranking system
for our experiments. Section 3 introduces infor-
mation status and Section 4 describes how we ex-
tract and measure asymmetries in information sta-
tus. In Section 5, we examine the syntactic charac-
teristics of the IS asymmetries. Section 6 outlines
realisation ranking experiments to test the integra-
tion of IS into the system. We discuss our findings
in Section 7 and finally we conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.98912" genericHeader="introduction">
2 Generation Ranking
</sectionHeader>
<bodyText confidence="0.999952083333333">
The task we are considering is generation rank-
ing. In generation (or more specifically, surface
realisation) ranking, we take an abstract represen-
tation of a sentence (for example, as produced by
a machine translation or automatic summarisation
system), produce a number of alternative string
realisations corresponding to that input and use
some model to choose the most likely string. We
take the model outlined in Cahill et al. (2007), a
log-linear model based on the Lexical Functional
Grammar (LFG) Framework (Kaplan and Bres-
nan, 1982). LFG has two main levels of represen-
</bodyText>
<page confidence="0.956268">
817
</page>
<note confidence="0.826869333333333">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 817–825,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
CS 1: ROOT:1458
</note>
<figure confidence="0.998493285714286">
CProot[std]:1451
PERIOD:397
vor:154
DPx[std]:1956
N[comm]:1252
Nachbeben:263
AP[std,+infl]:1946
APx[std,+infl]:1928
A[+infl]:1039
möglichen:185
DP[std]:906
.:389
Cbar:1448
NP:1952
&amp;quot;Die Behörden warnten vor möglichen Nachbeben.&amp;quot;
4
15
263
CASE dat, NUM pl, PERS 3
PRED &apos;warnen&lt;[34:Behörde], [263:Nachbeben]&gt;&apos;
PRED &apos;Behörde&apos;
SUBJ
CASE nom, NUM pl, PERS 3
34
SPEC
DET PRED
&apos;die&apos;
&apos;vor&lt;[263:Nachbeben]&gt;&apos;
PRED &apos;Nachbeben&apos;
PRED &apos;möglich&lt;[263:Nachbeben]&gt;&apos;
[263:Nachbeben]
ATYPE attributive
PRED
OBL
OBJ
ADJUNCT
185
SUBJ
TNS-ASP MOOD indicative, TENSE past
117
TOPIC
[34:Behörde]
Cbar-flat:1436
DPx[std]:903
V[v,fin]:976
Vx[v,fin]:973
warnten:117
NP:738
D[std]:593
die:34
N[comm]:693
Behörden:85
PP[std]:2081
PPx[std]:2072
P[pre]:1013
DP[std]:1894
</figure>
<figureCaption confidence="0.999957">
Figure 1: An example C(onstituent) and F(unctional) Structure pair for (1)
</figureCaption>
<bodyText confidence="0.989062047619048">
tation, C(onstituent)-Structure and F(unctional)-
Structure. C-Structure is a context-free tree rep-
resentation that captures characteristics of the sur-
face string while F-Structure is an abstract repre-
sentation of the basic predicate-argument structure
of the string. An example C- and F-Structure pair
for the sentence in (1) is given in Figure 1.
‘The authorities warned of possible aftershocks.’
The input to the generation system is an F-
Structure. A hand-crafted, bi-directional LFG of
German (Rohrer and Forst, 2006) is used to gener-
ate all possible strings (licensed by the grammar)
for this input. As the grammar is hand-crafted,
it is designed only to parse (and therefore) gen-
erate grammatical strings.2 The task of the reali-
sation ranking system is then to choose the most
likely string. Cahill et al. (2007) describe a log-
linear model that uses linguistically motivated fea-
tures and improves over a simple tri-gram lan-
guage model baseline. We take this log-linear
model as our starting point.3
</bodyText>
<footnote confidence="0.987713">
2There are some rare instances of the grammar parsing
and therefore also generating ungrammatical output.
3Forst (2007) presents a model for parse disambiguation
that incorporates features such as humanness, definiteness,
linear order of grammatical functions, constituent weight.
Many of these features are already present in the Cahill et
al. (2007) model.
</footnote>
<bodyText confidence="0.969897074074074">
An error analysis of the output of that system
revealed that sometimes “unnatural” outputs were
being selected as most probable, and that often
information structural effects were the cause of
subtle differences in possible alternatives. For
instance, Example (3) appeared in the original
TIGER corpus with the 2 preceding sentences (2).
(2) Denn ausdr¨ucklich ist darin der rechtliche Maßstab
der Vorinstanz, des S¨achsischen Oberverwaltungs-
gerichtes, best¨atigt worden. Und der besagt: Die
Beteiligung am politischen Strafrecht der DDR, der
Mangel an kritischer Auseinandersetzung mit to-
talit¨aren ¨Uberzeugungen rechtfertigen den Ausschluss
von der Dritten Gewalt.
‘Because, the legal benchmark has explicitly been con-
firmed by the lower instance, the Saxonian Higher Ad-
ministrative Court. And it indicates: the participation
in the political criminal law of the GDR as well as
deficits regarding the critical debate on totalitarian con-
victions justify an expulsion from the judiciary.’
Vergangenheitsaufarbeitung
coming to terms with the past
gelernt.
learnt
‘People have learnt from dealing with the past mis-
takes.’
The five alternatives output by the grammar are:
</bodyText>
<listItem confidence="0.895359166666667">
a. Man hat aus der Vergangenheitsaufarbeitung gelernt.
b. Aus der Vergangenheitsaufarbeitung hat man gelernt.
c. Aus der Vergangenheitsaufarbeitung gelernt hat man.
d. Gelernt hat man aus der Vergangenheitsaufarbeitung.
e. Gelernt hat aus der Vergangenheitsaufarbeitung man.
(1) Die
</listItem>
<figure confidence="0.991263818181818">
the
Beh¨orden
authorities
warnten
warned
vor m¨oglichen
of possible
Nachbeben.
aftershocks
(3) Man hat aus der
one has out of the
</figure>
<page confidence="0.99433">
818
</page>
<bodyText confidence="0.96609332">
The string chosen as most likely by the system of
Cahill et al. (2007) is Alternative (b). No mat-
ter whether the context in (2) is available or the
sentence is presented without any context, there
seems to be a preference by native speakers for
the original string (a). Alternative (e) is extremely
marked4 to the point of being ungrammatical. Al-
ternative (c) is also very marked and so is Alterna-
tive (d), although less so than (c) and (e). Alter-
native (b) is a little more marked than the original
string, but it is easier to imagine a preceding con-
text where this sentence would be perfectly appro-
priate. Such a context would be, e.g. (4).
(4) Vergangenheitsaufarbeitung und Abwiegeln sind zwei
sehr unterschiedliche Arten, mit dem Geschehenen
umzugehen.
‘Dealing with the mistakes or playing them down are
two very different ways to handle the past.’
If we limit ourselves to single sentences, the
task for the model is then to choose the string that
is closest to the “default” expected word order (i.e.
appropriate in the most number of contexts). In
this work, we concentrate on integrating insights
from work on information status into the realisa-
tion ranking process.
</bodyText>
<sectionHeader confidence="0.996679" genericHeader="method">
3 Information Status
</sectionHeader>
<bodyText confidence="0.999981761904762">
The concept of information status (Prince, 1981;
Prince, 1992) involves classifying NP/PP/DP ex-
pressions in texts according to various ways of
their being given or new. It replaces and specifies
more clearly the often vaguely used term given-
ness. The process of labelling a corpus for IS can
be seen as a means of discourse analysis. Different
classification systems have been proposed in the
literature; see Riester (2008a) for a comparison of
several IS labelling schemes and Riester (2008b)
for a new proposal based on criteria from presup-
position theory. In the work described here, we
use the scheme of Riester (2008b). His main theo-
retic assumption is that IS categories (for definites)
should group expressions according to the contex-
tual resources in which their presuppositions find
an antecedent. For definites, the set of main cate-
gory labels found in Table 1 is assumed.
The idea of resolution contexts derives from
the concept of a presupposition trigger (e.g. a
definite description) as potentially establishing an
</bodyText>
<footnote confidence="0.9334635">
4By marked, we mean that there are relatively few or spe-
cialised contexts in which this sentence is acceptable.
</footnote>
<table confidence="0.999853230769231">
Context resource IS label
discourse D-GIVEN
context
encyclopedic/ ACCESSIBLE-GENERAL
knowledge
context
environment/ SITUATIVE
situative
context
bridging BRIDGING
context (scenario)
accommodation ACCESSIBLE-
(no context) DESCRIPTION
</table>
<tableCaption confidence="0.999773">
Table 1: IS classification for definites
</tableCaption>
<bodyText confidence="0.957836032258065">
anaphoric relation (van der Sandt, 1992) to an en-
tity being available by some means or other. But
there are some expressions whose referent cannot
be identified and needs to be accommodated, com-
pare (5).
(5) [die monatelange F¨uhrungskrise der Hamburger
Sozialdemokraten]ACC-DESC
‘the leadership crisis lasting for months among the
Hamburg Social Democrats’
Examples like this one have been mentioned
early on in the literature (e.g. Hawkins (1978),
Clark and Marshall (1981)). Nevertheless, label-
ing schemes so far have neglected this issue, which
is explicitly incorporated in the system of Riester
(2008b).
The status of an expression is ACCESSIBLE-
GENERAL (or unused, following Prince (1981))
if it is not present in the previous discourse but
refers to an entity that is known to the intended
recipent. There is a further differentiation of the
ACCESSIBLE-GENERAL class into generic (TYPE)
and non-generic (TOKEN) items.
An expression is D-GIVEN (or textually evoked)
if and only if an antecedent is available in the
discourse context. D-GIVEN entities are subdi-
vided according to whether they are repetitions of
their antecedent, short forms thereof, pronouns or
whether they use new linguistic material to add in-
formation about an already existing discourse ref-
erent (label: EPITHET). Examples representing a
co-reference chain are shown in (6).
</bodyText>
<listItem confidence="0.9962748">
(6) [Angela Merkel]ACC-GEN (first mention) ... [An-
gela Merkel]D-GIV-REPEATED (second mention) ...
[Merkel]D-GIV-SHORT . . . [she]D-GIV-PRONOUN . . .
[herself]D-GIV-REFLEXIVE . . . [the Hamburg-born
politician]D-GIV-EPITHET
</listItem>
<bodyText confidence="0.968982">
Indexicals (referring to entities in the environ-
ment context) are labeled as SITUATIVE. Definite
</bodyText>
<page confidence="0.922459">
819
</page>
<bodyText confidence="0.969357769230769">
items that can be identified within a scenario con-
text evoked by a non-coreferential item receive the
label BRIDGING; compare Example (7).
(7) In Sri Lanka haben tamilische Rebellen
in Sri Lanka have Tamil rebels
erstmals einen Luftangriff [gegen die
for the first time an airstrike against the
Streitkr¨afte]BRIDG geflogen.
armed forces flown.
’In Sri Lanka, Tamil rebels have, for the first time, car-
ried out an airstrike against the armed forces.’
In the indefinite domain, a simple classification
along the lines of Table 2 is proposed.
</bodyText>
<table confidence="0.999194333333333">
Type ISlabel
unrelated to context NEW
part-whole relation PARTITIVE
to previous entity
other (unspecified) INDEF-REL
relation to context
</table>
<tableCaption confidence="0.837538">
Table 2: IS classification for indefinites
</tableCaption>
<table confidence="0.999681423076923">
Dominant order (&gt;&gt;: “before”) B/A Total
D-GIV-PRO»INDEF-REL 0 19
D-GIV-PRO»D-GIV-CAT 0.1 11
D-GIV-REL»NEW 0.11 31
D-GIV-PRO»SIT 0.13 17
ACC-DESC»INDEF-REL 0.14 24
ACC-DESC&gt;&gt;ACC-GEN-TY 0.19 19
D-GIV-EPI»INDEF-REL 0.2 12
D-GIV-REP»NEW 0.21 23
D-GIV-PRO»ACC-GEN-TY 0.22 11
ACC-GEN-TO&gt;&gt;ACC-GEN-TY 0.24 42
D-GIV-PRO»ACC-DESC 0.24 46
EXPL»NEW 0.25 30
D-GIV-REL&gt;&gt;D-GIV-EPI 0.25 15
BRIDG-CONT&gt;&gt;PART-CONT 0.25 15
ACC-DESC»EXPL 0.29 27
D-GIV-PRO»D-GIV-REP 0.29 18
D-GIV-PRO»NEW 0.29 88
D-GIV-REL»ACC-DESC 0.3 26
SIT»EXPL 0.31 17
D-GIV-PRO»BRIDG-CONT 0.31 21
D-GIV-PRO»D-GIV-SHORT 0.32 29
. . . . . .
ACC-DESC&gt;&gt;ACC-GEN-TO 0.91 201
SIT&gt;&gt;BRIDG 0.92 23
EXPL»ACC-DESC 1 12
</table>
<tableCaption confidence="0.999871">
Table 3: Asymmetric pairs of IS labels
</tableCaption>
<bodyText confidence="0.999586666666667">
There are a few more subdivisions. Table 3,
for instance, contains the labels BRIDGING-CON-
TAINED and PARTITIVE-CONTAINED, going back
to Prince’s (1981:236) “containing inferrables”.
The entire IS label inventory used in this study
comprises 19 (sub)classes in total.
</bodyText>
<sectionHeader confidence="0.991078" genericHeader="method">
4 Asymmetries in IS
</sectionHeader>
<bodyText confidence="0.9996986875">
In order to find out whether IS categories are un-
evenly distributed within German sentences we
examine a corpus of German radio news bulletins
that has been manually annotated for IS (496 an-
notated sentences in total) using the scheme of
Riester (2008b).5
For each pair of IS labels X and Y we count
how often they co-occur in the corpus within a sin-
gle clause. In doing so, we distinguish the num-
bers for “X preceding Y ” (= A) and “Y preceding
X” (= B). The larger group is referred to as the
dominant order. Subsequently, we compute a ratio
indicating the degree of asymmetry between the
two orders. If, for instance, the dominant pattern
occurs 20 times (A) and the reverse pattern only 5
times (B), the asymmetry ratio B/A is 0.25.6
</bodyText>
<footnote confidence="0.964946625">
5The corpus was labeled by two independent annotators
and the results were compared by a third person who took
the final decision in case of disagreement. An evaluation as
regards inter-coder agreement is currently underway.
6Even if some of the sentences we are learning from are
marked in terms of word order, the ratios allow us to still learn
the predominant order, since the marked order should occur
much less frequently and the ratio will remain low.
</footnote>
<bodyText confidence="0.904813722222222">
Table 3 gives the top asymmetry pairs down to
a ratio of about 1:3 as well as, down at the bottom,
the pairs that are most evenly distributed. This
means that the top pairs exhibit strong ordering
preferences and are, hence, unevenly distributed
in German sentences. For instance, the ordering
D-GIVEN-PRONOUN before INDEF-REL (top line),
shown in Example (8), occurs 19 times in the ex-
amined corpus while there is no example in the
corpus for the reverse order.7
(8) [Sie]D-GIV-PRO w¨urde auch [bei verringerter
she would also at reduced
Anzahl]INDEF-REL jede vern¨unftige
number every sensible
Verteidigungsplanung sprengen.
defence planning blast
‘Even if the numbers were reduced it would blow every
sensible defence planning out of proportion.’
</bodyText>
<sectionHeader confidence="0.995692" genericHeader="method">
5 Syntactic IS Asymmetries
</sectionHeader>
<bodyText confidence="0.999941333333333">
It seems that IS could, in principle, be quite bene-
ficial in the generation ranking task. The problem,
of course, is that we do not possess any reliable
system of automatically assigning IS labels to un-
known text and manual annotations are costly and
time-consuming. As a substitute, we identify a list
</bodyText>
<footnote confidence="0.838166666666667">
7Note that we are not claiming that the reverse pattern is
ungrammatical or impossible, we just observe that it is ex-
tremely infrequent.
</footnote>
<page confidence="0.991433">
820
</page>
<bodyText confidence="0.999899677419355">
of morphosyntactic characteristics that the expres-
sions can adopt and investigate how these are cor-
related to our inventory of IS categories.
For some IS labels there is a direct link between
the typical phrases that fall into that IS category,
and the syntactic features that describe it. One
such example is D-GIVEN-PRONOUN, which al-
ways corresponds to a pronoun, or EXPL which
always corresponds to expletive items. Such syn-
tactic markers can easily be identified in the LFG
F-structures. On the other hand, there are many
IS labels for which there is no clear cut syntac-
tic class that describes its typical phrases. Ex-
amples include NEW, ACCESSIBLE-GENERAL or
ACCESSIBLE-DESCRIPTION.
In order to determine whether we can ascertain
a set of syntactic features that are representative
of a particular IS label, we design an inventory of
syntactic features that are found in all types of IS
phrases. The complete inventory is given in Table
5. It is a much easier task to identify these syntac-
tic characteristics than to try and automatically de-
tect IS labels directly, which would require a deep
semantic understanding of the text. We automati-
cally mark up the news corpus with these syntactic
characteristics, giving us a corpus both annotated
for IS and syntactic features.
We can now identify, for each IS label, what the
most frequent syntactic characteristics of that la-
bel are. Some examples and their frequencies are
given in Table 4.
</bodyText>
<table confidence="0.999417384615384">
Syntactic feature Count
D-GIVEN-PRONOUN
PERS PRON 39
DA PRON 25
DEMON PRON 19
GENERIC PRON 11
NEW
SIMPLE INDEF 113
INDEF ATTR 53
INDEF NUM 32
INDEF PPADJ 26
INDEF GEN 25
. . .
</table>
<tableCaption confidence="0.999782">
Table 4: Syntactic characteristics of IS labels
</tableCaption>
<bodyText confidence="0.984141666666667">
Combining the most frequent syntactic charac-
teristics with the asymmetries presented in Table 3
gives us Table 6.8
</bodyText>
<footnote confidence="0.7502525">
8For reasons of space, we are only showing the very top
of the table.
</footnote>
<sectionHeader confidence="0.980463" genericHeader="method">
6 Generation Ranking Experiments
</sectionHeader>
<bodyText confidence="0.999818714285714">
Using the augmented set of IS asymmetries,
we design new features to be included into the
original model of Cahill et al. (2007). For each
IS asymmetry, we extract all precedence patterns
of the corresponding syntactic features. For
example, from the first asymmetry in Table 6, we
extract the following features:
</bodyText>
<table confidence="0.868048">
PERS PRON precedes INDEF ATTR
PERS PRON precedes SIMPLE INDEF
DA PRON precedes INDEF ATTR
DA PRON precedes SIMPLE INDEF
DEMON PRON precedes INDEF ATTR
DEMON PRON precedes SIMPLE INDEF
GENERIC PRON precedes INDEF ATTR
GENERIC PRON precedes SIMPLE INDEF
</table>
<bodyText confidence="0.9998">
We extract these patterns for all of the asym-
metric pairs in Table 3 (augmented with syntac-
tic characteristics) that have a ratio &gt;0.4. The
patterns we extract need to be checked for incon-
sistencies because not all of them are valid. By
inconsistencies, we mean patterns of the type X
precedes X, Y precedes Y, and any pat-
tern where the variant X precedes Y as well
as Y precedes X is present. These are all auto-
matically removed from the list of features to give
a total of 130 new features for the log-linear rank-
ing model.
We train the log-linear ranking model on 7759
F-structures from the TIGER treebank. We gen-
erate strings from each F-structure and take the
original treebank string to be the labelled exam-
ple. All other examples are viewed as unlabelled.
We tune the parameters of the log-linear model on
a small development set of 63 sentences, and carry
out the final evaluation on 261 unseen sentences.
The ranking results of the model with the addi-
tional IS-inspired features are given in Table 7.
</bodyText>
<table confidence="0.502931">
Exact
Model BLEU Match
(%)
Cahill et al. (2007) 0.7366 52.49
New Model (Model 1) 0.7534 54.40
</table>
<tableCaption confidence="0.8568785">
Table 7: Ranking Results for new model with IS-
inspired syntactic asymmetry features.
</tableCaption>
<bodyText confidence="0.999911">
We evaluate the string chosen by the log-linear
model against the original treebank string in terms
of exact match and BLEU score (Papineni et al.,
</bodyText>
<page confidence="0.995972">
821
</page>
<table confidence="0.999894342105263">
Syntactic feature Type
Definites
Definite descriptions
SIMPLE DEF simple definite descriptions
POSS DEF simple definite descriptions with a possessive determiner
(pronoun or possibly genitive name)
DEF ATTR ADJ definite descriptions with adjectival modifier
DEF GENARG definite descriptions with a genitive argument
DEF PPADJ definite descriptions with a PP adjunct
DEF RELARG definite descriptions including a relative clause
DEF APP definite descriptions including a title or job description
as well as a proper name (e.g. an apposition)
Names
PROPER combinations of position/title and proper name (without article)
BARE PROPER bare proper names
Demonstrative descriptions
SIMPLE DEMON simple demonstrative descriptions
MOD DEMON adjectivally modified demonstrative descriptions
Pronouns
PERS PRON personal pronouns
EXPL PRON expletive pronoun
REFL PRON reflexive pronoun
DEMON PRON demonstrative pronouns (not: determiners)
GENERIC PRON generic pronoun (man – one)
DA PRON ”da”-pronouns (darauf, dar¨uber, dazu, ... )
LOC ADV location-referring pronouns
TEMP ADV,YEAR Dates and times
Indefinites
SIMPLE INDEF simple indefinites
NEG INDEF negative indefinites
INDEF ATTR indefinites with adjectival modifiers
INDEF CONTRAST indefinites with contrastive modifiers
(einige – some, andere – other, weitere – further, ... )
INDEF PPADJ indefinites with PP adjuncts
INDEF REL indefinites with relative clause adjunct
INDEF GEN indefinites with genitive adjuncts
INDEF NUM measure/number phrases
INDEF QUANT quantified indefinites
</table>
<tableCaption confidence="0.99764">
Table 5: An inventory of interesting syntactic characteristics in IS phrases
</tableCaption>
<table confidence="0.999763833333334">
Label 1 (+ features) Label 2 (+ features) B/A Total
D-GIVEN-PRONOUN INDEF-REL 0 19
PERS PRON 39 INDEF ATTR 23
DA PRON 25 SIMPLE INDEF 17
DEMON PRON 19
GENERIC PRON 11
D-GIVEN-PRONOUN D-GIVEN-CATAPHOR 0.1 11
PERS PRON 39 SIMPLE DEF 13
DA PRON 25 DA PRON 10
DEMON PRON 19
GENERIC PRON 11
D-GIVEN-REFLEXIVE NEW 0.11 31
REFL PRON 54 SIMPLE INDEF 113
INDEF ATTR 53
INDEF NUM 32
INDEF PPADJ 26
INDEF GEN 25
...
</table>
<tableCaption confidence="0.995732">
Table 6: IS asymmetric pairs augmented with syntactic characteristics
</tableCaption>
<page confidence="0.996522">
822
</page>
<bodyText confidence="0.998524392857143">
2002). We achieve an improvement of 0.0168
BLEU points and 1.91 percentage points in exact
match. The improvement in BLEU is statistically
significant (p &lt; 0.01) using the paired bootstrap
resampling significance test (Koehn, 2004).
Going back to Example (3), the new model
chooses a “better” string than the Cahill et al.
(2007) model. The new model chooses the orig-
inal string. While the string chosen by the Cahill
et al. (2007) system is also a perfectly valid sen-
tence, our empirical findings from the news corpus
were that the default order of generic pronoun be-
fore definite NP were more frequent. The system
with the new features helped to choose the original
string, as it had learnt this asymmetry.
Was it just the syntax?
The results in Table 7 clearly show that the new
model is beneficial. However, we want to know
how much of the improvement gained is due to
the IS asymmetries, and how much the syntactic
asymmetries on their own can contribute. To this
end, we carry out a further experiment where we
calculate syntactic asymmetries based on the au-
tomatic markup of the corpus, and ignore the IS
labels completely. Again we remove any incon-
sistent asymmetries and only choose asymmetries
with a ratio of higher than 0.4. The top asymme-
tries are given in Table 8.
</bodyText>
<table confidence="0.565066">
Dominant order (&gt;&gt;: “before”) B/A Total
</table>
<tableCaption confidence="0.991779">
Table 8: Purely syntactic asymmetries
</tableCaption>
<bodyText confidence="0.9994609">
For each asymmetry, we create a new feature X
precedes Y. This results in a total of 66 fea-
tures. Of these 30 overlap with the features used
in the above experiment. We do not include the
features extracted in the first attempt in this exper-
iment. The same training procedure is carried out
and we test on the same heldout test set of 261 sen-
tences. The results are given in Table 9. Finally,
we combine the two lists of features and evaluate,
these results are also presented in Table 9.
</bodyText>
<table confidence="0.926268857142857">
Exact
Model BLEU Match
(%)
Cahill et al. (2007) 0.7366 52.49
Model 1 0.7534 54.40
Synt.-asym.-based Model 0.7419 54.02
Combination 0.7437 53.64
</table>
<tableCaption confidence="0.9655615">
Table 9: Results for ranking model with purely
syntactic asymmetry features
</tableCaption>
<bodyText confidence="0.997080538461538">
They show that although the syntactic asymme-
tries alone contribute to an improvement over the
baseline, the gain is not as large as when the syn-
tactic asymmetries are constrained to correspond
to IS label asymmetries (Model 1).9 Interest-
ingly, the combination of the lists of features does
not result in an improvement over Model 1. The
difference in BLEU score between the model of
Cahill et al. (2007) and the model that only takes
syntactic-based asymmetries into account is not
statistically significant, while the difference be-
tween Model 1 and this model is statistically sig-
nificant (p &lt; 0.05).
</bodyText>
<sectionHeader confidence="0.997967" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.994349">
In the work described here, we concentrate only on
taking advantage of the information that is read-
ily available to us. Ideally, we would like to be
able to use the IS asymmetries directly as features,
however, without any means of automatically an-
notating new text with these categories, this is im-
possible. Our experiments were designed to test,
whether we can achieve an improvement in the
generation of German text, without a fully labelled
corpus, using the insight that at least some IS cate-
gories correspond to morphosyntactic characteris-
tics that can be easily identified. We do not claim
to go beyond this level to the point where true IS
labels would be used, rather we attempt to pro-
vide a crude approximation of IS using only mor-
phosyntactic information. To be able to fully auto-
matically annotate text with IS labels, one would
need to supplement the morphosyntactic features
9The difference may also be due to the fewer features used
in the second experiment. However, this emphasises, that
the asymmetries gleaned from syntactic information alone are
not strong enough to be able to determine the prevailing order
of constituents. When we take the IS labels into account, we
are honing in on a particular subset of interesting syntactic
asymmetries.
</bodyText>
<table confidence="0.9989974">
BAREPROPER&gt;&gt;INDEF NUM 0 33
DA PRON&gt;&gt;INDEF NUM 0 16
DEF PPADJ&gt;&gt;TEMP ADV 0 15
SIMPLE INDEF&gt;&gt;INDEF QUANT 0 14
PERS PRON&gt;&gt;INDEF ATTR 0 12
DEF PPADJ&gt;&gt;EXPL PRON 0 12
GENERIC PRON&gt;&gt;INDEF ATTR 0 12
REFL PRON&gt;&gt;YEAR 0 11
INDEF PPADJ&gt;&gt;INDEF NUM 0.02 57
DEF APP&gt;&gt;BAREPROPER 0.03 34
BAREPROPER&gt;&gt;TEMP ADV 0.04 26
TEMP ADV&gt;&gt;INDEF NUM 0.04 25
PROPER&gt;&gt;INDEF GEN 0.05 20
DEF GENARG&gt;&gt;INDEF ATTR 0.06 18
. . . . . .
</table>
<page confidence="0.998336">
823
</page>
<bodyText confidence="0.9999009125">
with information about anaphora resolution, world
knowledge, ontologies, and possibly even build
dynamic discourse representations.
We would also like to emphasise that we are
only looking at one sentence at a time. Of course,
there are other inter-sentential factors (not relying
on external resources) that play a role in choosing
the optimal string realisation, for example paral-
lelism or the position of the sentence in the para-
graph or text. Given that we only looked at IS fac-
tors within a sentence, we think that such a sig-
nificant improvement in BLEU and exact match
scores is very encouraging. In future work, we will
look at what information can be automatically ac-
quired to help generation ranking based on more
than one sentence.
While the experiments presented this paper are
limited to a German realisation ranking system,
there is nothing in the methodology that precludes
it from being applied to another language. The IS
annotation scheme is language-independent, and
so all one needs to be able to apply this to another
language is a corpus annotated with IS categories.
We extracted our IS asymmetry patterns from a
small corpus of spoken news items. This corpus
contains text of a similar domain to the TIGER
treebank. Further experiments are required to de-
termine how domain specific the asymmetries are.
Much related work on incorporating informa-
tion status (or information structure) into language
generation has been on spoken text, since infor-
mation structure is often encoded by means of
prosody. In a limited domain setting, Prevost
(1996) describes a two-tiered information struc-
ture representation. During the high level plan-
ning stage of generation, using a small knowl-
edge base, elements in the discourse are automat-
ically marked as new or given. Contrast and fo-
cus are also assigned automatically. These mark-
ings influence the final string generated. We are
focusing on a broad-coverage system, and do not
use any external world-knowledge resources. Van
Deemter and Odijk (1997) annotate the syntac-
tic component from which they are generating
with information about givenness. This informa-
tion is determined by detecting contradictions and
parallel sentences. Pulman (1997) also uses in-
formation about parallelism to predict word or-
der. In contrast, we only look at one sentence
when we approximate information status, future
work will look at cross sentential factors. Endriss
and Klabunde (2000) describe a sentence planner
for German that annotates the propositional in-
put with discourse-related features in order to de-
termine the focus, and thus influence word order
and accentuation. Their system, again, is domain-
specific (generating monologue describing a film
plot) and requires the existence of a knowledge
base. The same holds for Yampolska (2007), who
presents suggestions for generating information
structure in Russian and Ukrainian football re-
ports, using rules to determine parallel structures
for the placement of contrastive accent, following
similar work by Theune (1997). While our paper
does not address the generation of speech / accen-
tuation, it is of course conceivable to employ the
IS annotated radio news corpus from which we de-
rived the label asymmetries (and which also exists
in a spoken and prosodically annotated version) in
a similar task of learning the correlations between
IS labels and pitch accents. Finally, Bresnan et
al. (2007) present work on predicting the dative
alternation in English using 14 features relating to
information status which were manually annotated
in their corpus. In our work, we manually annotate
a small corpus in order to learn generalisations.
From these we learn features that approximate the
generalisations, enabling us to apply them to large
amounts of unseen data without further manual an-
notation.
</bodyText>
<sectionHeader confidence="0.999312" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999632666666667">
In this paper we presented a novel method of in-
cluding IS into the task of generation ranking.
Since automatic annotation of IS labels them-
selves is not currently possible, we approximate
the IS categories by their syntactic characteristics.
By calculating strong asymmetries between pairs
of IS labels, and establishing the most frequent
syntactic characteristics of these asymmetries, we
designed a new set of features for a log-linear
ranking model. In comparison to a baseline model,
we achieve statistically significant improvement in
BLEU score. We showed that these improvements
were not only due to the effect of purely syntac-
tic asymmetries, but that the IS asymmetries were
what drove the improved model.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997566">
This work was funded by the Collaborative Re-
search Centre (SFB 732) at the University of
Stuttgart.
</bodyText>
<page confidence="0.997594">
824
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999947404255319">
Betty J. Birner. 1994. Information Status and Word
Order: an Analysis of English Inversion. Language,
70(2):233–259.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and
R. Harald Baayen. 2007. Predicting the Dative Al-
ternation. Cognitive Foundations of Interpretation,
pages 69–94.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic Realisation Ranking for a Free Word Or-
der Language. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 17–24, Saarbr¨ucken, Germany. DFKI GmbH.
Herbert H. Clark and Catherine R. Marshall. 1981.
Definite Reference and Mutual Knowledge. In Ar-
avind Joshi, Bonnie Webber, and Ivan Sag, editors,
Elements of Discourse Understanding, pages 10–63.
Cambridge University Press.
Kees van Deemter and Jan Odijk. 1997. Context
Modeling and the Generation of Spoken Discourse.
Speech Communication, 21(1-2):101–121.
Cornelia Endriss and Ralf Klabunde. 2000. Planning
Word-Order Dependent Focus Assignments. In Pro-
ceedings of the First International Conference on
Natural Language Generation (INLG), pages 156–
162, Morristown, NJ. Association for Computa-
tional Linguistics.
Martin Forst. 2007. Disambiguation for a Linguis-
tically Precise German Parser. Ph.D. thesis, Uni-
versity of Stuttgart. Arbeitspapiere des Instituts
f¨ur Maschinelle Sprachverarbeitung (AIMS), Vol.
13(3).
John A. Hawkins. 1978. Definiteness and Indefinite-
ness: A Study in Reference and Grammaticality Pre-
diction. Croom Helm, London.
Ron Kaplan and Joan Bresnan. 1982. Lexical Func-
tional Grammar, a Formal System for Grammatical
Representation. In Joan Bresnan, editor, The Men-
tal Representation of Grammatical Relations, pages
173–281. MIT Press, Cambridge, MA.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2004), pages 388–395, Barcelona.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 311–
318, Philadelphia, PA.
Scott Prevost. 1996. An Information Structural Ap-
proach to Spoken Language Generation. In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 1996),
pages 294–301, Morristown, NJ.
Ellen F. Prince. 1981. Toward a Taxonomy of Given-
New Information. In P. Cole, editor, Radical Prag-
matics, pages 233–255. Academic Press, New York.
Ellen F. Prince. 1992. The ZPG Letter: Subjects, Def-
initeness and Information Status. In W. C. Mann
and S. A. Thompson, editors, Discourse Descrip-
tion: Diverse Linguistic Analyses of a Fund-Raising
Text, pages 295–325. Benjamins, Amsterdam.
Stephen G. Pulman. 1997. Higher Order Unification
and the Interpretation of Focus. Linguistics and Phi-
losophy, 20:73–115.
Arndt Riester. 2008a. A Semantic Explication of ’In-
formation Status’ and the Underspecification of the
Recipients’ Knowledge. In Atle Grønn, editor, Pro-
ceedings of Sinn und Bedeutung 12, University of
Oslo.
Arndt Riester. 2008b. The Components of Focus
and their Use in Annotating Information Struc-
ture. Ph.D. thesis, University of Stuttgart. Ar-
beitspapiere des Instituts f¨ur Maschinelle Sprachver-
arbeitung (AIMS), Vol. 14(2).
Christian Rohrer and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-Scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC 2006),
Genoa, Italy.
Rob van der Sandt. 1992. Presupposition Projection as
Anaphora Resolution. Journal of Semantics, 9:333–
377.
Mari¨et Theune. 1997. Goalgetter: Predicting Con-
trastive Accent in Data-to-Speech Generation. In
Proceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL/EACL
1997), pages 519–521, Madrid. Student paper.
Nadiya Yampolska. 2007. Information Structure in
Natural Language Generation: an Account for East-
Slavic Languages. Term paper. Universit¨at des Saar-
landes.
</reference>
<page confidence="0.998868">
825
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973916">
<title confidence="0.999841">Incorporating Information Status into Generation Ranking</title>
<author confidence="0.990341">Aoife Cahill</author>
<author confidence="0.990341">Arndt Riester</author>
<affiliation confidence="0.999666">Institut f¨ur Maschinelle Sprachverarbeitung (IMS) University of Stuttgart</affiliation>
<address confidence="0.99989">70174 Stuttgart, Germany</address>
<abstract confidence="0.999004125">We investigate the influence of information status (IS) on constituent order in German, and integrate our findings into a loglinear surface realisation ranking model. We show that the distribution of pairs of IS categories is strongly asymmetric. Moreover, each category is correlated with morphosyntactic features, which can be automatically detected. We build a loglinear model that incorporates these asymmetries for ranking German string realisations from input LFG F-structures. We show that it achieves a statistically significantly higher BLEU score than the baseline system without these features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Betty J Birner</author>
</authors>
<title>Information Status and Word Order: an Analysis of English Inversion.</title>
<date>1994</date>
<journal>Language,</journal>
<volume>70</volume>
<issue>2</issue>
<contexts>
<context position="1467" citStr="Birner (1994)" startWordPosition="214" endWordPosition="215"> Introduction There are many factors that influence word order, e.g. humanness, definiteness, linear order of grammatical functions, givenness, focus, constituent weight. In some cases, it can be relatively straightforward to automatically detect these features (i.e. in the case of definiteness, this is a syntactic property). The more complex the feature, the more difficult it is to automatically detect. It is common knowledge that information status1 (henceforth, IS) has a strong influence on syntax and word order; for instance, in inversions, where the subject follows some preposed element, Birner (1994) reports that the preposed element must not be newer in the discourse than the subject. We would like to be able to use information related to IS in the automatic generation of German text. Ideally, we would automatically annotate text with IS labels and learn from this data. Unfortunately, however, to date, there has been little success in automatically annotating text with IS. 1We take information status to be a subarea of information structure; the one dealing with varieties of givenness but not with contrast and focus in the strictest sense. We believe, however, that despite this shortcomi</context>
</contexts>
<marker>Birner, 1994</marker>
<rawString>Betty J. Birner. 1994. Information Status and Word Order: an Analysis of English Inversion. Language, 70(2):233–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Anna Cueni</author>
<author>Tatiana Nikitina</author>
<author>R Harald Baayen</author>
</authors>
<title>Predicting the Dative Alternation. Cognitive Foundations of Interpretation,</title>
<date>2007</date>
<pages>69--94</pages>
<contexts>
<context position="29872" citStr="Bresnan et al. (2007)" startWordPosition="4726" endWordPosition="4729">lska (2007), who presents suggestions for generating information structure in Russian and Ukrainian football reports, using rules to determine parallel structures for the placement of contrastive accent, following similar work by Theune (1997). While our paper does not address the generation of speech / accentuation, it is of course conceivable to employ the IS annotated radio news corpus from which we derived the label asymmetries (and which also exists in a spoken and prosodically annotated version) in a similar task of learning the correlations between IS labels and pitch accents. Finally, Bresnan et al. (2007) present work on predicting the dative alternation in English using 14 features relating to information status which were manually annotated in their corpus. In our work, we manually annotate a small corpus in order to learn generalisations. From these we learn features that approximate the generalisations, enabling us to apply them to large amounts of unseen data without further manual annotation. 8 Conclusions In this paper we presented a novel method of including IS into the task of generation ranking. Since automatic annotation of IS labels themselves is not currently possible, we approxim</context>
</contexts>
<marker>Bresnan, Cueni, Nikitina, Baayen, 2007</marker>
<rawString>Joan Bresnan, Anna Cueni, Tatiana Nikitina, and R. Harald Baayen. 2007. Predicting the Dative Alternation. Cognitive Foundations of Interpretation, pages 69–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Christian Rohrer</author>
</authors>
<title>Stochastic Realisation Ranking for a Free Word Order Language.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<publisher>DFKI GmbH.</publisher>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="3764" citStr="Cahill et al. (2007)" startWordPosition="587" endWordPosition="590">utlines realisation ranking experiments to test the integration of IS into the system. We discuss our findings in Section 7 and finally we conclude in Section 8. 2 Generation Ranking The task we are considering is generation ranking. In generation (or more specifically, surface realisation) ranking, we take an abstract representation of a sentence (for example, as produced by a machine translation or automatic summarisation system), produce a number of alternative string realisations corresponding to that input and use some model to choose the most likely string. We take the model outlined in Cahill et al. (2007), a log-linear model based on the Lexical Functional Grammar (LFG) Framework (Kaplan and Bresnan, 1982). LFG has two main levels of represen817 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 817–825, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP CS 1: ROOT:1458 CProot[std]:1451 PERIOD:397 vor:154 DPx[std]:1956 N[comm]:1252 Nachbeben:263 AP[std,+infl]:1946 APx[std,+infl]:1928 A[+infl]:1039 möglichen:185 DP[std]:906 .:389 Cbar:1448 NP:1952 &amp;quot;Die Behörden warnten vor möglichen Nachbeben.&amp;quot; 4 15 263 CASE dat, NUM pl, PERS 3 PRED &apos;warnen&lt;[34:Behörd</context>
<context position="5710" citStr="Cahill et al. (2007)" startWordPosition="863" endWordPosition="866">presentation of the basic predicate-argument structure of the string. An example C- and F-Structure pair for the sentence in (1) is given in Figure 1. ‘The authorities warned of possible aftershocks.’ The input to the generation system is an FStructure. A hand-crafted, bi-directional LFG of German (Rohrer and Forst, 2006) is used to generate all possible strings (licensed by the grammar) for this input. As the grammar is hand-crafted, it is designed only to parse (and therefore) generate grammatical strings.2 The task of the realisation ranking system is then to choose the most likely string. Cahill et al. (2007) describe a loglinear model that uses linguistically motivated features and improves over a simple tri-gram language model baseline. We take this log-linear model as our starting point.3 2There are some rare instances of the grammar parsing and therefore also generating ungrammatical output. 3Forst (2007) presents a model for parse disambiguation that incorporates features such as humanness, definiteness, linear order of grammatical functions, constituent weight. Many of these features are already present in the Cahill et al. (2007) model. An error analysis of the output of that system reveale</context>
<context position="7913" citStr="Cahill et al. (2007)" startWordPosition="1187" endWordPosition="1190"> the past gelernt. learnt ‘People have learnt from dealing with the past mistakes.’ The five alternatives output by the grammar are: a. Man hat aus der Vergangenheitsaufarbeitung gelernt. b. Aus der Vergangenheitsaufarbeitung hat man gelernt. c. Aus der Vergangenheitsaufarbeitung gelernt hat man. d. Gelernt hat man aus der Vergangenheitsaufarbeitung. e. Gelernt hat aus der Vergangenheitsaufarbeitung man. (1) Die the Beh¨orden authorities warnten warned vor m¨oglichen of possible Nachbeben. aftershocks (3) Man hat aus der one has out of the 818 The string chosen as most likely by the system of Cahill et al. (2007) is Alternative (b). No matter whether the context in (2) is available or the sentence is presented without any context, there seems to be a preference by native speakers for the original string (a). Alternative (e) is extremely marked4 to the point of being ungrammatical. Alternative (c) is also very marked and so is Alternative (d), although less so than (c) and (e). Alternative (b) is a little more marked than the original string, but it is easier to imagine a preceding context where this sentence would be perfectly appropriate. Such a context would be, e.g. (4). (4) Vergangenheitsaufarbeit</context>
<context position="18265" citStr="Cahill et al. (2007)" startWordPosition="2835" endWordPosition="2838">mples and their frequencies are given in Table 4. Syntactic feature Count D-GIVEN-PRONOUN PERS PRON 39 DA PRON 25 DEMON PRON 19 GENERIC PRON 11 NEW SIMPLE INDEF 113 INDEF ATTR 53 INDEF NUM 32 INDEF PPADJ 26 INDEF GEN 25 . . . Table 4: Syntactic characteristics of IS labels Combining the most frequent syntactic characteristics with the asymmetries presented in Table 3 gives us Table 6.8 8For reasons of space, we are only showing the very top of the table. 6 Generation Ranking Experiments Using the augmented set of IS asymmetries, we design new features to be included into the original model of Cahill et al. (2007). For each IS asymmetry, we extract all precedence patterns of the corresponding syntactic features. For example, from the first asymmetry in Table 6, we extract the following features: PERS PRON precedes INDEF ATTR PERS PRON precedes SIMPLE INDEF DA PRON precedes INDEF ATTR DA PRON precedes SIMPLE INDEF DEMON PRON precedes INDEF ATTR DEMON PRON precedes SIMPLE INDEF GENERIC PRON precedes INDEF ATTR GENERIC PRON precedes SIMPLE INDEF We extract these patterns for all of the asymmetric pairs in Table 3 (augmented with syntactic characteristics) that have a ratio &gt;0.4. The patterns we extract ne</context>
<context position="19760" citStr="Cahill et al. (2007)" startWordPosition="3093" endWordPosition="3096">e list of features to give a total of 130 new features for the log-linear ranking model. We train the log-linear ranking model on 7759 F-structures from the TIGER treebank. We generate strings from each F-structure and take the original treebank string to be the labelled example. All other examples are viewed as unlabelled. We tune the parameters of the log-linear model on a small development set of 63 sentences, and carry out the final evaluation on 261 unseen sentences. The ranking results of the model with the additional IS-inspired features are given in Table 7. Exact Model BLEU Match (%) Cahill et al. (2007) 0.7366 52.49 New Model (Model 1) 0.7534 54.40 Table 7: Ranking Results for new model with ISinspired syntactic asymmetry features. We evaluate the string chosen by the log-linear model against the original treebank string in terms of exact match and BLEU score (Papineni et al., 821 Syntactic feature Type Definites Definite descriptions SIMPLE DEF simple definite descriptions POSS DEF simple definite descriptions with a possessive determiner (pronoun or possibly genitive name) DEF ATTR ADJ definite descriptions with adjectival modifier DEF GENARG definite descriptions with a genitive argument </context>
<context position="22456" citStr="Cahill et al. (2007)" startWordPosition="3497" endWordPosition="3500">EN-CATAPHOR 0.1 11 PERS PRON 39 SIMPLE DEF 13 DA PRON 25 DA PRON 10 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-REFLEXIVE NEW 0.11 31 REFL PRON 54 SIMPLE INDEF 113 INDEF ATTR 53 INDEF NUM 32 INDEF PPADJ 26 INDEF GEN 25 ... Table 6: IS asymmetric pairs augmented with syntactic characteristics 822 2002). We achieve an improvement of 0.0168 BLEU points and 1.91 percentage points in exact match. The improvement in BLEU is statistically significant (p &lt; 0.01) using the paired bootstrap resampling significance test (Koehn, 2004). Going back to Example (3), the new model chooses a “better” string than the Cahill et al. (2007) model. The new model chooses the original string. While the string chosen by the Cahill et al. (2007) system is also a perfectly valid sentence, our empirical findings from the news corpus were that the default order of generic pronoun before definite NP were more frequent. The system with the new features helped to choose the original string, as it had learnt this asymmetry. Was it just the syntax? The results in Table 7 clearly show that the new model is beneficial. However, we want to know how much of the improvement gained is due to the IS asymmetries, and how much the syntactic asymmetri</context>
<context position="24020" citStr="Cahill et al. (2007)" startWordPosition="3778" endWordPosition="3781">le 8. Dominant order (&gt;&gt;: “before”) B/A Total Table 8: Purely syntactic asymmetries For each asymmetry, we create a new feature X precedes Y. This results in a total of 66 features. Of these 30 overlap with the features used in the above experiment. We do not include the features extracted in the first attempt in this experiment. The same training procedure is carried out and we test on the same heldout test set of 261 sentences. The results are given in Table 9. Finally, we combine the two lists of features and evaluate, these results are also presented in Table 9. Exact Model BLEU Match (%) Cahill et al. (2007) 0.7366 52.49 Model 1 0.7534 54.40 Synt.-asym.-based Model 0.7419 54.02 Combination 0.7437 53.64 Table 9: Results for ranking model with purely syntactic asymmetry features They show that although the syntactic asymmetries alone contribute to an improvement over the baseline, the gain is not as large as when the syntactic asymmetries are constrained to correspond to IS label asymmetries (Model 1).9 Interestingly, the combination of the lists of features does not result in an improvement over Model 1. The difference in BLEU score between the model of Cahill et al. (2007) and the model that only</context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>Aoife Cahill, Martin Forst, and Christian Rohrer. 2007. Stochastic Realisation Ranking for a Free Word Order Language. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 17–24, Saarbr¨ucken, Germany. DFKI GmbH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Catherine R Marshall</author>
</authors>
<title>Definite Reference and Mutual Knowledge.</title>
<date>1981</date>
<booktitle>Elements of Discourse Understanding,</booktitle>
<pages>10--63</pages>
<editor>In Aravind Joshi, Bonnie Webber, and Ivan Sag, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10928" citStr="Clark and Marshall (1981)" startWordPosition="1663" endWordPosition="1666">TIVE situative context bridging BRIDGING context (scenario) accommodation ACCESSIBLE(no context) DESCRIPTION Table 1: IS classification for definites anaphoric relation (van der Sandt, 1992) to an entity being available by some means or other. But there are some expressions whose referent cannot be identified and needs to be accommodated, compare (5). (5) [die monatelange F¨uhrungskrise der Hamburger Sozialdemokraten]ACC-DESC ‘the leadership crisis lasting for months among the Hamburg Social Democrats’ Examples like this one have been mentioned early on in the literature (e.g. Hawkins (1978), Clark and Marshall (1981)). Nevertheless, labeling schemes so far have neglected this issue, which is explicitly incorporated in the system of Riester (2008b). The status of an expression is ACCESSIBLEGENERAL (or unused, following Prince (1981)) if it is not present in the previous discourse but refers to an entity that is known to the intended recipent. There is a further differentiation of the ACCESSIBLE-GENERAL class into generic (TYPE) and non-generic (TOKEN) items. An expression is D-GIVEN (or textually evoked) if and only if an antecedent is available in the discourse context. D-GIVEN entities are subdivided acc</context>
</contexts>
<marker>Clark, Marshall, 1981</marker>
<rawString>Herbert H. Clark and Catherine R. Marshall. 1981. Definite Reference and Mutual Knowledge. In Aravind Joshi, Bonnie Webber, and Ivan Sag, editors, Elements of Discourse Understanding, pages 10–63. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Jan Odijk</author>
</authors>
<title>Context Modeling and the Generation of Spoken Discourse.</title>
<date>1997</date>
<journal>Speech Communication,</journal>
<pages>21--1</pages>
<marker>van Deemter, Odijk, 1997</marker>
<rawString>Kees van Deemter and Jan Odijk. 1997. Context Modeling and the Generation of Spoken Discourse. Speech Communication, 21(1-2):101–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelia Endriss</author>
<author>Ralf Klabunde</author>
</authors>
<title>Planning Word-Order Dependent Focus Assignments.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Conference on Natural Language Generation (INLG),</booktitle>
<pages>156--162</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ.</location>
<contexts>
<context position="28902" citStr="Endriss and Klabunde (2000)" startWordPosition="4575" endWordPosition="4578">lso assigned automatically. These markings influence the final string generated. We are focusing on a broad-coverage system, and do not use any external world-knowledge resources. Van Deemter and Odijk (1997) annotate the syntactic component from which they are generating with information about givenness. This information is determined by detecting contradictions and parallel sentences. Pulman (1997) also uses information about parallelism to predict word order. In contrast, we only look at one sentence when we approximate information status, future work will look at cross sentential factors. Endriss and Klabunde (2000) describe a sentence planner for German that annotates the propositional input with discourse-related features in order to determine the focus, and thus influence word order and accentuation. Their system, again, is domainspecific (generating monologue describing a film plot) and requires the existence of a knowledge base. The same holds for Yampolska (2007), who presents suggestions for generating information structure in Russian and Ukrainian football reports, using rules to determine parallel structures for the placement of contrastive accent, following similar work by Theune (1997). While </context>
</contexts>
<marker>Endriss, Klabunde, 2000</marker>
<rawString>Cornelia Endriss and Ralf Klabunde. 2000. Planning Word-Order Dependent Focus Assignments. In Proceedings of the First International Conference on Natural Language Generation (INLG), pages 156– 162, Morristown, NJ. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Forst</author>
</authors>
<title>Disambiguation for a Linguistically Precise German Parser.</title>
<date>2007</date>
<booktitle>Arbeitspapiere des Instituts f¨ur Maschinelle Sprachverarbeitung (AIMS),</booktitle>
<tech>Ph.D. thesis,</tech>
<volume>13</volume>
<issue>3</issue>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="6016" citStr="Forst (2007)" startWordPosition="912" endWordPosition="913">st, 2006) is used to generate all possible strings (licensed by the grammar) for this input. As the grammar is hand-crafted, it is designed only to parse (and therefore) generate grammatical strings.2 The task of the realisation ranking system is then to choose the most likely string. Cahill et al. (2007) describe a loglinear model that uses linguistically motivated features and improves over a simple tri-gram language model baseline. We take this log-linear model as our starting point.3 2There are some rare instances of the grammar parsing and therefore also generating ungrammatical output. 3Forst (2007) presents a model for parse disambiguation that incorporates features such as humanness, definiteness, linear order of grammatical functions, constituent weight. Many of these features are already present in the Cahill et al. (2007) model. An error analysis of the output of that system revealed that sometimes “unnatural” outputs were being selected as most probable, and that often information structural effects were the cause of subtle differences in possible alternatives. For instance, Example (3) appeared in the original TIGER corpus with the 2 preceding sentences (2). (2) Denn ausdr¨ucklich</context>
</contexts>
<marker>Forst, 2007</marker>
<rawString>Martin Forst. 2007. Disambiguation for a Linguistically Precise German Parser. Ph.D. thesis, University of Stuttgart. Arbeitspapiere des Instituts f¨ur Maschinelle Sprachverarbeitung (AIMS), Vol. 13(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Hawkins</author>
</authors>
<title>Definiteness and Indefiniteness: A Study in Reference and Grammaticality Prediction. Croom Helm,</title>
<date>1978</date>
<location>London.</location>
<contexts>
<context position="10901" citStr="Hawkins (1978)" startWordPosition="1661" endWordPosition="1662">vironment/ SITUATIVE situative context bridging BRIDGING context (scenario) accommodation ACCESSIBLE(no context) DESCRIPTION Table 1: IS classification for definites anaphoric relation (van der Sandt, 1992) to an entity being available by some means or other. But there are some expressions whose referent cannot be identified and needs to be accommodated, compare (5). (5) [die monatelange F¨uhrungskrise der Hamburger Sozialdemokraten]ACC-DESC ‘the leadership crisis lasting for months among the Hamburg Social Democrats’ Examples like this one have been mentioned early on in the literature (e.g. Hawkins (1978), Clark and Marshall (1981)). Nevertheless, labeling schemes so far have neglected this issue, which is explicitly incorporated in the system of Riester (2008b). The status of an expression is ACCESSIBLEGENERAL (or unused, following Prince (1981)) if it is not present in the previous discourse but refers to an entity that is known to the intended recipent. There is a further differentiation of the ACCESSIBLE-GENERAL class into generic (TYPE) and non-generic (TOKEN) items. An expression is D-GIVEN (or textually evoked) if and only if an antecedent is available in the discourse context. D-GIVEN </context>
</contexts>
<marker>Hawkins, 1978</marker>
<rawString>John A. Hawkins. 1978. Definiteness and Indefiniteness: A Study in Reference and Grammaticality Prediction. Croom Helm, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical Functional Grammar, a Formal System for Grammatical Representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>In Joan Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3867" citStr="Kaplan and Bresnan, 1982" startWordPosition="602" endWordPosition="606">our findings in Section 7 and finally we conclude in Section 8. 2 Generation Ranking The task we are considering is generation ranking. In generation (or more specifically, surface realisation) ranking, we take an abstract representation of a sentence (for example, as produced by a machine translation or automatic summarisation system), produce a number of alternative string realisations corresponding to that input and use some model to choose the most likely string. We take the model outlined in Cahill et al. (2007), a log-linear model based on the Lexical Functional Grammar (LFG) Framework (Kaplan and Bresnan, 1982). LFG has two main levels of represen817 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 817–825, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP CS 1: ROOT:1458 CProot[std]:1451 PERIOD:397 vor:154 DPx[std]:1956 N[comm]:1252 Nachbeben:263 AP[std,+infl]:1946 APx[std,+infl]:1928 A[+infl]:1039 möglichen:185 DP[std]:906 .:389 Cbar:1448 NP:1952 &amp;quot;Die Behörden warnten vor möglichen Nachbeben.&amp;quot; 4 15 263 CASE dat, NUM pl, PERS 3 PRED &apos;warnen&lt;[34:Behörde], [263:Nachbeben]&gt;&apos; PRED &apos;Behörde&apos; SUBJ CASE nom, NUM pl, PERS 3 34 SPEC DET PRED &apos;die&apos; &apos;vor&lt;[263:Nac</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Ron Kaplan and Joan Bresnan. 1982. Lexical Functional Grammar, a Formal System for Grammatical Representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173–281. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona.</location>
<contexts>
<context position="22358" citStr="Koehn, 2004" startWordPosition="3481" endWordPosition="3482">DEF ATTR 23 DA PRON 25 SIMPLE INDEF 17 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-PRONOUN D-GIVEN-CATAPHOR 0.1 11 PERS PRON 39 SIMPLE DEF 13 DA PRON 25 DA PRON 10 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-REFLEXIVE NEW 0.11 31 REFL PRON 54 SIMPLE INDEF 113 INDEF ATTR 53 INDEF NUM 32 INDEF PPADJ 26 INDEF GEN 25 ... Table 6: IS asymmetric pairs augmented with syntactic characteristics 822 2002). We achieve an improvement of 0.0168 BLEU points and 1.91 percentage points in exact match. The improvement in BLEU is statistically significant (p &lt; 0.01) using the paired bootstrap resampling significance test (Koehn, 2004). Going back to Example (3), the new model chooses a “better” string than the Cahill et al. (2007) model. The new model chooses the original string. While the string chosen by the Cahill et al. (2007) system is also a perfectly valid sentence, our empirical findings from the news corpus were that the default order of generic pronoun before definite NP were more frequent. The system with the new features helped to choose the original string, as it had learnt this asymmetry. Was it just the syntax? The results in Table 7 clearly show that the new model is beneficial. However, we want to know how</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 388–395, Barcelona. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311– 318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Prevost</author>
</authors>
<title>An Information Structural Approach to Spoken Language Generation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>294--301</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="28039" citStr="Prevost (1996)" startWordPosition="4444" endWordPosition="4445">eme is language-independent, and so all one needs to be able to apply this to another language is a corpus annotated with IS categories. We extracted our IS asymmetry patterns from a small corpus of spoken news items. This corpus contains text of a similar domain to the TIGER treebank. Further experiments are required to determine how domain specific the asymmetries are. Much related work on incorporating information status (or information structure) into language generation has been on spoken text, since information structure is often encoded by means of prosody. In a limited domain setting, Prevost (1996) describes a two-tiered information structure representation. During the high level planning stage of generation, using a small knowledge base, elements in the discourse are automatically marked as new or given. Contrast and focus are also assigned automatically. These markings influence the final string generated. We are focusing on a broad-coverage system, and do not use any external world-knowledge resources. Van Deemter and Odijk (1997) annotate the syntactic component from which they are generating with information about givenness. This information is determined by detecting contradiction</context>
</contexts>
<marker>Prevost, 1996</marker>
<rawString>Scott Prevost. 1996. An Information Structural Approach to Spoken Language Generation. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL 1996), pages 294–301, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen F Prince</author>
</authors>
<title>Toward a Taxonomy of GivenNew Information.</title>
<date>1981</date>
<booktitle>Radical Pragmatics,</booktitle>
<pages>233--255</pages>
<editor>In P. Cole, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="9088" citStr="Prince, 1981" startWordPosition="1386" endWordPosition="1387">e.g. (4). (4) Vergangenheitsaufarbeitung und Abwiegeln sind zwei sehr unterschiedliche Arten, mit dem Geschehenen umzugehen. ‘Dealing with the mistakes or playing them down are two very different ways to handle the past.’ If we limit ourselves to single sentences, the task for the model is then to choose the string that is closest to the “default” expected word order (i.e. appropriate in the most number of contexts). In this work, we concentrate on integrating insights from work on information status into the realisation ranking process. 3 Information Status The concept of information status (Prince, 1981; Prince, 1992) involves classifying NP/PP/DP expressions in texts according to various ways of their being given or new. It replaces and specifies more clearly the often vaguely used term givenness. The process of labelling a corpus for IS can be seen as a means of discourse analysis. Different classification systems have been proposed in the literature; see Riester (2008a) for a comparison of several IS labelling schemes and Riester (2008b) for a new proposal based on criteria from presupposition theory. In the work described here, we use the scheme of Riester (2008b). His main theoretic ass</context>
<context position="11147" citStr="Prince (1981)" startWordPosition="1698" endWordPosition="1699">ns or other. But there are some expressions whose referent cannot be identified and needs to be accommodated, compare (5). (5) [die monatelange F¨uhrungskrise der Hamburger Sozialdemokraten]ACC-DESC ‘the leadership crisis lasting for months among the Hamburg Social Democrats’ Examples like this one have been mentioned early on in the literature (e.g. Hawkins (1978), Clark and Marshall (1981)). Nevertheless, labeling schemes so far have neglected this issue, which is explicitly incorporated in the system of Riester (2008b). The status of an expression is ACCESSIBLEGENERAL (or unused, following Prince (1981)) if it is not present in the previous discourse but refers to an entity that is known to the intended recipent. There is a further differentiation of the ACCESSIBLE-GENERAL class into generic (TYPE) and non-generic (TOKEN) items. An expression is D-GIVEN (or textually evoked) if and only if an antecedent is available in the discourse context. D-GIVEN entities are subdivided according to whether they are repetitions of their antecedent, short forms thereof, pronouns or whether they use new linguistic material to add information about an already existing discourse referent (label: EPITHET). Exa</context>
</contexts>
<marker>Prince, 1981</marker>
<rawString>Ellen F. Prince. 1981. Toward a Taxonomy of GivenNew Information. In P. Cole, editor, Radical Pragmatics, pages 233–255. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen F Prince</author>
</authors>
<title>The ZPG Letter: Subjects, Definiteness and Information Status.</title>
<date>1992</date>
<booktitle>Discourse Description: Diverse Linguistic Analyses of a Fund-Raising Text,</booktitle>
<pages>295--325</pages>
<editor>In W. C. Mann and S. A. Thompson, editors,</editor>
<publisher>Benjamins,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="9103" citStr="Prince, 1992" startWordPosition="1388" endWordPosition="1389">Vergangenheitsaufarbeitung und Abwiegeln sind zwei sehr unterschiedliche Arten, mit dem Geschehenen umzugehen. ‘Dealing with the mistakes or playing them down are two very different ways to handle the past.’ If we limit ourselves to single sentences, the task for the model is then to choose the string that is closest to the “default” expected word order (i.e. appropriate in the most number of contexts). In this work, we concentrate on integrating insights from work on information status into the realisation ranking process. 3 Information Status The concept of information status (Prince, 1981; Prince, 1992) involves classifying NP/PP/DP expressions in texts according to various ways of their being given or new. It replaces and specifies more clearly the often vaguely used term givenness. The process of labelling a corpus for IS can be seen as a means of discourse analysis. Different classification systems have been proposed in the literature; see Riester (2008a) for a comparison of several IS labelling schemes and Riester (2008b) for a new proposal based on criteria from presupposition theory. In the work described here, we use the scheme of Riester (2008b). His main theoretic assumption is that</context>
</contexts>
<marker>Prince, 1992</marker>
<rawString>Ellen F. Prince. 1992. The ZPG Letter: Subjects, Definiteness and Information Status. In W. C. Mann and S. A. Thompson, editors, Discourse Description: Diverse Linguistic Analyses of a Fund-Raising Text, pages 295–325. Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen G Pulman</author>
</authors>
<title>Higher Order Unification and the Interpretation of Focus. Linguistics and Philosophy,</title>
<date>1997</date>
<pages>20--73</pages>
<contexts>
<context position="28678" citStr="Pulman (1997)" startWordPosition="4541" endWordPosition="4542">formation structure representation. During the high level planning stage of generation, using a small knowledge base, elements in the discourse are automatically marked as new or given. Contrast and focus are also assigned automatically. These markings influence the final string generated. We are focusing on a broad-coverage system, and do not use any external world-knowledge resources. Van Deemter and Odijk (1997) annotate the syntactic component from which they are generating with information about givenness. This information is determined by detecting contradictions and parallel sentences. Pulman (1997) also uses information about parallelism to predict word order. In contrast, we only look at one sentence when we approximate information status, future work will look at cross sentential factors. Endriss and Klabunde (2000) describe a sentence planner for German that annotates the propositional input with discourse-related features in order to determine the focus, and thus influence word order and accentuation. Their system, again, is domainspecific (generating monologue describing a film plot) and requires the existence of a knowledge base. The same holds for Yampolska (2007), who presents s</context>
</contexts>
<marker>Pulman, 1997</marker>
<rawString>Stephen G. Pulman. 1997. Higher Order Unification and the Interpretation of Focus. Linguistics and Philosophy, 20:73–115.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Arndt Riester</author>
</authors>
<booktitle>2008a. A Semantic Explication of ’Information Status’ and the Underspecification of the Recipients’ Knowledge. In Atle Grønn, editor, Proceedings of Sinn und Bedeutung 12,</booktitle>
<institution>University of Oslo.</institution>
<marker>Riester, </marker>
<rawString>Arndt Riester. 2008a. A Semantic Explication of ’Information Status’ and the Underspecification of the Recipients’ Knowledge. In Atle Grønn, editor, Proceedings of Sinn und Bedeutung 12, University of Oslo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arndt Riester</author>
</authors>
<title>The Components of Focus and their Use in Annotating Information Structure.</title>
<date>2008</date>
<booktitle>Arbeitspapiere des Instituts f¨ur Maschinelle Sprachverarbeitung (AIMS),</booktitle>
<tech>Ph.D. thesis,</tech>
<volume>14</volume>
<issue>2</issue>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="9463" citStr="Riester (2008" startWordPosition="1447" endWordPosition="1448">. appropriate in the most number of contexts). In this work, we concentrate on integrating insights from work on information status into the realisation ranking process. 3 Information Status The concept of information status (Prince, 1981; Prince, 1992) involves classifying NP/PP/DP expressions in texts according to various ways of their being given or new. It replaces and specifies more clearly the often vaguely used term givenness. The process of labelling a corpus for IS can be seen as a means of discourse analysis. Different classification systems have been proposed in the literature; see Riester (2008a) for a comparison of several IS labelling schemes and Riester (2008b) for a new proposal based on criteria from presupposition theory. In the work described here, we use the scheme of Riester (2008b). His main theoretic assumption is that IS categories (for definites) should group expressions according to the contextual resources in which their presuppositions find an antecedent. For definites, the set of main category labels found in Table 1 is assumed. The idea of resolution contexts derives from the concept of a presupposition trigger (e.g. a definite description) as potentially establish</context>
<context position="11059" citStr="Riester (2008" startWordPosition="1685" endWordPosition="1686">inites anaphoric relation (van der Sandt, 1992) to an entity being available by some means or other. But there are some expressions whose referent cannot be identified and needs to be accommodated, compare (5). (5) [die monatelange F¨uhrungskrise der Hamburger Sozialdemokraten]ACC-DESC ‘the leadership crisis lasting for months among the Hamburg Social Democrats’ Examples like this one have been mentioned early on in the literature (e.g. Hawkins (1978), Clark and Marshall (1981)). Nevertheless, labeling schemes so far have neglected this issue, which is explicitly incorporated in the system of Riester (2008b). The status of an expression is ACCESSIBLEGENERAL (or unused, following Prince (1981)) if it is not present in the previous discourse but refers to an entity that is known to the intended recipent. There is a further differentiation of the ACCESSIBLE-GENERAL class into generic (TYPE) and non-generic (TOKEN) items. An expression is D-GIVEN (or textually evoked) if and only if an antecedent is available in the discourse context. D-GIVEN entities are subdivided according to whether they are repetitions of their antecedent, short forms thereof, pronouns or whether they use new linguistic materi</context>
<context position="14085" citStr="Riester (2008" startWordPosition="2130" endWordPosition="2131">T&gt;&gt;BRIDG 0.92 23 EXPL»ACC-DESC 1 12 Table 3: Asymmetric pairs of IS labels There are a few more subdivisions. Table 3, for instance, contains the labels BRIDGING-CONTAINED and PARTITIVE-CONTAINED, going back to Prince’s (1981:236) “containing inferrables”. The entire IS label inventory used in this study comprises 19 (sub)classes in total. 4 Asymmetries in IS In order to find out whether IS categories are unevenly distributed within German sentences we examine a corpus of German radio news bulletins that has been manually annotated for IS (496 annotated sentences in total) using the scheme of Riester (2008b).5 For each pair of IS labels X and Y we count how often they co-occur in the corpus within a single clause. In doing so, we distinguish the numbers for “X preceding Y ” (= A) and “Y preceding X” (= B). The larger group is referred to as the dominant order. Subsequently, we compute a ratio indicating the degree of asymmetry between the two orders. If, for instance, the dominant pattern occurs 20 times (A) and the reverse pattern only 5 times (B), the asymmetry ratio B/A is 0.25.6 5The corpus was labeled by two independent annotators and the results were compared by a third person who took th</context>
</contexts>
<marker>Riester, 2008</marker>
<rawString>Arndt Riester. 2008b. The Components of Focus and their Use in Annotating Information Structure. Ph.D. thesis, University of Stuttgart. Arbeitspapiere des Instituts f¨ur Maschinelle Sprachverarbeitung (AIMS), Vol. 14(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Rohrer</author>
<author>Martin Forst</author>
</authors>
<title>Improving Coverage and Parsing Quality of a Large-Scale LFG for German.</title>
<date>2006</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="5413" citStr="Rohrer and Forst, 2006" startWordPosition="811" endWordPosition="814">2 P[pre]:1013 DP[std]:1894 Figure 1: An example C(onstituent) and F(unctional) Structure pair for (1) tation, C(onstituent)-Structure and F(unctional)- Structure. C-Structure is a context-free tree representation that captures characteristics of the surface string while F-Structure is an abstract representation of the basic predicate-argument structure of the string. An example C- and F-Structure pair for the sentence in (1) is given in Figure 1. ‘The authorities warned of possible aftershocks.’ The input to the generation system is an FStructure. A hand-crafted, bi-directional LFG of German (Rohrer and Forst, 2006) is used to generate all possible strings (licensed by the grammar) for this input. As the grammar is hand-crafted, it is designed only to parse (and therefore) generate grammatical strings.2 The task of the realisation ranking system is then to choose the most likely string. Cahill et al. (2007) describe a loglinear model that uses linguistically motivated features and improves over a simple tri-gram language model baseline. We take this log-linear model as our starting point.3 2There are some rare instances of the grammar parsing and therefore also generating ungrammatical output. 3Forst (20</context>
</contexts>
<marker>Rohrer, Forst, 2006</marker>
<rawString>Christian Rohrer and Martin Forst. 2006. Improving Coverage and Parsing Quality of a Large-Scale LFG for German. In Proceedings of the Language Resources and Evaluation Conference (LREC 2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob van der Sandt</author>
</authors>
<title>Presupposition Projection as Anaphora Resolution.</title>
<date>1992</date>
<journal>Journal of Semantics,</journal>
<volume>9</volume>
<pages>377</pages>
<marker>van der Sandt, 1992</marker>
<rawString>Rob van der Sandt. 1992. Presupposition Projection as Anaphora Resolution. Journal of Semantics, 9:333– 377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari¨et Theune</author>
</authors>
<title>Goalgetter: Predicting Contrastive Accent in Data-to-Speech Generation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL/EACL</booktitle>
<pages>519--521</pages>
<location>Madrid.</location>
<note>Student paper.</note>
<contexts>
<context position="29494" citStr="Theune (1997)" startWordPosition="4664" endWordPosition="4665">s and Klabunde (2000) describe a sentence planner for German that annotates the propositional input with discourse-related features in order to determine the focus, and thus influence word order and accentuation. Their system, again, is domainspecific (generating monologue describing a film plot) and requires the existence of a knowledge base. The same holds for Yampolska (2007), who presents suggestions for generating information structure in Russian and Ukrainian football reports, using rules to determine parallel structures for the placement of contrastive accent, following similar work by Theune (1997). While our paper does not address the generation of speech / accentuation, it is of course conceivable to employ the IS annotated radio news corpus from which we derived the label asymmetries (and which also exists in a spoken and prosodically annotated version) in a similar task of learning the correlations between IS labels and pitch accents. Finally, Bresnan et al. (2007) present work on predicting the dative alternation in English using 14 features relating to information status which were manually annotated in their corpus. In our work, we manually annotate a small corpus in order to lea</context>
</contexts>
<marker>Theune, 1997</marker>
<rawString>Mari¨et Theune. 1997. Goalgetter: Predicting Contrastive Accent in Data-to-Speech Generation. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL/EACL 1997), pages 519–521, Madrid. Student paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadiya Yampolska</author>
</authors>
<title>Information Structure in Natural Language Generation: an Account for EastSlavic Languages. Term paper. Universit¨at des Saarlandes.</title>
<date>2007</date>
<contexts>
<context position="29262" citStr="Yampolska (2007)" startWordPosition="4632" endWordPosition="4633">arallel sentences. Pulman (1997) also uses information about parallelism to predict word order. In contrast, we only look at one sentence when we approximate information status, future work will look at cross sentential factors. Endriss and Klabunde (2000) describe a sentence planner for German that annotates the propositional input with discourse-related features in order to determine the focus, and thus influence word order and accentuation. Their system, again, is domainspecific (generating monologue describing a film plot) and requires the existence of a knowledge base. The same holds for Yampolska (2007), who presents suggestions for generating information structure in Russian and Ukrainian football reports, using rules to determine parallel structures for the placement of contrastive accent, following similar work by Theune (1997). While our paper does not address the generation of speech / accentuation, it is of course conceivable to employ the IS annotated radio news corpus from which we derived the label asymmetries (and which also exists in a spoken and prosodically annotated version) in a similar task of learning the correlations between IS labels and pitch accents. Finally, Bresnan et </context>
</contexts>
<marker>Yampolska, 2007</marker>
<rawString>Nadiya Yampolska. 2007. Information Structure in Natural Language Generation: an Account for EastSlavic Languages. Term paper. Universit¨at des Saarlandes.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>