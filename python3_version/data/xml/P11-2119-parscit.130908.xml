<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.050128">
<title confidence="0.991136">
Unary Constraints for Efficient Context-Free Parsing
</title>
<author confidence="0.998689">
Nathan Bodenstab† Kristy Hollingshead� and Brian Roark†
</author>
<affiliation confidence="0.9956965">
† Center for Spoken Language Understanding, Oregon Health &amp; Science University, Portland, OR
$University of Maryland Institute for Advanced Computer Studies, College Park, MD
</affiliation>
<email confidence="0.99808">
{bodensta,roark}@cslu.ogi.edu hollingk@umiacs.umd.edu
</email>
<sectionHeader confidence="0.993871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99969475">
We present a novel pruning method for
context-free parsing that increases efficiency
by disallowing phrase-level unary productions
in CKY chart cells spanning a single word.
Our work is orthogonal to recent work on
“closing” chart cells, which has focused on
multi-word constituents, leaving span-1 chart
cells unpruned. We show that a simple dis-
criminative classifier can learn with high ac-
curacy which span-1 chart cells to close to
phrase-level unary productions. Eliminating
these unary productions from the search can
have a large impact on downstream process-
ing, depending on implementation details of
the search. We apply our method to four pars-
ing architectures and demonstrate how it is
complementary to the cell-closing paradigm,
as well as other pruning methods such as
coarse-to-fine, agenda, and beam-search prun-
ing.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977346938776">
While there have been great advances in the statis-
tical modeling of hierarchical syntactic structure in
the past 15 years, exact inference with such models
remains very costly and most rich syntactic mod-
eling approaches resort to heavy pruning, pipelin-
ing, or both. Graph-based pruning methods such
as best-first and beam-search have both be used
within context-free parsers to increase their effi-
ciency. Pipeline systems make use of simpler mod-
els to reduce the search space of the full model. For
example, the well-known Charniak parser (Char-
niak, 2000) uses a simple grammar to prune the
search space for a richer model in a second pass.
Roark and Hollingshead (2008; 2009) have re-
cently shown that using a finite-state tagger to close
cells within the CKY chart can reduce the worst-case
and average-case complexity of context-free pars-
ing, without reducing accuracy. In their work, word
positions are classified as beginning and/or ending
multi-word constituents, and all chart cells not con-
forming to these constraints can be pruned. Zhang
et al. (2010) and Bodenstab et al. (2011) both ex-
tend this approach by classifying chart cells with a
finer granularity. Pruning based on constituent span
is straightforwardly applicable to all parsing archi-
tectures, yet the methods mentioned above only con-
sider spans of length two or greater. Lexical and
unary productions spanning a single word are never
pruned, and these can, in many cases, contribute sig-
nificantly to the parsing effort.
In this paper, we investigate complementary
methods to prune chart cells with finite-state pre-
processing. Informally, we use a tagger to re-
strict the number of unary productions with non-
terminals on the right-hand side that can be included
in cells spanning a single word. We term these sin-
gle word constituents (SWCs) (see Section 2 for a
formal definition). Disallowing SWCs alters span-1
cell population from potentially containing all non-
terminals to just pre-terminal part-of-speech (POS)
non-terminals. In practice, this decreases the num-
ber of active states in span-1 chart cells by 70%,
significantly reducing the number of allowable con-
stituents in larger spans. Span-1 chart cells are also
the most frequently queried cells in the CKY algo-
rithm. The search over possible midpoints will al-
ways include two cells spanning a single word – one
as the first left child and one as the last right child. It
is therefore critical that the number of active states
</bodyText>
<page confidence="0.987563">
676
</page>
<note confidence="0.625414">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 676–681,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.950292">
(a) Original tree (b) Transformed tree (c) Dynamic programming chart
</figure>
<figureCaption confidence="0.996887">
Figure 1: Example parse structure in (a) the original Penn treebank format and (b) after standard transformations have been
applied. The black cells in (c) indicate CKY chart cells containing a single-word constituent from the transformed tree.
</figureCaption>
<bodyText confidence="0.999961928571429">
in these cells be minimized so that the number of
grammar access requests is also minimized. Note,
however, that some methods of grammar access –
such as scanning through the rules of a grammar and
looking for matches in the chart – achieve less of a
speedup from diminished cell population than oth-
ers, something we investigate in this paper.
Importantly, our method is orthogonal to prior
work on tagging chart constraints and we expect ef-
ficiency gains to be additive. In what follows, we
will demonstrate that a finite-state tagger can learn,
with high accuracy, which span-1 chart cells can be
closed to SWCs, and how such pruning can increase
the efficiency of context-free parsing.
</bodyText>
<sectionHeader confidence="0.830199" genericHeader="introduction">
2 Grammar and Parsing Preliminaries
</sectionHeader>
<bodyText confidence="0.999780261904762">
Given a probabilistic context-free grammar (PCFG)
defined as the tuple (V, T, 5†, P, p) where V is the
set of non-terminals, T is the set of terminals, 5† is a
special start symbol, P is the set of grammar produc-
tions, and p is a mapping of grammar productions to
probabilities, we divide the set of non-terminals V
into two disjoint subsets VPOS and VPHR such that
VPOS contains all pre-terminal part-of-speech tags
and VPHR contains all phrase-level non-terminals.
We define a single word constituent (SWC) unary
production as any production A → B ∈ P such that
A ∈ VPHR and A spans (derives) a single word. An
example SWC unary production, VP → VBP, can be
seen in Figure 1b. Note that ROOT → SBAR and
RB → “quickly” in Figure 1b are also unary pro-
ductions, but by definition they are not SWC unary
productions.
One implementation detail necessary to leverage
the benefits of sparsely populated chart cells is the
grammar access method used by the inner loop of
the CKY algorithm.1 In bottom-up CKY parsing,
to extend derivations of adjacent substrings into new
constituents spanning the combined string, one can
either iterate over all binary productions in the gram-
mar and test if the new derivation is valid (gram-
mar loop), or one can take the cross-product of ac-
tive states in the cells spanning the substrings and
poll the grammar for possible derivations (cross-
product). With the cross-product approach, fewer
active states in either child cell leads to fewer gram-
mar access operations. Thus, pruning constituents
in lower cells directly affects the overall efficiency
of parsing. On the other hand, with the grammar
loop method there is a constant number of gram-
mar access operations (i.e., the number of grammar
rules) and the number of active states in each child
cell has no impact on efficiency. Therefore, with
the grammar loop implementation of the CYK algo-
rithm, pruning techniques such as unary constraints
will have very little impact on the final run-time effi-
ciency of the parser. We will report results in Section
5 with parsers using both approaches.
</bodyText>
<sectionHeader confidence="0.984806" genericHeader="method">
3 Treebank Unary Productions
</sectionHeader>
<bodyText confidence="0.99992775">
In this section, we discuss the use of unary produc-
tions both in the Penn WSJ treebank (Marcus et al.,
1999) and during parsing by analyzing their func-
tion and frequency. All statistics reported here are
computed from sections 2-21 of the treebank.
A common pre-processing step in treebank pars-
ing is to transform the original WSJ treebank be-
fore training and evaluation. There is some flex-
</bodyText>
<footnote confidence="0.975562">
1Some familiarity with the CKY algorithm is assumed. For
details on the algorithm, see Roark and Sproat (2007).
</footnote>
<page confidence="0.990627">
677
</page>
<table confidence="0.998397666666667">
Orig. Trans.
Empty nodes 48,895 0
Multi-Word Const. unaries 1,225 36,608
SWC unaries 98,467 105,973
Lexical unaries 950,028 950,028
Pct words with SWC unary 10.4% 11.2%
</table>
<tableCaption confidence="0.995183">
Table 1: Unary production counts from sections 2-21 of the
original and transformed WSJ treebank. All multisets are dis-
joint. Lexical unary count is identical to word count.
</tableCaption>
<bodyText confidence="0.999804459459459">
ibility in this process, but most pre-processing ef-
forts include (1) affixing a ROOT unary production
to the root symbol of the original tree, (2) removal
of empty nodes, and (3) striping functional tags and
cross-referencing annotations. See Figure 1 for an
example. Additional transforms include (4) remov-
ing X → X unary productions for all non-terminals
X, (5) collapsing unary chains to a single (possibly
composite) unary production (Klein and Manning,
2001), (6) introducing new categories such as AUX
(Charniak, 1997), and (7) collapsing of categories
such as PRT and ADVP (Collins, 1997). For this
paper we only apply transforms 1-3 and otherwise
leave the treebank in its original form. We also note
that ROOT unaries are a special case that do not af-
fect search, and we choose to ignore them for the
remainder of this paper.
These tree transformations have a large impact
on the number and type of unary productions in
the treebank. Table 1 displays the absolute counts
of unaries in the treebank before and after process-
ing. Multi-word constituent unary productions in the
original treebank are rare and used primarily to mark
quantifier phrases as noun phrases. But due to the
removal of empty nodes, the transformed treebank
contains many more unary productions that span
multiple words, such as S → VP, where the noun
phrase was left unspecified in the original clause.
The number of SWC unaries is relatively un-
changed after processing the original treebank, but
note that only 11.2% of words in the transformed
treebank are covered by SWCs. This implies that
we are unnecessarily adding SWC productions to al-
most 90% of span-1 chart cells during search. One
may argue that an unsmoothed grammar will nat-
urally disallow most SWC productions since they
are never observed in the training data, for example
</bodyText>
<table confidence="0.997912833333333">
Mk2 Mk2+S Latent
|VPOS |45 45 582
|VPHR |26 26 275
SWC grammar rules 159 1,170 91,858
Active VPOS states 2.5 45 75
Active VPHR states 5.9 26 152
</table>
<tableCaption confidence="0.98573975">
Table 2: Grammar statistics and averaged span-1 active state
counts for exhaustive parsing of section 24 using a Markov
order-2 (Mk2), a smoothed Markov order-2 (Mk2+S), and the
Berkeley latent variable (Latent) grammars.
</tableCaption>
<bodyText confidence="0.9998798">
VP → DT. This is true to some extent, but gram-
mars induced from the WSJ treebank are notorious
for over-generation. In addition, state-of-the-art ac-
curacy in context-free parsing is often achieved by
smoothing the grammar, so that rewrites from any
one non-terminal to another are permissible, albeit
with low probability.
To empirically evaluate the impact of SWCs on
span-1 chart cells, we parse the development set
(section 24) with three different grammars induced
from sections 2-21. Table 2 lists averaged counts
of active Viterbi states (derivations with probabil-
ity greater than zero) from span-1 cells within the
dynamic programming chart, as well as relevant
grammar statistics. Note that these counts are ex-
tracted from exhaustive parsing – no pruning has
been applied. We notice two points of interest.
First, although |VPOS |&gt; |VPHR|, for the unsmoothed
grammars more phrase-level states are active within
the span-1 cells than states derived from POS tags.
When parsing with the Markov order-2 grammar,
70% of active states are non-terminals from VPHR,
and with the latent-variable grammar, 67% (152 of
227). This is due to the highly generative nature
of SWC productions. Second, although using a
smoothed grammar maximizes the number of active
states, the unsmoothed grammars still provide many
possible derivations per word.
Given the infrequent use of SWCs in the treebank,
and the search-space explosion incurred by includ-
ing them in exhaustive search, it is clear that restrict-
ing SWCs in contexts where they are unlikely to oc-
cur has the potential for large efficiency gains. In the
next section, we discuss how to learn such contexts
via a finite-state tagger.
</bodyText>
<page confidence="0.994295">
678
</page>
<sectionHeader confidence="0.255998" genericHeader="method">
4 Tagging Unary Constraints 5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.984516489361702">
To automatically predict if word wi from sentence To evaluate the effectiveness of unary constraints,
w can be spanned by an SWC production, we train a we apply our technique to four parsers: an exhaus-
binary classifier from supervised data using sections tive CKY chart parser (Cocke and Schwartz, 1970);
2-21 of the Penn WSJ Treebank for training, section the Charniak parser (Charniak, 2000), which uses
00 as heldout, and section 24 as development. The agenda-based two-level coarse-to-fine pruning; the
class labels of all words in the training data are ex- Berkeley parser (Petrov and Klein, 2007a), a multi-
tracted from the treebank, where wi ∈ U if wi is level coarse-to-fine parser; and the BUBS parser
observed with a SWC production and wi ∈ U other- (Bodenstab et al., 2011), a single-pass beam-search
wise. We train a log linear model with the averaged parser with a figure-of-merit constituent ranking
perceptron algorithm (Collins, 2002) using unigram function. The Berkeley and BUBS parsers both
word and POS-tag2 features from a five word win- parse with the Berkeley latent-variable grammar
dow. We also trained models with bi-gram and tri- (Petrov and Klein, 2007b), while the Charniak
gram features, but tagging accuracy did not improve. parser uses a lexicalized grammar, and the exhaus-
Because the classifier output is imposing hard tive CKY algorithm is run with a simple Markov
constraints on the search space of the parser, we order-2 grammar. All grammars are induced from
may want to choose a tagger operating point that fa- the same data: sections 2-21 of the WSJ treebank.
vors precision over recall to avoid over-constraining Figure 2 contrasts the merit of unary constraints
the downstream parser. To compare the tradeoff be- on the three high-accuracy parsers, and several inter-
tween possible precision/recall values, we apply the esting comparisons emerge. First, as recall is traded
softmax activation function to the perceptron output for precision within the tagger, each parser reacts
to obtain the posterior probability of wi ∈ U: quite differently to the imposed constraints. We ap-
P(U|wi7 0) = (1 + exp(−f(wi) · 0))−1 (1) ply constraints to the Berkeley parser during the ini-
where 0 is a vector of model parameters and f(·) is a tial coarse-pass search, which is simply an exhaus-
feature function. The threshold 0.5 simply chooses tive CKY search with a coarse grammar. Applying
the most likely class, but to increase precision we unary and cell-closing constraints at this point in the
can move this threshold to favor U over U. To tune coarse-to-fine pipeline speeds up the initial coarse-
this value on a per-sentence basis, we follow meth- pass significantly, which accounted for almost half
ods similar to Roark &amp; Hollingshead (2009) and of the total parse time in the Berkeley parser. In ad-
rank each word position with respect to its poste- dition, all subsequent fine-pass searches also bene-
rior probability. If the total number of words wi fit from additional pruning as their search is guided
with P(U|wi7 0) &lt; 0.5 is k, we decrease the thresh- by the remaining constituents of the previous pass,
old value from 0.5 until Ak words have been moved which is the intersection of standard coarse-to-fine
from class U to U, where A is a tuning parameter be- pruning and our imposed constraints.
tween 0 and 1. Although the threshold 0.5 produces We apply constraints to the Charniak parser dur-
tagging precision and recall of 98.7% and 99.4% ing the first-pass agenda-based search. Because an
respectively, we can adjust A to increase precision agenda-based search operates at a constituent level
as high as 99.7%, while recall drops to a tolerable instead of a cell/span level, applying unary con-
82.1%. Similar methods are used to replicate cell- straints alters the search frontier instead of reduc-
closing constraints, which are combined with unary ing the absolute number of constituents placed in the
constraints in the next section. chart. We jointly tune lambda and the internal search
parameters of the Charniak parser until accuracy de-
grades.
Application of constraints to the CKY and BUBS
parsers is straightforward as they are both single
pass parsers – any constituent violating the con-
straints is pruned. We also note that the CKY and
2POS-tags were provided by a separately trained tagger.
</bodyText>
<page confidence="0.622035">
679
</page>
<figureCaption confidence="0.995473">
Figure 2: Development set results applying unary constraints
at multiple values of A to three parsers.
</figureCaption>
<bodyText confidence="0.999964827586207">
BUBS parsers both employ the cross-product gram-
mar access method discussed in Section 2, while
the Berkeley parser uses the grammar loop method.
This grammar access difference dampens the benefit
of unary constraints for the Berkeley parser.3
Referring back to Figure 2, we see that both speed
and accuracy increase in all but the Berkeley parser.
Although it is unusual that pruning leads to higher
accuracy during search, it is not unexpected here as
our finite-state tagger makes use of lexical relation-
ships that the PCFG does not. By leveraging this
new information to constrain the search space, we
are indirectly improving the quality of the model.
Finally, there is an obvious operating point for
each parser at which the unary constraints are too
severe and accuracy deteriorates rapidly. For test
conditions, we set the tuning parameter A based on
the development set results to prune as much of the
search space as possible before reaching this degra-
dation point.
Using lambda-values optimized for each parser,
we parse the unseen section 23 test data and present
results in Table 3. We see that in all cases, unary
constraints improve the efficiency of parsing without
significant accuracy loss. As one might expect, ex-
haustive CKY parsing benefits the most from unary
constraints since no other pruning is applied. But
even heavily pruned parsers using graph-based and
pipelining techniques still see substantial speedups
</bodyText>
<footnote confidence="0.992625">
3The Berkeley parser does maintain meta-information about
where non-terminals have been placed in the chart, giving it
some of the advantages of cross-product grammar access.
</footnote>
<table confidence="0.999972705882353">
Parser F-score Seconds Speedup
CKY 72.2 1,358
+ UC (A=0.2) 72.6 1,125 1.2x
+ CC 74.3 380 3.6x
+ CC + UC 74.6 249 5.5x
BUBS 88.4 586
+ UC (A=0.2) 88.5 486 1.2x
+ CC 88.7 349 1.7x
+ CC + UC 88.7 283 2.1x
Charniak 89.7 1,116
+ UC (A=0.2) 89.7 900 1.2x
+ CC 89.7 716 1.6x
+ CC + UC 89.6 679 1.6x
Berkeley 90.2 564
+ UC (A=0.4) 90.1 495 1.1x
+ CC 90.2 320 1.8x
+ CC + UC 90.2 289 2.0x
</table>
<tableCaption confidence="0.987147666666667">
Table 3: Test set results applying unary constraints (UC) and
cell-closing (CC) constraints (Roark and Hollingshead, 2008)
to various parsers.
</tableCaption>
<bodyText confidence="0.99982775">
with the additional application of unary constraints.
Furthermore, unary constraints consistently provide
an additive efficiency gain when combined with cell-
closing constraints.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999929444444445">
We have presented a new method to constrain
context-free chart parsing and have shown it to be or-
thogonal to many forms of graph-based and pipeline
pruning methods. In addition, our method parallels
the cell closing paradigm and is an elegant com-
plement to recent work, providing a finite-state tag-
ging framework to potentially constrain all areas of
the search space – both multi-word and single-word
constituents.
</bodyText>
<sectionHeader confidence="0.997485" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999961666666667">
We would like to thank Aaron Dunlop for his valu-
able discussions, as well as the anonymous review-
ers who gave very helpful feedback. This research
was supported in part by NSF Grants #IIS-0447214,
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
</bodyText>
<page confidence="0.996789">
680
</page>
<sectionHeader confidence="0.990098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892381578947">
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Beam-width prediction for efficient
context-free parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, Portland, Oregon. Association for Com-
putational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Arti-
ficial Intelligence, pages 598–603, Menlo Park, CA.
AAAI Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132–139, Seattle, Washington.
Morgan Kaufmann Publishers Inc.
John Cocke and Jacob T. Schwartz. 1970. Programming
languages and their compilers. Technical report Pre-
liminary notes, Courant Institute of Mathematical Sci-
ences, NYU.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
eighth conference on European chapter of the Associ-
ation for Computational Linguistics, page 1623, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical Methods in
Natural Language Processing, volume 10, pages 1–
8, Philadelphia, July. Association for Computational
Linguistics.
Dan Klein and Christopher D. Manning. 2001. Parsing
with treebank grammars: Empirical bounds, theoret-
ical models, and the structure of the Penn treebank.
In Proceedings of 39th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 338–345,
Toulouse, France, July. Association for Computational
Linguistics.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Slav Petrov and Dan Klein. 2007a. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404–411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2007b. Learning and in-
ference for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Donia Scott and Hans Uszkoreit, ed-
itors, Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING 2008),
pages 745–752, Manchester, UK, August. Association
for Computational Linguistics.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 647–655, Boulder, Colorado,
June. Association for Computational Linguistics.
Brian Roark and Richard W Sproat. 2007. Computa-
tional Approaches to Morphology and Syntax. Oxford
University Press, New York.
Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast lexicalised-grammar parsing. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 1472–1479, Beijing,
China, June.
</reference>
<page confidence="0.998304">
681
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844258">
<title confidence="0.99997">Unary Constraints for Efficient Context-Free Parsing</title>
<author confidence="0.996181">Brian</author>
<affiliation confidence="0.959228">for Spoken Language Understanding, Oregon Health &amp; Science University, Portland,</affiliation>
<address confidence="0.959951">of Maryland Institute for Advanced Computer Studies, College Park, MD</address>
<email confidence="0.998569">hollingk@umiacs.umd.edu</email>
<abstract confidence="0.995402809523809">We present a novel pruning method for context-free parsing that increases efficiency by disallowing phrase-level unary productions in CKY chart cells spanning a single word. Our work is orthogonal to recent work on “closing” chart cells, which has focused on multi-word constituents, leaving span-1 chart cells unpruned. We show that a simple discriminative classifier can learn with high accuracy which span-1 chart cells to close to phrase-level unary productions. Eliminating these unary productions from the search can have a large impact on downstream processing, depending on implementation details of the search. We apply our method to four parsing architectures and demonstrate how it is complementary to the cell-closing paradigm, as well as other pruning methods such as coarse-to-fine, agenda, and beam-search pruning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nathan Bodenstab</author>
<author>Aaron Dunlop</author>
<author>Keith Hall</author>
<author>Brian Roark</author>
</authors>
<title>Beam-width prediction for efficient context-free parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="2279" citStr="Bodenstab et al. (2011)" startWordPosition="341" endWordPosition="344">earch space of the full model. For example, the well-known Charniak parser (Charniak, 2000) uses a simple grammar to prune the search space for a richer model in a second pass. Roark and Hollingshead (2008; 2009) have recently shown that using a finite-state tagger to close cells within the CKY chart can reduce the worst-case and average-case complexity of context-free parsing, without reducing accuracy. In their work, word positions are classified as beginning and/or ending multi-word constituents, and all chart cells not conforming to these constraints can be pruned. Zhang et al. (2010) and Bodenstab et al. (2011) both extend this approach by classifying chart cells with a finer granularity. Pruning based on constituent span is straightforwardly applicable to all parsing architectures, yet the methods mentioned above only consider spans of length two or greater. Lexical and unary productions spanning a single word are never pruned, and these can, in many cases, contribute significantly to the parsing effort. In this paper, we investigate complementary methods to prune chart cells with finite-state preprocessing. Informally, we use a tagger to restrict the number of unary productions with nonterminals o</context>
<context position="12554" citStr="Bodenstab et al., 2011" startWordPosition="2029" endWordPosition="2032"> our technique to four parsers: an exhausbinary classifier from supervised data using sections tive CKY chart parser (Cocke and Schwartz, 1970); 2-21 of the Penn WSJ Treebank for training, section the Charniak parser (Charniak, 2000), which uses 00 as heldout, and section 24 as development. The agenda-based two-level coarse-to-fine pruning; the class labels of all words in the training data are ex- Berkeley parser (Petrov and Klein, 2007a), a multitracted from the treebank, where wi ∈ U if wi is level coarse-to-fine parser; and the BUBS parser observed with a SWC production and wi ∈ U other- (Bodenstab et al., 2011), a single-pass beam-search wise. We train a log linear model with the averaged parser with a figure-of-merit constituent ranking perceptron algorithm (Collins, 2002) using unigram function. The Berkeley and BUBS parsers both word and POS-tag2 features from a five word win- parse with the Berkeley latent-variable grammar dow. We also trained models with bi-gram and tri- (Petrov and Klein, 2007b), while the Charniak gram features, but tagging accuracy did not improve. parser uses a lexicalized grammar, and the exhausBecause the classifier output is imposing hard tive CKY algorithm is run with a</context>
</contexts>
<marker>Bodenstab, Dunlop, Hall, Roark, 2011</marker>
<rawString>Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian Roark. 2011. Beam-width prediction for efficient context-free parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="8367" citStr="Charniak, 1997" startWordPosition="1340" endWordPosition="1341"> WSJ treebank. All multisets are disjoint. Lexical unary count is identical to word count. ibility in this process, but most pre-processing efforts include (1) affixing a ROOT unary production to the root symbol of the original tree, (2) removal of empty nodes, and (3) striping functional tags and cross-referencing annotations. See Figure 1 for an example. Additional transforms include (4) removing X → X unary productions for all non-terminals X, (5) collapsing unary chains to a single (possibly composite) unary production (Klein and Manning, 2001), (6) introducing new categories such as AUX (Charniak, 1997), and (7) collapsing of categories such as PRT and ADVP (Collins, 1997). For this paper we only apply transforms 1-3 and otherwise leave the treebank in its original form. We also note that ROOT unaries are a special case that do not affect search, and we choose to ignore them for the remainder of this paper. These tree transformations have a large impact on the number and type of unary productions in the treebank. Table 1 displays the absolute counts of unaries in the treebank before and after processing. Multi-word constituent unary productions in the original treebank are rare and used prim</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 598–603, Menlo Park, CA. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>Seattle, Washington.</location>
<contexts>
<context position="1747" citStr="Charniak, 2000" startWordPosition="255" endWordPosition="257">se-to-fine, agenda, and beam-search pruning. 1 Introduction While there have been great advances in the statistical modeling of hierarchical syntactic structure in the past 15 years, exact inference with such models remains very costly and most rich syntactic modeling approaches resort to heavy pruning, pipelining, or both. Graph-based pruning methods such as best-first and beam-search have both be used within context-free parsers to increase their efficiency. Pipeline systems make use of simpler models to reduce the search space of the full model. For example, the well-known Charniak parser (Charniak, 2000) uses a simple grammar to prune the search space for a richer model in a second pass. Roark and Hollingshead (2008; 2009) have recently shown that using a finite-state tagger to close cells within the CKY chart can reduce the worst-case and average-case complexity of context-free parsing, without reducing accuracy. In their work, word positions are classified as beginning and/or ending multi-word constituents, and all chart cells not conforming to these constraints can be pruned. Zhang et al. (2010) and Bodenstab et al. (2011) both extend this approach by classifying chart cells with a finer g</context>
<context position="12164" citStr="Charniak, 2000" startWordPosition="1964" endWordPosition="1965">here they are unlikely to occur has the potential for large efficiency gains. In the next section, we discuss how to learn such contexts via a finite-state tagger. 678 4 Tagging Unary Constraints 5 Experiments and Results To automatically predict if word wi from sentence To evaluate the effectiveness of unary constraints, w can be spanned by an SWC production, we train a we apply our technique to four parsers: an exhausbinary classifier from supervised data using sections tive CKY chart parser (Cocke and Schwartz, 1970); 2-21 of the Penn WSJ Treebank for training, section the Charniak parser (Charniak, 2000), which uses 00 as heldout, and section 24 as development. The agenda-based two-level coarse-to-fine pruning; the class labels of all words in the training data are ex- Berkeley parser (Petrov and Klein, 2007a), a multitracted from the treebank, where wi ∈ U if wi is level coarse-to-fine parser; and the BUBS parser observed with a SWC production and wi ∈ U other- (Bodenstab et al., 2011), a single-pass beam-search wise. We train a log linear model with the averaged parser with a figure-of-merit constituent ranking perceptron algorithm (Collins, 2002) using unigram function. The Berkeley and BU</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132–139, Seattle, Washington. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cocke</author>
<author>Jacob T Schwartz</author>
</authors>
<title>Programming languages and their compilers.</title>
<date>1970</date>
<tech>Technical report Preliminary notes,</tech>
<institution>Courant Institute of Mathematical Sciences, NYU.</institution>
<contexts>
<context position="12074" citStr="Cocke and Schwartz, 1970" startWordPosition="1948" endWordPosition="1951">ion incurred by including them in exhaustive search, it is clear that restricting SWCs in contexts where they are unlikely to occur has the potential for large efficiency gains. In the next section, we discuss how to learn such contexts via a finite-state tagger. 678 4 Tagging Unary Constraints 5 Experiments and Results To automatically predict if word wi from sentence To evaluate the effectiveness of unary constraints, w can be spanned by an SWC production, we train a we apply our technique to four parsers: an exhausbinary classifier from supervised data using sections tive CKY chart parser (Cocke and Schwartz, 1970); 2-21 of the Penn WSJ Treebank for training, section the Charniak parser (Charniak, 2000), which uses 00 as heldout, and section 24 as development. The agenda-based two-level coarse-to-fine pruning; the class labels of all words in the training data are ex- Berkeley parser (Petrov and Klein, 2007a), a multitracted from the treebank, where wi ∈ U if wi is level coarse-to-fine parser; and the BUBS parser observed with a SWC production and wi ∈ U other- (Bodenstab et al., 2011), a single-pass beam-search wise. We train a log linear model with the averaged parser with a figure-of-merit constituen</context>
</contexts>
<marker>Cocke, Schwartz, 1970</marker>
<rawString>John Cocke and Jacob T. Schwartz. 1970. Programming languages and their compilers. Technical report Preliminary notes, Courant Institute of Mathematical Sciences, NYU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>1623</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8438" citStr="Collins, 1997" startWordPosition="1352" endWordPosition="1353">cal to word count. ibility in this process, but most pre-processing efforts include (1) affixing a ROOT unary production to the root symbol of the original tree, (2) removal of empty nodes, and (3) striping functional tags and cross-referencing annotations. See Figure 1 for an example. Additional transforms include (4) removing X → X unary productions for all non-terminals X, (5) collapsing unary chains to a single (possibly composite) unary production (Klein and Manning, 2001), (6) introducing new categories such as AUX (Charniak, 1997), and (7) collapsing of categories such as PRT and ADVP (Collins, 1997). For this paper we only apply transforms 1-3 and otherwise leave the treebank in its original form. We also note that ROOT unaries are a special case that do not affect search, and we choose to ignore them for the remainder of this paper. These tree transformations have a large impact on the number and type of unary productions in the treebank. Table 1 displays the absolute counts of unaries in the treebank before and after processing. Multi-word constituent unary productions in the original treebank are rare and used primarily to mark quantifier phrases as noun phrases. But due to the remova</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, page 1623, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>10</volume>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia,</location>
<contexts>
<context position="12720" citStr="Collins, 2002" startWordPosition="2054" endWordPosition="2055">for training, section the Charniak parser (Charniak, 2000), which uses 00 as heldout, and section 24 as development. The agenda-based two-level coarse-to-fine pruning; the class labels of all words in the training data are ex- Berkeley parser (Petrov and Klein, 2007a), a multitracted from the treebank, where wi ∈ U if wi is level coarse-to-fine parser; and the BUBS parser observed with a SWC production and wi ∈ U other- (Bodenstab et al., 2011), a single-pass beam-search wise. We train a log linear model with the averaged parser with a figure-of-merit constituent ranking perceptron algorithm (Collins, 2002) using unigram function. The Berkeley and BUBS parsers both word and POS-tag2 features from a five word win- parse with the Berkeley latent-variable grammar dow. We also trained models with bi-gram and tri- (Petrov and Klein, 2007b), while the Charniak gram features, but tagging accuracy did not improve. parser uses a lexicalized grammar, and the exhausBecause the classifier output is imposing hard tive CKY algorithm is run with a simple Markov constraints on the search space of the parser, we order-2 grammar. All grammars are induced from may want to choose a tagger operating point that fa- t</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical Methods in Natural Language Processing, volume 10, pages 1– 8, Philadelphia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing with treebank grammars: Empirical bounds, theoretical models, and the structure of the Penn treebank.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>338--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse, France,</location>
<contexts>
<context position="8306" citStr="Klein and Manning, 2001" startWordPosition="1329" endWordPosition="1332">y production counts from sections 2-21 of the original and transformed WSJ treebank. All multisets are disjoint. Lexical unary count is identical to word count. ibility in this process, but most pre-processing efforts include (1) affixing a ROOT unary production to the root symbol of the original tree, (2) removal of empty nodes, and (3) striping functional tags and cross-referencing annotations. See Figure 1 for an example. Additional transforms include (4) removing X → X unary productions for all non-terminals X, (5) collapsing unary chains to a single (possibly composite) unary production (Klein and Manning, 2001), (6) introducing new categories such as AUX (Charniak, 1997), and (7) collapsing of categories such as PRT and ADVP (Collins, 1997). For this paper we only apply transforms 1-3 and otherwise leave the treebank in its original form. We also note that ROOT unaries are a special case that do not affect search, and we choose to ignore them for the remainder of this paper. These tree transformations have a large impact on the number and type of unary productions in the treebank. Table 1 displays the absolute counts of unaries in the treebank before and after processing. Multi-word constituent unar</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing with treebank grammars: Empirical bounds, theoretical models, and the structure of the Penn treebank. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 338–345, Toulouse, France, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<booktitle>Treebank-3. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="7102" citStr="Marcus et al., 1999" startWordPosition="1137" endWordPosition="1140">n the other hand, with the grammar loop method there is a constant number of grammar access operations (i.e., the number of grammar rules) and the number of active states in each child cell has no impact on efficiency. Therefore, with the grammar loop implementation of the CYK algorithm, pruning techniques such as unary constraints will have very little impact on the final run-time efficiency of the parser. We will report results in Section 5 with parsers using both approaches. 3 Treebank Unary Productions In this section, we discuss the use of unary productions both in the Penn WSJ treebank (Marcus et al., 1999) and during parsing by analyzing their function and frequency. All statistics reported here are computed from sections 2-21 of the treebank. A common pre-processing step in treebank parsing is to transform the original WSJ treebank before training and evaluation. There is some flex1Some familiarity with the CKY algorithm is assumed. For details on the algorithm, see Roark and Sproat (2007). 677 Orig. Trans. Empty nodes 48,895 0 Multi-Word Const. unaries 1,225 36,608 SWC unaries 98,467 105,973 Lexical unaries 950,028 950,028 Pct words with SWC unary 10.4% 11.2% Table 1: Unary production counts </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank-3. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="12372" citStr="Petrov and Klein, 2007" startWordPosition="1995" endWordPosition="1998">eriments and Results To automatically predict if word wi from sentence To evaluate the effectiveness of unary constraints, w can be spanned by an SWC production, we train a we apply our technique to four parsers: an exhausbinary classifier from supervised data using sections tive CKY chart parser (Cocke and Schwartz, 1970); 2-21 of the Penn WSJ Treebank for training, section the Charniak parser (Charniak, 2000), which uses 00 as heldout, and section 24 as development. The agenda-based two-level coarse-to-fine pruning; the class labels of all words in the training data are ex- Berkeley parser (Petrov and Klein, 2007a), a multitracted from the treebank, where wi ∈ U if wi is level coarse-to-fine parser; and the BUBS parser observed with a SWC production and wi ∈ U other- (Bodenstab et al., 2011), a single-pass beam-search wise. We train a log linear model with the averaged parser with a figure-of-merit constituent ranking perceptron algorithm (Collins, 2002) using unigram function. The Berkeley and BUBS parsers both word and POS-tag2 features from a five word win- parse with the Berkeley latent-variable grammar dow. We also trained models with bi-gram and tri- (Petrov and Klein, 2007b), while the Charniak</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007a. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Learning and inference for hierarchically split PCFGs.</title>
<date>2007</date>
<booktitle>In AAAI</booktitle>
<note>(Nectar Track).</note>
<contexts>
<context position="12372" citStr="Petrov and Klein, 2007" startWordPosition="1995" endWordPosition="1998">eriments and Results To automatically predict if word wi from sentence To evaluate the effectiveness of unary constraints, w can be spanned by an SWC production, we train a we apply our technique to four parsers: an exhausbinary classifier from supervised data using sections tive CKY chart parser (Cocke and Schwartz, 1970); 2-21 of the Penn WSJ Treebank for training, section the Charniak parser (Charniak, 2000), which uses 00 as heldout, and section 24 as development. The agenda-based two-level coarse-to-fine pruning; the class labels of all words in the training data are ex- Berkeley parser (Petrov and Klein, 2007a), a multitracted from the treebank, where wi ∈ U if wi is level coarse-to-fine parser; and the BUBS parser observed with a SWC production and wi ∈ U other- (Bodenstab et al., 2011), a single-pass beam-search wise. We train a log linear model with the averaged parser with a figure-of-merit constituent ranking perceptron algorithm (Collins, 2002) using unigram function. The Berkeley and BUBS parsers both word and POS-tag2 features from a five word win- parse with the Berkeley latent-variable grammar dow. We also trained models with bi-gram and tri- (Petrov and Klein, 2007b), while the Charniak</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007b. Learning and inference for hierarchically split PCFGs. In AAAI 2007 (Nectar Track).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity context-free inference.</title>
<date>2008</date>
<booktitle>In Donia Scott and Hans Uszkoreit, editors, Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008),</booktitle>
<pages>745--752</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Manchester, UK,</location>
<contexts>
<context position="1861" citStr="Roark and Hollingshead (2008" startWordPosition="275" endWordPosition="278">he statistical modeling of hierarchical syntactic structure in the past 15 years, exact inference with such models remains very costly and most rich syntactic modeling approaches resort to heavy pruning, pipelining, or both. Graph-based pruning methods such as best-first and beam-search have both be used within context-free parsers to increase their efficiency. Pipeline systems make use of simpler models to reduce the search space of the full model. For example, the well-known Charniak parser (Charniak, 2000) uses a simple grammar to prune the search space for a richer model in a second pass. Roark and Hollingshead (2008; 2009) have recently shown that using a finite-state tagger to close cells within the CKY chart can reduce the worst-case and average-case complexity of context-free parsing, without reducing accuracy. In their work, word positions are classified as beginning and/or ending multi-word constituents, and all chart cells not conforming to these constraints can be pruned. Zhang et al. (2010) and Bodenstab et al. (2011) both extend this approach by classifying chart cells with a finer granularity. Pruning based on constituent span is straightforwardly applicable to all parsing architectures, yet th</context>
<context position="18328" citStr="Roark and Hollingshead, 2008" startWordPosition="2988" endWordPosition="2991">ion about where non-terminals have been placed in the chart, giving it some of the advantages of cross-product grammar access. Parser F-score Seconds Speedup CKY 72.2 1,358 + UC (A=0.2) 72.6 1,125 1.2x + CC 74.3 380 3.6x + CC + UC 74.6 249 5.5x BUBS 88.4 586 + UC (A=0.2) 88.5 486 1.2x + CC 88.7 349 1.7x + CC + UC 88.7 283 2.1x Charniak 89.7 1,116 + UC (A=0.2) 89.7 900 1.2x + CC 89.7 716 1.6x + CC + UC 89.6 679 1.6x Berkeley 90.2 564 + UC (A=0.4) 90.1 495 1.1x + CC 90.2 320 1.8x + CC + UC 90.2 289 2.0x Table 3: Test set results applying unary constraints (UC) and cell-closing (CC) constraints (Roark and Hollingshead, 2008) to various parsers. with the additional application of unary constraints. Furthermore, unary constraints consistently provide an additive efficiency gain when combined with cellclosing constraints. 6 Conclusion We have presented a new method to constrain context-free chart parsing and have shown it to be orthogonal to many forms of graph-based and pipeline pruning methods. In addition, our method parallels the cell closing paradigm and is an elegant complement to recent work, providing a finite-state tagging framework to potentially constrain all areas of the search space – both multi-word an</context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2008. Classifying chart cells for quadratic complexity context-free inference. In Donia Scott and Hans Uszkoreit, editors, Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), pages 745–752, Manchester, UK, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Linear complexity context-free parsing pipelines via chart constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>647--655</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="14541" citStr="Roark &amp; Hollingshead (2009)" startWordPosition="2349" endWordPosition="2352">) · 0))−1 (1) ply constraints to the Berkeley parser during the iniwhere 0 is a vector of model parameters and f(·) is a tial coarse-pass search, which is simply an exhausfeature function. The threshold 0.5 simply chooses tive CKY search with a coarse grammar. Applying the most likely class, but to increase precision we unary and cell-closing constraints at this point in the can move this threshold to favor U over U. To tune coarse-to-fine pipeline speeds up the initial coarsethis value on a per-sentence basis, we follow meth- pass significantly, which accounted for almost half ods similar to Roark &amp; Hollingshead (2009) and of the total parse time in the Berkeley parser. In adrank each word position with respect to its poste- dition, all subsequent fine-pass searches also benerior probability. If the total number of words wi fit from additional pruning as their search is guided with P(U|wi7 0) &lt; 0.5 is k, we decrease the thresh- by the remaining constituents of the previous pass, old value from 0.5 until Ak words have been moved which is the intersection of standard coarse-to-fine from class U to U, where A is a tuning parameter be- pruning and our imposed constraints. tween 0 and 1. Although the threshold 0</context>
</contexts>
<marker>Roark, Hollingshead, 2009</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2009. Linear complexity context-free parsing pipelines via chart constraints. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 647–655, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Richard W Sproat</author>
</authors>
<title>Computational Approaches to Morphology and Syntax.</title>
<date>2007</date>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="7494" citStr="Roark and Sproat (2007)" startWordPosition="1201" endWordPosition="1204">ciency of the parser. We will report results in Section 5 with parsers using both approaches. 3 Treebank Unary Productions In this section, we discuss the use of unary productions both in the Penn WSJ treebank (Marcus et al., 1999) and during parsing by analyzing their function and frequency. All statistics reported here are computed from sections 2-21 of the treebank. A common pre-processing step in treebank parsing is to transform the original WSJ treebank before training and evaluation. There is some flex1Some familiarity with the CKY algorithm is assumed. For details on the algorithm, see Roark and Sproat (2007). 677 Orig. Trans. Empty nodes 48,895 0 Multi-Word Const. unaries 1,225 36,608 SWC unaries 98,467 105,973 Lexical unaries 950,028 950,028 Pct words with SWC unary 10.4% 11.2% Table 1: Unary production counts from sections 2-21 of the original and transformed WSJ treebank. All multisets are disjoint. Lexical unary count is identical to word count. ibility in this process, but most pre-processing efforts include (1) affixing a ROOT unary production to the root symbol of the original tree, (2) removal of empty nodes, and (3) striping functional tags and cross-referencing annotations. See Figure 1</context>
</contexts>
<marker>Roark, Sproat, 2007</marker>
<rawString>Brian Roark and Richard W Sproat. 2007. Computational Approaches to Morphology and Syntax. Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Byung gyu Ahn</author>
<author>Stephen Clark</author>
<author>Curt Van Wyk</author>
<author>James R Curran</author>
<author>Laura Rimell</author>
</authors>
<title>Chart pruning for fast lexicalised-grammar parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1472--1479</pages>
<location>Beijing, China,</location>
<marker>Zhang, Ahn, Clark, Van Wyk, Curran, Rimell, 2010</marker>
<rawString>Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van Wyk, James R. Curran, and Laura Rimell. 2010. Chart pruning for fast lexicalised-grammar parsing. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1472–1479, Beijing, China, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>