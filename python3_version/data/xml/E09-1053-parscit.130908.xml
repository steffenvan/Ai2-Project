<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001684">
<title confidence="0.993332">
Dependency trees and the strong generative capacity of CCG
</title>
<author confidence="0.998009">
Alexander Koller
</author>
<affiliation confidence="0.765636">
Saarland University
Saarbrücken, Germany
</affiliation>
<email confidence="0.97159">
koller@mmci.uni-saarland.de
</email>
<author confidence="0.994547">
Marco Kuhlmann
</author>
<affiliation confidence="0.825868">
Uppsala University
Uppsala, Sweden
</affiliation>
<email confidence="0.987274">
marco.kuhlmann@lingfil.uu.se
</email>
<sectionHeader confidence="0.997233" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890583333333">
We propose a novel algorithm for extract-
ing dependencies from the derivations of
a large fragment of CCG. Unlike earlier
proposals, our dependency structures are
always tree-shaped. We then use these de-
pendency trees to compare the strong gen-
erative capacities of CCG and TAG and
obtain surprising results: Both formalisms
generate the same languages of derivation
trees – but the mechanisms they use to
bring the words in these trees into a linear
order are incomparable.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998348742424243">
Combinatory Categorial Grammar (CCG; Steed-
man (2001)) is an increasingly popular grammar
formalism. Next to being theoretically well-mo-
tivated due to its links to combinatory logic and
categorial grammar, it is distinguished by the avail-
ability of efficient open-source parsers (Clark and
Curran, 2007), annotated corpora (Hockenmaier
and Steedman, 2007; Hockenmaier, 2006), and
mechanisms for wide-coverage semantic construc-
tion (Bos et al., 2004).
However, there are limits to our understanding
of the formal properties of CCG and its relation
to other grammar formalisms. In particular, while
it is well-known that CCG belongs to a family of
mildly context-sensitive formalisms that all gener-
ate the same string languages (Vijay-Shanker and
Weir, 1994), there are few results about the strong
generative capacity of CCG. This makes it difficult
to gauge the similarities and differences between
CCG and other formalisms in how they model lin-
guistic phenomena such as scrambling and relat-
ive clauses (Hockenmaier and Young, 2008), and
hampers the transfer of algorithms from one form-
alism to another.
In this paper, we propose a new method for deriv-
ing a dependency tree from a CCG derivation tree
for PF-CCG, a large fragment of CCG. We then
explore the strong generative capacity of PF-CCG
in terms of dependency trees. In particular, we cast
new light on the relationship between CCG and
other mildly context-sensitive formalisms such as
Tree-Adjoining Grammar (TAG; Joshi and Schabes
(1997)) and Linear Context-Free Rewrite Systems
(LCFRS; Vijay-Shanker et al. (1987)). We show
that if we only look at valencies and ignore word
order, then the dependency trees induced by a PF-
CCG grammar form a regular tree language, just
as for TAG and LCFRS. To our knowledge, this is
the first time that the regularity of CCG’s deriva-
tional structures has been exposed. However, if we
take the word order into account, then the classes
of PF-CCG-induced and TAG-induced dependency
trees are incomparable; in particular, CCG-induced
dependency trees can be unboundedly non-project-
ive in a way that TAG-induced dependency trees
cannot.
The fact that all our dependency structures are
trees brings our approach in line with the emerging
mainstream in dependency parsing (McDonald et
al., 2005; Nivre et al., 2007) and TAG derivation
trees. The price we pay for restricting ourselves to
trees is that we derive fewer dependencies than the
more powerful approach by Clark et al. (2002). In-
deed, we do not claim that our dependencies are lin-
guistically meaningful beyond recording the way in
which syntactic valencies are filled. However, we
show that our dependency trees are still informative
enough to reconstruct the semantic representations.
The paper is structured as follows. In Section 2,
we introduce CCG and the fragment PF-CCG that
we consider in this paper, and compare our contri-
bution to earlier research. In Section 3, we then
show how to read off a dependency tree from a
CCG derivation. Finally, we explore the strong
generative capacity of CCG in Section 4 and con-
clude with ideas for future work.
</bodyText>
<note confidence="0.924217">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 460–468,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998637">
460
</page>
<figure confidence="0.9802859375">
mer
np : we&apos; L
em Hans
np : Hans&apos; L
es huus
np : house&apos; L
hälfed aastriche
((s\np)\np)/vp : help&apos; L vp\np : paint&apos; L
F
((s\np)\np)\np : Ax. help&apos;(paint&apos;(x))
B
(s\np)\np : help&apos; (paint&apos;(house&apos;))
B
s : help&apos; (paint&apos;(house&apos;)) Hans&apos; we&apos;
B
s\np : help&apos; (paint&apos;(house&apos;)) Hans&apos;
</figure>
<figureCaption confidence="0.999901">
Figure 1: A PF-CCG derivation
</figureCaption>
<sectionHeader confidence="0.979077" genericHeader="method">
2 Combinatory Categorial Grammars
</sectionHeader>
<bodyText confidence="0.99998675">
We start by introducing the Combinatory Categorial
Grammar (CCG) formalism. Then we introduce
the fragment of CCG that we consider in this paper,
and discuss some related work.
</bodyText>
<subsectionHeader confidence="0.91003">
2.1 CCG
</subsectionHeader>
<bodyText confidence="0.999725955555556">
Combinatory Categorial Grammar (Steedman,
2001) is a grammar formalism that assigns categor-
ies to substrings of an input sentence. There are
atomic categories such as s and np; and if A and B
are categories, then A\B and A/B are functional
categories representing a constituent that will have
category A once it is combined with another con-
stituent of type B to the left or right, respectively.
Each word is assigned a category by the lexicon;
adjacent substrings can then be combined by com-
binatory rules. As an example, Steedman and Bald-
ridge’s (2009) analysis of Shieber’s (1985) Swiss
German subordinate clause (das) mer em Hans es
huus hälfed aastriiche (‘(that) we help Hans paint
the house’) is shown in Figure 1.
Intuitively, the arguments of a functional cat-
egory can be thought of as the syntactic valencies
of the lexicon entry, or as arguments of a func-
tion that maps categories to categories. The core
combinatory mechanism underlying CCG is the
composition and application of these functions. In
their most general forms, the combinatory rules of
(forward and backward) application and compos-
ition can be written as in Figure 2. The symbol �
stands for an arbitrary (forward or backward) slash;
it is understood that the slash before each BZ above
the line is the same as below. The rules derive state-
ments about triples w �- A : f, expressing that the
substring w can be assigned the category A and the
semantic representation f; an entire string counts
as grammatical if it can be assigned the start cat-
egory s. In parallel to the combination of substrings
by the combinatory rules, their semantic represent-
ations are combined by functional composition.
We have presented the composition rules of CCG
in their most general form. In the literature, the
special cases for n = 0 are called forward and
backward application; the cases for n &gt; 0 where
the slash before Bn is the same as the slash be-
fore B are called composition of degree n; and
the cases where n &gt; 0 and the slashes have dif-
ferent directions are called crossed composition of
degree n. For instance, the F application that com-
bines hälfed and aastriche in Figure 1 is a forward
crossed composition of degree 1.
</bodyText>
<subsectionHeader confidence="0.998885">
2.2 PF-CCG
</subsectionHeader>
<bodyText confidence="0.999938344827586">
In addition to the composition rules introduced
above, CCG also allows rules of substitution and
type-raising. Substitution is used to handle syn-
tactic phenomena such as parasitic gaps; type-rais-
ing allows a constituent to serve syntactically as a
functor, while being used semantically as an argu-
ment. Furthermore, it is possible in CCG to restrict
the instances of the rule schemata in Figure 2—for
instance, to say that the application rule may only
be used for the case A = s. We call a CCG gram-
mar pure if it does not use substitution, type-raising,
or restricted rule schemata. Finally, the argument
categories of a CCG category may themselves be
functional categories; for instance, the category of
a VP modifier like passionately is (s\np)\(s\np).
We call a category that is either atomic or only has
atomic arguments a first-order category, and call a
CCG grammar first-order if all categories that its
lexicon assigns to words are first-order.
In this paper, we only consider CCG grammars
that are pure and first-order. This fragment, which
we call PF-CCG, is less expressive than full CCG,
but it significantly simplifies the definitions in Sec-
tion 3. At the same time, many real-world CCG
grammars do not use the substitution rule, and type-
raising can be compiled into the grammar in the
sense that for any CCG grammar, there is an equi-
valent CCG grammar that does not use type-raising
and assigns the same semantic representations to
</bodyText>
<page confidence="0.99592">
461
</page>
<figure confidence="0.8821585">
(a, A, f) is a lexical entry L
a � A : f
v �- A/B : λx. f(x) w �- B  |Bn  |...  |B1 : λy1,...,yn. g(y1, ...,yn)
vw �- A  |Bn  |...  |B1 : λy1, ... , yn. f(g(y1, ... , yn))
v �- B  |Bn  |...  |B1 : λy1, ... , yn. g(y1, ... , yn) w �- A\B : λx. f(x)
vw �- A  |Bn  |...  |B1 : λy1, ... , yn. f(g(y1, ... , yn))
</figure>
<figureCaption confidence="0.983808">
Figure 2: The generalized combinatory rules of CCG
</figureCaption>
<figure confidence="0.7872175">
F
B
</figure>
<bodyText confidence="0.996455833333333">
each string. On the other hand, the restriction to
first-order grammars is indeed a limitation in prac-
tice. We take the work reported here as a first step
towards a full dependency-tree analysis of CCG,
and discuss ideas for generalization in the conclu-
sion.
</bodyText>
<sectionHeader confidence="0.732025" genericHeader="method">
2.3 Related work
</sectionHeader>
<bodyText confidence="0.999977387096774">
The main objective of this paper is the definition
of a novel way in which dependency trees can
be extracted from CCG derivations. This is sim-
ilar to Clark et al. (2002), who aim at capturing
‘deep’ dependencies, and encode these into annot-
ated lexical categories. For instance, they write
(npi\npi)/(s\npi) for subject relative pronouns to
express that the relative pronoun, the trace of the
relative clause, and the modified noun phrase are
all semantically the same. This means that the rel-
ative pronoun has multiple parents; in general, their
dependency structures are not necessarily trees. By
contrast, we aim to extract only dependency trees,
and achieve this by recording only the fillers of syn-
tactic valencies, rather than the semantic dependen-
cies: the relative pronoun gets two dependents and
one parent (the verb whose argument the modified
np is), just as the category specifies. So Clark et
al.’s and our dependency approach represent two
alternatives of dealing with the tradeoff between
simple and expressive dependency structures.
Our paper differs from the well-known results
of Vijay-Shanker and Weir (1994) in that they es-
tablish the weak equivalence of different grammar
formalisms, while we focus on comparing the deriv-
ational structures. Hockenmaier and Young (2008)
present linguistic motivations for comparing the
strong generative capacities of CCG and TAG, and
the beginnings of a formal comparison between
CCG and spinal TAG in terms of Linear Indexed
Grammars.
</bodyText>
<sectionHeader confidence="0.96663" genericHeader="method">
3 Induction of dependency trees
</sectionHeader>
<bodyText confidence="0.999960833333333">
We now explain how to extract a dependency tree
from a PF-CCG derivation. The basic idea is to
associate, with every step of the derivation, a cor-
responding operation on dependency trees, in much
the same way as derivation steps can be associated
with operations on semantic representations.
</bodyText>
<subsectionHeader confidence="0.995708">
3.1 Dependency trees
</subsectionHeader>
<bodyText confidence="0.975796535714285">
When talking about a dependency tree, it is usually
convenient to specify its tree structure and the lin-
ear order of its nodes separately. The tree structure
encodes the valency structure of the sentence (im-
mediate dominance), whereas the linear precedence
of the words is captured by the linear order.
For the purposes of this paper, we represent a
dependency tree as a pair d = (t, s), where t is a
ground term over some suitable alphabet, and s is
a linearization of the nodes (term addresses) of t,
where by a linearization of a set S we mean a list of
elements of S in which each element occurs exactly
once (see also Kuhlmann and Möhl (2007)). As
examples, consider
(f(a, b), [1, ε, 2]) and (f(g(a)), [1 · 1, ε,1]) .
These expressions represent the dependency trees
d1 = and d2 = .
a f b a f g
Notice that it is because of the separate specifica-
tion of the tree and the order that dependency trees
can become non-projective; d2 is an example.
A partial dependency tree is a pair (t, s) where t
is a term that may contain variables, and s is a
linearization of those nodes of t that are not labelled
with variables. We restrict ourselves to terms in
which each variable appears exactly once, and will
also prefix partial dependency trees with λ-binders
to order the variables.
</bodyText>
<page confidence="0.997099">
462
</page>
<figure confidence="0.997745375">
e = (a, A  |Am · · ·  |A1) is a lexical entry
L
a ` A  |Am ···  |A1 : λx1,... , xm. (e(x1, ... , xm), [ε])
v ` A  |Am ···  |A1/B : λx,x1,...,xm. d w ` B  |Bn ···  |B1 : λy1,...,yn. d&apos;
F
vw ` A  |Am ···  |A1  |Bn ···  |B1 : λy1,...,yn,x1,...,xm. d[x := d&apos; ]F
w ` B  |Bn ···  |B1 : λy1,...,yn. d&apos; v ` A  |Am ···  |A1\B : λx, x1,...,xm. d B
wv ` A  |Am ···  |A1  |Bn ···  |B1 : λy1,...,yn,x1,...,xm. d[x := d&apos; ]B
</figure>
<figureCaption confidence="0.999788">
Figure 3: Computing dependency trees in CCG derivations
</figureCaption>
<subsectionHeader confidence="0.99963">
3.2 Operations on dependency trees
</subsectionHeader>
<bodyText confidence="0.952385842105263">
Let t be a term, and let x be a variable in t. The
result of the substitution of the term t&apos; into t for x
is denoted by t[ x := t&apos; ]. We extend this opera-
tion to dependency trees as follows. Given a list
of addresses s, let xs be the list of addresses ob-
tained from s by prefixing every address with the
address of the (unique) node that is labelled with x
in t. Then the operations of forward and backward
concatenation are defined as
(t, s)[ x := (t&apos;, s&apos;) ]F = (t[ x := t&apos; ], s · xs&apos;) ,
(t, s)[ x := (t&apos;, s&apos;) ]B = (t[ x := t&apos; ], xs&apos; · s) .
The concatenation operations combine two given
dependency trees (t, s) and (t&apos;, s&apos;) into a new tree
by substituting t&apos; into t for some variable x of t,
and adding the (appropriately prefixed) list s&apos; of
nodes of t&apos; either before or after the list s of nodes
of t. Using these two operations, the dependency
trees d1 and d2 from above can be written as fol-
lows. Let da = (a, [ε]) and db = (b, [ε]).
</bodyText>
<equation confidence="0.997963">
d1 = (f(x,y), [ε])[x := da]F[y := db ]F
d2 = (f(x), [ε])[x := (g(y), [ε]) ]F[y := da]B
</equation>
<bodyText confidence="0.999906333333333">
Here is an alternative graphical notation for the
composition of d2:
In this notation, nodes that are not marked with
variables are positioned (indicated by the dotted
projection lines), while the (dashed) variable nodes
dangle unpositioned.
</bodyText>
<subsectionHeader confidence="0.990774">
3.3 Dependency trees for CCG derivations
</subsectionHeader>
<bodyText confidence="0.993025054054054">
To encode CCG derivations as dependency trees,
we annotate each composition rule of PF-CCG with
instructions for combining the partial dependency
trees for the substrings into a partial dependency
tree for the larger string. Essentially, we now com-
bine partial dependency trees using forward and
backward concatenation rather than combining se-
mantic representations by functional composition
and application. From now on, we assume that the
node labels in the dependency trees are CCG lex-
icon entries, and represent these by just the word
in them.
The modified rules are shown in Figure 3. They
derive statements about triples w ` A : p, where w
is a substring, A is a category, and p is a lambda
expression over a partial dependency tree. Each
variable of p corresponds to an argument category
in A, and vice versa. Rule L covers the base case:
the dependency tree for a lexical entry e is a tree
with one node for the item itself, labelled with e,
and one node for each of its syntactic arguments,
labelled with a variable. Rule F captures forward
composition: given two dependency trees d and d&apos;,
the new dependency tree is obtained by forward
concatenation, binding the outermost variable in d.
Rule B is the rule for backward composition. The
result of translating a complete PF-CCG derivation
δ in this way is always a dependency tree without
variables; we call it d(δ).
As an example, Figure 4 shows the construc-
tion for the derivation in Figure 1. The induced
dependency tree looks like this:
mer em Hans es huus hälfed aastriche
For instance, the partial dependency tree for the
lexicon entry of aastriiche contains two nodes: the
root (with address ε) is labelled with the lexicon
entry, and its child (address 1) is labelled with the
</bodyText>
<figure confidence="0.99589975">
f g
y
2
6 4y
a
3
57 B
a f g
</figure>
<page confidence="0.798641">
463
</page>
<figure confidence="0.997637421052632">
(Hans, [E]) L
em Hans
F
�w, y, z. (hälfed(aastriiche(w), y, z), [E, 1])
B
�y, z. (hälfed(aastriiche(huus), y, z), [11, E, 1])
B
Az. (hälfed(aastriiche(huus), Hans, z), [2, 11, E, 1])
es huus
(huus, [E]) L
hälfed
L
Ax, y, z. (hälfed(x, y, z), [E])
aastriiche
Aw. (aastriiche(w), [E]) L
mer
(mer, [E]) L
B
(hälfed(aastriiche(huus), Hans, mer), [3,2, 11, E, 1])
</figure>
<figureCaption confidence="0.999598">
Figure 4: Computing a dependency tree for the derivation in Figure 1
</figureCaption>
<bodyText confidence="0.8599687">
variable x. This tree is inserted into the tree from
hälfed by forward concatenation. The variable w is
passed on into the new dependency tree, and later
filled by backward concatenation to huus. Passing
the argument slot of aastriiche to hälfed to be filled
on its left creates a non-projectivity; it corresponds
to a crossed composition in CCG terms. Notice
that the categories derived in Figure 1 mirror the
functional structure of the partial dependency trees
at each step of the derivation.
</bodyText>
<subsectionHeader confidence="0.993447">
3.4 Semantic equivalence
</subsectionHeader>
<bodyText confidence="0.999973233333333">
The mapping from derivations to dependency trees
loses some information: different derivations may
induce the same dependency tree. This is illus-
trated by Figure 5, which provides two possible
derivations for the phrase big white rabbit, both
of which induce the same dependency tree. Espe-
cially in light of the fact that our dependency trees
will typically contain fewer dependencies than the
DAGs derived by Clark et al. (2002), one could ask
whether dependency trees are an appropriate way
of representing the structure of a CCG derivation.
However, at the end of the day, the most import-
ant information that can be extracted from a CCG
derivation is the semantic representation it com-
putes; and it is possible to reconstruct the semantic
representation of a derivation S from d(S) alone. If
we forget the word order information in the depend-
ency trees, the rules F and B in Figure 3 are merely
,7-expanded versions of the semantic construction
rules in Figure 2. This means that d(S) records
everything we need to know about constructing the
semantic representation: We can traverse it bottom-
up and apply the lexical semantic representation
of each node to those of its subterms. So while
the dependency trees obliterate some information
in the CCG derivations (particularly its associative
structure), they are indeed appropriate represent-
ations because they record all syntactic valencies
and encode enough information to recompute the
semantics.
</bodyText>
<sectionHeader confidence="0.960912" genericHeader="method">
4 Strong generative capacity
</sectionHeader>
<bodyText confidence="0.9979426875">
Now that we know how to see PF-CCG derivations
as dependency trees, we can ask what sets of such
trees can be generated by PF-CCG grammars. This
is the question about the strong generative capa-
city of PF-CCG, measured in terms of dependency
trees (Miller, 2000). In this section, we give a
partial answer to this question: We show that the
sets of PF-CCG-induced valency trees (dependency
trees without their linear order) form regular tree
languages, but that the sets of dependency trees
themselves are irregular. This is in contrast to other
prominent mildly context-sensitive grammar form-
alisms such as Tree Adjoining Grammar (TAG;
Joshi and Schabes (1997)) and Linear Context-
Free Rewrite Systems (LCFRS; Vijay-Shanker et
al. (1987)), in which both languages are regular.
</bodyText>
<subsectionHeader confidence="0.996056">
4.1 CCG term languages
</subsectionHeader>
<bodyText confidence="0.999918">
Formally, we define the language of all dependency
trees generated by a PF-CCG grammar G as the set
</bodyText>
<equation confidence="0.968873">
LD(G) = { d(S)  |S is a derivation of G }.
</equation>
<bodyText confidence="0.999219">
Furthermore, we define the set of valency trees to
be the set of just the term parts of each d(S):
</bodyText>
<equation confidence="0.94848">
LV (G) = {t  |(t, s) ∈ LD(G) }.
</equation>
<bodyText confidence="0.998473142857143">
By our previous assumption, the node labels of a
valency tree are CCG lexicon entries.
We will now show that the valency tree lan-
guages of PF-CCG grammars are regular tree lan-
guages (Gécseg and Steinby, 1997). Regular tree
languages are sets of trees that can be generated
by regular tree grammars. Formally, a regular tree
grammar (RTG) is a construct T = (N, E, 5, P),
where N is an alphabet of non-terminal symbols,
E is an alphabet of ranked term constructors called
terminal symbols, 5 ∈ N is a distinguished start
symbol, and P is a finite set of production rules of
the form A → &apos;y, where A ∈ N and -y is a term
over E and N, where the nonterminals can be used
</bodyText>
<page confidence="0.999227">
464
</page>
<figure confidence="0.9996299375">
big
np/np
white
np/np
rabbit
np
big
np/np
white
np/np
np/np
rabbit
np
np/np
big white rabbit
np
</figure>
<figureCaption confidence="0.996356">
Figure 5: Different derivations may induce the same dependency tree
</figureCaption>
<bodyText confidence="0.98632632">
np
as constants. The grammar F generates trees from
the start symbol by successively expanding occur-
rences of nonterminals using production rules. For
instance, the grammar that contains the productions
5 —* f(A, A), A —* g(A), and A —* a generates
the tree language { f(gm(a), gn(a))  |m, n &gt; 0 }.
We now construct an RTG F(G) that generates
the set of valency trees of a PF-CCG G. For the
terminal alphabet, we choose the lexicon entries:
If e = (a, A  |Bi ...  |Bn, f) is a lexicon entry of
G, we take e as an n-ary term constructor. We also
take the atomic categories of G as our nonterminal
symbols; the start category s of G counts as the
start symbol. Finally, we encode each lexicon entry
as a production rule: The lexicon entry e above
encodes to the rule A —* e(Bn, ... , Bi).
Let us look at our running example to see how
this works. Representing the lexicon entries as just
the words for brevity, we can write the valency tree
corresponding to the CCG derivation in Figure 4
as to = hälfed(aastriiche(huus), Hans, mer); here
hälfed is a ternary constructor, aastriiche is unary,
and all others are constants. Taking the lexical
categories into account, we obtain the RTG with
</bodyText>
<equation confidence="0.995242333333333">
s —* hälfed(vp, np, np)
vp —* aastriiche(np)
np —* huus  |Hans  |mer
</equation>
<bodyText confidence="0.99899895">
This grammar indeed generates to, and all other
valency trees induced by the sample grammar.
More generally, LV (G) C_ L(F(G)) because
the construction rules in Figure 3 ensure that if
a node v becomes the i-th child of a node u in
the term, then the result category of v’s lexicon
entry equals the i-th argument category of u’s lex-
icon entry. This guarantees that the i-th nonter-
minal child introduced by the production for u can
be expanded by the production for v. The con-
verse inclusion can be shown by reconstructing,
for each valency tree t, a CCG derivation S that
induces t. This construction can be done by ar-
ranging the nodes in t into an order that allows
us to combine every parent in t with its children
using only forward and backward application. The
CCG derivation we obtain for the example is shown
in Figure 6; it is a derivation for the sentence
das mer em Hans hälfed es huus aastriiche, using
the same lexicon entries. Together, this shows that
</bodyText>
<equation confidence="0.789318333333333">
L(F(G)) = LV (G). Thus:
Theorem 1 The sets of valency trees generated by
PF-CCG are regular tree languages. �
</equation>
<bodyText confidence="0.9999174">
By this result, CCG falls in line with context-free
grammars, TAG, and LCFRS, whose sets of deriva-
tional structures are all regular (Vijay-Shanker et
al., 1987). To our knowledge, this is the first time
the regular structure of CCG derivations has been
exposed. It is important to note that while CCG
derivations themselves can be seen as trees as well,
they do not always form regular tree languages
(Vijay-Shanker et al., 1987). Consider for instance
the CCG grammar from Vijay-Shanker and Weir’s
(1994) Example 2.4, which generates the string lan-
guage anbncndn; Figure 7 shows the derivation of
aabbccdd. If we follow this derivation bottom-up,
starting at the first c, the intermediate categories
collect an increasingly long tail of\a arguments; for
longer words from the language, this tail becomes
as long as the number of cs in the string. The in-
finite set of categories this produces translates into
the need for an infinite nonterminal alphabet in an
RTG, which is of course not allowed.
</bodyText>
<subsectionHeader confidence="0.999669">
4.2 Comparison with TAG
</subsectionHeader>
<bodyText confidence="0.9998784">
If we now compare PF-CCG to its most promin-
ent mildly context-sensitive cousin, TAG, the reg-
ularity result above paints a suggestive picture: A
PF-CCG valency tree assigns a lexicon entry to
each word and says which other lexicon entry fills
each syntactic valency. In this respect, it is the
analogue of a TAG derivation tree (in which the
lexicon entries are elementary trees), and we just
saw that PF-CCG and TAG generate the same tree
languages. On the other hand, CCG and TAG are
weakly equivalent (Vijay-Shanker and Weir, 1994),
i.e. they generate the same linear word orders. So
one could expect that CCG and TAG also induce
the same dependency trees. Interestingly, this is
not the case.
</bodyText>
<page confidence="0.999401">
465
</page>
<figure confidence="0.995078947368421">
es huus aastriiche
hälfed
L
s\np\np/vp
s\np B B
L L
vp\np
vp
mer
L
np
em Hans
L
np
np
B
F
s\np\np
s
</figure>
<figureCaption confidence="0.999961">
Figure 6: CCG derivation reconstructed from the dependency tree from Figure 4 using only applications
</figureCaption>
<bodyText confidence="0.999978714285714">
We know from the literature that those depend-
ency trees that can be constructed from TAG deriva-
tion trees are exactly those that are well-nested and
have a block-degree of at most 2 (Kuhlmann and
Möhl, 2007). The block-degree of a node u in a de-
pendency tree is the number of ‘blocks’ into which
the subtree below u is separated by intervening
nodes that are not below u, and the block-degree
of a dependency tree is the maximum block-degree
of its nodes. So for instance, the dependency tree
on the right-hand side of Figure 8 has block-degree
two. It is also well-nested, and can therefore be
induced by TAG derivations.
Things are different for the dependency trees that
can be induced by PF-CCG. Consider the left-hand
dependency tree in Figure 8, which is induced by
a PF-CCG derivation built from words with the
lexical categories a/a, b\a, b\b, and a. While
this dependency tree is well-nested, it has block-
degree three: The subtree below the leftmost node
consists of three parts. More generally, we can in-
sert more words with the categories a/a and b\b
in the middle of the sentence to obtain depend-
ency trees with arbitrarily high block-degrees from
this grammar. This means that unlike for TAG-
induced dependency trees, there is no upper bound
on the block-degree of dependency trees induced
by PF-CCG—as a consequence, there are CCG
dependency trees that cannot be induced by TAG.
On the other hand, there are also dependency
trees that can be induced by TAG, but not by PF-
CCG. The tree on the right-hand side of Figure 8
is an example. We have already argued that this
tree can be induced by a TAG. However, it con-
tains no two adjacent nodes that are connected by
</bodyText>
<note confidence="0.326662">
a/a b\a a/a b\b a b\b 1 2 3 4
</note>
<figureCaption confidence="0.999629">
Figure 8: The divergence between CCG and TAG
</figureCaption>
<bodyText confidence="0.986390076923077">
an edge; and every nontrivial PF-CCG derivation
must combine two adjacent words at least at one
point during the derivation. Therefore, the tree
cannot be induced by a PF-CCG grammar. Further-
more, it is known that all dependency languages
that can be generated by TAG or even, more gener-
ally, by LCRFS, are regular in the sense of Kuhl-
mann and Möhl (2007). One crucial property of
regular dependency languages is that they have a
bounded block-degree; but as we have seen, there
are PF-CCG dependency languages with unboun-
ded block-degree. Therefore there are PF-CCG
dependency languages that are not regular. Hence:
Theorem 2 The sets of dependency trees gener-
ated by PF-CCG and TAG are incomparable. ❑
We believe that these results will generalize to
full CCG. While we have not yet worked out the
induction of dependency trees from full CCG, the
basic rule that CCG combines adjacent substrings
should still hold; therefore, every CCG-induced
dependency tree will contain at least one edge
between adjacent nodes. We are thus left with
a very surprising result: TAG and CCG both gener-
ate the same string languages and the same sets of
valency trees, but they use incomparable mechan-
isms for linearizing valency trees into sentences.
</bodyText>
<subsectionHeader confidence="0.99816">
4.3 A note on weak generative capacity
</subsectionHeader>
<bodyText confidence="0.997610357142857">
As a final aside, we note that the construction for
extracting purely applicative derivations from the
terms described by the RTG has interesting con-
sequences for the weak generative capacity of PF-
CCG. In particular, it has the corollary that for any
PF-CCG derivation 6 over a string w, there is a per-
mutation of w that can be accepted by a PF-CCG
derivation that uses only application—that is, every
string language L that can be generated by a PF-
CCG grammar has a context-free sublanguage L&apos;
such that all words in L are permutations of words
in L&apos;.
This means that many string languages that we
commonly associate with CCG cannot be generated
</bodyText>
<page confidence="0.999074">
466
</page>
<figure confidence="0.999885866666667">
b
b L
b
b L
c
s\a/t\b L
B
s\a/t
F
s\a\a\b
c
t\a\b L
a
a/d L
a
a/d L
B
s/d
B
s\a/d
B
s\a\a
F
s\a
d
d L
d
d L
F
s
</figure>
<figureCaption confidence="0.999996">
Figure 7: The CCG derivation of aabbccdd using Example 2.4 in Vijay-Shanker and Weir (1994)
</figureCaption>
<bodyText confidence="0.999490307692308">
by PF-CCG. One such language is anbncndn. This
language is not itself context-free, and therefore
any PF-CCG grammar whose language contains it
also contains permutations in which the order of
the symbols is mixed up. The culprit for this among
the restrictions that distinguish PF-CCG from full
CCG seems to be that PF-CCG grammars must
allow all instances of the application rules. This
would mean that the ability of CCG to generate non-
context-free languages (also linguistically relevant
ones) hinges crucially on its ability to restrict the
allowable instances of rule schemata, for instance,
using slash types (Baldridge and Kruijff, 2003).
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999969704918033">
In this paper, we have shown how to read deriva-
tions of PF-CCG as dependency trees. Unlike pre-
vious proposals, our view on CCG dependencies
is in line with the mainstream dependency parsing
literature, which assumes tree-shaped dependency
structures; while our dependency trees are less in-
formative than the CCG derivations themselves,
they contain sufficient information to reconstruct
the semantic representation. We used our new de-
pendency view to compare the strong generative
capacity of PF-CCG with other mildly context-
sensitive grammar formalisms. It turns out that
the valency trees generated by a PF-CCG grammar
form regular tree languages, as in TAG and LCFRS;
however, unlike these formalisms, the sets of de-
pendency trees including word order are not regular,
and in particular can be more non-projective than
the other formalisms permit. Finally, we found
new formal evidence for the importance of restrict-
ing rule schemata for describing non-context-free
languages in CCG.
All these results were technically restricted to
the fragment of PF-CCG, and one focus of future
work will be to extend them to as large a fragment
of CCG as possible. In particular, we plan to extend
the lambda notation used in Figure 3 to cover type-
raising and higher-order categories. We would then
be set to compare the behavior of wide-coverage
statistical parsers for CCG with statistical depend-
ency parsers.
We anticipate that our results about the strong
generative capacity of PF-CCG will be useful to
transfer algorithms and linguistic insights between
formalisms. For instance, the CRISP generation
algorithm (Koller and Stone, 2007), while specified
for TAG, could be generalized to arbitrary gram-
mar formalisms that use regular tree languages—
given our results, to CCG in particular. On the
other hand, we find it striking that CCG and TAG
generate the same string languages from the same
tree languages by incomparable mechanisms for
ordering the words in the tree. Indeed, the exact
characterization of the class of CCG-inducable de-
pendency languages is an open issue. This also
has consequences for parsing complexity: We can
understand why TAG and LCFRS can be parsed in
polynomial time from the bounded block-degree
of their dependency trees (Kuhlmann and Möhl,
2007), but CCG can be parsed in polynomial time
(Vijay-Shanker and Weir, 1990) without being re-
stricted in this way. This constitutes a most inter-
esting avenue of future research that is opened up
by our results.
Acknowledgments. We thank Mark Steedman,
Jason Baldridge, and Julia Hockenmaier for valu-
able discussions about CCG, and the reviewers for
their comments. The work of Alexander Koller
was funded by a DFG Research Fellowship and the
Cluster of Excellence “Multimodal Computing and
Interaction”. The work of Marco Kuhlmann was
funded by the Swedish Research Council.
</bodyText>
<page confidence="0.99903">
467
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999809863013699">
Jason Baldridge and Geert-Jan M. Kruijff. 2003.
Multi-modal Combinatory Categorial Grammar. In
Proceedings of the Tenth EACL, Budapest, Hungary.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of the 20th COLING, Geneva,
Switzerland.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Stephen Clark, Julia Hockenmaier, and Mark Steed-
man. 2002. Building deep dependency structures
with a wide-coverage CCG parser. In Proceedings
of the 40th ACL, Philadelphia, USA.
Ferenc Gécseg and Magnus Steinby. 1997. Tree lan-
guages. In Rozenberg and Salomaa (Rozenberg and
Salomaa, 1997), pages 1–68.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Julia Hockenmaier and Peter Young. 2008. Non-local
scrambling: the equivalence of TAG and CCG re-
visited. In Proceedings of TAG+9, Tübingen, Ger-
many.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of COLING/ACL, Sydney, Australia.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Rozenberg and Salomaa
(Rozenberg and Salomaa, 1997), pages 69–123.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of the 45th
ACL, Prague, Czech Republic.
Marco Kuhlmann and Mathias Möhl. 2007. Mildly
context-sensitive dependency languages. In Pro-
ceedings of the 45th ACL, Prague, Czech Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP.
Philip H. Miller. 2000. Strong Generative Capacity:
The Semantics of Linguistic Formalism. University
of Chicago Press.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Grzegorz Rozenberg and Arto Salomaa, editors. 1997.
Handbook of Formal Languages. Springer.
Stuart Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philo-
sophy, 8:333–343.
Mark Steedman and Jason Baldridge. 2009. Combin-
atory categorial grammar. In R. Borsley and K. Bor-
jars, editors, Non-Transformational Syntax. Black-
well. To appear.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
K. Vijay-Shanker and David Weir. 1990. Polynomial
time parsing of combinatory categorial grammars.
In Proceedings of the 28th ACL, Pittsburgh, USA.
K. Vijay-Shanker and David J. Weir. 1994. The equi-
valence of four extensions of context-free grammars.
Mathematical Systems Theory, 27(6):511–546.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Pro-
ceedings of the 25th ACL, Stanford, CA, USA.
</reference>
<page confidence="0.999122">
468
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.926764">
<title confidence="0.995324">Dependency trees and the strong generative capacity of CCG</title>
<author confidence="0.999924">Alexander Koller</author>
<affiliation confidence="0.999999">Saarland University</affiliation>
<address confidence="0.994642">Saarbrücken, Germany</address>
<email confidence="0.998738">koller@mmci.uni-saarland.de</email>
<author confidence="0.999913">Marco Kuhlmann</author>
<affiliation confidence="0.999998">Uppsala University</affiliation>
<address confidence="0.963705">Uppsala, Sweden</address>
<email confidence="0.975219">marco.kuhlmann@lingfil.uu.se</email>
<abstract confidence="0.999717">We propose a novel algorithm for extracting dependencies from the derivations of a large fragment of CCG. Unlike earlier proposals, our dependency structures are always tree-shaped. We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>Multi-modal Combinatory Categorial Grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the Tenth EACL,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="28609" citStr="Baldridge and Kruijff, 2003" startWordPosition="4985" endWordPosition="4988">ch language is anbncndn. This language is not itself context-free, and therefore any PF-CCG grammar whose language contains it also contains permutations in which the order of the symbols is mixed up. The culprit for this among the restrictions that distinguish PF-CCG from full CCG seems to be that PF-CCG grammars must allow all instances of the application rules. This would mean that the ability of CCG to generate noncontext-free languages (also linguistically relevant ones) hinges crucially on its ability to restrict the allowable instances of rule schemata, for instance, using slash types (Baldridge and Kruijff, 2003). 5 Conclusion In this paper, we have shown how to read derivations of PF-CCG as dependency trees. Unlike previous proposals, our view on CCG dependencies is in line with the mainstream dependency parsing literature, which assumes tree-shaped dependency structures; while our dependency trees are less informative than the CCG derivations themselves, they contain sufficient information to reconstruct the semantic representation. We used our new dependency view to compare the strong generative capacity of PF-CCG with other mildly contextsensitive grammar formalisms. It turns out that the valency </context>
</contexts>
<marker>Baldridge, Kruijff, 2003</marker>
<rawString>Jason Baldridge and Geert-Jan M. Kruijff. 2003. Multi-modal Combinatory Categorial Grammar. In Proceedings of the Tenth EACL, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Widecoverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1169" citStr="Bos et al., 2004" startWordPosition="162" endWordPosition="165">sms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Widecoverage semantic representations from a CCG parser. In Proceedings of the 20th COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1023" citStr="Clark and Curran, 2007" startWordPosition="143" endWordPosition="146">s tree-shaped. We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG </context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building deep dependency structures with a wide-coverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="3194" citStr="Clark et al. (2002)" startWordPosition="490" endWordPosition="493">exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (McDonald et al., 2005; Nivre et al., 2007) and TAG derivation trees. The price we pay for restricting ourselves to trees is that we derive fewer dependencies than the more powerful approach by Clark et al. (2002). Indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled. However, we show that our dependency trees are still informative enough to reconstruct the semantic representations. The paper is structured as follows. In Section 2, we introduce CCG and the fragment PF-CCG that we consider in this paper, and compare our contribution to earlier research. In Section 3, we then show how to read off a dependency tree from a CCG derivation. Finally, we explore the strong generative capacity of CCG in Section 4 and conclud</context>
<context position="8972" citStr="Clark et al. (2002)" startWordPosition="1508" endWordPosition="1511">|... |B1 : λy1, ... , yn. g(y1, ... , yn) w �- A\B : λx. f(x) vw �- A |Bn |... |B1 : λy1, ... , yn. f(g(y1, ... , yn)) Figure 2: The generalized combinatory rules of CCG F B each string. On the other hand, the restriction to first-order grammars is indeed a limitation in practice. We take the work reported here as a first step towards a full dependency-tree analysis of CCG, and discuss ideas for generalization in the conclusion. 2.3 Related work The main objective of this paper is the definition of a novel way in which dependency trees can be extracted from CCG derivations. This is similar to Clark et al. (2002), who aim at capturing ‘deep’ dependencies, and encode these into annotated lexical categories. For instance, they write (npi\npi)/(s\npi) for subject relative pronouns to express that the relative pronoun, the trace of the relative clause, and the modified noun phrase are all semantically the same. This means that the relative pronoun has multiple parents; in general, their dependency structures are not necessarily trees. By contrast, we aim to extract only dependency trees, and achieve this by recording only the fillers of syntactic valencies, rather than the semantic dependencies: the relat</context>
<context position="16871" citStr="Clark et al. (2002)" startWordPosition="2921" endWordPosition="2924">osition in CCG terms. Notice that the categories derived in Figure 1 mirror the functional structure of the partial dependency trees at each step of the derivation. 3.4 Semantic equivalence The mapping from derivations to dependency trees loses some information: different derivations may induce the same dependency tree. This is illustrated by Figure 5, which provides two possible derivations for the phrase big white rabbit, both of which induce the same dependency tree. Especially in light of the fact that our dependency trees will typically contain fewer dependencies than the DAGs derived by Clark et al. (2002), one could ask whether dependency trees are an appropriate way of representing the structure of a CCG derivation. However, at the end of the day, the most important information that can be extracted from a CCG derivation is the semantic representation it computes; and it is possible to reconstruct the semantic representation of a derivation S from d(S) alone. If we forget the word order information in the dependency trees, the rules F and B in Figure 3 are merely ,7-expanded versions of the semantic construction rules in Figure 2. This means that d(S) records everything we need to know about </context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002. Building deep dependency structures with a wide-coverage CCG parser. In Proceedings of the 40th ACL, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc Gécseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree languages. In Rozenberg and Salomaa (Rozenberg and Salomaa,</title>
<date>1997</date>
<pages>1--68</pages>
<contexts>
<context position="19203" citStr="Gécseg and Steinby, 1997" startWordPosition="3313" endWordPosition="3316">(1997)) and Linear ContextFree Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)), in which both languages are regular. 4.1 CCG term languages Formally, we define the language of all dependency trees generated by a PF-CCG grammar G as the set LD(G) = { d(S) |S is a derivation of G }. Furthermore, we define the set of valency trees to be the set of just the term parts of each d(S): LV (G) = {t |(t, s) ∈ LD(G) }. By our previous assumption, the node labels of a valency tree are CCG lexicon entries. We will now show that the valency tree languages of PF-CCG grammars are regular tree languages (Gécseg and Steinby, 1997). Regular tree languages are sets of trees that can be generated by regular tree grammars. Formally, a regular tree grammar (RTG) is a construct T = (N, E, 5, P), where N is an alphabet of non-terminal symbols, E is an alphabet of ranked term constructors called terminal symbols, 5 ∈ N is a distinguished start symbol, and P is a finite set of production rules of the form A → &apos;y, where A ∈ N and -y is a term over E and N, where the nonterminals can be used 464 big np/np white np/np rabbit np big np/np white np/np np/np rabbit np np/np big white rabbit np Figure 5: Different derivations may indu</context>
</contexts>
<marker>Gécseg, Steinby, 1997</marker>
<rawString>Ferenc Gécseg and Magnus Steinby. 1997. Tree languages. In Rozenberg and Salomaa (Rozenberg and Salomaa, 1997), pages 1–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="1074" citStr="Hockenmaier and Steedman, 2007" startWordPosition="149" endWordPosition="152">trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic p</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Peter Young</author>
</authors>
<title>Non-local scrambling: the equivalence of TAG and CCG revisited.</title>
<date>2008</date>
<booktitle>In Proceedings of TAG+9,</booktitle>
<location>Tübingen, Germany.</location>
<contexts>
<context position="1752" citStr="Hockenmaier and Young, 2008" startWordPosition="254" endWordPosition="257">rage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)). We show that if we only look at valencies and ignore wo</context>
<context position="10093" citStr="Hockenmaier and Young (2008)" startWordPosition="1680" endWordPosition="1683">this by recording only the fillers of syntactic valencies, rather than the semantic dependencies: the relative pronoun gets two dependents and one parent (the verb whose argument the modified np is), just as the category specifies. So Clark et al.’s and our dependency approach represent two alternatives of dealing with the tradeoff between simple and expressive dependency structures. Our paper differs from the well-known results of Vijay-Shanker and Weir (1994) in that they establish the weak equivalence of different grammar formalisms, while we focus on comparing the derivational structures. Hockenmaier and Young (2008) present linguistic motivations for comparing the strong generative capacities of CCG and TAG, and the beginnings of a formal comparison between CCG and spinal TAG in terms of Linear Indexed Grammars. 3 Induction of dependency trees We now explain how to extract a dependency tree from a PF-CCG derivation. The basic idea is to associate, with every step of the derivation, a corresponding operation on dependency trees, in much the same way as derivation steps can be associated with operations on semantic representations. 3.1 Dependency trees When talking about a dependency tree, it is usually co</context>
</contexts>
<marker>Hockenmaier, Young, 2008</marker>
<rawString>Julia Hockenmaier and Peter Young. 2008. Non-local scrambling: the equivalence of TAG and CCG revisited. In Proceedings of TAG+9, Tübingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Creating a CCGbank and a wide-coverage CCG lexicon for German.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1094" citStr="Hockenmaier, 2006" startWordPosition="153" endWordPosition="154">rative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scr</context>
</contexts>
<marker>Hockenmaier, 2006</marker>
<rawString>Julia Hockenmaier. 2006. Creating a CCGbank and a wide-coverage CCG lexicon for German. In Proceedings of COLING/ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>TreeAdjoining Grammars. In Rozenberg and Salomaa (Rozenberg and Salomaa,</title>
<date>1997</date>
<pages>69--123</pages>
<contexts>
<context position="2217" citStr="Joshi and Schabes (1997)" startWordPosition="331" endWordPosition="334">s and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)). We show that if we only look at valencies and ignore word order, then the dependency trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-in</context>
<context position="18584" citStr="Joshi and Schabes (1997)" startWordPosition="3197" endWordPosition="3200">erivations as dependency trees, we can ask what sets of such trees can be generated by PF-CCG grammars. This is the question about the strong generative capacity of PF-CCG, measured in terms of dependency trees (Miller, 2000). In this section, we give a partial answer to this question: We show that the sets of PF-CCG-induced valency trees (dependency trees without their linear order) form regular tree languages, but that the sets of dependency trees themselves are irregular. This is in contrast to other prominent mildly context-sensitive grammar formalisms such as Tree Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear ContextFree Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)), in which both languages are regular. 4.1 CCG term languages Formally, we define the language of all dependency trees generated by a PF-CCG grammar G as the set LD(G) = { d(S) |S is a derivation of G }. Furthermore, we define the set of valency trees to be the set of just the term parts of each d(S): LV (G) = {t |(t, s) ∈ LD(G) }. By our previous assumption, the node labels of a valency tree are CCG lexicon entries. We will now show that the valency tree languages of PF-CCG grammars are regular tree languages (Gécseg</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. TreeAdjoining Grammars. In Rozenberg and Salomaa (Rozenberg and Salomaa, 1997), pages 69–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Matthew Stone</author>
</authors>
<title>Sentence generation as planning.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="30255" citStr="Koller and Stone, 2007" startWordPosition="5243" endWordPosition="5246">e technically restricted to the fragment of PF-CCG, and one focus of future work will be to extend them to as large a fragment of CCG as possible. In particular, we plan to extend the lambda notation used in Figure 3 to cover typeraising and higher-order categories. We would then be set to compare the behavior of wide-coverage statistical parsers for CCG with statistical dependency parsers. We anticipate that our results about the strong generative capacity of PF-CCG will be useful to transfer algorithms and linguistic insights between formalisms. For instance, the CRISP generation algorithm (Koller and Stone, 2007), while specified for TAG, could be generalized to arbitrary grammar formalisms that use regular tree languages— given our results, to CCG in particular. On the other hand, we find it striking that CCG and TAG generate the same string languages from the same tree languages by incomparable mechanisms for ordering the words in the tree. Indeed, the exact characterization of the class of CCG-inducable dependency languages is an open issue. This also has consequences for parsing complexity: We can understand why TAG and LCFRS can be parsed in polynomial time from the bounded block-degree of their </context>
</contexts>
<marker>Koller, Stone, 2007</marker>
<rawString>Alexander Koller and Matthew Stone. 2007. Sentence generation as planning. In Proceedings of the 45th ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Mathias Möhl</author>
</authors>
<title>Mildly context-sensitive dependency languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11286" citStr="Kuhlmann and Möhl (2007)" startWordPosition="1886" endWordPosition="1889">pendency tree, it is usually convenient to specify its tree structure and the linear order of its nodes separately. The tree structure encodes the valency structure of the sentence (immediate dominance), whereas the linear precedence of the words is captured by the linear order. For the purposes of this paper, we represent a dependency tree as a pair d = (t, s), where t is a ground term over some suitable alphabet, and s is a linearization of the nodes (term addresses) of t, where by a linearization of a set S we mean a list of elements of S in which each element occurs exactly once (see also Kuhlmann and Möhl (2007)). As examples, consider (f(a, b), [1, ε, 2]) and (f(g(a)), [1 · 1, ε,1]) . These expressions represent the dependency trees d1 = and d2 = . a f b a f g Notice that it is because of the separate specification of the tree and the order that dependency trees can become non-projective; d2 is an example. A partial dependency tree is a pair (t, s) where t is a term that may contain variables, and s is a linearization of those nodes of t that are not labelled with variables. We restrict ourselves to terms in which each variable appears exactly once, and will also prefix partial dependency trees with</context>
<context position="24290" citStr="Kuhlmann and Möhl, 2007" startWordPosition="4225" endWordPosition="4228">weakly equivalent (Vijay-Shanker and Weir, 1994), i.e. they generate the same linear word orders. So one could expect that CCG and TAG also induce the same dependency trees. Interestingly, this is not the case. 465 es huus aastriiche hälfed L s\np\np/vp s\np B B L L vp\np vp mer L np em Hans L np np B F s\np\np s Figure 6: CCG derivation reconstructed from the dependency tree from Figure 4 using only applications We know from the literature that those dependency trees that can be constructed from TAG derivation trees are exactly those that are well-nested and have a block-degree of at most 2 (Kuhlmann and Möhl, 2007). The block-degree of a node u in a dependency tree is the number of ‘blocks’ into which the subtree below u is separated by intervening nodes that are not below u, and the block-degree of a dependency tree is the maximum block-degree of its nodes. So for instance, the dependency tree on the right-hand side of Figure 8 has block-degree two. It is also well-nested, and can therefore be induced by TAG derivations. Things are different for the dependency trees that can be induced by PF-CCG. Consider the left-hand dependency tree in Figure 8, which is induced by a PF-CCG derivation built from word</context>
<context position="26189" citStr="Kuhlmann and Möhl (2007)" startWordPosition="4565" endWordPosition="4569">PFCCG. The tree on the right-hand side of Figure 8 is an example. We have already argued that this tree can be induced by a TAG. However, it contains no two adjacent nodes that are connected by a/a b\a a/a b\b a b\b 1 2 3 4 Figure 8: The divergence between CCG and TAG an edge; and every nontrivial PF-CCG derivation must combine two adjacent words at least at one point during the derivation. Therefore, the tree cannot be induced by a PF-CCG grammar. Furthermore, it is known that all dependency languages that can be generated by TAG or even, more generally, by LCRFS, are regular in the sense of Kuhlmann and Möhl (2007). One crucial property of regular dependency languages is that they have a bounded block-degree; but as we have seen, there are PF-CCG dependency languages with unbounded block-degree. Therefore there are PF-CCG dependency languages that are not regular. Hence: Theorem 2 The sets of dependency trees generated by PF-CCG and TAG are incomparable. ❑ We believe that these results will generalize to full CCG. While we have not yet worked out the induction of dependency trees from full CCG, the basic rule that CCG combines adjacent substrings should still hold; therefore, every CCG-induced dependenc</context>
</contexts>
<marker>Kuhlmann, Möhl, 2007</marker>
<rawString>Marco Kuhlmann and Mathias Möhl. 2007. Mildly context-sensitive dependency languages. In Proceedings of the 45th ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="3003" citStr="McDonald et al., 2005" startWordPosition="457" endWordPosition="460">trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (McDonald et al., 2005; Nivre et al., 2007) and TAG derivation trees. The price we pay for restricting ourselves to trees is that we derive fewer dependencies than the more powerful approach by Clark et al. (2002). Indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled. However, we show that our dependency trees are still informative enough to reconstruct the semantic representations. The paper is structured as follows. In Section 2, we introduce CCG and the fragment PF-CCG that we consider in this paper, and compare our contribut</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip H Miller</author>
</authors>
<title>Strong Generative Capacity: The Semantics of Linguistic Formalism.</title>
<date>2000</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="18185" citStr="Miller, 2000" startWordPosition="3137" endWordPosition="3138">representation of each node to those of its subterms. So while the dependency trees obliterate some information in the CCG derivations (particularly its associative structure), they are indeed appropriate representations because they record all syntactic valencies and encode enough information to recompute the semantics. 4 Strong generative capacity Now that we know how to see PF-CCG derivations as dependency trees, we can ask what sets of such trees can be generated by PF-CCG grammars. This is the question about the strong generative capacity of PF-CCG, measured in terms of dependency trees (Miller, 2000). In this section, we give a partial answer to this question: We show that the sets of PF-CCG-induced valency trees (dependency trees without their linear order) form regular tree languages, but that the sets of dependency trees themselves are irregular. This is in contrast to other prominent mildly context-sensitive grammar formalisms such as Tree Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear ContextFree Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)), in which both languages are regular. 4.1 CCG term languages Formally, we define the language of all dependency trees gener</context>
</contexts>
<marker>Miller, 2000</marker>
<rawString>Philip H. Miller. 2000. Strong Generative Capacity: The Semantics of Linguistic Formalism. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="3024" citStr="Nivre et al., 2007" startWordPosition="461" endWordPosition="464">G grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (McDonald et al., 2005; Nivre et al., 2007) and TAG derivation trees. The price we pay for restricting ourselves to trees is that we derive fewer dependencies than the more powerful approach by Clark et al. (2002). Indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled. However, we show that our dependency trees are still informative enough to reconstruct the semantic representations. The paper is structured as follows. In Section 2, we introduce CCG and the fragment PF-CCG that we consider in this paper, and compare our contribution to earlier resear</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>Handbook of Formal Languages.</booktitle>
<editor>Grzegorz Rozenberg and Arto Salomaa, editors.</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2217" citStr="(1997)" startWordPosition="334" endWordPosition="334">between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)). We show that if we only look at valencies and ignore word order, then the dependency trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-in</context>
<context position="18584" citStr="(1997)" startWordPosition="3200" endWordPosition="3200">ndency trees, we can ask what sets of such trees can be generated by PF-CCG grammars. This is the question about the strong generative capacity of PF-CCG, measured in terms of dependency trees (Miller, 2000). In this section, we give a partial answer to this question: We show that the sets of PF-CCG-induced valency trees (dependency trees without their linear order) form regular tree languages, but that the sets of dependency trees themselves are irregular. This is in contrast to other prominent mildly context-sensitive grammar formalisms such as Tree Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear ContextFree Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)), in which both languages are regular. 4.1 CCG term languages Formally, we define the language of all dependency trees generated by a PF-CCG grammar G as the set LD(G) = { d(S) |S is a derivation of G }. Furthermore, we define the set of valency trees to be the set of just the term parts of each d(S): LV (G) = {t |(t, s) ∈ LD(G) }. By our previous assumption, the node labels of a valency tree are CCG lexicon entries. We will now show that the valency tree languages of PF-CCG grammars are regular tree languages (Gécseg</context>
</contexts>
<marker>1997</marker>
<rawString>Grzegorz Rozenberg and Arto Salomaa, editors. 1997. Handbook of Formal Languages. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>Evidence against the contextfreeness of natural language. Linguistics and Philosophy,</title>
<date>1985</date>
<pages>8--333</pages>
<marker>Shieber, 1985</marker>
<rawString>Stuart Shieber. 1985. Evidence against the contextfreeness of natural language. Linguistics and Philosophy, 8:333–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Jason Baldridge</author>
</authors>
<title>Combinatory categorial grammar.</title>
<date>2009</date>
<editor>In R. Borsley and K. Borjars, editors, Non-Transformational Syntax.</editor>
<publisher>Blackwell.</publisher>
<note>To appear.</note>
<marker>Steedman, Baldridge, 2009</marker>
<rawString>Mark Steedman and Jason Baldridge. 2009. Combinatory categorial grammar. In R. Borsley and K. Borjars, editors, Non-Transformational Syntax. Blackwell. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="773" citStr="Steedman (2001)" startWordPosition="107" endWordPosition="109">n Uppsala University Uppsala, Sweden marco.kuhlmann@lingfil.uu.se Abstract We propose a novel algorithm for extracting dependencies from the derivations of a large fragment of CCG. Unlike earlier proposals, our dependency structures are always tree-shaped. We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly </context>
<context position="4571" citStr="Steedman, 2001" startWordPosition="720" endWordPosition="721">9 Association for Computational Linguistics 460 mer np : we&apos; L em Hans np : Hans&apos; L es huus np : house&apos; L hälfed aastriche ((s\np)\np)/vp : help&apos; L vp\np : paint&apos; L F ((s\np)\np)\np : Ax. help&apos;(paint&apos;(x)) B (s\np)\np : help&apos; (paint&apos;(house&apos;)) B s : help&apos; (paint&apos;(house&apos;)) Hans&apos; we&apos; B s\np : help&apos; (paint&apos;(house&apos;)) Hans&apos; Figure 1: A PF-CCG derivation 2 Combinatory Categorial Grammars We start by introducing the Combinatory Categorial Grammar (CCG) formalism. Then we introduce the fragment of CCG that we consider in this paper, and discuss some related work. 2.1 CCG Combinatory Categorial Grammar (Steedman, 2001) is a grammar formalism that assigns categories to substrings of an input sentence. There are atomic categories such as s and np; and if A and B are categories, then A\B and A/B are functional categories representing a constituent that will have category A once it is combined with another constituent of type B to the left or right, respectively. Each word is assigned a category by the lexicon; adjacent substrings can then be combined by combinatory rules. As an example, Steedman and Baldridge’s (2009) analysis of Shieber’s (1985) Swiss German subordinate clause (das) mer em Hans es huus hälfed</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. 2001. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>Polynomial time parsing of combinatory categorial grammars.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th ACL,</booktitle>
<location>Pittsburgh, USA.</location>
<marker>Vijay-Shanker, Weir, 1990</marker>
<rawString>K. Vijay-Shanker and David Weir. 1990. Polynomial time parsing of combinatory categorial grammars. In Proceedings of the 28th ACL, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>The equivalence of four extensions of context-free grammars.</title>
<date>1994</date>
<booktitle>Mathematical Systems Theory,</booktitle>
<pages>27--6</pages>
<contexts>
<context position="1476" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="210" endWordPosition="213">y well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new ligh</context>
<context position="9930" citStr="Vijay-Shanker and Weir (1994)" startWordPosition="1655" endWordPosition="1658"> pronoun has multiple parents; in general, their dependency structures are not necessarily trees. By contrast, we aim to extract only dependency trees, and achieve this by recording only the fillers of syntactic valencies, rather than the semantic dependencies: the relative pronoun gets two dependents and one parent (the verb whose argument the modified np is), just as the category specifies. So Clark et al.’s and our dependency approach represent two alternatives of dealing with the tradeoff between simple and expressive dependency structures. Our paper differs from the well-known results of Vijay-Shanker and Weir (1994) in that they establish the weak equivalence of different grammar formalisms, while we focus on comparing the derivational structures. Hockenmaier and Young (2008) present linguistic motivations for comparing the strong generative capacities of CCG and TAG, and the beginnings of a formal comparison between CCG and spinal TAG in terms of Linear Indexed Grammars. 3 Induction of dependency trees We now explain how to extract a dependency tree from a PF-CCG derivation. The basic idea is to associate, with every step of the derivation, a corresponding operation on dependency trees, in much the same</context>
<context position="23714" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="4119" endWordPosition="4122">nfinite nonterminal alphabet in an RTG, which is of course not allowed. 4.2 Comparison with TAG If we now compare PF-CCG to its most prominent mildly context-sensitive cousin, TAG, the regularity result above paints a suggestive picture: A PF-CCG valency tree assigns a lexicon entry to each word and says which other lexicon entry fills each syntactic valency. In this respect, it is the analogue of a TAG derivation tree (in which the lexicon entries are elementary trees), and we just saw that PF-CCG and TAG generate the same tree languages. On the other hand, CCG and TAG are weakly equivalent (Vijay-Shanker and Weir, 1994), i.e. they generate the same linear word orders. So one could expect that CCG and TAG also induce the same dependency trees. Interestingly, this is not the case. 465 es huus aastriiche hälfed L s\np\np/vp s\np B B L L vp\np vp mer L np em Hans L np np B F s\np\np s Figure 6: CCG derivation reconstructed from the dependency tree from Figure 4 using only applications We know from the literature that those dependency trees that can be constructed from TAG derivation trees are exactly those that are well-nested and have a block-degree of at most 2 (Kuhlmann and Möhl, 2007). The block-degree of a </context>
<context position="27963" citStr="Vijay-Shanker and Weir (1994)" startWordPosition="4884" endWordPosition="4887">he corollary that for any PF-CCG derivation 6 over a string w, there is a permutation of w that can be accepted by a PF-CCG derivation that uses only application—that is, every string language L that can be generated by a PFCCG grammar has a context-free sublanguage L&apos; such that all words in L are permutations of words in L&apos;. This means that many string languages that we commonly associate with CCG cannot be generated 466 b b L b b L c s\a/t\b L B s\a/t F s\a\a\b c t\a\b L a a/d L a a/d L B s/d B s\a/d B s\a\a F s\a d d L d d L F s Figure 7: The CCG derivation of aabbccdd using Example 2.4 in Vijay-Shanker and Weir (1994) by PF-CCG. One such language is anbncndn. This language is not itself context-free, and therefore any PF-CCG grammar whose language contains it also contains permutations in which the order of the symbols is mixed up. The culprit for this among the restrictions that distinguish PF-CCG from full CCG seems to be that PF-CCG grammars must allow all instances of the application rules. This would mean that the ability of CCG to generate noncontext-free languages (also linguistically relevant ones) hinges crucially on its ability to restrict the allowable instances of rule schemata, for instance, u</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1994</marker>
<rawString>K. Vijay-Shanker and David J. Weir. 1994. The equivalence of four extensions of context-free grammars. Mathematical Systems Theory, 27(6):511–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th ACL,</booktitle>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="2294" citStr="Vijay-Shanker et al. (1987)" startWordPosition="341" endWordPosition="344">istic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)). We show that if we only look at valencies and ignore word order, then the dependency trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures ar</context>
<context position="18660" citStr="Vijay-Shanker et al. (1987)" startWordPosition="3208" endWordPosition="3211"> generated by PF-CCG grammars. This is the question about the strong generative capacity of PF-CCG, measured in terms of dependency trees (Miller, 2000). In this section, we give a partial answer to this question: We show that the sets of PF-CCG-induced valency trees (dependency trees without their linear order) form regular tree languages, but that the sets of dependency trees themselves are irregular. This is in contrast to other prominent mildly context-sensitive grammar formalisms such as Tree Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear ContextFree Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)), in which both languages are regular. 4.1 CCG term languages Formally, we define the language of all dependency trees generated by a PF-CCG grammar G as the set LD(G) = { d(S) |S is a derivation of G }. Furthermore, we define the set of valency trees to be the set of just the term parts of each d(S): LV (G) = {t |(t, s) ∈ LD(G) }. By our previous assumption, the node labels of a valency tree are CCG lexicon entries. We will now show that the valency tree languages of PF-CCG grammars are regular tree languages (Gécseg and Steinby, 1997). Regular tree languages are sets of trees that can be ge</context>
<context position="22319" citStr="Vijay-Shanker et al., 1987" startWordPosition="3885" endWordPosition="3888">one by arranging the nodes in t into an order that allows us to combine every parent in t with its children using only forward and backward application. The CCG derivation we obtain for the example is shown in Figure 6; it is a derivation for the sentence das mer em Hans hälfed es huus aastriiche, using the same lexicon entries. Together, this shows that L(F(G)) = LV (G). Thus: Theorem 1 The sets of valency trees generated by PF-CCG are regular tree languages. � By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987). To our knowledge, this is the first time the regular structure of CCG derivations has been exposed. It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al., 1987). Consider for instance the CCG grammar from Vijay-Shanker and Weir’s (1994) Example 2.4, which generates the string language anbncndn; Figure 7 shows the derivation of aabbccdd. If we follow this derivation bottom-up, starting at the first c, the intermediate categories collect an increasingly long tail of\a arguments; for longe</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of the 25th ACL, Stanford, CA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>