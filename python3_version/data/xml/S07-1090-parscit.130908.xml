<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.179464">
<title confidence="0.993866">
UNT-Yahoo: SuperSenseLearner: Combining SenseLearner with
SuperSense and other Coarse Semantic Features
</title>
<author confidence="0.996659">
Rada Mihalcea and Andras Csomai Massimiliano Ciaramita
</author>
<affiliation confidence="0.999507">
University of North Texas Yahoo! Research Barcelona
</affiliation>
<email confidence="0.998965">
rada@cs.unt.edu,csomaia@unt.edu massi@yahoo-inc.com
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999748375">
We describe the SUPERSENSELEARNER
system that participated in the English all-
words disambiguation task. The system re-
lies on automatically-learned semantic mod-
els using collocational features coupled with
features extracted from the annotations of
coarse-grained semantic categories gener-
ated by an HMM tagger.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965264705883">
The task of word sense disambiguation consists of
assigning the most appropriate meaning to a poly-
semous word within a given context. Applications
such as machine translation, knowledge acquisition,
common sense reasoning, and others, require knowl-
edge about word meanings, and word sense disam-
biguation is considered essential for all these tasks.
Most of the efforts in solving this problem
were concentrated so far toward targeted supervised
learning, where each sense tagged occurrence of a
particular word is transformed into a feature vector,
which is then used in an automatic learning process.
The applicability of such supervised algorithms is
however limited only to those few words for which
sense tagged data is available, and their accuracy
is strongly connected to the amount of labeled data
available at hand.
Instead, methods that address all words in unre-
stricted text have received significantly less atten-
tion. While the performance of such methods is usu-
ally exceeded by their supervised lexical-sample al-
ternatives, they have however the advantage of pro-
viding larger coverage.
In this paper, we describe SUPERSENSE-
LEARNER – a system for solving the semantic am-
biguity of all words in unrestricted text. SUPER-
SENSELEARNER brings together under one system
the features previously used in the SENSELEARNER
(Mihalcea and Csomai, 2005) and the SUPERSENSE
(Ciaramita and Altun, 2006) all-words word sense
disambiguation systems. The system is using a rel-
atively small pre-existing sense-annotated data set
for training purposes, and it learns global semantic
models for general word categories.
</bodyText>
<sectionHeader confidence="0.9957425" genericHeader="introduction">
2 Learning for All-Words Word Sense
Disambiguation
</sectionHeader>
<bodyText confidence="0.999994058823529">
Our goal is to use as little annotated data as possi-
ble, and at the same time make the algorithm gen-
eral enough to be able to disambiguate as many
content words as possible in a text, and efficient
enough so that large amounts of text can be anno-
tated in real time. SUPERSENSELEARNER is at-
tempting to learn general semantic models for var-
ious word categories, starting with a relatively small
sense-annotated corpus. We base our experiments
on SemCor (Miller et al., 1993), a balanced, se-
mantically annotated dataset, with all content words
manually tagged by trained lexicographers.
The input to the disambiguation algorithm con-
sists of raw text. The output is a text with word
meaning annotations for all open-class words.
The algorithm starts with a preprocessing stage,
where the text is tokenized and annotated with part-
</bodyText>
<page confidence="0.98734">
406
</page>
<bodyText confidence="0.989811032258064">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 406–409,
Prague, June 2007. c�2007 Association for Computational Linguistics
of-speech tags; collocations are identified using a
sliding window approach, where a collocation is de-
fined as a sequence of words that forms a compound
concept defined in WordNet (Miller, 1995).
Next, a semantic model is learned for all pre-
defined word categories, where a word category is
defined as a group of words that share some com-
mon syntactic or semantic properties. Word cate-
gories can be of various granularities. For instance,
a model can be defined and trained to handle all the
nouns in the test corpus. Similarly, using the same
mechanism, a finer-grained model can be defined to
handle all the verbs for which at least one of the
meanings is of type e.g., “&lt;move&gt;”. Finally, small
coverage models that address one word at a time, for
example a model for the adjective “small,” can be
also defined within the same framework. Once de-
fined and trained, the models are used to annotate the
ambiguous words in the test corpus with their corre-
sponding meaning. Sections 3 and 4 below provide
details on the features implemented by the various
models.
Note that the semantic models are applicable only
to: (1) words that are covered by the word category
defined in the models; and (2) words that appeared
at least once in the training corpus. The words that
are not covered by these models (typically about 10-
15% of the words in the test corpus) are assigned the
most frequent sense in WordNet.
</bodyText>
<sectionHeader confidence="0.986428" genericHeader="method">
3 SenseLearner Semantic Models
</sectionHeader>
<bodyText confidence="0.9997175625">
Different semantic models can be defined and
trained for the disambiguation of different word cat-
egories. Although more general than models that
are built individually for each word in a test corpus
(Decadt et al., 2004), the applicability of the seman-
tic models built as part of SENSELEARNER is still
limited to those words previously seen in the train-
ing corpus, and therefore their overall coverage is
not 100%.
Starting with an annotated corpus consisting of
all the annotated files in SemCor, augmented with
the SENSEVAL-2 and SENSEVAL-3 all-words data
sets, a separate training data set is built for each
model. There are seven models provided with the
current SENSELEARNER distribution, implementing
the following features:
</bodyText>
<subsectionHeader confidence="0.995552">
3.1 Noun Models
</subsectionHeader>
<bodyText confidence="0.999843571428571">
modelNN1: A contextual model that relies on the
first noun, verb, or adjective before the target noun,
and their corresponding part-of-speech tags.
modelNNColl: A collocation model that imple-
ments collocation-like features based on the first
word to the left and the first word to the right of the
target noun.
</bodyText>
<subsectionHeader confidence="0.998095">
3.2 Verb Models
</subsectionHeader>
<bodyText confidence="0.999653428571428">
modelVB1 A contextual model that relies on the
first word before and the first word after the target
verb, and their part-of-speech tags.
modelVBColl A collocation model that implements
collocation-like features based on the first word to
the left and the first word to the right of the target
verb.
</bodyText>
<subsectionHeader confidence="0.977713">
3.3 Adjective Models
</subsectionHeader>
<bodyText confidence="0.993330428571429">
modelJJ1 A contextual model that relies on the first
noun after the target adjective.
modelJJ2 A contextual model that relies on the first
word before and the first word after the target adjec-
tive, and their part-of-speech tags.
modelJJColl A collocation model that implements
collocation-like features using the first word to the
left and the first word to the right of the target adjec-
tive.
Based on previous performance in the
SENSEVAL-2 and SENSEVAL-3 evaluations,
we selected the noun and verb collocational models
for inclusion in the SUPERSENSELEARNER system
participating in the SEMEVAL all-words task.
</bodyText>
<sectionHeader confidence="0.9468765" genericHeader="method">
4 SuperSenses and other Coarse-Grained
Semantic Features
</sectionHeader>
<bodyText confidence="0.999849272727273">
A great deal of work has focused in recent years
on shallow semantic annotation tasks such as named
entity recognition and semantic role labeling. In the
former task, systems analyze text to detect mentions
of instances of coarse-grained semantic categories
such as “person”, “organization” and “location”. It
seems natural to ask if this type of shallow seman-
tic information can be leveraged to improve lexical
disambiguation. Particularly, since the best perform-
ing taggers typically implement sequential decoding
schemes, e.g., Viterbi decoding, which have linear
</bodyText>
<page confidence="0.992921">
407
</page>
<bodyText confidence="0.99985475">
complexity and can be performed quite efficiently.
In practice thus, this type of pre-processing resem-
bles POS-tagging and could provide the WSD sys-
tem with useful additional evidence.
</bodyText>
<subsectionHeader confidence="0.952026">
4.1 Tagsets
</subsectionHeader>
<bodyText confidence="0.999971590909091">
We use three different tagsets. The first is the set of
WordNet supersenses (Ciaramita and Altun, 2006):
a mapping of WordNet’s synsets to 45 broad lexi-
cographers categories, 26 for nouns, 15 for verbs,
3 for adjectives and 1 for adverbs. The second
tagset is based on the ACE 2007 English data for
entity mention detection (EMD) (ACE, 2007). This
tagset defines seven entity types: Facility, Geo-
Political Entity, Location, Organization, Person, Ve-
hicle, Weapon; further subdivided in 44 subtypes.
The third tagset is derived from the BBN Entity
Corpus (BBN, 2005) which complements the Wall
Street Journal Penn Treebank with annotations of a
large set of entities: 12 named entity types (Person,
Facility, Organization, GPE, Location, Nationality,
Product, Event, Work of Art, Law, Language, and
Contact-Info), nine nominal entity types (Person,
Facility, Organization, GPE, Product, Plant, Animal,
Substance, Disease and Game), and seven numeric
types (Date, Time, Percent, Money, Quantity, Ordi-
nal and Cardinal). Several of these types are further
divided into subtypes, for a total of 105 classes.1
</bodyText>
<subsectionHeader confidence="0.962016">
4.2 Taggers
</subsectionHeader>
<bodyText confidence="0.99982375">
We annotate the training and evaluation data using
three sequential taggers, one for each tagset. The
tagger is a Hidden Markov Model trained with the
perceptron algorithm introduced in (Collins, 2002),
which applies Viterbi decoding and is regularized
using averaging. Label to label dependencies are
limited to the previous tag (first order HMM). We
use a generic feature set for NER based on words,
lemmas, POS tags, and word shape features, in addi-
tion we use as a feature of each token the supersense
of a first (super)sense baseline. A detailed descrip-
tion of the features used and the tagger can be found
in (Ciaramita and Altun, 2006). The supersense tag-
ger is trained on the Brown sections one and two of
SemCor. The BBN tagger is trained on sections 2-
21 of the BBN corpus. The ACE tagger is trained
</bodyText>
<footnote confidence="0.502539">
1BBN Corpus documentation.
</footnote>
<bodyText confidence="0.99986225">
on the 599 ACE 2007 training files. The accuracy
of the tagger is, approximately, 78% F-score for su-
persenses and ACE, and 87% F-score for the BBN
corpus.
</bodyText>
<subsectionHeader confidence="0.965125">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.9999515">
The taggers disregard the lemmatization of the eval-
uation data. In practice, this means that multiword
lemmas such as “take off”, are split into their ba-
sic components. In fact, the goal of the tagger is
to guess the elements of the instances of semantic
categories by means of the usual BIO encoding. In
other words, the tagger predicts a labeled bracket-
ing of the tokens in each sentence. As an exam-
ple, the supersense tagger annotates the tokens in the
phrase “substance abuse” as “substanceB−noun.act”
and “abuser−noun.act”, although the gold standard
segmentation of the data does not identify the phrase
as one lemma. We use the labels generated in this
way as features of each token to disambiguate.
</bodyText>
<sectionHeader confidence="0.988997" genericHeader="method">
5 Feature Combination
</sectionHeader>
<bodyText confidence="0.99987388">
For the final system we create a combined feature set
for each target word, consisting of the lemma, the
part of speech, the collocational SENSELEARNER
features, and the three coarse grained semantic tags
of the target word. Note that the semantic fea-
tures are represented as lemma TAG to avoid over-
generalization.
In the training stage, a feature vector is con-
structed for each sense-annotated word covered by
a semantic model. The features are model-specific,
and feature vectors are added to the training set
pertaining to the corresponding model. The label
of each such feature vector consists of the target
word and the corresponding sense, represented as
word#sense. Table 1 shows the number of feature
vectors constructed in this learning stage for each
semantic model. To annotate new text, similar vec-
tors are created for all the content-words in the raw
text. Similar to the training stage, feature vectors
are created and stored separately for each semantic
model.
Next, word sense predictions are made for all the
test examples, with a separate learning process run
for each semantic model. For learning, we are using
the Timbl memory based learning algorithm (Daele-
</bodyText>
<page confidence="0.995654">
408
</page>
<table confidence="0.999348833333333">
mode Training RESULTS
size
Precision Recall
noun 89052 0.658 0.228
verb 48936 0.539 0.353
all 137988 0.583 0.583
</table>
<tableCaption confidence="0.912101">
Table 1: Precision and recall for the SUPERSENSE-
LEARNER semantic models.
</tableCaption>
<table confidence="0.999582">
mode Training RESULTS
size
Precision Recall
noun 89052 0.666 0.233
verb 48936 0.554 0.360
all 137988 0.593 0.593
</table>
<tableCaption confidence="0.9793475">
Table 2: Precision and recall for the SUPERSENSE-
LEARNER semantic models - without U labels.
</tableCaption>
<bodyText confidence="0.992094230769231">
mans et al., 2001), which was previously found use-
ful for the task of word sense disambiguation (Hoste
et al., 2002; Mihalcea, 2002).
Following the learning stage, each vector in the
test data set is labeled with a predicted word and
sense. If the word predicted by the learning algo-
rithm coincides with the target word in the test fea-
ture vector, then the predicted sense is used to an-
notate the test instance. Otherwise, if the predicted
word is different from the target word, no annota-
tion is produced, and the word is left for annotation
in a later stage (e.g., using the most frequent sense
back-off method).
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999979222222222">
The SUPERSENSELEARNER system participated in
the SEMEVAL all-words word sense disambigua-
tion task. Table 1 shows the results obtained for
each part-of-speech (nouns and verbs), as well as
the overall results. We have also ran a separate
evaluation excluding the U (unknown) tag, which
is shown in Table 2. SUPERSENSELEARNER was
ranked the third among the fourteen participating
systems, proving the validity of the approach.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993725">
We would like to thank Mihai Surdeanu for provid-
ing a pre-processed version of the ACE data.
</bodyText>
<sectionHeader confidence="0.989462" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999660170212766">
2007. Automatic content extraction workshop.
http://www.nist.gov/speech/tests/ace/ace07/index.htm.
2005. BBN pronoun coreference and entity type cor-
pus. Linguistic Data Consortium (LDC) catalog num-
ber LDC2005T33.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Philadelphia, July. Association for
Computational Linguistics.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner,
version 4.0, reference guide. Technical report, Univer-
sity of Antwerp.
B. Decadt, V. Hoste, W. Daelemans, and A. Van den
Bosch. 2004. Gambl, genetic algorithm optimization
of memory-based wsd. In Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, Barcelona, Spain, July.
V. Hoste, W. Daelemans, I. Hendrickx, and A. van den
Bosch. 2002. Evaluating the results of a memory-
based word-expert approach to unrestricted word sense
disambiguation. In Proceedings of the ACL Workshop
on ”Word Sense Disambiguatuion: Recent Successes
and Future Directions”, Philadelphia, July.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
R. Mihalcea. 2002. Instance based learning with auto-
matic feature selection applied to Word Sense Disam-
biguation. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), Taipei, Taiwan, August.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Commu-
nication of the ACM, 38(11):39–41.
</reference>
<page confidence="0.999107">
409
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.799530">
<title confidence="0.9961125">UNT-Yahoo: SuperSenseLearner: Combining SenseLearner with SuperSense and other Coarse Semantic Features</title>
<author confidence="0.999303">Mihalcea Csomai Massimiliano Ciaramita</author>
<affiliation confidence="0.999891">University of North Texas Yahoo! Research Barcelona</affiliation>
<email confidence="0.999473">rada@cs.unt.edu,csomaia@unt.edumassi@yahoo-inc.com</email>
<abstract confidence="0.978412">describe the system that participated in the English allwords disambiguation task. The system relies on automatically-learned semantic models using collocational features coupled with features extracted from the annotations of coarse-grained semantic categories generated by an HMM tagger.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Automatic content extraction workshop.</title>
<date>2007</date>
<note>http://www.nist.gov/speech/tests/ace/ace07/index.htm.</note>
<marker>2007</marker>
<rawString>2007. Automatic content extraction workshop. http://www.nist.gov/speech/tests/ace/ace07/index.htm.</rawString>
</citation>
<citation valid="true">
<title>BBN pronoun coreference and entity type corpus. Linguistic Data Consortium (LDC) catalog number LDC2005T33.</title>
<date>2005</date>
<marker>2005</marker>
<rawString>2005. BBN pronoun coreference and entity type corpus. Linguistic Data Consortium (LDC) catalog number LDC2005T33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1997" citStr="Ciaramita and Altun, 2006" startWordPosition="286" endWordPosition="289">ed to the amount of labeled data available at hand. Instead, methods that address all words in unrestricted text have received significantly less attention. While the performance of such methods is usually exceeded by their supervised lexical-sample alternatives, they have however the advantage of providing larger coverage. In this paper, we describe SUPERSENSELEARNER – a system for solving the semantic ambiguity of all words in unrestricted text. SUPERSENSELEARNER brings together under one system the features previously used in the SENSELEARNER (Mihalcea and Csomai, 2005) and the SUPERSENSE (Ciaramita and Altun, 2006) all-words word sense disambiguation systems. The system is using a relatively small pre-existing sense-annotated data set for training purposes, and it learns global semantic models for general word categories. 2 Learning for All-Words Word Sense Disambiguation Our goal is to use as little annotated data as possible, and at the same time make the algorithm general enough to be able to disambiguate as many content words as possible in a text, and efficient enough so that large amounts of text can be annotated in real time. SUPERSENSELEARNER is attempting to learn general semantic models for va</context>
<context position="7629" citStr="Ciaramita and Altun, 2006" startWordPosition="1190" endWordPosition="1193">c categories such as “person”, “organization” and “location”. It seems natural to ask if this type of shallow semantic information can be leveraged to improve lexical disambiguation. Particularly, since the best performing taggers typically implement sequential decoding schemes, e.g., Viterbi decoding, which have linear 407 complexity and can be performed quite efficiently. In practice thus, this type of pre-processing resembles POS-tagging and could provide the WSD system with useful additional evidence. 4.1 Tagsets We use three different tagsets. The first is the set of WordNet supersenses (Ciaramita and Altun, 2006): a mapping of WordNet’s synsets to 45 broad lexicographers categories, 26 for nouns, 15 for verbs, 3 for adjectives and 1 for adverbs. The second tagset is based on the ACE 2007 English data for entity mention detection (EMD) (ACE, 2007). This tagset defines seven entity types: Facility, GeoPolitical Entity, Location, Organization, Person, Vehicle, Weapon; further subdivided in 44 subtypes. The third tagset is derived from the BBN Entity Corpus (BBN, 2005) which complements the Wall Street Journal Penn Treebank with annotations of a large set of entities: 12 named entity types (Person, Facili</context>
<context position="9283" citStr="Ciaramita and Altun, 2006" startWordPosition="1453" endWordPosition="1456">training and evaluation data using three sequential taggers, one for each tagset. The tagger is a Hidden Markov Model trained with the perceptron algorithm introduced in (Collins, 2002), which applies Viterbi decoding and is regularized using averaging. Label to label dependencies are limited to the previous tag (first order HMM). We use a generic feature set for NER based on words, lemmas, POS tags, and word shape features, in addition we use as a feature of each token the supersense of a first (super)sense baseline. A detailed description of the features used and the tagger can be found in (Ciaramita and Altun, 2006). The supersense tagger is trained on the Brown sections one and two of SemCor. The BBN tagger is trained on sections 2- 21 of the BBN corpus. The ACE tagger is trained 1BBN Corpus documentation. on the 599 ACE 2007 training files. The accuracy of the tagger is, approximately, 78% F-score for supersenses and ACE, and 87% F-score for the BBN corpus. 4.3 Features The taggers disregard the lemmatization of the evaluation data. In practice, this means that multiword lemmas such as “take off”, are split into their basic components. In fact, the goal of the tagger is to guess the elements of the ins</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>M. Ciaramita and Y. Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia,</location>
<contexts>
<context position="8842" citStr="Collins, 2002" startWordPosition="1378" endWordPosition="1379">ity, Organization, GPE, Location, Nationality, Product, Event, Work of Art, Law, Language, and Contact-Info), nine nominal entity types (Person, Facility, Organization, GPE, Product, Plant, Animal, Substance, Disease and Game), and seven numeric types (Date, Time, Percent, Money, Quantity, Ordinal and Cardinal). Several of these types are further divided into subtypes, for a total of 105 classes.1 4.2 Taggers We annotate the training and evaluation data using three sequential taggers, one for each tagset. The tagger is a Hidden Markov Model trained with the perceptron algorithm introduced in (Collins, 2002), which applies Viterbi decoding and is regularized using averaging. Label to label dependencies are limited to the previous tag (first order HMM). We use a generic feature set for NER based on words, lemmas, POS tags, and word shape features, in addition we use as a feature of each token the supersense of a first (super)sense baseline. A detailed description of the features used and the tagger can be found in (Ciaramita and Altun, 2006). The supersense tagger is trained on the Brown sections one and two of SemCor. The BBN tagger is trained on sections 2- 21 of the BBN corpus. The ACE tagger i</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 4.0, reference guide.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>University of Antwerp.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2001</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2001. Timbl: Tilburg memory based learner, version 4.0, reference guide. Technical report, University of Antwerp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Decadt</author>
<author>V Hoste</author>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
</authors>
<title>Gambl, genetic algorithm optimization of memory-based wsd.</title>
<date>2004</date>
<booktitle>In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<location>Barcelona, Spain,</location>
<marker>Decadt, Hoste, Daelemans, Van den Bosch, 2004</marker>
<rawString>B. Decadt, V. Hoste, W. Daelemans, and A. Van den Bosch. 2004. Gambl, genetic algorithm optimization of memory-based wsd. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hoste</author>
<author>W Daelemans</author>
<author>I Hendrickx</author>
<author>A van den Bosch</author>
</authors>
<title>Evaluating the results of a memorybased word-expert approach to unrestricted word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on ”Word Sense Disambiguatuion: Recent Successes and Future Directions”,</booktitle>
<location>Philadelphia,</location>
<marker>Hoste, Daelemans, Hendrickx, van den Bosch, 2002</marker>
<rawString>V. Hoste, W. Daelemans, I. Hendrickx, and A. van den Bosch. 2002. Evaluating the results of a memorybased word-expert approach to unrestricted word sense disambiguation. In Proceedings of the ACL Workshop on ”Word Sense Disambiguatuion: Recent Successes and Future Directions”, Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>A Csomai</author>
</authors>
<title>Senselearner: Word sense disambiguation for all words in unrestricted text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1950" citStr="Mihalcea and Csomai, 2005" startWordPosition="279" endWordPosition="282">ailable, and their accuracy is strongly connected to the amount of labeled data available at hand. Instead, methods that address all words in unrestricted text have received significantly less attention. While the performance of such methods is usually exceeded by their supervised lexical-sample alternatives, they have however the advantage of providing larger coverage. In this paper, we describe SUPERSENSELEARNER – a system for solving the semantic ambiguity of all words in unrestricted text. SUPERSENSELEARNER brings together under one system the features previously used in the SENSELEARNER (Mihalcea and Csomai, 2005) and the SUPERSENSE (Ciaramita and Altun, 2006) all-words word sense disambiguation systems. The system is using a relatively small pre-existing sense-annotated data set for training purposes, and it learns global semantic models for general word categories. 2 Learning for All-Words Word Sense Disambiguation Our goal is to use as little annotated data as possible, and at the same time make the algorithm general enough to be able to disambiguate as many content words as possible in a text, and efficient enough so that large amounts of text can be annotated in real time. SUPERSENSELEARNER is att</context>
</contexts>
<marker>Mihalcea, Csomai, 2005</marker>
<rawString>R. Mihalcea and A. Csomai. 2005. Senselearner: Word sense disambiguation for all words in unrestricted text. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002),</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="12081" citStr="Mihalcea, 2002" startWordPosition="1920" endWordPosition="1921">antic model. For learning, we are using the Timbl memory based learning algorithm (Daele408 mode Training RESULTS size Precision Recall noun 89052 0.658 0.228 verb 48936 0.539 0.353 all 137988 0.583 0.583 Table 1: Precision and recall for the SUPERSENSELEARNER semantic models. mode Training RESULTS size Precision Recall noun 89052 0.666 0.233 verb 48936 0.554 0.360 all 137988 0.593 0.593 Table 2: Precision and recall for the SUPERSENSELEARNER semantic models - without U labels. mans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002; Mihalcea, 2002). Following the learning stage, each vector in the test data set is labeled with a predicted word and sense. If the word predicted by the learning algorithm coincides with the target word in the test feature vector, then the predicted sense is used to annotate the test instance. Otherwise, if the predicted word is different from the target word, no annotation is produced, and the word is left for annotation in a later stage (e.g., using the most frequent sense back-off method). 6 Results The SUPERSENSELEARNER system participated in the SEMEVAL all-words word sense disambiguation task. Table 1 </context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>R. Mihalcea. 2002. Instance based learning with automatic feature selection applied to Word Sense Disambiguation. In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002), Taipei, Taiwan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>C Leacock</author>
<author>T Randee</author>
<author>R Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd DARPA Workshop on Human Language Technology,</booktitle>
<location>Plainsboro, New Jersey.</location>
<contexts>
<context position="2732" citStr="Miller et al., 1993" startWordPosition="407" endWordPosition="410">ata set for training purposes, and it learns global semantic models for general word categories. 2 Learning for All-Words Word Sense Disambiguation Our goal is to use as little annotated data as possible, and at the same time make the algorithm general enough to be able to disambiguate as many content words as possible in a text, and efficient enough so that large amounts of text can be annotated in real time. SUPERSENSELEARNER is attempting to learn general semantic models for various word categories, starting with a relatively small sense-annotated corpus. We base our experiments on SemCor (Miller et al., 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers. The input to the disambiguation algorithm consists of raw text. The output is a text with word meaning annotations for all open-class words. The algorithm starts with a preprocessing stage, where the text is tokenized and annotated with part406 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 406–409, Prague, June 2007. c�2007 Association for Computational Linguistics of-speech tags; collocations are identified using a sliding window approac</context>
</contexts>
<marker>Miller, Leacock, Randee, Bunker, 1993</marker>
<rawString>G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993. A semantic concordance. In Proceedings of the 3rd DARPA Workshop on Human Language Technology, Plainsboro, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: A lexical database.</title>
<date>1995</date>
<journal>Communication of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3452" citStr="Miller, 1995" startWordPosition="516" endWordPosition="517">ers. The input to the disambiguation algorithm consists of raw text. The output is a text with word meaning annotations for all open-class words. The algorithm starts with a preprocessing stage, where the text is tokenized and annotated with part406 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 406–409, Prague, June 2007. c�2007 Association for Computational Linguistics of-speech tags; collocations are identified using a sliding window approach, where a collocation is defined as a sequence of words that forms a compound concept defined in WordNet (Miller, 1995). Next, a semantic model is learned for all predefined word categories, where a word category is defined as a group of words that share some common syntactic or semantic properties. Word categories can be of various granularities. For instance, a model can be defined and trained to handle all the nouns in the test corpus. Similarly, using the same mechanism, a finer-grained model can be defined to handle all the verbs for which at least one of the meanings is of type e.g., “&lt;move&gt;”. Finally, small coverage models that address one word at a time, for example a model for the adjective “small,” c</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. Miller. 1995. Wordnet: A lexical database. Communication of the ACM, 38(11):39–41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>