<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000094">
<title confidence="0.9976995">
You’ve Got Answers: Towards Personalized Models for Predicting Success
in Community Question Answering
</title>
<author confidence="0.996225">
Yandong Liu and Eugene Agichtein
</author>
<affiliation confidence="0.988265">
Emory University
</affiliation>
<email confidence="0.993452">
lyliu49,eugenel@mathcs.emory.edu
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998812666666667">
Question answering communities such as Ya-
hoo! Answers have emerged as a popular al-
ternative to general-purpose web search. By
directly interacting with other participants, in-
formation seekers can obtain specific answers
to their questions. However, user success in
obtaining satisfactory answers varies greatly.
We hypothesize that satisfaction with the con-
tributed answers is largely determined by the
asker’s prior experience, expectations, and
personal preferences. Hence, we begin to de-
velop personalized models of asker satisfac-
tion to predict whether a particular question
author will be satisfied with the answers con-
tributed by the community participants. We
formalize this problem, and explore a variety
of content, structure, and interaction features
for this task using standard machine learning
techniques. Our experimental evaluation over
thousands of real questions indicates that in-
deed it is beneficial to personalize satisfaction
predictions when sufficient prior user history
exists, significantly improving accuracy over
a “one-size-fits-all” prediction model.
</bodyText>
<sectionHeader confidence="0.999106" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9970236">
Community Question Answering (CQA) has re-
cently become a viable method for seeking infor-
mation online. As an alternative to using general-
purpose web search engines, information seekers
now have an option to post their questions (often
complex, specific, and subjective) on Community
QA sites such as Yahoo! Answers, and have their
questions answered by other users. Hundreds of mil-
lions of answers have already been posted for tens of
millions of questions in Yahoo! Answers. However,
the success of obtaining satisfactory answers in the
available CQA portals varies greatly. In many cases,
the questions posted by askers go un-answered, or
are answered poorly, never obtaining a satisfactory
answer.
</bodyText>
<page confidence="0.993609">
97
</page>
<bodyText confidence="0.999972125">
In our recent work (Liu et al., 2008) we have in-
troduced a general model for predicting asker sat-
isfaction in community question answering. We
found that previous asker history is a significant fac-
tor that correlates with satisfaction. We hypothesize
that asker’s satisfaction with contributed answers is
largely determined by the asker expectations, prior
knowledge and previous experience with using the
CQA site. Therefore, in this paper we begin to ex-
plore how to personalize satisfaction prediction -
that is, to attempt to predict whether a specific in-
formation seeker will be satisfied with any of the
contributed answers. Our aim is to provide a “per-
sonalized” recommendation to the user that they’ve
got answers that satisfy their information need.
To the best of our knowledge, ours is the first ex-
ploration of personalizing prediction of user satis-
faction in complex and subjective information seek-
ing environments. While information seeker sat-
isfaction has been studied in ad-hoc IR context
(see (Kobayashi and Takeda, 2000) for an overview),
previous studies have been limited by the lack of re-
alistic user feedback. In contrast, we deal with com-
plex information needs and community-provided
answers, trying to predict subjective ratings pro-
vided by users themselves. Furthermore, while au-
tomatic complex QA has been an active area of re-
search, ranging from simple modification to factoid
QA technique (e.g., (Soricut and Brill, 2004)) to
knowledge intensive approaches for specialized do-
mains, the technology does not yet exist to automat-
ically answer open domain, complex, and subjective
questions. Hence, this paper contributes to both the
understanding of complex question answering, and
explores evaluation issues in a new setting.
The rest of the paper is organized as follows. We
describe the problem and our approach in Section
2, including our initial attempt at personalizing sat-
isfaction prediction. We report results of a large-
scale evaluation over thousands of real users and
</bodyText>
<subsubsectionHeader confidence="0.354779">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 97–100,
</subsubsectionHeader>
<page confidence="0.484746">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999288666666667">
tens of thousands of questions in Section 3. Our
results demonstrate that when sufficient prior asker
history exists, even simple personalized models re-
sult in significant improvement over a general pre-
diction model. We discuss our findings and future
work in Section 4.
</bodyText>
<sectionHeader confidence="0.559219" genericHeader="method">
2 Predicting Asker Satisfaction in CQA
</sectionHeader>
<bodyText confidence="0.991433536585366">
We first briefly review the life of a question in a
QA community. A user (the asker) posts a question
by selecting a topical category (e.g., “History”), and
then enters the question and, optionally, additional
details. After a short delay the question appears in
the respective category list of open questions. At
this point, other users can answer the question, vote
on other users’ answers, or interact in other ways.
The asker may be notified of the answers as they are
submitted, or may check the contributed answers pe-
riodically. If the asker is satisfied with any of the
answers, she can choose it as best, and rate the an-
swer by assigning stars. At that point, the question
is considered as closed by asker. For more detailed
treatment of user interactions in CQA see (Liu et
al., 2008). If the asker rates the best answer with
at least three out of five “stars”, we believe the asker
is satisfied with the response. But often the asker
never closes the answer personally, and instead, af-
ter a period of time, the question is closed automat-
ically. In this case, the “best” answer may be cho-
sen by the votes, or alternatively by automatically
predicting answer quality (e.g., (Jeon et al., 2006)
or (Agichtein et al., 2008)). While the best answer
chosen automatically may be of high quality, it is un-
known if the asker’s information need was satisfied.
Based on our exploration we believe that the main
reasons for not “closing” a question are a) the asker
loses interest in the information and b) none of the
answers are satisfactory. In both cases, the QA com-
munity has failed to provide satisfactory answers in
a timely manner and “lost” the asker’s interest. We
consider this outcome to be “unsatisfied”. We now
define asker satisfaction more precisely:
Definition 1 An asker in a QA community is consid-
ered satisfied iff: the asker personally has closed the
question and rated the best answer with at least 3
“stars”. Otherwise, the asker is unsatisfied.
This definition captures a key aspect of asker satis-
faction, namely that we can reliably identify when
the asker is satisfied but not the converse.
</bodyText>
<subsectionHeader confidence="0.978567">
2.1 Asker Satisfaction Prediction Framework
</subsectionHeader>
<bodyText confidence="0.999847076923077">
We now briefly review our ASP (Asker Satisfac-
tion Prediction) framework that learns to classify
whether a question has been satisfactorily answered,
originally introduced in (Liu et al., 2008). ASP em-
ploys standard classification techniques to predict,
given a question thread, whether an asker would be
satisfied. A sample of features used to represent this
problem is listed in Table 1. Our features are or-
ganized around the basic entities in a question an-
swering community: questions, answers, question-
answer pairs, users, and categories. In total, we de-
veloped 51 features for this task. A sample of the
features used are listed in the Figure 1.
</bodyText>
<listItem confidence="0.963701933333333">
• Question Features: Traditional question answer-
ing features such as the wh-type of the question
(e.g., “what” or “where”), and whether the ques-
tion is similar to other questions in the category.
• Question-Answer Relationship Features: Over-
lap between question and answer, answer length,
and number of candidate answers. We also use
features such as the number of positive votes
(“thumbs up” in Yahoo! Answers), negative votes
(“thumbs down”), and derived statistics such as
the maximum of positive or negative votes re-
ceived for any answer (e.g., to detect cases of bril-
liant answers or, conversely, blatant abuse).
• Asker User History: Past asker activity history
such as the most recent rating, average past satis-
faction, and number of previous questions posted.
Note that only the information available about the
asker prior to posting the question was used.
• Category Features: We hypothesized that user
behavior (and asker satisfaction) varies by topi-
cal question category, as recently shown in refer-
ence (Agichtein et al., 2008). Therefore we model
the prior of asker satisfaction for the category,
such as the average asker rating (satisfaction).
• Text Features: We also include word unigrams and
bigrams to represent the text of the question sub-
ject, question detail, and the answer content. Sep-
arate feature spaces were used for each attribute to
keep answer text distinct from question text, with
frequency-based filtering.
</listItem>
<bodyText confidence="0.4601718">
Classification Algorithms: We experimented with
a variety of classifiers in the Weka framework (Wit-
ten and Frank, 2005). In particular, we com-
pared Support Vector Machines, Decision trees, and
Boosting-based classifiers. SVM performed the best
</bodyText>
<page confidence="0.975226">
98
</page>
<table confidence="0.99986975">
Feature Description
Question Features
Q: Q punctuation density Ratio of punctuation to words in the question
Q: Q KL div wikipedia KL divergence with Wikipedia corpus
Q: Q KL div category KL divergence with “satisfied” questions in category
Q: Q KL div trec KL divergence with TREC questions corpus
Question-Answer Relationship Features
QA: QA sum pos vote Sum of positive votes for all the answers
QA: QA sum neg vote Sum of negative votes for all the answers
QA: QA KL div wikipedia KL Divergence of all answers with Wikipedia corpus
Asker User History Features
UH: UH questions resolved Number of questions resolved in the past
UH: UH num answers Number of all answers this user has received in the past
UH: UH more recent rating Rating for the last question before current question
UH: UH avg past rating Average rating given when closing questions in the past
Category Features
CA: CA avg time to close Average interval between opening and closing
CA: CA avg num answers Average number of answers for that category
CA: CA avg asker rating Average rating given by asker for category
CA: CA avg num votes Average number of “best answer” votes in category
</table>
<tableCaption confidence="0.994632666666667">
Table 1: Sample features: Question (Q), Question-
Answer Relationship (QA), Asker history (UH), and Cat-
egory (CA).
</tableCaption>
<bodyText confidence="0.985175">
of the three during development, so we report results
using SVM for all the subsequent experiments.
</bodyText>
<subsectionHeader confidence="0.997751">
2.2 Personalizing Asker Satisfaction Prediction
</subsectionHeader>
<bodyText confidence="0.9984745">
We now describe our initial attempt at personalizing
the ASP framework described above to each asker:
</bodyText>
<listItem confidence="0.961563">
• ASP Pers+Text: We first consider the naive per-
</listItem>
<bodyText confidence="0.93826647368421">
sonalization approach where we train a separate
classifier for each user. That is, to predict a par-
ticular asker’s satisfaction with the provided an-
swers, we apply the individual classifier trained
solely on the questions (and satisfaction labels)
provided in the past by that user.
• ASP Group: A more robust approach is to train a
classifier on the questions from the group of users
similar to each other. Our current grouping was
done simply by the number of questions posted,
essentially grouping users with similar levels of
“activity”. As we will show below, text features
only help for users with at least 20 previous ques-
tions. So, we only include text features for groups
of users with at least 20 questions.
Certainly, more sophisticated personalization mod-
els and user clustering methods could be devised.
However, as we show next, even the simple models
described above prove surprisingly effective.
</bodyText>
<sectionHeader confidence="0.998615" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.997535142857143">
We want to predict, for a given user and their current
question whether the user will be satisfied, accord-
ing to our definition in Section 2. In other words, our
“truth” labels are based on the rating subsequently
given to the best answer by the asker herself. It is
usually more valuable to correctly predict whether
a user is satisfied (e.g., to notify a user of success).
</bodyText>
<table confidence="0.999688454545455">
#QuestionsperAsker #Questions #Answers # Users
1 132,279 1,197,089 132,279
2 31,692 287,681 15,846
3-4 23,296 213,507 7,048
5-9 15,811 143,483 2,568
10-14 5,554 54,781 481
15-19 2,304 21,835 137
20-29 2,226 23,729 93
30-49 1,866 16,982 49
50-100 842 4,528 14
Total: 216,170 1,963,615 158,515
</table>
<tableCaption confidence="0.997831">
Table 2: Distribution of questions, answers and askers
</tableCaption>
<bodyText confidence="0.997883944444445">
Hence, we focus on the Precision, Recall, and F1
values for the satisfied class.
Datasets: Our data was based on a snapshot of Ya-
hoo! Answers crawled in early 2008, containing
216,170 questions posted in 100 topical categories
by 158,515 askers, with associated 1,963,615 an-
swers in total. More detailed statistics, arranged by
the number of questions posted by each asker are
reported in (Table 2). The askers with only one
question (i.e., no prior history) dominate the dataset,
as many users try the service once and never come
back. However, for personalized satisfaction, at least
some prior history is needed. Therefore, in this early
version of our work, we focus on users who have
posted at least 2 questions - i.e., have the minimal
history of at least one prior question. In the future,
we plan to address the “cold start” problem of pre-
dicting satisfaction of new users.
</bodyText>
<sectionHeader confidence="0.478513" genericHeader="method">
Methods compared:
</sectionHeader>
<listItem confidence="0.9968395">
• ASP: A “one-size-fits-all” satisfaction predictor
that is trained on 10,000 randomly sampled ques-
tions with only non-textual features (Section 2.1).
• ASP+Text: The ASP classifier with text features.
• ASP Pers+Text and ASP Group: A personal-
ized classifiers described in Section 2.2.
</listItem>
<subsectionHeader confidence="0.999267">
3.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999354066666667">
Figure 1 reports the satisfaction prediction accu-
racy for ASP, ASP Text, ASP Pers+Text, and
ASP Group for groups of askers with varying num-
ber of previous questions posted. Surprisingly,
for ASP Text, textual features only become help-
ful for users with more than 20 or 30 previous
questions posted and degrade performance other-
wise. Also note that baseline ASP classifier is
not able to achieve higher accuracy even for users
with large amount of past history. In contrast,
the ASP Pers+Text classifier, trained only on the
past question(s) of each user, achieves surprisingly
good accuracy – often significantly outperforming
the ASP and ASP Text classifiers. The improve-
ment is especially dramatic for users with at least
</bodyText>
<page confidence="0.99765">
99
</page>
<figureCaption confidence="0.7062355">
Figure 1: Precision, Recall, and F1 of ASP, ASP Text, ASP Pers+Text, and ASP Group for predicting satisfaction of
askers with varying number of questions
</figureCaption>
<bodyText confidence="0.999767275862069">
20 previous questions. Interestingly, the simple
strategy of grouping users by number of previous
questions (ASP Group) is even more effective, re-
sulting in accuracy higher than both other meth-
ods for users with moderate amount of history. Fi-
nally, for users with only 2 questions total (that is,
only 1 previous question posted) the performance
of ASP Pers+Text is surprisingly high. We found
that the classifier simply “memorizes” the outcome
of the only available previous question, and uses it
to predict the rating of the current question.
To better understand the improvement of person-
alized models, we report the most significant fea-
tures, sorted by Information Gain (IG), for three
sample ASP Pers+Text models (Table 3). Interest-
ingly, whereas for Pers 1 and Pers 2, textual features
such as “good luck” in the answer are significant, for
Pers 3 non-textual features are most significant.
We also report the top 10 features with the high-
est information gain for the ASP and ASP Group
models (Table 4). Interestingly, while asker’s aver-
age previous rating is the top feature for ASP, the
length of membership of the asker is the most impor-
tant feature for ASP Group, perhaps allowing the
classifier to distinguish more expert users from the
active newbies. In summary, we have demonstrated
promising preliminary results on personalizing sat-
isfaction prediction even with relatively simple per-
sonalization models.
</bodyText>
<tableCaption confidence="0.949083">
Table 3: Top 10 features by Information Gain for three
sample ASP Pers+Text models
</tableCaption>
<table confidence="0.993770909090909">
IG ASP IG ASP Group
0.104117 Q prev avg rating 0.30981 UH membersince in days
0.102117 Q most recent rating 0.25541 Q prev avg rating
0.047222 Q avg pos vote 0.22556 Q most recent rating
0.041773 Q sum pos vote 0.15237 CA avg num votes
0.041076 Q max pos vote 0.14466 CA avg time close
0.03535 A ques timediff in minutes 0.13489 CA avg asker rating
0.032261 UH membersince in days 0.13175 CA num ans per hour
0.031812 CA avg asker rating 0.12437 CA num ques per hour
0.03001 CA ratio ans ques 0.09314 Q avg pos vote
0.029858 CA num ans per hour 0.08572 CA ratio ans ques
</table>
<tableCaption confidence="0.99125">
Table 4: Top 10 features by information gain for ASP
(trained for all askers) and ASP Group (trained for the
group of askers with 20 to 29 questions)
</tableCaption>
<sectionHeader confidence="0.999012" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999959666666667">
We have presented preliminary results on personal-
izing satisfaction prediction, demonstrating signif-
icant accuracy improvements over a “one-size-fits-
all” satisfaction prediction model. In the future we
plan to explore the personalization more deeply fol-
lowing the rich work in recommender systems and
collaborative filtering, with the key difference that
the asker satisfaction, and each question, are unique
(instead of shared items such as movies). In sum-
mary, our work opens a promising direction towards
modeling personalized user intent, expectations, and
satisfaction.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801647058824">
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. 2008. Finding high-quality content in
social media with an application to community-based
question answering. In Proceedings of WSDM.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with non-
textual features. In Proceedings of SIGIR.
Mei Kobayashi and Koichi Takeda. 2000. Information
retrieval on the web. ACM Computing Surveys, 32(2).
Y. Liu, J. Bian, and E. Agichtein. 2008. Predicting in-
formation seeker satisfaction in community question
answering. In Proceedings of SIGIR.
R. Soricut and E. Brill. 2004. Automatic question an-
swering: Beyond the factoid. In HLT-NAACL.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
man, 2nd edition.
</reference>
<figure confidence="0.419998777777778">
Pers 1 (97 questions) Pers 2 (49 questions) Pers 3 (25 questions)
UH total answers received
UH questions resolved
”good luck” in answer
”is an” in answer
”want to” in answer
”we” in answer
”want in” answer
”adenocarcinoma” in question
</figure>
<figureCaption confidence="0.736842583333333">
”was” in question
”live” in answer
Q avg pos votes
”would” in answer
”answer” in question
”just” in answer
”me” in answer
”be” in answer
”in the” in question
CA History
”who is” in question
”those” in answer
</figureCaption>
<figure confidence="0.7977814">
Q content kl trec
Q content kl wikipedia
UH total answers received
UH questions resolved
Q content kl asker all cate
Q prev avg rating
CA avg asker rating
“anybody” in question
Q content typo density
Q detail len
</figure>
<page confidence="0.713787">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943073">
<title confidence="0.998723">You’ve Got Answers: Towards Personalized Models for Predicting Success in Community Question Answering</title>
<author confidence="0.959681">Liu Agichtein</author>
<affiliation confidence="0.996615">Emory University</affiliation>
<email confidence="0.995963">lyliu49,eugenel@mathcs.emory.edu</email>
<abstract confidence="0.99957708">Question answering communities such as Yahoo! Answers have emerged as a popular alternative to general-purpose web search. By directly interacting with other participants, information seekers can obtain specific answers to their questions. However, user success in obtaining satisfactory answers varies greatly. We hypothesize that satisfaction with the contributed answers is largely determined by the asker’s prior experience, expectations, and personal preferences. Hence, we begin to deof asker satisfaction to predict whether a particular question author will be satisfied with the answers contributed by the community participants. We formalize this problem, and explore a variety of content, structure, and interaction features for this task using standard machine learning techniques. Our experimental evaluation over thousands of real questions indicates that indeed it is beneficial to personalize satisfaction predictions when sufficient prior user history exists, significantly improving accuracy over a “one-size-fits-all” prediction model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>C Castillo</author>
<author>D Donato</author>
<author>A Gionis</author>
<author>G Mishne</author>
</authors>
<title>Finding high-quality content in social media with an application to community-based question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of WSDM.</booktitle>
<contexts>
<context position="5691" citStr="Agichtein et al., 2008" startWordPosition="882" endWordPosition="885"> best, and rate the answer by assigning stars. At that point, the question is considered as closed by asker. For more detailed treatment of user interactions in CQA see (Liu et al., 2008). If the asker rates the best answer with at least three out of five “stars”, we believe the asker is satisfied with the response. But often the asker never closes the answer personally, and instead, after a period of time, the question is closed automatically. In this case, the “best” answer may be chosen by the votes, or alternatively by automatically predicting answer quality (e.g., (Jeon et al., 2006) or (Agichtein et al., 2008)). While the best answer chosen automatically may be of high quality, it is unknown if the asker’s information need was satisfied. Based on our exploration we believe that the main reasons for not “closing” a question are a) the asker loses interest in the information and b) none of the answers are satisfactory. In both cases, the QA community has failed to provide satisfactory answers in a timely manner and “lost” the asker’s interest. We consider this outcome to be “unsatisfied”. We now define asker satisfaction more precisely: Definition 1 An asker in a QA community is considered satisfied </context>
<context position="8309" citStr="Agichtein et al., 2008" startWordPosition="1308" endWordPosition="1311">gative votes (“thumbs down”), and derived statistics such as the maximum of positive or negative votes received for any answer (e.g., to detect cases of brilliant answers or, conversely, blatant abuse). • Asker User History: Past asker activity history such as the most recent rating, average past satisfaction, and number of previous questions posted. Note that only the information available about the asker prior to posting the question was used. • Category Features: We hypothesized that user behavior (and asker satisfaction) varies by topical question category, as recently shown in reference (Agichtein et al., 2008). Therefore we model the prior of asker satisfaction for the category, such as the average asker rating (satisfaction). • Text Features: We also include word unigrams and bigrams to represent the text of the question subject, question detail, and the answer content. Separate feature spaces were used for each attribute to keep answer text distinct from question text, with frequency-based filtering. Classification Algorithms: We experimented with a variety of classifiers in the Weka framework (Witten and Frank, 2005). In particular, we compared Support Vector Machines, Decision trees, and Boosti</context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. 2008. Finding high-quality content in social media with an application to community-based question answering. In Proceedings of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J H Lee</author>
<author>S Park</author>
</authors>
<title>A framework to predict the quality of answers with nontextual features.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="5663" citStr="Jeon et al., 2006" startWordPosition="877" endWordPosition="880">s, she can choose it as best, and rate the answer by assigning stars. At that point, the question is considered as closed by asker. For more detailed treatment of user interactions in CQA see (Liu et al., 2008). If the asker rates the best answer with at least three out of five “stars”, we believe the asker is satisfied with the response. But often the asker never closes the answer personally, and instead, after a period of time, the question is closed automatically. In this case, the “best” answer may be chosen by the votes, or alternatively by automatically predicting answer quality (e.g., (Jeon et al., 2006) or (Agichtein et al., 2008)). While the best answer chosen automatically may be of high quality, it is unknown if the asker’s information need was satisfied. Based on our exploration we believe that the main reasons for not “closing” a question are a) the asker loses interest in the information and b) none of the answers are satisfactory. In both cases, the QA community has failed to provide satisfactory answers in a timely manner and “lost” the asker’s interest. We consider this outcome to be “unsatisfied”. We now define asker satisfaction more precisely: Definition 1 An asker in a QA commun</context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A framework to predict the quality of answers with nontextual features. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mei Kobayashi</author>
<author>Koichi Takeda</author>
</authors>
<title>Information retrieval on the web.</title>
<date>2000</date>
<journal>ACM Computing Surveys,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="3030" citStr="Kobayashi and Takeda, 2000" startWordPosition="446" endWordPosition="449"> site. Therefore, in this paper we begin to explore how to personalize satisfaction prediction - that is, to attempt to predict whether a specific information seeker will be satisfied with any of the contributed answers. Our aim is to provide a “personalized” recommendation to the user that they’ve got answers that satisfy their information need. To the best of our knowledge, ours is the first exploration of personalizing prediction of user satisfaction in complex and subjective information seeking environments. While information seeker satisfaction has been studied in ad-hoc IR context (see (Kobayashi and Takeda, 2000) for an overview), previous studies have been limited by the lack of realistic user feedback. In contrast, we deal with complex information needs and community-provided answers, trying to predict subjective ratings provided by users themselves. Furthermore, while automatic complex QA has been an active area of research, ranging from simple modification to factoid QA technique (e.g., (Soricut and Brill, 2004)) to knowledge intensive approaches for specialized domains, the technology does not yet exist to automatically answer open domain, complex, and subjective questions. Hence, this paper cont</context>
</contexts>
<marker>Kobayashi, Takeda, 2000</marker>
<rawString>Mei Kobayashi and Koichi Takeda. 2000. Information retrieval on the web. ACM Computing Surveys, 32(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>J Bian</author>
<author>E Agichtein</author>
</authors>
<title>Predicting information seeker satisfaction in community question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="2031" citStr="Liu et al., 2008" startWordPosition="289" endWordPosition="292">using generalpurpose web search engines, information seekers now have an option to post their questions (often complex, specific, and subjective) on Community QA sites such as Yahoo! Answers, and have their questions answered by other users. Hundreds of millions of answers have already been posted for tens of millions of questions in Yahoo! Answers. However, the success of obtaining satisfactory answers in the available CQA portals varies greatly. In many cases, the questions posted by askers go un-answered, or are answered poorly, never obtaining a satisfactory answer. 97 In our recent work (Liu et al., 2008) we have introduced a general model for predicting asker satisfaction in community question answering. We found that previous asker history is a significant factor that correlates with satisfaction. We hypothesize that asker’s satisfaction with contributed answers is largely determined by the asker expectations, prior knowledge and previous experience with using the CQA site. Therefore, in this paper we begin to explore how to personalize satisfaction prediction - that is, to attempt to predict whether a specific information seeker will be satisfied with any of the contributed answers. Our aim</context>
<context position="5255" citStr="Liu et al., 2008" startWordPosition="805" endWordPosition="808">stion and, optionally, additional details. After a short delay the question appears in the respective category list of open questions. At this point, other users can answer the question, vote on other users’ answers, or interact in other ways. The asker may be notified of the answers as they are submitted, or may check the contributed answers periodically. If the asker is satisfied with any of the answers, she can choose it as best, and rate the answer by assigning stars. At that point, the question is considered as closed by asker. For more detailed treatment of user interactions in CQA see (Liu et al., 2008). If the asker rates the best answer with at least three out of five “stars”, we believe the asker is satisfied with the response. But often the asker never closes the answer personally, and instead, after a period of time, the question is closed automatically. In this case, the “best” answer may be chosen by the votes, or alternatively by automatically predicting answer quality (e.g., (Jeon et al., 2006) or (Agichtein et al., 2008)). While the best answer chosen automatically may be of high quality, it is unknown if the asker’s information need was satisfied. Based on our exploration we belie</context>
<context position="6813" citStr="Liu et al., 2008" startWordPosition="1066" endWordPosition="1069">r satisfaction more precisely: Definition 1 An asker in a QA community is considered satisfied iff: the asker personally has closed the question and rated the best answer with at least 3 “stars”. Otherwise, the asker is unsatisfied. This definition captures a key aspect of asker satisfaction, namely that we can reliably identify when the asker is satisfied but not the converse. 2.1 Asker Satisfaction Prediction Framework We now briefly review our ASP (Asker Satisfaction Prediction) framework that learns to classify whether a question has been satisfactorily answered, originally introduced in (Liu et al., 2008). ASP employs standard classification techniques to predict, given a question thread, whether an asker would be satisfied. A sample of features used to represent this problem is listed in Table 1. Our features are organized around the basic entities in a question answering community: questions, answers, questionanswer pairs, users, and categories. In total, we developed 51 features for this task. A sample of the features used are listed in the Figure 1. • Question Features: Traditional question answering features such as the wh-type of the question (e.g., “what” or “where”), and whether the qu</context>
</contexts>
<marker>Liu, Bian, Agichtein, 2008</marker>
<rawString>Y. Liu, J. Bian, and E. Agichtein. 2008. Predicting information seeker satisfaction in community question answering. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>E Brill</author>
</authors>
<title>Automatic question answering: Beyond the factoid.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="3441" citStr="Soricut and Brill, 2004" startWordPosition="511" endWordPosition="514">rsonalizing prediction of user satisfaction in complex and subjective information seeking environments. While information seeker satisfaction has been studied in ad-hoc IR context (see (Kobayashi and Takeda, 2000) for an overview), previous studies have been limited by the lack of realistic user feedback. In contrast, we deal with complex information needs and community-provided answers, trying to predict subjective ratings provided by users themselves. Furthermore, while automatic complex QA has been an active area of research, ranging from simple modification to factoid QA technique (e.g., (Soricut and Brill, 2004)) to knowledge intensive approaches for specialized domains, the technology does not yet exist to automatically answer open domain, complex, and subjective questions. Hence, this paper contributes to both the understanding of complex question answering, and explores evaluation issues in a new setting. The rest of the paper is organized as follows. We describe the problem and our approach in Section 2, including our initial attempt at personalizing satisfaction prediction. We report results of a largescale evaluation over thousands of real users and Proceedings of ACL-08: HLT, Short Papers (Com</context>
</contexts>
<marker>Soricut, Brill, 2004</marker>
<rawString>R. Soricut and E. Brill. 2004. Automatic question answering: Beyond the factoid. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques. Morgan Kaufman, 2nd edition.</title>
<date>2005</date>
<contexts>
<context position="8829" citStr="Witten and Frank, 2005" startWordPosition="1388" endWordPosition="1392">isfaction) varies by topical question category, as recently shown in reference (Agichtein et al., 2008). Therefore we model the prior of asker satisfaction for the category, such as the average asker rating (satisfaction). • Text Features: We also include word unigrams and bigrams to represent the text of the question subject, question detail, and the answer content. Separate feature spaces were used for each attribute to keep answer text distinct from question text, with frequency-based filtering. Classification Algorithms: We experimented with a variety of classifiers in the Weka framework (Witten and Frank, 2005). In particular, we compared Support Vector Machines, Decision trees, and Boosting-based classifiers. SVM performed the best 98 Feature Description Question Features Q: Q punctuation density Ratio of punctuation to words in the question Q: Q KL div wikipedia KL divergence with Wikipedia corpus Q: Q KL div category KL divergence with “satisfied” questions in category Q: Q KL div trec KL divergence with TREC questions corpus Question-Answer Relationship Features QA: QA sum pos vote Sum of positive votes for all the answers QA: QA sum neg vote Sum of negative votes for all the answers QA: QA KL d</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufman, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>