<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.993824">
Phylogenetic Grammar Induction
</title>
<author confidence="0.998028">
Taylor Berg-Kirkpatrick and Dan Klein
</author>
<affiliation confidence="0.9984975">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.996706">
{tberg, klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999821875">
We present an approach to multilin-
gual grammar induction that exploits a
phylogeny-structured model of parameter
drift. Our method does not require any
translated texts or token-level alignments.
Instead, the phylogenetic prior couples
languages at a parameter level. Joint in-
duction in the multilingual model substan-
tially outperforms independent learning,
with larger gains both from more articu-
lated phylogenies and as well as from in-
creasing numbers of languages. Across
eight languages, the multilingual approach
gives error reductions over the standard
monolingual DMV averaging 21.1% and
reaching as high as 39%.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971048387097">
Learning multiple languages together should be
easier than learning them separately. For exam-
ple, in the domain of syntactic parsing, a range
of recent work has exploited the mutual constraint
between two languages’ parses of the same bi-
text (Kuhn, 2004; Burkett and Klein, 2008; Kuz-
man et al., 2009; Smith and Eisner, 2009; Sny-
der et al., 2009a). Moreover, Snyder et al. (2009b)
in the context of unsupervised part-of-speech in-
duction (and Bouchard-Cˆot´e et al. (2007) in the
context of phonology) show that extending be-
yond two languages can provide increasing ben-
efit. However, multitexts are only available for
limited languages and domains. In this work, we
consider unsupervised grammar induction without
bitexts or multitexts. Without translation exam-
ples, multilingual constraints cannot be exploited
at the sentence token level. Rather, we capture
multilingual constraints at a parameter level, us-
ing a phylogeny-structured prior to tie together the
various individual languages’ learning problems.
Our joint, hierarchical prior couples model param-
eters for different languages in a way that respects
knowledge about how the languages evolved.
Aspects of this work are closely related to Co-
hen and Smith (2009) and Bouchard-Cˆot´e et al.
(2007). Cohen and Smith (2009) present a model
for jointly learning English and Chinese depen-
dency grammars without bitexts. In their work,
structurally constrained covariance in a logistic
normal prior is used to couple parameters between
the two languages. Our work, though also differ-
ent in technical approach, differs most centrally in
the extension to multiple languages and the use of
a phylogeny. Bouchard-Cˆot´e et al. (2007) consid-
ers an entirely different problem, phonological re-
construction, but shares with this work both the
use of a phylogenetic structure as well as the use
of log-linear parameterization of local model com-
ponents. Our work differs from theirs primarily
in the task (syntax vs. phonology) and the vari-
ables governed by the phylogeny: in our model it
is the grammar parameters that drift (in the prior)
rather than individual word forms (in the likeli-
hood model).
Specifically, we consider dependency induction
in the DMV model of Klein and Manning (2004).
Our data is a collection of standard dependency
data sets in eight languages: English, Dutch, Dan-
ish, Swedish, Spanish, Portuguese, Slovene, and
Chinese. Our focus is not the DMV model itself,
which is well-studied, but rather the prior which
couples the various languages’ parameters. While
some choices of prior structure can greatly com-
plicate inference (Cohen and Smith, 2009), we
choose a hierarchical Gaussian form for the drift
term, which allows the gradient of the observed
data likelihood to be easily computed using stan-
dard dynamic programming methods.
In our experiments, joint multilingual learning
substantially outperforms independent monolin-
gual learning. Using a limited phylogeny that
</bodyText>
<page confidence="0.931754">
1288
</page>
<note confidence="0.9417705">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288–1297,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999804142857143">
only couples languages within linguistic families
reduces error by 5.6% over the monolingual base-
line. Using a flat, global phylogeny gives a greater
reduction, almost 10%. Finally, a more articu-
lated phylogeny that captures both inter- and intra-
family effects gives an even larger average relative
error reduction of 21.1%.
</bodyText>
<sectionHeader confidence="0.94201" genericHeader="introduction">
2 Model
</sectionHeader>
<figure confidence="0.989911571428571">
Global
Indo-
European
Balto-
Italic Slavic
Sino-
Tibetan
</figure>
<subsectionHeader confidence="0.827411">
Sinitic
</subsectionHeader>
<bodyText confidence="0.999993416666667">
We define our model over two kinds of random
variables: dependency trees and parameters. For
each language E in a set L, our model will generate
a collection tt of dependency trees tP. We assume
that these dependency trees are generated by the
DMV model of Klein and Manning (2004), which
we write as tP ∼ DMV(Ot). Here, Ot is a vector
of the various model parameters for language E.
The prior is what couples the Ot parameter vectors
across languages; it is the focus of this work. We
first consider the likelihood model before moving
on to the prior.
</bodyText>
<subsectionHeader confidence="0.5715">
2.1 Dependency Model with Valence
</subsectionHeader>
<bodyText confidence="0.999855178571428">
A dependency parse is a directed tree t over tokens
in a sentence s. Each edge of the tree specifies a
directed dependency from a head token to a de-
pendent, or argument token. The DMV is a gen-
erative model for trees t, which has been widely
used for dependency parse induction. The ob-
served data likelihood, used for parameter estima-
tion, is the marginal probability of generating the
observed sentences s, which are simply the leaves
of the trees t. Generation in the DMV model in-
volves two types of local conditional probabilities:
CONTINUE distributions that capture valence and
ATTACH distributions that capture argument selec-
tion.
First, the Bernoulli CONTINUE probability dis-
tributions PCONTINUE(c|h, dir, adj; Ot) model the
fertility of a particular head type h. The outcome
c E {stop, continue} is conditioned on the head
type h, direction dir, and adjacency adj. If ahead
type’s continue probability is low, tokens of this
type will tend to generate few arguments.
Second, the ATTACH multinomial probability
distributions PATTACH(a|h, dir; Ot) capture attach-
ment preferences of heads, where a and h are both
token types. We take the same approach as pre-
vious work (Klein and Manning, 2004; Cohen and
Smith, 2009) and use gold part-of-speech labels as
tokens. Thus, the basic observed “word” types are
</bodyText>
<subsectionHeader confidence="0.891618333333333">
West North
Germanic Germanic
English Dutch Danish Swedish Spanish Portuguese Slovene Chinese
</subsectionHeader>
<figureCaption confidence="0.992852">
Figure 1: An example of a linguistically-plausible phylo-
genetic tree over the languages in our training data. Leaves
correspond to (observed) modern languages, while internal
nodes represent (unobserved) ancestral languages.
</figureCaption>
<bodyText confidence="0.7776">
actually word classes.
</bodyText>
<subsubsectionHeader confidence="0.353338">
2.1.1 Log-Linear Parameterization
</subsubsectionHeader>
<bodyText confidence="0.987692833333333">
The DMV’s local conditional distributions were
originally given as simple multinomial distribu-
tions with one parameter per outcome. However,
they can be re-parameterized to give the following
log-linear form (Eisner, 2002; Bouchard-Cˆot´e et
al., 2007; Berg-Kirkpatrick et al., 2010):
</bodyText>
<equation confidence="0.9712535">
PCONTINUE(c|h, dir, adj; θℓ) =
exp ˆθℓT f CONTINUE(c, h, dir, adj)˜
Pc′ exp ˆθℓT f CONTINUE(c′, h, dir, adj)˜
PATTACH(a|h, dir; θℓ) =
exp ˆθℓT f ATTACH(a, h, dir)˜
P a′ exp ˆθℓT f ATTACH(a′, h, dir)˜
</equation>
<bodyText confidence="0.999942571428571">
The parameters are weights Ot with one weight
vector per language. In the case where the vec-
tor of feature functions f has an indicator for each
possible conjunction of outcome and conditions,
the original multinomial distributions are recov-
ered. We refer to these full indicator features as
the set of SPECIFIC features.
</bodyText>
<subsectionHeader confidence="0.997797">
2.2 Phylogenetic Prior
</subsectionHeader>
<bodyText confidence="0.999734666666667">
The focus of this work is coupling each of the pa-
rameters Ot in a phylogeny-structured prior. Con-
sider a phylogeny like the one shown in Fig-
ure 1, where each modern language E in L is a
leaf. We would like to say that the leaves’ pa-
rameter vectors arise from a process which slowly
</bodyText>
<figure confidence="0.99078775">
Germanic
Ibero-
Slavic
Romance
</figure>
<page confidence="0.994306">
1289
</page>
<bodyText confidence="0.999961485714286">
drifts along each branch. A convenient choice is
to posit additional parameter variables Ot+ at in-
ternal nodes E+ E L+, a set of ancestral lan-
guages, and to assume that the conditional dis-
tribution P(OtJOpar(t)) at each branch in the phy-
logeny is a Gaussian centered on Opar(t), where
par(E) is the parent of E in the phylogeny and
E ranges over L U L+. The variance structure
of the Gaussian would then determine how much
drift (and in what directions) is expected. Con-
cretely, we assume that each drift distribution is
an isotropic Gaussian with mean Opar(t) and scalar
variance Q2. The root is centered at zero. We have
thus defined a joint distribution P(OJQ2) where
O = (Ot : E E LUL+). Q2 is a hyperparameter for
this prior which could itself be re-parameterized to
depend on branch length or be learned; we simply
set it to a plausible constant value.
Two primary challenges remain. First, infer-
ence under arbitrary priors can become complex.
However, in the simple case of our diagonal co-
variance Gaussians, the gradient of the observed
data likelihood can be computed directly using the
DMV’s expected counts and maximum-likelihood
estimation can be accomplished by applying stan-
dard gradient optimization methods. Second,
while the choice of diagonal covariance is effi-
cient, it causes components of O that correspond
to features occurring in only one language to be
marginally independent of the parameters of all
other languages. In other words, only features
which fire in more than one language are coupled
by the prior. In the next section, we therefore in-
crease the overlap between languages’ features by
using coarse projections of parts-of-speech.
</bodyText>
<subsectionHeader confidence="0.998727">
2.3 Projected Features
</subsectionHeader>
<bodyText confidence="0.999972857142857">
With diagonal covariance in the Gaussian drift
terms, each parameter evolves independently of
the others. Therefore, our prior will be most
informative when features activate in multiple
languages. In phonology, it is useful to map
phonemes to the International Phonetic Alphabet
(IPA) in order to have a language-independent
parameterization. We introduce a similarly neu-
tral representation here by projecting language-
specific parts-of-speech to a coarse, shared inven-
tory.
Indeed, we assume that each language has a dis-
tinct tagset, and so the basic configurational fea-
tures will be language specific. For example, when
</bodyText>
<note confidence="0.520202">
SPECIFIC: Activate for only one conjunction of out-
come and conditions:
</note>
<figure confidence="0.6838776">
1(c = ·, h = ·, dir = ·, adj = ·)
SHARED: Activate for heads from multiple languages
using cross-lingual POS projection ir(·):
1(c = ·, ir(h) = ·, dir = ·, adj = ·)
CONTINUE distribution feature templates.
SPECIFIC: Activate for only one conjunction of out-
come and conditions:
1(a = ·, h = ·, dir = ·)
SHARED: Activate for heads and arguments from
multiple languages using cross-lingual
</figure>
<equation confidence="0.91190475">
POS projection ir(·):
1(ir(a) = ·, ir(h) = ·, dir = ·)
1(ir(a) = ·, h = ·, dir = ·)
1(a = ·, ir(h) = ·, dir = ·)
</equation>
<tableCaption confidence="0.764757">
ATTACH distribution feature templates.
Table 1: Feature templates for CONTINUE and ATTACH con-
ditional distributions.
</tableCaption>
<bodyText confidence="0.999972441176471">
an English VBZ takes a left argument headed by a
NNS, a feature will activate specific to VBZ-NNS-
LEFT. That feature will be used in the log-linear
attachment probability for English. However, be-
cause that feature does not show up in any other
language, it is not usefully controlled by the prior.
Therefore, we also include coarser features which
activate on more abstract, cross-linguistic config-
urations. In the same example, a feature will fire
indicating a coarse, direction-free NOUN-VERB at-
tachment. This feature will now occur in multiple
languages and will contribute to each of those lan-
guages’ attachment models. Although such cross-
lingual features will have different weight param-
eters in each language, those weights will covary,
being correlated by the prior.
The coarse features are defined via a projec-
tion 7r from language-specific part-of-speech la-
bels to coarser, cross-lingual word classes, and
hence we refer to them as SHARED features. For
each corpus used in this paper, we use the tagging
annotation guidelines to manually define a fixed
mapping from the corpus tagset to the following
coarse tagset: noun, verb, adjective, adverb, con-
junction, preposition, determiner, interjection, nu-
meral, and pronoun. Parts-of-speech for which
this coarse mapping is ambiguous or impossible
are not mapped, and do not have corresponding
SHARED features.
We summarize the feature templates for the
CONTINUE and ATTACH conditional distributions
in Table 1. Variants of all feature templates that
ignore direction and/or adjacency are included. In
practice, we found it beneficial for all language-
</bodyText>
<page confidence="0.934472">
1290
</page>
<bodyText confidence="0.9965698">
independent features to ignore direction.
Again, only the coarse features occur in mul-
tiple languages, so all phylogenetic influence is
through those. Nonetheless, the effect of the phy-
logeny turns out to be quite strong.
</bodyText>
<subsectionHeader confidence="0.997433">
2.4 Learning
</subsectionHeader>
<bodyText confidence="0.999972">
We now turn to learning with the phylogenetic
prior. Since the prior couples parameters across
languages, this learning problem requires param-
eters for all languages be estimated jointly. We
seek to find 0 = (Ot : E ∈ L ∪ L+) which
optimizes log P(0|s), where s aggregates the ob-
served leaves of all the dependency trees in all the
languages. This can be written as
</bodyText>
<equation confidence="0.990894">
log P(0) + log P(s|0) − log P(s)
</equation>
<bodyText confidence="0.96733225">
The third term is a constant and can be ignored.
The first term can be written as
The expected gradient of the log joint likelihood
of sentences and parses is equal to the gradient of
the log marginal likelihood of just sentences, or
the observed data likelihood (Salakhutdinov et al.,
2003). ea,h,dir(st; Ot) is the expected count of the
number of times head h is attached to a in direc-
tion dir given the observed sentences st and DMV
parameters Ot. ec,h,dir,adj(st; Ot) is defined simi-
larly. Note that these are the same expected counts
required to perform EM on the DMV, and are com-
putable by dynamic programming.
The computation time is dominated by the com-
putation of each sentence’s posterior expected
counts, which are independent given the parame-
ters, so the time required per iteration is essentially
the same whether training all languages jointly or
independently. In practice, the total number of it-
erations was also similar.
� 1 3 Experimental Setup
log P(0) = 2σ2 k Ot − Opar(t) k2 + C 3.1 Data
tELUL+
where C is a constant. The form of log P(0) im-
mediately shows how parameters are penalized for
being different across languages, more so for lan-
guages that are near each other in the phylogeny.
The second term
</bodyText>
<equation confidence="0.9846335">
log P(s|0) = � log P (st|Ot)
tEL
</equation>
<bodyText confidence="0.9999496">
is a sum of observed data likelihoods under
the standard DMV models for each language,
computable by dynamic programming (Klein
and Manning, 2004). Together, this yields the
following objective function:
</bodyText>
<equation confidence="0.906318">
l(0) = EtELUL+ 1
2σ2 kOt − Opar(t)k22 + EtEL log P(st|Ot)
</equation>
<bodyText confidence="0.994063">
which can be optimized using gradient methods
or (MAP) EM. Here we used L-BFGS (Liu et al.,
1989). This requires computation of the gradient
of the observed data likelihood log P(st|Ot)
which is given by:
</bodyText>
<equation confidence="0.705096">
∇ log P(st|Ot) = Etℓ|sℓ L∇ log P(st, tt|Ot)] =
</equation>
<bodyText confidence="0.9999573">
We ran experiments with the following languages:
English, Dutch, Danish, Swedish, Spanish, Por-
tuguese, Slovene, and Chinese. For all languages
but English and Chinese, we used corpora from the
2006 CoNLL-X Shared Task dependency parsing
data set (Buchholz and Marsi, 2006). We used the
shared task training set to both train and test our
models. These corpora provide hand-labeled part-
of-speech tags (except for Dutch, which is auto-
matically tagged) and provide dependency parses,
which are either themselves hand-labeled or have
been converted from hand-labeled parses of other
kinds. For English and Chinese we use sections
2-21 of the Penn Treebank (PTB) (Marcus et al.,
1993) and sections 1-270 of the Chinese Tree-
bank (CTB) (Xue et al., 2002) respectively. Sim-
ilarly, these sections were used for both training
and testing. The English and Chinese data sets
have hand-labeled constituency parses and part-of-
speech tags, but no dependency parses. We used
the Bikel Chinese head finder (Bikel and Chiang,
2000) and the Collins English head finder (Collins,
1999) to transform the gold constituency parses
into gold dependency parses. None of the corpora
are bitexts. For all languages, we ran experiments
on all sentences of length 10 or less after punctua-
tion has been removed.
When constructing phylogenies over the lan-
guages we made use of their linguistic classifica-
tions. English and Dutch are part of the West Ger-
</bodyText>
<equation confidence="0.980122625">
Ec,h,dir,adj ec,h,dir,adj (st; Ot) · LfCONTINUE(c, h, dir, adj) − 1
Ec′ PCONTINUE(c′ |h, dir, adj; Ot) f CONTINUE(c′, h, dir, adj)]
Ea,h,dir ea,h,dir(st; Ot) · LfATTACH(a, h, dir) − 11
Ea′ PATTACH(a′|h, dir; Ot) f ATTACH(a′, h, dir)]
�
� � � � � � � � � � � � � � � � �
�
� � � � � � � � � � � � � � � � �
</equation>
<page confidence="0.979252">
1291
</page>
<figure confidence="0.788579">
English Dutch Danish Swedish Spanish Portuguese Slovene Chinese
(b) Global
</figure>
<subsectionHeader confidence="0.563717">
English Dutch Danish Swedish Spanish Portuguese Slovene Chinese
</subsectionHeader>
<bodyText confidence="0.999927333333333">
share a common parent node in the prior, meaning
that global regularities that are consistent across
all languages can be captured. We refer to this
structure as GLOBAL.
While the global model couples the parameters
for all eight languages, it does so without sensi-
tivity to the articulated structure of their descent.
Figure 2(c) shows a more nuanced prior struc-
ture, LINGUISTIC, which groups languages first
by family and then under a global node. This
structure allows global regularities as well as reg-
ularities within families to be learned.
</bodyText>
<figure confidence="0.956616625">
West North Ibero= Slavic Sinitic
Romance
Germanic
Germanic
(c) Global 3.2.2 Parameterization and ALLPAIRS Model
West North Ibero= Slavic Sinitic
Germanic Germanic Romance
English Dutch Danish Swedish Spanish Portuguese Slovene Chinese
</figure>
<figureCaption confidence="0.9974615">
Figure 2: (a) Phylogeny for FAMILIES model. (b) Phylogeny
for GLOBAL model. (c) Phylogeny for LINGUISTIC model.
</figureCaption>
<bodyText confidence="0.999788">
manic family of languages, whereas Danish and
Swedish are part of the North Germanic family.
Spanish and Portuguese are both part of the Ibero-
Romance family. Slovene is part of the Slavic
family. Finally, Chinese is in the Sinitic family,
and is not an Indo-European language like the oth-
ers. We interchangeably speak of a language fam-
ily and the ancestral node corresponding to that
family’s root language in a phylogeny.
</bodyText>
<subsectionHeader confidence="0.999604">
3.2 Models Compared
</subsectionHeader>
<bodyText confidence="0.999904833333333">
We evaluated three phylogenetic priors, each with
a different phylogenetic structure. We compare
with two monolingual baselines, as well as an all-
pairs multilingual model that does not have a phy-
logenetic interpretation, but which provides very
similar capacity for parameter coupling.
</bodyText>
<subsectionHeader confidence="0.441405">
3.2.1 Phylogenetic Models
</subsectionHeader>
<bodyText confidence="0.999986378378378">
The first phylogenetic model uses the shallow phy-
logeny shown in Figure 2(a), in which only lan-
guages within the same family have a shared par-
ent node. We refer to this structure as FAMILIES.
Under this prior, the learning task decouples into
independent subtasks for each family, but no reg-
ularities across families can be captured.
The family-level model misses the constraints
between distant languages. Figure 2(b) shows an-
other simple configuration, wherein all languages
Daum´e III (2007) and Finkel and Manning (2009)
consider a formally similar Gaussian hierarchy for
domain adaptation. As pointed out in Finkel and
Manning (2009), there is a simple equivalence be-
tween hierarchical regularization as described here
and the addition of new tied features in a “flat”
model with zero-meaned Gaussian regularization
on all parameters. In particular, instead of param-
eterizing the objective in Section 2.4 in terms of
multiple sets of weights, one at each node in the
phylogeny (the hierarchical parameterization, de-
scribed in Section 2.4), it is equivalent to param-
eterize this same objective in terms of a single set
of weights on a larger of group features (the flat
parameterization). This larger group of features
contains a duplicate set of the features discussed in
Section 2.3 for each node in the phylogeny, each
of which is active only on the languages that are its
descendants. A linear transformation between pa-
rameterizations gives equivalence. See Finkel and
Manning (2009) for details.
In the flat parameterization, it seems equally
reasonable to simply tie all pairs of languages by
adding duplicate sets of features for each pair.
This gives the ALLPAIRS setting, which we also
compare to the tree-structured phylogenetic mod-
els above.
</bodyText>
<subsectionHeader confidence="0.996628">
3.3 Baselines
</subsectionHeader>
<bodyText confidence="0.997448111111111">
To evaluate the impact of multilingual constraint,
we compared against two monolingual baselines.
The first baseline is the standard DMV with
only SPECIFIC features, which yields the standard
multinomial DMV (weak baseline). To facilitate
comparison to past work, we used no prior for this
monolingual model. The second baseline is the
DMV with added SHARED features. This model
includes a simple isotropic Gaussian prior on pa-
</bodyText>
<page confidence="0.98726">
1292
</page>
<figure confidence="0.998094666666667">
Monolingual Multilingual
Phylogenetic
Corpus Size
Baseline
Baseline w/ SHARED
ALLPAIRS
FAMILIES
BESTPAIR
GLOBAL
LINGUISTIC
West Germanic
English 6008
Dutch 6678
47.1
36.3
51.3
36.0
48.5
44.0
51.3
36.1
51.3 (Ch)
36.2 (Sw)
51.2
44.0
62.3
45.1
North Germanic
Danish 1870
Swedish 3571
33.5
45.3
33.6
44.8
40.5
56.3
31.4
44.8
34.2 (Du)
44.8 (Ch)
39.6
44.5
41.6
58.3
Ibero-Romance
Spanish 712
Portuguese 2515
28.0
38.5
40.5
38.5
58.7
63.1
63.4
37.4
63.8 (Da)
38.4 (Sw)
59.4
37.4
58.4
63.0
Slavic
Slovene 627
38.5
39.7
49.0
–
49.6 (En)
49.4
48.4
Sinitic
Chinese 959
36.3
43.3
50.7
–
49.7 (Sw)
50.1
49.6
Macro-Avg. Relative Error Reduction
17.1 5.6 8.5 9.9 21.1
</figure>
<figureCaption confidence="0.8464665">
Table 2: Directed dependency accuracy of monolingual and multilingual models, and relative error reduction over the monolin-
gual baseline with SHARED features macro-averaged over languages. Multilingual models outperformed monolingual models
in general, with larger gains from increasing numbers of languages. Additionally, more nuanced phylogenetic structures out-
performed cruder ones.
</figureCaption>
<bodyText confidence="0.978297666666667">
rameters. This second baseline is the more direct
comparison to the multilingual experiments here
(strong baseline).
</bodyText>
<subsectionHeader confidence="0.869602">
3.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999953222222222">
For each setting, we evaluated the directed de-
pendency accuracy of the minimum Bayes risk
(MBR) dependency parses produced by our mod-
els under maximum (posterior) likelihood parame-
ter estimates. We computed accuracies separately
for each language in each condition. In addition,
for multilingual models, we computed the relative
error reduction over the strong monolingual base-
line, macro-averaged over languages.
</bodyText>
<subsectionHeader confidence="0.899509">
3.5 Training
</subsectionHeader>
<bodyText confidence="0.981984720930233">
Our implementation used the flat parameteriza-
tion described in Section 3.2.2 for both the phy-
logenetic and ALLPAIRS models. We originally
did this in order to facilitate comparison with the
non-phylogenetic ALLPAIRS model, which has no
equivalent hierarchical parameterization. In prac-
tice, optimizing with the hierarchical parameteri-
zation also seemed to underperform.1
1We noticed that the weights of features shared across lan-
guages had larger magnitude early in the optimization proce-
dure when using the flat parameterization compared to us-
ing the hierarchical parameterization, perhaps indicating that
cross-lingual influences had a larger effect on learning in its
initial stages.
All models were trained by directly optimizing
the observed data likelihood using L-BFGS (Liu et
al., 1989). Berg-Kirkpatrick et al. (2010) suggest
that directly optimizing the observed data likeli-
hood may offer improvements over the more stan-
dard expectation-maximization (EM) optimization
procedure for models such as the DMV, espe-
cially when the model is parameterized using fea-
tures. We stopped training after 200 iterations in
all cases. This fixed stopping criterion seemed to
be adequate in all experiments, but presumably
there is a potential gain to be had in fine tuning.
To initialize, we used the harmonic initializer pre-
sented in Klein and Manning (2004). This type of
initialization is deterministic, and thus we did not
perform random restarts.
We found that for all models Q2 = 0.2 gave rea-
sonable results, and we used this setting in all ex-
periments. For most models, we found that vary-
ing Q2 in a reasonable range did not substantially
affect accuracy. For some models, the directed ac-
curacy was less flat with respect to Q2. In these
less-stable cases, there seemed to be an interac-
tion between the variance and the choice between
head conventions. For example, for some settings
of Q2, but not others, the model would learn that
determiners head noun phrases. In particular, we
observed that even when direct accuracy did fluc-
tuate, undirected accuracy remained more stable.
</bodyText>
<page confidence="0.986187">
1293
</page>
<sectionHeader confidence="0.999793" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.9998025">
Table 2 shows the overall results. In all cases,
methods which coupled the languages in some
way outperformed the independent baselines that
considered each language independently.
</bodyText>
<subsectionHeader confidence="0.996486">
4.1 Bilingual Models
</subsectionHeader>
<bodyText confidence="0.99999612">
The weakest of the coupled models was FAMI-
LIES, which had an average relative error reduc-
tion of 5.6% over the strong baseline. In this case,
most of the average improvement came from a sin-
gle family: Spanish and Portuguese. The limited
improvement of the family-level prior compared
to other phylogenies suggests that there are impor-
tant multilingual interactions that do not happen
within families. Table 2 also reports the maximum
accuracy achieved for each language when it was
paired with another language (same family or oth-
erwise) and trained together with a single common
parent. These results appear in the column headed
by BESTPAIR, and show the best accuracy for the
language on that row over all possible pairings
with other languages. When pairs of languages
were trained together in isolation, the largest bene-
fit was seen for languages with small training cor-
pora, not necessarily languages with common an-
cestry. In our setup, Spanish, Slovene, and Chi-
nese have substantially smaller training corpora
than the rest of the languages considered. Other-
wise, the patterns are not particularly clear; com-
bined with subsequent results, it seems that pair-
wise constraint is fairly limited.
</bodyText>
<subsectionHeader confidence="0.986157">
4.2 Multilingual Models
</subsectionHeader>
<bodyText confidence="0.999991633333333">
Models that coupled multiple languages per-
formed better in general than models that only
considered pairs of languages. The GLOBAL
model, which couples all languages, if crudely,
yielded an average relative error reduction of
9.9%. This improvement comes as the number
of languages able to exert mutual constraint in-
creases. For example, Dutch and Danish had large
improvements, over and above any improvements
these two languages gained when trained with a
single additional language. Beyond the simplistic
GLOBAL phylogeny, the more nuanced LINGUIS-
TIC model gave large improvements for English,
Swedish, and Portuguese. Indeed, the LINGUIS-
TIC model is the only model we evaluated that
gave improvements for all the languages we con-
sidered.
It is reasonable to worry that the improvements
from these multilingual models might be partially
due to having more total training data in the mul-
tilingual setting. However, we found that halv-
ing the amount of data used to train the English,
Dutch, and Swedish (the languages with the most
training data) monolingual models did not sub-
stantially affect their performance, suggesting that
for languages with several thousand sentences or
more, the increase in statistical support due to ad-
ditional monolingual data was not an important ef-
fect (the DMV is a relatively low-capacity model
in any case).
</bodyText>
<subsectionHeader confidence="0.999935">
4.3 Comparison of Phylogenies
</subsectionHeader>
<bodyText confidence="0.999966">
Recall the structures of the three phylogenies
presented in Figure 2. These phylogenies dif-
fer in the correlations they can represent. The
GLOBAL phylogeny captures only “universals,”
while FAMILIES captures only correlations be-
tween languages that are known to be similar. The
LINGUISTIC model captures both of these effects
simultaneously by using a two layer hierarchy.
Notably, the improvement due to the LINGUISTIC
model is more than the sum of the improvements
due to the GLOBAL and FAMILIES models.
</bodyText>
<subsectionHeader confidence="0.974731">
4.4 Phylogenetic vs. ALLPAIRS
</subsectionHeader>
<bodyText confidence="0.99998495">
The phylogeny is capable of allowing appropri-
ate influence to pass between languages at mul-
tiple levels. We compare these results to the
ALLPAIRS model in order to see whether limi-
tation to a tree structure is helpful. The ALL-
PAIRS model achieved an average relative error
reduction of 17.1%, certainly outperforming both
the simple phylogenetic models. However, the
rich phylogeny of the LINGUISTIC model, which
incorporates linguistic constraints, outperformed
the freer ALLPAIRS model. A large portion of
this improvement came from English, a language
for which the LINGUISTIC model greatly outper-
formed all other models evaluated. We found that
the improved English analyses produced by the
LINGUISTIC model were more consistent with this
model’s analyses of other languages. This consis-
tency was not present for the English analyses pro-
duced by other models. We explore consistency in
more detail in Section 5.
</bodyText>
<subsectionHeader confidence="0.971687">
4.5 Comparison to Related Work
</subsectionHeader>
<bodyText confidence="0.998247">
The likelihood models for both the strong mono-
lingual baseline and the various multilingual mod-
</bodyText>
<page confidence="0.979601">
1294
</page>
<bodyText confidence="0.999991842105263">
els are the same, both expanding upon the standard
DMV by adding coarse SHARED features. These
coarse features, even in a monolingual setting, im-
proved performance slightly over the weak base-
line, perhaps by encouraging consistent treatment
of the different finer-grained variants of parts-
of-speech (Berg-Kirkpatrick et al., 2010).2 The
only difference between the multilingual systems
and the strong baseline is whether or not cross-
language influence is allowed through the prior.
While this progression of model structure is
similar to that explored in Cohen and Smith
(2009), Cohen and Smith saw their largest im-
provements from tying together parameters for the
varieties of coarse parts-of-speech monolinugally,
and then only moderate improvements from allow-
ing cross-linguistic influence on top of monolin-
gual sharing. When Cohen and Smith compared
their best shared logistic-normal bilingual mod-
els to monolingual counter-parts for the languages
they investigate (Chinese and English), they re-
ported a relative error reduction of 5.3%. In com-
parison, with the LINGUISTIC model, we saw a
much larger 16.9% relative error reduction over
our strong baseline for these languages. Evaluat-
ing our LINGUISTIC model on the same test sets
as (Cohen and Smith, 2009), sentences of length
10 or less in section 23 of PTB and sections 271-
300 of CTB, we achieved an accuracy of 56.6 for
Chinese and 60.3 for English. The best models
of Cohen and Smith (2009) achieved accuracies of
52.0 and 62.0 respectively on these same test sets.
Our results indicate that the majority of our
model’s power beyond that of the standard DMV
is derived from multilingual, and in particular,
more-than-bilingual, interaction. These are, to the
best of our knowledge, the first results of this kind
for grammar induction without bitext.
</bodyText>
<sectionHeader confidence="0.995519" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.9999945">
By examining the proposed parses we found that
the LINGUISTIC and ALLPAIRS models produced
analyses that were more consistent across lan-
guages than those of the other models. We
also observed that the most common errors can
be summarized succinctly by looking at attach-
ment counts between coarse parts-of-speech. Fig-
ure 3 shows matrix representations of dependency
</bodyText>
<footnote confidence="0.95642875">
2Coarse features that only tie nouns and verbs are ex-
plored in Berg-Kirkpatrick et al. (2010). We found that these
were very effective for English and Chinese, but gave worse
performance for other languages.
</footnote>
<bodyText confidence="0.99985731372549">
counts. The area of a square is proportional to the
number of order-collapsed dependencies where
the column label is the head and the row label is
the argument in the parses from each system. For
ease of comprehension, we use the cross-lingual
projections and only show counts for selected in-
teresting classes.
Comparing Figure 3(c), which shows depen-
dency counts proposed by the LINGUISTIC model,
to Figure 3(a), which shows the same for the
strong monolingual baseline, suggests that the
analyses proposed by the LINGUISTIC model are
more consistent across languages than are the
analyses proposed by the monolingual model. For
example, the monolingual learners are divided
as to whether determiners or nouns head noun
phrases. There is also confusion about which la-
bels head whole sentences. Dutch has the problem
that verbs modify pronouns more often than pro-
nouns modify verbs, and pronouns are predicted
to head sentences as often as verbs are. Span-
ish has some confusion about conjunctions, hy-
pothesizing that verbs often attach to conjunctions,
and conjunctions frequently head sentences. More
subtly, the monolingual analyses are inconsistent
in the way they head prepositional phrases. In
the monolingual Portuguese hypotheses, preposi-
tions modify nouns more often than nouns mod-
ify prepositions. In English, nouns modify prepo-
sitions, and prepositions modify verbs. Both the
Dutch and Spanish models are ambivalent about
the attachment of prepositions.
As has often been observed in other contexts
(Liang et al., 2008), promoting agreement can
improve accuracy in unsupervised learning. Not
only are the analyses proposed by the LINGUISTIC
model more consistent, they are also more in ac-
cordance with the gold analyses. Under the LIN-
GUISTIC model, Dutch now attaches pronouns to
verbs, and thus looks more like English, its sister
in the phylogenetic tree. The LINGUISTIC model
has also chosen consistent analyses for preposi-
tional phrases and noun phrases, calling preposi-
tions and nouns the heads of each, respectively.
The problem of conjunctions heading Spanish sen-
tences has also been corrected.
Figure 3(b) shows dependency counts for the
GLOBAL multilingual model. Unsurprisingly, the
analyses proposed under global constraint appear
somewhat more consistent than those proposed
under no multi-lingual constraint (now three lan-
</bodyText>
<page confidence="0.990696">
1295
</page>
<figureCaption confidence="0.928948">
Figure 3: Dependency counts in proposed parses. Row label modifies column label. (a) Monolingual baseline with SHARED
features. (b) GLOBAL model. (c) LINGUISTIC model. (d) Dependency counts in hand-labeled parses. Analyses proposed by
monolingual baseline show significant inconsistencies across languages. Analyses proposed by LINGUISTIC model are more
consistent across languages than those proposed by either the monolingual baseline or the GLOBAL model.
</figureCaption>
<bodyText confidence="0.999019375">
guages agree that prepositional phrases are headed
by prepositions), but not as consistent as those pro-
posed by the LINGUISTIC model.
Finally, Figure 3(d) shows dependency counts
in the hand-labeled dependency parses. It appears
that even the very consistent LINGUISTIC parses
do not capture the non-determinism of preposi-
tional phrase attachment to both nouns and verbs.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999955">
Even without translated texts, multilingual con-
straints expressed in the form of a phylogenetic
prior on parameters can give substantial gains
in grammar induction accuracy over treating lan-
guages in isolation. Additionally, articulated phy-
logenies that are sensitive to evolutionary structure
can outperform not only limited flatter priors but
also unconstrained all-pairs interactions.
</bodyText>
<sectionHeader confidence="0.999294" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998067333333333">
This project is funded in part by the NSF un-
der grant 0915265 and DARPA under grant
N10AP20007.
</bodyText>
<page confidence="0.987157">
1296
</page>
<sectionHeader confidence="0.995869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882064102564">
T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In North American Chapter of the
Association for Computational Linguistics.
D. M. Bikel and D. Chiang. 2000. Two statistical pars-
ing models applied to the Chinese treebank. In Sec-
ond Chinese Language Processing Workshop.
A. Bouchard-Cˆot´e, P. Liang, D. Klein, and T. L. Grif-
fiths. 2007. A probabilistic approach to diachronic
phonology. In Empirical Methods in Natural Lan-
guage Processing.
S. Buchholz and E. Marsi. 2006. Computational Nat-
ural Language Learning-X shared task on multilin-
gual dependency parsing. In Conference on Compu-
tational Natural Language Learning.
D. Burkett and D. Klein. 2008. Two languages are
better than one (for syntactic parsing). In Empirical
Methods in Natural Language Processing.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in un-
supervised grammar induction. In North American
Chapter of the Association for Computational Lin-
guistics.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. In Ph.D. thesis, University
of Pennsylvania, Philadelphia.
H. Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Association for Computational Linguis-
tics.
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Association for Compu-
tational Linguistics.
J. R. Finkel and C. D. Manning. 2009. Hierarchi-
cal bayesian domain adaptation. In North American
Chapter of the Association for Computational Lin-
guistics.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Association for Compu-
tational Linguistics.
J. Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Association for Computa-
tional Linguistics.
G. Kuzman, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Association for Computational Lin-
guistics/International Joint Conference on Natural
Language Processing.
P. Liang, D. Klein, and M. I. Jordan. 2008.
Agreement-based learning. In Advances in Neural
Information Processing Systems.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the
limited memory BFGS method for large scale opti-
mization. Mathematical Programming.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani.
2003. Optimization with EM and expectation-
conjugate-gradient. In International Conference on
Machine Learning.
D. A. Smith and J. Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous gram-
mar features. In Empirical Methods in Natural Lan-
guage Processing.
B. Snyder, T. Naseem, and R. Barzilay. 2009a. Unsu-
pervised multilingual grammar induction. In Asso-
ciation for Computational Linguistics/International
Joint Conference on Natural Language Processing.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzi-
lay. 2009b. Adding more languages improves un-
supervised multilingual part-of-speech tagging: A
Bayesian non-parametric approach. In North Amer-
ican Chapter of the Association for Computational
Linguistics.
N. Xue, F-D Chiou, and M. Palmer. 2002. Building
a large-scale annotated Chinese corpus. In Interna-
tional Conference on Computational Linguistics.
</reference>
<page confidence="0.992129">
1297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.830706">
<title confidence="0.999698">Phylogenetic Grammar Induction</title>
<author confidence="0.999228">Berg-Kirkpatrick Klein</author>
<affiliation confidence="0.996011">Computer Science Division University of California, Berkeley</affiliation>
<abstract confidence="0.989756705882353">We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>A Bouchard-Cˆot´e</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and D. Klein. 2010. Painless unsupervised learning with features. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>D Chiang</author>
</authors>
<title>Two statistical parsing models applied to the Chinese treebank.</title>
<date>2000</date>
<booktitle>In Second Chinese Language Processing Workshop.</booktitle>
<contexts>
<context position="15779" citStr="Bikel and Chiang, 2000" startWordPosition="2545" endWordPosition="2548">eech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been removed. When constructing phylogenies over the languages we made use of their linguistic classifications. English and Dutch are part of the West GerEc,h,dir,adj ec,h,dir,adj (st; Ot) · LfCONTINUE(c, h, dir, adj) − 1 Ec′ PCONTINUE(c′ |h, dir, adj; Ot) f CONTINUE(c′, h, dir, adj)] Ea,h,dir ea,h,dir(st; Ot) · LfATTACH(a, h, dir) − 11 Ea′ PA</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>D. M. Bikel and D. Chiang. 2000. Two statistical parsing models applied to the Chinese treebank. In Second Chinese Language Processing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bouchard-Cˆot´e</author>
<author>P Liang</author>
<author>D Klein</author>
<author>T L Griffiths</author>
</authors>
<title>A probabilistic approach to diachronic phonology.</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<marker>Bouchard-Cˆot´e, Liang, Klein, Griffiths, 2007</marker>
<rawString>A. Bouchard-Cˆot´e, P. Liang, D. Klein, and T. L. Griffiths. 2007. A probabilistic approach to diachronic phonology. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>Computational Natural Language Learning-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="15038" citStr="Buchholz and Marsi, 2006" startWordPosition="2426" endWordPosition="2429">s yields the following objective function: l(0) = EtELUL+ 1 2σ2 kOt − Opar(t)k22 + EtEL log P(st|Ot) which can be optimized using gradient methods or (MAP) EM. Here we used L-BFGS (Liu et al., 1989). This requires computation of the gradient of the observed data likelihood log P(st|Ot) which is given by: ∇ log P(st|Ot) = Etℓ|sℓ L∇ log P(st, tt|Ot)] = We ran experiments with the following languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. For all languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. Computational Natural Language Learning-X shared task on multilingual dependency parsing. In Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Burkett</author>
<author>D Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1083" citStr="Burkett and Klein, 2008" startWordPosition="152" endWordPosition="155"> multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%. 1 Introduction Learning multiple languages together should be easier than learning them separately. For example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between two languages’ parses of the same bitext (Kuhn, 2004; Burkett and Klein, 2008; Kuzman et al., 2009; Smith and Eisner, 2009; Snyder et al., 2009a). Moreover, Snyder et al. (2009b) in the context of unsupervised part-of-speech induction (and Bouchard-Cˆot´e et al. (2007) in the context of phonology) show that extending beyond two languages can provide increasing benefit. However, multitexts are only available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constra</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>D. Burkett and D. Klein. 2008. Two languages are better than one (for syntactic parsing). In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2025" citStr="Cohen and Smith (2009)" startWordPosition="294" endWordPosition="298">nly available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constraints at a parameter level, using a phylogeny-structured prior to tie together the various individual languages’ learning problems. Our joint, hierarchical prior couples model parameters for different languages in a way that respects knowledge about how the languages evolved. Aspects of this work are closely related to Cohen and Smith (2009) and Bouchard-Cˆot´e et al. (2007). Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. In their work, structurally constrained covariance in a logistic normal prior is used to couple parameters between the two languages. Our work, though also different in technical approach, differs most centrally in the extension to multiple languages and the use of a phylogeny. Bouchard-Cˆot´e et al. (2007) considers an entirely different problem, phonological reconstruction, but shares with this work both the use of a phylogenetic structure a</context>
<context position="3422" citStr="Cohen and Smith, 2009" startWordPosition="516" endWordPosition="519">d by the phylogeny: in our model it is the grammar parameters that drift (in the prior) rather than individual word forms (in the likelihood model). Specifically, we consider dependency induction in the DMV model of Klein and Manning (2004). Our data is a collection of standard dependency data sets in eight languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. Our focus is not the DMV model itself, which is well-studied, but rather the prior which couples the various languages’ parameters. While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. In our experiments, joint multilingual learning substantially outperforms independent monolingual learning. Using a limited phylogeny that 1288 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288–1297, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics only couples languages within linguistic families reduces error by 5.6% over the m</context>
<context position="6140" citStr="Cohen and Smith, 2009" startWordPosition="956" endWordPosition="959">ons that capture argument selection. First, the Bernoulli CONTINUE probability distributions PCONTINUE(c|h, dir, adj; Ot) model the fertility of a particular head type h. The outcome c E {stop, continue} is conditioned on the head type h, direction dir, and adjacency adj. If ahead type’s continue probability is low, tokens of this type will tend to generate few arguments. Second, the ATTACH multinomial probability distributions PATTACH(a|h, dir; Ot) capture attachment preferences of heads, where a and h are both token types. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold part-of-speech labels as tokens. Thus, the basic observed “word” types are West North Germanic Germanic English Dutch Danish Swedish Spanish Portuguese Slovene Chinese Figure 1: An example of a linguistically-plausible phylogenetic tree over the languages in our training data. Leaves correspond to (observed) modern languages, while internal nodes represent (unobserved) ancestral languages. actually word classes. 2.1.1 Log-Linear Parameterization The DMV’s local conditional distributions were originally given as simple multinomial distributions with one parameter per outcome. Howe</context>
<context position="29143" citStr="Cohen and Smith (2009)" startWordPosition="4659" endWordPosition="4662">aseline and the various multilingual mod1294 els are the same, both expanding upon the standard DMV by adding coarse SHARED features. These coarse features, even in a monolingual setting, improved performance slightly over the weak baseline, perhaps by encouraging consistent treatment of the different finer-grained variants of partsof-speech (Berg-Kirkpatrick et al., 2010).2 The only difference between the multilingual systems and the strong baseline is whether or not crosslanguage influence is allowed through the prior. While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. When Cohen and Smith compared their best shared logistic-normal bilingual models to monolingual counter-parts for the languages they investigate (Chinese and English), they reported a relative error reduction of 5.3%. In comparison, with the LINGUISTIC model, we saw a much larger 16.9% relative error reduction over our strong baseline for these langua</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<booktitle>In Ph.D. thesis,</booktitle>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="15831" citStr="Collins, 1999" startWordPosition="2555" endWordPosition="2556">nd provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been removed. When constructing phylogenies over the languages we made use of their linguistic classifications. English and Dutch are part of the West GerEc,h,dir,adj ec,h,dir,adj (st; Ot) · LfCONTINUE(c, h, dir, adj) − 1 Ec′ PCONTINUE(c′ |h, dir, adj; Ot) f CONTINUE(c′, h, dir, adj)] Ea,h,dir ea,h,dir(st; Ot) · LfATTACH(a, h, dir) − 11 Ea′ PATTACH(a′|h, dir; Ot) f ATTACH(a′, h, dir)] � � � � �</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-driven statistical models for natural language parsing. In Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>H. Daum´e III. 2007. Frustratingly easy domain adaptation. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6825" citStr="Eisner, 2002" startWordPosition="1051" endWordPosition="1052">word” types are West North Germanic Germanic English Dutch Danish Swedish Spanish Portuguese Slovene Chinese Figure 1: An example of a linguistically-plausible phylogenetic tree over the languages in our training data. Leaves correspond to (observed) modern languages, while internal nodes represent (unobserved) ancestral languages. actually word classes. 2.1.1 Log-Linear Parameterization The DMV’s local conditional distributions were originally given as simple multinomial distributions with one parameter per outcome. However, they can be re-parameterized to give the following log-linear form (Eisner, 2002; Bouchard-Cˆot´e et al., 2007; Berg-Kirkpatrick et al., 2010): PCONTINUE(c|h, dir, adj; θℓ) = exp ˆθℓT f CONTINUE(c, h, dir, adj)˜ Pc′ exp ˆθℓT f CONTINUE(c′, h, dir, adj)˜ PATTACH(a|h, dir; θℓ) = exp ˆθℓT f ATTACH(a, h, dir)˜ P a′ exp ˆθℓT f ATTACH(a′, h, dir)˜ The parameters are weights Ot with one weight vector per language. In the case where the vector of feature functions f has an indicator for each possible conjunction of outcome and conditions, the original multinomial distributions are recovered. We refer to these full indicator features as the set of SPECIFIC features. 2.2 Phylogenet</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>J. Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18811" citStr="Finkel and Manning (2009)" startWordPosition="3047" endWordPosition="3050">retation, but which provides very similar capacity for parameter coupling. 3.2.1 Phylogenetic Models The first phylogenetic model uses the shallow phylogeny shown in Figure 2(a), in which only languages within the same family have a shared parent node. We refer to this structure as FAMILIES. Under this prior, the learning task decouples into independent subtasks for each family, but no regularities across families can be captured. The family-level model misses the constraints between distant languages. Figure 2(b) shows another simple configuration, wherein all languages Daum´e III (2007) and Finkel and Manning (2009) consider a formally similar Gaussian hierarchy for domain adaptation. As pointed out in Finkel and Manning (2009), there is a simple equivalence between hierarchical regularization as described here and the addition of new tied features in a “flat” model with zero-meaned Gaussian regularization on all parameters. In particular, instead of parameterizing the objective in Section 2.4 in terms of multiple sets of weights, one at each node in the phylogeny (the hierarchical parameterization, described in Section 2.4), it is equivalent to parameterize this same objective in terms of a single set o</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel and C. D. Manning. 2009. Hierarchical bayesian domain adaptation. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3040" citStr="Klein and Manning (2004)" startWordPosition="457" endWordPosition="460">le languages and the use of a phylogeny. Bouchard-Cˆot´e et al. (2007) considers an entirely different problem, phonological reconstruction, but shares with this work both the use of a phylogenetic structure as well as the use of log-linear parameterization of local model components. Our work differs from theirs primarily in the task (syntax vs. phonology) and the variables governed by the phylogeny: in our model it is the grammar parameters that drift (in the prior) rather than individual word forms (in the likelihood model). Specifically, we consider dependency induction in the DMV model of Klein and Manning (2004). Our data is a collection of standard dependency data sets in eight languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. Our focus is not the DMV model itself, which is well-studied, but rather the prior which couples the various languages’ parameters. While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. In our experiments, joint multili</context>
<context position="4613" citStr="Klein and Manning (2004)" startWordPosition="698" endWordPosition="701"> reduces error by 5.6% over the monolingual baseline. Using a flat, global phylogeny gives a greater reduction, almost 10%. Finally, a more articulated phylogeny that captures both inter- and intrafamily effects gives an even larger average relative error reduction of 21.1%. 2 Model Global IndoEuropean BaltoItalic Slavic SinoTibetan Sinitic We define our model over two kinds of random variables: dependency trees and parameters. For each language E in a set L, our model will generate a collection tt of dependency trees tP. We assume that these dependency trees are generated by the DMV model of Klein and Manning (2004), which we write as tP ∼ DMV(Ot). Here, Ot is a vector of the various model parameters for language E. The prior is what couples the Ot parameter vectors across languages; it is the focus of this work. We first consider the likelihood model before moving on to the prior. 2.1 Dependency Model with Valence A dependency parse is a directed tree t over tokens in a sentence s. Each edge of the tree specifies a directed dependency from a head token to a dependent, or argument token. The DMV is a generative model for trees t, which has been widely used for dependency parse induction. The observed dat</context>
<context position="6116" citStr="Klein and Manning, 2004" startWordPosition="952" endWordPosition="955">nce and ATTACH distributions that capture argument selection. First, the Bernoulli CONTINUE probability distributions PCONTINUE(c|h, dir, adj; Ot) model the fertility of a particular head type h. The outcome c E {stop, continue} is conditioned on the head type h, direction dir, and adjacency adj. If ahead type’s continue probability is low, tokens of this type will tend to generate few arguments. Second, the ATTACH multinomial probability distributions PATTACH(a|h, dir; Ot) capture attachment preferences of heads, where a and h are both token types. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold part-of-speech labels as tokens. Thus, the basic observed “word” types are West North Germanic Germanic English Dutch Danish Swedish Spanish Portuguese Slovene Chinese Figure 1: An example of a linguistically-plausible phylogenetic tree over the languages in our training data. Leaves correspond to (observed) modern languages, while internal nodes represent (unobserved) ancestral languages. actually word classes. 2.1.1 Log-Linear Parameterization The DMV’s local conditional distributions were originally given as simple multinomial distributions with one par</context>
<context position="14398" citStr="Klein and Manning, 2004" startWordPosition="2321" endWordPosition="2324">per iteration is essentially the same whether training all languages jointly or independently. In practice, the total number of iterations was also similar. � 1 3 Experimental Setup log P(0) = 2σ2 k Ot − Opar(t) k2 + C 3.1 Data tELUL+ where C is a constant. The form of log P(0) immediately shows how parameters are penalized for being different across languages, more so for languages that are near each other in the phylogeny. The second term log P(s|0) = � log P (st|Ot) tEL is a sum of observed data likelihoods under the standard DMV models for each language, computable by dynamic programming (Klein and Manning, 2004). Together, this yields the following objective function: l(0) = EtELUL+ 1 2σ2 kOt − Opar(t)k22 + EtEL log P(st|Ot) which can be optimized using gradient methods or (MAP) EM. Here we used L-BFGS (Liu et al., 1989). This requires computation of the gradient of the observed data likelihood log P(st|Ot) which is given by: ∇ log P(st|Ot) = Etℓ|sℓ L∇ log P(st, tt|Ot)] = We ran experiments with the following languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. For all languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency par</context>
<context position="23443" citStr="Klein and Manning (2004)" startWordPosition="3755" endWordPosition="3758">ing the observed data likelihood using L-BFGS (Liu et al., 1989). Berg-Kirkpatrick et al. (2010) suggest that directly optimizing the observed data likelihood may offer improvements over the more standard expectation-maximization (EM) optimization procedure for models such as the DMV, especially when the model is parameterized using features. We stopped training after 200 iterations in all cases. This fixed stopping criterion seemed to be adequate in all experiments, but presumably there is a potential gain to be had in fine tuning. To initialize, we used the harmonic initializer presented in Klein and Manning (2004). This type of initialization is deterministic, and thus we did not perform random restarts. We found that for all models Q2 = 0.2 gave reasonable results, and we used this setting in all experiments. For most models, we found that varying Q2 in a reasonable range did not substantially affect accuracy. For some models, the directed accuracy was less flat with respect to Q2. In these less-stable cases, there seemed to be an interaction between the variance and the choice between head conventions. For example, for some settings of Q2, but not others, the model would learn that determiners head n</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kuhn</author>
</authors>
<title>Experiments in parallel-text based grammar induction.</title>
<date>2004</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1058" citStr="Kuhn, 2004" startWordPosition="150" endWordPosition="151">ction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%. 1 Introduction Learning multiple languages together should be easier than learning them separately. For example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between two languages’ parses of the same bitext (Kuhn, 2004; Burkett and Klein, 2008; Kuzman et al., 2009; Smith and Eisner, 2009; Snyder et al., 2009a). Moreover, Snyder et al. (2009b) in the context of unsupervised part-of-speech induction (and Bouchard-Cˆot´e et al. (2007) in the context of phonology) show that extending beyond two languages can provide increasing benefit. However, multitexts are only available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we cap</context>
</contexts>
<marker>Kuhn, 2004</marker>
<rawString>J. Kuhn. 2004. Experiments in parallel-text based grammar induction. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kuzman</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics/International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="1104" citStr="Kuzman et al., 2009" startWordPosition="156" endWordPosition="160">antially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%. 1 Introduction Learning multiple languages together should be easier than learning them separately. For example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between two languages’ parses of the same bitext (Kuhn, 2004; Burkett and Klein, 2008; Kuzman et al., 2009; Smith and Eisner, 2009; Snyder et al., 2009a). Moreover, Snyder et al. (2009b) in the context of unsupervised part-of-speech induction (and Bouchard-Cˆot´e et al. (2007) in the context of phonology) show that extending beyond two languages can provide increasing benefit. However, multitexts are only available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constraints at a parameter l</context>
</contexts>
<marker>Kuzman, Gillenwater, Taskar, 2009</marker>
<rawString>G. Kuzman, J. Gillenwater, and B. Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Association for Computational Linguistics/International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
<author>M I Jordan</author>
</authors>
<title>Agreement-based learning.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="32493" citStr="Liang et al., 2008" startWordPosition="5189" endWordPosition="5192">to head sentences as often as verbs are. Spanish has some confusion about conjunctions, hypothesizing that verbs often attach to conjunctions, and conjunctions frequently head sentences. More subtly, the monolingual analyses are inconsistent in the way they head prepositional phrases. In the monolingual Portuguese hypotheses, prepositions modify nouns more often than nouns modify prepositions. In English, nouns modify prepositions, and prepositions modify verbs. Both the Dutch and Spanish models are ambivalent about the attachment of prepositions. As has often been observed in other contexts (Liang et al., 2008), promoting agreement can improve accuracy in unsupervised learning. Not only are the analyses proposed by the LINGUISTIC model more consistent, they are also more in accordance with the gold analyses. Under the LINGUISTIC model, Dutch now attaches pronouns to verbs, and thus looks more like English, its sister in the phylogenetic tree. The LINGUISTIC model has also chosen consistent analyses for prepositional phrases and noun phrases, calling prepositions and nouns the heads of each, respectively. The problem of conjunctions heading Spanish sentences has also been corrected. Figure 3(b) shows</context>
</contexts>
<marker>Liang, Klein, Jordan, 2008</marker>
<rawString>P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-based learning. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
<author>C Dong</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming.</booktitle>
<contexts>
<context position="14611" citStr="Liu et al., 1989" startWordPosition="2358" endWordPosition="2361">.1 Data tELUL+ where C is a constant. The form of log P(0) immediately shows how parameters are penalized for being different across languages, more so for languages that are near each other in the phylogeny. The second term log P(s|0) = � log P (st|Ot) tEL is a sum of observed data likelihoods under the standard DMV models for each language, computable by dynamic programming (Klein and Manning, 2004). Together, this yields the following objective function: l(0) = EtELUL+ 1 2σ2 kOt − Opar(t)k22 + EtEL log P(st|Ot) which can be optimized using gradient methods or (MAP) EM. Here we used L-BFGS (Liu et al., 1989). This requires computation of the gradient of the observed data likelihood log P(st|Ot) which is given by: ∇ log P(st|Ot) = Etℓ|sℓ L∇ log P(st, tt|Ot)] = We ran experiments with the following languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. For all languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tag</context>
<context position="22883" citStr="Liu et al., 1989" startWordPosition="3667" endWordPosition="3670">son with the non-phylogenetic ALLPAIRS model, which has no equivalent hierarchical parameterization. In practice, optimizing with the hierarchical parameterization also seemed to underperform.1 1We noticed that the weights of features shared across languages had larger magnitude early in the optimization procedure when using the flat parameterization compared to using the hierarchical parameterization, perhaps indicating that cross-lingual influences had a larger effect on learning in its initial stages. All models were trained by directly optimizing the observed data likelihood using L-BFGS (Liu et al., 1989). Berg-Kirkpatrick et al. (2010) suggest that directly optimizing the observed data likelihood may offer improvements over the more standard expectation-maximization (EM) optimization procedure for models such as the DMV, especially when the model is parameterized using features. We stopped training after 200 iterations in all cases. This fixed stopping criterion seemed to be adequate in all experiments, but presumably there is a potential gain to be had in fine tuning. To initialize, we used the harmonic initializer presented in Klein and Manning (2004). This type of initialization is determi</context>
</contexts>
<marker>Liu, Nocedal, Dong, 1989</marker>
<rawString>D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the penn treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="15445" citStr="Marcus et al., 1993" startWordPosition="2491" endWordPosition="2494">utch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. For all languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been remove</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the penn treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Salakhutdinov</author>
<author>S Roweis</author>
<author>Z Ghahramani</author>
</authors>
<title>Optimization with EM and expectationconjugate-gradient.</title>
<date>2003</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="13284" citStr="Salakhutdinov et al., 2003" startWordPosition="2125" endWordPosition="2128">prior couples parameters across languages, this learning problem requires parameters for all languages be estimated jointly. We seek to find 0 = (Ot : E ∈ L ∪ L+) which optimizes log P(0|s), where s aggregates the observed leaves of all the dependency trees in all the languages. This can be written as log P(0) + log P(s|0) − log P(s) The third term is a constant and can be ignored. The first term can be written as The expected gradient of the log joint likelihood of sentences and parses is equal to the gradient of the log marginal likelihood of just sentences, or the observed data likelihood (Salakhutdinov et al., 2003). ea,h,dir(st; Ot) is the expected count of the number of times head h is attached to a in direction dir given the observed sentences st and DMV parameters Ot. ec,h,dir,adj(st; Ot) is defined similarly. Note that these are the same expected counts required to perform EM on the DMV, and are computable by dynamic programming. The computation time is dominated by the computation of each sentence’s posterior expected counts, which are independent given the parameters, so the time required per iteration is essentially the same whether training all languages jointly or independently. In practice, th</context>
</contexts>
<marker>Salakhutdinov, Roweis, Ghahramani, 2003</marker>
<rawString>R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003. Optimization with EM and expectationconjugate-gradient. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1128" citStr="Smith and Eisner, 2009" startWordPosition="161" endWordPosition="164">independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%. 1 Introduction Learning multiple languages together should be easier than learning them separately. For example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between two languages’ parses of the same bitext (Kuhn, 2004; Burkett and Klein, 2008; Kuzman et al., 2009; Smith and Eisner, 2009; Snyder et al., 2009a). Moreover, Snyder et al. (2009b) in the context of unsupervised part-of-speech induction (and Bouchard-Cˆot´e et al. (2007) in the context of phonology) show that extending beyond two languages can provide increasing benefit. However, multitexts are only available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constraints at a parameter level, using a phylogeny-</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>D. A. Smith and J. Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>T Naseem</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics/International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="1149" citStr="Snyder et al., 2009" startWordPosition="165" endWordPosition="169">th larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%. 1 Introduction Learning multiple languages together should be easier than learning them separately. For example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between two languages’ parses of the same bitext (Kuhn, 2004; Burkett and Klein, 2008; Kuzman et al., 2009; Smith and Eisner, 2009; Snyder et al., 2009a). Moreover, Snyder et al. (2009b) in the context of unsupervised part-of-speech induction (and Bouchard-Cˆot´e et al. (2007) in the context of phonology) show that extending beyond two languages can provide increasing benefit. However, multitexts are only available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constraints at a parameter level, using a phylogeny-structured prior to t</context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>B. Snyder, T. Naseem, and R. Barzilay. 2009a. Unsupervised multilingual grammar induction. In Association for Computational Linguistics/International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>T Naseem</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Adding more languages improves unsupervised multilingual part-of-speech tagging: A Bayesian non-parametric approach.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1149" citStr="Snyder et al., 2009" startWordPosition="165" endWordPosition="169">th larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%. 1 Introduction Learning multiple languages together should be easier than learning them separately. For example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between two languages’ parses of the same bitext (Kuhn, 2004; Burkett and Klein, 2008; Kuzman et al., 2009; Smith and Eisner, 2009; Snyder et al., 2009a). Moreover, Snyder et al. (2009b) in the context of unsupervised part-of-speech induction (and Bouchard-Cˆot´e et al. (2007) in the context of phonology) show that extending beyond two languages can provide increasing benefit. However, multitexts are only available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constraints at a parameter level, using a phylogeny-structured prior to t</context>
</contexts>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2009</marker>
<rawString>B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay. 2009b. Adding more languages improves unsupervised multilingual part-of-speech tagging: A Bayesian non-parametric approach. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>Building a large-scale annotated Chinese corpus.</title>
<date>2002</date>
<booktitle>In International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="15513" citStr="Xue et al., 2002" startWordPosition="2504" endWordPosition="2507">ll languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been removed. When constructing phylogenies over the languages we made use of t</context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>N. Xue, F-D Chiou, and M. Palmer. 2002. Building a large-scale annotated Chinese corpus. In International Conference on Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>