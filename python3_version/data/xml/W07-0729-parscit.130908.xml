<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018213">
<title confidence="0.946055">
The ‘noisier channel’: translation from morphologically complex languages
</title>
<author confidence="0.986255">
Christopher I Dyer
</author>
<affiliation confidence="0.99728">
Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.972025">
College Park, MD 20742
</address>
<email confidence="0.998977">
redpony@umd.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999635111111111">
This paper presents a new paradigm for
translation from inflectionally rich lan-
guages that was used in the University
of Maryland statistical machine transla-
tion system for the WMT07 Shared Task.
The system is based on a hierarchical
phrase-based decoder that has been aug-
mented to translate ambiguous input given
in the form of a confusion network (CN),
a weighted finite state representation of a
set of strings. By treating morphologi-
cally derived forms of the input sequence
as possible, albeit more “costly” paths that
the decoder may select, we find that sig-
nificant gains (10% BLEU relative) can
be attained when translating from Czech,
a language with considerable inflectional
complexity, into English.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971666666667">
Morphological analysis occupies a tenuous position
statistical machine translation systems. Conven-
tional translation models are constructed with no
consideration of the relationships between lexical
items and instead treat different inflected (observed)
forms of identical underlying lemmas as completely
independent of one another. While the variously
inflected forms of one lemma may express differ-
ences in meaning that are crucial to correct transla-
tion, the strict independence assumptions normally
made exacerbate data sparseness and lead to poorly
estimated models and suboptimal translations. A va-
riety of solutions have been proposed: Niessen and
Ney (2001) use of morphological information to im-
prove word reordering before training and after de-
coding. Goldwater and McClosky (2005) show im-
provements in a Czech to English word-based trans-
lation system when inflectional endings are simpli-
fied or removed entirely. Their method can, how-
ever, actually harm performance since the discarded
morphemes carry some information that may have
bearing on the translation (cf. Section 3.3). To avoid
this pitfall, Talbot and Osborne (2006) use a data-
driven approach to cluster source-language morpho-
logical variants that are meaningless in the target
language, and Yang and Kirchhoff (2006) propose
the use of a backoff model that uses morphologically
reduced forms only when the translation of the sur-
face form is unavailable. All of these approaches
have in common that the decisions about whether to
use morphological information are made in either a
pre- or post-processing step.
Recent work in spoken language translation sug-
gests that allowing decisions about the use of mor-
phological information to be made along side other
translation decisions (i.e., inside the decoder), will
yield better results. At least as early as Ney (1999),
it has been shown that when translating the out-
put from automatic speech regonition (ASR) sys-
tems, the quality can be improved by considering
multiple (rather than only a single best) transcrip-
tion hypothesis. Although state-of-the-art statistical
machine translation systems have conventionally as-
sumed unambiguous input; recent work has demon-
strated the possibility of efficient decoding of am-
</bodyText>
<page confidence="0.966588">
207
</page>
<bodyText confidence="0.9179045">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 207–211,
Prague, June 2007. c�2007 Association for Computational Linguistics
biguous input (represented as confusion networks or
word lattices) within standard phrase-based models
(Bertoldi et al., to appear 2007) as well as hierarchi-
cal phrase-based models (Dyer and Resnik, 2007).
These hybrid decoders search for the target language
sentence e� that maximizes the following probability,
where !9(o) represents the set of weighted transcrip-
tion hypotheses produced by an ASR decoder:
</bodyText>
<equation confidence="0.9934435">
e� = arg max max
e f&apos;Eg(o)
</equation>
<bodyText confidence="0.999797333333333">
The conditional probability p(e, f|o) that is maxi-
mized is modeled directly using a log-linear model
(Och and Ney, 2002), whose parameters can be
tuned to optimize either the probability of a devel-
opment set or some other objective (such as max-
imizing BLEU). In addition to the standard trans-
lation model features, the ASR system’s posterior
probability is another feature. The decoder thus
finds a translation hypothesis e� that maximizes the
joint translation/transcription probability, which is
not necessarily the one that corresponds to the best
single transcription hypothesis.
</bodyText>
<sectionHeader confidence="0.953236" genericHeader="method">
2 Noisier channel translation
</sectionHeader>
<bodyText confidence="0.999979913043478">
We extend the concept of translating from an am-
biguous set of source hypotheses to the domain of
text translation by redefining !9(·) to be a set of
weighted sentences derived by applying morpholog-
ical transformations (such as stemming, compound
splitting, clitic splitting, etc.) to a given source sen-
tence f. This model for translation extends the usual
noisy channel metaphor by suggesting that an “En-
glish” source signal is first distorted into a morpho-
logically neutral “French” and then morphological
processes represent a further distortion of the signal,
which can be modeled independently. Whereas in
the context of an ASR transcription hypothesis, !9(·)
assigns a posterior probability to each sentence, we
redefine of this value to be a backoff penalty. This
can be intuitively thought of as a measure of the
“distance” that a given morphological alternative is
from the observed input sentence.
The remainder of the paper is structured as fol-
lows. In Section 2, we describe the basic hierarchi-
cal translation model. In Section 3, we describe the
data and tools used and present experimental results
for Czech-English. Section 4 concludes.
</bodyText>
<sectionHeader confidence="0.94678" genericHeader="method">
3 Hierarchical phrase-based decoding
</sectionHeader>
<bodyText confidence="0.999778857142857">
Chiang (2005; to appear 2007) introduced hierar-
chical phrase-based translation models, which are
formally based on synchronous context-free gram-
mars. These generalize phrase-based translation
models by allowing phrase pairs to contain vari-
ables. Like phrase correspondences, the correspond-
ing synchronous grammar rules can be learned auto-
matically from aligned, but otherwise unannotated,
training bitext. For details about the extraction algo-
rithm, refer to Chiang (to appear 2007).
The rules of the induced grammar consist of pairs
of strings of terminals and non-terminals in the
source and target languages, as well one-to-one cor-
respondences between non-terminals on the source
and target side of each pair (shown as indexes in
the examples below). Thus they encapsulate not
only meaning translation (of possibly discontinuous
spans), but also typical reordering patterns. For ex-
ample, the following two rules were extracted from
the Spanish H English segment of the Europarl cor-
pus (Koehn, 2003):
</bodyText>
<equation confidence="0.999898">
X —* (la X1 de X2, X2 ’s X1) (2)
X —* (el X1 verde, the green X1) (3)
</equation>
<bodyText confidence="0.9995687">
Rule (2) expresses the fact that possessors can
be expressed prior to the possessed object in En-
glish but must follow in Spanish. Rule (3) shows
that the adjective verde follows the modified expres-
sion in Spanish whereas the corresponding English
lexical item green precedes what it modifies. Al-
though the rules given here correspond to syntactic
constituents, this is accidental. The grammars ex-
tracted make use of only a single non-terminal cate-
gory and variables are posited that may or may not
correspond to linguistically meaningful spans.
Given a synchronous grammar G, the translation
process is equivalent to parsing an input sentence
with the source side of G and thereby inducing a
target sentence. The decoder we used is based on
the CKY+ algorithm, which permits the parsing of
rules that are not in Chomsky normal form (Chep-
palier and Rajman, 1998) and that has been adapted
to admit input that is in the form of a confusion net-
work (Dyer and Resnik, 2007). To incorporate target
</bodyText>
<equation confidence="0.89657">
P(e, f,|o) (1)
</equation>
<page confidence="0.922104">
208
</page>
<table confidence="0.999961875">
Language Tokens Types Singletons
Czech surface 1.2M 88037 42341
Czech lemmas 1.2M 34227 13129
Czech truncated 1.2M 37263 13093
English 1.4M 31221 10508
Spanish 1.4M 47852 20740
French 1.2M 38241 15264
German 1.4M 75885 39222
</table>
<tableCaption confidence="0.895009333333333">
Table 1: Corpus statistics, by language, for the
WMT07 training subset of the News Commentary
corpus.
</tableCaption>
<bodyText confidence="0.999402666666667">
language model probabilities into the model, which
is important for translation quality, the grammar is
intersected during decoding with an m-gram lan-
guage model. This process significantly increases
the effective size of the grammar, and so a beam-
search heuristic called cube pruning is used, which
has been experimentally determined to be nearly as
effective as an exhaustive search but far more effi-
cient.
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9852868125">
We carried out a series of experiments using differ-
ent strategies for making use of morphological in-
formation on the News Commentary Czech-English
data set provided for the WMT07 Shared Task.
Czech was selected because it exhibits a rich inflec-
tional morphology, but its other morphological pro-
cesses (such as compounding and cliticization) that
affect multiple lemmas are relatively limited. This
has the advantage that a morphologically simpli-
fied (i.e., lemmatized) form of a Czech sentence has
the same number of tokens as the surface form has
words, which makes representing !9(f) as a confu-
sion network relatively straightforward. The relative
morphological complexity of Czech, as well as the
potential benefits that can be realized by stemming,
can be inferred from the corpus statistics given in
</bodyText>
<tableCaption confidence="0.570594">
Table 1.
</tableCaption>
<subsectionHeader confidence="0.993181">
4.1 Technical details
</subsectionHeader>
<bodyText confidence="0.999975571428572">
A trigram English language model with modified
Kneser-Ney smoothing (Kneser and Ney, 1995) was
trained using the SRI Language Modeling Toolkit
(Stolcke, 2002) on the English side of the News
Commentary corpus as well as portions of the
GigaWord v2 English Corpus and was used for
all experiments. Recasing was carried out using
SRI’s disambig tool using a trigram language
model. The feature set used included bidirectional
translation probabilities for rules, lexical transla-
tion probabilities, a target language model proba-
bility, and count features for target words, num-
ber of non-terminal symbols used, and finally the
number of morphologically simplified forms se-
lected in the CN. Feature weight tuning was carried
out using minimum error rate training, maximizing
BLEU scores on a held-out development set (Och,
2003). Translation scores are reported using case-
insensitive BLEU (Papineni et al., 2002) with a sin-
gle reference translation. Significance testing was
done using bootstrap resampling (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.997988">
4.2 Data preparation and training
</subsectionHeader>
<bodyText confidence="0.999889166666667">
We used a Czech morphological analyzer by Hajiˇc
and Hladk´a (1998) to extract the lemmas from the
Czech portions of the training, development, and
test data (the Czech-English portion of the News
Commentary corpus distributed as as part of the
WMT07 Shared Task). Data sets consisting of trun-
cated forms were also generated; using a length limit
of 6, which Goldwater and McClosky (2005) exper-
imentally determined to be optimal for translation
performance. We refer to the three data sets and the
models derived from them as SURFACE, LEMMA,
and TRUNC. Czech—*English grammars were ex-
tracted from the three training sets using the meth-
ods described in Chiang (to appear 2007). Two ad-
ditional grammars were created by combining the
rules from the SURFACE grammar and the LEMMA
or TRUNC grammar and renormalizing the condi-
tional probabilities, yielding the combined models
SURFACE+LEMMA and SURFACE+TRUNC.
Confusion networks for the development and test
sets were constructed by providing a single back-
off form at each position in the sentence where the
lemmatizer or truncation process yielded a different
word form. The backoff form was assigned a cost of
1 and the surface form a cost of 0. Numbers and
punctuation were not truncated. A “backoff” set,
corresponding approximately to the method of Yang
and Kirchhoff (2006) was generated by lemmatiz-
ing only unknown words. Figure 1 shows a sample
surface+lemma CN from the test set.
</bodyText>
<page confidence="0.995698">
209
</page>
<table confidence="0.829910538461539">
1 2 3 4 5 6 7 8 9 10 11 12
z americk´eho bˇrehu atlantiku se vesker´a takov´a od˚uvodnˇen´ı jev´ı jako naprosto bizarn´ı
americk´y bˇreh atlantik s takov´y jevit
Figure 1: Example confusion network generated by lemmatizing the source sentence to generate alternates at
each position in the sentence. The upper element in each column is the surface form and the lower element,
when present, is the lemma.
Input BLEU Sample translation
SURFACE 22.74 From the US side of the Atlantic all such od˚uvodnˇeniappears to be a totally bizarre.
LEMMA 22.50 From the side of the Atlantic with any such justification seem completely bizarre.
TRUNC (1=6) 22.07 From the bank of the Atlantic, all such justification appears to be totally bizarre.
backoff (SURFACE+LEMMA) 23.94 From the US bank of the Atlantic, all such justification appears to be totally bizarre.
CN (SURFACE+LEMMA) 25.01 From the US side of the Atlantic all such justification appears to be a totally bizarre.
CN (SURFACE+TRUNC) 23.57 From the US Atlantic any such justification appears to be a totally bizarre.
</table>
<tableCaption confidence="0.992748">
Table 2: Czech-English results on WMT07 Shared Task DEVTEST set. The sample translations are transla-
tions of the sentence shown in Figure 1.
</tableCaption>
<subsectionHeader confidence="0.997999">
4.3 Experimental results
</subsectionHeader>
<bodyText confidence="0.999988647058824">
Table 2 summarizes the performance of the six
Czech—*English models on the WMT07 Shared
Task development set. The basic SURFACE model
tends to outperform both the LEMMA and TRUNC
models, although the difference is only marginally
significant. This suggests that the Goldwater and
McClosky (2005) results are highly dependent on
the kind of translation model and quantity of data.
The backoff model, a slightly modified version
of the method proposed by Yang and Kirchhoff
(2006),1 does significantly better than the baseline
(p &lt; .05). However, the joint (SURFACE+LEMMA)
model outperforms both surface and backoff base-
lines (p &lt; .01 and p &lt; .05, respectively). The SUR-
FACE+TRUNC model is an improvement over the
SURFACE model, but it performances significantly
worse than the SURFACE+LEMMA model.
</bodyText>
<sectionHeader confidence="0.995189" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.991077518518519">
We presented a novel model-driven method for us-
ing morphologically reduced forms when translat-
ing from a language with complex inflectional mor-
1Our backoff model has two primary differences from model
described by Y&amp;K. The first is that our model effectively cre-
ates backoff forms for every surface string, whereas Y&amp;K do
this only for forms that are not found in the surface string. This
means that in our model, the probabilities of a larger number
of surface rules have been altered by backoff discounting than
would be the case in the more conservative model. Second, the
joint model we used has the benefit of using morphologically
simpler forms to improve alignment.
phology. By allowing the decoder to select among
the surface form of a word or phrase and variants
of morphological alternatives on the source side,
we outperform baselines where hard decisions about
what form to use are made in advance of decod-
ing, as has typically been done in systems that make
use of morphological information. This “decoder-
guided” incorporation of morphology was enabled
by adopting techniques for translating from ambigu-
ous sources that were developed to address problems
specific to spoken language translation. Although
the results presented here were obtained using a hi-
erarchical phrase-based system, the model general-
izes to any system where the decoder can accept a
weighted word graph as its input.
</bodyText>
<sectionHeader confidence="0.996491" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999857142857143">
The author would like to thank David Chiang for
making the Hiero decoder sources available to us
and Daniel Zeman for his assistance in the prepara-
tion of the Czech data. This work was generously
supported by the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-2-0001.
</bodyText>
<sectionHeader confidence="0.998069" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9033755">
N. Bertoldi, R. Zens, and M. Federico. to appear 2007. Speech
translation by confusion network decoding. In 32nd Inter-
national Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), Honolulu, Hawaii, April.
</reference>
<page confidence="0.997589">
210
</page>
<note confidence="0.715559142857143">
J. Cheppalier and M. Rajman. 1998. A generalized CYK
algorithm for parsing stochastic CFG. In Proceedings
of the Workshop on Tabulation in Parsing and Deduction
(TAPD98), pages 133–137, Paris, France.
M. Yang and K. Kirchhoff. 2006. Phrase-based backoff mod-
els for machine translation of highly inflected languages. In
Proceedings of the EACL 2006, pages 41–48.
</note>
<reference confidence="0.998725770833333">
D. Chiang. to appear 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
C. Dyer and P. Resnik. 2007. Word Lattice Parsing for Sta-
tistical Machine Translation. Technical report, University of
Maryland, College Park, April.
S. Goldwater and D. McClosky. 2005. Improving statistical
mt through morphological analysis. In Proceedings of Hu-
man Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing, pages
676–683, Vancouver, British Columbia.
J. Hajiˇc and B. Hladk´a. 1998. Tagging inflective languages:
Prediction of morphological categories for a rich, structured
tagset. In Proceedings of the COLING-ACL Conference,
pages 483–490.
R. Kneser and H. Ney. 1995. Improved backing-off for m-
gram language modeling. In Proceedings of IEEE Interna-
tion Conference on Acoustics, Speech, and Signal Process-
ing, pages 181–184.
P. Koehn. 2003. Europarl: A multilingual corpus for evaluation
of machine translation. Draft, unpublished.
P. Koehn. 2004. Statistical signficiance tests for machine
translation evluation. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP), pages 388–395.
H. Ney. 1999. Speech translation: Coupling of recognition
and translation. In IEEE International Conference on Acous-
tic, Speech and Signal Processing, pages 517–520, Phoenix,
AR, March.
S. Niessen and H. Ney. 2001. Morpho-syntactic analysis for
reordering in statistical machine translation. In Proceedings
of MT Summit VIII, Santiago de Compostela, Galicia, Spain.
F. Och and H. Ney. 2002. Discriminitive training and maxi-
mum entropy models for statistical machine translation. In
Proceedings of the 40th Annual Meeting of the ACL, pages
295–302.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics (ACL),
pages 160–167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the ACL, pages
311–318.
A. Stolcke. 2002. SRILM – an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
D. Talbot and M. Osborne. 2006. Modelling lexical redun-
dancy for machine translation. In Proceedings of ACL 2006,
Sydney, Australia.
</reference>
<page confidence="0.998793">
211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942858">
<title confidence="0.999475">The ‘noisier channel’: translation from morphologically complex languages</title>
<author confidence="0.999632">I Christopher</author>
<affiliation confidence="0.999748">Department of University of</affiliation>
<address confidence="0.999777">College Park, MD 20742</address>
<email confidence="0.999847">redpony@umd.edu</email>
<abstract confidence="0.996391210526316">This paper presents a new paradigm for translation from inflectionally rich languages that was used in the University of Maryland statistical machine translation system for the WMT07 Shared Task. The system is based on a hierarchical phrase-based decoder that has been augmented to translate ambiguous input given the form of a network a weighted finite state representation of a set of strings. By treating morphologically derived forms of the input sequence as possible, albeit more “costly” paths that the decoder may select, we find that significant gains (10% BLEU relative) can be attained when translating from Czech, a language with considerable inflectional complexity, into English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>R Zens</author>
<author>M Federico</author>
</authors>
<title>to appear 2007. Speech translation by confusion network decoding.</title>
<date></date>
<booktitle>In 32nd International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<location>Honolulu, Hawaii,</location>
<marker>Bertoldi, Zens, Federico, </marker>
<rawString>N. Bertoldi, R. Zens, and M. Federico. to appear 2007. Speech translation by confusion network decoding. In 32nd International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Honolulu, Hawaii, April.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Chiang</author>
</authors>
<title>to appear 2007. Hierarchical phrase-based translation.</title>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Chiang, </marker>
<rawString>D. Chiang. to appear 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>P Resnik</author>
</authors>
<title>Word Lattice Parsing for Statistical Machine Translation.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>University of Maryland, College Park,</institution>
<contexts>
<context position="3514" citStr="Dyer and Resnik, 2007" startWordPosition="520" endWordPosition="523">dering multiple (rather than only a single best) transcription hypothesis. Although state-of-the-art statistical machine translation systems have conventionally assumed unambiguous input; recent work has demonstrated the possibility of efficient decoding of am207 Proceedings of the Second Workshop on Statistical Machine Translation, pages 207–211, Prague, June 2007. c�2007 Association for Computational Linguistics biguous input (represented as confusion networks or word lattices) within standard phrase-based models (Bertoldi et al., to appear 2007) as well as hierarchical phrase-based models (Dyer and Resnik, 2007). These hybrid decoders search for the target language sentence e� that maximizes the following probability, where !9(o) represents the set of weighted transcription hypotheses produced by an ASR decoder: e� = arg max max e f&apos;Eg(o) The conditional probability p(e, f|o) that is maximized is modeled directly using a log-linear model (Och and Ney, 2002), whose parameters can be tuned to optimize either the probability of a development set or some other objective (such as maximizing BLEU). In addition to the standard translation model features, the ASR system’s posterior probability is another fea</context>
<context position="7590" citStr="Dyer and Resnik, 2007" startWordPosition="1171" endWordPosition="1174">constituents, this is accidental. The grammars extracted make use of only a single non-terminal category and variables are posited that may or may not correspond to linguistically meaningful spans. Given a synchronous grammar G, the translation process is equivalent to parsing an input sentence with the source side of G and thereby inducing a target sentence. The decoder we used is based on the CKY+ algorithm, which permits the parsing of rules that are not in Chomsky normal form (Cheppalier and Rajman, 1998) and that has been adapted to admit input that is in the form of a confusion network (Dyer and Resnik, 2007). To incorporate target P(e, f,|o) (1) 208 Language Tokens Types Singletons Czech surface 1.2M 88037 42341 Czech lemmas 1.2M 34227 13129 Czech truncated 1.2M 37263 13093 English 1.4M 31221 10508 Spanish 1.4M 47852 20740 French 1.2M 38241 15264 German 1.4M 75885 39222 Table 1: Corpus statistics, by language, for the WMT07 training subset of the News Commentary corpus. language model probabilities into the model, which is important for translation quality, the grammar is intersected during decoding with an m-gram language model. This process significantly increases the effective size of the gram</context>
</contexts>
<marker>Dyer, Resnik, 2007</marker>
<rawString>C. Dyer and P. Resnik. 2007. Word Lattice Parsing for Statistical Machine Translation. Technical report, University of Maryland, College Park, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>D McClosky</author>
</authors>
<title>Improving statistical mt through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>676--683</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="1708" citStr="Goldwater and McClosky (2005)" startWordPosition="246" endWordPosition="249">of the relationships between lexical items and instead treat different inflected (observed) forms of identical underlying lemmas as completely independent of one another. While the variously inflected forms of one lemma may express differences in meaning that are crucial to correct translation, the strict independence assumptions normally made exacerbate data sparseness and lead to poorly estimated models and suboptimal translations. A variety of solutions have been proposed: Niessen and Ney (2001) use of morphological information to improve word reordering before training and after decoding. Goldwater and McClosky (2005) show improvements in a Czech to English word-based translation system when inflectional endings are simplified or removed entirely. Their method can, however, actually harm performance since the discarded morphemes carry some information that may have bearing on the translation (cf. Section 3.3). To avoid this pitfall, Talbot and Osborne (2006) use a datadriven approach to cluster source-language morphological variants that are meaningless in the target language, and Yang and Kirchhoff (2006) propose the use of a backoff model that uses morphologically reduced forms only when the translation </context>
<context position="10658" citStr="Goldwater and McClosky (2005)" startWordPosition="1649" endWordPosition="1652">t (Och, 2003). Translation scores are reported using caseinsensitive BLEU (Papineni et al., 2002) with a single reference translation. Significance testing was done using bootstrap resampling (Koehn, 2004). 4.2 Data preparation and training We used a Czech morphological analyzer by Hajiˇc and Hladk´a (1998) to extract the lemmas from the Czech portions of the training, development, and test data (the Czech-English portion of the News Commentary corpus distributed as as part of the WMT07 Shared Task). Data sets consisting of truncated forms were also generated; using a length limit of 6, which Goldwater and McClosky (2005) experimentally determined to be optimal for translation performance. We refer to the three data sets and the models derived from them as SURFACE, LEMMA, and TRUNC. Czech—*English grammars were extracted from the three training sets using the methods described in Chiang (to appear 2007). Two additional grammars were created by combining the rules from the SURFACE grammar and the LEMMA or TRUNC grammar and renormalizing the conditional probabilities, yielding the combined models SURFACE+LEMMA and SURFACE+TRUNC. Confusion networks for the development and test sets were constructed by providing a</context>
<context position="13235" citStr="Goldwater and McClosky (2005)" startWordPosition="2065" endWordPosition="2068"> Atlantic all such justification appears to be a totally bizarre. CN (SURFACE+TRUNC) 23.57 From the US Atlantic any such justification appears to be a totally bizarre. Table 2: Czech-English results on WMT07 Shared Task DEVTEST set. The sample translations are translations of the sentence shown in Figure 1. 4.3 Experimental results Table 2 summarizes the performance of the six Czech—*English models on the WMT07 Shared Task development set. The basic SURFACE model tends to outperform both the LEMMA and TRUNC models, although the difference is only marginally significant. This suggests that the Goldwater and McClosky (2005) results are highly dependent on the kind of translation model and quantity of data. The backoff model, a slightly modified version of the method proposed by Yang and Kirchhoff (2006),1 does significantly better than the baseline (p &lt; .05). However, the joint (SURFACE+LEMMA) model outperforms both surface and backoff baselines (p &lt; .01 and p &lt; .05, respectively). The SURFACE+TRUNC model is an improvement over the SURFACE model, but it performances significantly worse than the SURFACE+LEMMA model. 5 Conclusion We presented a novel model-driven method for using morphologically reduced forms when</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>S. Goldwater and D. McClosky. 2005. Improving statistical mt through morphological analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 676–683, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>B Hladk´a</author>
</authors>
<title>Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL Conference,</booktitle>
<pages>483--490</pages>
<marker>Hajiˇc, Hladk´a, 1998</marker>
<rawString>J. Hajiˇc and B. Hladk´a. 1998. Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset. In Proceedings of the COLING-ACL Conference, pages 483–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for mgram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of IEEE Internation Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="9309" citStr="Kneser and Ney, 1995" startWordPosition="1439" endWordPosition="1442">h as compounding and cliticization) that affect multiple lemmas are relatively limited. This has the advantage that a morphologically simplified (i.e., lemmatized) form of a Czech sentence has the same number of tokens as the surface form has words, which makes representing !9(f) as a confusion network relatively straightforward. The relative morphological complexity of Czech, as well as the potential benefits that can be realized by stemming, can be inferred from the corpus statistics given in Table 1. 4.1 Technical details A trigram English language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) was trained using the SRI Language Modeling Toolkit (Stolcke, 2002) on the English side of the News Commentary corpus as well as portions of the GigaWord v2 English Corpus and was used for all experiments. Recasing was carried out using SRI’s disambig tool using a trigram language model. The feature set used included bidirectional translation probabilities for rules, lexical translation probabilities, a target language model probability, and count features for target words, number of non-terminal symbols used, and finally the number of morphologically simplified forms selected in the CN. Feat</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for mgram language modeling. In Proceedings of IEEE Internation Conference on Acoustics, Speech, and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2003</date>
<tech>Draft, unpublished.</tech>
<contexts>
<context position="6549" citStr="Koehn, 2003" startWordPosition="989" endWordPosition="990">training bitext. For details about the extraction algorithm, refer to Chiang (to appear 2007). The rules of the induced grammar consist of pairs of strings of terminals and non-terminals in the source and target languages, as well one-to-one correspondences between non-terminals on the source and target side of each pair (shown as indexes in the examples below). Thus they encapsulate not only meaning translation (of possibly discontinuous spans), but also typical reordering patterns. For example, the following two rules were extracted from the Spanish H English segment of the Europarl corpus (Koehn, 2003): X —* (la X1 de X2, X2 ’s X1) (2) X —* (el X1 verde, the green X1) (3) Rule (2) expresses the fact that possessors can be expressed prior to the possessed object in English but must follow in Spanish. Rule (3) shows that the adjective verde follows the modified expression in Spanish whereas the corresponding English lexical item green precedes what it modifies. Although the rules given here correspond to syntactic constituents, this is accidental. The grammars extracted make use of only a single non-terminal category and variables are posited that may or may not correspond to linguistically m</context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>P. Koehn. 2003. Europarl: A multilingual corpus for evaluation of machine translation. Draft, unpublished.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical signficiance tests for machine translation evluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<contexts>
<context position="10234" citStr="Koehn, 2004" startWordPosition="1582" endWordPosition="1583">cluded bidirectional translation probabilities for rules, lexical translation probabilities, a target language model probability, and count features for target words, number of non-terminal symbols used, and finally the number of morphologically simplified forms selected in the CN. Feature weight tuning was carried out using minimum error rate training, maximizing BLEU scores on a held-out development set (Och, 2003). Translation scores are reported using caseinsensitive BLEU (Papineni et al., 2002) with a single reference translation. Significance testing was done using bootstrap resampling (Koehn, 2004). 4.2 Data preparation and training We used a Czech morphological analyzer by Hajiˇc and Hladk´a (1998) to extract the lemmas from the Czech portions of the training, development, and test data (the Czech-English portion of the News Commentary corpus distributed as as part of the WMT07 Shared Task). Data sets consisting of truncated forms were also generated; using a length limit of 6, which Goldwater and McClosky (2005) experimentally determined to be optimal for translation performance. We refer to the three data sets and the models derived from them as SURFACE, LEMMA, and TRUNC. Czech—*Engl</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical signficiance tests for machine translation evluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
</authors>
<title>Speech translation: Coupling of recognition and translation.</title>
<date>1999</date>
<booktitle>In IEEE International Conference on Acoustic, Speech and Signal Processing,</booktitle>
<pages>517--520</pages>
<location>Phoenix, AR,</location>
<contexts>
<context position="2755" citStr="Ney (1999)" startWordPosition="414" endWordPosition="415">ingless in the target language, and Yang and Kirchhoff (2006) propose the use of a backoff model that uses morphologically reduced forms only when the translation of the surface form is unavailable. All of these approaches have in common that the decisions about whether to use morphological information are made in either a pre- or post-processing step. Recent work in spoken language translation suggests that allowing decisions about the use of morphological information to be made along side other translation decisions (i.e., inside the decoder), will yield better results. At least as early as Ney (1999), it has been shown that when translating the output from automatic speech regonition (ASR) systems, the quality can be improved by considering multiple (rather than only a single best) transcription hypothesis. Although state-of-the-art statistical machine translation systems have conventionally assumed unambiguous input; recent work has demonstrated the possibility of efficient decoding of am207 Proceedings of the Second Workshop on Statistical Machine Translation, pages 207–211, Prague, June 2007. c�2007 Association for Computational Linguistics biguous input (represented as confusion netwo</context>
</contexts>
<marker>Ney, 1999</marker>
<rawString>H. Ney. 1999. Speech translation: Coupling of recognition and translation. In IEEE International Conference on Acoustic, Speech and Signal Processing, pages 517–520, Phoenix, AR, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>H Ney</author>
</authors>
<title>Morpho-syntactic analysis for reordering in statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of MT Summit VIII, Santiago de</booktitle>
<location>Compostela, Galicia,</location>
<contexts>
<context position="1582" citStr="Niessen and Ney (2001)" startWordPosition="227" endWordPosition="230">osition statistical machine translation systems. Conventional translation models are constructed with no consideration of the relationships between lexical items and instead treat different inflected (observed) forms of identical underlying lemmas as completely independent of one another. While the variously inflected forms of one lemma may express differences in meaning that are crucial to correct translation, the strict independence assumptions normally made exacerbate data sparseness and lead to poorly estimated models and suboptimal translations. A variety of solutions have been proposed: Niessen and Ney (2001) use of morphological information to improve word reordering before training and after decoding. Goldwater and McClosky (2005) show improvements in a Czech to English word-based translation system when inflectional endings are simplified or removed entirely. Their method can, however, actually harm performance since the discarded morphemes carry some information that may have bearing on the translation (cf. Section 3.3). To avoid this pitfall, Talbot and Osborne (2006) use a datadriven approach to cluster source-language morphological variants that are meaningless in the target language, and Y</context>
</contexts>
<marker>Niessen, Ney, 2001</marker>
<rawString>S. Niessen and H. Ney. 2001. Morpho-syntactic analysis for reordering in statistical machine translation. In Proceedings of MT Summit VIII, Santiago de Compostela, Galicia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Discriminitive training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="3866" citStr="Och and Ney, 2002" startWordPosition="577" endWordPosition="580">, June 2007. c�2007 Association for Computational Linguistics biguous input (represented as confusion networks or word lattices) within standard phrase-based models (Bertoldi et al., to appear 2007) as well as hierarchical phrase-based models (Dyer and Resnik, 2007). These hybrid decoders search for the target language sentence e� that maximizes the following probability, where !9(o) represents the set of weighted transcription hypotheses produced by an ASR decoder: e� = arg max max e f&apos;Eg(o) The conditional probability p(e, f|o) that is maximized is modeled directly using a log-linear model (Och and Ney, 2002), whose parameters can be tuned to optimize either the probability of a development set or some other objective (such as maximizing BLEU). In addition to the standard translation model features, the ASR system’s posterior probability is another feature. The decoder thus finds a translation hypothesis e� that maximizes the joint translation/transcription probability, which is not necessarily the one that corresponds to the best single transcription hypothesis. 2 Noisier channel translation We extend the concept of translating from an ambiguous set of source hypotheses to the domain of text tran</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. Och and H. Ney. 2002. Discriminitive training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="10042" citStr="Och, 2003" startWordPosition="1555" endWordPosition="1556">ell as portions of the GigaWord v2 English Corpus and was used for all experiments. Recasing was carried out using SRI’s disambig tool using a trigram language model. The feature set used included bidirectional translation probabilities for rules, lexical translation probabilities, a target language model probability, and count features for target words, number of non-terminal symbols used, and finally the number of morphologically simplified forms selected in the CN. Feature weight tuning was carried out using minimum error rate training, maximizing BLEU scores on a held-out development set (Och, 2003). Translation scores are reported using caseinsensitive BLEU (Papineni et al., 2002) with a single reference translation. Significance testing was done using bootstrap resampling (Koehn, 2004). 4.2 Data preparation and training We used a Czech morphological analyzer by Hajiˇc and Hladk´a (1998) to extract the lemmas from the Czech portions of the training, development, and test data (the Czech-English portion of the News Commentary corpus distributed as as part of the WMT07 Shared Task). Data sets consisting of truncated forms were also generated; using a length limit of 6, which Goldwater and</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="10126" citStr="Papineni et al., 2002" startWordPosition="1565" endWordPosition="1568">xperiments. Recasing was carried out using SRI’s disambig tool using a trigram language model. The feature set used included bidirectional translation probabilities for rules, lexical translation probabilities, a target language model probability, and count features for target words, number of non-terminal symbols used, and finally the number of morphologically simplified forms selected in the CN. Feature weight tuning was carried out using minimum error rate training, maximizing BLEU scores on a held-out development set (Och, 2003). Translation scores are reported using caseinsensitive BLEU (Papineni et al., 2002) with a single reference translation. Significance testing was done using bootstrap resampling (Koehn, 2004). 4.2 Data preparation and training We used a Czech morphological analyzer by Hajiˇc and Hladk´a (1998) to extract the lemmas from the Czech portions of the training, development, and test data (the Czech-English portion of the News Commentary corpus distributed as as part of the WMT07 Shared Task). Data sets consisting of truncated forms were also generated; using a length limit of 6, which Goldwater and McClosky (2005) experimentally determined to be optimal for translation performance</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. on Spoken Language Processing.</booktitle>
<contexts>
<context position="9377" citStr="Stolcke, 2002" startWordPosition="1451" endWordPosition="1452">ely limited. This has the advantage that a morphologically simplified (i.e., lemmatized) form of a Czech sentence has the same number of tokens as the surface form has words, which makes representing !9(f) as a confusion network relatively straightforward. The relative morphological complexity of Czech, as well as the potential benefits that can be realized by stemming, can be inferred from the corpus statistics given in Table 1. 4.1 Technical details A trigram English language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) was trained using the SRI Language Modeling Toolkit (Stolcke, 2002) on the English side of the News Commentary corpus as well as portions of the GigaWord v2 English Corpus and was used for all experiments. Recasing was carried out using SRI’s disambig tool using a trigram language model. The feature set used included bidirectional translation probabilities for rules, lexical translation probabilities, a target language model probability, and count features for target words, number of non-terminal symbols used, and finally the number of morphologically simplified forms selected in the CN. Feature weight tuning was carried out using minimum error rate training,</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Intl. Conf. on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Talbot</author>
<author>M Osborne</author>
</authors>
<title>Modelling lexical redundancy for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL 2006,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2055" citStr="Talbot and Osborne (2006)" startWordPosition="300" endWordPosition="303">rbate data sparseness and lead to poorly estimated models and suboptimal translations. A variety of solutions have been proposed: Niessen and Ney (2001) use of morphological information to improve word reordering before training and after decoding. Goldwater and McClosky (2005) show improvements in a Czech to English word-based translation system when inflectional endings are simplified or removed entirely. Their method can, however, actually harm performance since the discarded morphemes carry some information that may have bearing on the translation (cf. Section 3.3). To avoid this pitfall, Talbot and Osborne (2006) use a datadriven approach to cluster source-language morphological variants that are meaningless in the target language, and Yang and Kirchhoff (2006) propose the use of a backoff model that uses morphologically reduced forms only when the translation of the surface form is unavailable. All of these approaches have in common that the decisions about whether to use morphological information are made in either a pre- or post-processing step. Recent work in spoken language translation suggests that allowing decisions about the use of morphological information to be made along side other translat</context>
</contexts>
<marker>Talbot, Osborne, 2006</marker>
<rawString>D. Talbot and M. Osborne. 2006. Modelling lexical redundancy for machine translation. In Proceedings of ACL 2006, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>