<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.678822">
Paragraph-, word-, and coherence-based approaches to sentence ranking:
</note>
<title confidence="0.655651">
A comparison of algorithm and human performance
</title>
<author confidence="0.947031">
Florian WOLF
</author>
<affiliation confidence="0.96953">
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.9269985">
MIT NE20-448, 3 Cambridge Center
Cambridge, MA 02139, USA
</address>
<email confidence="0.97486">
fwolf@mit.edu
</email>
<sectionHeader confidence="0.991186" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999758037037037">
Sentence ranking is a crucial part of
generating text summaries. We compared
human sentence rankings obtained in a
psycholinguistic experiment to three different
approaches to sentence ranking: A simple
paragraph-based approach intended as a
baseline, two word-based approaches, and two
coherence-based approaches. In the
paragraph-based approach, sentences in the
beginning of paragraphs received higher
importance ratings than other sentences. The
word-based approaches determined sentence
rankings based on relative word frequencies
(Luhn (1958); Salton &amp; Buckley (1988)).
Coherence-based approaches determined
sentence rankings based on some property of
the coherence structure of a text (Marcu
(2000); Page et al. (1998)). Our results
suggest poor performance for the simple
paragraph-based approach, whereas word-
based approaches perform remarkably well.
The best performance was achieved by a
coherence-based approach where coherence
structures are represented in a non-tree
structure. Most approaches also outperformed
the commercially available MSWord
summarizer.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98168085">
Automatic generation of text summaries is a
natural language engineering application that has
received considerable interest, particularly due to
the ever-increasing volume of text information
available through the internet. The task of a
human generating a summary generally involves
three subtasks (Brandow et al. (1995); Mitra et al.
(1997)): (1) understanding a text; (2) ranking text
pieces (sentences, paragraphs, phrases, etc.) for
importance; (3) generating a new text (the
summary). Like most approaches to
summarization, we are concerned with the second
subtask (e.g. Carlson et al. (2001); Goldstein et al.
(1999); Gong &amp; Liu (2001); Jing et al. (1998);
Edward GIBSON
Massachusetts Institute of Technology
MIT NE20-459, 3 Cambridge Center
Cambridge, MA 02139, USA
egibson@mit.edu
Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp;
Sakai (2001); Zechner (1996)). Furthermore, we
are concerned with obtaining generic rather than
query-relevant importance rankings (cf. Goldstein
et al. (1999), Radev et al. (2002) for that
distinction).
We evaluated different approaches to sentence
ranking against human sentence rankings. To
obtain human sentence rankings, we asked people
to read 15 texts from the Wall Street Journal on a
wide variety of topics (e.g. economics, foreign and
domestic affairs, political commentaries). For each
of the sentences in the text, they provided a
ranking of how important that sentence is with
respect to the content of the text, on an integer
scale from 1 (not important) to 7 (very important).
The approaches we evaluated are a simple
paragraph-based approach that serves as a baseline,
two word-based algorithms, and two coherence-
based approaches1. We furthermore evaluated the
MSWord summarizer.
</bodyText>
<sectionHeader confidence="0.848715" genericHeader="method">
2 Approaches to sentence ranking
2.1 Paragraph-based approach
</sectionHeader>
<bodyText confidence="0.915272291666667">
Sentences at the beginning of a paragraph are
usually more important than sentences that are
further down in a paragraph, due in part to the way
people are instructed to write. Therefore, probably
the simplest approach conceivable to sentence
ranking is to choose the first sentences of each
1 We did not use any machine learning techniques to
boost performance of the algorithms we tested.
Therefore performance of the algorithms tested here
will almost certainly be below the level of performance
that could be reached if we had augmented the
algorithms with such techniques (e.g. Carlson et al.
(2001)). However, we think that a comparison between
‘bare-bones’ algorithms is viable because it allows to
see how performance differs due to different basic
approaches to sentence ranking, and not due to
potentially different effects of different machine
learning algorithms on different basic approaches to
sentence ranking. In future research we plan to address
the impact of machine learning on the algorithms tested
here.
paragraph as important, and the other sentences as
not important. We included this approach merely
as a simple baseline.
</bodyText>
<subsectionHeader confidence="0.996868">
2.2 Word-based approaches
</subsectionHeader>
<bodyText confidence="0.999976170731708">
Word-based approaches to summarization are
based on the idea that discourse segments are
important if they contain “important” words.
Different approaches have different definitions of
what an important word is. For example, Luhn
(1958), in a classic approach to summarization,
argues that sentences are more important if they
contain many significant words. Significant words
are words that are not in some predefined stoplist
of words with high overall corpus frequency2.
Once significant words are marked in a text,
clusters of significant words are formed. A cluster
has to start and end with a significant word, and
fewer than n insignificant words must separate any
two significant words (we chose n = 3, cf. Luhn
(1958)). Then, the weight of each cluster is
calculated by dividing the square of the number of
significant words in the cluster by the total number
of words in the cluster. Sentences can contain
multiple clusters. In order to compute the weight
of a sentence, the weights of all clusters in that
sentence are added. The higher the weight of a
sentence, the higher is its ranking.
A more recent and frequently used word-based
method used for text piece ranking is tf.idf (e.g.
Manning &amp; Schuetze (2000); Salton &amp; Buckley
(1988); Sparck-Jones &amp; Sakai (2001); Zechner
(1996)). The tf.idf measure relates the frequency
of words in a text piece, in the text, and in a
collection of texts respectively. The intuition
behind tf.idf is to give more weight to sentences
that contain terms with high frequency in a
document but low frequency in a reference corpus.
Figure 1 shows a formula for calculating tf.idf,
where dsij is the tf.idf weight of sentence i in
document j, nsi is the number of words in sentence
i, k is the kth word in sentence i, tfjk is the
frequency of word k in document j, nd is the
number of documents in the reference corpus, and
dfk is the number of documents in the reference
corpus in which word k appears.
</bodyText>
<equation confidence="0.7074598">
 
n
d
 
df k 
</equation>
<figureCaption confidence="0.966859">
Figure 1. Formula for calculating tf.idf (Salton &amp;
Buckley (1988)).
</figureCaption>
<bodyText confidence="0.8363445">
2 Instead of stoplists, tf.idf values have also been used
to determine significant words (e.g. Buyukkokten et al.
(2001)).
We compared both Luhn (1958)’s measure and
tf.idf scores to human rankings of sentence
importance. We will show that both methods
performed remarkably well, although one
coherence-based method performed better.
</bodyText>
<subsectionHeader confidence="0.997138">
2.3 Coherence-based approaches
</subsectionHeader>
<bodyText confidence="0.999941090909091">
The sentence ranking methods introduced in the
two previous sections are solely based on layout or
on properties of word distributions in sentences,
texts, and document collections. Other approaches
to sentence ranking are based on the informational
structure of texts. With informational structure, we
mean the set of informational relations that hold
between sentences in a text. This set can be
represented in a graph, where the nodes represent
sentences, and labeled directed arcs represent
informational relations that hold between the
sentences (cf. Hobbs (1985)). Often, informational
structures of texts have been represented as trees
(e.g. Carlson et al. (2001), Corston-Oliver (1998),
Mann &amp; Thompson (1988), Ono et al. (1994)). We
will present one coherence-based approach that
assumes trees as a data structure for representing
discourse structure, and one approach that assumes
less constrained graphs. As we will show, the
approach based on less constrained graphs
performs better than the tree-based approach when
compared to human sentence rankings.
</bodyText>
<sectionHeader confidence="0.956645" genericHeader="method">
3 Coherence-based summarization revisited
</sectionHeader>
<bodyText confidence="0.9998885">
This section will discuss in more detail the data
structures we used to represent discourse structure,
as well as the algorithms used to calculate sentence
importance, based on discourse structures.
</bodyText>
<subsectionHeader confidence="0.9995695">
3.1 Representing coherence structures
3.1.1 Discourse segments
</subsectionHeader>
<bodyText confidence="0.999983692307692">
Discourse segments can be defined as non-
overlapping spans of prosodic units (Hirschberg &amp;
Nakatani (1996)), intentional units (Grosz &amp;
Sidner (1986)), phrasal units (Lascarides &amp; Asher
(1993)), or sentences (Hobbs (1985)). We adopted
a sentence unit-based definition of discourse
segments for the coherence-based approach that
assumes non-tree graphs. For the coherence-based
approach that assumes trees, we used Marcu
(2000)’s more fine-grained definition of discourse
segments because we used the discourse trees from
Carlson et al. (2002)’s database of coherence-
annotated texts.
</bodyText>
<subsectionHeader confidence="0.970885">
3.1.2 Kinds of coherence relations
</subsectionHeader>
<bodyText confidence="0.999902">
We assume a set of coherence relations that is
similar to that of Hobbs (1985). Below are
examples of each coherence relation.
</bodyText>
<equation confidence="0.89852">
nsi
=∑ tf 4&amp;quot; 0g
jk
k =1
ij
ds
(1) Cause-Effect
</equation>
<bodyText confidence="0.9949035">
[There was bad weather at the airport]a [and so our
flight got delayed.]b
</bodyText>
<listItem confidence="0.938033">
(2) Violated Expectation
</listItem>
<bodyText confidence="0.8179275">
[The weather was nice]a [but our flight got
delayed.]b
</bodyText>
<listItem confidence="0.966173">
(3) Condition
</listItem>
<bodyText confidence="0.8612955">
[If the new software works,]a [everyone will be
happy.]b
</bodyText>
<listItem confidence="0.874286166666667">
(4) Similarity
[There is a train on Platform A.]a [There is another
train on Platform B.]b
(5) Contrast
[John supported Bush]a [but Susan opposed him.]b
(6) Elaboration
</listItem>
<bodyText confidence="0.578348666666667">
[A probe to Mars was launched this week.]a [The
European-built ‘Mars Express’ is scheduled to
reach Mars by late December.]b
</bodyText>
<listItem confidence="0.847564">
(7) Attribution
</listItem>
<bodyText confidence="0.9457285">
[John said that]a [the weather would be nice
tomorrow.]b
</bodyText>
<listItem confidence="0.949726">
(8) Temporal Sequence
</listItem>
<bodyText confidence="0.96095334375">
[Before he went to bed,]a [John took a shower.]b
Cause-effect, violated expectation, condition,
elaboration, temporal sequence, and attribution
are asymmetrical or directed relations, whereas
similarity, contrast, and temporal sequence are
symmetrical or undirected relations (Mann &amp;
Thompson, 1988; Marcu, 2000). In the non-tree-
based approach, the directions of asymmetrical or
directed relations are as follows: cause 4 effect
for cause-effect; cause 4 absent effect for violated
expectation; condition 4 consequence for
condition; elaborating 4 elaborated for
elaboration, and source 4 attributed for
attribution. In the tree-based approach, the
asymmetrical or directed relations are between a
more important discourse segment, or a Nucleus,
and a less important discourse segment, or a
Satellite (Marcu (2000)). The Nucleus is the
equivalent of the arc destination, and the Satellite
is the equivalent of the arc origin in the non-tree-
based approach. The symmetrical or undirected
relations are between two discourse elements of
equal importance, or two Nuclei. Below we will
explain how the difference between Satellites and
Nuclei is considered in tree-based sentence
rankings.
3.1.3 Data structures for representing discourse
coherence
As mentioned above, we used two alternative
representations for discourse structure, tree- and
non-tree based. In order to illustrate both data
structures, consider (9) as an example:
</bodyText>
<listItem confidence="0.9440106">
(9) Example text
0. Susan wanted to buy some tomatoes.
1. She also tried to find some basil.
2. The basil would probably be quite expensive
at this time of the year.
</listItem>
<bodyText confidence="0.575376666666667">
Figure 2 shows one possible tree representation
of the coherence structure of (9)3. Sim represents a
similarity relation, and elab an elaboration
relation. Furthermore, nodes with a “Nuc”
subscript are Nuclei, and nodes with a “Sat”
subscript are Satellites.
</bodyText>
<figureCaption confidence="0.9564522">
Figure 2. Coherence tree for (9).
Figure 3 shows a non-tree representation of the
coherence structure of (9). Here, the heads of the
arrows represent the directionality of a relation.
Figure 3. Non-tree coherence graph for (9).
</figureCaption>
<subsectionHeader confidence="0.992347">
3.2 Coherence-based sentence ranking
</subsectionHeader>
<bodyText confidence="0.9998935">
This section explains the algorithms for the tree-
and the non-tree-based sentence ranking approach.
</bodyText>
<subsectionHeader confidence="0.827516">
3.2.1 Tree-based approach
</subsectionHeader>
<bodyText confidence="0.999709352941176">
We used Marcu (2000)’s algorithm to determine
sentence rankings based on tree discourse
structures. In this algorithm, sentence salience is
determined based on the tree level of a discourse
segment in the coherence tree. Figure 4 shows
Marcu (2000)’s algorithm, where r(s,D,d) is the
rank of a sentence s in a discourse tree D with
depth d. Every node in a discourse tree D has a
promotion set promotion(D), which is the union of
all Nucleus children of that node. Associated with
every node in a discourse tree D is also a set of
parenthetical nodes parentheticals(D) (for
example, in “Mars – half the size of Earth – is
red”, “half the size of earth” would be a
parenthetical node in a discourse tree). Both
promotion(D) and parentheticals(D) can be empty
sets. Furthermore, each node has a left subtree,
</bodyText>
<figure confidence="0.970394488888889">
3 Another possible tree structure might be
( elab ( par ( 0 1 ) 2 ) ).
sim
0Nuc
1Nuc
2Sat
elabNuc
elab
sim
0 1 2
calculated PageRanks for α set to values between
0.05 and 0.95, in increments of 0.05; changing α
did not affect performance.
lc(D), and a right subtree, rc(D). Both lc(D) and
rc(D) can also be empty.
d if s promotion D
E ( ),
if s parentheticals D
E ( ),
max( ( , ( ),
r s lc D d
d −1)) otherwise







=
r(s,D,d)

,
D
0
is NIL
if
1
−
d
1
−
),
r
(s, rc(D),
</figure>
<figureCaption confidence="0.9928605">
Figure 5. Formula for calculating PageRank (Page
et al. (1998)).
</figureCaption>
<figure confidence="0.9812071">
1
−
n
PR
= − α + α
1
n
PR
1
on
</figure>
<figureCaption confidence="0.910894">
Figure 4. Formula for calculating coherence-tree-
based sentence rank (Marcu (2000)).
</figureCaption>
<bodyText confidence="0.999925454545454">
The discourse segments in Carlson et al.
(2002)’s database are often sub-sentential.
Therefore, we had to calculate sentence rankings
from the rankings of the discourse segments that
form the sentence under consideration. We did
this by calculating the average ranking, the
minimal ranking, and the maximal ranking of all
discourse segments in a sentence. Our results
showed that choosing the minimal ranking
performed best, followed by the average ranking,
followed by the maximal ranking (cf. Section 4.4).
</bodyText>
<subsectionHeader confidence="0.423208">
3.2.2 Non-tree-based approach
</subsectionHeader>
<bodyText confidence="0.999847555555556">
We used two different methods to determine
sentence rankings for the non-tree coherence
graphs4. Both methods implement the intuition
that sentences are more important if other
sentences relate to them (Sparck-Jones (1993)).
The first method consists of simply determining
the in-degree of each node in the graph. A node
represents a sentence, and the in-degree of a node
represents the number of sentences that relate to
that sentence.
The second method uses Page et al. (1998)’s
PageRank algorithm, which is used, for example,
in the GoogleTM search engine. Unlike just
determining the in-degree of a node, PageRank
takes into account the importance of sentences that
relate to a sentence. PageRank thus is a recursive
algorithm that implements the idea that the more
important sentences relate to a sentence, the more
important that sentence becomes. Figure 5 shows
how PageRank is calculated. PRn is the PageRank
of the current sentence, PRn-1 is the PageRank of
the sentence that relates to sentence n, on-1 is the
out-degree of sentence n-1, and α is a damping
parameter that is set to a value between 0 and 1.
We report results for α set to 0.85 because this is a
value often used in applications of PageRank (e.g.
Ding et al. (2002); Page et al. (1998)). We also
</bodyText>
<footnote confidence="0.81334925">
4 Neither of these methods could be implemented for
coherence trees since Marcu (2000)’s tree-based
algorithm assumes binary branching trees. Thus, the in-
degree for all non-terminal nodes is always 2.
</footnote>
<sectionHeader confidence="0.99753" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999935333333333">
In order to test algorithm performance, we
compared algorithm sentence rankings to human
sentence rankings. This section describes the
experiments we conducted. In Experiment 1, the
texts were presented with paragraph breaks; in
Experiment 2, the texts were presented without
paragraph breaks. This was done to control for the
effect of paragraph information on human sentence
rankings.
</bodyText>
<subsectionHeader confidence="0.5037705">
4.1 Materials for the coherence-based
approaches
</subsectionHeader>
<bodyText confidence="0.999987722222222">
In order to test the tree-based approach, we took
coherence trees for 15 texts from a database of 385
texts from the Wall Street Journal that were
annotated for coherence (Carlson et al. (2002)).
The database was independently annotated by six
annotators. Inter-annotator agreement was
determined for six pairs of two annotators each,
resulting in kappa values (Carletta (1996)) ranging
from 0.62 to 0.82 for the whole database (Carlson
et al. (2003)). No kappa values for just the 15 texts
we used were available.
For the non-tree based approach, we used
coherence graphs from a database of 135 texts
from the Wall Street Journal and the AP
Newswire, annotated for coherence. Each text was
independently annotated by two annotators. For
the 15 texts we used, kappa was 0.78, for the
whole database, kappa was 0.84.
</bodyText>
<sectionHeader confidence="0.5291775" genericHeader="method">
4.2 Experiment 1: With paragraph
information
</sectionHeader>
<bodyText confidence="0.99411325">
15 participants from the MIT community were
paid for their participation. All were native
speakers of English and were naïve as to the
purpose of the study (i.e. none of the subjects was
familiar with theories of coherence in natural
language, for example).
Participants were asked to read 15 texts from the
Wall Street Journal, and, for each sentence in each
text, to provide a ranking of how important that
sentence is with respect to the content of the text,
on an integer scale from 1 to 7 (1 = not important;
7 = very important). The texts were selected so
</bodyText>
<figure confidence="0.997549">
importance ranking
6
4
2
8
7
5
3
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
sentence number
KbParagraph
WithParagraph
</figure>
<figureCaption confidence="0.999431">
Figure 6. Human ranking results for one text (wsj_1306).
</figureCaption>
<bodyText confidence="0.9999728125">
that there was a coherence tree annotation
available in Carlson et al. (2002)’s database. Text
lengths for the 15 texts we selected ranged from
130 to 901 words (5 to 47 sentences); average text
length was 442 words (20 sentences), median was
368 words (16 sentences). Additionally, texts were
selected so that they were about as diverse topics
as possible.
The experiment was conducted in front of
personal computers. Texts were presented in a
web browser as one webpage per text; for some
texts, participants had to scroll to see the whole
text. Each sentence was presented on a new line.
Paragraph breaks were indicated by empty lines;
this was pointed out to the participants during the
instructions for the experiment.
</bodyText>
<subsectionHeader confidence="0.657242">
4.3 Experiment 2: Without paragraph
information
</subsectionHeader>
<bodyText confidence="0.999841">
The method was the same as in Experiment 1,
except that texts in Experiment 2 did not include
paragraph information. Each sentence was
presented on a new line. None of the 15
participants who participated in Experiment 2 had
participated in Experiment 1.
</bodyText>
<subsectionHeader confidence="0.996785">
4.4 Results of the experiments
</subsectionHeader>
<bodyText confidence="0.99991787037037">
Human sentence rankings did not differ
significantly between Experiment 1 and
Experiment 2 for any of the 15 texts (all Fs &lt; 1).
This suggests that paragraph information does not
have a big effect on human sentence rankings, at
least not for the 15 texts that we examined. Figure
6 shows the results from both experiments for one
text.
We compared human sentence rankings to
different algorithmic approaches. The paragraph-
based rankings do not provide scaled importance
rankings but only “important” vs. “not important”.
Therefore, in order to compare human rankings to
the paragraph-based baseline approach, we
calculated point biserial correlations (cf. Bortz
(1999)). We obtained significant correlations
between paragraph-based rankings and human
rankings only for one of the 15 texts.
All other algorithms provided scaled importance
rankings. Many evaluations of scalable sentence
ranking algorithms are based on precision/recall/F-
scores (e.g. Carlson et al. (2001); Ono et al.
(1994)). However, Jing et al. (1998) argue that
such measures are inadequate because they only
distinguish between hits and misses or false
alarms, but do not account for a degree of
agreement. For example, imagine a situation
where the human ranking for a given sentence is
“7” (“very important”) on an integer scale ranging
from 1 to 7, and Algorithm A gives the same
sentence a ranking of “7” on the same scale,
Algorithm B gives a ranking of “6”, and Algorithm
C gives a ranking of “2”. Intuitively, Algorithm B,
although it does not reach perfect performance,
still performs better than Algorithm C.
Precision/recall/F-scores do not account for that
difference and would rate Algorithm A as “hit” but
Algorithm B as well as Algorithm C as “miss”. In
order to collect performance measures that are
more adequate to the evaluation of scaled
importance rankings, we computed Spearman’s
rank correlation coefficients. The rank correlation
coefficients were corrected for tied ranks because
in our rankings it was possible for more than one
sentence to have the same importance rank, i.e. to
have tied ranks (Horn (1942); Bortz (1999)).
In addition to evaluating word-based and
coherence-based algorithms, we evaluated one
commercially available summarizer, the MSWord
summarizer, against human sentence rankings.
Our reason for including an evaluation of the
MSWord summarizer was to have a more useful
baseline for scalable sentence rankings than the
paragraph-based approach provides.
</bodyText>
<figure confidence="0.993977454545455">
mean rank correlation coefficient
0.6
0.5
0.4
0.3
0.2
0.1
0
MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank
NoParagraph
WithParagraph
</figure>
<figureCaption confidence="0.94570575">
Figure 7. Average rank correlations of algorithm and human sentence rankings.
Figure 7 shows average rank correlations (ρavg)
of each algorithm and human sentence ranking for
the 15 texts. MarcuAvg refers to the version of
</figureCaption>
<bodyText confidence="0.985257914893617">
Marcu (2000)’s algorithm where we calculated
sentence rankings as the average of the rankings of
all discourse segments that constitute that sentence;
for MarcuMin, sentence rankings were the
minimum of the rankings of all discourse segments
in that sentence; for MarcuMax we selected the
maximum of the rankings of all discourse
segments in that sentence.
Figure 7 shows that the MSWord summarizer
performed numerically worse than most other
algorithms, except MarcuMin. Figure 7 also
shows that PageRank performed numerically better
than all other algorithms. Performance was
significantly better than most other algorithms
(MSWord, NoParagraph: F(1,28) = 21.405, p =
0.0001; MSWord, WithParagraph: F(1,28) =
26.071, p = 0.0001; Luhn, WithParagraph: F(1,28)
= 5.495, p = 0.026; MarcuAvg, NoParagraph:
F(1,28) = 9.186, p = 0.005; MarcuAvg,
WithParagraph: F(1,28) = 9.097, p = 0.005;
MarcuMin, NoParagraph: F(1,28) = 4.753, p =
0.038; MarcuMax, NoParagraph F(1,28) = 24.633,
p = 0.0001; MarcuMax, WithParagraph: F(1,28) =
31.430, p =0.0001). Exceptions are Luhn,
NoParagraph (F(1,28) = 1.859, p = 0.184); tf.idf,
NoParagraph (F(1,28) = 2.307, p = 0.14);
MarcuMin, WithParagraph (F(1,28) = 2.555, p =
0.121). The difference between PageRank and
tf.idf, WithParagraph was marginally significant
(F(1,28) = 3.113, p = 0.089).
As mentioned above, human sentence rankings
did not differ significantly between Experiment 1
and Experiment 2 for any of the 15 texts (all Fs &lt;
1). Therefore, in order to lend more power to our
statistical tests, we collapsed the data for each text
for the WithParagraph and the NoParagraph
condition, and treated them as one experiment.
Figure 8 shows that when the data from
Experiments 1 and 2 are collapsed, PageRank
performed significantly better than all other
algorithms except in-degree (two-tailed t-test
results: MSWord: F(1, 58) = 48.717, p = 0.0001;
Luhn: F(1,58) = 6.368, p = 0.014; tf.idf: F(1,58) =
5.522, p = 0.022; MarcuAvg: F(1,58) = 18.922, p =
0.0001; MarcuMin: F(1,58) = 7.362, p = 0.009;
MarcuMax: F(1,58) = 56.989, p = 0.0001; in-
degree: F(1,58) &lt; 1).
</bodyText>
<figureCaption confidence="0.7141605">
Figure 8. Average rank correlations of algorithm
and human sentence rankings with collapsed data.
</figureCaption>
<sectionHeader confidence="0.997875" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999802944444444">
The goal of this paper was to evaluate the results
of three different kinds of sentence ranking
algorithms and one commercially available
summarizer. In order to evaluate the algorithms,
we compared their sentence rankings to human
sentence rankings of fifteen texts of varying length
from the Wall Street Journal.
Our results indicated that a simple paragraph-
based algorithm that was intended as a baseline
performed very poorly, and that word-based and
some coherence-based algorithms showed the best
performance. The only commercially available
summarizer that we tested, the MSWord
summarizer, showed worse performance than most
other algorithms. Furthermore, we found that a
coherence-based algorithm that uses PageRank and
takes non-tree coherence graphs as input
performed better than most versions of a
</bodyText>
<figure confidence="0.954583111111111">
mean rank correlation coefficient
0.5
0.4
0.3
0.2
0.
0
1
MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank
</figure>
<bodyText confidence="0.977389285714286">
coherence-based algorithm that operates on
coherence trees. When data from Experiments 1
and 2 were collapsed, the PageRank algorithm
performed significantly better than all other
algorithms, except the coherence-based algorithm
that uses in-degrees of nodes in non-tree coherence
graphs.
</bodyText>
<sectionHeader confidence="0.998766" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.974883345588235">
Jürgen Bortz. 1999. Statistik für Sozialwissen-
schaftler. Berlin: Springer Verlag.
Ronald Brandow, Karl Mitze, &amp; Lisa F Rau. 1995.
Automatic condensation of electronic
publications by sentence selection.
Information Processing and Management,
31(5), 675-685.
Orkut Buyukkokten, Hector Garcia-Molina, &amp;
Andreas Paepcke. 2001. Seeing the whole
in parts: Text summarization for web
browsing on handheld devices. Paper
presented at the 10th International WWW
Conference, Hong Kong, China.
Jean Carletta. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2), 249-
254.
Lynn Carlson, John M Conroy, Daniel Marcu,
Dianne P O&apos;Leary, Mary E Okurowski,
Anthony Taylor, et al. 2001. An empirical
study on the relation between abstracts,
extracts, and the discourse structure of
texts. Paper presented at the DUC-2001,
New Orleans, LA, USA.
Lynn Carlson, Daniel Marcu, &amp; Mary E
Okurowski. 2002. RST Discourse
Treebank. Philadelphia, PA: Linguistic
Data Consortium.
Lynn Carlson, Daniel Marcu, &amp; Mary E
Okurowski. 2003. Building a discourse-
tagged corpus in the framework of
rhetorical structure theory. In J. van
Kuppevelt &amp; R. Smith (Eds.), Current
directions in discourse and dialogue. New
York: Kluwer Academic Publishers.
Simon Corston-Oliver. 1998. Computing
representations of the structure of written
discourse. Redmont, WA.
Chris Ding, Xiaofeng He, Perry Husbands,
Hongyuan Zha, &amp; Horst Simon. 2002.
PageRank, HITS, and a unified framework
for link analysis. (No. 49372). Berkeley,
CA, USA.
Jade Goldstein, Mark Kantrowitz, Vibhu O Mittal,
&amp; Jamie O Carbonell. 1999. Summarizing
text documents: Sentence selection and
evaluation metrics. Paper presented at the
SIGIR-99, Melbourne, Australia.
Yihong Gong, &amp; Xin Liu. 2001. Generic text
summarization using relevance measure
and latent semantic analysis. Paper
presented at the Annual ACM Conference
on Research and Development in
Information Retrieval, New Orleans, LA,
USA.
Barbara J Grosz, &amp; Candace L Sidner. 1986.
Attention, intentions, and the structure of
discourse. Computational Linguistics,
12(3), 175-204.
Julia Hirschberg, &amp; Christine H Nakatani. 1996. A
prosodic analysis of discourse segments in
direction-giving monologues. Paper
presented at the 34th Annual Meeting of
the Association for Computational
Linguistics, Santa Cruz, CA.
Jerry R Hobbs. 1985. On the coherence and
structure of discourse. Stanford, CA.
D Horn. 1942. A correction for the effect of tied
ranks on the value of the rank difference
correlation coefficient. Journal of
Educational Psychology, 33, 686-690.
Hongyan Jing, Kathleen R McKeown, Regina
Barzilay, &amp; Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. Paper presented
at the AAAI-98 Spring Symposium on
Intelligent Text Summarization, Stanford,
CA, USA.
Alex Lascarides, &amp; Nicholas Asher. 1993.
Temporal interpretation, discourse
relations and common sense entailment.
Linguistics and Philosophy, 16(5), 437-
493.
Hans Peter Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of
Research and Development, 2(2), 159-165.
William C Mann, &amp; Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a
functional theory of text organization.
Text, 8(3), 243-281.
Christopher D Manning, &amp; Hinrich Schuetze.
2000. Foundations of statistical natural
language processing. Cambridge, MA,
USA: MIT Press.
Daniel Marcu. 2000. The theory and practice of
discourse parsing and summarization.
Cambridge, MA: MIT Press.
Mandar Mitra, Amit Singhal, &amp; Chris Buckley.
1997. Automatic text summarization by
paragraph extraction. Paper presented at
the ACL/EACL-97 Workshop on
Intelligent Scalable Text Summarization,
Madrid, Spain.
Kenji Ono, Kazuo Sumita, &amp; Seiji Miike. 1994.
Abstract generation based on rhetorical
structure extraction. Paper presented at the
COLING-94, Kyoto, Japan.
Lawrence Page, Sergey Brin, Rajeev Motwani, &amp;
Terry Winograd. 1998. The PageRank
citation ranking: Bringing order to the
web. Stanford, CA.
Dragomir R Radev, Eduard Hovy, &amp; Kathleen R
McKeown. 2002. Introduction to the
special issue on summarization.
Computational Linguistics, 28(4), 399-
408.
Gerard Salton, &amp; Christopher Buckley. 1988.
Term-weighting approaches in automatic
text retrieval. Information Processing and
Management, 24(5), 513-523.
Karen Sparck-Jones. 1993. What might be in a
summary? In G. Knorz, J. Krause &amp; C.
Womser-Hacker (Eds.), Information
retrieval 93: Von der Modellierung zur
Anwendung (pp. 9-26). Konstanz:
Universitaetsverlag.
Karen Sparck-Jones, &amp; Tetsuya Sakai. 2001,
September 2001. Generic summaries for
indexing in IR. Paper presented at the
ACM SIGIR-2001, New Orleans, LA,
USA.
Klaus Zechner. 1996. Fast generation of abstracts
from general domain text corpora by
extracting relevant sentences. Paper
presented at the COLING-96,
Copenhagen, Denmark.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797287">
<title confidence="0.9973495">Paragraph-, word-, and coherence-based approaches to sentence ranking: A comparison of algorithm and human performance</title>
<author confidence="0.999945">Florian WOLF</author>
<affiliation confidence="0.9432995">Massachusetts Institute of Technology MIT NE20-448, 3 Cambridge Center</affiliation>
<address confidence="0.999957">Cambridge, MA 02139, USA</address>
<email confidence="0.999922">fwolf@mit.edu</email>
<abstract confidence="0.99589275">Sentence ranking is a crucial part of generating text summaries. We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence-based approaches. In the paragraph-based approach, sentences in the beginning of paragraphs received higher importance ratings than other sentences. The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton &amp; Buckley (1988)). Coherence-based approaches determined sentence rankings based on some property of the coherence structure of a text (Marcu (2000); Page et al. (1998)). Our results suggest poor performance for the simple paragraph-based approach, whereas wordbased approaches perform remarkably well. The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure. Most approaches also outperformed the commercially available MSWord summarizer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jürgen Bortz</author>
</authors>
<title>Statistik für Sozialwissenschaftler.</title>
<date>1999</date>
<publisher>Springer Verlag.</publisher>
<location>Berlin:</location>
<contexts>
<context position="18988" citStr="Bortz (1999)" startWordPosition="3022" endWordPosition="3023">ween Experiment 1 and Experiment 2 for any of the 15 texts (all Fs &lt; 1). This suggests that paragraph information does not have a big effect on human sentence rankings, at least not for the 15 texts that we examined. Figure 6 shows the results from both experiments for one text. We compared human sentence rankings to different algorithmic approaches. The paragraphbased rankings do not provide scaled importance rankings but only “important” vs. “not important”. Therefore, in order to compare human rankings to the paragraph-based baseline approach, we calculated point biserial correlations (cf. Bortz (1999)). We obtained significant correlations between paragraph-based rankings and human rankings only for one of the 15 texts. All other algorithms provided scaled importance rankings. Many evaluations of scalable sentence ranking algorithms are based on precision/recall/Fscores (e.g. Carlson et al. (2001); Ono et al. (1994)). However, Jing et al. (1998) argue that such measures are inadequate because they only distinguish between hits and misses or false alarms, but do not account for a degree of agreement. For example, imagine a situation where the human ranking for a given sentence is “7” (“very</context>
<context position="20437" citStr="Bortz (1999)" startWordPosition="3253" endWordPosition="3254">ugh it does not reach perfect performance, still performs better than Algorithm C. Precision/recall/F-scores do not account for that difference and would rate Algorithm A as “hit” but Algorithm B as well as Algorithm C as “miss”. In order to collect performance measures that are more adequate to the evaluation of scaled importance rankings, we computed Spearman’s rank correlation coefficients. The rank correlation coefficients were corrected for tied ranks because in our rankings it was possible for more than one sentence to have the same importance rank, i.e. to have tied ranks (Horn (1942); Bortz (1999)). In addition to evaluating word-based and coherence-based algorithms, we evaluated one commercially available summarizer, the MSWord summarizer, against human sentence rankings. Our reason for including an evaluation of the MSWord summarizer was to have a more useful baseline for scalable sentence rankings than the paragraph-based approach provides. mean rank correlation coefficient 0.6 0.5 0.4 0.3 0.2 0.1 0 MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank NoParagraph WithParagraph Figure 7. Average rank correlations of algorithm and human sentence rankings. Figure 7 shows av</context>
</contexts>
<marker>Bortz, 1999</marker>
<rawString>Jürgen Bortz. 1999. Statistik für Sozialwissenschaftler. Berlin: Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Brandow</author>
<author>Karl Mitze</author>
<author>Lisa F Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1995</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>31</volume>
<issue>5</issue>
<pages>675--685</pages>
<contexts>
<context position="1660" citStr="Brandow et al. (1995)" startWordPosition="217" endWordPosition="220">h-based approach, whereas wordbased approaches perform remarkably well. The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure. Most approaches also outperformed the commercially available MSWord summarizer. 1 Introduction Automatic generation of text summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet. The task of a human generating a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generi</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>Ronald Brandow, Karl Mitze, &amp; Lisa F Rau. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing and Management, 31(5), 675-685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Orkut Buyukkokten</author>
<author>Hector Garcia-Molina</author>
<author>Andreas Paepcke</author>
</authors>
<title>Seeing the whole in parts: Text summarization for web browsing on handheld devices.</title>
<date>2001</date>
<booktitle>Paper presented at the 10th International WWW Conference,</booktitle>
<location>Hong Kong, China.</location>
<contexts>
<context position="6466" citStr="Buyukkokten et al. (2001)" startWordPosition="994" endWordPosition="997">nt but low frequency in a reference corpus. Figure 1 shows a formula for calculating tf.idf, where dsij is the tf.idf weight of sentence i in document j, nsi is the number of words in sentence i, k is the kth word in sentence i, tfjk is the frequency of word k in document j, nd is the number of documents in the reference corpus, and dfk is the number of documents in the reference corpus in which word k appears.   n d   df k  Figure 1. Formula for calculating tf.idf (Salton &amp; Buckley (1988)). 2 Instead of stoplists, tf.idf values have also been used to determine significant words (e.g. Buyukkokten et al. (2001)). We compared both Luhn (1958)’s measure and tf.idf scores to human rankings of sentence importance. We will show that both methods performed remarkably well, although one coherence-based method performed better. 2.3 Coherence-based approaches The sentence ranking methods introduced in the two previous sections are solely based on layout or on properties of word distributions in sentences, texts, and document collections. Other approaches to sentence ranking are based on the informational structure of texts. With informational structure, we mean the set of informational relations that hold be</context>
</contexts>
<marker>Buyukkokten, Garcia-Molina, Paepcke, 2001</marker>
<rawString>Orkut Buyukkokten, Hector Garcia-Molina, &amp; Andreas Paepcke. 2001. Seeing the whole in parts: Text summarization for web browsing on handheld devices. Paper presented at the 10th International WWW Conference, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>249--254</pages>
<contexts>
<context position="16034" citStr="Carletta (1996)" startWordPosition="2518" endWordPosition="2519">nted with paragraph breaks; in Experiment 2, the texts were presented without paragraph breaks. This was done to control for the effect of paragraph information on human sentence rankings. 4.1 Materials for the coherence-based approaches In order to test the tree-based approach, we took coherence trees for 15 texts from a database of 385 texts from the Wall Street Journal that were annotated for coherence (Carlson et al. (2002)). The database was independently annotated by six annotators. Inter-annotator agreement was determined for six pairs of two annotators each, resulting in kappa values (Carletta (1996)) ranging from 0.62 to 0.82 for the whole database (Carlson et al. (2003)). No kappa values for just the 15 texts we used were available. For the non-tree based approach, we used coherence graphs from a database of 135 texts from the Wall Street Journal and the AP Newswire, annotated for coherence. Each text was independently annotated by two annotators. For the 15 texts we used, kappa was 0.78, for the whole database, kappa was 0.84. 4.2 Experiment 1: With paragraph information 15 participants from the MIT community were paid for their participation. All were native speakers of English and we</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2), 249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>John M Conroy</author>
<author>Daniel Marcu</author>
<author>Dianne P O&apos;Leary</author>
<author>Mary E Okurowski</author>
<author>Anthony Taylor</author>
</authors>
<title>An empirical study on the relation between abstracts, extracts, and the discourse structure of texts. Paper presented at the DUC-2001,</title>
<date>2001</date>
<location>New Orleans, LA, USA.</location>
<contexts>
<context position="1937" citStr="Carlson et al. (2001)" startWordPosition="259" endWordPosition="262">rizer. 1 Introduction Automatic generation of text summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet. The task of a human generating a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf. Goldstein et al. (1999), Radev et al. (2002) for that distinction). We evaluated different approaches to sentence ranking against human sentence rankings. To obtain human sentence rankings, we asked people to read 15 texts </context>
<context position="3740" citStr="Carlson et al. (2001)" startWordPosition="538" endWordPosition="541">h-based approach Sentences at the beginning of a paragraph are usually more important than sentences that are further down in a paragraph, due in part to the way people are instructed to write. Therefore, probably the simplest approach conceivable to sentence ranking is to choose the first sentences of each 1 We did not use any machine learning techniques to boost performance of the algorithms we tested. Therefore performance of the algorithms tested here will almost certainly be below the level of performance that could be reached if we had augmented the algorithms with such techniques (e.g. Carlson et al. (2001)). However, we think that a comparison between ‘bare-bones’ algorithms is viable because it allows to see how performance differs due to different basic approaches to sentence ranking, and not due to potentially different effects of different machine learning algorithms on different basic approaches to sentence ranking. In future research we plan to address the impact of machine learning on the algorithms tested here. paragraph as important, and the other sentences as not important. We included this approach merely as a simple baseline. 2.2 Word-based approaches Word-based approaches to summar</context>
<context position="7381" citStr="Carlson et al. (2001)" startWordPosition="1127" endWordPosition="1130">o previous sections are solely based on layout or on properties of word distributions in sentences, texts, and document collections. Other approaches to sentence ranking are based on the informational structure of texts. With informational structure, we mean the set of informational relations that hold between sentences in a text. This set can be represented in a graph, where the nodes represent sentences, and labeled directed arcs represent informational relations that hold between the sentences (cf. Hobbs (1985)). Often, informational structures of texts have been represented as trees (e.g. Carlson et al. (2001), Corston-Oliver (1998), Mann &amp; Thompson (1988), Ono et al. (1994)). We will present one coherence-based approach that assumes trees as a data structure for representing discourse structure, and one approach that assumes less constrained graphs. As we will show, the approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings. 3 Coherence-based summarization revisited This section will discuss in more detail the data structures we used to represent discourse structure, as well as the algorithms used to calculate sentence import</context>
<context position="19290" citStr="Carlson et al. (2001)" startWordPosition="3061" endWordPosition="3064">uman sentence rankings to different algorithmic approaches. The paragraphbased rankings do not provide scaled importance rankings but only “important” vs. “not important”. Therefore, in order to compare human rankings to the paragraph-based baseline approach, we calculated point biserial correlations (cf. Bortz (1999)). We obtained significant correlations between paragraph-based rankings and human rankings only for one of the 15 texts. All other algorithms provided scaled importance rankings. Many evaluations of scalable sentence ranking algorithms are based on precision/recall/Fscores (e.g. Carlson et al. (2001); Ono et al. (1994)). However, Jing et al. (1998) argue that such measures are inadequate because they only distinguish between hits and misses or false alarms, but do not account for a degree of agreement. For example, imagine a situation where the human ranking for a given sentence is “7” (“very important”) on an integer scale ranging from 1 to 7, and Algorithm A gives the same sentence a ranking of “7” on the same scale, Algorithm B gives a ranking of “6”, and Algorithm C gives a ranking of “2”. Intuitively, Algorithm B, although it does not reach perfect performance, still performs better </context>
</contexts>
<marker>Carlson, Conroy, Marcu, O&apos;Leary, Okurowski, Taylor, 2001</marker>
<rawString>Lynn Carlson, John M Conroy, Daniel Marcu, Dianne P O&apos;Leary, Mary E Okurowski, Anthony Taylor, et al. 2001. An empirical study on the relation between abstracts, extracts, and the discourse structure of texts. Paper presented at the DUC-2001, New Orleans, LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary E Okurowski</author>
</authors>
<date>2002</date>
<booktitle>RST Discourse Treebank.</booktitle>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, PA:</location>
<contexts>
<context position="8622" citStr="Carlson et al. (2002)" startWordPosition="1303" endWordPosition="1306">urse structures. 3.1 Representing coherence structures 3.1.1 Discourse segments Discourse segments can be defined as nonoverlapping spans of prosodic units (Hirschberg &amp; Nakatani (1996)), intentional units (Grosz &amp; Sidner (1986)), phrasal units (Lascarides &amp; Asher (1993)), or sentences (Hobbs (1985)). We adopted a sentence unit-based definition of discourse segments for the coherence-based approach that assumes non-tree graphs. For the coherence-based approach that assumes trees, we used Marcu (2000)’s more fine-grained definition of discourse segments because we used the discourse trees from Carlson et al. (2002)’s database of coherenceannotated texts. 3.1.2 Kinds of coherence relations We assume a set of coherence relations that is similar to that of Hobbs (1985). Below are examples of each coherence relation. nsi =∑ tf 4&amp;quot; 0g jk k =1 ij ds (1) Cause-Effect [There was bad weather at the airport]a [and so our flight got delayed.]b (2) Violated Expectation [The weather was nice]a [but our flight got delayed.]b (3) Condition [If the new software works,]a [everyone will be happy.]b (4) Similarity [There is a train on Platform A.]a [There is another train on Platform B.]b (5) Contrast [John supported Bush]</context>
<context position="13243" citStr="Carlson et al. (2002)" startWordPosition="2075" endWordPosition="2078">Sat elabNuc elab sim 0 1 2 calculated PageRanks for α set to values between 0.05 and 0.95, in increments of 0.05; changing α did not affect performance. lc(D), and a right subtree, rc(D). Both lc(D) and rc(D) can also be empty. d if s promotion D E ( ), if s parentheticals D E ( ), max( ( , ( ), r s lc D d d −1)) otherwise        = r(s,D,d)  , D 0 is NIL if 1 − d 1 − ), r (s, rc(D), Figure 5. Formula for calculating PageRank (Page et al. (1998)). 1 − n PR = − α + α 1 n PR 1 on Figure 4. Formula for calculating coherence-treebased sentence rank (Marcu (2000)). The discourse segments in Carlson et al. (2002)’s database are often sub-sentential. Therefore, we had to calculate sentence rankings from the rankings of the discourse segments that form the sentence under consideration. We did this by calculating the average ranking, the minimal ranking, and the maximal ranking of all discourse segments in a sentence. Our results showed that choosing the minimal ranking performed best, followed by the average ranking, followed by the maximal ranking (cf. Section 4.4). 3.2.2 Non-tree-based approach We used two different methods to determine sentence rankings for the non-tree coherence graphs4. Both method</context>
<context position="15850" citStr="Carlson et al. (2002)" startWordPosition="2491" endWordPosition="2494"> to test algorithm performance, we compared algorithm sentence rankings to human sentence rankings. This section describes the experiments we conducted. In Experiment 1, the texts were presented with paragraph breaks; in Experiment 2, the texts were presented without paragraph breaks. This was done to control for the effect of paragraph information on human sentence rankings. 4.1 Materials for the coherence-based approaches In order to test the tree-based approach, we took coherence trees for 15 texts from a database of 385 texts from the Wall Street Journal that were annotated for coherence (Carlson et al. (2002)). The database was independently annotated by six annotators. Inter-annotator agreement was determined for six pairs of two annotators each, resulting in kappa values (Carletta (1996)) ranging from 0.62 to 0.82 for the whole database (Carlson et al. (2003)). No kappa values for just the 15 texts we used were available. For the non-tree based approach, we used coherence graphs from a database of 135 texts from the Wall Street Journal and the AP Newswire, annotated for coherence. Each text was independently annotated by two annotators. For the 15 texts we used, kappa was 0.78, for the whole dat</context>
<context position="17339" citStr="Carlson et al. (2002)" startWordPosition="2754" endWordPosition="2757"> theories of coherence in natural language, for example). Participants were asked to read 15 texts from the Wall Street Journal, and, for each sentence in each text, to provide a ranking of how important that sentence is with respect to the content of the text, on an integer scale from 1 to 7 (1 = not important; 7 = very important). The texts were selected so importance ranking 6 4 2 8 7 5 3 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sentence number KbParagraph WithParagraph Figure 6. Human ranking results for one text (wsj_1306). that there was a coherence tree annotation available in Carlson et al. (2002)’s database. Text lengths for the 15 texts we selected ranged from 130 to 901 words (5 to 47 sentences); average text length was 442 words (20 sentences), median was 368 words (16 sentences). Additionally, texts were selected so that they were about as diverse topics as possible. The experiment was conducted in front of personal computers. Texts were presented in a web browser as one webpage per text; for some texts, participants had to scroll to see the whole text. Each sentence was presented on a new line. Paragraph breaks were indicated by empty lines; this was pointed out to the participan</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>Lynn Carlson, Daniel Marcu, &amp; Mary E Okurowski. 2002. RST Discourse Treebank. Philadelphia, PA: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary E Okurowski</author>
</authors>
<title>Building a discoursetagged corpus in the framework of rhetorical structure theory.</title>
<date>2003</date>
<journal>In J. van Kuppevelt</journal>
<publisher>Kluwer Academic Publishers.</publisher>
<location>New York:</location>
<contexts>
<context position="16107" citStr="Carlson et al. (2003)" startWordPosition="2529" endWordPosition="2532">ed without paragraph breaks. This was done to control for the effect of paragraph information on human sentence rankings. 4.1 Materials for the coherence-based approaches In order to test the tree-based approach, we took coherence trees for 15 texts from a database of 385 texts from the Wall Street Journal that were annotated for coherence (Carlson et al. (2002)). The database was independently annotated by six annotators. Inter-annotator agreement was determined for six pairs of two annotators each, resulting in kappa values (Carletta (1996)) ranging from 0.62 to 0.82 for the whole database (Carlson et al. (2003)). No kappa values for just the 15 texts we used were available. For the non-tree based approach, we used coherence graphs from a database of 135 texts from the Wall Street Journal and the AP Newswire, annotated for coherence. Each text was independently annotated by two annotators. For the 15 texts we used, kappa was 0.78, for the whole database, kappa was 0.84. 4.2 Experiment 1: With paragraph information 15 participants from the MIT community were paid for their participation. All were native speakers of English and were naïve as to the purpose of the study (i.e. none of the subjects was fa</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2003</marker>
<rawString>Lynn Carlson, Daniel Marcu, &amp; Mary E Okurowski. 2003. Building a discoursetagged corpus in the framework of rhetorical structure theory. In J. van Kuppevelt &amp; R. Smith (Eds.), Current directions in discourse and dialogue. New York: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
</authors>
<title>Computing representations of the structure of written discourse.</title>
<date>1998</date>
<location>Redmont, WA.</location>
<contexts>
<context position="7404" citStr="Corston-Oliver (1998)" startWordPosition="1131" endWordPosition="1132"> solely based on layout or on properties of word distributions in sentences, texts, and document collections. Other approaches to sentence ranking are based on the informational structure of texts. With informational structure, we mean the set of informational relations that hold between sentences in a text. This set can be represented in a graph, where the nodes represent sentences, and labeled directed arcs represent informational relations that hold between the sentences (cf. Hobbs (1985)). Often, informational structures of texts have been represented as trees (e.g. Carlson et al. (2001), Corston-Oliver (1998), Mann &amp; Thompson (1988), Ono et al. (1994)). We will present one coherence-based approach that assumes trees as a data structure for representing discourse structure, and one approach that assumes less constrained graphs. As we will show, the approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings. 3 Coherence-based summarization revisited This section will discuss in more detail the data structures we used to represent discourse structure, as well as the algorithms used to calculate sentence importance, based on discours</context>
</contexts>
<marker>Corston-Oliver, 1998</marker>
<rawString>Simon Corston-Oliver. 1998. Computing representations of the structure of written discourse. Redmont, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Xiaofeng He</author>
<author>Perry Husbands</author>
<author>Hongyuan Zha</author>
<author>Horst Simon</author>
</authors>
<title>PageRank, HITS, and a unified framework for link analysis.</title>
<date>2002</date>
<location>(No. 49372). Berkeley, CA, USA.</location>
<contexts>
<context position="14975" citStr="Ding et al. (2002)" startWordPosition="2355" endWordPosition="2358">s into account the importance of sentences that relate to a sentence. PageRank thus is a recursive algorithm that implements the idea that the more important sentences relate to a sentence, the more important that sentence becomes. Figure 5 shows how PageRank is calculated. PRn is the PageRank of the current sentence, PRn-1 is the PageRank of the sentence that relates to sentence n, on-1 is the out-degree of sentence n-1, and α is a damping parameter that is set to a value between 0 and 1. We report results for α set to 0.85 because this is a value often used in applications of PageRank (e.g. Ding et al. (2002); Page et al. (1998)). We also 4 Neither of these methods could be implemented for coherence trees since Marcu (2000)’s tree-based algorithm assumes binary branching trees. Thus, the indegree for all non-terminal nodes is always 2. 4 Experiments In order to test algorithm performance, we compared algorithm sentence rankings to human sentence rankings. This section describes the experiments we conducted. In Experiment 1, the texts were presented with paragraph breaks; in Experiment 2, the texts were presented without paragraph breaks. This was done to control for the effect of paragraph informa</context>
</contexts>
<marker>Ding, He, Husbands, Zha, Simon, 2002</marker>
<rawString>Chris Ding, Xiaofeng He, Perry Husbands, Hongyuan Zha, &amp; Horst Simon. 2002. PageRank, HITS, and a unified framework for link analysis. (No. 49372). Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu O Mittal</author>
<author>Jamie O Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics. Paper presented at the SIGIR-99,</title>
<date>1999</date>
<location>Melbourne, Australia.</location>
<contexts>
<context position="1962" citStr="Goldstein et al. (1999)" startWordPosition="263" endWordPosition="266">utomatic generation of text summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet. The task of a human generating a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf. Goldstein et al. (1999), Radev et al. (2002) for that distinction). We evaluated different approaches to sentence ranking against human sentence rankings. To obtain human sentence rankings, we asked people to read 15 texts from the Wall Street Jour</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu O Mittal, &amp; Jamie O Carbonell. 1999. Summarizing text documents: Sentence selection and evaluation metrics. Paper presented at the SIGIR-99, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yihong Gong</author>
<author>Xin Liu</author>
</authors>
<title>Generic text summarization using relevance measure and latent semantic analysis.</title>
<date>2001</date>
<booktitle>Paper presented at the Annual ACM Conference on Research and Development in Information Retrieval,</booktitle>
<location>New Orleans, LA, USA.</location>
<contexts>
<context position="1981" citStr="Gong &amp; Liu (2001)" startWordPosition="267" endWordPosition="270">xt summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet. The task of a human generating a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf. Goldstein et al. (1999), Radev et al. (2002) for that distinction). We evaluated different approaches to sentence ranking against human sentence rankings. To obtain human sentence rankings, we asked people to read 15 texts from the Wall Street Journal on a wide varie</context>
</contexts>
<marker>Gong, Liu, 2001</marker>
<rawString>Yihong Gong, &amp; Xin Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. Paper presented at the Annual ACM Conference on Research and Development in Information Retrieval, New Orleans, LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<contexts>
<context position="8229" citStr="Grosz &amp; Sidner (1986)" startWordPosition="1248" endWordPosition="1251">onstrained graphs. As we will show, the approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings. 3 Coherence-based summarization revisited This section will discuss in more detail the data structures we used to represent discourse structure, as well as the algorithms used to calculate sentence importance, based on discourse structures. 3.1 Representing coherence structures 3.1.1 Discourse segments Discourse segments can be defined as nonoverlapping spans of prosodic units (Hirschberg &amp; Nakatani (1996)), intentional units (Grosz &amp; Sidner (1986)), phrasal units (Lascarides &amp; Asher (1993)), or sentences (Hobbs (1985)). We adopted a sentence unit-based definition of discourse segments for the coherence-based approach that assumes non-tree graphs. For the coherence-based approach that assumes trees, we used Marcu (2000)’s more fine-grained definition of discourse segments because we used the discourse trees from Carlson et al. (2002)’s database of coherenceannotated texts. 3.1.2 Kinds of coherence relations We assume a set of coherence relations that is similar to that of Hobbs (1985). Below are examples of each coherence relation. nsi </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J Grosz, &amp; Candace L Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3), 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Christine H Nakatani</author>
</authors>
<title>A prosodic analysis of discourse segments in direction-giving monologues.</title>
<date>1996</date>
<booktitle>Paper presented at the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="8186" citStr="Hirschberg &amp; Nakatani (1996)" startWordPosition="1242" endWordPosition="1245">se structure, and one approach that assumes less constrained graphs. As we will show, the approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings. 3 Coherence-based summarization revisited This section will discuss in more detail the data structures we used to represent discourse structure, as well as the algorithms used to calculate sentence importance, based on discourse structures. 3.1 Representing coherence structures 3.1.1 Discourse segments Discourse segments can be defined as nonoverlapping spans of prosodic units (Hirschberg &amp; Nakatani (1996)), intentional units (Grosz &amp; Sidner (1986)), phrasal units (Lascarides &amp; Asher (1993)), or sentences (Hobbs (1985)). We adopted a sentence unit-based definition of discourse segments for the coherence-based approach that assumes non-tree graphs. For the coherence-based approach that assumes trees, we used Marcu (2000)’s more fine-grained definition of discourse segments because we used the discourse trees from Carlson et al. (2002)’s database of coherenceannotated texts. 3.1.2 Kinds of coherence relations We assume a set of coherence relations that is similar to that of Hobbs (1985). Below ar</context>
</contexts>
<marker>Hirschberg, Nakatani, 1996</marker>
<rawString>Julia Hirschberg, &amp; Christine H Nakatani. 1996. A prosodic analysis of discourse segments in direction-giving monologues. Paper presented at the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>On the coherence and structure of discourse.</title>
<date>1985</date>
<location>Stanford, CA.</location>
<contexts>
<context position="7279" citStr="Hobbs (1985)" startWordPosition="1114" endWordPosition="1115">rmed better. 2.3 Coherence-based approaches The sentence ranking methods introduced in the two previous sections are solely based on layout or on properties of word distributions in sentences, texts, and document collections. Other approaches to sentence ranking are based on the informational structure of texts. With informational structure, we mean the set of informational relations that hold between sentences in a text. This set can be represented in a graph, where the nodes represent sentences, and labeled directed arcs represent informational relations that hold between the sentences (cf. Hobbs (1985)). Often, informational structures of texts have been represented as trees (e.g. Carlson et al. (2001), Corston-Oliver (1998), Mann &amp; Thompson (1988), Ono et al. (1994)). We will present one coherence-based approach that assumes trees as a data structure for representing discourse structure, and one approach that assumes less constrained graphs. As we will show, the approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings. 3 Coherence-based summarization revisited This section will discuss in more detail the data structures</context>
<context position="8776" citStr="Hobbs (1985)" startWordPosition="1330" endWordPosition="1331">berg &amp; Nakatani (1996)), intentional units (Grosz &amp; Sidner (1986)), phrasal units (Lascarides &amp; Asher (1993)), or sentences (Hobbs (1985)). We adopted a sentence unit-based definition of discourse segments for the coherence-based approach that assumes non-tree graphs. For the coherence-based approach that assumes trees, we used Marcu (2000)’s more fine-grained definition of discourse segments because we used the discourse trees from Carlson et al. (2002)’s database of coherenceannotated texts. 3.1.2 Kinds of coherence relations We assume a set of coherence relations that is similar to that of Hobbs (1985). Below are examples of each coherence relation. nsi =∑ tf 4&amp;quot; 0g jk k =1 ij ds (1) Cause-Effect [There was bad weather at the airport]a [and so our flight got delayed.]b (2) Violated Expectation [The weather was nice]a [but our flight got delayed.]b (3) Condition [If the new software works,]a [everyone will be happy.]b (4) Similarity [There is a train on Platform A.]a [There is another train on Platform B.]b (5) Contrast [John supported Bush]a [but Susan opposed him.]b (6) Elaboration [A probe to Mars was launched this week.]a [The European-built ‘Mars Express’ is scheduled to reach Mars by la</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Jerry R Hobbs. 1985. On the coherence and structure of discourse. Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Horn</author>
</authors>
<title>A correction for the effect of tied ranks on the value of the rank difference correlation coefficient.</title>
<date>1942</date>
<journal>Journal of Educational Psychology,</journal>
<volume>33</volume>
<pages>686--690</pages>
<contexts>
<context position="20423" citStr="Horn (1942)" startWordPosition="3251" endWordPosition="3252">ithm B, although it does not reach perfect performance, still performs better than Algorithm C. Precision/recall/F-scores do not account for that difference and would rate Algorithm A as “hit” but Algorithm B as well as Algorithm C as “miss”. In order to collect performance measures that are more adequate to the evaluation of scaled importance rankings, we computed Spearman’s rank correlation coefficients. The rank correlation coefficients were corrected for tied ranks because in our rankings it was possible for more than one sentence to have the same importance rank, i.e. to have tied ranks (Horn (1942); Bortz (1999)). In addition to evaluating word-based and coherence-based algorithms, we evaluated one commercially available summarizer, the MSWord summarizer, against human sentence rankings. Our reason for including an evaluation of the MSWord summarizer was to have a more useful baseline for scalable sentence rankings than the paragraph-based approach provides. mean rank correlation coefficient 0.6 0.5 0.4 0.3 0.2 0.1 0 MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank NoParagraph WithParagraph Figure 7. Average rank correlations of algorithm and human sentence rankings. Fig</context>
</contexts>
<marker>Horn, 1942</marker>
<rawString>D Horn. 1942. A correction for the effect of tied ranks on the value of the rank difference correlation coefficient. Journal of Educational Psychology, 33, 686-690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen R McKeown</author>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Summarization evaluation methods: Experiments and analysis.</title>
<date>1998</date>
<booktitle>Paper presented at the AAAI-98 Spring Symposium on Intelligent Text Summarization,</booktitle>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="2001" citStr="Jing et al. (1998)" startWordPosition="271" endWordPosition="274">atural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet. The task of a human generating a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf. Goldstein et al. (1999), Radev et al. (2002) for that distinction). We evaluated different approaches to sentence ranking against human sentence rankings. To obtain human sentence rankings, we asked people to read 15 texts from the Wall Street Journal on a wide variety of topics (e.g. e</context>
<context position="19339" citStr="Jing et al. (1998)" startWordPosition="3070" endWordPosition="3073">oaches. The paragraphbased rankings do not provide scaled importance rankings but only “important” vs. “not important”. Therefore, in order to compare human rankings to the paragraph-based baseline approach, we calculated point biserial correlations (cf. Bortz (1999)). We obtained significant correlations between paragraph-based rankings and human rankings only for one of the 15 texts. All other algorithms provided scaled importance rankings. Many evaluations of scalable sentence ranking algorithms are based on precision/recall/Fscores (e.g. Carlson et al. (2001); Ono et al. (1994)). However, Jing et al. (1998) argue that such measures are inadequate because they only distinguish between hits and misses or false alarms, but do not account for a degree of agreement. For example, imagine a situation where the human ranking for a given sentence is “7” (“very important”) on an integer scale ranging from 1 to 7, and Algorithm A gives the same sentence a ranking of “7” on the same scale, Algorithm B gives a ranking of “6”, and Algorithm C gives a ranking of “2”. Intuitively, Algorithm B, although it does not reach perfect performance, still performs better than Algorithm C. Precision/recall/F-scores do no</context>
</contexts>
<marker>Jing, McKeown, Barzilay, Elhadad, 1998</marker>
<rawString>Hongyan Jing, Kathleen R McKeown, Regina Barzilay, &amp; Michael Elhadad. 1998. Summarization evaluation methods: Experiments and analysis. Paper presented at the AAAI-98 Spring Symposium on Intelligent Text Summarization, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Temporal interpretation, discourse relations and common sense entailment.</title>
<date>1993</date>
<journal>Linguistics and Philosophy,</journal>
<volume>16</volume>
<issue>5</issue>
<pages>437--493</pages>
<contexts>
<context position="8272" citStr="Lascarides &amp; Asher (1993)" startWordPosition="1254" endWordPosition="1257"> approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings. 3 Coherence-based summarization revisited This section will discuss in more detail the data structures we used to represent discourse structure, as well as the algorithms used to calculate sentence importance, based on discourse structures. 3.1 Representing coherence structures 3.1.1 Discourse segments Discourse segments can be defined as nonoverlapping spans of prosodic units (Hirschberg &amp; Nakatani (1996)), intentional units (Grosz &amp; Sidner (1986)), phrasal units (Lascarides &amp; Asher (1993)), or sentences (Hobbs (1985)). We adopted a sentence unit-based definition of discourse segments for the coherence-based approach that assumes non-tree graphs. For the coherence-based approach that assumes trees, we used Marcu (2000)’s more fine-grained definition of discourse segments because we used the discourse trees from Carlson et al. (2002)’s database of coherenceannotated texts. 3.1.2 Kinds of coherence relations We assume a set of coherence relations that is similar to that of Hobbs (1985). Below are examples of each coherence relation. nsi =∑ tf 4&amp;quot; 0g jk k =1 ij ds (1) Cause-Effect </context>
</contexts>
<marker>Lascarides, Asher, 1993</marker>
<rawString>Alex Lascarides, &amp; Nicholas Asher. 1993. Temporal interpretation, discourse relations and common sense entailment. Linguistics and Philosophy, 16(5), 437-493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Peter Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>159--165</pages>
<contexts>
<context position="799" citStr="Luhn (1958)" startWordPosition="102" endWordPosition="103">mbridge Center Cambridge, MA 02139, USA fwolf@mit.edu Abstract Sentence ranking is a crucial part of generating text summaries. We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence-based approaches. In the paragraph-based approach, sentences in the beginning of paragraphs received higher importance ratings than other sentences. The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton &amp; Buckley (1988)). Coherence-based approaches determined sentence rankings based on some property of the coherence structure of a text (Marcu (2000); Page et al. (1998)). Our results suggest poor performance for the simple paragraph-based approach, whereas wordbased approaches perform remarkably well. The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure. Most approaches also outperformed the commercially available MSWord summarizer. 1 Introduction Automatic generation of text summaries is a natural language </context>
<context position="2140" citStr="Luhn (1958)" startWordPosition="291" endWordPosition="292">n available through the internet. The task of a human generating a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf. Goldstein et al. (1999), Radev et al. (2002) for that distinction). We evaluated different approaches to sentence ranking against human sentence rankings. To obtain human sentence rankings, we asked people to read 15 texts from the Wall Street Journal on a wide variety of topics (e.g. economics, foreign and domestic affairs, political commentaries). For each of the sentences in the text, they provided a ranking of how impo</context>
<context position="4545" citStr="Luhn (1958)" startWordPosition="659" endWordPosition="660">ue to potentially different effects of different machine learning algorithms on different basic approaches to sentence ranking. In future research we plan to address the impact of machine learning on the algorithms tested here. paragraph as important, and the other sentences as not important. We included this approach merely as a simple baseline. 2.2 Word-based approaches Word-based approaches to summarization are based on the idea that discourse segments are important if they contain “important” words. Different approaches have different definitions of what an important word is. For example, Luhn (1958), in a classic approach to summarization, argues that sentences are more important if they contain many significant words. Significant words are words that are not in some predefined stoplist of words with high overall corpus frequency2. Once significant words are marked in a text, clusters of significant words are formed. A cluster has to start and end with a significant word, and fewer than n insignificant words must separate any two significant words (we chose n = 3, cf. Luhn (1958)). Then, the weight of each cluster is calculated by dividing the square of the number of significant words in</context>
<context position="6497" citStr="Luhn (1958)" startWordPosition="1001" endWordPosition="1002">igure 1 shows a formula for calculating tf.idf, where dsij is the tf.idf weight of sentence i in document j, nsi is the number of words in sentence i, k is the kth word in sentence i, tfjk is the frequency of word k in document j, nd is the number of documents in the reference corpus, and dfk is the number of documents in the reference corpus in which word k appears.   n d   df k  Figure 1. Formula for calculating tf.idf (Salton &amp; Buckley (1988)). 2 Instead of stoplists, tf.idf values have also been used to determine significant words (e.g. Buyukkokten et al. (2001)). We compared both Luhn (1958)’s measure and tf.idf scores to human rankings of sentence importance. We will show that both methods performed remarkably well, although one coherence-based method performed better. 2.3 Coherence-based approaches The sentence ranking methods introduced in the two previous sections are solely based on layout or on properties of word distributions in sentences, texts, and document collections. Other approaches to sentence ranking are based on the informational structure of texts. With informational structure, we mean the set of informational relations that hold between sentences in a text. This</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Hans Peter Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2), 159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>243--281</pages>
<contexts>
<context position="7428" citStr="Mann &amp; Thompson (1988)" startWordPosition="1133" endWordPosition="1136"> or on properties of word distributions in sentences, texts, and document collections. Other approaches to sentence ranking are based on the informational structure of texts. With informational structure, we mean the set of informational relations that hold between sentences in a text. This set can be represented in a graph, where the nodes represent sentences, and labeled directed arcs represent informational relations that hold between the sentences (cf. Hobbs (1985)). Often, informational structures of texts have been represented as trees (e.g. Carlson et al. (2001), Corston-Oliver (1998), Mann &amp; Thompson (1988), Ono et al. (1994)). We will present one coherence-based approach that assumes trees as a data structure for representing discourse structure, and one approach that assumes less constrained graphs. As we will show, the approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings. 3 Coherence-based summarization revisited This section will discuss in more detail the data structures we used to represent discourse structure, as well as the algorithms used to calculate sentence importance, based on discourse structures. 3.1 Repres</context>
<context position="9784" citStr="Mann &amp; Thompson, 1988" startWordPosition="1482" endWordPosition="1485">r train on Platform B.]b (5) Contrast [John supported Bush]a [but Susan opposed him.]b (6) Elaboration [A probe to Mars was launched this week.]a [The European-built ‘Mars Express’ is scheduled to reach Mars by late December.]b (7) Attribution [John said that]a [the weather would be nice tomorrow.]b (8) Temporal Sequence [Before he went to bed,]a [John took a shower.]b Cause-effect, violated expectation, condition, elaboration, temporal sequence, and attribution are asymmetrical or directed relations, whereas similarity, contrast, and temporal sequence are symmetrical or undirected relations (Mann &amp; Thompson, 1988; Marcu, 2000). In the non-treebased approach, the directions of asymmetrical or directed relations are as follows: cause 4 effect for cause-effect; cause 4 absent effect for violated expectation; condition 4 consequence for condition; elaborating 4 elaborated for elaboration, and source 4 attributed for attribution. In the tree-based approach, the asymmetrical or directed relations are between a more important discourse segment, or a Nucleus, and a less important discourse segment, or a Satellite (Marcu (2000)). The Nucleus is the equivalent of the arc destination, and the Satellite is the eq</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann, &amp; Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3), 243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schuetze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA, USA:</location>
<contexts>
<context position="5531" citStr="Manning &amp; Schuetze (2000)" startWordPosition="824" endWordPosition="827">a significant word, and fewer than n insignificant words must separate any two significant words (we chose n = 3, cf. Luhn (1958)). Then, the weight of each cluster is calculated by dividing the square of the number of significant words in the cluster by the total number of words in the cluster. Sentences can contain multiple clusters. In order to compute the weight of a sentence, the weights of all clusters in that sentence are added. The higher the weight of a sentence, the higher is its ranking. A more recent and frequently used word-based method used for text piece ranking is tf.idf (e.g. Manning &amp; Schuetze (2000); Salton &amp; Buckley (1988); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). The tf.idf measure relates the frequency of words in a text piece, in the text, and in a collection of texts respectively. The intuition behind tf.idf is to give more weight to sentences that contain terms with high frequency in a document but low frequency in a reference corpus. Figure 1 shows a formula for calculating tf.idf, where dsij is the tf.idf weight of sentence i in document j, nsi is the number of words in sentence i, k is the kth word in sentence i, tfjk is the frequency of word k in document j, nd is the numb</context>
</contexts>
<marker>Manning, Schuetze, 2000</marker>
<rawString>Christopher D Manning, &amp; Hinrich Schuetze. 2000. Foundations of statistical natural language processing. Cambridge, MA, USA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The theory and practice of discourse parsing and summarization.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="956" citStr="Marcu (2000)" startWordPosition="124" endWordPosition="125">ankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence-based approaches. In the paragraph-based approach, sentences in the beginning of paragraphs received higher importance ratings than other sentences. The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton &amp; Buckley (1988)). Coherence-based approaches determined sentence rankings based on some property of the coherence structure of a text (Marcu (2000); Page et al. (1998)). Our results suggest poor performance for the simple paragraph-based approach, whereas wordbased approaches perform remarkably well. The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure. Most approaches also outperformed the commercially available MSWord summarizer. 1 Introduction Automatic generation of text summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the int</context>
<context position="8506" citStr="Marcu (2000)" startWordPosition="1288" endWordPosition="1289">resent discourse structure, as well as the algorithms used to calculate sentence importance, based on discourse structures. 3.1 Representing coherence structures 3.1.1 Discourse segments Discourse segments can be defined as nonoverlapping spans of prosodic units (Hirschberg &amp; Nakatani (1996)), intentional units (Grosz &amp; Sidner (1986)), phrasal units (Lascarides &amp; Asher (1993)), or sentences (Hobbs (1985)). We adopted a sentence unit-based definition of discourse segments for the coherence-based approach that assumes non-tree graphs. For the coherence-based approach that assumes trees, we used Marcu (2000)’s more fine-grained definition of discourse segments because we used the discourse trees from Carlson et al. (2002)’s database of coherenceannotated texts. 3.1.2 Kinds of coherence relations We assume a set of coherence relations that is similar to that of Hobbs (1985). Below are examples of each coherence relation. nsi =∑ tf 4&amp;quot; 0g jk k =1 ij ds (1) Cause-Effect [There was bad weather at the airport]a [and so our flight got delayed.]b (2) Violated Expectation [The weather was nice]a [but our flight got delayed.]b (3) Condition [If the new software works,]a [everyone will be happy.]b (4) Simil</context>
<context position="9798" citStr="Marcu, 2000" startWordPosition="1486" endWordPosition="1487">b (5) Contrast [John supported Bush]a [but Susan opposed him.]b (6) Elaboration [A probe to Mars was launched this week.]a [The European-built ‘Mars Express’ is scheduled to reach Mars by late December.]b (7) Attribution [John said that]a [the weather would be nice tomorrow.]b (8) Temporal Sequence [Before he went to bed,]a [John took a shower.]b Cause-effect, violated expectation, condition, elaboration, temporal sequence, and attribution are asymmetrical or directed relations, whereas similarity, contrast, and temporal sequence are symmetrical or undirected relations (Mann &amp; Thompson, 1988; Marcu, 2000). In the non-treebased approach, the directions of asymmetrical or directed relations are as follows: cause 4 effect for cause-effect; cause 4 absent effect for violated expectation; condition 4 consequence for condition; elaborating 4 elaborated for elaboration, and source 4 attributed for attribution. In the tree-based approach, the asymmetrical or directed relations are between a more important discourse segment, or a Nucleus, and a less important discourse segment, or a Satellite (Marcu (2000)). The Nucleus is the equivalent of the arc destination, and the Satellite is the equivalent of th</context>
<context position="11749" citStr="Marcu (2000)" startWordPosition="1783" endWordPosition="1784">herence structure of (9)3. Sim represents a similarity relation, and elab an elaboration relation. Furthermore, nodes with a “Nuc” subscript are Nuclei, and nodes with a “Sat” subscript are Satellites. Figure 2. Coherence tree for (9). Figure 3 shows a non-tree representation of the coherence structure of (9). Here, the heads of the arrows represent the directionality of a relation. Figure 3. Non-tree coherence graph for (9). 3.2 Coherence-based sentence ranking This section explains the algorithms for the treeand the non-tree-based sentence ranking approach. 3.2.1 Tree-based approach We used Marcu (2000)’s algorithm to determine sentence rankings based on tree discourse structures. In this algorithm, sentence salience is determined based on the tree level of a discourse segment in the coherence tree. Figure 4 shows Marcu (2000)’s algorithm, where r(s,D,d) is the rank of a sentence s in a discourse tree D with depth d. Every node in a discourse tree D has a promotion set promotion(D), which is the union of all Nucleus children of that node. Associated with every node in a discourse tree D is also a set of parenthetical nodes parentheticals(D) (for example, in “Mars – half the size of Earth – i</context>
<context position="13193" citStr="Marcu (2000)" startWordPosition="2069" endWordPosition="2070">elab ( par ( 0 1 ) 2 ) ). sim 0Nuc 1Nuc 2Sat elabNuc elab sim 0 1 2 calculated PageRanks for α set to values between 0.05 and 0.95, in increments of 0.05; changing α did not affect performance. lc(D), and a right subtree, rc(D). Both lc(D) and rc(D) can also be empty. d if s promotion D E ( ), if s parentheticals D E ( ), max( ( , ( ), r s lc D d d −1)) otherwise        = r(s,D,d)  , D 0 is NIL if 1 − d 1 − ), r (s, rc(D), Figure 5. Formula for calculating PageRank (Page et al. (1998)). 1 − n PR = − α + α 1 n PR 1 on Figure 4. Formula for calculating coherence-treebased sentence rank (Marcu (2000)). The discourse segments in Carlson et al. (2002)’s database are often sub-sentential. Therefore, we had to calculate sentence rankings from the rankings of the discourse segments that form the sentence under consideration. We did this by calculating the average ranking, the minimal ranking, and the maximal ranking of all discourse segments in a sentence. Our results showed that choosing the minimal ranking performed best, followed by the average ranking, followed by the maximal ranking (cf. Section 4.4). 3.2.2 Non-tree-based approach We used two different methods to determine sentence rankin</context>
<context position="15092" citStr="Marcu (2000)" startWordPosition="2377" endWordPosition="2378">s the idea that the more important sentences relate to a sentence, the more important that sentence becomes. Figure 5 shows how PageRank is calculated. PRn is the PageRank of the current sentence, PRn-1 is the PageRank of the sentence that relates to sentence n, on-1 is the out-degree of sentence n-1, and α is a damping parameter that is set to a value between 0 and 1. We report results for α set to 0.85 because this is a value often used in applications of PageRank (e.g. Ding et al. (2002); Page et al. (1998)). We also 4 Neither of these methods could be implemented for coherence trees since Marcu (2000)’s tree-based algorithm assumes binary branching trees. Thus, the indegree for all non-terminal nodes is always 2. 4 Experiments In order to test algorithm performance, we compared algorithm sentence rankings to human sentence rankings. This section describes the experiments we conducted. In Experiment 1, the texts were presented with paragraph breaks; in Experiment 2, the texts were presented without paragraph breaks. This was done to control for the effect of paragraph information on human sentence rankings. 4.1 Materials for the coherence-based approaches In order to test the tree-based app</context>
<context position="21177" citStr="Marcu (2000)" startWordPosition="3358" endWordPosition="3359">MSWord summarizer, against human sentence rankings. Our reason for including an evaluation of the MSWord summarizer was to have a more useful baseline for scalable sentence rankings than the paragraph-based approach provides. mean rank correlation coefficient 0.6 0.5 0.4 0.3 0.2 0.1 0 MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank NoParagraph WithParagraph Figure 7. Average rank correlations of algorithm and human sentence rankings. Figure 7 shows average rank correlations (ρavg) of each algorithm and human sentence ranking for the 15 texts. MarcuAvg refers to the version of Marcu (2000)’s algorithm where we calculated sentence rankings as the average of the rankings of all discourse segments that constitute that sentence; for MarcuMin, sentence rankings were the minimum of the rankings of all discourse segments in that sentence; for MarcuMax we selected the maximum of the rankings of all discourse segments in that sentence. Figure 7 shows that the MSWord summarizer performed numerically worse than most other algorithms, except MarcuMin. Figure 7 also shows that PageRank performed numerically better than all other algorithms. Performance was significantly better than most oth</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The theory and practice of discourse parsing and summarization. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mandar Mitra</author>
<author>Amit Singhal</author>
<author>Chris Buckley</author>
</authors>
<title>Automatic text summarization by paragraph extraction.</title>
<date>1997</date>
<booktitle>Paper presented at the ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1681" citStr="Mitra et al. (1997)" startWordPosition="221" endWordPosition="224">as wordbased approaches perform remarkably well. The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure. Most approaches also outperformed the commercially available MSWord summarizer. 1 Introduction Automatic generation of text summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet. The task of a human generating a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-r</context>
</contexts>
<marker>Mitra, Singhal, Buckley, 1997</marker>
<rawString>Mandar Mitra, Amit Singhal, &amp; Chris Buckley. 1997. Automatic text summarization by paragraph extraction. Paper presented at the ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Ono</author>
<author>Kazuo Sumita</author>
<author>Seiji Miike</author>
</authors>
<title>Abstract generation based on rhetorical structure extraction. Paper presented at the COLING-94,</title>
<date>1994</date>
<location>Kyoto, Japan.</location>
<contexts>
<context position="7447" citStr="Ono et al. (1994)" startWordPosition="1137" endWordPosition="1140">d distributions in sentences, texts, and document collections. Other approaches to sentence ranking are based on the informational structure of texts. With informational structure, we mean the set of informational relations that hold between sentences in a text. This set can be represented in a graph, where the nodes represent sentences, and labeled directed arcs represent informational relations that hold between the sentences (cf. Hobbs (1985)). Often, informational structures of texts have been represented as trees (e.g. Carlson et al. (2001), Corston-Oliver (1998), Mann &amp; Thompson (1988), Ono et al. (1994)). We will present one coherence-based approach that assumes trees as a data structure for representing discourse structure, and one approach that assumes less constrained graphs. As we will show, the approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings. 3 Coherence-based summarization revisited This section will discuss in more detail the data structures we used to represent discourse structure, as well as the algorithms used to calculate sentence importance, based on discourse structures. 3.1 Representing coherence st</context>
<context position="19309" citStr="Ono et al. (1994)" startWordPosition="3065" endWordPosition="3068">to different algorithmic approaches. The paragraphbased rankings do not provide scaled importance rankings but only “important” vs. “not important”. Therefore, in order to compare human rankings to the paragraph-based baseline approach, we calculated point biserial correlations (cf. Bortz (1999)). We obtained significant correlations between paragraph-based rankings and human rankings only for one of the 15 texts. All other algorithms provided scaled importance rankings. Many evaluations of scalable sentence ranking algorithms are based on precision/recall/Fscores (e.g. Carlson et al. (2001); Ono et al. (1994)). However, Jing et al. (1998) argue that such measures are inadequate because they only distinguish between hits and misses or false alarms, but do not account for a degree of agreement. For example, imagine a situation where the human ranking for a given sentence is “7” (“very important”) on an integer scale ranging from 1 to 7, and Algorithm A gives the same sentence a ranking of “7” on the same scale, Algorithm B gives a ranking of “6”, and Algorithm C gives a ranking of “2”. Intuitively, Algorithm B, although it does not reach perfect performance, still performs better than Algorithm C. P</context>
</contexts>
<marker>Ono, Sumita, Miike, 1994</marker>
<rawString>Kenji Ono, Kazuo Sumita, &amp; Seiji Miike. 1994. Abstract generation based on rhetorical structure extraction. Paper presented at the COLING-94, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The PageRank citation ranking: Bringing order to the web.</title>
<date>1998</date>
<location>Stanford, CA.</location>
<contexts>
<context position="976" citStr="Page et al. (1998)" startWordPosition="126" endWordPosition="129">ed in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence-based approaches. In the paragraph-based approach, sentences in the beginning of paragraphs received higher importance ratings than other sentences. The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton &amp; Buckley (1988)). Coherence-based approaches determined sentence rankings based on some property of the coherence structure of a text (Marcu (2000); Page et al. (1998)). Our results suggest poor performance for the simple paragraph-based approach, whereas wordbased approaches perform remarkably well. The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure. Most approaches also outperformed the commercially available MSWord summarizer. 1 Introduction Automatic generation of text summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet. The task of a</context>
<context position="13078" citStr="Page et al. (1998)" startWordPosition="2042" endWordPosition="2045">ntheticals(D) can be empty sets. Furthermore, each node has a left subtree, 3 Another possible tree structure might be ( elab ( par ( 0 1 ) 2 ) ). sim 0Nuc 1Nuc 2Sat elabNuc elab sim 0 1 2 calculated PageRanks for α set to values between 0.05 and 0.95, in increments of 0.05; changing α did not affect performance. lc(D), and a right subtree, rc(D). Both lc(D) and rc(D) can also be empty. d if s promotion D E ( ), if s parentheticals D E ( ), max( ( , ( ), r s lc D d d −1)) otherwise        = r(s,D,d)  , D 0 is NIL if 1 − d 1 − ), r (s, rc(D), Figure 5. Formula for calculating PageRank (Page et al. (1998)). 1 − n PR = − α + α 1 n PR 1 on Figure 4. Formula for calculating coherence-treebased sentence rank (Marcu (2000)). The discourse segments in Carlson et al. (2002)’s database are often sub-sentential. Therefore, we had to calculate sentence rankings from the rankings of the discourse segments that form the sentence under consideration. We did this by calculating the average ranking, the minimal ranking, and the maximal ranking of all discourse segments in a sentence. Our results showed that choosing the minimal ranking performed best, followed by the average ranking, followed by the maximal </context>
<context position="14995" citStr="Page et al. (1998)" startWordPosition="2359" endWordPosition="2362">mportance of sentences that relate to a sentence. PageRank thus is a recursive algorithm that implements the idea that the more important sentences relate to a sentence, the more important that sentence becomes. Figure 5 shows how PageRank is calculated. PRn is the PageRank of the current sentence, PRn-1 is the PageRank of the sentence that relates to sentence n, on-1 is the out-degree of sentence n-1, and α is a damping parameter that is set to a value between 0 and 1. We report results for α set to 0.85 because this is a value often used in applications of PageRank (e.g. Ding et al. (2002); Page et al. (1998)). We also 4 Neither of these methods could be implemented for coherence trees since Marcu (2000)’s tree-based algorithm assumes binary branching trees. Thus, the indegree for all non-terminal nodes is always 2. 4 Experiments In order to test algorithm performance, we compared algorithm sentence rankings to human sentence rankings. This section describes the experiments we conducted. In Experiment 1, the texts were presented with paragraph breaks; in Experiment 2, the texts were presented without paragraph breaks. This was done to control for the effect of paragraph information on human senten</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, &amp; Terry Winograd. 1998. The PageRank citation ranking: Bringing order to the web. Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Eduard Hovy</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Introduction to the special issue on summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>399--408</pages>
<contexts>
<context position="2358" citStr="Radev et al. (2002)" startWordPosition="320" endWordPosition="323">ences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf. Goldstein et al. (1999), Radev et al. (2002) for that distinction). We evaluated different approaches to sentence ranking against human sentence rankings. To obtain human sentence rankings, we asked people to read 15 texts from the Wall Street Journal on a wide variety of topics (e.g. economics, foreign and domestic affairs, political commentaries). For each of the sentences in the text, they provided a ranking of how important that sentence is with respect to the content of the text, on an integer scale from 1 (not important) to 7 (very important). The approaches we evaluated are a simple paragraph-based approach that serves as a basel</context>
</contexts>
<marker>Radev, Hovy, McKeown, 2002</marker>
<rawString>Dragomir R Radev, Eduard Hovy, &amp; Kathleen R McKeown. 2002. Introduction to the special issue on summarization. Computational Linguistics, 28(4), 399-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<pages>513--523</pages>
<contexts>
<context position="824" citStr="Salton &amp; Buckley (1988)" startWordPosition="104" endWordPosition="107">r Cambridge, MA 02139, USA fwolf@mit.edu Abstract Sentence ranking is a crucial part of generating text summaries. We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence-based approaches. In the paragraph-based approach, sentences in the beginning of paragraphs received higher importance ratings than other sentences. The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton &amp; Buckley (1988)). Coherence-based approaches determined sentence rankings based on some property of the coherence structure of a text (Marcu (2000); Page et al. (1998)). Our results suggest poor performance for the simple paragraph-based approach, whereas wordbased approaches perform remarkably well. The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure. Most approaches also outperformed the commercially available MSWord summarizer. 1 Introduction Automatic generation of text summaries is a natural language engineering application t</context>
<context position="5556" citStr="Salton &amp; Buckley (1988)" startWordPosition="828" endWordPosition="831">er than n insignificant words must separate any two significant words (we chose n = 3, cf. Luhn (1958)). Then, the weight of each cluster is calculated by dividing the square of the number of significant words in the cluster by the total number of words in the cluster. Sentences can contain multiple clusters. In order to compute the weight of a sentence, the weights of all clusters in that sentence are added. The higher the weight of a sentence, the higher is its ranking. A more recent and frequently used word-based method used for text piece ranking is tf.idf (e.g. Manning &amp; Schuetze (2000); Salton &amp; Buckley (1988); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). The tf.idf measure relates the frequency of words in a text piece, in the text, and in a collection of texts respectively. The intuition behind tf.idf is to give more weight to sentences that contain terms with high frequency in a document but low frequency in a reference corpus. Figure 1 shows a formula for calculating tf.idf, where dsij is the tf.idf weight of sentence i in document j, nsi is the number of words in sentence i, k is the kth word in sentence i, tfjk is the frequency of word k in document j, nd is the number of documents in the re</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton, &amp; Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5), 513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck-Jones</author>
</authors>
<title>What might be in a summary? In</title>
<date>1993</date>
<booktitle>Information retrieval 93: Von der Modellierung zur Anwendung</booktitle>
<pages>9--26</pages>
<publisher>Universitaetsverlag.</publisher>
<location>Konstanz:</location>
<contexts>
<context position="13957" citStr="Sparck-Jones (1993)" startWordPosition="2181" endWordPosition="2182"> rankings of the discourse segments that form the sentence under consideration. We did this by calculating the average ranking, the minimal ranking, and the maximal ranking of all discourse segments in a sentence. Our results showed that choosing the minimal ranking performed best, followed by the average ranking, followed by the maximal ranking (cf. Section 4.4). 3.2.2 Non-tree-based approach We used two different methods to determine sentence rankings for the non-tree coherence graphs4. Both methods implement the intuition that sentences are more important if other sentences relate to them (Sparck-Jones (1993)). The first method consists of simply determining the in-degree of each node in the graph. A node represents a sentence, and the in-degree of a node represents the number of sentences that relate to that sentence. The second method uses Page et al. (1998)’s PageRank algorithm, which is used, for example, in the GoogleTM search engine. Unlike just determining the in-degree of a node, PageRank takes into account the importance of sentences that relate to a sentence. PageRank thus is a recursive algorithm that implements the idea that the more important sentences relate to a sentence, the more i</context>
</contexts>
<marker>Sparck-Jones, 1993</marker>
<rawString>Karen Sparck-Jones. 1993. What might be in a summary? In G. Knorz, J. Krause &amp; C. Womser-Hacker (Eds.), Information retrieval 93: Von der Modellierung zur Anwendung (pp. 9-26). Konstanz: Universitaetsverlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck-Jones</author>
<author>Tetsuya Sakai</author>
</authors>
<title>Generic summaries for indexing in IR.</title>
<date>2001</date>
<booktitle>Paper presented at the ACM SIGIR-2001,</booktitle>
<location>New Orleans, LA, USA.</location>
<contexts>
<context position="2190" citStr="Sparck-Jones &amp; Sakai (2001)" startWordPosition="297" endWordPosition="300">The task of a human generating a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf. Goldstein et al. (1999), Radev et al. (2002) for that distinction). We evaluated different approaches to sentence ranking against human sentence rankings. To obtain human sentence rankings, we asked people to read 15 texts from the Wall Street Journal on a wide variety of topics (e.g. economics, foreign and domestic affairs, political commentaries). For each of the sentences in the text, they provided a ranking of how important that sentence is with respect to the content</context>
<context position="5585" citStr="Sparck-Jones &amp; Sakai (2001)" startWordPosition="832" endWordPosition="835">ords must separate any two significant words (we chose n = 3, cf. Luhn (1958)). Then, the weight of each cluster is calculated by dividing the square of the number of significant words in the cluster by the total number of words in the cluster. Sentences can contain multiple clusters. In order to compute the weight of a sentence, the weights of all clusters in that sentence are added. The higher the weight of a sentence, the higher is its ranking. A more recent and frequently used word-based method used for text piece ranking is tf.idf (e.g. Manning &amp; Schuetze (2000); Salton &amp; Buckley (1988); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). The tf.idf measure relates the frequency of words in a text piece, in the text, and in a collection of texts respectively. The intuition behind tf.idf is to give more weight to sentences that contain terms with high frequency in a document but low frequency in a reference corpus. Figure 1 shows a formula for calculating tf.idf, where dsij is the tf.idf weight of sentence i in document j, nsi is the number of words in sentence i, k is the kth word in sentence i, tfjk is the frequency of word k in document j, nd is the number of documents in the reference corpus, and dfk is th</context>
</contexts>
<marker>Sparck-Jones, Sakai, 2001</marker>
<rawString>Karen Sparck-Jones, &amp; Tetsuya Sakai. 2001, September 2001. Generic summaries for indexing in IR. Paper presented at the ACM SIGIR-2001, New Orleans, LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
</authors>
<title>Fast generation of abstracts from general domain text corpora by extracting relevant sentences. Paper presented at the COLING-96,</title>
<date>1996</date>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2206" citStr="Zechner (1996)" startWordPosition="301" endWordPosition="302">g a summary generally involves three subtasks (Brandow et al. (1995); Mitra et al. (1997)): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating a new text (the summary). Like most approaches to summarization, we are concerned with the second subtask (e.g. Carlson et al. (2001); Goldstein et al. (1999); Gong &amp; Liu (2001); Jing et al. (1998); Edward GIBSON Massachusetts Institute of Technology MIT NE20-459, 3 Cambridge Center Cambridge, MA 02139, USA egibson@mit.edu Luhn (1958); Mitra et al. (1997); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf. Goldstein et al. (1999), Radev et al. (2002) for that distinction). We evaluated different approaches to sentence ranking against human sentence rankings. To obtain human sentence rankings, we asked people to read 15 texts from the Wall Street Journal on a wide variety of topics (e.g. economics, foreign and domestic affairs, political commentaries). For each of the sentences in the text, they provided a ranking of how important that sentence is with respect to the content of the text, on</context>
<context position="5601" citStr="Zechner (1996)" startWordPosition="836" endWordPosition="837">gnificant words (we chose n = 3, cf. Luhn (1958)). Then, the weight of each cluster is calculated by dividing the square of the number of significant words in the cluster by the total number of words in the cluster. Sentences can contain multiple clusters. In order to compute the weight of a sentence, the weights of all clusters in that sentence are added. The higher the weight of a sentence, the higher is its ranking. A more recent and frequently used word-based method used for text piece ranking is tf.idf (e.g. Manning &amp; Schuetze (2000); Salton &amp; Buckley (1988); Sparck-Jones &amp; Sakai (2001); Zechner (1996)). The tf.idf measure relates the frequency of words in a text piece, in the text, and in a collection of texts respectively. The intuition behind tf.idf is to give more weight to sentences that contain terms with high frequency in a document but low frequency in a reference corpus. Figure 1 shows a formula for calculating tf.idf, where dsij is the tf.idf weight of sentence i in document j, nsi is the number of words in sentence i, k is the kth word in sentence i, tfjk is the frequency of word k in document j, nd is the number of documents in the reference corpus, and dfk is the number of docu</context>
</contexts>
<marker>Zechner, 1996</marker>
<rawString>Klaus Zechner. 1996. Fast generation of abstracts from general domain text corpora by extracting relevant sentences. Paper presented at the COLING-96, Copenhagen, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>